[01/17 01:27:18] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:27:21] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:27:21] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:27:21] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:27:21] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:27:21] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:27:21] d2.utils.env INFO: Using a generated random seed 21518269
[01/17 01:27:23] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:27:23] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:28:03] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:28:07] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:28:07] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:28:07] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:28:07] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:28:07] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:28:07] d2.utils.env INFO: Using a generated random seed 7525150
[01/17 01:28:08] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:29:03] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:29:07] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:29:07] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:29:07] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:29:07] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:29:07] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:29:07] d2.utils.env INFO: Using a generated random seed 8006357
[01/17 01:29:09] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:29:09] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:29:55] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:29:59] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:29:59] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:29:59] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:29:59] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:29:59] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:29:59] d2.utils.env INFO: Using a generated random seed 59707374
[01/17 01:30:01] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:30:01] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:30:02] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:30:02] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:30:02] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:30:02] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 01:30:03] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 01:30:03] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 01:30:03] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:30:03] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 01:30:03] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 01:30:13] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 391, in run_step
    data = next(self._data_loader_iter)
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 234, in __iter__
    for d in self.dataset:
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 28, in fetch
    data.append(next(self.dataset_iter))
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 201, in __iter__
    yield self.dataset[idx]
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 90, in __getitem__
    data = self._map_func(self._dataset[cur_idx])
  File "/home/nstarli/Mask2Former/mask2former/data/dataset_mappers/mask_former_sceneflow_dataset_mapper.py", line 117, in __call__
    gt = readPFM(self.input_file_to_gt_file[input["file_name"]])[0]
AttributeError: 'MaskFormerSceneFlowDatasetMapper' object has no attribute 'input_file_to_gt_file'

[01/17 01:30:13] d2.engine.hooks INFO: Total training time: 0:00:09 (0:00:00 on hooks)
[01/17 01:30:13] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 496M
[01/17 01:33:32] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:33:37] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:33:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:33:37] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:33:37] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:33:37] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:33:37] d2.utils.env INFO: Using a generated random seed 37615766
[01/17 01:33:39] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:33:39] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:33:40] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:33:40] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:33:40] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:33:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 01:33:41] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 01:33:41] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 01:33:41] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:33:41] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 01:33:41] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 01:33:50] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 391, in run_step
    data = next(self._data_loader_iter)
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 234, in __iter__
    for d in self.dataset:
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
UnboundLocalError: Caught UnboundLocalError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 28, in fetch
    data.append(next(self.dataset_iter))
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 201, in __iter__
    yield self.dataset[idx]
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 90, in __getitem__
    data = self._map_func(self._dataset[cur_idx])
  File "/home/nstarli/Mask2Former/mask2former/data/dataset_mappers/mask_former_sceneflow_dataset_mapper.py", line 124, in __call__
    if sem_seg_gt is None:
UnboundLocalError: local variable 'sem_seg_gt' referenced before assignment

[01/17 01:33:50] d2.engine.hooks INFO: Total training time: 0:00:08 (0:00:00 on hooks)
[01/17 01:33:50] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 494M
[01/17 01:35:16] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:35:19] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:35:19] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:35:19] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:35:19] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:35:20] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:35:20] d2.utils.env INFO: Using a generated random seed 20133345
[01/17 01:35:21] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:35:21] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:35:22] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:35:23] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:35:23] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:35:23] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 01:35:23] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 01:35:24] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 01:35:24] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:35:24] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 01:35:24] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 01:35:33] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 391, in run_step
    data = next(self._data_loader_iter)
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 234, in __iter__
    for d in self.dataset:
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
NameError: Caught NameError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 28, in fetch
    data.append(next(self.dataset_iter))
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 201, in __iter__
    yield self.dataset[idx]
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 90, in __getitem__
    data = self._map_func(self._dataset[cur_idx])
  File "/home/nstarli/Mask2Former/mask2former/data/dataset_mappers/mask_former_sceneflow_dataset_mapper.py", line 120, in __call__
    sem_seg_gt = np.array(gt.round(), dtype=np.int)
NameError: name 'gt' is not defined

[01/17 01:35:33] d2.engine.hooks INFO: Total training time: 0:00:09 (0:00:00 on hooks)
[01/17 01:35:33] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 494M
[01/17 01:36:15] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:36:19] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:36:19] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:36:19] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:36:19] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:36:19] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:36:19] d2.utils.env INFO: Using a generated random seed 19949792
[01/17 01:36:21] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:36:21] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:36:22] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:36:22] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:36:22] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:36:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 01:36:22] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 01:36:23] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 01:36:23] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:36:23] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 01:36:23] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 01:46:11] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:46:14] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3,4,5,6,7     Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:46:14] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:46:14] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBase-SceneFlow-SemanticSegmentationStereo.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerStereo[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerHead[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mGN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;242m# pixel decoder[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMSDeformAttnPixelDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMultiScaleMaskedTransformerDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmulti_scale_pixel_decoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m[38;5;15m  [39m[38;5;242m# 9 decoder layers, add one for the loss on learnable query[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m

[01/17 01:46:14] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/torchvision/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:46:14] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:46:15] d2.utils.env INFO: Using a generated random seed 15166407
[01/17 01:46:16] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:46:16] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:46:20] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:46:22] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:46:22] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:46:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[01/17 01:46:22] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[01/17 01:46:23] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
[01/17 01:46:23] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:46:23] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[01/17 01:47:24] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 01:51:51] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:51:55] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:51:55] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:51:55] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:51:55] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:51:55] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:51:55] d2.utils.env INFO: Using a generated random seed 56067150
[01/17 01:51:57] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:51:58] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:51:59] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:51:59] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:51:59] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:51:59] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 01:51:59] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 01:51:59] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 01:52:00] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:52:00] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 01:52:00] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 01:52:18] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 395, in run_step
    loss_dict = self.model(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 260, in forward
    losses = self.criterion(outputs, targets)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/criterion.py", line 244, in forward
    indices = self.matcher(outputs_without_aux, targets)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/matcher.py", line 179, in forward
    return self.memory_efficient_forward(outputs, targets)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/matcher.py", line 138, in memory_efficient_forward
    cost_mask = batch_sigmoid_ce_loss_jit(out_mask, tgt_mask)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: CUDA driver error: device-side assert triggered

[01/17 01:52:18] d2.engine.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[01/17 01:52:18] d2.utils.events INFO:  iter: 1  total_loss: 188.3  loss_ce: 10.81  loss_mask: 2.093  loss_dice: 4.907  loss_ce_0: 9.272  loss_mask_0: 3.552  loss_dice_0: 4.913  loss_ce_1: 9.699  loss_mask_1: 3.654  loss_dice_1: 4.913  loss_ce_2: 10.04  loss_mask_2: 3.624  loss_dice_2: 4.909  loss_ce_3: 10.58  loss_mask_3: 3.306  loss_dice_3: 4.897  loss_ce_4: 10.81  loss_mask_4: 4.022  loss_dice_4: 4.911  loss_ce_5: 10.85  loss_mask_5: 3.945  loss_dice_5: 4.905  loss_ce_6: 10.69  loss_mask_6: 3.697  loss_dice_6: 4.904  loss_ce_7: 10.83  loss_mask_7: 4.21  loss_dice_7: 4.905  loss_ce_8: 10.63  loss_mask_8: 2.952  loss_dice_8: 4.903  data_time: 10.5422  lr: 1e-05  max_mem: 19017M
[01/17 01:55:01] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:55:05] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:55:05] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:55:05] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:55:05] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:55:05] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:55:05] d2.utils.env INFO: Using a generated random seed 5387224
[01/17 01:55:07] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:55:07] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:55:08] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:55:10] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:55:10] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:55:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 01:55:10] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 01:55:10] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 01:55:10] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:55:10] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 01:55:10] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 01:56:04] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 01:56:08] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 01:56:08] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 01:56:08] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 01:56:08] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 01:56:08] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 01:56:09] d2.utils.env INFO: Using a generated random seed 9244808
[01/17 01:56:10] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 01:56:11] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 01:56:12] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 01:56:13] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 01:56:13] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 01:56:13] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 01:56:13] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 01:56:14] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 01:56:14] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 01:56:14] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 01:56:14] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 02:00:28] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 02:00:31] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3,4,5,6,7     Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 02:00:31] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 02:00:31] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBase-SceneFlow-SemanticSegmentationStereo.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerStereo[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerHead[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mGN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;242m# pixel decoder[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMSDeformAttnPixelDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMultiScaleMaskedTransformerDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmulti_scale_pixel_decoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m[38;5;15m  [39m[38;5;242m# 9 decoder layers, add one for the loss on learnable query[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m

[01/17 02:00:32] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/torchvision/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 02:00:32] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 02:00:32] d2.utils.env INFO: Using a generated random seed 32764669
[01/17 02:00:34] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 02:00:34] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 02:00:38] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 02:00:39] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 02:00:39] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 02:00:40] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[01/17 02:00:40] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[01/17 02:00:41] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
[01/17 02:00:41] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 02:00:41] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[01/17 02:09:38] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 02:28:46] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 02:28:50] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 02:28:50] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 02:28:50] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 02:28:51] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1500[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 02:28:51] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 02:28:51] d2.utils.env INFO: Using a generated random seed 51282096
[01/17 02:28:52] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 02:28:53] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 02:28:54] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 02:28:54] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 02:28:54] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 02:28:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 02:28:55] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 02:28:55] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 02:28:55] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 02:28:55] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 02:28:55] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 02:29:52] d2.utils.events INFO:  eta: 2 days, 6:18:35  iter: 19  total_loss: 148  loss_ce: 9.793  loss_mask: 0.6377  loss_dice: 4.82  loss_ce_0: 9.275  loss_mask_0: 0.3253  loss_dice_0: 4.863  loss_ce_1: 9.335  loss_mask_1: 0.4286  loss_dice_1: 4.825  loss_ce_2: 9.449  loss_mask_2: 0.4272  loss_dice_2: 4.881  loss_ce_3: 9.611  loss_mask_3: 0.4385  loss_dice_3: 4.89  loss_ce_4: 9.671  loss_mask_4: 0.5468  loss_dice_4: 4.881  loss_ce_5: 9.76  loss_mask_5: 0.5054  loss_dice_5: 4.905  loss_ce_6: 9.794  loss_mask_6: 0.583  loss_dice_6: 4.875  loss_ce_7: 9.779  loss_mask_7: 0.5978  loss_dice_7: 4.857  loss_ce_8: 9.765  loss_mask_8: 0.5549  loss_dice_8: 4.89  time: 2.2036  data_time: 0.6045  lr: 9.9981e-06  max_mem: 20304M
[01/17 02:30:25] d2.utils.events INFO:  eta: 2 days, 2:05:08  iter: 39  total_loss: 131.2  loss_ce: 9.153  loss_mask: 0.4608  loss_dice: 4.456  loss_ce_0: 9.098  loss_mask_0: 0.2885  loss_dice_0: 4.444  loss_ce_1: 7.963  loss_mask_1: 0.2937  loss_dice_1: 4.432  loss_ce_2: 7.782  loss_mask_2: 0.2988  loss_dice_2: 4.412  loss_ce_3: 7.706  loss_mask_3: 0.3002  loss_dice_3: 4.432  loss_ce_4: 7.785  loss_mask_4: 0.2978  loss_dice_4: 4.432  loss_ce_5: 7.788  loss_mask_5: 0.2978  loss_dice_5: 4.426  loss_ce_6: 8.278  loss_mask_6: 0.3144  loss_dice_6: 4.448  loss_ce_7: 8.745  loss_mask_7: 0.3252  loss_dice_7: 4.459  loss_ce_8: 9.025  loss_mask_8: 0.4077  loss_dice_8: 4.476  time: 1.9099  data_time: 0.0830  lr: 9.9961e-06  max_mem: 20304M
[01/17 02:30:57] d2.utils.events INFO:  eta: 1 day, 17:17:42  iter: 59  total_loss: 110.7  loss_ce: 6.384  loss_mask: 0.291  loss_dice: 4.44  loss_ce_0: 8.877  loss_mask_0: 0.2942  loss_dice_0: 4.448  loss_ce_1: 7.254  loss_mask_1: 0.2974  loss_dice_1: 4.427  loss_ce_2: 6.498  loss_mask_2: 0.3065  loss_dice_2: 4.422  loss_ce_3: 5.902  loss_mask_3: 0.3135  loss_dice_3: 4.459  loss_ce_4: 5.54  loss_mask_4: 0.3117  loss_dice_4: 4.455  loss_ce_5: 5.345  loss_mask_5: 0.3096  loss_dice_5: 4.456  loss_ce_6: 5.401  loss_mask_6: 0.3083  loss_dice_6: 4.466  loss_ce_7: 5.529  loss_mask_7: 0.3067  loss_dice_7: 4.465  loss_ce_8: 5.778  loss_mask_8: 0.3041  loss_dice_8: 4.454  time: 1.7983  data_time: 0.0945  lr: 9.9941e-06  max_mem: 20457M
[01/17 02:31:28] d2.utils.events INFO:  eta: 1 day, 15:59:37  iter: 79  total_loss: 88.98  loss_ce: 2.616  loss_mask: 0.374  loss_dice: 4.526  loss_ce_0: 8.818  loss_mask_0: 0.3083  loss_dice_0: 4.491  loss_ce_1: 6.493  loss_mask_1: 0.3177  loss_dice_1: 4.482  loss_ce_2: 5.03  loss_mask_2: 0.3436  loss_dice_2: 4.482  loss_ce_3: 3.893  loss_mask_3: 0.3508  loss_dice_3: 4.511  loss_ce_4: 3.209  loss_mask_4: 0.3591  loss_dice_4: 4.518  loss_ce_5: 2.793  loss_mask_5: 0.3685  loss_dice_5: 4.521  loss_ce_6: 2.613  loss_mask_6: 0.3675  loss_dice_6: 4.546  loss_ce_7: 2.557  loss_mask_7: 0.3713  loss_dice_7: 4.549  loss_ce_8: 2.561  loss_mask_8: 0.3757  loss_dice_8: 4.541  time: 1.7337  data_time: 0.0886  lr: 9.9921e-06  max_mem: 20457M
[01/17 02:31:59] d2.utils.events INFO:  eta: 1 day, 15:42:32  iter: 99  total_loss: 80.28  loss_ce: 1.65  loss_mask: 0.3895  loss_dice: 4.599  loss_ce_0: 8.677  loss_mask_0: 0.3158  loss_dice_0: 4.524  loss_ce_1: 5.735  loss_mask_1: 0.333  loss_dice_1: 4.519  loss_ce_2: 3.816  loss_mask_2: 0.361  loss_dice_2: 4.536  loss_ce_3: 2.69  loss_mask_3: 0.3645  loss_dice_3: 4.577  loss_ce_4: 2.075  loss_mask_4: 0.3646  loss_dice_4: 4.576  loss_ce_5: 1.772  loss_mask_5: 0.3672  loss_dice_5: 4.59  loss_ce_6: 1.668  loss_mask_6: 0.3622  loss_dice_6: 4.588  loss_ce_7: 1.593  loss_mask_7: 0.3633  loss_dice_7: 4.6  loss_ce_8: 1.607  loss_mask_8: 0.3681  loss_dice_8: 4.608  time: 1.6945  data_time: 0.0925  lr: 9.9901e-06  max_mem: 20457M
[01/17 02:32:29] d2.utils.events INFO:  eta: 1 day, 15:22:34  iter: 119  total_loss: 74.74  loss_ce: 1.343  loss_mask: 0.3945  loss_dice: 4.597  loss_ce_0: 8.489  loss_mask_0: 0.3391  loss_dice_0: 4.508  loss_ce_1: 4.62  loss_mask_1: 0.3656  loss_dice_1: 4.524  loss_ce_2: 2.63  loss_mask_2: 0.3849  loss_dice_2: 4.561  loss_ce_3: 1.773  loss_mask_3: 0.3823  loss_dice_3: 4.589  loss_ce_4: 1.416  loss_mask_4: 0.3859  loss_dice_4: 4.583  loss_ce_5: 1.312  loss_mask_5: 0.3886  loss_dice_5: 4.589  loss_ce_6: 1.254  loss_mask_6: 0.3934  loss_dice_6: 4.591  loss_ce_7: 1.232  loss_mask_7: 0.3995  loss_dice_7: 4.591  loss_ce_8: 1.267  loss_mask_8: 0.3944  loss_dice_8: 4.601  time: 1.6678  data_time: 0.0934  lr: 9.9881e-06  max_mem: 20457M
[01/17 02:33:00] d2.utils.events INFO:  eta: 1 day, 15:10:51  iter: 139  total_loss: 70.57  loss_ce: 0.9913  loss_mask: 0.4096  loss_dice: 4.632  loss_ce_0: 8.259  loss_mask_0: 0.3595  loss_dice_0: 4.529  loss_ce_1: 3.262  loss_mask_1: 0.3936  loss_dice_1: 4.592  loss_ce_2: 1.729  loss_mask_2: 0.4062  loss_dice_2: 4.61  loss_ce_3: 1.199  loss_mask_3: 0.4068  loss_dice_3: 4.618  loss_ce_4: 1.058  loss_mask_4: 0.4067  loss_dice_4: 4.623  loss_ce_5: 0.9948  loss_mask_5: 0.4057  loss_dice_5: 4.631  loss_ce_6: 0.9652  loss_mask_6: 0.414  loss_dice_6: 4.63  loss_ce_7: 0.9226  loss_mask_7: 0.4124  loss_dice_7: 4.637  loss_ce_8: 0.9373  loss_mask_8: 0.4138  loss_dice_8: 4.63  time: 1.6475  data_time: 0.0948  lr: 9.9861e-06  max_mem: 20457M
[01/17 02:33:31] d2.utils.events INFO:  eta: 1 day, 15:07:32  iter: 159  total_loss: 69.38  loss_ce: 1.029  loss_mask: 0.4031  loss_dice: 4.654  loss_ce_0: 7.938  loss_mask_0: 0.3574  loss_dice_0: 4.553  loss_ce_1: 2.546  loss_mask_1: 0.3817  loss_dice_1: 4.621  loss_ce_2: 1.444  loss_mask_2: 0.4037  loss_dice_2: 4.647  loss_ce_3: 1.182  loss_mask_3: 0.4056  loss_dice_3: 4.648  loss_ce_4: 1.108  loss_mask_4: 0.4084  loss_dice_4: 4.651  loss_ce_5: 1.036  loss_mask_5: 0.4036  loss_dice_5: 4.656  loss_ce_6: 0.9993  loss_mask_6: 0.4036  loss_dice_6: 4.65  loss_ce_7: 0.977  loss_mask_7: 0.4027  loss_dice_7: 4.643  loss_ce_8: 0.9945  loss_mask_8: 0.4011  loss_dice_8: 4.65  time: 1.6356  data_time: 0.1040  lr: 9.9841e-06  max_mem: 20531M
[01/17 02:34:02] d2.utils.events INFO:  eta: 1 day, 14:59:23  iter: 179  total_loss: 66.97  loss_ce: 0.8262  loss_mask: 0.4211  loss_dice: 4.64  loss_ce_0: 7.506  loss_mask_0: 0.3692  loss_dice_0: 4.545  loss_ce_1: 1.863  loss_mask_1: 0.4053  loss_dice_1: 4.614  loss_ce_2: 1.11  loss_mask_2: 0.418  loss_dice_2: 4.633  loss_ce_3: 0.8986  loss_mask_3: 0.4234  loss_dice_3: 4.644  loss_ce_4: 0.8507  loss_mask_4: 0.4224  loss_dice_4: 4.645  loss_ce_5: 0.8183  loss_mask_5: 0.4238  loss_dice_5: 4.636  loss_ce_6: 0.8094  loss_mask_6: 0.4324  loss_dice_6: 4.641  loss_ce_7: 0.7864  loss_mask_7: 0.4308  loss_dice_7: 4.646  loss_ce_8: 0.775  loss_mask_8: 0.4326  loss_dice_8: 4.645  time: 1.6261  data_time: 0.0871  lr: 9.9821e-06  max_mem: 21030M
[01/17 02:34:32] d2.utils.events INFO:  eta: 1 day, 14:50:48  iter: 199  total_loss: 66.44  loss_ce: 0.8616  loss_mask: 0.4521  loss_dice: 4.656  loss_ce_0: 7.214  loss_mask_0: 0.3942  loss_dice_0: 4.549  loss_ce_1: 1.564  loss_mask_1: 0.438  loss_dice_1: 4.652  loss_ce_2: 1.036  loss_mask_2: 0.4609  loss_dice_2: 4.66  loss_ce_3: 0.9038  loss_mask_3: 0.4662  loss_dice_3: 4.66  loss_ce_4: 0.8684  loss_mask_4: 0.4663  loss_dice_4: 4.654  loss_ce_5: 0.8451  loss_mask_5: 0.4632  loss_dice_5: 4.658  loss_ce_6: 0.8029  loss_mask_6: 0.4649  loss_dice_6: 4.663  loss_ce_7: 0.8069  loss_mask_7: 0.4612  loss_dice_7: 4.664  loss_ce_8: 0.823  loss_mask_8: 0.4621  loss_dice_8: 4.664  time: 1.6134  data_time: 0.0865  lr: 9.9801e-06  max_mem: 21030M
[01/17 02:35:03] d2.utils.events INFO:  eta: 1 day, 14:45:54  iter: 219  total_loss: 65.56  loss_ce: 0.777  loss_mask: 0.454  loss_dice: 4.64  loss_ce_0: 6.754  loss_mask_0: 0.3923  loss_dice_0: 4.54  loss_ce_1: 1.303  loss_mask_1: 0.4377  loss_dice_1: 4.631  loss_ce_2: 0.9279  loss_mask_2: 0.4533  loss_dice_2: 4.631  loss_ce_3: 0.858  loss_mask_3: 0.459  loss_dice_3: 4.632  loss_ce_4: 0.8154  loss_mask_4: 0.4547  loss_dice_4: 4.629  loss_ce_5: 0.8022  loss_mask_5: 0.4643  loss_dice_5: 4.638  loss_ce_6: 0.771  loss_mask_6: 0.4596  loss_dice_6: 4.642  loss_ce_7: 0.7642  loss_mask_7: 0.4599  loss_dice_7: 4.637  loss_ce_8: 0.7632  loss_mask_8: 0.4596  loss_dice_8: 4.639  time: 1.6056  data_time: 0.0831  lr: 9.9781e-06  max_mem: 21030M
[01/17 02:35:32] d2.engine.hooks INFO: Overall training speed: 237 iterations in 0:06:19 (1.5998 s / it)
[01/17 02:35:32] d2.engine.hooks INFO: Total training time: 0:06:19 (0:00:00 on hooks)
[01/17 02:35:32] d2.utils.events INFO:  eta: 1 day, 14:43:38  iter: 239  total_loss: 64.81  loss_ce: 0.7616  loss_mask: 0.4769  loss_dice: 4.658  loss_ce_0: 6.381  loss_mask_0: 0.3969  loss_dice_0: 4.558  loss_ce_1: 1.22  loss_mask_1: 0.4483  loss_dice_1: 4.658  loss_ce_2: 0.8977  loss_mask_2: 0.4691  loss_dice_2: 4.653  loss_ce_3: 0.8352  loss_mask_3: 0.4706  loss_dice_3: 4.657  loss_ce_4: 0.7868  loss_mask_4: 0.4671  loss_dice_4: 4.655  loss_ce_5: 0.7546  loss_mask_5: 0.474  loss_dice_5: 4.66  loss_ce_6: 0.7665  loss_mask_6: 0.476  loss_dice_6: 4.654  loss_ce_7: 0.7546  loss_mask_7: 0.4798  loss_dice_7: 4.66  loss_ce_8: 0.7658  loss_mask_8: 0.4788  loss_dice_8: 4.666  time: 1.5991  data_time: 0.0891  lr: 9.9762e-06  max_mem: 21030M
[01/17 02:37:34] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 02:37:37] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 02:37:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 02:37:37] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 02:37:37] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mscenflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 02:37:37] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 02:37:37] d2.utils.env INFO: Using a generated random seed 37354398
[01/17 02:37:38] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 02:37:39] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 02:37:40] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 02:37:41] d2.data.common INFO: Serializing 2239 elements to byte tensors and concatenating them all ...
[01/17 02:37:41] d2.data.common INFO: Serialized dataset takes 0.77 MiB
[01/17 02:37:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 02:37:41] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 02:37:42] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 02:37:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 02:37:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 02:37:42] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 02:38:38] d2.utils.events INFO:  eta: 2 days, 5:46:18  iter: 19  total_loss: 147.9  loss_ce: 9.814  loss_mask: 0.6001  loss_dice: 4.777  loss_ce_0: 9.191  loss_mask_0: 0.3567  loss_dice_0: 4.855  loss_ce_1: 9.148  loss_mask_1: 0.4413  loss_dice_1: 4.792  loss_ce_2: 9.402  loss_mask_2: 0.4584  loss_dice_2: 4.721  loss_ce_3: 9.645  loss_mask_3: 0.5234  loss_dice_3: 4.671  loss_ce_4: 9.805  loss_mask_4: 0.5688  loss_dice_4: 4.759  loss_ce_5: 9.805  loss_mask_5: 0.592  loss_dice_5: 4.755  loss_ce_6: 9.827  loss_mask_6: 0.5777  loss_dice_6: 4.811  loss_ce_7: 9.862  loss_mask_7: 0.5581  loss_dice_7: 4.841  loss_ce_8: 9.831  loss_mask_8: 0.5666  loss_dice_8: 4.803  time: 2.1674  data_time: 0.6092  lr: 9.9981e-06  max_mem: 20453M
[01/17 02:39:10] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/data/catalog.py", line 51, in get
    f = self[name]
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/collections/__init__.py", line 1010, in __getitem__
    raise KeyError(key)
KeyError: 'scenflow_test'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 150, in train
    self.after_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 180, in after_step
    h.after_step()
  File "/home/nstarli/detectron2/detectron2/engine/hooks.py", line 552, in after_step
    self._do_eval()
  File "/home/nstarli/detectron2/detectron2/engine/hooks.py", line 525, in _do_eval
    results = self._func()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 453, in test_and_save_results
    self._last_eval_results = self.test(self.cfg, self.model)
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 602, in test
    data_loader = cls.build_test_loader(cfg, dataset_name)
  File "/home/nstarli/Mask2Former/train_net_stereo.py", line 130, in build_test_loader
    return build_detection_test_loader(cfg, dataset_name, mapper=mapper)
  File "/home/nstarli/detectron2/detectron2/config/config.py", line 207, in wrapped
    explicit_args = _get_args_from_config(from_config, *args, **kwargs)
  File "/home/nstarli/detectron2/detectron2/config/config.py", line 245, in _get_args_from_config
    ret = from_config_func(*args, **kwargs)
  File "/home/nstarli/detectron2/detectron2/data/build.py", line 449, in _test_loader_from_config
    dataset = get_detection_dataset_dicts(
  File "/home/nstarli/detectron2/detectron2/data/build.py", line 241, in get_detection_dataset_dicts
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in names]
  File "/home/nstarli/detectron2/detectron2/data/build.py", line 241, in <listcomp>
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in names]
  File "/home/nstarli/detectron2/detectron2/data/catalog.py", line 53, in get
    raise KeyError(
KeyError: "Dataset 'scenflow_test' is not registered! Available datasets are: coco_2014_train, coco_2014_val, coco_2014_minival, coco_2014_valminusminival, coco_2017_train, coco_2017_val, coco_2017_test, coco_2017_test-dev, coco_2017_val_100, keypoints_coco_2014_train, keypoints_coco_2014_val, keypoints_coco_2014_minival, keypoints_coco_2014_valminusminival, keypoints_coco_2017_train, keypoints_coco_2017_val, keypoints_coco_2017_val_100, coco_2017_train_panoptic_separated, coco_2017_train_panoptic_stuffonly, coco_2017_train_panoptic, coco_2017_val_panoptic_separated, coco_2017_val_panoptic_stuffonly, coco_2017_val_panoptic, coco_2017_val_100_panoptic_separated, coco_2017_val_100_panoptic_stuffonly, coco_2017_val_100_panoptic, lvis_v1_train, lvis_v1_val, lvis_v1_test_dev, lvis_v1_test_challenge, lvis_v0.5_train, lvis_v0.5_val, lvis_v0.5_val_rand_100, lvis_v0.5_test, lvis_v0.5_train_cocofied, lvis_v0.5_val_cocofied, cityscapes_fine_instance_seg_train, cityscapes_fine_sem_seg_train, cityscapes_fine_instance_seg_val, cityscapes_fine_sem_seg_val, cityscapes_fine_instance_seg_test, cityscapes_fine_sem_seg_test, cityscapes_fine_panoptic_train, cityscapes_fine_panoptic_val, voc_2007_trainval, voc_2007_train, voc_2007_val, voc_2007_test, voc_2012_trainval, voc_2012_train, voc_2012_val, ade20k_sem_seg_train, ade20k_sem_seg_val, ade20k_full_sem_seg_train, ade20k_full_sem_seg_val, ade20k_panoptic_train, ade20k_panoptic_val, coco_2017_train_stuff_10k_sem_seg, coco_2017_test_stuff_10k_sem_seg, mapillary_vistas_sem_seg_train, mapillary_vistas_sem_seg_val, coco_2017_train_panoptic_with_sem_seg, coco_2017_val_panoptic_with_sem_seg, ade20k_instance_train, ade20k_instance_val, mapillary_vistas_panoptic_train, mapillary_vistas_panoptic_val, sceneflow_train, sceneflow_test"
[01/17 02:39:10] d2.engine.hooks INFO: Overall training speed: 37 iterations in 0:01:11 (1.9277 s / it)
[01/17 02:39:10] d2.engine.hooks INFO: Total training time: 0:01:11 (0:00:00 on hooks)
[01/17 02:39:10] d2.utils.events INFO:  eta: 1 day, 22:14:50  iter: 39  total_loss: 129.8  loss_ce: 9.114  loss_mask: 0.4051  loss_dice: 4.471  loss_ce_0: 8.959  loss_mask_0: 0.309  loss_dice_0: 4.439  loss_ce_1: 7.87  loss_mask_1: 0.2873  loss_dice_1: 4.426  loss_ce_2: 7.625  loss_mask_2: 0.2902  loss_dice_2: 4.428  loss_ce_3: 7.593  loss_mask_3: 0.308  loss_dice_3: 4.425  loss_ce_4: 7.637  loss_mask_4: 0.2962  loss_dice_4: 4.435  loss_ce_5: 7.698  loss_mask_5: 0.3023  loss_dice_5: 4.453  loss_ce_6: 8.085  loss_mask_6: 0.3252  loss_dice_6: 4.452  loss_ce_7: 8.583  loss_mask_7: 0.3339  loss_dice_7: 4.466  loss_ce_8: 8.934  loss_mask_8: 0.3448  loss_dice_8: 4.485  time: 1.8769  data_time: 0.0853  lr: 9.9961e-06  max_mem: 20453M
[01/17 02:40:26] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 02:40:31] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3,4,5,6,7     Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 02:40:31] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 02:40:32] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBase-SceneFlow-SemanticSegmentationStereo.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerStereo[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerHead[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mGN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;242m# pixel decoder[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMSDeformAttnPixelDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMultiScaleMaskedTransformerDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmulti_scale_pixel_decoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m[38;5;15m  [39m[38;5;242m# 9 decoder layers, add one for the loss on learnable query[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m

[01/17 02:40:32] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/torchvision/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 02:40:32] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 02:40:32] d2.utils.env INFO: Using a generated random seed 32938948
[01/17 02:44:06] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 02:44:11] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 02:44:11] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 02:44:11] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 02:44:11] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m192[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 02:44:11] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 02:44:11] d2.utils.env INFO: Using a generated random seed 11333493
[01/17 02:44:12] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=193, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 192
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 02:44:12] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 02:44:23] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 02:44:23] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 02:44:23] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 02:44:23] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 02:44:23] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 02:44:23] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 02:44:23] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 02:44:23] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 02:44:23] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 02:45:19] d2.utils.events INFO:  eta: 2 days, 5:18:23  iter: 19  total_loss: 148.3  loss_ce: 9.839  loss_mask: 0.5873  loss_dice: 4.874  loss_ce_0: 9.111  loss_mask_0: 0.3574  loss_dice_0: 4.861  loss_ce_1: 9.213  loss_mask_1: 0.3785  loss_dice_1: 4.864  loss_ce_2: 9.332  loss_mask_2: 0.4679  loss_dice_2: 4.812  loss_ce_3: 9.624  loss_mask_3: 0.4275  loss_dice_3: 4.875  loss_ce_4: 9.765  loss_mask_4: 0.5015  loss_dice_4: 4.901  loss_ce_5: 9.855  loss_mask_5: 0.5801  loss_dice_5: 4.868  loss_ce_6: 9.85  loss_mask_6: 0.6075  loss_dice_6: 4.865  loss_ce_7: 9.847  loss_mask_7: 0.5503  loss_dice_7: 4.883  loss_ce_8: 9.835  loss_mask_8: 0.5757  loss_dice_8: 4.864  time: 2.1685  data_time: 0.5989  lr: 9.9981e-06  max_mem: 20160M
[01/17 02:45:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[01/17 02:45:55] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 02:45:55] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 02:45:56] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 02:46:05] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 150, in train
    self.after_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 180, in after_step
    h.after_step()
  File "/home/nstarli/detectron2/detectron2/engine/hooks.py", line 552, in after_step
    self._do_eval()
  File "/home/nstarli/detectron2/detectron2/engine/hooks.py", line 525, in _do_eval
    results = self._func()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 453, in test_and_save_results
    self._last_eval_results = self.test(self.cfg, self.model)
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 617, in test
    results_i = inference_on_dataset(model, data_loader, evaluator)
  File "/home/nstarli/detectron2/detectron2/evaluation/evaluator.py", line 149, in inference_on_dataset
    for idx, inputs in enumerate(data_loader):
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1203, in _next_data
    return self._process_data(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1229, in _process_data
    data.reraise()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/_utils.py", line 425, in reraise
    raise self.exc_type(msg)
PIL.UnidentifiedImageError: Caught UnidentifiedImageError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/nstarli/detectron2/detectron2/data/common.py", line 90, in __getitem__
    data = self._map_func(self._dataset[cur_idx])
  File "/home/nstarli/detectron2/detectron2/data/dataset_mapper.py", line 159, in __call__
    sem_seg_gt = utils.read_image(dataset_dict.pop("sem_seg_file_name"), "L").squeeze(2)
  File "/home/nstarli/detectron2/detectron2/data/detection_utils.py", line 181, in read_image
    image = Image.open(f)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/PIL/Image.py", line 3030, in open
    raise UnidentifiedImageError(
PIL.UnidentifiedImageError: cannot identify image file <_io.BufferedReader name='/home/Datasets/sceneflow/disparity/TEST/A/0077/left/0013.pfm'>

[01/17 02:46:05] d2.engine.hooks INFO: Overall training speed: 37 iterations in 0:01:12 (1.9524 s / it)
[01/17 02:46:05] d2.engine.hooks INFO: Total training time: 0:01:24 (0:00:12 on hooks)
[01/17 02:46:05] d2.utils.events INFO:  eta: 2 days, 1:37:06  iter: 39  total_loss: 128.9  loss_ce: 9.089  loss_mask: 0.4261  loss_dice: 4.451  loss_ce_0: 8.926  loss_mask_0: 0.3054  loss_dice_0: 4.442  loss_ce_1: 7.878  loss_mask_1: 0.2945  loss_dice_1: 4.427  loss_ce_2: 7.637  loss_mask_2: 0.2998  loss_dice_2: 4.417  loss_ce_3: 7.616  loss_mask_3: 0.301  loss_dice_3: 4.424  loss_ce_4: 7.549  loss_mask_4: 0.3089  loss_dice_4: 4.431  loss_ce_5: 7.738  loss_mask_5: 0.2945  loss_dice_5: 4.41  loss_ce_6: 8.118  loss_mask_6: 0.3033  loss_dice_6: 4.416  loss_ce_7: 8.68  loss_mask_7: 0.3037  loss_dice_7: 4.436  loss_ce_8: 8.96  loss_mask_8: 0.3158  loss_dice_8: 4.424  time: 1.9010  data_time: 0.0826  lr: 9.9961e-06  max_mem: 20258M
[01/17 02:52:58] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 02:53:02] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 02:53:02] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 02:53:02] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 02:53:02] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 02:53:02] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 02:53:02] d2.utils.env INFO: Using a generated random seed 2663456
[01/17 02:53:04] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=256, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 255
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 02:53:04] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 02:53:07] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 02:53:08] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 02:53:08] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 02:53:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 02:53:08] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 02:53:09] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 02:53:09] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 02:53:09] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 02:53:09] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 02:54:06] d2.utils.events INFO:  eta: 2 days, 5:24:39  iter: 19  total_loss: 156.4  loss_ce: 10.81  loss_mask: 0.5852  loss_dice: 4.901  loss_ce_0: 9.917  loss_mask_0: 0.3722  loss_dice_0: 4.861  loss_ce_1: 10.33  loss_mask_1: 0.5002  loss_dice_1: 4.863  loss_ce_2: 10.26  loss_mask_2: 0.478  loss_dice_2: 4.895  loss_ce_3: 10.32  loss_mask_3: 0.5766  loss_dice_3: 4.909  loss_ce_4: 10.58  loss_mask_4: 0.5918  loss_dice_4: 4.906  loss_ce_5: 10.53  loss_mask_5: 0.6446  loss_dice_5: 4.864  loss_ce_6: 10.5  loss_mask_6: 0.6172  loss_dice_6: 4.906  loss_ce_7: 10.62  loss_mask_7: 0.7419  loss_dice_7: 4.885  loss_ce_8: 10.61  loss_mask_8: 0.661  loss_dice_8: 4.902  time: 2.1807  data_time: 0.5997  lr: 9.9981e-06  max_mem: 20403M
[01/17 02:54:39] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 02:54:39] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 02:54:40] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 02:54:40] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 02:54:52] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0097 s/iter. Inference: 0.2088 s/iter. Eval: 0.0297 s/iter. Total: 0.2482 s/iter. ETA=0:04:28
[01/17 02:54:57] d2.evaluation.evaluator INFO: Inference done 34/1093. Dataloading: 0.0129 s/iter. Inference: 0.1944 s/iter. Eval: 0.0230 s/iter. Total: 0.2304 s/iter. ETA=0:04:03
[01/17 02:55:02] d2.evaluation.evaluator INFO: Inference done 57/1093. Dataloading: 0.0142 s/iter. Inference: 0.1903 s/iter. Eval: 0.0227 s/iter. Total: 0.2272 s/iter. ETA=0:03:55
[01/17 02:55:07] d2.evaluation.evaluator INFO: Inference done 78/1093. Dataloading: 0.0146 s/iter. Inference: 0.1931 s/iter. Eval: 0.0230 s/iter. Total: 0.2308 s/iter. ETA=0:03:54
[01/17 02:55:13] d2.evaluation.evaluator INFO: Inference done 99/1093. Dataloading: 0.0151 s/iter. Inference: 0.1950 s/iter. Eval: 0.0225 s/iter. Total: 0.2327 s/iter. ETA=0:03:51
[01/17 02:55:18] d2.evaluation.evaluator INFO: Inference done 123/1093. Dataloading: 0.0146 s/iter. Inference: 0.1919 s/iter. Eval: 0.0226 s/iter. Total: 0.2292 s/iter. ETA=0:03:42
[01/17 02:55:23] d2.evaluation.evaluator INFO: Inference done 144/1093. Dataloading: 0.0145 s/iter. Inference: 0.1942 s/iter. Eval: 0.0226 s/iter. Total: 0.2315 s/iter. ETA=0:03:39
[01/17 02:55:28] d2.evaluation.evaluator INFO: Inference done 166/1093. Dataloading: 0.0145 s/iter. Inference: 0.1938 s/iter. Eval: 0.0227 s/iter. Total: 0.2310 s/iter. ETA=0:03:34
[01/17 02:55:33] d2.evaluation.evaluator INFO: Inference done 188/1093. Dataloading: 0.0145 s/iter. Inference: 0.1945 s/iter. Eval: 0.0226 s/iter. Total: 0.2316 s/iter. ETA=0:03:29
[01/17 02:55:38] d2.evaluation.evaluator INFO: Inference done 210/1093. Dataloading: 0.0145 s/iter. Inference: 0.1939 s/iter. Eval: 0.0227 s/iter. Total: 0.2312 s/iter. ETA=0:03:24
[01/17 02:55:43] d2.evaluation.evaluator INFO: Inference done 234/1093. Dataloading: 0.0145 s/iter. Inference: 0.1922 s/iter. Eval: 0.0225 s/iter. Total: 0.2294 s/iter. ETA=0:03:17
[01/17 02:55:48] d2.evaluation.evaluator INFO: Inference done 258/1093. Dataloading: 0.0149 s/iter. Inference: 0.1905 s/iter. Eval: 0.0226 s/iter. Total: 0.2280 s/iter. ETA=0:03:10
[01/17 02:55:53] d2.evaluation.evaluator INFO: Inference done 278/1093. Dataloading: 0.0149 s/iter. Inference: 0.1922 s/iter. Eval: 0.0227 s/iter. Total: 0.2298 s/iter. ETA=0:03:07
[01/17 02:55:59] d2.evaluation.evaluator INFO: Inference done 299/1093. Dataloading: 0.0149 s/iter. Inference: 0.1936 s/iter. Eval: 0.0226 s/iter. Total: 0.2312 s/iter. ETA=0:03:03
[01/17 02:56:04] d2.evaluation.evaluator INFO: Inference done 321/1093. Dataloading: 0.0149 s/iter. Inference: 0.1936 s/iter. Eval: 0.0226 s/iter. Total: 0.2313 s/iter. ETA=0:02:58
[01/17 02:56:09] d2.evaluation.evaluator INFO: Inference done 344/1093. Dataloading: 0.0148 s/iter. Inference: 0.1928 s/iter. Eval: 0.0227 s/iter. Total: 0.2304 s/iter. ETA=0:02:52
[01/17 02:56:14] d2.evaluation.evaluator INFO: Inference done 367/1093. Dataloading: 0.0149 s/iter. Inference: 0.1925 s/iter. Eval: 0.0227 s/iter. Total: 0.2302 s/iter. ETA=0:02:47
[01/17 02:56:19] d2.evaluation.evaluator INFO: Inference done 389/1093. Dataloading: 0.0150 s/iter. Inference: 0.1926 s/iter. Eval: 0.0226 s/iter. Total: 0.2303 s/iter. ETA=0:02:42
[01/17 02:56:24] d2.evaluation.evaluator INFO: Inference done 411/1093. Dataloading: 0.0149 s/iter. Inference: 0.1932 s/iter. Eval: 0.0226 s/iter. Total: 0.2307 s/iter. ETA=0:02:37
[01/17 02:56:29] d2.evaluation.evaluator INFO: Inference done 434/1093. Dataloading: 0.0149 s/iter. Inference: 0.1929 s/iter. Eval: 0.0226 s/iter. Total: 0.2304 s/iter. ETA=0:02:31
[01/17 02:56:35] d2.evaluation.evaluator INFO: Inference done 456/1093. Dataloading: 0.0148 s/iter. Inference: 0.1930 s/iter. Eval: 0.0225 s/iter. Total: 0.2304 s/iter. ETA=0:02:26
[01/17 02:56:40] d2.evaluation.evaluator INFO: Inference done 479/1093. Dataloading: 0.0147 s/iter. Inference: 0.1926 s/iter. Eval: 0.0225 s/iter. Total: 0.2298 s/iter. ETA=0:02:21
[01/17 02:56:45] d2.evaluation.evaluator INFO: Inference done 501/1093. Dataloading: 0.0146 s/iter. Inference: 0.1927 s/iter. Eval: 0.0225 s/iter. Total: 0.2300 s/iter. ETA=0:02:16
[01/17 02:56:50] d2.evaluation.evaluator INFO: Inference done 523/1093. Dataloading: 0.0146 s/iter. Inference: 0.1930 s/iter. Eval: 0.0225 s/iter. Total: 0.2302 s/iter. ETA=0:02:11
[01/17 02:56:55] d2.evaluation.evaluator INFO: Inference done 545/1093. Dataloading: 0.0146 s/iter. Inference: 0.1927 s/iter. Eval: 0.0227 s/iter. Total: 0.2302 s/iter. ETA=0:02:06
[01/17 02:57:00] d2.evaluation.evaluator INFO: Inference done 566/1093. Dataloading: 0.0147 s/iter. Inference: 0.1932 s/iter. Eval: 0.0228 s/iter. Total: 0.2308 s/iter. ETA=0:02:01
[01/17 02:57:05] d2.evaluation.evaluator INFO: Inference done 587/1093. Dataloading: 0.0148 s/iter. Inference: 0.1936 s/iter. Eval: 0.0229 s/iter. Total: 0.2314 s/iter. ETA=0:01:57
[01/17 02:57:10] d2.evaluation.evaluator INFO: Inference done 609/1093. Dataloading: 0.0149 s/iter. Inference: 0.1936 s/iter. Eval: 0.0228 s/iter. Total: 0.2314 s/iter. ETA=0:01:51
[01/17 02:57:15] d2.evaluation.evaluator INFO: Inference done 632/1093. Dataloading: 0.0149 s/iter. Inference: 0.1932 s/iter. Eval: 0.0227 s/iter. Total: 0.2309 s/iter. ETA=0:01:46
[01/17 02:57:21] d2.evaluation.evaluator INFO: Inference done 656/1093. Dataloading: 0.0148 s/iter. Inference: 0.1927 s/iter. Eval: 0.0227 s/iter. Total: 0.2303 s/iter. ETA=0:01:40
[01/17 02:57:26] d2.evaluation.evaluator INFO: Inference done 679/1093. Dataloading: 0.0147 s/iter. Inference: 0.1925 s/iter. Eval: 0.0227 s/iter. Total: 0.2300 s/iter. ETA=0:01:35
[01/17 02:57:31] d2.evaluation.evaluator INFO: Inference done 702/1093. Dataloading: 0.0148 s/iter. Inference: 0.1923 s/iter. Eval: 0.0226 s/iter. Total: 0.2298 s/iter. ETA=0:01:29
[01/17 02:57:36] d2.evaluation.evaluator INFO: Inference done 728/1093. Dataloading: 0.0147 s/iter. Inference: 0.1913 s/iter. Eval: 0.0225 s/iter. Total: 0.2286 s/iter. ETA=0:01:23
[01/17 02:57:41] d2.evaluation.evaluator INFO: Inference done 751/1093. Dataloading: 0.0147 s/iter. Inference: 0.1913 s/iter. Eval: 0.0225 s/iter. Total: 0.2286 s/iter. ETA=0:01:18
[01/17 02:57:46] d2.evaluation.evaluator INFO: Inference done 772/1093. Dataloading: 0.0147 s/iter. Inference: 0.1916 s/iter. Eval: 0.0225 s/iter. Total: 0.2288 s/iter. ETA=0:01:13
[01/17 02:57:51] d2.evaluation.evaluator INFO: Inference done 797/1093. Dataloading: 0.0146 s/iter. Inference: 0.1910 s/iter. Eval: 0.0224 s/iter. Total: 0.2281 s/iter. ETA=0:01:07
[01/17 02:57:56] d2.evaluation.evaluator INFO: Inference done 820/1093. Dataloading: 0.0145 s/iter. Inference: 0.1911 s/iter. Eval: 0.0222 s/iter. Total: 0.2279 s/iter. ETA=0:01:02
[01/17 02:58:02] d2.evaluation.evaluator INFO: Inference done 843/1093. Dataloading: 0.0144 s/iter. Inference: 0.1912 s/iter. Eval: 0.0222 s/iter. Total: 0.2278 s/iter. ETA=0:00:56
[01/17 02:58:07] d2.evaluation.evaluator INFO: Inference done 866/1093. Dataloading: 0.0144 s/iter. Inference: 0.1911 s/iter. Eval: 0.0221 s/iter. Total: 0.2277 s/iter. ETA=0:00:51
[01/17 02:58:12] d2.evaluation.evaluator INFO: Inference done 889/1093. Dataloading: 0.0146 s/iter. Inference: 0.1909 s/iter. Eval: 0.0221 s/iter. Total: 0.2276 s/iter. ETA=0:00:46
[01/17 02:58:17] d2.evaluation.evaluator INFO: Inference done 913/1093. Dataloading: 0.0145 s/iter. Inference: 0.1907 s/iter. Eval: 0.0221 s/iter. Total: 0.2274 s/iter. ETA=0:00:40
[01/17 02:58:22] d2.evaluation.evaluator INFO: Inference done 935/1093. Dataloading: 0.0145 s/iter. Inference: 0.1909 s/iter. Eval: 0.0221 s/iter. Total: 0.2276 s/iter. ETA=0:00:35
[01/17 02:58:27] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0145 s/iter. Inference: 0.1908 s/iter. Eval: 0.0220 s/iter. Total: 0.2275 s/iter. ETA=0:00:30
[01/17 02:58:32] d2.evaluation.evaluator INFO: Inference done 981/1093. Dataloading: 0.0145 s/iter. Inference: 0.1906 s/iter. Eval: 0.0220 s/iter. Total: 0.2273 s/iter. ETA=0:00:25
[01/17 02:58:38] d2.evaluation.evaluator INFO: Inference done 1004/1093. Dataloading: 0.0145 s/iter. Inference: 0.1906 s/iter. Eval: 0.0220 s/iter. Total: 0.2273 s/iter. ETA=0:00:20
[01/17 02:58:43] d2.evaluation.evaluator INFO: Inference done 1028/1093. Dataloading: 0.0145 s/iter. Inference: 0.1905 s/iter. Eval: 0.0220 s/iter. Total: 0.2270 s/iter. ETA=0:00:14
[01/17 02:58:48] d2.evaluation.evaluator INFO: Inference done 1052/1093. Dataloading: 0.0145 s/iter. Inference: 0.1903 s/iter. Eval: 0.0219 s/iter. Total: 0.2268 s/iter. ETA=0:00:09
[01/17 02:58:53] d2.evaluation.evaluator INFO: Inference done 1077/1093. Dataloading: 0.0144 s/iter. Inference: 0.1899 s/iter. Eval: 0.0219 s/iter. Total: 0.2262 s/iter. ETA=0:00:03
[01/17 02:58:57] d2.evaluation.evaluator INFO: Total inference time: 0:04:06.009638 (0.226112 s / iter per device, on 4 devices)
[01/17 02:58:57] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:26 (0.189446 s / iter per device, on 4 devices)
[01/17 02:58:58] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 0.0017191582586868683, 'fwIoU': 0.0019225191747777298, 'IoU-1': nan, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 0.0, 'IoU-20': 0.0, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 0.0, 'IoU-28': 0.0, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.0, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0022652613875965375, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 0.0, 'IoU-42': 0.0, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.0, 'IoU-50': 0.0, 'IoU-51': 0.0, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.434400936318868, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'IoU-192': 0.0, 'IoU-193': 0.0, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 0.39353268116145423, 'pACC': 0.43426520448184247, 'ACC-1': nan, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 0.0, 'ACC-20': 0.0, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 0.0, 'ACC-28': 0.0, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 0.0, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0023229301036586944, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 0.0, 'ACC-42': 0.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 0.0, 'ACC-50': 0.0, 'ACC-51': 0.0, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 99.9549780849057, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0, 'ACC-192': 0.0, 'ACC-193': 0.0, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 02:58:58] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 02:58:58] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 02:58:58] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 02:58:58] d2.evaluation.testing INFO: copypaste: 0.0017,0.0019,0.3935,0.4343
[01/17 02:58:58] d2.utils.events INFO:  eta: 1 day, 22:50:23  iter: 39  total_loss: 138.3  loss_ce: 9.626  loss_mask: 0.513  loss_dice: 4.436  loss_ce_0: 9.645  loss_mask_0: 0.2892  loss_dice_0: 4.408  loss_ce_1: 8.569  loss_mask_1: 0.2797  loss_dice_1: 4.384  loss_ce_2: 8.505  loss_mask_2: 0.2866  loss_dice_2: 4.391  loss_ce_3: 8.517  loss_mask_3: 0.2952  loss_dice_3: 4.392  loss_ce_4: 8.619  loss_mask_4: 0.2935  loss_dice_4: 4.416  loss_ce_5: 9.011  loss_mask_5: 0.3191  loss_dice_5: 4.41  loss_ce_6: 9.331  loss_mask_6: 0.3386  loss_dice_6: 4.418  loss_ce_7: 9.453  loss_mask_7: 0.4409  loss_dice_7: 4.428  loss_ce_8: 9.526  loss_mask_8: 0.4695  loss_dice_8: 4.427  time: 1.9059  data_time: 0.0919  lr: 9.9961e-06  max_mem: 20403M
[01/17 02:59:30] d2.utils.events INFO:  eta: 1 day, 18:03:07  iter: 59  total_loss: 119.6  loss_ce: 8.08  loss_mask: 0.2826  loss_dice: 4.431  loss_ce_0: 9.568  loss_mask_0: 0.2987  loss_dice_0: 4.429  loss_ce_1: 7.743  loss_mask_1: 0.2996  loss_dice_1: 4.425  loss_ce_2: 7.144  loss_mask_2: 0.3027  loss_dice_2: 4.42  loss_ce_3: 6.574  loss_mask_3: 0.3065  loss_dice_3: 4.419  loss_ce_4: 6.298  loss_mask_4: 0.3042  loss_dice_4: 4.43  loss_ce_5: 6.301  loss_mask_5: 0.3024  loss_dice_5: 4.432  loss_ce_6: 6.467  loss_mask_6: 0.2974  loss_dice_6: 4.43  loss_ce_7: 6.691  loss_mask_7: 0.3054  loss_dice_7: 4.45  loss_ce_8: 7.162  loss_mask_8: 0.2918  loss_dice_8: 4.425  time: 1.8058  data_time: 0.0859  lr: 9.9941e-06  max_mem: 20403M
[01/17 03:00:02] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 03:00:02] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 03:00:03] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 03:00:03] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 03:00:16] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0108 s/iter. Inference: 0.2137 s/iter. Eval: 0.0215 s/iter. Total: 0.2459 s/iter. ETA=0:04:26
[01/17 03:00:21] d2.evaluation.evaluator INFO: Inference done 33/1093. Dataloading: 0.0133 s/iter. Inference: 0.1967 s/iter. Eval: 0.0212 s/iter. Total: 0.2313 s/iter. ETA=0:04:05
[01/17 03:00:26] d2.evaluation.evaluator INFO: Inference done 56/1093. Dataloading: 0.0151 s/iter. Inference: 0.1911 s/iter. Eval: 0.0212 s/iter. Total: 0.2274 s/iter. ETA=0:03:55
[01/17 03:00:31] d2.evaluation.evaluator INFO: Inference done 78/1093. Dataloading: 0.0150 s/iter. Inference: 0.1951 s/iter. Eval: 0.0212 s/iter. Total: 0.2314 s/iter. ETA=0:03:54
[01/17 03:00:36] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0153 s/iter. Inference: 0.1948 s/iter. Eval: 0.0211 s/iter. Total: 0.2313 s/iter. ETA=0:03:49
[01/17 03:00:41] d2.evaluation.evaluator INFO: Inference done 122/1093. Dataloading: 0.0151 s/iter. Inference: 0.1950 s/iter. Eval: 0.0211 s/iter. Total: 0.2314 s/iter. ETA=0:03:44
[01/17 03:00:46] d2.evaluation.evaluator INFO: Inference done 146/1093. Dataloading: 0.0151 s/iter. Inference: 0.1917 s/iter. Eval: 0.0211 s/iter. Total: 0.2281 s/iter. ETA=0:03:35
[01/17 03:00:51] d2.evaluation.evaluator INFO: Inference done 169/1093. Dataloading: 0.0153 s/iter. Inference: 0.1902 s/iter. Eval: 0.0212 s/iter. Total: 0.2268 s/iter. ETA=0:03:29
[01/17 03:00:57] d2.evaluation.evaluator INFO: Inference done 192/1093. Dataloading: 0.0153 s/iter. Inference: 0.1892 s/iter. Eval: 0.0216 s/iter. Total: 0.2262 s/iter. ETA=0:03:23
[01/17 03:01:01] d2.engine.hooks INFO: Overall training speed: 77 iterations in 0:02:16 (1.7721 s / it)
[01/17 03:01:01] d2.engine.hooks INFO: Total training time: 0:07:34 (0:05:17 on hooks)
[01/17 03:01:01] d2.utils.events INFO:  eta: 1 day, 16:57:38  iter: 79  total_loss: 99.61  loss_ce: 4.187  loss_mask: 0.3548  loss_dice: 4.485  loss_ce_0: 9.536  loss_mask_0: 0.308  loss_dice_0: 4.457  loss_ce_1: 7.114  loss_mask_1: 0.311  loss_dice_1: 4.454  loss_ce_2: 5.796  loss_mask_2: 0.3254  loss_dice_2: 4.468  loss_ce_3: 4.844  loss_mask_3: 0.3368  loss_dice_3: 4.466  loss_ce_4: 4.265  loss_mask_4: 0.3421  loss_dice_4: 4.477  loss_ce_5: 4.009  loss_mask_5: 0.3438  loss_dice_5: 4.491  loss_ce_6: 3.895  loss_mask_6: 0.3557  loss_dice_6: 4.489  loss_ce_7: 3.841  loss_mask_7: 0.3636  loss_dice_7: 4.499  loss_ce_8: 3.932  loss_mask_8: 0.3576  loss_dice_8: 4.493  time: 1.7494  data_time: 0.0859  lr: 9.9921e-06  max_mem: 21029M
[01/17 03:02:34] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 03:02:38] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 03:02:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 03:02:38] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 03:02:38] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 03:02:38] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 03:02:38] d2.utils.env INFO: Using a generated random seed 39041293
[01/17 03:02:40] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=256, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 255
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 03:02:40] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 03:02:43] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 03:02:43] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 03:02:43] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 03:02:43] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 03:02:44] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 03:02:44] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 03:02:44] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 03:02:44] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 03:02:44] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 03:02:59] d2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 395, in run_step
    loss_dict = self.model(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 249, in forward
    outputs = self.sem_seg_head(features)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/meta_arch/mask_former_head.py", line 116, in forward
    return self.layers(features, mask)
  File "/home/nstarli/Mask2Former/mask2former/modeling/meta_arch/mask_former_head.py", line 121, in layers
    predictions = self.predictor(multi_scale_features, mask_features, mask)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py", line 398, in forward
    attn_mask[torch.where(attn_mask.sum(-1) == attn_mask.shape[-1])] = False
RuntimeError: CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 31.75 GiB total capacity; 29.28 GiB already allocated; 1005.75 MiB free; 29.33 GiB reserved in total by PyTorch)
[01/17 03:02:59] d2.engine.hooks INFO: Total training time: 0:00:14 (0:00:00 on hooks)
[01/17 03:02:59] d2.utils.events INFO:  iter: 0    lr: N/A  max_mem: 30394M
[01/17 03:03:38] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 03:03:42] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 03:03:42] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 03:03:42] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 03:03:42] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m90000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 03:03:42] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla/config.yaml
[01/17 03:03:42] d2.utils.env INFO: Using a generated random seed 42590037
[01/17 03:03:45] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=256, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 255
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 03:03:45] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 03:03:49] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 03:03:49] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 03:03:49] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 03:03:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 03:03:50] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 03:03:50] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 03:03:50] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 03:03:50] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 03:03:50] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 03:04:48] d2.utils.events INFO:  eta: 2 days, 7:54:14  iter: 19  total_loss: 154  loss_ce: 10.45  loss_mask: 0.5592  loss_dice: 4.899  loss_ce_0: 9.965  loss_mask_0: 0.3325  loss_dice_0: 4.876  loss_ce_1: 9.846  loss_mask_1: 0.356  loss_dice_1: 4.879  loss_ce_2: 9.993  loss_mask_2: 0.4471  loss_dice_2: 4.841  loss_ce_3: 10.09  loss_mask_3: 0.5235  loss_dice_3: 4.835  loss_ce_4: 10.27  loss_mask_4: 0.4501  loss_dice_4: 4.909  loss_ce_5: 10.32  loss_mask_5: 0.4927  loss_dice_5: 4.902  loss_ce_6: 10.34  loss_mask_6: 0.5451  loss_dice_6: 4.895  loss_ce_7: 10.38  loss_mask_7: 0.5317  loss_dice_7: 4.906  loss_ce_8: 10.35  loss_mask_8: 0.5028  loss_dice_8: 4.907  time: 2.2363  data_time: 0.6348  lr: 9.9981e-06  max_mem: 20903M
[01/17 03:05:22] d2.utils.events INFO:  eta: 2 days, 3:00:42  iter: 39  total_loss: 136.6  loss_ce: 9.397  loss_mask: 0.4675  loss_dice: 4.468  loss_ce_0: 9.674  loss_mask_0: 0.2875  loss_dice_0: 4.419  loss_ce_1: 8.505  loss_mask_1: 0.2885  loss_dice_1: 4.407  loss_ce_2: 8.424  loss_mask_2: 0.2868  loss_dice_2: 4.408  loss_ce_3: 8.417  loss_mask_3: 0.2845  loss_dice_3: 4.424  loss_ce_4: 8.476  loss_mask_4: 0.2857  loss_dice_4: 4.411  loss_ce_5: 8.743  loss_mask_5: 0.296  loss_dice_5: 4.415  loss_ce_6: 9.011  loss_mask_6: 0.3016  loss_dice_6: 4.42  loss_ce_7: 9.208  loss_mask_7: 0.3014  loss_dice_7: 4.432  loss_ce_8: 9.336  loss_mask_8: 0.3447  loss_dice_8: 4.46  time: 1.9514  data_time: 0.1000  lr: 9.9961e-06  max_mem: 20956M
[01/17 03:05:54] d2.utils.events INFO:  eta: 1 day, 18:07:55  iter: 59  total_loss: 117.5  loss_ce: 7.653  loss_mask: 0.2953  loss_dice: 4.429  loss_ce_0: 9.569  loss_mask_0: 0.2936  loss_dice_0: 4.423  loss_ce_1: 7.763  loss_mask_1: 0.2992  loss_dice_1: 4.415  loss_ce_2: 7.147  loss_mask_2: 0.3053  loss_dice_2: 4.421  loss_ce_3: 6.649  loss_mask_3: 0.3097  loss_dice_3: 4.434  loss_ce_4: 6.316  loss_mask_4: 0.3134  loss_dice_4: 4.429  loss_ce_5: 6.183  loss_mask_5: 0.3189  loss_dice_5: 4.431  loss_ce_6: 6.159  loss_mask_6: 0.3109  loss_dice_6: 4.451  loss_ce_7: 6.431  loss_mask_7: 0.2941  loss_dice_7: 4.444  loss_ce_8: 6.755  loss_mask_8: 0.295  loss_dice_8: 4.433  time: 1.8336  data_time: 0.0909  lr: 9.9941e-06  max_mem: 20956M
[01/17 03:06:26] d2.utils.events INFO:  eta: 1 day, 16:51:23  iter: 79  total_loss: 99.33  loss_ce: 4.032  loss_mask: 0.3599  loss_dice: 4.443  loss_ce_0: 9.495  loss_mask_0: 0.305  loss_dice_0: 4.408  loss_ce_1: 7.09  loss_mask_1: 0.3137  loss_dice_1: 4.415  loss_ce_2: 5.974  loss_mask_2: 0.3327  loss_dice_2: 4.42  loss_ce_3: 5.06  loss_mask_3: 0.3399  loss_dice_3: 4.423  loss_ce_4: 4.468  loss_mask_4: 0.3479  loss_dice_4: 4.419  loss_ce_5: 4.136  loss_mask_5: 0.3549  loss_dice_5: 4.435  loss_ce_6: 3.879  loss_mask_6: 0.3601  loss_dice_6: 4.442  loss_ce_7: 3.806  loss_mask_7: 0.3646  loss_dice_7: 4.433  loss_ce_8: 3.831  loss_mask_8: 0.3685  loss_dice_8: 4.424  time: 1.7693  data_time: 0.0941  lr: 9.9921e-06  max_mem: 20956M
[01/17 03:06:58] d2.utils.events INFO:  eta: 1 day, 16:31:12  iter: 99  total_loss: 88.14  loss_ce: 2.408  loss_mask: 0.3873  loss_dice: 4.53  loss_ce_0: 9.454  loss_mask_0: 0.3254  loss_dice_0: 4.463  loss_ce_1: 6.42  loss_mask_1: 0.3361  loss_dice_1: 4.484  loss_ce_2: 4.763  loss_mask_2: 0.3604  loss_dice_2: 4.503  loss_ce_3: 3.694  loss_mask_3: 0.3765  loss_dice_3: 4.509  loss_ce_4: 3.065  loss_mask_4: 0.3806  loss_dice_4: 4.516  loss_ce_5: 2.674  loss_mask_5: 0.3764  loss_dice_5: 4.526  loss_ce_6: 2.425  loss_mask_6: 0.3737  loss_dice_6: 4.542  loss_ce_7: 2.38  loss_mask_7: 0.3749  loss_dice_7: 4.545  loss_ce_8: 2.332  loss_mask_8: 0.3859  loss_dice_8: 4.535  time: 1.7348  data_time: 0.0951  lr: 9.9901e-06  max_mem: 20956M
[01/17 03:07:30] d2.utils.events INFO:  eta: 1 day, 16:20:12  iter: 119  total_loss: 78.88  loss_ce: 1.494  loss_mask: 0.3903  loss_dice: 4.593  loss_ce_0: 9.315  loss_mask_0: 0.3441  loss_dice_0: 4.514  loss_ce_1: 5.184  loss_mask_1: 0.358  loss_dice_1: 4.54  loss_ce_2: 3.221  loss_mask_2: 0.3762  loss_dice_2: 4.577  loss_ce_3: 2.248  loss_mask_3: 0.3804  loss_dice_3: 4.587  loss_ce_4: 1.823  loss_mask_4: 0.3869  loss_dice_4: 4.594  loss_ce_5: 1.611  loss_mask_5: 0.3809  loss_dice_5: 4.595  loss_ce_6: 1.506  loss_mask_6: 0.3845  loss_dice_6: 4.599  loss_ce_7: 1.495  loss_mask_7: 0.3847  loss_dice_7: 4.602  loss_ce_8: 1.449  loss_mask_8: 0.3882  loss_dice_8: 4.598  time: 1.7087  data_time: 0.0981  lr: 9.9881e-06  max_mem: 20956M
[01/17 03:08:01] d2.utils.events INFO:  eta: 1 day, 15:49:29  iter: 139  total_loss: 74.99  loss_ce: 1.288  loss_mask: 0.4061  loss_dice: 4.604  loss_ce_0: 9.153  loss_mask_0: 0.3537  loss_dice_0: 4.502  loss_ce_1: 4.012  loss_mask_1: 0.3737  loss_dice_1: 4.546  loss_ce_2: 2.256  loss_mask_2: 0.3924  loss_dice_2: 4.573  loss_ce_3: 1.693  loss_mask_3: 0.3931  loss_dice_3: 4.59  loss_ce_4: 1.507  loss_mask_4: 0.4025  loss_dice_4: 4.585  loss_ce_5: 1.385  loss_mask_5: 0.4021  loss_dice_5: 4.596  loss_ce_6: 1.306  loss_mask_6: 0.4057  loss_dice_6: 4.599  loss_ce_7: 1.283  loss_mask_7: 0.4056  loss_dice_7: 4.598  loss_ce_8: 1.271  loss_mask_8: 0.4142  loss_dice_8: 4.6  time: 1.6860  data_time: 0.0987  lr: 9.9861e-06  max_mem: 20956M
[01/17 03:08:32] d2.utils.events INFO:  eta: 1 day, 15:47:18  iter: 159  total_loss: 71.73  loss_ce: 1.066  loss_mask: 0.4174  loss_dice: 4.622  loss_ce_0: 8.889  loss_mask_0: 0.3627  loss_dice_0: 4.529  loss_ce_1: 3.018  loss_mask_1: 0.3784  loss_dice_1: 4.591  loss_ce_2: 1.633  loss_mask_2: 0.3968  loss_dice_2: 4.609  loss_ce_3: 1.286  loss_mask_3: 0.4071  loss_dice_3: 4.62  loss_ce_4: 1.145  loss_mask_4: 0.4158  loss_dice_4: 4.615  loss_ce_5: 1.1  loss_mask_5: 0.4183  loss_dice_5: 4.611  loss_ce_6: 1.082  loss_mask_6: 0.4164  loss_dice_6: 4.629  loss_ce_7: 1.065  loss_mask_7: 0.4186  loss_dice_7: 4.622  loss_ce_8: 1.055  loss_mask_8: 0.4188  loss_dice_8: 4.627  time: 1.6676  data_time: 0.0928  lr: 9.9841e-06  max_mem: 20956M
[01/17 03:09:03] d2.utils.events INFO:  eta: 1 day, 15:39:49  iter: 179  total_loss: 69.47  loss_ce: 0.9698  loss_mask: 0.4315  loss_dice: 4.624  loss_ce_0: 8.625  loss_mask_0: 0.3859  loss_dice_0: 4.522  loss_ce_1: 2.236  loss_mask_1: 0.3989  loss_dice_1: 4.586  loss_ce_2: 1.243  loss_mask_2: 0.4227  loss_dice_2: 4.605  loss_ce_3: 1.084  loss_mask_3: 0.4277  loss_dice_3: 4.608  loss_ce_4: 1.042  loss_mask_4: 0.4317  loss_dice_4: 4.616  loss_ce_5: 0.9927  loss_mask_5: 0.435  loss_dice_5: 4.618  loss_ce_6: 0.9777  loss_mask_6: 0.4332  loss_dice_6: 4.624  loss_ce_7: 0.9523  loss_mask_7: 0.4319  loss_dice_7: 4.619  loss_ce_8: 0.9969  loss_mask_8: 0.4321  loss_dice_8: 4.62  time: 1.6539  data_time: 0.1076  lr: 9.9821e-06  max_mem: 20956M
[01/17 03:09:34] d2.utils.events INFO:  eta: 1 day, 15:32:25  iter: 199  total_loss: 67.78  loss_ce: 0.8813  loss_mask: 0.4363  loss_dice: 4.628  loss_ce_0: 8.291  loss_mask_0: 0.3925  loss_dice_0: 4.529  loss_ce_1: 1.797  loss_mask_1: 0.407  loss_dice_1: 4.609  loss_ce_2: 1.141  loss_mask_2: 0.425  loss_dice_2: 4.622  loss_ce_3: 0.99  loss_mask_3: 0.4334  loss_dice_3: 4.625  loss_ce_4: 0.9464  loss_mask_4: 0.4348  loss_dice_4: 4.631  loss_ce_5: 0.892  loss_mask_5: 0.4376  loss_dice_5: 4.631  loss_ce_6: 0.8761  loss_mask_6: 0.4412  loss_dice_6: 4.631  loss_ce_7: 0.8621  loss_mask_7: 0.4389  loss_dice_7: 4.622  loss_ce_8: 0.8575  loss_mask_8: 0.4384  loss_dice_8: 4.62  time: 1.6427  data_time: 0.0833  lr: 9.9801e-06  max_mem: 20978M
[01/17 03:10:05] d2.utils.events INFO:  eta: 1 day, 15:24:47  iter: 219  total_loss: 66.76  loss_ce: 0.8297  loss_mask: 0.4379  loss_dice: 4.645  loss_ce_0: 7.859  loss_mask_0: 0.3839  loss_dice_0: 4.541  loss_ce_1: 1.425  loss_mask_1: 0.4129  loss_dice_1: 4.635  loss_ce_2: 0.9434  loss_mask_2: 0.434  loss_dice_2: 4.64  loss_ce_3: 0.8532  loss_mask_3: 0.4364  loss_dice_3: 4.637  loss_ce_4: 0.8301  loss_mask_4: 0.4376  loss_dice_4: 4.643  loss_ce_5: 0.7785  loss_mask_5: 0.4357  loss_dice_5: 4.646  loss_ce_6: 0.7628  loss_mask_6: 0.4309  loss_dice_6: 4.647  loss_ce_7: 0.7439  loss_mask_7: 0.4367  loss_dice_7: 4.661  loss_ce_8: 0.8112  loss_mask_8: 0.4367  loss_dice_8: 4.648  time: 1.6353  data_time: 0.0992  lr: 9.9781e-06  max_mem: 20978M
[01/17 03:10:36] d2.utils.events INFO:  eta: 1 day, 15:10:20  iter: 239  total_loss: 66.04  loss_ce: 0.8301  loss_mask: 0.452  loss_dice: 4.629  loss_ce_0: 7.457  loss_mask_0: 0.4043  loss_dice_0: 4.538  loss_ce_1: 1.295  loss_mask_1: 0.4412  loss_dice_1: 4.62  loss_ce_2: 0.9205  loss_mask_2: 0.46  loss_dice_2: 4.626  loss_ce_3: 0.8721  loss_mask_3: 0.4612  loss_dice_3: 4.633  loss_ce_4: 0.8584  loss_mask_4: 0.4626  loss_dice_4: 4.637  loss_ce_5: 0.8026  loss_mask_5: 0.459  loss_dice_5: 4.632  loss_ce_6: 0.788  loss_mask_6: 0.4562  loss_dice_6: 4.646  loss_ce_7: 0.7902  loss_mask_7: 0.4577  loss_dice_7: 4.64  loss_ce_8: 0.8058  loss_mask_8: 0.4503  loss_dice_8: 4.644  time: 1.6256  data_time: 0.0822  lr: 9.9761e-06  max_mem: 20978M
[01/17 03:11:06] d2.utils.events INFO:  eta: 1 day, 15:07:33  iter: 259  total_loss: 64.66  loss_ce: 0.7077  loss_mask: 0.4643  loss_dice: 4.634  loss_ce_0: 6.994  loss_mask_0: 0.3993  loss_dice_0: 4.53  loss_ce_1: 1.185  loss_mask_1: 0.4486  loss_dice_1: 4.623  loss_ce_2: 0.8662  loss_mask_2: 0.4693  loss_dice_2: 4.621  loss_ce_3: 0.7547  loss_mask_3: 0.4665  loss_dice_3: 4.635  loss_ce_4: 0.7534  loss_mask_4: 0.465  loss_dice_4: 4.637  loss_ce_5: 0.7308  loss_mask_5: 0.4721  loss_dice_5: 4.638  loss_ce_6: 0.6848  loss_mask_6: 0.468  loss_dice_6: 4.641  loss_ce_7: 0.688  loss_mask_7: 0.4617  loss_dice_7: 4.645  loss_ce_8: 0.7061  loss_mask_8: 0.4639  loss_dice_8: 4.637  time: 1.6177  data_time: 0.0940  lr: 9.9741e-06  max_mem: 20978M
[01/17 03:11:37] d2.utils.events INFO:  eta: 1 day, 15:03:13  iter: 279  total_loss: 64.12  loss_ce: 0.6619  loss_mask: 0.4819  loss_dice: 4.653  loss_ce_0: 6.621  loss_mask_0: 0.4145  loss_dice_0: 4.549  loss_ce_1: 0.9958  loss_mask_1: 0.4674  loss_dice_1: 4.646  loss_ce_2: 0.733  loss_mask_2: 0.4791  loss_dice_2: 4.648  loss_ce_3: 0.6804  loss_mask_3: 0.482  loss_dice_3: 4.656  loss_ce_4: 0.7027  loss_mask_4: 0.4757  loss_dice_4: 4.657  loss_ce_5: 0.6715  loss_mask_5: 0.4834  loss_dice_5: 4.649  loss_ce_6: 0.6305  loss_mask_6: 0.488  loss_dice_6: 4.647  loss_ce_7: 0.6329  loss_mask_7: 0.4893  loss_dice_7: 4.663  loss_ce_8: 0.6523  loss_mask_8: 0.4855  loss_dice_8: 4.662  time: 1.6123  data_time: 0.0901  lr: 9.9721e-06  max_mem: 20978M
[01/17 03:12:08] d2.utils.events INFO:  eta: 1 day, 14:57:53  iter: 299  total_loss: 63.11  loss_ce: 0.6576  loss_mask: 0.4687  loss_dice: 4.641  loss_ce_0: 6.08  loss_mask_0: 0.4132  loss_dice_0: 4.551  loss_ce_1: 0.8489  loss_mask_1: 0.4675  loss_dice_1: 4.629  loss_ce_2: 0.6888  loss_mask_2: 0.4748  loss_dice_2: 4.628  loss_ce_3: 0.647  loss_mask_3: 0.4694  loss_dice_3: 4.641  loss_ce_4: 0.6546  loss_mask_4: 0.4681  loss_dice_4: 4.64  loss_ce_5: 0.6262  loss_mask_5: 0.4686  loss_dice_5: 4.644  loss_ce_6: 0.6055  loss_mask_6: 0.4676  loss_dice_6: 4.639  loss_ce_7: 0.5953  loss_mask_7: 0.4722  loss_dice_7: 4.646  loss_ce_8: 0.6178  loss_mask_8: 0.4723  loss_dice_8: 4.639  time: 1.6066  data_time: 0.0869  lr: 9.9701e-06  max_mem: 20978M
[01/17 03:12:38] d2.utils.events INFO:  eta: 1 day, 14:54:26  iter: 319  total_loss: 62.37  loss_ce: 0.6391  loss_mask: 0.4785  loss_dice: 4.636  loss_ce_0: 5.594  loss_mask_0: 0.4182  loss_dice_0: 4.546  loss_ce_1: 0.8473  loss_mask_1: 0.4795  loss_dice_1: 4.627  loss_ce_2: 0.6998  loss_mask_2: 0.4887  loss_dice_2: 4.635  loss_ce_3: 0.665  loss_mask_3: 0.4868  loss_dice_3: 4.63  loss_ce_4: 0.6419  loss_mask_4: 0.4841  loss_dice_4: 4.631  loss_ce_5: 0.6208  loss_mask_5: 0.4875  loss_dice_5: 4.628  loss_ce_6: 0.6244  loss_mask_6: 0.484  loss_dice_6: 4.644  loss_ce_7: 0.6115  loss_mask_7: 0.4821  loss_dice_7: 4.644  loss_ce_8: 0.6123  loss_mask_8: 0.4853  loss_dice_8: 4.635  time: 1.6009  data_time: 0.0897  lr: 9.9681e-06  max_mem: 20978M
[01/17 03:13:08] d2.utils.events INFO:  eta: 1 day, 14:47:26  iter: 339  total_loss: 62.02  loss_ce: 0.6384  loss_mask: 0.4719  loss_dice: 4.651  loss_ce_0: 5.103  loss_mask_0: 0.4226  loss_dice_0: 4.573  loss_ce_1: 0.787  loss_mask_1: 0.4819  loss_dice_1: 4.644  loss_ce_2: 0.6844  loss_mask_2: 0.4827  loss_dice_2: 4.651  loss_ce_3: 0.6527  loss_mask_3: 0.4797  loss_dice_3: 4.647  loss_ce_4: 0.6449  loss_mask_4: 0.4744  loss_dice_4: 4.646  loss_ce_5: 0.634  loss_mask_5: 0.476  loss_dice_5: 4.643  loss_ce_6: 0.6145  loss_mask_6: 0.4707  loss_dice_6: 4.654  loss_ce_7: 0.6004  loss_mask_7: 0.4719  loss_dice_7: 4.65  loss_ce_8: 0.5918  loss_mask_8: 0.4721  loss_dice_8: 4.65  time: 1.5957  data_time: 0.0804  lr: 9.9661e-06  max_mem: 20978M
[01/17 03:13:39] d2.utils.events INFO:  eta: 1 day, 14:42:07  iter: 359  total_loss: 61.53  loss_ce: 0.6021  loss_mask: 0.4582  loss_dice: 4.657  loss_ce_0: 4.756  loss_mask_0: 0.413  loss_dice_0: 4.578  loss_ce_1: 0.8287  loss_mask_1: 0.457  loss_dice_1: 4.654  loss_ce_2: 0.6891  loss_mask_2: 0.4543  loss_dice_2: 4.663  loss_ce_3: 0.6447  loss_mask_3: 0.4498  loss_dice_3: 4.661  loss_ce_4: 0.6431  loss_mask_4: 0.4514  loss_dice_4: 4.655  loss_ce_5: 0.6077  loss_mask_5: 0.4561  loss_dice_5: 4.658  loss_ce_6: 0.5877  loss_mask_6: 0.4552  loss_dice_6: 4.656  loss_ce_7: 0.5878  loss_mask_7: 0.4539  loss_dice_7: 4.664  loss_ce_8: 0.5918  loss_mask_8: 0.4571  loss_dice_8: 4.653  time: 1.5919  data_time: 0.0890  lr: 9.9641e-06  max_mem: 20978M
[01/17 03:14:10] d2.utils.events INFO:  eta: 1 day, 14:39:40  iter: 379  total_loss: 60.84  loss_ce: 0.601  loss_mask: 0.4641  loss_dice: 4.64  loss_ce_0: 4.423  loss_mask_0: 0.4296  loss_dice_0: 4.575  loss_ce_1: 0.7452  loss_mask_1: 0.4715  loss_dice_1: 4.638  loss_ce_2: 0.6338  loss_mask_2: 0.468  loss_dice_2: 4.638  loss_ce_3: 0.6236  loss_mask_3: 0.464  loss_dice_3: 4.639  loss_ce_4: 0.612  loss_mask_4: 0.4657  loss_dice_4: 4.644  loss_ce_5: 0.5896  loss_mask_5: 0.4657  loss_dice_5: 4.643  loss_ce_6: 0.5809  loss_mask_6: 0.4662  loss_dice_6: 4.639  loss_ce_7: 0.5892  loss_mask_7: 0.4634  loss_dice_7: 4.633  loss_ce_8: 0.5828  loss_mask_8: 0.465  loss_dice_8: 4.638  time: 1.5893  data_time: 0.0917  lr: 9.9621e-06  max_mem: 20978M
[01/17 03:14:41] d2.utils.events INFO:  eta: 1 day, 14:37:16  iter: 399  total_loss: 60.81  loss_ce: 0.5729  loss_mask: 0.465  loss_dice: 4.658  loss_ce_0: 4.083  loss_mask_0: 0.4264  loss_dice_0: 4.579  loss_ce_1: 0.7484  loss_mask_1: 0.4603  loss_dice_1: 4.646  loss_ce_2: 0.6304  loss_mask_2: 0.4614  loss_dice_2: 4.65  loss_ce_3: 0.6149  loss_mask_3: 0.4579  loss_dice_3: 4.653  loss_ce_4: 0.5794  loss_mask_4: 0.4612  loss_dice_4: 4.652  loss_ce_5: 0.5863  loss_mask_5: 0.4651  loss_dice_5: 4.641  loss_ce_6: 0.5588  loss_mask_6: 0.4637  loss_dice_6: 4.647  loss_ce_7: 0.5618  loss_mask_7: 0.4657  loss_dice_7: 4.653  loss_ce_8: 0.5579  loss_mask_8: 0.4627  loss_dice_8: 4.649  time: 1.5869  data_time: 0.0918  lr: 9.9601e-06  max_mem: 20978M
[01/17 03:15:12] d2.utils.events INFO:  eta: 1 day, 14:36:45  iter: 419  total_loss: 60.16  loss_ce: 0.5762  loss_mask: 0.4686  loss_dice: 4.633  loss_ce_0: 3.781  loss_mask_0: 0.4301  loss_dice_0: 4.579  loss_ce_1: 0.7928  loss_mask_1: 0.4601  loss_dice_1: 4.634  loss_ce_2: 0.6582  loss_mask_2: 0.4665  loss_dice_2: 4.637  loss_ce_3: 0.6229  loss_mask_3: 0.4664  loss_dice_3: 4.637  loss_ce_4: 0.6304  loss_mask_4: 0.4666  loss_dice_4: 4.63  loss_ce_5: 0.5928  loss_mask_5: 0.4677  loss_dice_5: 4.631  loss_ce_6: 0.5728  loss_mask_6: 0.4691  loss_dice_6: 4.635  loss_ce_7: 0.5787  loss_mask_7: 0.4694  loss_dice_7: 4.633  loss_ce_8: 0.5731  loss_mask_8: 0.4668  loss_dice_8: 4.636  time: 1.5848  data_time: 0.0856  lr: 9.9581e-06  max_mem: 20978M
[01/17 03:15:43] d2.utils.events INFO:  eta: 1 day, 14:36:54  iter: 439  total_loss: 59.74  loss_ce: 0.5985  loss_mask: 0.472  loss_dice: 4.611  loss_ce_0: 3.647  loss_mask_0: 0.4376  loss_dice_0: 4.567  loss_ce_1: 0.6866  loss_mask_1: 0.4736  loss_dice_1: 4.613  loss_ce_2: 0.6187  loss_mask_2: 0.4698  loss_dice_2: 4.606  loss_ce_3: 0.5989  loss_mask_3: 0.4715  loss_dice_3: 4.612  loss_ce_4: 0.6037  loss_mask_4: 0.4735  loss_dice_4: 4.604  loss_ce_5: 0.582  loss_mask_5: 0.4738  loss_dice_5: 4.596  loss_ce_6: 0.5655  loss_mask_6: 0.4726  loss_dice_6: 4.595  loss_ce_7: 0.5713  loss_mask_7: 0.474  loss_dice_7: 4.598  loss_ce_8: 0.6006  loss_mask_8: 0.4726  loss_dice_8: 4.601  time: 1.5833  data_time: 0.0933  lr: 9.9561e-06  max_mem: 20978M
[01/17 03:16:13] d2.utils.events INFO:  eta: 1 day, 14:34:13  iter: 459  total_loss: 59.51  loss_ce: 0.5281  loss_mask: 0.4818  loss_dice: 4.637  loss_ce_0: 3.365  loss_mask_0: 0.4614  loss_dice_0: 4.597  loss_ce_1: 0.6314  loss_mask_1: 0.4824  loss_dice_1: 4.644  loss_ce_2: 0.5726  loss_mask_2: 0.4824  loss_dice_2: 4.641  loss_ce_3: 0.5391  loss_mask_3: 0.4802  loss_dice_3: 4.647  loss_ce_4: 0.5474  loss_mask_4: 0.4787  loss_dice_4: 4.648  loss_ce_5: 0.5282  loss_mask_5: 0.4802  loss_dice_5: 4.638  loss_ce_6: 0.5152  loss_mask_6: 0.4833  loss_dice_6: 4.639  loss_ce_7: 0.5203  loss_mask_7: 0.4844  loss_dice_7: 4.641  loss_ce_8: 0.5226  loss_mask_8: 0.4838  loss_dice_8: 4.637  time: 1.5809  data_time: 0.0898  lr: 9.9541e-06  max_mem: 20978M
[01/17 03:16:44] d2.utils.events INFO:  eta: 1 day, 14:33:05  iter: 479  total_loss: 59.38  loss_ce: 0.5642  loss_mask: 0.4725  loss_dice: 4.647  loss_ce_0: 3.273  loss_mask_0: 0.4477  loss_dice_0: 4.595  loss_ce_1: 0.6668  loss_mask_1: 0.4693  loss_dice_1: 4.632  loss_ce_2: 0.5906  loss_mask_2: 0.4722  loss_dice_2: 4.638  loss_ce_3: 0.5585  loss_mask_3: 0.4727  loss_dice_3: 4.64  loss_ce_4: 0.5715  loss_mask_4: 0.4738  loss_dice_4: 4.636  loss_ce_5: 0.5507  loss_mask_5: 0.4764  loss_dice_5: 4.64  loss_ce_6: 0.5353  loss_mask_6: 0.474  loss_dice_6: 4.641  loss_ce_7: 0.5261  loss_mask_7: 0.4758  loss_dice_7: 4.629  loss_ce_8: 0.5453  loss_mask_8: 0.4718  loss_dice_8: 4.641  time: 1.5795  data_time: 0.1106  lr: 9.9521e-06  max_mem: 20978M
[01/17 03:17:15] d2.utils.events INFO:  eta: 1 day, 14:31:41  iter: 499  total_loss: 59.14  loss_ce: 0.5651  loss_mask: 0.4905  loss_dice: 4.628  loss_ce_0: 3.052  loss_mask_0: 0.4674  loss_dice_0: 4.6  loss_ce_1: 0.6349  loss_mask_1: 0.4905  loss_dice_1: 4.635  loss_ce_2: 0.5615  loss_mask_2: 0.4943  loss_dice_2: 4.623  loss_ce_3: 0.5607  loss_mask_3: 0.4938  loss_dice_3: 4.627  loss_ce_4: 0.5698  loss_mask_4: 0.4905  loss_dice_4: 4.627  loss_ce_5: 0.535  loss_mask_5: 0.4909  loss_dice_5: 4.63  loss_ce_6: 0.5332  loss_mask_6: 0.4919  loss_dice_6: 4.629  loss_ce_7: 0.5326  loss_mask_7: 0.4915  loss_dice_7: 4.622  loss_ce_8: 0.5484  loss_mask_8: 0.4895  loss_dice_8: 4.626  time: 1.5775  data_time: 0.0912  lr: 9.9501e-06  max_mem: 20978M
[01/17 03:17:46] d2.utils.events INFO:  eta: 1 day, 14:32:03  iter: 519  total_loss: 59.06  loss_ce: 0.5388  loss_mask: 0.4817  loss_dice: 4.624  loss_ce_0: 2.911  loss_mask_0: 0.4557  loss_dice_0: 4.609  loss_ce_1: 0.6715  loss_mask_1: 0.4784  loss_dice_1: 4.631  loss_ce_2: 0.5857  loss_mask_2: 0.4817  loss_dice_2: 4.635  loss_ce_3: 0.5509  loss_mask_3: 0.4818  loss_dice_3: 4.631  loss_ce_4: 0.5441  loss_mask_4: 0.4824  loss_dice_4: 4.626  loss_ce_5: 0.5326  loss_mask_5: 0.4796  loss_dice_5: 4.62  loss_ce_6: 0.5188  loss_mask_6: 0.4794  loss_dice_6: 4.623  loss_ce_7: 0.5214  loss_mask_7: 0.4807  loss_dice_7: 4.627  loss_ce_8: 0.5255  loss_mask_8: 0.4821  loss_dice_8: 4.622  time: 1.5772  data_time: 0.0890  lr: 9.9481e-06  max_mem: 20978M
[01/17 03:18:17] d2.utils.events INFO:  eta: 1 day, 14:31:11  iter: 539  total_loss: 59.09  loss_ce: 0.5563  loss_mask: 0.4945  loss_dice: 4.636  loss_ce_0: 2.8  loss_mask_0: 0.4736  loss_dice_0: 4.615  loss_ce_1: 0.646  loss_mask_1: 0.4984  loss_dice_1: 4.623  loss_ce_2: 0.5777  loss_mask_2: 0.4974  loss_dice_2: 4.629  loss_ce_3: 0.5583  loss_mask_3: 0.4965  loss_dice_3: 4.636  loss_ce_4: 0.5548  loss_mask_4: 0.4941  loss_dice_4: 4.63  loss_ce_5: 0.5583  loss_mask_5: 0.4973  loss_dice_5: 4.626  loss_ce_6: 0.5377  loss_mask_6: 0.4965  loss_dice_6: 4.625  loss_ce_7: 0.5475  loss_mask_7: 0.4978  loss_dice_7: 4.631  loss_ce_8: 0.5394  loss_mask_8: 0.4919  loss_dice_8: 4.628  time: 1.5755  data_time: 0.0989  lr: 9.9461e-06  max_mem: 20978M
[01/17 03:18:48] d2.utils.events INFO:  eta: 1 day, 14:30:55  iter: 559  total_loss: 59.01  loss_ce: 0.5336  loss_mask: 0.489  loss_dice: 4.637  loss_ce_0: 2.76  loss_mask_0: 0.4684  loss_dice_0: 4.633  loss_ce_1: 0.6194  loss_mask_1: 0.4893  loss_dice_1: 4.638  loss_ce_2: 0.5664  loss_mask_2: 0.4935  loss_dice_2: 4.63  loss_ce_3: 0.5328  loss_mask_3: 0.4934  loss_dice_3: 4.635  loss_ce_4: 0.5281  loss_mask_4: 0.492  loss_dice_4: 4.636  loss_ce_5: 0.5227  loss_mask_5: 0.4935  loss_dice_5: 4.633  loss_ce_6: 0.5277  loss_mask_6: 0.4926  loss_dice_6: 4.639  loss_ce_7: 0.5367  loss_mask_7: 0.4886  loss_dice_7: 4.639  loss_ce_8: 0.5265  loss_mask_8: 0.4892  loss_dice_8: 4.633  time: 1.5744  data_time: 0.0967  lr: 9.9441e-06  max_mem: 20978M
[01/17 03:19:19] d2.utils.events INFO:  eta: 1 day, 14:30:09  iter: 579  total_loss: 58.58  loss_ce: 0.5066  loss_mask: 0.4779  loss_dice: 4.623  loss_ce_0: 2.624  loss_mask_0: 0.4728  loss_dice_0: 4.621  loss_ce_1: 0.6248  loss_mask_1: 0.4843  loss_dice_1: 4.633  loss_ce_2: 0.5455  loss_mask_2: 0.4848  loss_dice_2: 4.631  loss_ce_3: 0.5325  loss_mask_3: 0.4824  loss_dice_3: 4.629  loss_ce_4: 0.5331  loss_mask_4: 0.4796  loss_dice_4: 4.625  loss_ce_5: 0.5205  loss_mask_5: 0.4797  loss_dice_5: 4.624  loss_ce_6: 0.5179  loss_mask_6: 0.4815  loss_dice_6: 4.625  loss_ce_7: 0.5114  loss_mask_7: 0.4824  loss_dice_7: 4.622  loss_ce_8: 0.5077  loss_mask_8: 0.4822  loss_dice_8: 4.619  time: 1.5734  data_time: 0.0914  lr: 9.9421e-06  max_mem: 20978M
[01/17 03:19:50] d2.utils.events INFO:  eta: 1 day, 14:29:13  iter: 599  total_loss: 58.83  loss_ce: 0.5471  loss_mask: 0.4876  loss_dice: 4.642  loss_ce_0: 2.671  loss_mask_0: 0.4813  loss_dice_0: 4.63  loss_ce_1: 0.6233  loss_mask_1: 0.4977  loss_dice_1: 4.642  loss_ce_2: 0.5634  loss_mask_2: 0.4966  loss_dice_2: 4.644  loss_ce_3: 0.5453  loss_mask_3: 0.4963  loss_dice_3: 4.636  loss_ce_4: 0.5315  loss_mask_4: 0.4949  loss_dice_4: 4.636  loss_ce_5: 0.5232  loss_mask_5: 0.4944  loss_dice_5: 4.637  loss_ce_6: 0.5339  loss_mask_6: 0.4908  loss_dice_6: 4.643  loss_ce_7: 0.5419  loss_mask_7: 0.4917  loss_dice_7: 4.647  loss_ce_8: 0.5469  loss_mask_8: 0.4914  loss_dice_8: 4.64  time: 1.5721  data_time: 0.0971  lr: 9.9401e-06  max_mem: 20978M
[01/17 03:20:21] d2.utils.events INFO:  eta: 1 day, 14:28:35  iter: 619  total_loss: 58.41  loss_ce: 0.5161  loss_mask: 0.4945  loss_dice: 4.616  loss_ce_0: 2.563  loss_mask_0: 0.4867  loss_dice_0: 4.614  loss_ce_1: 0.611  loss_mask_1: 0.4985  loss_dice_1: 4.627  loss_ce_2: 0.5509  loss_mask_2: 0.4984  loss_dice_2: 4.616  loss_ce_3: 0.5185  loss_mask_3: 0.4961  loss_dice_3: 4.622  loss_ce_4: 0.5261  loss_mask_4: 0.4957  loss_dice_4: 4.612  loss_ce_5: 0.5108  loss_mask_5: 0.4982  loss_dice_5: 4.622  loss_ce_6: 0.5263  loss_mask_6: 0.4989  loss_dice_6: 4.614  loss_ce_7: 0.5148  loss_mask_7: 0.4953  loss_dice_7: 4.614  loss_ce_8: 0.5058  loss_mask_8: 0.4968  loss_dice_8: 4.614  time: 1.5708  data_time: 0.0852  lr: 9.9381e-06  max_mem: 20978M
[01/17 03:20:52] d2.utils.events INFO:  eta: 1 day, 14:28:11  iter: 639  total_loss: 58.9  loss_ce: 0.5475  loss_mask: 0.5056  loss_dice: 4.641  loss_ce_0: 2.52  loss_mask_0: 0.4949  loss_dice_0: 4.639  loss_ce_1: 0.6315  loss_mask_1: 0.5129  loss_dice_1: 4.647  loss_ce_2: 0.5819  loss_mask_2: 0.5121  loss_dice_2: 4.645  loss_ce_3: 0.5517  loss_mask_3: 0.5107  loss_dice_3: 4.65  loss_ce_4: 0.5516  loss_mask_4: 0.5058  loss_dice_4: 4.65  loss_ce_5: 0.5489  loss_mask_5: 0.507  loss_dice_5: 4.651  loss_ce_6: 0.5436  loss_mask_6: 0.5065  loss_dice_6: 4.644  loss_ce_7: 0.5411  loss_mask_7: 0.5079  loss_dice_7: 4.648  loss_ce_8: 0.5398  loss_mask_8: 0.5051  loss_dice_8: 4.648  time: 1.5704  data_time: 0.0909  lr: 9.9361e-06  max_mem: 20978M
[01/17 03:21:23] d2.utils.events INFO:  eta: 1 day, 14:27:51  iter: 659  total_loss: 58.04  loss_ce: 0.518  loss_mask: 0.4922  loss_dice: 4.619  loss_ce_0: 2.431  loss_mask_0: 0.4826  loss_dice_0: 4.622  loss_ce_1: 0.5729  loss_mask_1: 0.4875  loss_dice_1: 4.614  loss_ce_2: 0.5008  loss_mask_2: 0.4894  loss_dice_2: 4.622  loss_ce_3: 0.5219  loss_mask_3: 0.4862  loss_dice_3: 4.607  loss_ce_4: 0.525  loss_mask_4: 0.4866  loss_dice_4: 4.612  loss_ce_5: 0.5124  loss_mask_5: 0.4878  loss_dice_5: 4.619  loss_ce_6: 0.4941  loss_mask_6: 0.4878  loss_dice_6: 4.626  loss_ce_7: 0.4991  loss_mask_7: 0.4908  loss_dice_7: 4.617  loss_ce_8: 0.5134  loss_mask_8: 0.4922  loss_dice_8: 4.618  time: 1.5701  data_time: 0.0936  lr: 9.9341e-06  max_mem: 20978M
[01/17 03:21:53] d2.utils.events INFO:  eta: 1 day, 14:27:05  iter: 679  total_loss: 58.39  loss_ce: 0.5338  loss_mask: 0.5152  loss_dice: 4.62  loss_ce_0: 2.467  loss_mask_0: 0.5032  loss_dice_0: 4.629  loss_ce_1: 0.571  loss_mask_1: 0.5125  loss_dice_1: 4.627  loss_ce_2: 0.5514  loss_mask_2: 0.5172  loss_dice_2: 4.633  loss_ce_3: 0.5207  loss_mask_3: 0.5146  loss_dice_3: 4.628  loss_ce_4: 0.5113  loss_mask_4: 0.5146  loss_dice_4: 4.624  loss_ce_5: 0.5182  loss_mask_5: 0.5127  loss_dice_5: 4.629  loss_ce_6: 0.5126  loss_mask_6: 0.5128  loss_dice_6: 4.626  loss_ce_7: 0.5335  loss_mask_7: 0.5137  loss_dice_7: 4.624  loss_ce_8: 0.5489  loss_mask_8: 0.5116  loss_dice_8: 4.627  time: 1.5686  data_time: 0.0925  lr: 9.9321e-06  max_mem: 20978M
[01/17 03:22:25] d2.utils.events INFO:  eta: 1 day, 14:26:38  iter: 699  total_loss: 58.6  loss_ce: 0.5511  loss_mask: 0.5006  loss_dice: 4.62  loss_ce_0: 2.444  loss_mask_0: 0.4946  loss_dice_0: 4.626  loss_ce_1: 0.6004  loss_mask_1: 0.5025  loss_dice_1: 4.63  loss_ce_2: 0.5153  loss_mask_2: 0.5036  loss_dice_2: 4.632  loss_ce_3: 0.5198  loss_mask_3: 0.4959  loss_dice_3: 4.619  loss_ce_4: 0.5344  loss_mask_4: 0.4953  loss_dice_4: 4.616  loss_ce_5: 0.5407  loss_mask_5: 0.4968  loss_dice_5: 4.61  loss_ce_6: 0.5238  loss_mask_6: 0.496  loss_dice_6: 4.612  loss_ce_7: 0.5323  loss_mask_7: 0.494  loss_dice_7: 4.624  loss_ce_8: 0.5315  loss_mask_8: 0.4981  loss_dice_8: 4.613  time: 1.5682  data_time: 0.0980  lr: 9.9301e-06  max_mem: 20978M
[01/17 03:22:56] d2.utils.events INFO:  eta: 1 day, 14:26:18  iter: 719  total_loss: 57.91  loss_ce: 0.5048  loss_mask: 0.5019  loss_dice: 4.607  loss_ce_0: 2.314  loss_mask_0: 0.4841  loss_dice_0: 4.62  loss_ce_1: 0.566  loss_mask_1: 0.4971  loss_dice_1: 4.626  loss_ce_2: 0.5214  loss_mask_2: 0.4937  loss_dice_2: 4.62  loss_ce_3: 0.5043  loss_mask_3: 0.4954  loss_dice_3: 4.613  loss_ce_4: 0.4975  loss_mask_4: 0.4965  loss_dice_4: 4.611  loss_ce_5: 0.4903  loss_mask_5: 0.4965  loss_dice_5: 4.605  loss_ce_6: 0.4868  loss_mask_6: 0.4992  loss_dice_6: 4.604  loss_ce_7: 0.4904  loss_mask_7: 0.5007  loss_dice_7: 4.609  loss_ce_8: 0.4986  loss_mask_8: 0.502  loss_dice_8: 4.605  time: 1.5676  data_time: 0.0882  lr: 9.9281e-06  max_mem: 20978M
[01/17 03:23:26] d2.utils.events INFO:  eta: 1 day, 14:25:32  iter: 739  total_loss: 57.68  loss_ce: 0.5001  loss_mask: 0.5002  loss_dice: 4.605  loss_ce_0: 2.356  loss_mask_0: 0.5006  loss_dice_0: 4.619  loss_ce_1: 0.5402  loss_mask_1: 0.5033  loss_dice_1: 4.621  loss_ce_2: 0.4976  loss_mask_2: 0.5049  loss_dice_2: 4.612  loss_ce_3: 0.4885  loss_mask_3: 0.5014  loss_dice_3: 4.601  loss_ce_4: 0.4941  loss_mask_4: 0.504  loss_dice_4: 4.602  loss_ce_5: 0.494  loss_mask_5: 0.5019  loss_dice_5: 4.608  loss_ce_6: 0.4928  loss_mask_6: 0.4996  loss_dice_6: 4.602  loss_ce_7: 0.4963  loss_mask_7: 0.4995  loss_dice_7: 4.597  loss_ce_8: 0.5054  loss_mask_8: 0.4974  loss_dice_8: 4.604  time: 1.5662  data_time: 0.0933  lr: 9.9261e-06  max_mem: 20978M
[01/17 03:23:57] d2.utils.events INFO:  eta: 1 day, 14:25:01  iter: 759  total_loss: 57.79  loss_ce: 0.4753  loss_mask: 0.5024  loss_dice: 4.621  loss_ce_0: 2.232  loss_mask_0: 0.4954  loss_dice_0: 4.633  loss_ce_1: 0.5121  loss_mask_1: 0.502  loss_dice_1: 4.631  loss_ce_2: 0.4802  loss_mask_2: 0.5026  loss_dice_2: 4.628  loss_ce_3: 0.4867  loss_mask_3: 0.5004  loss_dice_3: 4.616  loss_ce_4: 0.4614  loss_mask_4: 0.4962  loss_dice_4: 4.622  loss_ce_5: 0.469  loss_mask_5: 0.4981  loss_dice_5: 4.618  loss_ce_6: 0.4633  loss_mask_6: 0.5006  loss_dice_6: 4.615  loss_ce_7: 0.4835  loss_mask_7: 0.5009  loss_dice_7: 4.623  loss_ce_8: 0.4764  loss_mask_8: 0.5038  loss_dice_8: 4.617  time: 1.5655  data_time: 0.0892  lr: 9.9241e-06  max_mem: 20978M
[01/17 03:24:28] d2.utils.events INFO:  eta: 1 day, 14:24:24  iter: 779  total_loss: 57.77  loss_ce: 0.4405  loss_mask: 0.5143  loss_dice: 4.623  loss_ce_0: 2.248  loss_mask_0: 0.5112  loss_dice_0: 4.635  loss_ce_1: 0.493  loss_mask_1: 0.5208  loss_dice_1: 4.632  loss_ce_2: 0.4645  loss_mask_2: 0.5224  loss_dice_2: 4.627  loss_ce_3: 0.4532  loss_mask_3: 0.5214  loss_dice_3: 4.624  loss_ce_4: 0.4376  loss_mask_4: 0.5224  loss_dice_4: 4.618  loss_ce_5: 0.4339  loss_mask_5: 0.5211  loss_dice_5: 4.621  loss_ce_6: 0.4349  loss_mask_6: 0.5215  loss_dice_6: 4.621  loss_ce_7: 0.442  loss_mask_7: 0.5152  loss_dice_7: 4.619  loss_ce_8: 0.4351  loss_mask_8: 0.5131  loss_dice_8: 4.621  time: 1.5650  data_time: 0.0898  lr: 9.9221e-06  max_mem: 20978M
[01/17 03:24:58] d2.utils.events INFO:  eta: 1 day, 14:22:45  iter: 799  total_loss: 57.75  loss_ce: 0.4689  loss_mask: 0.5084  loss_dice: 4.611  loss_ce_0: 2.153  loss_mask_0: 0.5016  loss_dice_0: 4.641  loss_ce_1: 0.508  loss_mask_1: 0.5022  loss_dice_1: 4.623  loss_ce_2: 0.4744  loss_mask_2: 0.5042  loss_dice_2: 4.619  loss_ce_3: 0.4697  loss_mask_3: 0.5032  loss_dice_3: 4.619  loss_ce_4: 0.4689  loss_mask_4: 0.5026  loss_dice_4: 4.61  loss_ce_5: 0.4641  loss_mask_5: 0.5033  loss_dice_5: 4.613  loss_ce_6: 0.4609  loss_mask_6: 0.5054  loss_dice_6: 4.615  loss_ce_7: 0.4663  loss_mask_7: 0.506  loss_dice_7: 4.615  loss_ce_8: 0.4683  loss_mask_8: 0.5057  loss_dice_8: 4.615  time: 1.5639  data_time: 0.0984  lr: 9.9201e-06  max_mem: 20978M
[01/17 03:25:29] d2.utils.events INFO:  eta: 1 day, 14:21:16  iter: 819  total_loss: 57.41  loss_ce: 0.4825  loss_mask: 0.5104  loss_dice: 4.589  loss_ce_0: 2.142  loss_mask_0: 0.4984  loss_dice_0: 4.616  loss_ce_1: 0.522  loss_mask_1: 0.5083  loss_dice_1: 4.611  loss_ce_2: 0.4681  loss_mask_2: 0.5102  loss_dice_2: 4.603  loss_ce_3: 0.4679  loss_mask_3: 0.5091  loss_dice_3: 4.604  loss_ce_4: 0.4664  loss_mask_4: 0.5076  loss_dice_4: 4.598  loss_ce_5: 0.46  loss_mask_5: 0.5082  loss_dice_5: 4.599  loss_ce_6: 0.4559  loss_mask_6: 0.5122  loss_dice_6: 4.596  loss_ce_7: 0.4625  loss_mask_7: 0.508  loss_dice_7: 4.597  loss_ce_8: 0.461  loss_mask_8: 0.5091  loss_dice_8: 4.59  time: 1.5630  data_time: 0.0880  lr: 9.9181e-06  max_mem: 20978M
[01/17 03:26:00] d2.utils.events INFO:  eta: 1 day, 14:20:13  iter: 839  total_loss: 57.44  loss_ce: 0.471  loss_mask: 0.5028  loss_dice: 4.598  loss_ce_0: 2.074  loss_mask_0: 0.4907  loss_dice_0: 4.633  loss_ce_1: 0.5119  loss_mask_1: 0.4987  loss_dice_1: 4.618  loss_ce_2: 0.4775  loss_mask_2: 0.5006  loss_dice_2: 4.612  loss_ce_3: 0.4745  loss_mask_3: 0.4974  loss_dice_3: 4.614  loss_ce_4: 0.4749  loss_mask_4: 0.4994  loss_dice_4: 4.606  loss_ce_5: 0.4719  loss_mask_5: 0.4991  loss_dice_5: 4.603  loss_ce_6: 0.4702  loss_mask_6: 0.5007  loss_dice_6: 4.598  loss_ce_7: 0.4742  loss_mask_7: 0.4993  loss_dice_7: 4.597  loss_ce_8: 0.4821  loss_mask_8: 0.5003  loss_dice_8: 4.601  time: 1.5626  data_time: 0.1035  lr: 9.9161e-06  max_mem: 20978M
[01/17 03:26:30] d2.utils.events INFO:  eta: 1 day, 14:17:59  iter: 859  total_loss: 57.35  loss_ce: 0.4473  loss_mask: 0.5106  loss_dice: 4.587  loss_ce_0: 2.079  loss_mask_0: 0.5164  loss_dice_0: 4.62  loss_ce_1: 0.5112  loss_mask_1: 0.5141  loss_dice_1: 4.61  loss_ce_2: 0.4661  loss_mask_2: 0.512  loss_dice_2: 4.603  loss_ce_3: 0.4615  loss_mask_3: 0.5098  loss_dice_3: 4.595  loss_ce_4: 0.4544  loss_mask_4: 0.5081  loss_dice_4: 4.587  loss_ce_5: 0.4425  loss_mask_5: 0.5088  loss_dice_5: 4.588  loss_ce_6: 0.4354  loss_mask_6: 0.5096  loss_dice_6: 4.586  loss_ce_7: 0.4405  loss_mask_7: 0.5123  loss_dice_7: 4.586  loss_ce_8: 0.4426  loss_mask_8: 0.5085  loss_dice_8: 4.583  time: 1.5610  data_time: 0.0908  lr: 9.9141e-06  max_mem: 20978M
[01/17 03:27:01] d2.utils.events INFO:  eta: 1 day, 14:17:47  iter: 879  total_loss: 57.4  loss_ce: 0.4527  loss_mask: 0.5072  loss_dice: 4.602  loss_ce_0: 2.126  loss_mask_0: 0.4964  loss_dice_0: 4.613  loss_ce_1: 0.4917  loss_mask_1: 0.5058  loss_dice_1: 4.6  loss_ce_2: 0.4663  loss_mask_2: 0.5081  loss_dice_2: 4.599  loss_ce_3: 0.4549  loss_mask_3: 0.5124  loss_dice_3: 4.583  loss_ce_4: 0.4599  loss_mask_4: 0.5096  loss_dice_4: 4.59  loss_ce_5: 0.4551  loss_mask_5: 0.5087  loss_dice_5: 4.586  loss_ce_6: 0.4401  loss_mask_6: 0.5056  loss_dice_6: 4.586  loss_ce_7: 0.443  loss_mask_7: 0.5043  loss_dice_7: 4.592  loss_ce_8: 0.4321  loss_mask_8: 0.5042  loss_dice_8: 4.588  time: 1.5607  data_time: 0.1046  lr: 9.9121e-06  max_mem: 20978M
[01/17 03:27:32] d2.utils.events INFO:  eta: 1 day, 14:14:49  iter: 899  total_loss: 57.33  loss_ce: 0.4717  loss_mask: 0.5232  loss_dice: 4.57  loss_ce_0: 2.098  loss_mask_0: 0.5147  loss_dice_0: 4.622  loss_ce_1: 0.5268  loss_mask_1: 0.5256  loss_dice_1: 4.596  loss_ce_2: 0.4729  loss_mask_2: 0.5231  loss_dice_2: 4.588  loss_ce_3: 0.4706  loss_mask_3: 0.5211  loss_dice_3: 4.584  loss_ce_4: 0.4736  loss_mask_4: 0.5206  loss_dice_4: 4.577  loss_ce_5: 0.4815  loss_mask_5: 0.5215  loss_dice_5: 4.574  loss_ce_6: 0.4649  loss_mask_6: 0.5212  loss_dice_6: 4.574  loss_ce_7: 0.463  loss_mask_7: 0.522  loss_dice_7: 4.573  loss_ce_8: 0.4824  loss_mask_8: 0.5243  loss_dice_8: 4.571  time: 1.5601  data_time: 0.0878  lr: 9.9101e-06  max_mem: 21152M
[01/17 03:28:02] d2.utils.events INFO:  eta: 1 day, 14:15:15  iter: 919  total_loss: 57.32  loss_ce: 0.4387  loss_mask: 0.5138  loss_dice: 4.574  loss_ce_0: 2.088  loss_mask_0: 0.5127  loss_dice_0: 4.607  loss_ce_1: 0.5147  loss_mask_1: 0.5141  loss_dice_1: 4.592  loss_ce_2: 0.4583  loss_mask_2: 0.5163  loss_dice_2: 4.587  loss_ce_3: 0.4451  loss_mask_3: 0.5155  loss_dice_3: 4.578  loss_ce_4: 0.4446  loss_mask_4: 0.5135  loss_dice_4: 4.582  loss_ce_5: 0.4388  loss_mask_5: 0.5142  loss_dice_5: 4.579  loss_ce_6: 0.4349  loss_mask_6: 0.514  loss_dice_6: 4.572  loss_ce_7: 0.4396  loss_mask_7: 0.5121  loss_dice_7: 4.576  loss_ce_8: 0.4436  loss_mask_8: 0.5157  loss_dice_8: 4.574  time: 1.5596  data_time: 0.0955  lr: 9.9081e-06  max_mem: 21152M
[01/17 03:28:33] d2.utils.events INFO:  eta: 1 day, 14:12:49  iter: 939  total_loss: 57.44  loss_ce: 0.4426  loss_mask: 0.5279  loss_dice: 4.604  loss_ce_0: 2.053  loss_mask_0: 0.5205  loss_dice_0: 4.631  loss_ce_1: 0.491  loss_mask_1: 0.5218  loss_dice_1: 4.617  loss_ce_2: 0.4398  loss_mask_2: 0.5212  loss_dice_2: 4.616  loss_ce_3: 0.4262  loss_mask_3: 0.52  loss_dice_3: 4.607  loss_ce_4: 0.4416  loss_mask_4: 0.521  loss_dice_4: 4.604  loss_ce_5: 0.4403  loss_mask_5: 0.5216  loss_dice_5: 4.61  loss_ce_6: 0.4274  loss_mask_6: 0.5231  loss_dice_6: 4.599  loss_ce_7: 0.4267  loss_mask_7: 0.5259  loss_dice_7: 4.602  loss_ce_8: 0.4298  loss_mask_8: 0.5253  loss_dice_8: 4.598  time: 1.5589  data_time: 0.0907  lr: 9.9061e-06  max_mem: 21152M
[01/17 03:29:03] d2.utils.events INFO:  eta: 1 day, 14:10:41  iter: 959  total_loss: 57.37  loss_ce: 0.469  loss_mask: 0.5241  loss_dice: 4.578  loss_ce_0: 1.986  loss_mask_0: 0.5246  loss_dice_0: 4.627  loss_ce_1: 0.5143  loss_mask_1: 0.524  loss_dice_1: 4.603  loss_ce_2: 0.4721  loss_mask_2: 0.5246  loss_dice_2: 4.597  loss_ce_3: 0.4752  loss_mask_3: 0.5245  loss_dice_3: 4.585  loss_ce_4: 0.4605  loss_mask_4: 0.5242  loss_dice_4: 4.582  loss_ce_5: 0.4687  loss_mask_5: 0.5259  loss_dice_5: 4.582  loss_ce_6: 0.4684  loss_mask_6: 0.5244  loss_dice_6: 4.588  loss_ce_7: 0.4712  loss_mask_7: 0.5236  loss_dice_7: 4.576  loss_ce_8: 0.4627  loss_mask_8: 0.524  loss_dice_8: 4.587  time: 1.5579  data_time: 0.0842  lr: 9.904e-06  max_mem: 21152M
[01/17 03:29:34] d2.utils.events INFO:  eta: 1 day, 14:09:58  iter: 979  total_loss: 57.2  loss_ce: 0.4593  loss_mask: 0.5211  loss_dice: 4.588  loss_ce_0: 2.026  loss_mask_0: 0.5164  loss_dice_0: 4.612  loss_ce_1: 0.4909  loss_mask_1: 0.5218  loss_dice_1: 4.601  loss_ce_2: 0.4663  loss_mask_2: 0.5225  loss_dice_2: 4.584  loss_ce_3: 0.4603  loss_mask_3: 0.5189  loss_dice_3: 4.581  loss_ce_4: 0.4619  loss_mask_4: 0.5206  loss_dice_4: 4.581  loss_ce_5: 0.4507  loss_mask_5: 0.5204  loss_dice_5: 4.584  loss_ce_6: 0.459  loss_mask_6: 0.5184  loss_dice_6: 4.591  loss_ce_7: 0.461  loss_mask_7: 0.5183  loss_dice_7: 4.581  loss_ce_8: 0.4564  loss_mask_8: 0.521  loss_dice_8: 4.578  time: 1.5573  data_time: 0.0912  lr: 9.902e-06  max_mem: 21152M
[01/17 03:30:05] d2.utils.events INFO:  eta: 1 day, 14:09:39  iter: 999  total_loss: 57.14  loss_ce: 0.4816  loss_mask: 0.5196  loss_dice: 4.564  loss_ce_0: 2.015  loss_mask_0: 0.513  loss_dice_0: 4.616  loss_ce_1: 0.5291  loss_mask_1: 0.5168  loss_dice_1: 4.594  loss_ce_2: 0.488  loss_mask_2: 0.515  loss_dice_2: 4.579  loss_ce_3: 0.4693  loss_mask_3: 0.5145  loss_dice_3: 4.581  loss_ce_4: 0.4738  loss_mask_4: 0.5146  loss_dice_4: 4.566  loss_ce_5: 0.475  loss_mask_5: 0.5185  loss_dice_5: 4.555  loss_ce_6: 0.4688  loss_mask_6: 0.5177  loss_dice_6: 4.573  loss_ce_7: 0.4728  loss_mask_7: 0.518  loss_dice_7: 4.559  loss_ce_8: 0.4822  loss_mask_8: 0.5206  loss_dice_8: 4.558  time: 1.5573  data_time: 0.0905  lr: 9.9e-06  max_mem: 21152M
[01/17 03:30:36] d2.utils.events INFO:  eta: 1 day, 14:06:40  iter: 1019  total_loss: 57.12  loss_ce: 0.4693  loss_mask: 0.521  loss_dice: 4.557  loss_ce_0: 1.96  loss_mask_0: 0.5168  loss_dice_0: 4.604  loss_ce_1: 0.5322  loss_mask_1: 0.5185  loss_dice_1: 4.576  loss_ce_2: 0.4795  loss_mask_2: 0.5208  loss_dice_2: 4.571  loss_ce_3: 0.4823  loss_mask_3: 0.5197  loss_dice_3: 4.56  loss_ce_4: 0.4625  loss_mask_4: 0.5216  loss_dice_4: 4.56  loss_ce_5: 0.4746  loss_mask_5: 0.521  loss_dice_5: 4.558  loss_ce_6: 0.4816  loss_mask_6: 0.5182  loss_dice_6: 4.554  loss_ce_7: 0.4765  loss_mask_7: 0.5202  loss_dice_7: 4.564  loss_ce_8: 0.4681  loss_mask_8: 0.5207  loss_dice_8: 4.556  time: 1.5569  data_time: 0.0771  lr: 9.898e-06  max_mem: 21152M
[01/17 03:31:06] d2.utils.events INFO:  eta: 1 day, 14:03:06  iter: 1039  total_loss: 57.05  loss_ce: 0.4485  loss_mask: 0.5308  loss_dice: 4.546  loss_ce_0: 1.999  loss_mask_0: 0.5242  loss_dice_0: 4.594  loss_ce_1: 0.5111  loss_mask_1: 0.5337  loss_dice_1: 4.574  loss_ce_2: 0.4771  loss_mask_2: 0.5325  loss_dice_2: 4.565  loss_ce_3: 0.4759  loss_mask_3: 0.5296  loss_dice_3: 4.554  loss_ce_4: 0.4611  loss_mask_4: 0.5326  loss_dice_4: 4.551  loss_ce_5: 0.4454  loss_mask_5: 0.5346  loss_dice_5: 4.544  loss_ce_6: 0.4599  loss_mask_6: 0.5371  loss_dice_6: 4.546  loss_ce_7: 0.4568  loss_mask_7: 0.5367  loss_dice_7: 4.546  loss_ce_8: 0.4491  loss_mask_8: 0.533  loss_dice_8: 4.547  time: 1.5559  data_time: 0.0855  lr: 9.896e-06  max_mem: 21152M
[01/17 03:31:37] d2.utils.events INFO:  eta: 1 day, 14:01:38  iter: 1059  total_loss: 57.04  loss_ce: 0.447  loss_mask: 0.5461  loss_dice: 4.56  loss_ce_0: 1.908  loss_mask_0: 0.5343  loss_dice_0: 4.611  loss_ce_1: 0.4836  loss_mask_1: 0.5373  loss_dice_1: 4.584  loss_ce_2: 0.4507  loss_mask_2: 0.5418  loss_dice_2: 4.565  loss_ce_3: 0.4442  loss_mask_3: 0.5412  loss_dice_3: 4.571  loss_ce_4: 0.4516  loss_mask_4: 0.5398  loss_dice_4: 4.565  loss_ce_5: 0.4401  loss_mask_5: 0.543  loss_dice_5: 4.567  loss_ce_6: 0.4423  loss_mask_6: 0.5437  loss_dice_6: 4.569  loss_ce_7: 0.4472  loss_mask_7: 0.54  loss_dice_7: 4.572  loss_ce_8: 0.4484  loss_mask_8: 0.5435  loss_dice_8: 4.559  time: 1.5557  data_time: 0.0937  lr: 9.894e-06  max_mem: 21152M
[01/17 03:32:08] d2.utils.events INFO:  eta: 1 day, 14:00:34  iter: 1079  total_loss: 56.98  loss_ce: 0.4523  loss_mask: 0.5044  loss_dice: 4.553  loss_ce_0: 1.943  loss_mask_0: 0.5056  loss_dice_0: 4.599  loss_ce_1: 0.5079  loss_mask_1: 0.5011  loss_dice_1: 4.587  loss_ce_2: 0.488  loss_mask_2: 0.5021  loss_dice_2: 4.565  loss_ce_3: 0.4707  loss_mask_3: 0.5024  loss_dice_3: 4.562  loss_ce_4: 0.4726  loss_mask_4: 0.503  loss_dice_4: 4.553  loss_ce_5: 0.458  loss_mask_5: 0.5012  loss_dice_5: 4.556  loss_ce_6: 0.4605  loss_mask_6: 0.5  loss_dice_6: 4.555  loss_ce_7: 0.4608  loss_mask_7: 0.5043  loss_dice_7: 4.547  loss_ce_8: 0.4618  loss_mask_8: 0.5019  loss_dice_8: 4.552  time: 1.5553  data_time: 0.0885  lr: 9.892e-06  max_mem: 21152M
[01/17 03:32:38] d2.utils.events INFO:  eta: 1 day, 13:59:14  iter: 1099  total_loss: 56.86  loss_ce: 0.4586  loss_mask: 0.5215  loss_dice: 4.541  loss_ce_0: 1.896  loss_mask_0: 0.5212  loss_dice_0: 4.574  loss_ce_1: 0.496  loss_mask_1: 0.5268  loss_dice_1: 4.559  loss_ce_2: 0.469  loss_mask_2: 0.5257  loss_dice_2: 4.551  loss_ce_3: 0.4702  loss_mask_3: 0.5243  loss_dice_3: 4.552  loss_ce_4: 0.4613  loss_mask_4: 0.5217  loss_dice_4: 4.546  loss_ce_5: 0.4586  loss_mask_5: 0.5216  loss_dice_5: 4.541  loss_ce_6: 0.4554  loss_mask_6: 0.5237  loss_dice_6: 4.541  loss_ce_7: 0.4657  loss_mask_7: 0.5208  loss_dice_7: 4.539  loss_ce_8: 0.4667  loss_mask_8: 0.5253  loss_dice_8: 4.537  time: 1.5546  data_time: 0.0903  lr: 9.89e-06  max_mem: 21152M
[01/17 03:33:09] d2.utils.events INFO:  eta: 1 day, 13:58:43  iter: 1119  total_loss: 56.73  loss_ce: 0.4316  loss_mask: 0.5297  loss_dice: 4.552  loss_ce_0: 1.833  loss_mask_0: 0.5267  loss_dice_0: 4.602  loss_ce_1: 0.4714  loss_mask_1: 0.5291  loss_dice_1: 4.573  loss_ce_2: 0.4555  loss_mask_2: 0.5294  loss_dice_2: 4.566  loss_ce_3: 0.4442  loss_mask_3: 0.5287  loss_dice_3: 4.563  loss_ce_4: 0.4511  loss_mask_4: 0.5318  loss_dice_4: 4.557  loss_ce_5: 0.4459  loss_mask_5: 0.5317  loss_dice_5: 4.556  loss_ce_6: 0.439  loss_mask_6: 0.5301  loss_dice_6: 4.559  loss_ce_7: 0.4361  loss_mask_7: 0.5308  loss_dice_7: 4.561  loss_ce_8: 0.4394  loss_mask_8: 0.5305  loss_dice_8: 4.552  time: 1.5543  data_time: 0.0844  lr: 9.888e-06  max_mem: 21152M
[01/17 03:33:39] d2.utils.events INFO:  eta: 1 day, 13:56:54  iter: 1139  total_loss: 56.56  loss_ce: 0.4399  loss_mask: 0.5305  loss_dice: 4.535  loss_ce_0: 1.839  loss_mask_0: 0.5301  loss_dice_0: 4.591  loss_ce_1: 0.5008  loss_mask_1: 0.5258  loss_dice_1: 4.556  loss_ce_2: 0.4585  loss_mask_2: 0.5281  loss_dice_2: 4.542  loss_ce_3: 0.4559  loss_mask_3: 0.5308  loss_dice_3: 4.535  loss_ce_4: 0.4557  loss_mask_4: 0.5327  loss_dice_4: 4.538  loss_ce_5: 0.4521  loss_mask_5: 0.5315  loss_dice_5: 4.537  loss_ce_6: 0.4456  loss_mask_6: 0.5307  loss_dice_6: 4.534  loss_ce_7: 0.4443  loss_mask_7: 0.53  loss_dice_7: 4.534  loss_ce_8: 0.4498  loss_mask_8: 0.5298  loss_dice_8: 4.534  time: 1.5533  data_time: 0.0951  lr: 9.886e-06  max_mem: 21152M
[01/17 03:34:09] d2.utils.events INFO:  eta: 1 day, 13:55:37  iter: 1159  total_loss: 56.65  loss_ce: 0.4774  loss_mask: 0.5272  loss_dice: 4.539  loss_ce_0: 1.799  loss_mask_0: 0.5254  loss_dice_0: 4.584  loss_ce_1: 0.5189  loss_mask_1: 0.5308  loss_dice_1: 4.553  loss_ce_2: 0.4683  loss_mask_2: 0.5323  loss_dice_2: 4.539  loss_ce_3: 0.4702  loss_mask_3: 0.5333  loss_dice_3: 4.539  loss_ce_4: 0.4699  loss_mask_4: 0.5328  loss_dice_4: 4.531  loss_ce_5: 0.4628  loss_mask_5: 0.5311  loss_dice_5: 4.545  loss_ce_6: 0.4517  loss_mask_6: 0.5324  loss_dice_6: 4.542  loss_ce_7: 0.4623  loss_mask_7: 0.5294  loss_dice_7: 4.534  loss_ce_8: 0.4643  loss_mask_8: 0.5287  loss_dice_8: 4.543  time: 1.5528  data_time: 0.0961  lr: 9.884e-06  max_mem: 21152M
[01/17 03:34:40] d2.utils.events INFO:  eta: 1 day, 13:55:47  iter: 1179  total_loss: 56.57  loss_ce: 0.446  loss_mask: 0.5404  loss_dice: 4.539  loss_ce_0: 1.824  loss_mask_0: 0.5269  loss_dice_0: 4.584  loss_ce_1: 0.4839  loss_mask_1: 0.5337  loss_dice_1: 4.563  loss_ce_2: 0.4592  loss_mask_2: 0.5364  loss_dice_2: 4.549  loss_ce_3: 0.4515  loss_mask_3: 0.5386  loss_dice_3: 4.539  loss_ce_4: 0.4569  loss_mask_4: 0.5383  loss_dice_4: 4.539  loss_ce_5: 0.4553  loss_mask_5: 0.5381  loss_dice_5: 4.537  loss_ce_6: 0.4442  loss_mask_6: 0.5408  loss_dice_6: 4.537  loss_ce_7: 0.4433  loss_mask_7: 0.5384  loss_dice_7: 4.53  loss_ce_8: 0.4522  loss_mask_8: 0.5412  loss_dice_8: 4.528  time: 1.5523  data_time: 0.0942  lr: 9.882e-06  max_mem: 21152M
[01/17 03:35:11] d2.utils.events INFO:  eta: 1 day, 13:53:10  iter: 1199  total_loss: 56.6  loss_ce: 0.4407  loss_mask: 0.544  loss_dice: 4.528  loss_ce_0: 1.827  loss_mask_0: 0.5348  loss_dice_0: 4.577  loss_ce_1: 0.4607  loss_mask_1: 0.5426  loss_dice_1: 4.551  loss_ce_2: 0.4335  loss_mask_2: 0.5428  loss_dice_2: 4.543  loss_ce_3: 0.435  loss_mask_3: 0.5432  loss_dice_3: 4.53  loss_ce_4: 0.4365  loss_mask_4: 0.5414  loss_dice_4: 4.524  loss_ce_5: 0.4301  loss_mask_5: 0.5401  loss_dice_5: 4.525  loss_ce_6: 0.4319  loss_mask_6: 0.5414  loss_dice_6: 4.532  loss_ce_7: 0.4433  loss_mask_7: 0.5408  loss_dice_7: 4.527  loss_ce_8: 0.4372  loss_mask_8: 0.543  loss_dice_8: 4.528  time: 1.5518  data_time: 0.0916  lr: 9.88e-06  max_mem: 21152M
[01/17 03:35:41] d2.utils.events INFO:  eta: 1 day, 13:51:28  iter: 1219  total_loss: 56.59  loss_ce: 0.4409  loss_mask: 0.537  loss_dice: 4.511  loss_ce_0: 1.786  loss_mask_0: 0.5257  loss_dice_0: 4.567  loss_ce_1: 0.4903  loss_mask_1: 0.5267  loss_dice_1: 4.539  loss_ce_2: 0.4518  loss_mask_2: 0.5291  loss_dice_2: 4.527  loss_ce_3: 0.441  loss_mask_3: 0.5303  loss_dice_3: 4.52  loss_ce_4: 0.4459  loss_mask_4: 0.5304  loss_dice_4: 4.516  loss_ce_5: 0.4435  loss_mask_5: 0.5326  loss_dice_5: 4.512  loss_ce_6: 0.4429  loss_mask_6: 0.5347  loss_dice_6: 4.517  loss_ce_7: 0.4271  loss_mask_7: 0.5339  loss_dice_7: 4.517  loss_ce_8: 0.4316  loss_mask_8: 0.5343  loss_dice_8: 4.512  time: 1.5516  data_time: 0.0924  lr: 9.878e-06  max_mem: 21152M
[01/17 03:36:12] d2.utils.events INFO:  eta: 1 day, 13:51:43  iter: 1239  total_loss: 56.35  loss_ce: 0.4408  loss_mask: 0.5357  loss_dice: 4.52  loss_ce_0: 1.758  loss_mask_0: 0.5262  loss_dice_0: 4.567  loss_ce_1: 0.4674  loss_mask_1: 0.5292  loss_dice_1: 4.531  loss_ce_2: 0.4519  loss_mask_2: 0.531  loss_dice_2: 4.529  loss_ce_3: 0.4343  loss_mask_3: 0.5315  loss_dice_3: 4.526  loss_ce_4: 0.4429  loss_mask_4: 0.5343  loss_dice_4: 4.528  loss_ce_5: 0.4401  loss_mask_5: 0.5359  loss_dice_5: 4.531  loss_ce_6: 0.4454  loss_mask_6: 0.5371  loss_dice_6: 4.519  loss_ce_7: 0.4504  loss_mask_7: 0.5377  loss_dice_7: 4.522  loss_ce_8: 0.4418  loss_mask_8: 0.5372  loss_dice_8: 4.521  time: 1.5515  data_time: 0.0902  lr: 9.876e-06  max_mem: 21234M
[01/17 03:36:43] d2.utils.events INFO:  eta: 1 day, 13:51:13  iter: 1259  total_loss: 56.4  loss_ce: 0.4263  loss_mask: 0.5327  loss_dice: 4.519  loss_ce_0: 1.66  loss_mask_0: 0.5257  loss_dice_0: 4.577  loss_ce_1: 0.4695  loss_mask_1: 0.5288  loss_dice_1: 4.557  loss_ce_2: 0.4467  loss_mask_2: 0.5297  loss_dice_2: 4.533  loss_ce_3: 0.4334  loss_mask_3: 0.5293  loss_dice_3: 4.529  loss_ce_4: 0.4315  loss_mask_4: 0.5311  loss_dice_4: 4.53  loss_ce_5: 0.4284  loss_mask_5: 0.5293  loss_dice_5: 4.526  loss_ce_6: 0.4276  loss_mask_6: 0.5308  loss_dice_6: 4.522  loss_ce_7: 0.4411  loss_mask_7: 0.532  loss_dice_7: 4.53  loss_ce_8: 0.4282  loss_mask_8: 0.5333  loss_dice_8: 4.524  time: 1.5509  data_time: 0.0929  lr: 9.874e-06  max_mem: 21234M
[01/17 03:37:13] d2.utils.events INFO:  eta: 1 day, 13:50:42  iter: 1279  total_loss: 56.38  loss_ce: 0.4586  loss_mask: 0.5335  loss_dice: 4.487  loss_ce_0: 1.723  loss_mask_0: 0.5232  loss_dice_0: 4.538  loss_ce_1: 0.4957  loss_mask_1: 0.5314  loss_dice_1: 4.515  loss_ce_2: 0.484  loss_mask_2: 0.5324  loss_dice_2: 4.5  loss_ce_3: 0.4786  loss_mask_3: 0.539  loss_dice_3: 4.497  loss_ce_4: 0.4606  loss_mask_4: 0.5356  loss_dice_4: 4.498  loss_ce_5: 0.4709  loss_mask_5: 0.5375  loss_dice_5: 4.487  loss_ce_6: 0.4628  loss_mask_6: 0.5366  loss_dice_6: 4.494  loss_ce_7: 0.4569  loss_mask_7: 0.5375  loss_dice_7: 4.487  loss_ce_8: 0.4671  loss_mask_8: 0.5366  loss_dice_8: 4.491  time: 1.5505  data_time: 0.0853  lr: 9.872e-06  max_mem: 21234M
[01/17 03:37:44] d2.utils.events INFO:  eta: 1 day, 13:52:02  iter: 1299  total_loss: 56.1  loss_ce: 0.4117  loss_mask: 0.5401  loss_dice: 4.529  loss_ce_0: 1.62  loss_mask_0: 0.5304  loss_dice_0: 4.576  loss_ce_1: 0.4603  loss_mask_1: 0.5332  loss_dice_1: 4.548  loss_ce_2: 0.438  loss_mask_2: 0.5369  loss_dice_2: 4.54  loss_ce_3: 0.4249  loss_mask_3: 0.5356  loss_dice_3: 4.531  loss_ce_4: 0.4207  loss_mask_4: 0.5377  loss_dice_4: 4.524  loss_ce_5: 0.4132  loss_mask_5: 0.5388  loss_dice_5: 4.526  loss_ce_6: 0.417  loss_mask_6: 0.5406  loss_dice_6: 4.527  loss_ce_7: 0.4134  loss_mask_7: 0.5417  loss_dice_7: 4.529  loss_ce_8: 0.4069  loss_mask_8: 0.5397  loss_dice_8: 4.533  time: 1.5504  data_time: 0.0909  lr: 9.87e-06  max_mem: 21234M
[01/17 03:38:15] d2.utils.events INFO:  eta: 1 day, 13:52:02  iter: 1319  total_loss: 56.34  loss_ce: 0.4583  loss_mask: 0.5422  loss_dice: 4.51  loss_ce_0: 1.678  loss_mask_0: 0.5356  loss_dice_0: 4.55  loss_ce_1: 0.4868  loss_mask_1: 0.5394  loss_dice_1: 4.527  loss_ce_2: 0.4616  loss_mask_2: 0.5395  loss_dice_2: 4.514  loss_ce_3: 0.4496  loss_mask_3: 0.5422  loss_dice_3: 4.511  loss_ce_4: 0.4594  loss_mask_4: 0.5428  loss_dice_4: 4.514  loss_ce_5: 0.4441  loss_mask_5: 0.542  loss_dice_5: 4.516  loss_ce_6: 0.4311  loss_mask_6: 0.5463  loss_dice_6: 4.519  loss_ce_7: 0.4377  loss_mask_7: 0.5445  loss_dice_7: 4.516  loss_ce_8: 0.4405  loss_mask_8: 0.5426  loss_dice_8: 4.514  time: 1.5500  data_time: 0.0821  lr: 9.868e-06  max_mem: 21234M
[01/17 03:38:45] d2.utils.events INFO:  eta: 1 day, 13:51:46  iter: 1339  total_loss: 56.17  loss_ce: 0.457  loss_mask: 0.5379  loss_dice: 4.507  loss_ce_0: 1.605  loss_mask_0: 0.5256  loss_dice_0: 4.565  loss_ce_1: 0.4614  loss_mask_1: 0.5335  loss_dice_1: 4.536  loss_ce_2: 0.4618  loss_mask_2: 0.5366  loss_dice_2: 4.518  loss_ce_3: 0.4402  loss_mask_3: 0.5372  loss_dice_3: 4.515  loss_ce_4: 0.4435  loss_mask_4: 0.5386  loss_dice_4: 4.514  loss_ce_5: 0.4453  loss_mask_5: 0.5402  loss_dice_5: 4.511  loss_ce_6: 0.4445  loss_mask_6: 0.5386  loss_dice_6: 4.507  loss_ce_7: 0.4442  loss_mask_7: 0.5383  loss_dice_7: 4.503  loss_ce_8: 0.4532  loss_mask_8: 0.5364  loss_dice_8: 4.511  time: 1.5497  data_time: 0.1031  lr: 9.866e-06  max_mem: 21234M
[01/17 03:39:16] d2.utils.events INFO:  eta: 1 day, 13:51:24  iter: 1359  total_loss: 56.22  loss_ce: 0.4403  loss_mask: 0.532  loss_dice: 4.519  loss_ce_0: 1.62  loss_mask_0: 0.5167  loss_dice_0: 4.566  loss_ce_1: 0.4611  loss_mask_1: 0.5257  loss_dice_1: 4.535  loss_ce_2: 0.4504  loss_mask_2: 0.5266  loss_dice_2: 4.526  loss_ce_3: 0.4371  loss_mask_3: 0.529  loss_dice_3: 4.524  loss_ce_4: 0.437  loss_mask_4: 0.5315  loss_dice_4: 4.514  loss_ce_5: 0.4327  loss_mask_5: 0.5311  loss_dice_5: 4.523  loss_ce_6: 0.4354  loss_mask_6: 0.5325  loss_dice_6: 4.516  loss_ce_7: 0.442  loss_mask_7: 0.5349  loss_dice_7: 4.519  loss_ce_8: 0.435  loss_mask_8: 0.5352  loss_dice_8: 4.521  time: 1.5494  data_time: 0.0859  lr: 9.864e-06  max_mem: 21234M
[01/17 03:39:47] d2.utils.events INFO:  eta: 1 day, 13:50:29  iter: 1379  total_loss: 56.23  loss_ce: 0.4536  loss_mask: 0.5521  loss_dice: 4.503  loss_ce_0: 1.582  loss_mask_0: 0.5385  loss_dice_0: 4.554  loss_ce_1: 0.4952  loss_mask_1: 0.5442  loss_dice_1: 4.521  loss_ce_2: 0.4694  loss_mask_2: 0.5467  loss_dice_2: 4.503  loss_ce_3: 0.4566  loss_mask_3: 0.5474  loss_dice_3: 4.501  loss_ce_4: 0.4522  loss_mask_4: 0.5497  loss_dice_4: 4.5  loss_ce_5: 0.4481  loss_mask_5: 0.5526  loss_dice_5: 4.503  loss_ce_6: 0.4588  loss_mask_6: 0.5542  loss_dice_6: 4.5  loss_ce_7: 0.4554  loss_mask_7: 0.5545  loss_dice_7: 4.492  loss_ce_8: 0.4525  loss_mask_8: 0.5553  loss_dice_8: 4.497  time: 1.5492  data_time: 0.1006  lr: 9.862e-06  max_mem: 21234M
[01/17 03:40:17] d2.utils.events INFO:  eta: 1 day, 13:49:23  iter: 1399  total_loss: 55.91  loss_ce: 0.4428  loss_mask: 0.5332  loss_dice: 4.495  loss_ce_0: 1.552  loss_mask_0: 0.5191  loss_dice_0: 4.545  loss_ce_1: 0.4806  loss_mask_1: 0.5246  loss_dice_1: 4.519  loss_ce_2: 0.473  loss_mask_2: 0.5283  loss_dice_2: 4.511  loss_ce_3: 0.4482  loss_mask_3: 0.5304  loss_dice_3: 4.497  loss_ce_4: 0.455  loss_mask_4: 0.5316  loss_dice_4: 4.492  loss_ce_5: 0.4419  loss_mask_5: 0.5333  loss_dice_5: 4.491  loss_ce_6: 0.4421  loss_mask_6: 0.5348  loss_dice_6: 4.488  loss_ce_7: 0.4373  loss_mask_7: 0.5331  loss_dice_7: 4.489  loss_ce_8: 0.443  loss_mask_8: 0.5343  loss_dice_8: 4.495  time: 1.5489  data_time: 0.0890  lr: 9.86e-06  max_mem: 21234M
[01/17 03:40:48] d2.utils.events INFO:  eta: 1 day, 13:48:21  iter: 1419  total_loss: 56.02  loss_ce: 0.4189  loss_mask: 0.5396  loss_dice: 4.49  loss_ce_0: 1.572  loss_mask_0: 0.5327  loss_dice_0: 4.54  loss_ce_1: 0.4866  loss_mask_1: 0.5388  loss_dice_1: 4.513  loss_ce_2: 0.4694  loss_mask_2: 0.5388  loss_dice_2: 4.503  loss_ce_3: 0.4537  loss_mask_3: 0.5386  loss_dice_3: 4.491  loss_ce_4: 0.449  loss_mask_4: 0.5387  loss_dice_4: 4.491  loss_ce_5: 0.4437  loss_mask_5: 0.5388  loss_dice_5: 4.494  loss_ce_6: 0.4358  loss_mask_6: 0.5397  loss_dice_6: 4.485  loss_ce_7: 0.4294  loss_mask_7: 0.5389  loss_dice_7: 4.495  loss_ce_8: 0.4329  loss_mask_8: 0.5408  loss_dice_8: 4.485  time: 1.5489  data_time: 0.0897  lr: 9.858e-06  max_mem: 21234M
[01/17 03:41:20] d2.utils.events INFO:  eta: 1 day, 13:49:07  iter: 1439  total_loss: 55.89  loss_ce: 0.4389  loss_mask: 0.5279  loss_dice: 4.512  loss_ce_0: 1.525  loss_mask_0: 0.5151  loss_dice_0: 4.561  loss_ce_1: 0.4603  loss_mask_1: 0.5211  loss_dice_1: 4.532  loss_ce_2: 0.4515  loss_mask_2: 0.5245  loss_dice_2: 4.517  loss_ce_3: 0.4413  loss_mask_3: 0.5259  loss_dice_3: 4.511  loss_ce_4: 0.4479  loss_mask_4: 0.5267  loss_dice_4: 4.506  loss_ce_5: 0.4427  loss_mask_5: 0.5266  loss_dice_5: 4.509  loss_ce_6: 0.4359  loss_mask_6: 0.5279  loss_dice_6: 4.508  loss_ce_7: 0.4306  loss_mask_7: 0.5295  loss_dice_7: 4.507  loss_ce_8: 0.437  loss_mask_8: 0.5279  loss_dice_8: 4.51  time: 1.5493  data_time: 0.0968  lr: 9.856e-06  max_mem: 21234M
[01/17 03:41:51] d2.utils.events INFO:  eta: 1 day, 13:48:42  iter: 1459  total_loss: 56  loss_ce: 0.4346  loss_mask: 0.5615  loss_dice: 4.498  loss_ce_0: 1.558  loss_mask_0: 0.5527  loss_dice_0: 4.534  loss_ce_1: 0.4644  loss_mask_1: 0.555  loss_dice_1: 4.514  loss_ce_2: 0.442  loss_mask_2: 0.5565  loss_dice_2: 4.497  loss_ce_3: 0.4388  loss_mask_3: 0.5564  loss_dice_3: 4.501  loss_ce_4: 0.4427  loss_mask_4: 0.5587  loss_dice_4: 4.49  loss_ce_5: 0.4349  loss_mask_5: 0.5621  loss_dice_5: 4.493  loss_ce_6: 0.4282  loss_mask_6: 0.5614  loss_dice_6: 4.49  loss_ce_7: 0.424  loss_mask_7: 0.561  loss_dice_7: 4.496  loss_ce_8: 0.4305  loss_mask_8: 0.5595  loss_dice_8: 4.498  time: 1.5492  data_time: 0.0957  lr: 9.854e-06  max_mem: 21234M
[01/17 03:42:22] d2.utils.events INFO:  eta: 1 day, 13:48:06  iter: 1479  total_loss: 55.81  loss_ce: 0.4387  loss_mask: 0.5355  loss_dice: 4.484  loss_ce_0: 1.503  loss_mask_0: 0.5322  loss_dice_0: 4.535  loss_ce_1: 0.4677  loss_mask_1: 0.5357  loss_dice_1: 4.496  loss_ce_2: 0.4447  loss_mask_2: 0.5354  loss_dice_2: 4.485  loss_ce_3: 0.4379  loss_mask_3: 0.5337  loss_dice_3: 4.482  loss_ce_4: 0.4387  loss_mask_4: 0.5371  loss_dice_4: 4.474  loss_ce_5: 0.4514  loss_mask_5: 0.5371  loss_dice_5: 4.469  loss_ce_6: 0.4472  loss_mask_6: 0.5387  loss_dice_6: 4.475  loss_ce_7: 0.4438  loss_mask_7: 0.5378  loss_dice_7: 4.475  loss_ce_8: 0.4372  loss_mask_8: 0.5373  loss_dice_8: 4.481  time: 1.5491  data_time: 0.0844  lr: 9.852e-06  max_mem: 21234M
[01/17 03:42:53] d2.utils.events INFO:  eta: 1 day, 13:47:25  iter: 1499  total_loss: 55.54  loss_ce: 0.4127  loss_mask: 0.559  loss_dice: 4.463  loss_ce_0: 1.522  loss_mask_0: 0.5402  loss_dice_0: 4.516  loss_ce_1: 0.4404  loss_mask_1: 0.5537  loss_dice_1: 4.474  loss_ce_2: 0.4201  loss_mask_2: 0.5514  loss_dice_2: 4.47  loss_ce_3: 0.4291  loss_mask_3: 0.5528  loss_dice_3: 4.462  loss_ce_4: 0.4398  loss_mask_4: 0.5539  loss_dice_4: 4.453  loss_ce_5: 0.4342  loss_mask_5: 0.5542  loss_dice_5: 4.458  loss_ce_6: 0.4208  loss_mask_6: 0.5564  loss_dice_6: 4.458  loss_ce_7: 0.4241  loss_mask_7: 0.5541  loss_dice_7: 4.461  loss_ce_8: 0.4139  loss_mask_8: 0.5543  loss_dice_8: 4.461  time: 1.5488  data_time: 0.0929  lr: 9.85e-06  max_mem: 21234M
[01/17 03:43:23] d2.utils.events INFO:  eta: 1 day, 13:44:33  iter: 1519  total_loss: 55.69  loss_ce: 0.441  loss_mask: 0.5527  loss_dice: 4.47  loss_ce_0: 1.48  loss_mask_0: 0.5358  loss_dice_0: 4.527  loss_ce_1: 0.4669  loss_mask_1: 0.544  loss_dice_1: 4.485  loss_ce_2: 0.4598  loss_mask_2: 0.5471  loss_dice_2: 4.476  loss_ce_3: 0.4451  loss_mask_3: 0.549  loss_dice_3: 4.462  loss_ce_4: 0.4542  loss_mask_4: 0.5513  loss_dice_4: 4.466  loss_ce_5: 0.4541  loss_mask_5: 0.5513  loss_dice_5: 4.464  loss_ce_6: 0.4529  loss_mask_6: 0.5521  loss_dice_6: 4.464  loss_ce_7: 0.4455  loss_mask_7: 0.5542  loss_dice_7: 4.463  loss_ce_8: 0.4405  loss_mask_8: 0.5537  loss_dice_8: 4.472  time: 1.5484  data_time: 0.0960  lr: 9.848e-06  max_mem: 21234M
[01/17 03:43:53] d2.utils.events INFO:  eta: 1 day, 13:43:08  iter: 1539  total_loss: 55.85  loss_ce: 0.4302  loss_mask: 0.5559  loss_dice: 4.489  loss_ce_0: 1.451  loss_mask_0: 0.5454  loss_dice_0: 4.54  loss_ce_1: 0.4292  loss_mask_1: 0.5517  loss_dice_1: 4.502  loss_ce_2: 0.4242  loss_mask_2: 0.5549  loss_dice_2: 4.493  loss_ce_3: 0.4137  loss_mask_3: 0.5557  loss_dice_3: 4.488  loss_ce_4: 0.4151  loss_mask_4: 0.5559  loss_dice_4: 4.483  loss_ce_5: 0.4281  loss_mask_5: 0.5571  loss_dice_5: 4.487  loss_ce_6: 0.4222  loss_mask_6: 0.5566  loss_dice_6: 4.486  loss_ce_7: 0.4309  loss_mask_7: 0.558  loss_dice_7: 4.487  loss_ce_8: 0.4169  loss_mask_8: 0.5591  loss_dice_8: 4.485  time: 1.5479  data_time: 0.0932  lr: 9.846e-06  max_mem: 21234M
[01/17 03:44:24] d2.utils.events INFO:  eta: 1 day, 13:42:04  iter: 1559  total_loss: 55.41  loss_ce: 0.4279  loss_mask: 0.5294  loss_dice: 4.477  loss_ce_0: 1.393  loss_mask_0: 0.5169  loss_dice_0: 4.534  loss_ce_1: 0.4614  loss_mask_1: 0.5227  loss_dice_1: 4.493  loss_ce_2: 0.4259  loss_mask_2: 0.5241  loss_dice_2: 4.485  loss_ce_3: 0.4225  loss_mask_3: 0.5299  loss_dice_3: 4.472  loss_ce_4: 0.4201  loss_mask_4: 0.5295  loss_dice_4: 4.472  loss_ce_5: 0.4309  loss_mask_5: 0.5276  loss_dice_5: 4.479  loss_ce_6: 0.4219  loss_mask_6: 0.5316  loss_dice_6: 4.475  loss_ce_7: 0.422  loss_mask_7: 0.529  loss_dice_7: 4.478  loss_ce_8: 0.4224  loss_mask_8: 0.5285  loss_dice_8: 4.473  time: 1.5476  data_time: 0.0957  lr: 9.844e-06  max_mem: 21234M
[01/17 03:44:54] d2.utils.events INFO:  eta: 1 day, 13:40:22  iter: 1579  total_loss: 55.23  loss_ce: 0.4048  loss_mask: 0.5384  loss_dice: 4.453  loss_ce_0: 1.412  loss_mask_0: 0.5286  loss_dice_0: 4.51  loss_ce_1: 0.441  loss_mask_1: 0.5356  loss_dice_1: 4.463  loss_ce_2: 0.4228  loss_mask_2: 0.534  loss_dice_2: 4.461  loss_ce_3: 0.4057  loss_mask_3: 0.537  loss_dice_3: 4.463  loss_ce_4: 0.4047  loss_mask_4: 0.5366  loss_dice_4: 4.456  loss_ce_5: 0.4029  loss_mask_5: 0.5432  loss_dice_5: 4.456  loss_ce_6: 0.402  loss_mask_6: 0.5417  loss_dice_6: 4.452  loss_ce_7: 0.4017  loss_mask_7: 0.5388  loss_dice_7: 4.454  loss_ce_8: 0.4092  loss_mask_8: 0.5371  loss_dice_8: 4.456  time: 1.5474  data_time: 0.0872  lr: 9.842e-06  max_mem: 21234M
[01/17 03:45:25] d2.utils.events INFO:  eta: 1 day, 13:40:25  iter: 1599  total_loss: 55.23  loss_ce: 0.4152  loss_mask: 0.5647  loss_dice: 4.437  loss_ce_0: 1.438  loss_mask_0: 0.5473  loss_dice_0: 4.502  loss_ce_1: 0.4491  loss_mask_1: 0.5573  loss_dice_1: 4.456  loss_ce_2: 0.42  loss_mask_2: 0.56  loss_dice_2: 4.445  loss_ce_3: 0.4081  loss_mask_3: 0.5593  loss_dice_3: 4.441  loss_ce_4: 0.4116  loss_mask_4: 0.5587  loss_dice_4: 4.433  loss_ce_5: 0.4122  loss_mask_5: 0.5626  loss_dice_5: 4.442  loss_ce_6: 0.4075  loss_mask_6: 0.5668  loss_dice_6: 4.432  loss_ce_7: 0.403  loss_mask_7: 0.5643  loss_dice_7: 4.437  loss_ce_8: 0.4115  loss_mask_8: 0.5649  loss_dice_8: 4.438  time: 1.5472  data_time: 0.0966  lr: 9.84e-06  max_mem: 21234M
[01/17 03:45:56] d2.utils.events INFO:  eta: 1 day, 13:40:31  iter: 1619  total_loss: 54.99  loss_ce: 0.4201  loss_mask: 0.5465  loss_dice: 4.426  loss_ce_0: 1.432  loss_mask_0: 0.5356  loss_dice_0: 4.478  loss_ce_1: 0.4412  loss_mask_1: 0.5412  loss_dice_1: 4.431  loss_ce_2: 0.4288  loss_mask_2: 0.5442  loss_dice_2: 4.425  loss_ce_3: 0.4319  loss_mask_3: 0.5449  loss_dice_3: 4.418  loss_ce_4: 0.4287  loss_mask_4: 0.5488  loss_dice_4: 4.414  loss_ce_5: 0.415  loss_mask_5: 0.5472  loss_dice_5: 4.418  loss_ce_6: 0.4128  loss_mask_6: 0.5478  loss_dice_6: 4.415  loss_ce_7: 0.4194  loss_mask_7: 0.5462  loss_dice_7: 4.422  loss_ce_8: 0.4188  loss_mask_8: 0.544  loss_dice_8: 4.425  time: 1.5473  data_time: 0.0931  lr: 9.838e-06  max_mem: 21234M
[01/17 03:46:27] d2.utils.events INFO:  eta: 1 day, 13:40:43  iter: 1639  total_loss: 55.03  loss_ce: 0.4302  loss_mask: 0.5341  loss_dice: 4.413  loss_ce_0: 1.394  loss_mask_0: 0.5308  loss_dice_0: 4.49  loss_ce_1: 0.4619  loss_mask_1: 0.5317  loss_dice_1: 4.449  loss_ce_2: 0.4327  loss_mask_2: 0.5333  loss_dice_2: 4.437  loss_ce_3: 0.4229  loss_mask_3: 0.5324  loss_dice_3: 4.425  loss_ce_4: 0.4329  loss_mask_4: 0.5333  loss_dice_4: 4.427  loss_ce_5: 0.4186  loss_mask_5: 0.5375  loss_dice_5: 4.414  loss_ce_6: 0.4281  loss_mask_6: 0.5358  loss_dice_6: 4.418  loss_ce_7: 0.4152  loss_mask_7: 0.5355  loss_dice_7: 4.415  loss_ce_8: 0.4175  loss_mask_8: 0.5356  loss_dice_8: 4.406  time: 1.5473  data_time: 0.0988  lr: 9.8359e-06  max_mem: 21234M
[01/17 03:46:59] d2.utils.events INFO:  eta: 1 day, 13:39:21  iter: 1659  total_loss: 55.05  loss_ce: 0.4238  loss_mask: 0.566  loss_dice: 4.429  loss_ce_0: 1.368  loss_mask_0: 0.5468  loss_dice_0: 4.488  loss_ce_1: 0.4353  loss_mask_1: 0.5627  loss_dice_1: 4.443  loss_ce_2: 0.425  loss_mask_2: 0.5629  loss_dice_2: 4.43  loss_ce_3: 0.4166  loss_mask_3: 0.5629  loss_dice_3: 4.424  loss_ce_4: 0.4311  loss_mask_4: 0.5628  loss_dice_4: 4.427  loss_ce_5: 0.4191  loss_mask_5: 0.5621  loss_dice_5: 4.422  loss_ce_6: 0.4338  loss_mask_6: 0.5619  loss_dice_6: 4.429  loss_ce_7: 0.412  loss_mask_7: 0.5634  loss_dice_7: 4.427  loss_ce_8: 0.4249  loss_mask_8: 0.5659  loss_dice_8: 4.425  time: 1.5475  data_time: 0.1100  lr: 9.8339e-06  max_mem: 21234M
[01/17 03:47:30] d2.utils.events INFO:  eta: 1 day, 13:38:55  iter: 1679  total_loss: 55.09  loss_ce: 0.4132  loss_mask: 0.5576  loss_dice: 4.425  loss_ce_0: 1.338  loss_mask_0: 0.5417  loss_dice_0: 4.485  loss_ce_1: 0.4424  loss_mask_1: 0.5483  loss_dice_1: 4.446  loss_ce_2: 0.4216  loss_mask_2: 0.5535  loss_dice_2: 4.438  loss_ce_3: 0.423  loss_mask_3: 0.5536  loss_dice_3: 4.426  loss_ce_4: 0.4112  loss_mask_4: 0.5556  loss_dice_4: 4.426  loss_ce_5: 0.4111  loss_mask_5: 0.5546  loss_dice_5: 4.427  loss_ce_6: 0.4098  loss_mask_6: 0.5515  loss_dice_6: 4.428  loss_ce_7: 0.4126  loss_mask_7: 0.5551  loss_dice_7: 4.424  loss_ce_8: 0.4063  loss_mask_8: 0.5546  loss_dice_8: 4.427  time: 1.5476  data_time: 0.1058  lr: 9.8319e-06  max_mem: 21234M
[01/17 03:48:02] d2.utils.events INFO:  eta: 1 day, 13:38:29  iter: 1699  total_loss: 54.85  loss_ce: 0.4046  loss_mask: 0.5364  loss_dice: 4.432  loss_ce_0: 1.281  loss_mask_0: 0.5296  loss_dice_0: 4.497  loss_ce_1: 0.429  loss_mask_1: 0.5367  loss_dice_1: 4.455  loss_ce_2: 0.4257  loss_mask_2: 0.5352  loss_dice_2: 4.441  loss_ce_3: 0.4152  loss_mask_3: 0.5353  loss_dice_3: 4.431  loss_ce_4: 0.4103  loss_mask_4: 0.5352  loss_dice_4: 4.431  loss_ce_5: 0.4091  loss_mask_5: 0.5329  loss_dice_5: 4.429  loss_ce_6: 0.3972  loss_mask_6: 0.5359  loss_dice_6: 4.437  loss_ce_7: 0.4132  loss_mask_7: 0.5339  loss_dice_7: 4.428  loss_ce_8: 0.4023  loss_mask_8: 0.5338  loss_dice_8: 4.427  time: 1.5483  data_time: 0.1038  lr: 9.8299e-06  max_mem: 21234M
[01/17 03:48:33] d2.utils.events INFO:  eta: 1 day, 13:37:52  iter: 1719  total_loss: 54.92  loss_ce: 0.4252  loss_mask: 0.5443  loss_dice: 4.425  loss_ce_0: 1.345  loss_mask_0: 0.5371  loss_dice_0: 4.48  loss_ce_1: 0.4618  loss_mask_1: 0.5481  loss_dice_1: 4.446  loss_ce_2: 0.4316  loss_mask_2: 0.5415  loss_dice_2: 4.43  loss_ce_3: 0.4253  loss_mask_3: 0.5421  loss_dice_3: 4.417  loss_ce_4: 0.4134  loss_mask_4: 0.5417  loss_dice_4: 4.42  loss_ce_5: 0.4312  loss_mask_5: 0.547  loss_dice_5: 4.421  loss_ce_6: 0.4323  loss_mask_6: 0.5481  loss_dice_6: 4.418  loss_ce_7: 0.4152  loss_mask_7: 0.5498  loss_dice_7: 4.411  loss_ce_8: 0.4327  loss_mask_8: 0.5489  loss_dice_8: 4.415  time: 1.5480  data_time: 0.1007  lr: 9.8279e-06  max_mem: 21234M
[01/17 03:49:04] d2.utils.events INFO:  eta: 1 day, 13:37:21  iter: 1739  total_loss: 54.9  loss_ce: 0.4078  loss_mask: 0.5605  loss_dice: 4.43  loss_ce_0: 1.28  loss_mask_0: 0.5477  loss_dice_0: 4.5  loss_ce_1: 0.418  loss_mask_1: 0.5512  loss_dice_1: 4.464  loss_ce_2: 0.3994  loss_mask_2: 0.5543  loss_dice_2: 4.444  loss_ce_3: 0.4059  loss_mask_3: 0.5566  loss_dice_3: 4.432  loss_ce_4: 0.4095  loss_mask_4: 0.5587  loss_dice_4: 4.431  loss_ce_5: 0.3958  loss_mask_5: 0.5607  loss_dice_5: 4.434  loss_ce_6: 0.4043  loss_mask_6: 0.5596  loss_dice_6: 4.428  loss_ce_7: 0.4076  loss_mask_7: 0.5629  loss_dice_7: 4.433  loss_ce_8: 0.4077  loss_mask_8: 0.5601  loss_dice_8: 4.423  time: 1.5479  data_time: 0.0994  lr: 9.8259e-06  max_mem: 21234M
[01/17 03:49:35] d2.utils.events INFO:  eta: 1 day, 13:36:57  iter: 1759  total_loss: 55.18  loss_ce: 0.4526  loss_mask: 0.5379  loss_dice: 4.423  loss_ce_0: 1.317  loss_mask_0: 0.523  loss_dice_0: 4.492  loss_ce_1: 0.4842  loss_mask_1: 0.534  loss_dice_1: 4.455  loss_ce_2: 0.4654  loss_mask_2: 0.5366  loss_dice_2: 4.432  loss_ce_3: 0.4621  loss_mask_3: 0.5371  loss_dice_3: 4.434  loss_ce_4: 0.4462  loss_mask_4: 0.5375  loss_dice_4: 4.433  loss_ce_5: 0.4456  loss_mask_5: 0.5383  loss_dice_5: 4.433  loss_ce_6: 0.4449  loss_mask_6: 0.5387  loss_dice_6: 4.426  loss_ce_7: 0.4431  loss_mask_7: 0.538  loss_dice_7: 4.432  loss_ce_8: 0.4556  loss_mask_8: 0.54  loss_dice_8: 4.424  time: 1.5479  data_time: 0.0893  lr: 9.8239e-06  max_mem: 21234M
[01/17 03:50:05] d2.utils.events INFO:  eta: 1 day, 13:35:49  iter: 1779  total_loss: 54.95  loss_ce: 0.4279  loss_mask: 0.5561  loss_dice: 4.407  loss_ce_0: 1.265  loss_mask_0: 0.5447  loss_dice_0: 4.472  loss_ce_1: 0.4539  loss_mask_1: 0.5513  loss_dice_1: 4.429  loss_ce_2: 0.4429  loss_mask_2: 0.5511  loss_dice_2: 4.413  loss_ce_3: 0.4126  loss_mask_3: 0.5505  loss_dice_3: 4.419  loss_ce_4: 0.4241  loss_mask_4: 0.5534  loss_dice_4: 4.41  loss_ce_5: 0.4283  loss_mask_5: 0.5556  loss_dice_5: 4.407  loss_ce_6: 0.4134  loss_mask_6: 0.5567  loss_dice_6: 4.406  loss_ce_7: 0.4117  loss_mask_7: 0.5567  loss_dice_7: 4.409  loss_ce_8: 0.4239  loss_mask_8: 0.5569  loss_dice_8: 4.412  time: 1.5476  data_time: 0.0945  lr: 9.8219e-06  max_mem: 21234M
[01/17 03:50:36] d2.utils.events INFO:  eta: 1 day, 13:34:34  iter: 1799  total_loss: 54.68  loss_ce: 0.4258  loss_mask: 0.5573  loss_dice: 4.414  loss_ce_0: 1.254  loss_mask_0: 0.5424  loss_dice_0: 4.469  loss_ce_1: 0.4169  loss_mask_1: 0.5514  loss_dice_1: 4.422  loss_ce_2: 0.4127  loss_mask_2: 0.5507  loss_dice_2: 4.413  loss_ce_3: 0.4196  loss_mask_3: 0.551  loss_dice_3: 4.408  loss_ce_4: 0.415  loss_mask_4: 0.5516  loss_dice_4: 4.402  loss_ce_5: 0.4152  loss_mask_5: 0.5513  loss_dice_5: 4.405  loss_ce_6: 0.416  loss_mask_6: 0.5556  loss_dice_6: 4.405  loss_ce_7: 0.415  loss_mask_7: 0.557  loss_dice_7: 4.408  loss_ce_8: 0.3985  loss_mask_8: 0.5575  loss_dice_8: 4.406  time: 1.5475  data_time: 0.0874  lr: 9.8199e-06  max_mem: 21234M
[01/17 03:51:06] d2.utils.events INFO:  eta: 1 day, 13:31:44  iter: 1819  total_loss: 54.87  loss_ce: 0.3908  loss_mask: 0.5617  loss_dice: 4.42  loss_ce_0: 1.243  loss_mask_0: 0.5476  loss_dice_0: 4.49  loss_ce_1: 0.4207  loss_mask_1: 0.5531  loss_dice_1: 4.438  loss_ce_2: 0.4247  loss_mask_2: 0.5543  loss_dice_2: 4.422  loss_ce_3: 0.4155  loss_mask_3: 0.5552  loss_dice_3: 4.416  loss_ce_4: 0.4127  loss_mask_4: 0.5586  loss_dice_4: 4.414  loss_ce_5: 0.4002  loss_mask_5: 0.5562  loss_dice_5: 4.421  loss_ce_6: 0.3987  loss_mask_6: 0.557  loss_dice_6: 4.42  loss_ce_7: 0.3933  loss_mask_7: 0.5598  loss_dice_7: 4.424  loss_ce_8: 0.3922  loss_mask_8: 0.5611  loss_dice_8: 4.419  time: 1.5469  data_time: 0.0851  lr: 9.8179e-06  max_mem: 21234M
[01/17 03:51:36] d2.utils.events INFO:  eta: 1 day, 13:30:12  iter: 1839  total_loss: 54.57  loss_ce: 0.3904  loss_mask: 0.5555  loss_dice: 4.409  loss_ce_0: 1.273  loss_mask_0: 0.543  loss_dice_0: 4.467  loss_ce_1: 0.46  loss_mask_1: 0.5501  loss_dice_1: 4.429  loss_ce_2: 0.4192  loss_mask_2: 0.5523  loss_dice_2: 4.413  loss_ce_3: 0.4186  loss_mask_3: 0.5539  loss_dice_3: 4.401  loss_ce_4: 0.4116  loss_mask_4: 0.5561  loss_dice_4: 4.407  loss_ce_5: 0.3906  loss_mask_5: 0.5596  loss_dice_5: 4.401  loss_ce_6: 0.3866  loss_mask_6: 0.5561  loss_dice_6: 4.411  loss_ce_7: 0.3832  loss_mask_7: 0.5552  loss_dice_7: 4.409  loss_ce_8: 0.396  loss_mask_8: 0.558  loss_dice_8: 4.404  time: 1.5467  data_time: 0.0924  lr: 9.8159e-06  max_mem: 21234M
[01/17 03:52:07] d2.utils.events INFO:  eta: 1 day, 13:29:41  iter: 1859  total_loss: 54.48  loss_ce: 0.3869  loss_mask: 0.552  loss_dice: 4.389  loss_ce_0: 1.214  loss_mask_0: 0.5397  loss_dice_0: 4.466  loss_ce_1: 0.4282  loss_mask_1: 0.55  loss_dice_1: 4.414  loss_ce_2: 0.4047  loss_mask_2: 0.5504  loss_dice_2: 4.395  loss_ce_3: 0.4028  loss_mask_3: 0.5504  loss_dice_3: 4.383  loss_ce_4: 0.4063  loss_mask_4: 0.549  loss_dice_4: 4.385  loss_ce_5: 0.3893  loss_mask_5: 0.5491  loss_dice_5: 4.386  loss_ce_6: 0.3946  loss_mask_6: 0.5517  loss_dice_6: 4.386  loss_ce_7: 0.393  loss_mask_7: 0.5559  loss_dice_7: 4.386  loss_ce_8: 0.3968  loss_mask_8: 0.5539  loss_dice_8: 4.39  time: 1.5466  data_time: 0.1047  lr: 9.8139e-06  max_mem: 21234M
[01/17 03:52:37] d2.utils.events INFO:  eta: 1 day, 13:27:46  iter: 1879  total_loss: 54.7  loss_ce: 0.3989  loss_mask: 0.584  loss_dice: 4.395  loss_ce_0: 1.237  loss_mask_0: 0.56  loss_dice_0: 4.461  loss_ce_1: 0.427  loss_mask_1: 0.5751  loss_dice_1: 4.416  loss_ce_2: 0.4026  loss_mask_2: 0.5774  loss_dice_2: 4.405  loss_ce_3: 0.3975  loss_mask_3: 0.5794  loss_dice_3: 4.398  loss_ce_4: 0.4021  loss_mask_4: 0.5813  loss_dice_4: 4.397  loss_ce_5: 0.4002  loss_mask_5: 0.5818  loss_dice_5: 4.397  loss_ce_6: 0.4081  loss_mask_6: 0.5848  loss_dice_6: 4.389  loss_ce_7: 0.4032  loss_mask_7: 0.5852  loss_dice_7: 4.393  loss_ce_8: 0.4072  loss_mask_8: 0.5858  loss_dice_8: 4.399  time: 1.5462  data_time: 0.0889  lr: 9.8119e-06  max_mem: 21234M
[01/17 03:53:08] d2.utils.events INFO:  eta: 1 day, 13:28:28  iter: 1899  total_loss: 54.5  loss_ce: 0.4333  loss_mask: 0.555  loss_dice: 4.394  loss_ce_0: 1.22  loss_mask_0: 0.5366  loss_dice_0: 4.463  loss_ce_1: 0.4599  loss_mask_1: 0.5457  loss_dice_1: 4.412  loss_ce_2: 0.4497  loss_mask_2: 0.5469  loss_dice_2: 4.401  loss_ce_3: 0.4426  loss_mask_3: 0.5469  loss_dice_3: 4.393  loss_ce_4: 0.4325  loss_mask_4: 0.5513  loss_dice_4: 4.393  loss_ce_5: 0.4296  loss_mask_5: 0.5515  loss_dice_5: 4.387  loss_ce_6: 0.432  loss_mask_6: 0.5504  loss_dice_6: 4.391  loss_ce_7: 0.4233  loss_mask_7: 0.5563  loss_dice_7: 4.379  loss_ce_8: 0.4253  loss_mask_8: 0.5543  loss_dice_8: 4.392  time: 1.5461  data_time: 0.0878  lr: 9.8099e-06  max_mem: 21234M
[01/17 03:53:39] d2.utils.events INFO:  eta: 1 day, 13:25:48  iter: 1919  total_loss: 54.33  loss_ce: 0.3934  loss_mask: 0.5725  loss_dice: 4.379  loss_ce_0: 1.209  loss_mask_0: 0.5512  loss_dice_0: 4.451  loss_ce_1: 0.4321  loss_mask_1: 0.5624  loss_dice_1: 4.403  loss_ce_2: 0.4082  loss_mask_2: 0.5687  loss_dice_2: 4.381  loss_ce_3: 0.3981  loss_mask_3: 0.5688  loss_dice_3: 4.378  loss_ce_4: 0.3838  loss_mask_4: 0.5689  loss_dice_4: 4.382  loss_ce_5: 0.3857  loss_mask_5: 0.5699  loss_dice_5: 4.378  loss_ce_6: 0.3818  loss_mask_6: 0.5692  loss_dice_6: 4.377  loss_ce_7: 0.3926  loss_mask_7: 0.5726  loss_dice_7: 4.38  loss_ce_8: 0.3913  loss_mask_8: 0.5721  loss_dice_8: 4.382  time: 1.5459  data_time: 0.0930  lr: 9.8079e-06  max_mem: 21234M
[01/17 03:54:10] d2.utils.events INFO:  eta: 1 day, 13:25:00  iter: 1939  total_loss: 54.46  loss_ce: 0.4469  loss_mask: 0.558  loss_dice: 4.366  loss_ce_0: 1.227  loss_mask_0: 0.5428  loss_dice_0: 4.438  loss_ce_1: 0.4615  loss_mask_1: 0.5561  loss_dice_1: 4.39  loss_ce_2: 0.4431  loss_mask_2: 0.5547  loss_dice_2: 4.377  loss_ce_3: 0.4302  loss_mask_3: 0.5559  loss_dice_3: 4.377  loss_ce_4: 0.4414  loss_mask_4: 0.56  loss_dice_4: 4.365  loss_ce_5: 0.4371  loss_mask_5: 0.564  loss_dice_5: 4.369  loss_ce_6: 0.4359  loss_mask_6: 0.5625  loss_dice_6: 4.368  loss_ce_7: 0.4257  loss_mask_7: 0.5588  loss_dice_7: 4.372  loss_ce_8: 0.426  loss_mask_8: 0.5584  loss_dice_8: 4.375  time: 1.5458  data_time: 0.0851  lr: 9.8059e-06  max_mem: 21234M
[01/17 03:54:40] d2.utils.events INFO:  eta: 1 day, 13:25:32  iter: 1959  total_loss: 54.42  loss_ce: 0.4411  loss_mask: 0.5494  loss_dice: 4.392  loss_ce_0: 1.172  loss_mask_0: 0.5319  loss_dice_0: 4.463  loss_ce_1: 0.4361  loss_mask_1: 0.5435  loss_dice_1: 4.414  loss_ce_2: 0.4256  loss_mask_2: 0.5473  loss_dice_2: 4.404  loss_ce_3: 0.4021  loss_mask_3: 0.5499  loss_dice_3: 4.386  loss_ce_4: 0.4259  loss_mask_4: 0.5494  loss_dice_4: 4.393  loss_ce_5: 0.3993  loss_mask_5: 0.5501  loss_dice_5: 4.39  loss_ce_6: 0.409  loss_mask_6: 0.5543  loss_dice_6: 4.383  loss_ce_7: 0.4274  loss_mask_7: 0.553  loss_dice_7: 4.393  loss_ce_8: 0.4199  loss_mask_8: 0.5499  loss_dice_8: 4.388  time: 1.5455  data_time: 0.0905  lr: 9.8039e-06  max_mem: 21234M
[01/17 03:55:11] d2.utils.events INFO:  eta: 1 day, 13:25:13  iter: 1979  total_loss: 54.36  loss_ce: 0.4199  loss_mask: 0.5514  loss_dice: 4.388  loss_ce_0: 1.143  loss_mask_0: 0.537  loss_dice_0: 4.457  loss_ce_1: 0.4398  loss_mask_1: 0.5439  loss_dice_1: 4.401  loss_ce_2: 0.4333  loss_mask_2: 0.5497  loss_dice_2: 4.386  loss_ce_3: 0.4091  loss_mask_3: 0.547  loss_dice_3: 4.386  loss_ce_4: 0.4144  loss_mask_4: 0.5503  loss_dice_4: 4.384  loss_ce_5: 0.4184  loss_mask_5: 0.5516  loss_dice_5: 4.376  loss_ce_6: 0.4091  loss_mask_6: 0.5502  loss_dice_6: 4.38  loss_ce_7: 0.4089  loss_mask_7: 0.5516  loss_dice_7: 4.385  loss_ce_8: 0.4239  loss_mask_8: 0.5495  loss_dice_8: 4.381  time: 1.5455  data_time: 0.1029  lr: 9.8019e-06  max_mem: 21234M
[01/17 03:55:42] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 03:55:42] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 03:55:42] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 03:55:43] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 03:55:57] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0084 s/iter. Inference: 0.1574 s/iter. Eval: 0.1912 s/iter. Total: 0.3571 s/iter. ETA=0:06:26
[01/17 03:56:02] d2.evaluation.evaluator INFO: Inference done 25/1093. Dataloading: 0.0126 s/iter. Inference: 0.1478 s/iter. Eval: 0.2052 s/iter. Total: 0.3657 s/iter. ETA=0:06:30
[01/17 03:56:07] d2.evaluation.evaluator INFO: Inference done 40/1093. Dataloading: 0.0119 s/iter. Inference: 0.1501 s/iter. Eval: 0.1977 s/iter. Total: 0.3598 s/iter. ETA=0:06:18
[01/17 03:56:13] d2.evaluation.evaluator INFO: Inference done 52/1093. Dataloading: 0.0138 s/iter. Inference: 0.1623 s/iter. Eval: 0.2057 s/iter. Total: 0.3819 s/iter. ETA=0:06:37
[01/17 03:56:18] d2.evaluation.evaluator INFO: Inference done 65/1093. Dataloading: 0.0144 s/iter. Inference: 0.1618 s/iter. Eval: 0.2102 s/iter. Total: 0.3866 s/iter. ETA=0:06:37
[01/17 03:56:23] d2.evaluation.evaluator INFO: Inference done 78/1093. Dataloading: 0.0138 s/iter. Inference: 0.1608 s/iter. Eval: 0.2120 s/iter. Total: 0.3866 s/iter. ETA=0:06:32
[01/17 03:56:28] d2.evaluation.evaluator INFO: Inference done 90/1093. Dataloading: 0.0137 s/iter. Inference: 0.1615 s/iter. Eval: 0.2166 s/iter. Total: 0.3920 s/iter. ETA=0:06:33
[01/17 03:56:33] d2.evaluation.evaluator INFO: Inference done 104/1093. Dataloading: 0.0138 s/iter. Inference: 0.1634 s/iter. Eval: 0.2101 s/iter. Total: 0.3874 s/iter. ETA=0:06:23
[01/17 03:56:38] d2.evaluation.evaluator INFO: Inference done 116/1093. Dataloading: 0.0139 s/iter. Inference: 0.1650 s/iter. Eval: 0.2119 s/iter. Total: 0.3910 s/iter. ETA=0:06:22
[01/17 03:56:43] d2.evaluation.evaluator INFO: Inference done 129/1093. Dataloading: 0.0136 s/iter. Inference: 0.1652 s/iter. Eval: 0.2122 s/iter. Total: 0.3911 s/iter. ETA=0:06:16
[01/17 03:56:48] d2.evaluation.evaluator INFO: Inference done 142/1093. Dataloading: 0.0132 s/iter. Inference: 0.1666 s/iter. Eval: 0.2106 s/iter. Total: 0.3905 s/iter. ETA=0:06:11
[01/17 03:56:54] d2.evaluation.evaluator INFO: Inference done 156/1093. Dataloading: 0.0134 s/iter. Inference: 0.1680 s/iter. Eval: 0.2081 s/iter. Total: 0.3895 s/iter. ETA=0:06:05
[01/17 03:56:59] d2.evaluation.evaluator INFO: Inference done 170/1093. Dataloading: 0.0133 s/iter. Inference: 0.1675 s/iter. Eval: 0.2077 s/iter. Total: 0.3886 s/iter. ETA=0:05:58
[01/17 03:57:04] d2.evaluation.evaluator INFO: Inference done 183/1093. Dataloading: 0.0133 s/iter. Inference: 0.1678 s/iter. Eval: 0.2085 s/iter. Total: 0.3896 s/iter. ETA=0:05:54
[01/17 03:57:09] d2.evaluation.evaluator INFO: Inference done 196/1093. Dataloading: 0.0132 s/iter. Inference: 0.1675 s/iter. Eval: 0.2091 s/iter. Total: 0.3898 s/iter. ETA=0:05:49
[01/17 03:57:15] d2.evaluation.evaluator INFO: Inference done 209/1093. Dataloading: 0.0131 s/iter. Inference: 0.1673 s/iter. Eval: 0.2100 s/iter. Total: 0.3905 s/iter. ETA=0:05:45
[01/17 03:57:20] d2.evaluation.evaluator INFO: Inference done 222/1093. Dataloading: 0.0132 s/iter. Inference: 0.1681 s/iter. Eval: 0.2094 s/iter. Total: 0.3908 s/iter. ETA=0:05:40
[01/17 03:57:25] d2.evaluation.evaluator INFO: Inference done 237/1093. Dataloading: 0.0132 s/iter. Inference: 0.1667 s/iter. Eval: 0.2084 s/iter. Total: 0.3883 s/iter. ETA=0:05:32
[01/17 03:57:30] d2.evaluation.evaluator INFO: Inference done 251/1093. Dataloading: 0.0132 s/iter. Inference: 0.1662 s/iter. Eval: 0.2071 s/iter. Total: 0.3867 s/iter. ETA=0:05:25
[01/17 03:57:35] d2.evaluation.evaluator INFO: Inference done 263/1093. Dataloading: 0.0134 s/iter. Inference: 0.1674 s/iter. Eval: 0.2082 s/iter. Total: 0.3891 s/iter. ETA=0:05:22
[01/17 03:57:40] d2.evaluation.evaluator INFO: Inference done 275/1093. Dataloading: 0.0134 s/iter. Inference: 0.1684 s/iter. Eval: 0.2086 s/iter. Total: 0.3906 s/iter. ETA=0:05:19
[01/17 03:57:46] d2.evaluation.evaluator INFO: Inference done 288/1093. Dataloading: 0.0136 s/iter. Inference: 0.1689 s/iter. Eval: 0.2085 s/iter. Total: 0.3911 s/iter. ETA=0:05:14
[01/17 03:57:51] d2.evaluation.evaluator INFO: Inference done 301/1093. Dataloading: 0.0136 s/iter. Inference: 0.1690 s/iter. Eval: 0.2089 s/iter. Total: 0.3917 s/iter. ETA=0:05:10
[01/17 03:57:56] d2.evaluation.evaluator INFO: Inference done 313/1093. Dataloading: 0.0137 s/iter. Inference: 0.1702 s/iter. Eval: 0.2099 s/iter. Total: 0.3939 s/iter. ETA=0:05:07
[01/17 03:58:02] d2.evaluation.evaluator INFO: Inference done 327/1093. Dataloading: 0.0137 s/iter. Inference: 0.1699 s/iter. Eval: 0.2098 s/iter. Total: 0.3935 s/iter. ETA=0:05:01
[01/17 03:58:07] d2.evaluation.evaluator INFO: Inference done 342/1093. Dataloading: 0.0135 s/iter. Inference: 0.1700 s/iter. Eval: 0.2081 s/iter. Total: 0.3917 s/iter. ETA=0:04:54
[01/17 03:58:12] d2.evaluation.evaluator INFO: Inference done 355/1093. Dataloading: 0.0135 s/iter. Inference: 0.1706 s/iter. Eval: 0.2079 s/iter. Total: 0.3921 s/iter. ETA=0:04:49
[01/17 03:58:17] d2.evaluation.evaluator INFO: Inference done 368/1093. Dataloading: 0.0135 s/iter. Inference: 0.1703 s/iter. Eval: 0.2084 s/iter. Total: 0.3923 s/iter. ETA=0:04:44
[01/17 03:58:22] d2.evaluation.evaluator INFO: Inference done 381/1093. Dataloading: 0.0134 s/iter. Inference: 0.1703 s/iter. Eval: 0.2087 s/iter. Total: 0.3925 s/iter. ETA=0:04:39
[01/17 03:58:28] d2.evaluation.evaluator INFO: Inference done 395/1093. Dataloading: 0.0134 s/iter. Inference: 0.1697 s/iter. Eval: 0.2085 s/iter. Total: 0.3918 s/iter. ETA=0:04:33
[01/17 03:58:33] d2.evaluation.evaluator INFO: Inference done 410/1093. Dataloading: 0.0134 s/iter. Inference: 0.1692 s/iter. Eval: 0.2073 s/iter. Total: 0.3900 s/iter. ETA=0:04:26
[01/17 03:58:38] d2.evaluation.evaluator INFO: Inference done 423/1093. Dataloading: 0.0134 s/iter. Inference: 0.1691 s/iter. Eval: 0.2080 s/iter. Total: 0.3906 s/iter. ETA=0:04:21
[01/17 03:58:43] d2.evaluation.evaluator INFO: Inference done 437/1093. Dataloading: 0.0135 s/iter. Inference: 0.1695 s/iter. Eval: 0.2072 s/iter. Total: 0.3902 s/iter. ETA=0:04:15
[01/17 03:58:49] d2.evaluation.evaluator INFO: Inference done 450/1093. Dataloading: 0.0134 s/iter. Inference: 0.1695 s/iter. Eval: 0.2073 s/iter. Total: 0.3904 s/iter. ETA=0:04:11
[01/17 03:58:54] d2.evaluation.evaluator INFO: Inference done 463/1093. Dataloading: 0.0134 s/iter. Inference: 0.1696 s/iter. Eval: 0.2077 s/iter. Total: 0.3908 s/iter. ETA=0:04:06
[01/17 03:58:59] d2.evaluation.evaluator INFO: Inference done 477/1093. Dataloading: 0.0134 s/iter. Inference: 0.1697 s/iter. Eval: 0.2069 s/iter. Total: 0.3901 s/iter. ETA=0:04:00
[01/17 03:59:04] d2.evaluation.evaluator INFO: Inference done 491/1093. Dataloading: 0.0133 s/iter. Inference: 0.1698 s/iter. Eval: 0.2065 s/iter. Total: 0.3897 s/iter. ETA=0:03:54
[01/17 03:59:09] d2.evaluation.evaluator INFO: Inference done 507/1093. Dataloading: 0.0132 s/iter. Inference: 0.1693 s/iter. Eval: 0.2049 s/iter. Total: 0.3876 s/iter. ETA=0:03:47
[01/17 03:59:15] d2.evaluation.evaluator INFO: Inference done 522/1093. Dataloading: 0.0132 s/iter. Inference: 0.1692 s/iter. Eval: 0.2041 s/iter. Total: 0.3866 s/iter. ETA=0:03:40
[01/17 03:59:20] d2.evaluation.evaluator INFO: Inference done 535/1093. Dataloading: 0.0132 s/iter. Inference: 0.1693 s/iter. Eval: 0.2044 s/iter. Total: 0.3870 s/iter. ETA=0:03:35
[01/17 03:59:25] d2.evaluation.evaluator INFO: Inference done 548/1093. Dataloading: 0.0132 s/iter. Inference: 0.1692 s/iter. Eval: 0.2048 s/iter. Total: 0.3872 s/iter. ETA=0:03:31
[01/17 03:59:30] d2.evaluation.evaluator INFO: Inference done 562/1093. Dataloading: 0.0132 s/iter. Inference: 0.1692 s/iter. Eval: 0.2044 s/iter. Total: 0.3869 s/iter. ETA=0:03:25
[01/17 03:59:35] d2.evaluation.evaluator INFO: Inference done 578/1093. Dataloading: 0.0132 s/iter. Inference: 0.1688 s/iter. Eval: 0.2027 s/iter. Total: 0.3848 s/iter. ETA=0:03:18
[01/17 03:59:41] d2.evaluation.evaluator INFO: Inference done 594/1093. Dataloading: 0.0131 s/iter. Inference: 0.1685 s/iter. Eval: 0.2018 s/iter. Total: 0.3835 s/iter. ETA=0:03:11
[01/17 03:59:46] d2.evaluation.evaluator INFO: Inference done 606/1093. Dataloading: 0.0132 s/iter. Inference: 0.1686 s/iter. Eval: 0.2026 s/iter. Total: 0.3844 s/iter. ETA=0:03:07
[01/17 03:59:51] d2.evaluation.evaluator INFO: Inference done 620/1093. Dataloading: 0.0131 s/iter. Inference: 0.1685 s/iter. Eval: 0.2024 s/iter. Total: 0.3842 s/iter. ETA=0:03:01
[01/17 03:59:56] d2.evaluation.evaluator INFO: Inference done 633/1093. Dataloading: 0.0131 s/iter. Inference: 0.1688 s/iter. Eval: 0.2024 s/iter. Total: 0.3844 s/iter. ETA=0:02:56
[01/17 04:00:02] d2.evaluation.evaluator INFO: Inference done 648/1093. Dataloading: 0.0130 s/iter. Inference: 0.1690 s/iter. Eval: 0.2016 s/iter. Total: 0.3837 s/iter. ETA=0:02:50
[01/17 04:00:07] d2.evaluation.evaluator INFO: Inference done 662/1093. Dataloading: 0.0130 s/iter. Inference: 0.1692 s/iter. Eval: 0.2013 s/iter. Total: 0.3837 s/iter. ETA=0:02:45
[01/17 04:00:12] d2.evaluation.evaluator INFO: Inference done 677/1093. Dataloading: 0.0130 s/iter. Inference: 0.1691 s/iter. Eval: 0.2007 s/iter. Total: 0.3829 s/iter. ETA=0:02:39
[01/17 04:00:17] d2.evaluation.evaluator INFO: Inference done 691/1093. Dataloading: 0.0129 s/iter. Inference: 0.1694 s/iter. Eval: 0.2002 s/iter. Total: 0.3826 s/iter. ETA=0:02:33
[01/17 04:00:22] d2.evaluation.evaluator INFO: Inference done 704/1093. Dataloading: 0.0130 s/iter. Inference: 0.1696 s/iter. Eval: 0.2000 s/iter. Total: 0.3827 s/iter. ETA=0:02:28
[01/17 04:00:28] d2.evaluation.evaluator INFO: Inference done 716/1093. Dataloading: 0.0130 s/iter. Inference: 0.1694 s/iter. Eval: 0.2009 s/iter. Total: 0.3834 s/iter. ETA=0:02:24
[01/17 04:00:33] d2.evaluation.evaluator INFO: Inference done 732/1093. Dataloading: 0.0129 s/iter. Inference: 0.1693 s/iter. Eval: 0.1999 s/iter. Total: 0.3822 s/iter. ETA=0:02:17
[01/17 04:00:38] d2.evaluation.evaluator INFO: Inference done 745/1093. Dataloading: 0.0129 s/iter. Inference: 0.1696 s/iter. Eval: 0.2001 s/iter. Total: 0.3828 s/iter. ETA=0:02:13
[01/17 04:00:43] d2.evaluation.evaluator INFO: Inference done 759/1093. Dataloading: 0.0129 s/iter. Inference: 0.1694 s/iter. Eval: 0.2001 s/iter. Total: 0.3826 s/iter. ETA=0:02:07
[01/17 04:00:49] d2.evaluation.evaluator INFO: Inference done 772/1093. Dataloading: 0.0129 s/iter. Inference: 0.1694 s/iter. Eval: 0.2004 s/iter. Total: 0.3829 s/iter. ETA=0:02:02
[01/17 04:00:54] d2.evaluation.evaluator INFO: Inference done 787/1093. Dataloading: 0.0129 s/iter. Inference: 0.1692 s/iter. Eval: 0.2001 s/iter. Total: 0.3823 s/iter. ETA=0:01:56
[01/17 04:00:59] d2.evaluation.evaluator INFO: Inference done 802/1093. Dataloading: 0.0128 s/iter. Inference: 0.1690 s/iter. Eval: 0.1998 s/iter. Total: 0.3819 s/iter. ETA=0:01:51
[01/17 04:01:04] d2.evaluation.evaluator INFO: Inference done 815/1093. Dataloading: 0.0128 s/iter. Inference: 0.1692 s/iter. Eval: 0.1997 s/iter. Total: 0.3820 s/iter. ETA=0:01:46
[01/17 04:01:09] d2.evaluation.evaluator INFO: Inference done 830/1093. Dataloading: 0.0128 s/iter. Inference: 0.1693 s/iter. Eval: 0.1990 s/iter. Total: 0.3812 s/iter. ETA=0:01:40
[01/17 04:01:15] d2.evaluation.evaluator INFO: Inference done 844/1093. Dataloading: 0.0128 s/iter. Inference: 0.1691 s/iter. Eval: 0.1991 s/iter. Total: 0.3812 s/iter. ETA=0:01:34
[01/17 04:01:20] d2.evaluation.evaluator INFO: Inference done 857/1093. Dataloading: 0.0128 s/iter. Inference: 0.1691 s/iter. Eval: 0.1993 s/iter. Total: 0.3814 s/iter. ETA=0:01:30
[01/17 04:01:25] d2.evaluation.evaluator INFO: Inference done 870/1093. Dataloading: 0.0128 s/iter. Inference: 0.1693 s/iter. Eval: 0.1992 s/iter. Total: 0.3815 s/iter. ETA=0:01:25
[01/17 04:01:30] d2.evaluation.evaluator INFO: Inference done 883/1093. Dataloading: 0.0128 s/iter. Inference: 0.1696 s/iter. Eval: 0.1991 s/iter. Total: 0.3817 s/iter. ETA=0:01:20
[01/17 04:01:35] d2.evaluation.evaluator INFO: Inference done 897/1093. Dataloading: 0.0128 s/iter. Inference: 0.1694 s/iter. Eval: 0.1991 s/iter. Total: 0.3815 s/iter. ETA=0:01:14
[01/17 04:01:41] d2.evaluation.evaluator INFO: Inference done 912/1093. Dataloading: 0.0127 s/iter. Inference: 0.1694 s/iter. Eval: 0.1988 s/iter. Total: 0.3811 s/iter. ETA=0:01:08
[01/17 04:01:46] d2.evaluation.evaluator INFO: Inference done 924/1093. Dataloading: 0.0127 s/iter. Inference: 0.1696 s/iter. Eval: 0.1994 s/iter. Total: 0.3819 s/iter. ETA=0:01:04
[01/17 04:01:51] d2.evaluation.evaluator INFO: Inference done 938/1093. Dataloading: 0.0127 s/iter. Inference: 0.1695 s/iter. Eval: 0.1991 s/iter. Total: 0.3815 s/iter. ETA=0:00:59
[01/17 04:01:56] d2.evaluation.evaluator INFO: Inference done 951/1093. Dataloading: 0.0128 s/iter. Inference: 0.1696 s/iter. Eval: 0.1994 s/iter. Total: 0.3819 s/iter. ETA=0:00:54
[01/17 04:02:01] d2.evaluation.evaluator INFO: Inference done 963/1093. Dataloading: 0.0127 s/iter. Inference: 0.1698 s/iter. Eval: 0.1996 s/iter. Total: 0.3824 s/iter. ETA=0:00:49
[01/17 04:02:07] d2.evaluation.evaluator INFO: Inference done 978/1093. Dataloading: 0.0127 s/iter. Inference: 0.1696 s/iter. Eval: 0.1994 s/iter. Total: 0.3819 s/iter. ETA=0:00:43
[01/17 04:02:12] d2.evaluation.evaluator INFO: Inference done 993/1093. Dataloading: 0.0127 s/iter. Inference: 0.1695 s/iter. Eval: 0.1992 s/iter. Total: 0.3815 s/iter. ETA=0:00:38
[01/17 04:02:17] d2.evaluation.evaluator INFO: Inference done 1007/1093. Dataloading: 0.0127 s/iter. Inference: 0.1696 s/iter. Eval: 0.1991 s/iter. Total: 0.3815 s/iter. ETA=0:00:32
[01/17 04:02:22] d2.evaluation.evaluator INFO: Inference done 1022/1093. Dataloading: 0.0126 s/iter. Inference: 0.1691 s/iter. Eval: 0.1989 s/iter. Total: 0.3809 s/iter. ETA=0:00:27
[01/17 04:02:27] d2.evaluation.evaluator INFO: Inference done 1037/1093. Dataloading: 0.0126 s/iter. Inference: 0.1689 s/iter. Eval: 0.1987 s/iter. Total: 0.3804 s/iter. ETA=0:00:21
[01/17 04:02:32] d2.evaluation.evaluator INFO: Inference done 1049/1093. Dataloading: 0.0126 s/iter. Inference: 0.1691 s/iter. Eval: 0.1989 s/iter. Total: 0.3808 s/iter. ETA=0:00:16
[01/17 04:02:37] d2.evaluation.evaluator INFO: Inference done 1062/1093. Dataloading: 0.0126 s/iter. Inference: 0.1693 s/iter. Eval: 0.1988 s/iter. Total: 0.3809 s/iter. ETA=0:00:11
[01/17 04:02:43] d2.evaluation.evaluator INFO: Inference done 1078/1093. Dataloading: 0.0126 s/iter. Inference: 0.1692 s/iter. Eval: 0.1981 s/iter. Total: 0.3800 s/iter. ETA=0:00:05
[01/17 04:02:48] d2.evaluation.evaluator INFO: Total inference time: 0:06:53.251671 (0.379827 s / iter per device, on 4 devices)
[01/17 04:02:48] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:03 (0.168900 s / iter per device, on 4 devices)
[01/17 04:03:24] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 2.855780505842924, 'fwIoU': 13.981649327742488, 'IoU-1': nan, 'IoU-2': 93.58956624003156, 'IoU-3': 30.51264559021271, 'IoU-4': 27.53710931444654, 'IoU-5': 23.96289671926057, 'IoU-6': 18.850257970827737, 'IoU-7': 16.040992968008155, 'IoU-8': 14.314572924313621, 'IoU-9': 3.6427342373425944, 'IoU-10': 9.31158900392648, 'IoU-11': 7.90765274625542, 'IoU-12': 14.09921310869173, 'IoU-13': 11.832840355935883, 'IoU-14': 13.041237620934238, 'IoU-15': 8.723632909826339, 'IoU-16': 7.566717506509263, 'IoU-17': 5.165179352283589, 'IoU-18': 4.936455628202438, 'IoU-19': 4.15045418336959, 'IoU-20': 5.685868320829478, 'IoU-21': 4.731829458475326, 'IoU-22': 4.092501153017473, 'IoU-23': 5.52808097931644, 'IoU-24': 4.100176492961452, 'IoU-25': 3.784277772145881, 'IoU-26': 3.0360607888619437, 'IoU-27': 1.2836674797958363, 'IoU-28': 7.395713814111986, 'IoU-29': 0.5459027429126759, 'IoU-30': 3.467056837704232, 'IoU-31': 5.538103844878863, 'IoU-32': 5.825516192300134, 'IoU-33': 7.027442504752507, 'IoU-34': 0.6487185397980111, 'IoU-35': 4.421858738681405, 'IoU-36': 5.572201647056091, 'IoU-37': 4.407477111380791, 'IoU-38': 6.397461311754754, 'IoU-39': 4.0251601995539215, 'IoU-40': 3.456295010900508, 'IoU-41': 5.81013447976122, 'IoU-42': 5.014101063653552, 'IoU-43': 4.3376488072671915, 'IoU-44': 3.414561293684873, 'IoU-45': 3.427671935701419, 'IoU-46': 5.374604562481003, 'IoU-47': 3.815693181840697, 'IoU-48': 4.795687851825287, 'IoU-49': 5.4239008999724545, 'IoU-50': 2.221183194130525, 'IoU-51': 4.280536941996552, 'IoU-52': 5.086496599359018, 'IoU-53': 4.614465793890223, 'IoU-54': 1.44904135278571, 'IoU-55': 2.734562995378749, 'IoU-56': 2.939316291022829, 'IoU-57': 5.683479217218357, 'IoU-58': 2.931869692501252, 'IoU-59': 2.315080958327086, 'IoU-60': 3.7163329627497266, 'IoU-61': 5.937280635925582, 'IoU-62': 6.014750734939763, 'IoU-63': 2.986655924969556, 'IoU-64': 2.2307830754651015, 'IoU-65': 1.6818970130764794, 'IoU-66': 3.509214061904108, 'IoU-67': 5.7801422747115785, 'IoU-68': 3.3455754488152967, 'IoU-69': 4.21294693928836, 'IoU-70': 4.956012379142749, 'IoU-71': 3.4199929400752818, 'IoU-72': 2.1538683519188964, 'IoU-73': 5.503774414284407, 'IoU-74': 2.700405114146382, 'IoU-75': 4.9698153710912365, 'IoU-76': 3.388062280303836, 'IoU-77': 3.0777472832113553, 'IoU-78': 3.5760015361832385, 'IoU-79': 4.857529312849271, 'IoU-80': 5.962531715813307, 'IoU-81': 3.4750296174856143, 'IoU-82': 2.7115137904550215, 'IoU-83': 1.7552911768095898, 'IoU-84': 4.70809814219121, 'IoU-85': 4.2555918162998765, 'IoU-86': 5.224053647129565, 'IoU-87': 3.4923188678015165, 'IoU-88': 2.188908132028209, 'IoU-89': 4.572477537538027, 'IoU-90': 2.1525294852581016, 'IoU-91': 4.920426925583211, 'IoU-92': 4.500361879359733, 'IoU-93': 2.7161295388796454, 'IoU-94': 4.176040511492282, 'IoU-95': 4.994226436653373, 'IoU-96': 1.9355631149356396, 'IoU-97': 4.1755139663625735, 'IoU-98': 3.9236045125363455, 'IoU-99': 2.6853037842970062, 'IoU-100': 2.8121582398921365, 'IoU-101': 1.9014263079355955, 'IoU-102': 4.591890938770096, 'IoU-103': 3.5669037161293438, 'IoU-104': 2.252199540588567, 'IoU-105': 3.0548750132762605, 'IoU-106': 2.4086054062237268, 'IoU-107': 2.4365242675881538, 'IoU-108': 3.9402867237121755, 'IoU-109': 4.448485498099903, 'IoU-110': 1.5764555744856603, 'IoU-111': 2.959913410929812, 'IoU-112': 1.9889849450247372, 'IoU-113': 1.5083616832452023, 'IoU-114': 3.716882384635959, 'IoU-115': 0.6927997407664028, 'IoU-116': 1.3577695482231222, 'IoU-117': 1.9775413192852587, 'IoU-118': 1.5805836870048617, 'IoU-119': 2.0343134631288975, 'IoU-120': 2.0480964570829263, 'IoU-121': 2.7181107558719093, 'IoU-122': 1.2277689865995813, 'IoU-123': 2.399106247518289, 'IoU-124': 1.9128639996884522, 'IoU-125': 2.7111878845287296, 'IoU-126': 2.8789028844304303, 'IoU-127': 1.0772438463815075, 'IoU-128': 1.0823461086249482, 'IoU-129': 0.8089805135530012, 'IoU-130': 1.7040887258954547, 'IoU-131': 2.587233211768335, 'IoU-132': 0.8046064260076556, 'IoU-133': 1.4069002168476894, 'IoU-134': 0.8670143464466863, 'IoU-135': 0.6145656724270685, 'IoU-136': 2.5947235933831525, 'IoU-137': 0.0755193988764121, 'IoU-138': 2.1867530902434362, 'IoU-139': 1.4522326045497145, 'IoU-140': 1.1699123605903818, 'IoU-141': 0.7991740988213702, 'IoU-142': 0.055759252395827824, 'IoU-143': 0.11801751118745861, 'IoU-144': 1.628713091730577, 'IoU-145': 1.329359659116724, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.48940737082808417, 'IoU-149': 0.0, 'IoU-150': 0.6078684714839448, 'IoU-151': 0.0, 'IoU-152': 0.0006352722194399652, 'IoU-153': 0.008216433499031409, 'IoU-154': 0.0, 'IoU-155': 0.8217872194049174, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.00013952858871018377, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0025927462368140336, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'IoU-192': 0.0, 'IoU-193': 0.0, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 5.263938716280188, 'pACC': 18.840375074477787, 'ACC-1': nan, 'ACC-2': 98.69261006194179, 'ACC-3': 35.99030627640314, 'ACC-4': 36.94399319701168, 'ACC-5': 37.06333466087911, 'ACC-6': 30.01185209968389, 'ACC-7': 25.288486246997, 'ACC-8': 24.920601930348475, 'ACC-9': 4.305454230501451, 'ACC-10': 18.510227038967823, 'ACC-11': 12.858262336493228, 'ACC-12': 22.953012547928303, 'ACC-13': 20.35657435391858, 'ACC-14': 30.366212431657814, 'ACC-15': 17.079926787498554, 'ACC-16': 14.695417180107109, 'ACC-17': 9.498490632802902, 'ACC-18': 11.969609389710978, 'ACC-19': 7.346161646657218, 'ACC-20': 10.361752422943558, 'ACC-21': 9.690559462953402, 'ACC-22': 6.922638226965205, 'ACC-23': 11.398517963826329, 'ACC-24': 8.633252741714204, 'ACC-25': 5.8559216582540765, 'ACC-26': 4.995605378699518, 'ACC-27': 1.7362831683484454, 'ACC-28': 18.645020352452544, 'ACC-29': 0.6334576987788235, 'ACC-30': 5.364847237607514, 'ACC-31': 12.609752152024559, 'ACC-32': 11.52913507592517, 'ACC-33': 18.442967496098493, 'ACC-34': 0.6919636653161757, 'ACC-35': 7.452765053396328, 'ACC-36': 11.50595604965143, 'ACC-37': 6.530571599501315, 'ACC-38': 12.718164577010494, 'ACC-39': 6.095628932706236, 'ACC-40': 6.505802384691298, 'ACC-41': 11.402906282600435, 'ACC-42': 8.900972950401604, 'ACC-43': 7.856918402242908, 'ACC-44': 4.932696406651053, 'ACC-45': 5.526027018515788, 'ACC-46': 11.525356090663683, 'ACC-47': 8.638257945971304, 'ACC-48': 9.923438405854364, 'ACC-49': 15.655799365385423, 'ACC-50': 3.017905491010887, 'ACC-51': 8.169950554019714, 'ACC-52': 13.466666747110361, 'ACC-53': 10.895065922189447, 'ACC-54': 1.8870008812878831, 'ACC-55': 4.353829111031045, 'ACC-56': 6.946145209452391, 'ACC-57': 20.106284465908196, 'ACC-58': 4.765433352973366, 'ACC-59': 3.169923422124215, 'ACC-60': 7.020454342737668, 'ACC-61': 17.071378660445138, 'ACC-62': 14.576469280204282, 'ACC-63': 5.619897120481058, 'ACC-64': 4.064957420089808, 'ACC-65': 2.513629328109855, 'ACC-66': 6.3249627699273, 'ACC-67': 16.925112559898057, 'ACC-68': 5.5265066255189925, 'ACC-69': 9.41842000258676, 'ACC-70': 11.173166427974717, 'ACC-71': 6.726680981878365, 'ACC-72': 2.9856767739470658, 'ACC-73': 14.444212065246735, 'ACC-74': 5.198493394013619, 'ACC-75': 8.917215768962343, 'ACC-76': 5.854419217702861, 'ACC-77': 4.703144467052084, 'ACC-78': 6.566282539349162, 'ACC-79': 11.047206680549353, 'ACC-80': 15.318641414646006, 'ACC-81': 7.365232625374714, 'ACC-82': 3.972767271973824, 'ACC-83': 2.1931872468631224, 'ACC-84': 9.233029559339434, 'ACC-85': 10.571822906465036, 'ACC-86': 13.223351863398747, 'ACC-87': 5.551685148764932, 'ACC-88': 2.8767805075712864, 'ACC-89': 8.893518538302152, 'ACC-90': 3.1132301389931896, 'ACC-91': 11.195772133535927, 'ACC-92': 9.989410518884574, 'ACC-93': 4.072507831253603, 'ACC-94': 7.838287803125292, 'ACC-95': 13.510914704014832, 'ACC-96': 2.6588839326468747, 'ACC-97': 11.04032411031781, 'ACC-98': 7.822571076811186, 'ACC-99': 4.082648050738022, 'ACC-100': 4.938360406878395, 'ACC-101': 2.5083384219399636, 'ACC-102': 10.237464557207975, 'ACC-103': 7.840688717716357, 'ACC-104': 3.4358647397751962, 'ACC-105': 5.0648041120484, 'ACC-106': 3.4131594761503554, 'ACC-107': 5.453257697512834, 'ACC-108': 10.657682031600155, 'ACC-109': 9.97864541691735, 'ACC-110': 2.2160704085002756, 'ACC-111': 6.192849565920915, 'ACC-112': 3.2721057287940223, 'ACC-113': 2.1776210028567826, 'ACC-114': 9.994468921058154, 'ACC-115': 0.9864077839735247, 'ACC-116': 2.067377805842467, 'ACC-117': 3.329594350493876, 'ACC-118': 3.12675044421603, 'ACC-119': 3.6705340771727055, 'ACC-120': 3.5646871570127345, 'ACC-121': 7.084418276951405, 'ACC-122': 1.8427735488345662, 'ACC-123': 4.204242029682885, 'ACC-124': 4.847247413992172, 'ACC-125': 6.27606079800714, 'ACC-126': 10.56541322633171, 'ACC-127': 1.4772447836501243, 'ACC-128': 1.4672831257183974, 'ACC-129': 0.9809230739208017, 'ACC-130': 2.9905387877072642, 'ACC-131': 18.152720415433787, 'ACC-132': 1.0848580426965142, 'ACC-133': 2.5833324955682224, 'ACC-134': 2.37583947416767, 'ACC-135': 0.7818698836708317, 'ACC-136': 9.232512399521122, 'ACC-137': 0.08124068207230713, 'ACC-138': 6.1324192862709, 'ACC-139': 1.9002081376754576, 'ACC-140': 4.56436672014744, 'ACC-141': 2.3081497531005306, 'ACC-142': 0.05672073370036304, 'ACC-143': 0.14051653143369655, 'ACC-144': 8.348182554209766, 'ACC-145': 10.4307761732852, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.9266627235527394, 'ACC-149': 0.0, 'ACC-150': 0.7109035980240733, 'ACC-151': 0.0, 'ACC-152': 0.0006394659180652319, 'ACC-153': 0.008218693101355451, 'ACC-154': 0.0, 'ACC-155': 1.840155835555446, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.00014138352259874227, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0025928806904470868, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0, 'ACC-192': 0.0, 'ACC-193': 0.0, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 04:03:24] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 04:03:24] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 04:03:24] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 04:03:24] d2.evaluation.testing INFO: copypaste: 2.8558,13.9816,5.2639,18.8404
[01/17 04:03:24] d2.utils.events INFO:  eta: 1 day, 13:23:45  iter: 1999  total_loss: 54.49  loss_ce: 0.4198  loss_mask: 0.5584  loss_dice: 4.386  loss_ce_0: 1.161  loss_mask_0: 0.5409  loss_dice_0: 4.447  loss_ce_1: 0.4324  loss_mask_1: 0.5483  loss_dice_1: 4.406  loss_ce_2: 0.4014  loss_mask_2: 0.5525  loss_dice_2: 4.395  loss_ce_3: 0.4116  loss_mask_3: 0.5513  loss_dice_3: 4.388  loss_ce_4: 0.4134  loss_mask_4: 0.5524  loss_dice_4: 4.38  loss_ce_5: 0.4171  loss_mask_5: 0.555  loss_dice_5: 4.385  loss_ce_6: 0.4104  loss_mask_6: 0.5557  loss_dice_6: 4.386  loss_ce_7: 0.4041  loss_mask_7: 0.5558  loss_dice_7: 4.382  loss_ce_8: 0.4079  loss_mask_8: 0.5536  loss_dice_8: 4.381  time: 1.5454  data_time: 0.0901  lr: 9.7999e-06  max_mem: 21234M
[01/17 04:03:55] d2.utils.events INFO:  eta: 1 day, 13:23:15  iter: 2019  total_loss: 54.17  loss_ce: 0.4149  loss_mask: 0.5585  loss_dice: 4.372  loss_ce_0: 1.12  loss_mask_0: 0.5385  loss_dice_0: 4.451  loss_ce_1: 0.4494  loss_mask_1: 0.5509  loss_dice_1: 4.399  loss_ce_2: 0.4123  loss_mask_2: 0.553  loss_dice_2: 4.38  loss_ce_3: 0.4016  loss_mask_3: 0.5568  loss_dice_3: 4.378  loss_ce_4: 0.414  loss_mask_4: 0.5558  loss_dice_4: 4.37  loss_ce_5: 0.4006  loss_mask_5: 0.5557  loss_dice_5: 4.372  loss_ce_6: 0.4037  loss_mask_6: 0.5543  loss_dice_6: 4.371  loss_ce_7: 0.4003  loss_mask_7: 0.5569  loss_dice_7: 4.366  loss_ce_8: 0.4071  loss_mask_8: 0.5569  loss_dice_8: 4.373  time: 1.5452  data_time: 0.0946  lr: 9.7979e-06  max_mem: 21234M
[01/17 04:04:25] d2.utils.events INFO:  eta: 1 day, 13:22:57  iter: 2039  total_loss: 54.35  loss_ce: 0.4302  loss_mask: 0.5524  loss_dice: 4.38  loss_ce_0: 1.158  loss_mask_0: 0.5312  loss_dice_0: 4.44  loss_ce_1: 0.4617  loss_mask_1: 0.5405  loss_dice_1: 4.398  loss_ce_2: 0.4233  loss_mask_2: 0.5441  loss_dice_2: 4.381  loss_ce_3: 0.4226  loss_mask_3: 0.5442  loss_dice_3: 4.379  loss_ce_4: 0.441  loss_mask_4: 0.5474  loss_dice_4: 4.385  loss_ce_5: 0.4322  loss_mask_5: 0.5487  loss_dice_5: 4.379  loss_ce_6: 0.427  loss_mask_6: 0.5465  loss_dice_6: 4.382  loss_ce_7: 0.4188  loss_mask_7: 0.5488  loss_dice_7: 4.376  loss_ce_8: 0.4279  loss_mask_8: 0.5494  loss_dice_8: 4.386  time: 1.5449  data_time: 0.0883  lr: 9.7959e-06  max_mem: 21234M
[01/17 04:04:56] d2.utils.events INFO:  eta: 1 day, 13:21:56  iter: 2059  total_loss: 54.19  loss_ce: 0.4081  loss_mask: 0.5532  loss_dice: 4.375  loss_ce_0: 1.083  loss_mask_0: 0.54  loss_dice_0: 4.445  loss_ce_1: 0.4433  loss_mask_1: 0.548  loss_dice_1: 4.396  loss_ce_2: 0.4175  loss_mask_2: 0.5473  loss_dice_2: 4.382  loss_ce_3: 0.4213  loss_mask_3: 0.5501  loss_dice_3: 4.372  loss_ce_4: 0.426  loss_mask_4: 0.5528  loss_dice_4: 4.379  loss_ce_5: 0.4163  loss_mask_5: 0.5511  loss_dice_5: 4.378  loss_ce_6: 0.4166  loss_mask_6: 0.5537  loss_dice_6: 4.385  loss_ce_7: 0.4101  loss_mask_7: 0.5531  loss_dice_7: 4.39  loss_ce_8: 0.4107  loss_mask_8: 0.5558  loss_dice_8: 4.384  time: 1.5449  data_time: 0.0978  lr: 9.7939e-06  max_mem: 21234M
[01/17 04:05:26] d2.utils.events INFO:  eta: 1 day, 13:19:26  iter: 2079  total_loss: 54.23  loss_ce: 0.4054  loss_mask: 0.5786  loss_dice: 4.356  loss_ce_0: 1.115  loss_mask_0: 0.5593  loss_dice_0: 4.42  loss_ce_1: 0.4264  loss_mask_1: 0.569  loss_dice_1: 4.379  loss_ce_2: 0.4131  loss_mask_2: 0.5731  loss_dice_2: 4.369  loss_ce_3: 0.4102  loss_mask_3: 0.5744  loss_dice_3: 4.359  loss_ce_4: 0.4122  loss_mask_4: 0.5786  loss_dice_4: 4.358  loss_ce_5: 0.3989  loss_mask_5: 0.58  loss_dice_5: 4.363  loss_ce_6: 0.4072  loss_mask_6: 0.5827  loss_dice_6: 4.358  loss_ce_7: 0.3969  loss_mask_7: 0.5785  loss_dice_7: 4.358  loss_ce_8: 0.4038  loss_mask_8: 0.5814  loss_dice_8: 4.359  time: 1.5445  data_time: 0.0957  lr: 9.7919e-06  max_mem: 21234M
[01/17 04:05:57] d2.utils.events INFO:  eta: 1 day, 13:20:12  iter: 2099  total_loss: 54.01  loss_ce: 0.4281  loss_mask: 0.5558  loss_dice: 4.351  loss_ce_0: 1.095  loss_mask_0: 0.5363  loss_dice_0: 4.437  loss_ce_1: 0.4742  loss_mask_1: 0.5477  loss_dice_1: 4.369  loss_ce_2: 0.4256  loss_mask_2: 0.5506  loss_dice_2: 4.349  loss_ce_3: 0.433  loss_mask_3: 0.5516  loss_dice_3: 4.347  loss_ce_4: 0.4219  loss_mask_4: 0.5534  loss_dice_4: 4.35  loss_ce_5: 0.4183  loss_mask_5: 0.5527  loss_dice_5: 4.352  loss_ce_6: 0.4195  loss_mask_6: 0.5547  loss_dice_6: 4.347  loss_ce_7: 0.426  loss_mask_7: 0.5552  loss_dice_7: 4.346  loss_ce_8: 0.4214  loss_mask_8: 0.5562  loss_dice_8: 4.349  time: 1.5444  data_time: 0.1063  lr: 9.7899e-06  max_mem: 21234M
[01/17 04:06:27] d2.utils.events INFO:  eta: 1 day, 13:19:34  iter: 2119  total_loss: 53.91  loss_ce: 0.3987  loss_mask: 0.5656  loss_dice: 4.36  loss_ce_0: 1.095  loss_mask_0: 0.5413  loss_dice_0: 4.427  loss_ce_1: 0.4297  loss_mask_1: 0.5532  loss_dice_1: 4.376  loss_ce_2: 0.409  loss_mask_2: 0.5543  loss_dice_2: 4.365  loss_ce_3: 0.4192  loss_mask_3: 0.5535  loss_dice_3: 4.346  loss_ce_4: 0.4071  loss_mask_4: 0.5555  loss_dice_4: 4.354  loss_ce_5: 0.4127  loss_mask_5: 0.5601  loss_dice_5: 4.359  loss_ce_6: 0.4059  loss_mask_6: 0.5604  loss_dice_6: 4.35  loss_ce_7: 0.4051  loss_mask_7: 0.5613  loss_dice_7: 4.358  loss_ce_8: 0.3922  loss_mask_8: 0.563  loss_dice_8: 4.356  time: 1.5442  data_time: 0.0945  lr: 9.7878e-06  max_mem: 21234M
[01/17 04:06:58] d2.utils.events INFO:  eta: 1 day, 13:19:54  iter: 2139  total_loss: 54.21  loss_ce: 0.4228  loss_mask: 0.5639  loss_dice: 4.354  loss_ce_0: 1.12  loss_mask_0: 0.5321  loss_dice_0: 4.436  loss_ce_1: 0.4487  loss_mask_1: 0.5552  loss_dice_1: 4.373  loss_ce_2: 0.4305  loss_mask_2: 0.5576  loss_dice_2: 4.355  loss_ce_3: 0.4184  loss_mask_3: 0.5584  loss_dice_3: 4.352  loss_ce_4: 0.4269  loss_mask_4: 0.5601  loss_dice_4: 4.347  loss_ce_5: 0.4058  loss_mask_5: 0.5613  loss_dice_5: 4.351  loss_ce_6: 0.4239  loss_mask_6: 0.5592  loss_dice_6: 4.349  loss_ce_7: 0.423  loss_mask_7: 0.5611  loss_dice_7: 4.341  loss_ce_8: 0.4206  loss_mask_8: 0.5605  loss_dice_8: 4.348  time: 1.5440  data_time: 0.0878  lr: 9.7858e-06  max_mem: 21234M
[01/17 04:07:29] d2.utils.events INFO:  eta: 1 day, 13:19:40  iter: 2159  total_loss: 53.98  loss_ce: 0.4072  loss_mask: 0.5485  loss_dice: 4.364  loss_ce_0: 1.059  loss_mask_0: 0.5275  loss_dice_0: 4.429  loss_ce_1: 0.4332  loss_mask_1: 0.5392  loss_dice_1: 4.384  loss_ce_2: 0.4165  loss_mask_2: 0.5401  loss_dice_2: 4.359  loss_ce_3: 0.4162  loss_mask_3: 0.5454  loss_dice_3: 4.361  loss_ce_4: 0.4128  loss_mask_4: 0.545  loss_dice_4: 4.358  loss_ce_5: 0.3983  loss_mask_5: 0.5479  loss_dice_5: 4.363  loss_ce_6: 0.4095  loss_mask_6: 0.5489  loss_dice_6: 4.359  loss_ce_7: 0.3953  loss_mask_7: 0.5501  loss_dice_7: 4.362  loss_ce_8: 0.3961  loss_mask_8: 0.5472  loss_dice_8: 4.36  time: 1.5440  data_time: 0.0945  lr: 9.7838e-06  max_mem: 21234M
[01/17 04:07:59] d2.utils.events INFO:  eta: 1 day, 13:19:55  iter: 2179  total_loss: 53.67  loss_ce: 0.4164  loss_mask: 0.5469  loss_dice: 4.36  loss_ce_0: 1.041  loss_mask_0: 0.527  loss_dice_0: 4.428  loss_ce_1: 0.4427  loss_mask_1: 0.5416  loss_dice_1: 4.376  loss_ce_2: 0.4376  loss_mask_2: 0.5382  loss_dice_2: 4.357  loss_ce_3: 0.424  loss_mask_3: 0.5408  loss_dice_3: 4.355  loss_ce_4: 0.4198  loss_mask_4: 0.5423  loss_dice_4: 4.357  loss_ce_5: 0.4105  loss_mask_5: 0.5433  loss_dice_5: 4.349  loss_ce_6: 0.4101  loss_mask_6: 0.5457  loss_dice_6: 4.356  loss_ce_7: 0.4245  loss_mask_7: 0.5466  loss_dice_7: 4.356  loss_ce_8: 0.4152  loss_mask_8: 0.5466  loss_dice_8: 4.349  time: 1.5438  data_time: 0.0987  lr: 9.7818e-06  max_mem: 21234M
[01/17 04:08:30] d2.utils.events INFO:  eta: 1 day, 13:19:42  iter: 2199  total_loss: 53.78  loss_ce: 0.4114  loss_mask: 0.569  loss_dice: 4.339  loss_ce_0: 1.081  loss_mask_0: 0.5516  loss_dice_0: 4.417  loss_ce_1: 0.4579  loss_mask_1: 0.5581  loss_dice_1: 4.353  loss_ce_2: 0.4285  loss_mask_2: 0.5645  loss_dice_2: 4.338  loss_ce_3: 0.428  loss_mask_3: 0.566  loss_dice_3: 4.338  loss_ce_4: 0.4198  loss_mask_4: 0.5684  loss_dice_4: 4.335  loss_ce_5: 0.4193  loss_mask_5: 0.5673  loss_dice_5: 4.337  loss_ce_6: 0.4144  loss_mask_6: 0.5671  loss_dice_6: 4.338  loss_ce_7: 0.4164  loss_mask_7: 0.5688  loss_dice_7: 4.332  loss_ce_8: 0.42  loss_mask_8: 0.5663  loss_dice_8: 4.338  time: 1.5438  data_time: 0.1014  lr: 9.7798e-06  max_mem: 21234M
[01/17 04:09:01] d2.utils.events INFO:  eta: 1 day, 13:19:11  iter: 2219  total_loss: 53.65  loss_ce: 0.4203  loss_mask: 0.551  loss_dice: 4.346  loss_ce_0: 1.027  loss_mask_0: 0.531  loss_dice_0: 4.42  loss_ce_1: 0.4373  loss_mask_1: 0.5474  loss_dice_1: 4.361  loss_ce_2: 0.422  loss_mask_2: 0.5481  loss_dice_2: 4.339  loss_ce_3: 0.4214  loss_mask_3: 0.5477  loss_dice_3: 4.336  loss_ce_4: 0.4225  loss_mask_4: 0.5501  loss_dice_4: 4.338  loss_ce_5: 0.4203  loss_mask_5: 0.5482  loss_dice_5: 4.343  loss_ce_6: 0.4235  loss_mask_6: 0.5493  loss_dice_6: 4.339  loss_ce_7: 0.4242  loss_mask_7: 0.5488  loss_dice_7: 4.335  loss_ce_8: 0.4287  loss_mask_8: 0.5506  loss_dice_8: 4.34  time: 1.5435  data_time: 0.0927  lr: 9.7778e-06  max_mem: 21234M
[01/17 04:09:31] d2.utils.events INFO:  eta: 1 day, 13:17:30  iter: 2239  total_loss: 53.85  loss_ce: 0.4365  loss_mask: 0.5591  loss_dice: 4.313  loss_ce_0: 1.07  loss_mask_0: 0.5372  loss_dice_0: 4.388  loss_ce_1: 0.467  loss_mask_1: 0.5484  loss_dice_1: 4.338  loss_ce_2: 0.4714  loss_mask_2: 0.5525  loss_dice_2: 4.324  loss_ce_3: 0.4479  loss_mask_3: 0.5537  loss_dice_3: 4.32  loss_ce_4: 0.4335  loss_mask_4: 0.5581  loss_dice_4: 4.316  loss_ce_5: 0.4377  loss_mask_5: 0.5585  loss_dice_5: 4.314  loss_ce_6: 0.4383  loss_mask_6: 0.5579  loss_dice_6: 4.303  loss_ce_7: 0.4345  loss_mask_7: 0.5614  loss_dice_7: 4.308  loss_ce_8: 0.4419  loss_mask_8: 0.5585  loss_dice_8: 4.31  time: 1.5434  data_time: 0.0927  lr: 9.7758e-06  max_mem: 21234M
[01/17 04:10:02] d2.utils.events INFO:  eta: 1 day, 13:18:10  iter: 2259  total_loss: 53.78  loss_ce: 0.4183  loss_mask: 0.5521  loss_dice: 4.349  loss_ce_0: 1.032  loss_mask_0: 0.5351  loss_dice_0: 4.419  loss_ce_1: 0.4235  loss_mask_1: 0.5427  loss_dice_1: 4.361  loss_ce_2: 0.4308  loss_mask_2: 0.5456  loss_dice_2: 4.352  loss_ce_3: 0.4219  loss_mask_3: 0.5482  loss_dice_3: 4.347  loss_ce_4: 0.4266  loss_mask_4: 0.5485  loss_dice_4: 4.344  loss_ce_5: 0.3997  loss_mask_5: 0.5514  loss_dice_5: 4.327  loss_ce_6: 0.4126  loss_mask_6: 0.5524  loss_dice_6: 4.34  loss_ce_7: 0.4027  loss_mask_7: 0.5539  loss_dice_7: 4.341  loss_ce_8: 0.4088  loss_mask_8: 0.5528  loss_dice_8: 4.342  time: 1.5434  data_time: 0.0951  lr: 9.7738e-06  max_mem: 21234M
[01/17 04:10:32] d2.utils.events INFO:  eta: 1 day, 13:17:34  iter: 2279  total_loss: 53.49  loss_ce: 0.3824  loss_mask: 0.5742  loss_dice: 4.326  loss_ce_0: 0.9758  loss_mask_0: 0.5478  loss_dice_0: 4.397  loss_ce_1: 0.4152  loss_mask_1: 0.5621  loss_dice_1: 4.348  loss_ce_2: 0.3961  loss_mask_2: 0.5679  loss_dice_2: 4.333  loss_ce_3: 0.3999  loss_mask_3: 0.5688  loss_dice_3: 4.326  loss_ce_4: 0.3834  loss_mask_4: 0.5721  loss_dice_4: 4.326  loss_ce_5: 0.3757  loss_mask_5: 0.5716  loss_dice_5: 4.316  loss_ce_6: 0.3787  loss_mask_6: 0.5693  loss_dice_6: 4.324  loss_ce_7: 0.3841  loss_mask_7: 0.5694  loss_dice_7: 4.319  loss_ce_8: 0.3909  loss_mask_8: 0.5711  loss_dice_8: 4.322  time: 1.5431  data_time: 0.0888  lr: 9.7718e-06  max_mem: 21234M
[01/17 04:11:03] d2.utils.events INFO:  eta: 1 day, 13:15:19  iter: 2299  total_loss: 53.54  loss_ce: 0.3913  loss_mask: 0.5716  loss_dice: 4.32  loss_ce_0: 1.003  loss_mask_0: 0.5424  loss_dice_0: 4.399  loss_ce_1: 0.4226  loss_mask_1: 0.5598  loss_dice_1: 4.339  loss_ce_2: 0.3886  loss_mask_2: 0.5663  loss_dice_2: 4.328  loss_ce_3: 0.3978  loss_mask_3: 0.5681  loss_dice_3: 4.323  loss_ce_4: 0.3998  loss_mask_4: 0.5683  loss_dice_4: 4.319  loss_ce_5: 0.3825  loss_mask_5: 0.5685  loss_dice_5: 4.325  loss_ce_6: 0.3907  loss_mask_6: 0.5722  loss_dice_6: 4.32  loss_ce_7: 0.3931  loss_mask_7: 0.5739  loss_dice_7: 4.319  loss_ce_8: 0.4012  loss_mask_8: 0.5721  loss_dice_8: 4.326  time: 1.5429  data_time: 0.0924  lr: 9.7698e-06  max_mem: 21234M
[01/17 04:11:33] d2.utils.events INFO:  eta: 1 day, 13:11:58  iter: 2319  total_loss: 53.38  loss_ce: 0.416  loss_mask: 0.5606  loss_dice: 4.315  loss_ce_0: 0.9894  loss_mask_0: 0.5417  loss_dice_0: 4.387  loss_ce_1: 0.4323  loss_mask_1: 0.5536  loss_dice_1: 4.333  loss_ce_2: 0.4233  loss_mask_2: 0.5556  loss_dice_2: 4.319  loss_ce_3: 0.4096  loss_mask_3: 0.5544  loss_dice_3: 4.311  loss_ce_4: 0.4199  loss_mask_4: 0.5587  loss_dice_4: 4.317  loss_ce_5: 0.4121  loss_mask_5: 0.5608  loss_dice_5: 4.311  loss_ce_6: 0.4102  loss_mask_6: 0.5605  loss_dice_6: 4.319  loss_ce_7: 0.4215  loss_mask_7: 0.562  loss_dice_7: 4.314  loss_ce_8: 0.4155  loss_mask_8: 0.563  loss_dice_8: 4.309  time: 1.5426  data_time: 0.0866  lr: 9.7678e-06  max_mem: 21234M
[01/17 04:12:04] d2.utils.events INFO:  eta: 1 day, 13:10:55  iter: 2339  total_loss: 53.45  loss_ce: 0.4337  loss_mask: 0.5407  loss_dice: 4.303  loss_ce_0: 0.9785  loss_mask_0: 0.5153  loss_dice_0: 4.404  loss_ce_1: 0.4483  loss_mask_1: 0.5308  loss_dice_1: 4.341  loss_ce_2: 0.4357  loss_mask_2: 0.5336  loss_dice_2: 4.313  loss_ce_3: 0.4254  loss_mask_3: 0.5348  loss_dice_3: 4.305  loss_ce_4: 0.4141  loss_mask_4: 0.5381  loss_dice_4: 4.304  loss_ce_5: 0.4074  loss_mask_5: 0.5363  loss_dice_5: 4.314  loss_ce_6: 0.4171  loss_mask_6: 0.5337  loss_dice_6: 4.312  loss_ce_7: 0.4119  loss_mask_7: 0.5379  loss_dice_7: 4.306  loss_ce_8: 0.4179  loss_mask_8: 0.536  loss_dice_8: 4.31  time: 1.5425  data_time: 0.0886  lr: 9.7658e-06  max_mem: 21234M
[01/17 04:12:34] d2.utils.events INFO:  eta: 1 day, 13:10:24  iter: 2359  total_loss: 53.53  loss_ce: 0.4146  loss_mask: 0.5513  loss_dice: 4.295  loss_ce_0: 1.011  loss_mask_0: 0.5386  loss_dice_0: 4.374  loss_ce_1: 0.4253  loss_mask_1: 0.5425  loss_dice_1: 4.319  loss_ce_2: 0.4209  loss_mask_2: 0.5439  loss_dice_2: 4.313  loss_ce_3: 0.4118  loss_mask_3: 0.5471  loss_dice_3: 4.315  loss_ce_4: 0.422  loss_mask_4: 0.5492  loss_dice_4: 4.314  loss_ce_5: 0.4162  loss_mask_5: 0.5488  loss_dice_5: 4.31  loss_ce_6: 0.4214  loss_mask_6: 0.5529  loss_dice_6: 4.291  loss_ce_7: 0.4099  loss_mask_7: 0.5514  loss_dice_7: 4.297  loss_ce_8: 0.411  loss_mask_8: 0.5515  loss_dice_8: 4.3  time: 1.5422  data_time: 0.0958  lr: 9.7638e-06  max_mem: 21234M
[01/17 04:13:04] d2.utils.events INFO:  eta: 1 day, 13:10:26  iter: 2379  total_loss: 53.24  loss_ce: 0.4134  loss_mask: 0.5515  loss_dice: 4.303  loss_ce_0: 0.9657  loss_mask_0: 0.5334  loss_dice_0: 4.376  loss_ce_1: 0.4476  loss_mask_1: 0.5453  loss_dice_1: 4.327  loss_ce_2: 0.4263  loss_mask_2: 0.5462  loss_dice_2: 4.309  loss_ce_3: 0.3995  loss_mask_3: 0.5501  loss_dice_3: 4.301  loss_ce_4: 0.4016  loss_mask_4: 0.55  loss_dice_4: 4.301  loss_ce_5: 0.4063  loss_mask_5: 0.5538  loss_dice_5: 4.296  loss_ce_6: 0.401  loss_mask_6: 0.5535  loss_dice_6: 4.297  loss_ce_7: 0.4162  loss_mask_7: 0.5542  loss_dice_7: 4.297  loss_ce_8: 0.4099  loss_mask_8: 0.5529  loss_dice_8: 4.294  time: 1.5420  data_time: 0.0991  lr: 9.7618e-06  max_mem: 21234M
[01/17 04:13:35] d2.utils.events INFO:  eta: 1 day, 13:07:57  iter: 2399  total_loss: 53.24  loss_ce: 0.3937  loss_mask: 0.5698  loss_dice: 4.29  loss_ce_0: 0.9661  loss_mask_0: 0.5476  loss_dice_0: 4.366  loss_ce_1: 0.4305  loss_mask_1: 0.5604  loss_dice_1: 4.301  loss_ce_2: 0.4041  loss_mask_2: 0.5662  loss_dice_2: 4.295  loss_ce_3: 0.3974  loss_mask_3: 0.5686  loss_dice_3: 4.282  loss_ce_4: 0.393  loss_mask_4: 0.5674  loss_dice_4: 4.287  loss_ce_5: 0.4066  loss_mask_5: 0.5672  loss_dice_5: 4.296  loss_ce_6: 0.4039  loss_mask_6: 0.5686  loss_dice_6: 4.289  loss_ce_7: 0.3974  loss_mask_7: 0.5701  loss_dice_7: 4.286  loss_ce_8: 0.4038  loss_mask_8: 0.5716  loss_dice_8: 4.292  time: 1.5418  data_time: 0.0930  lr: 9.7598e-06  max_mem: 21234M
[01/17 04:14:05] d2.utils.events INFO:  eta: 1 day, 13:07:09  iter: 2419  total_loss: 53.35  loss_ce: 0.4092  loss_mask: 0.5576  loss_dice: 4.298  loss_ce_0: 0.9719  loss_mask_0: 0.5331  loss_dice_0: 4.386  loss_ce_1: 0.4397  loss_mask_1: 0.5474  loss_dice_1: 4.324  loss_ce_2: 0.423  loss_mask_2: 0.5531  loss_dice_2: 4.3  loss_ce_3: 0.4039  loss_mask_3: 0.5548  loss_dice_3: 4.295  loss_ce_4: 0.4117  loss_mask_4: 0.5531  loss_dice_4: 4.297  loss_ce_5: 0.4048  loss_mask_5: 0.5584  loss_dice_5: 4.303  loss_ce_6: 0.4169  loss_mask_6: 0.5605  loss_dice_6: 4.294  loss_ce_7: 0.4087  loss_mask_7: 0.5601  loss_dice_7: 4.294  loss_ce_8: 0.4054  loss_mask_8: 0.5582  loss_dice_8: 4.293  time: 1.5416  data_time: 0.0902  lr: 9.7578e-06  max_mem: 21234M
[01/17 04:14:35] d2.utils.events INFO:  eta: 1 day, 13:05:42  iter: 2439  total_loss: 53.74  loss_ce: 0.4332  loss_mask: 0.5766  loss_dice: 4.299  loss_ce_0: 1.014  loss_mask_0: 0.5569  loss_dice_0: 4.371  loss_ce_1: 0.455  loss_mask_1: 0.5735  loss_dice_1: 4.318  loss_ce_2: 0.4511  loss_mask_2: 0.5752  loss_dice_2: 4.3  loss_ce_3: 0.4432  loss_mask_3: 0.5723  loss_dice_3: 4.297  loss_ce_4: 0.4359  loss_mask_4: 0.5756  loss_dice_4: 4.292  loss_ce_5: 0.4355  loss_mask_5: 0.5783  loss_dice_5: 4.297  loss_ce_6: 0.4466  loss_mask_6: 0.5758  loss_dice_6: 4.297  loss_ce_7: 0.4324  loss_mask_7: 0.5753  loss_dice_7: 4.295  loss_ce_8: 0.4334  loss_mask_8: 0.5786  loss_dice_8: 4.299  time: 1.5413  data_time: 0.0904  lr: 9.7558e-06  max_mem: 21234M
[01/17 04:15:05] d2.utils.events INFO:  eta: 1 day, 13:04:57  iter: 2459  total_loss: 53.46  loss_ce: 0.4056  loss_mask: 0.5743  loss_dice: 4.295  loss_ce_0: 0.9691  loss_mask_0: 0.5528  loss_dice_0: 4.381  loss_ce_1: 0.451  loss_mask_1: 0.5653  loss_dice_1: 4.317  loss_ce_2: 0.4303  loss_mask_2: 0.5719  loss_dice_2: 4.309  loss_ce_3: 0.4224  loss_mask_3: 0.5731  loss_dice_3: 4.296  loss_ce_4: 0.4051  loss_mask_4: 0.5729  loss_dice_4: 4.293  loss_ce_5: 0.4184  loss_mask_5: 0.5736  loss_dice_5: 4.295  loss_ce_6: 0.4078  loss_mask_6: 0.5761  loss_dice_6: 4.297  loss_ce_7: 0.4043  loss_mask_7: 0.5769  loss_dice_7: 4.298  loss_ce_8: 0.411  loss_mask_8: 0.5742  loss_dice_8: 4.297  time: 1.5410  data_time: 0.0876  lr: 9.7538e-06  max_mem: 21234M
[01/17 04:15:36] d2.utils.events INFO:  eta: 1 day, 13:04:08  iter: 2479  total_loss: 53.28  loss_ce: 0.3948  loss_mask: 0.5592  loss_dice: 4.306  loss_ce_0: 0.9333  loss_mask_0: 0.533  loss_dice_0: 4.379  loss_ce_1: 0.4273  loss_mask_1: 0.5586  loss_dice_1: 4.324  loss_ce_2: 0.4057  loss_mask_2: 0.5582  loss_dice_2: 4.311  loss_ce_3: 0.3988  loss_mask_3: 0.5627  loss_dice_3: 4.311  loss_ce_4: 0.4017  loss_mask_4: 0.5575  loss_dice_4: 4.314  loss_ce_5: 0.3891  loss_mask_5: 0.5597  loss_dice_5: 4.307  loss_ce_6: 0.3896  loss_mask_6: 0.5602  loss_dice_6: 4.299  loss_ce_7: 0.3918  loss_mask_7: 0.5574  loss_dice_7: 4.306  loss_ce_8: 0.3967  loss_mask_8: 0.558  loss_dice_8: 4.301  time: 1.5409  data_time: 0.0920  lr: 9.7518e-06  max_mem: 21234M
[01/17 04:16:07] d2.utils.events INFO:  eta: 1 day, 13:03:56  iter: 2499  total_loss: 53.32  loss_ce: 0.457  loss_mask: 0.5446  loss_dice: 4.312  loss_ce_0: 0.9599  loss_mask_0: 0.5259  loss_dice_0: 4.382  loss_ce_1: 0.4463  loss_mask_1: 0.5354  loss_dice_1: 4.337  loss_ce_2: 0.448  loss_mask_2: 0.5404  loss_dice_2: 4.319  loss_ce_3: 0.4542  loss_mask_3: 0.5443  loss_dice_3: 4.312  loss_ce_4: 0.4497  loss_mask_4: 0.5445  loss_dice_4: 4.308  loss_ce_5: 0.4482  loss_mask_5: 0.5463  loss_dice_5: 4.309  loss_ce_6: 0.4469  loss_mask_6: 0.5488  loss_dice_6: 4.307  loss_ce_7: 0.4571  loss_mask_7: 0.5473  loss_dice_7: 4.308  loss_ce_8: 0.4543  loss_mask_8: 0.5443  loss_dice_8: 4.31  time: 1.5410  data_time: 0.0980  lr: 9.7497e-06  max_mem: 21234M
[01/17 04:16:38] d2.utils.events INFO:  eta: 1 day, 13:03:27  iter: 2519  total_loss: 53.18  loss_ce: 0.4023  loss_mask: 0.5657  loss_dice: 4.285  loss_ce_0: 0.9551  loss_mask_0: 0.5407  loss_dice_0: 4.371  loss_ce_1: 0.4293  loss_mask_1: 0.5617  loss_dice_1: 4.306  loss_ce_2: 0.4081  loss_mask_2: 0.5612  loss_dice_2: 4.295  loss_ce_3: 0.3974  loss_mask_3: 0.5604  loss_dice_3: 4.284  loss_ce_4: 0.4157  loss_mask_4: 0.5612  loss_dice_4: 4.29  loss_ce_5: 0.3909  loss_mask_5: 0.5638  loss_dice_5: 4.285  loss_ce_6: 0.4113  loss_mask_6: 0.5619  loss_dice_6: 4.294  loss_ce_7: 0.4068  loss_mask_7: 0.5647  loss_dice_7: 4.286  loss_ce_8: 0.4018  loss_mask_8: 0.5664  loss_dice_8: 4.288  time: 1.5409  data_time: 0.0956  lr: 9.7477e-06  max_mem: 21234M
[01/17 04:17:08] d2.utils.events INFO:  eta: 1 day, 13:02:56  iter: 2539  total_loss: 53.52  loss_ce: 0.4082  loss_mask: 0.5686  loss_dice: 4.306  loss_ce_0: 0.9204  loss_mask_0: 0.5382  loss_dice_0: 4.388  loss_ce_1: 0.4389  loss_mask_1: 0.5543  loss_dice_1: 4.331  loss_ce_2: 0.4011  loss_mask_2: 0.5609  loss_dice_2: 4.321  loss_ce_3: 0.4076  loss_mask_3: 0.5626  loss_dice_3: 4.313  loss_ce_4: 0.4149  loss_mask_4: 0.5632  loss_dice_4: 4.311  loss_ce_5: 0.3885  loss_mask_5: 0.5627  loss_dice_5: 4.308  loss_ce_6: 0.4113  loss_mask_6: 0.5641  loss_dice_6: 4.305  loss_ce_7: 0.4082  loss_mask_7: 0.5637  loss_dice_7: 4.313  loss_ce_8: 0.4138  loss_mask_8: 0.5659  loss_dice_8: 4.308  time: 1.5406  data_time: 0.0831  lr: 9.7457e-06  max_mem: 21234M
[01/17 04:17:38] d2.utils.events INFO:  eta: 1 day, 13:02:15  iter: 2559  total_loss: 53.07  loss_ce: 0.4021  loss_mask: 0.5518  loss_dice: 4.278  loss_ce_0: 0.9326  loss_mask_0: 0.5367  loss_dice_0: 4.358  loss_ce_1: 0.4266  loss_mask_1: 0.5532  loss_dice_1: 4.303  loss_ce_2: 0.434  loss_mask_2: 0.5511  loss_dice_2: 4.285  loss_ce_3: 0.4138  loss_mask_3: 0.5505  loss_dice_3: 4.274  loss_ce_4: 0.4034  loss_mask_4: 0.5481  loss_dice_4: 4.277  loss_ce_5: 0.414  loss_mask_5: 0.5508  loss_dice_5: 4.272  loss_ce_6: 0.4057  loss_mask_6: 0.5537  loss_dice_6: 4.28  loss_ce_7: 0.4256  loss_mask_7: 0.5538  loss_dice_7: 4.271  loss_ce_8: 0.4203  loss_mask_8: 0.5528  loss_dice_8: 4.282  time: 1.5404  data_time: 0.0966  lr: 9.7437e-06  max_mem: 21234M
[01/17 04:18:09] d2.utils.events INFO:  eta: 1 day, 13:01:53  iter: 2579  total_loss: 53.25  loss_ce: 0.3741  loss_mask: 0.5614  loss_dice: 4.271  loss_ce_0: 0.9336  loss_mask_0: 0.5385  loss_dice_0: 4.349  loss_ce_1: 0.4191  loss_mask_1: 0.5537  loss_dice_1: 4.291  loss_ce_2: 0.4098  loss_mask_2: 0.5596  loss_dice_2: 4.276  loss_ce_3: 0.4057  loss_mask_3: 0.5598  loss_dice_3: 4.272  loss_ce_4: 0.4016  loss_mask_4: 0.5619  loss_dice_4: 4.271  loss_ce_5: 0.3896  loss_mask_5: 0.5645  loss_dice_5: 4.273  loss_ce_6: 0.3822  loss_mask_6: 0.5635  loss_dice_6: 4.272  loss_ce_7: 0.3961  loss_mask_7: 0.5604  loss_dice_7: 4.271  loss_ce_8: 0.3929  loss_mask_8: 0.5621  loss_dice_8: 4.266  time: 1.5401  data_time: 0.0953  lr: 9.7417e-06  max_mem: 21234M
[01/17 04:18:39] d2.utils.events INFO:  eta: 1 day, 13:01:25  iter: 2599  total_loss: 53.32  loss_ce: 0.4165  loss_mask: 0.5642  loss_dice: 4.271  loss_ce_0: 0.9234  loss_mask_0: 0.5349  loss_dice_0: 4.353  loss_ce_1: 0.4212  loss_mask_1: 0.5523  loss_dice_1: 4.306  loss_ce_2: 0.4222  loss_mask_2: 0.5576  loss_dice_2: 4.288  loss_ce_3: 0.415  loss_mask_3: 0.5565  loss_dice_3: 4.275  loss_ce_4: 0.4078  loss_mask_4: 0.5589  loss_dice_4: 4.277  loss_ce_5: 0.4177  loss_mask_5: 0.5607  loss_dice_5: 4.277  loss_ce_6: 0.4144  loss_mask_6: 0.5619  loss_dice_6: 4.281  loss_ce_7: 0.4172  loss_mask_7: 0.5652  loss_dice_7: 4.27  loss_ce_8: 0.4127  loss_mask_8: 0.5616  loss_dice_8: 4.271  time: 1.5399  data_time: 0.0972  lr: 9.7397e-06  max_mem: 21234M
[01/17 04:19:09] d2.utils.events INFO:  eta: 1 day, 12:59:07  iter: 2619  total_loss: 52.84  loss_ce: 0.3831  loss_mask: 0.5692  loss_dice: 4.262  loss_ce_0: 0.8857  loss_mask_0: 0.5436  loss_dice_0: 4.351  loss_ce_1: 0.4178  loss_mask_1: 0.559  loss_dice_1: 4.277  loss_ce_2: 0.4017  loss_mask_2: 0.5657  loss_dice_2: 4.257  loss_ce_3: 0.3978  loss_mask_3: 0.5647  loss_dice_3: 4.253  loss_ce_4: 0.3883  loss_mask_4: 0.564  loss_dice_4: 4.251  loss_ce_5: 0.3933  loss_mask_5: 0.5677  loss_dice_5: 4.255  loss_ce_6: 0.3936  loss_mask_6: 0.5683  loss_dice_6: 4.25  loss_ce_7: 0.3866  loss_mask_7: 0.5682  loss_dice_7: 4.248  loss_ce_8: 0.3876  loss_mask_8: 0.5721  loss_dice_8: 4.251  time: 1.5395  data_time: 0.0732  lr: 9.7377e-06  max_mem: 21234M
[01/17 04:19:39] d2.utils.events INFO:  eta: 1 day, 12:56:04  iter: 2639  total_loss: 52.95  loss_ce: 0.3745  loss_mask: 0.5809  loss_dice: 4.264  loss_ce_0: 0.868  loss_mask_0: 0.5498  loss_dice_0: 4.342  loss_ce_1: 0.4093  loss_mask_1: 0.5635  loss_dice_1: 4.286  loss_ce_2: 0.3887  loss_mask_2: 0.5666  loss_dice_2: 4.266  loss_ce_3: 0.3679  loss_mask_3: 0.5689  loss_dice_3: 4.27  loss_ce_4: 0.3671  loss_mask_4: 0.569  loss_dice_4: 4.263  loss_ce_5: 0.362  loss_mask_5: 0.575  loss_dice_5: 4.27  loss_ce_6: 0.3722  loss_mask_6: 0.579  loss_dice_6: 4.267  loss_ce_7: 0.3775  loss_mask_7: 0.5774  loss_dice_7: 4.266  loss_ce_8: 0.3768  loss_mask_8: 0.5771  loss_dice_8: 4.269  time: 1.5391  data_time: 0.0784  lr: 9.7357e-06  max_mem: 21234M
[01/17 04:20:09] d2.utils.events INFO:  eta: 1 day, 12:56:51  iter: 2659  total_loss: 52.59  loss_ce: 0.3857  loss_mask: 0.5472  loss_dice: 4.267  loss_ce_0: 0.8738  loss_mask_0: 0.5266  loss_dice_0: 4.332  loss_ce_1: 0.4057  loss_mask_1: 0.5398  loss_dice_1: 4.292  loss_ce_2: 0.3863  loss_mask_2: 0.5429  loss_dice_2: 4.274  loss_ce_3: 0.3736  loss_mask_3: 0.5472  loss_dice_3: 4.267  loss_ce_4: 0.3713  loss_mask_4: 0.5493  loss_dice_4: 4.269  loss_ce_5: 0.3781  loss_mask_5: 0.5499  loss_dice_5: 4.266  loss_ce_6: 0.3932  loss_mask_6: 0.5492  loss_dice_6: 4.269  loss_ce_7: 0.3925  loss_mask_7: 0.5495  loss_dice_7: 4.265  loss_ce_8: 0.3876  loss_mask_8: 0.5502  loss_dice_8: 4.266  time: 1.5390  data_time: 0.0853  lr: 9.7337e-06  max_mem: 21234M
[01/17 04:20:39] d2.utils.events INFO:  eta: 1 day, 12:54:31  iter: 2679  total_loss: 53.07  loss_ce: 0.4105  loss_mask: 0.5511  loss_dice: 4.283  loss_ce_0: 0.903  loss_mask_0: 0.5179  loss_dice_0: 4.349  loss_ce_1: 0.4339  loss_mask_1: 0.5411  loss_dice_1: 4.304  loss_ce_2: 0.4195  loss_mask_2: 0.5448  loss_dice_2: 4.287  loss_ce_3: 0.4203  loss_mask_3: 0.548  loss_dice_3: 4.284  loss_ce_4: 0.4203  loss_mask_4: 0.5531  loss_dice_4: 4.279  loss_ce_5: 0.4111  loss_mask_5: 0.5503  loss_dice_5: 4.286  loss_ce_6: 0.4138  loss_mask_6: 0.5533  loss_dice_6: 4.272  loss_ce_7: 0.4148  loss_mask_7: 0.5497  loss_dice_7: 4.279  loss_ce_8: 0.4021  loss_mask_8: 0.548  loss_dice_8: 4.284  time: 1.5388  data_time: 0.0968  lr: 9.7317e-06  max_mem: 21234M
[01/17 04:21:10] d2.utils.events INFO:  eta: 1 day, 12:53:09  iter: 2699  total_loss: 52.64  loss_ce: 0.377  loss_mask: 0.5513  loss_dice: 4.266  loss_ce_0: 0.8598  loss_mask_0: 0.5296  loss_dice_0: 4.336  loss_ce_1: 0.4091  loss_mask_1: 0.5434  loss_dice_1: 4.288  loss_ce_2: 0.3943  loss_mask_2: 0.5452  loss_dice_2: 4.275  loss_ce_3: 0.3839  loss_mask_3: 0.5498  loss_dice_3: 4.266  loss_ce_4: 0.3958  loss_mask_4: 0.5478  loss_dice_4: 4.271  loss_ce_5: 0.3805  loss_mask_5: 0.5494  loss_dice_5: 4.267  loss_ce_6: 0.3838  loss_mask_6: 0.5495  loss_dice_6: 4.267  loss_ce_7: 0.3777  loss_mask_7: 0.5481  loss_dice_7: 4.267  loss_ce_8: 0.3777  loss_mask_8: 0.5497  loss_dice_8: 4.269  time: 1.5387  data_time: 0.0899  lr: 9.7297e-06  max_mem: 21234M
[01/17 04:21:40] d2.utils.events INFO:  eta: 1 day, 12:51:43  iter: 2719  total_loss: 52.5  loss_ce: 0.388  loss_mask: 0.5443  loss_dice: 4.236  loss_ce_0: 0.8805  loss_mask_0: 0.5212  loss_dice_0: 4.306  loss_ce_1: 0.4141  loss_mask_1: 0.5406  loss_dice_1: 4.257  loss_ce_2: 0.4  loss_mask_2: 0.5454  loss_dice_2: 4.24  loss_ce_3: 0.3779  loss_mask_3: 0.5441  loss_dice_3: 4.239  loss_ce_4: 0.389  loss_mask_4: 0.5422  loss_dice_4: 4.24  loss_ce_5: 0.3913  loss_mask_5: 0.5439  loss_dice_5: 4.232  loss_ce_6: 0.4011  loss_mask_6: 0.5432  loss_dice_6: 4.237  loss_ce_7: 0.3991  loss_mask_7: 0.5437  loss_dice_7: 4.233  loss_ce_8: 0.3917  loss_mask_8: 0.5463  loss_dice_8: 4.243  time: 1.5384  data_time: 0.0823  lr: 9.7277e-06  max_mem: 21234M
[01/17 04:22:11] d2.utils.events INFO:  eta: 1 day, 12:51:13  iter: 2739  total_loss: 52.8  loss_ce: 0.4127  loss_mask: 0.5558  loss_dice: 4.256  loss_ce_0: 0.9002  loss_mask_0: 0.5389  loss_dice_0: 4.337  loss_ce_1: 0.4454  loss_mask_1: 0.5515  loss_dice_1: 4.277  loss_ce_2: 0.4267  loss_mask_2: 0.5512  loss_dice_2: 4.256  loss_ce_3: 0.4131  loss_mask_3: 0.5536  loss_dice_3: 4.26  loss_ce_4: 0.4045  loss_mask_4: 0.5548  loss_dice_4: 4.261  loss_ce_5: 0.4028  loss_mask_5: 0.5544  loss_dice_5: 4.254  loss_ce_6: 0.4081  loss_mask_6: 0.5546  loss_dice_6: 4.258  loss_ce_7: 0.4133  loss_mask_7: 0.5553  loss_dice_7: 4.251  loss_ce_8: 0.4104  loss_mask_8: 0.5575  loss_dice_8: 4.259  time: 1.5383  data_time: 0.0905  lr: 9.7257e-06  max_mem: 21234M
[01/17 04:22:41] d2.utils.events INFO:  eta: 1 day, 12:48:54  iter: 2759  total_loss: 52.49  loss_ce: 0.3868  loss_mask: 0.561  loss_dice: 4.233  loss_ce_0: 0.8602  loss_mask_0: 0.535  loss_dice_0: 4.313  loss_ce_1: 0.4146  loss_mask_1: 0.5524  loss_dice_1: 4.247  loss_ce_2: 0.4095  loss_mask_2: 0.5548  loss_dice_2: 4.235  loss_ce_3: 0.4061  loss_mask_3: 0.5547  loss_dice_3: 4.235  loss_ce_4: 0.4093  loss_mask_4: 0.5565  loss_dice_4: 4.231  loss_ce_5: 0.3966  loss_mask_5: 0.5591  loss_dice_5: 4.234  loss_ce_6: 0.3897  loss_mask_6: 0.5595  loss_dice_6: 4.227  loss_ce_7: 0.393  loss_mask_7: 0.5599  loss_dice_7: 4.228  loss_ce_8: 0.3846  loss_mask_8: 0.5576  loss_dice_8: 4.231  time: 1.5382  data_time: 0.0864  lr: 9.7237e-06  max_mem: 21234M
[01/17 04:23:11] d2.utils.events INFO:  eta: 1 day, 12:47:46  iter: 2779  total_loss: 52.71  loss_ce: 0.4084  loss_mask: 0.5639  loss_dice: 4.239  loss_ce_0: 0.8751  loss_mask_0: 0.5421  loss_dice_0: 4.31  loss_ce_1: 0.4295  loss_mask_1: 0.5565  loss_dice_1: 4.258  loss_ce_2: 0.4205  loss_mask_2: 0.562  loss_dice_2: 4.237  loss_ce_3: 0.4158  loss_mask_3: 0.5623  loss_dice_3: 4.228  loss_ce_4: 0.4226  loss_mask_4: 0.5636  loss_dice_4: 4.23  loss_ce_5: 0.4179  loss_mask_5: 0.5646  loss_dice_5: 4.231  loss_ce_6: 0.4191  loss_mask_6: 0.5647  loss_dice_6: 4.23  loss_ce_7: 0.4096  loss_mask_7: 0.5656  loss_dice_7: 4.231  loss_ce_8: 0.4079  loss_mask_8: 0.5661  loss_dice_8: 4.237  time: 1.5380  data_time: 0.0825  lr: 9.7217e-06  max_mem: 21234M
[01/17 04:23:42] d2.utils.events INFO:  eta: 1 day, 12:47:22  iter: 2799  total_loss: 52.03  loss_ce: 0.3674  loss_mask: 0.5471  loss_dice: 4.224  loss_ce_0: 0.8583  loss_mask_0: 0.5183  loss_dice_0: 4.306  loss_ce_1: 0.3955  loss_mask_1: 0.5365  loss_dice_1: 4.254  loss_ce_2: 0.3831  loss_mask_2: 0.5403  loss_dice_2: 4.231  loss_ce_3: 0.3673  loss_mask_3: 0.5432  loss_dice_3: 4.229  loss_ce_4: 0.3658  loss_mask_4: 0.5444  loss_dice_4: 4.224  loss_ce_5: 0.3715  loss_mask_5: 0.5469  loss_dice_5: 4.225  loss_ce_6: 0.3703  loss_mask_6: 0.5479  loss_dice_6: 4.208  loss_ce_7: 0.3791  loss_mask_7: 0.5448  loss_dice_7: 4.217  loss_ce_8: 0.3689  loss_mask_8: 0.5465  loss_dice_8: 4.219  time: 1.5380  data_time: 0.0803  lr: 9.7197e-06  max_mem: 21234M
[01/17 04:24:12] d2.utils.events INFO:  eta: 1 day, 12:48:01  iter: 2819  total_loss: 52.05  loss_ce: 0.3798  loss_mask: 0.5616  loss_dice: 4.202  loss_ce_0: 0.8578  loss_mask_0: 0.5335  loss_dice_0: 4.28  loss_ce_1: 0.4271  loss_mask_1: 0.5523  loss_dice_1: 4.235  loss_ce_2: 0.3928  loss_mask_2: 0.5582  loss_dice_2: 4.221  loss_ce_3: 0.3998  loss_mask_3: 0.5628  loss_dice_3: 4.2  loss_ce_4: 0.3904  loss_mask_4: 0.5613  loss_dice_4: 4.205  loss_ce_5: 0.3795  loss_mask_5: 0.5636  loss_dice_5: 4.196  loss_ce_6: 0.3761  loss_mask_6: 0.5621  loss_dice_6: 4.207  loss_ce_7: 0.3764  loss_mask_7: 0.5647  loss_dice_7: 4.201  loss_ce_8: 0.3633  loss_mask_8: 0.5628  loss_dice_8: 4.198  time: 1.5378  data_time: 0.0906  lr: 9.7177e-06  max_mem: 21234M
[01/17 04:24:42] d2.utils.events INFO:  eta: 1 day, 12:48:28  iter: 2839  total_loss: 51.87  loss_ce: 0.3687  loss_mask: 0.5581  loss_dice: 4.196  loss_ce_0: 0.8629  loss_mask_0: 0.5337  loss_dice_0: 4.283  loss_ce_1: 0.4138  loss_mask_1: 0.5533  loss_dice_1: 4.211  loss_ce_2: 0.3829  loss_mask_2: 0.5565  loss_dice_2: 4.2  loss_ce_3: 0.3763  loss_mask_3: 0.5561  loss_dice_3: 4.197  loss_ce_4: 0.3932  loss_mask_4: 0.5582  loss_dice_4: 4.189  loss_ce_5: 0.3685  loss_mask_5: 0.5591  loss_dice_5: 4.195  loss_ce_6: 0.386  loss_mask_6: 0.5612  loss_dice_6: 4.193  loss_ce_7: 0.3707  loss_mask_7: 0.5599  loss_dice_7: 4.192  loss_ce_8: 0.3783  loss_mask_8: 0.5601  loss_dice_8: 4.197  time: 1.5376  data_time: 0.0861  lr: 9.7156e-06  max_mem: 21234M
[01/17 04:25:13] d2.utils.events INFO:  eta: 1 day, 12:48:23  iter: 2859  total_loss: 52  loss_ce: 0.3918  loss_mask: 0.5575  loss_dice: 4.215  loss_ce_0: 0.8479  loss_mask_0: 0.5313  loss_dice_0: 4.298  loss_ce_1: 0.4181  loss_mask_1: 0.5501  loss_dice_1: 4.243  loss_ce_2: 0.3962  loss_mask_2: 0.5528  loss_dice_2: 4.22  loss_ce_3: 0.394  loss_mask_3: 0.553  loss_dice_3: 4.216  loss_ce_4: 0.3855  loss_mask_4: 0.5556  loss_dice_4: 4.212  loss_ce_5: 0.3856  loss_mask_5: 0.5562  loss_dice_5: 4.212  loss_ce_6: 0.3722  loss_mask_6: 0.5603  loss_dice_6: 4.208  loss_ce_7: 0.3958  loss_mask_7: 0.5595  loss_dice_7: 4.205  loss_ce_8: 0.3846  loss_mask_8: 0.5602  loss_dice_8: 4.206  time: 1.5375  data_time: 0.0852  lr: 9.7136e-06  max_mem: 21234M
[01/17 04:25:44] d2.utils.events INFO:  eta: 1 day, 12:48:24  iter: 2879  total_loss: 52.47  loss_ce: 0.4024  loss_mask: 0.5545  loss_dice: 4.209  loss_ce_0: 0.8607  loss_mask_0: 0.5261  loss_dice_0: 4.293  loss_ce_1: 0.4236  loss_mask_1: 0.5482  loss_dice_1: 4.235  loss_ce_2: 0.4227  loss_mask_2: 0.5508  loss_dice_2: 4.218  loss_ce_3: 0.4069  loss_mask_3: 0.5529  loss_dice_3: 4.212  loss_ce_4: 0.4084  loss_mask_4: 0.5543  loss_dice_4: 4.206  loss_ce_5: 0.4004  loss_mask_5: 0.5538  loss_dice_5: 4.207  loss_ce_6: 0.4091  loss_mask_6: 0.5561  loss_dice_6: 4.213  loss_ce_7: 0.4  loss_mask_7: 0.5567  loss_dice_7: 4.207  loss_ce_8: 0.4111  loss_mask_8: 0.5563  loss_dice_8: 4.21  time: 1.5375  data_time: 0.0904  lr: 9.7116e-06  max_mem: 21234M
[01/17 04:26:15] d2.utils.events INFO:  eta: 1 day, 12:47:43  iter: 2899  total_loss: 51.71  loss_ce: 0.3838  loss_mask: 0.5563  loss_dice: 4.177  loss_ce_0: 0.8212  loss_mask_0: 0.5311  loss_dice_0: 4.263  loss_ce_1: 0.4148  loss_mask_1: 0.5407  loss_dice_1: 4.198  loss_ce_2: 0.3861  loss_mask_2: 0.5437  loss_dice_2: 4.19  loss_ce_3: 0.3879  loss_mask_3: 0.5487  loss_dice_3: 4.184  loss_ce_4: 0.3869  loss_mask_4: 0.5501  loss_dice_4: 4.173  loss_ce_5: 0.3729  loss_mask_5: 0.5571  loss_dice_5: 4.181  loss_ce_6: 0.3776  loss_mask_6: 0.5544  loss_dice_6: 4.184  loss_ce_7: 0.3758  loss_mask_7: 0.556  loss_dice_7: 4.178  loss_ce_8: 0.3768  loss_mask_8: 0.5544  loss_dice_8: 4.172  time: 1.5375  data_time: 0.0943  lr: 9.7096e-06  max_mem: 21234M
[01/17 04:26:45] d2.utils.events INFO:  eta: 1 day, 12:47:42  iter: 2919  total_loss: 51.91  loss_ce: 0.3768  loss_mask: 0.5468  loss_dice: 4.202  loss_ce_0: 0.8556  loss_mask_0: 0.5216  loss_dice_0: 4.303  loss_ce_1: 0.3995  loss_mask_1: 0.5369  loss_dice_1: 4.227  loss_ce_2: 0.3798  loss_mask_2: 0.5448  loss_dice_2: 4.211  loss_ce_3: 0.371  loss_mask_3: 0.5448  loss_dice_3: 4.209  loss_ce_4: 0.3808  loss_mask_4: 0.5435  loss_dice_4: 4.206  loss_ce_5: 0.3763  loss_mask_5: 0.5441  loss_dice_5: 4.213  loss_ce_6: 0.3702  loss_mask_6: 0.5463  loss_dice_6: 4.211  loss_ce_7: 0.3711  loss_mask_7: 0.5482  loss_dice_7: 4.203  loss_ce_8: 0.3754  loss_mask_8: 0.5459  loss_dice_8: 4.209  time: 1.5374  data_time: 0.0907  lr: 9.7076e-06  max_mem: 21234M
[01/17 04:27:16] d2.utils.events INFO:  eta: 1 day, 12:46:52  iter: 2939  total_loss: 52.4  loss_ce: 0.3975  loss_mask: 0.5621  loss_dice: 4.222  loss_ce_0: 0.8494  loss_mask_0: 0.5369  loss_dice_0: 4.292  loss_ce_1: 0.4397  loss_mask_1: 0.5588  loss_dice_1: 4.241  loss_ce_2: 0.4206  loss_mask_2: 0.5604  loss_dice_2: 4.231  loss_ce_3: 0.3928  loss_mask_3: 0.5628  loss_dice_3: 4.22  loss_ce_4: 0.3965  loss_mask_4: 0.5644  loss_dice_4: 4.216  loss_ce_5: 0.3998  loss_mask_5: 0.5677  loss_dice_5: 4.215  loss_ce_6: 0.4027  loss_mask_6: 0.5638  loss_dice_6: 4.221  loss_ce_7: 0.3919  loss_mask_7: 0.5617  loss_dice_7: 4.216  loss_ce_8: 0.3953  loss_mask_8: 0.5624  loss_dice_8: 4.217  time: 1.5372  data_time: 0.0907  lr: 9.7056e-06  max_mem: 21234M
[01/17 04:27:46] d2.utils.events INFO:  eta: 1 day, 12:46:34  iter: 2959  total_loss: 51.93  loss_ce: 0.3678  loss_mask: 0.5399  loss_dice: 4.214  loss_ce_0: 0.8134  loss_mask_0: 0.5191  loss_dice_0: 4.284  loss_ce_1: 0.4123  loss_mask_1: 0.5347  loss_dice_1: 4.227  loss_ce_2: 0.4009  loss_mask_2: 0.536  loss_dice_2: 4.211  loss_ce_3: 0.39  loss_mask_3: 0.541  loss_dice_3: 4.214  loss_ce_4: 0.3865  loss_mask_4: 0.5396  loss_dice_4: 4.21  loss_ce_5: 0.3808  loss_mask_5: 0.5375  loss_dice_5: 4.214  loss_ce_6: 0.3894  loss_mask_6: 0.5395  loss_dice_6: 4.207  loss_ce_7: 0.3836  loss_mask_7: 0.5417  loss_dice_7: 4.209  loss_ce_8: 0.3846  loss_mask_8: 0.5388  loss_dice_8: 4.213  time: 1.5371  data_time: 0.0874  lr: 9.7036e-06  max_mem: 21234M
[01/17 04:28:16] d2.utils.events INFO:  eta: 1 day, 12:44:56  iter: 2979  total_loss: 51.77  loss_ce: 0.3721  loss_mask: 0.5701  loss_dice: 4.172  loss_ce_0: 0.8552  loss_mask_0: 0.5411  loss_dice_0: 4.252  loss_ce_1: 0.4245  loss_mask_1: 0.5628  loss_dice_1: 4.192  loss_ce_2: 0.4073  loss_mask_2: 0.568  loss_dice_2: 4.177  loss_ce_3: 0.3885  loss_mask_3: 0.5674  loss_dice_3: 4.173  loss_ce_4: 0.3778  loss_mask_4: 0.5671  loss_dice_4: 4.172  loss_ce_5: 0.3772  loss_mask_5: 0.5659  loss_dice_5: 4.169  loss_ce_6: 0.3724  loss_mask_6: 0.5654  loss_dice_6: 4.173  loss_ce_7: 0.379  loss_mask_7: 0.5679  loss_dice_7: 4.172  loss_ce_8: 0.3823  loss_mask_8: 0.5692  loss_dice_8: 4.17  time: 1.5369  data_time: 0.0887  lr: 9.7016e-06  max_mem: 21234M
[01/17 04:28:46] d2.utils.events INFO:  eta: 1 day, 12:44:06  iter: 2999  total_loss: 52.1  loss_ce: 0.4047  loss_mask: 0.5624  loss_dice: 4.185  loss_ce_0: 0.7995  loss_mask_0: 0.5347  loss_dice_0: 4.275  loss_ce_1: 0.4391  loss_mask_1: 0.5585  loss_dice_1: 4.208  loss_ce_2: 0.4193  loss_mask_2: 0.5582  loss_dice_2: 4.198  loss_ce_3: 0.4174  loss_mask_3: 0.5625  loss_dice_3: 4.183  loss_ce_4: 0.3984  loss_mask_4: 0.5614  loss_dice_4: 4.184  loss_ce_5: 0.4028  loss_mask_5: 0.5602  loss_dice_5: 4.187  loss_ce_6: 0.4026  loss_mask_6: 0.561  loss_dice_6: 4.181  loss_ce_7: 0.4115  loss_mask_7: 0.5619  loss_dice_7: 4.185  loss_ce_8: 0.4061  loss_mask_8: 0.5634  loss_dice_8: 4.184  time: 1.5367  data_time: 0.1017  lr: 9.6996e-06  max_mem: 21234M
[01/17 04:29:17] d2.utils.events INFO:  eta: 1 day, 12:43:36  iter: 3019  total_loss: 52.18  loss_ce: 0.4063  loss_mask: 0.5645  loss_dice: 4.173  loss_ce_0: 0.8576  loss_mask_0: 0.5357  loss_dice_0: 4.252  loss_ce_1: 0.4491  loss_mask_1: 0.5545  loss_dice_1: 4.195  loss_ce_2: 0.4397  loss_mask_2: 0.5604  loss_dice_2: 4.178  loss_ce_3: 0.4163  loss_mask_3: 0.5629  loss_dice_3: 4.17  loss_ce_4: 0.4342  loss_mask_4: 0.5646  loss_dice_4: 4.174  loss_ce_5: 0.4137  loss_mask_5: 0.5594  loss_dice_5: 4.175  loss_ce_6: 0.4132  loss_mask_6: 0.5583  loss_dice_6: 4.169  loss_ce_7: 0.4201  loss_mask_7: 0.5608  loss_dice_7: 4.164  loss_ce_8: 0.4078  loss_mask_8: 0.5587  loss_dice_8: 4.175  time: 1.5368  data_time: 0.0947  lr: 9.6976e-06  max_mem: 21234M
[01/17 04:29:47] d2.utils.events INFO:  eta: 1 day, 12:42:27  iter: 3039  total_loss: 52.11  loss_ce: 0.4053  loss_mask: 0.558  loss_dice: 4.176  loss_ce_0: 0.8198  loss_mask_0: 0.532  loss_dice_0: 4.261  loss_ce_1: 0.4257  loss_mask_1: 0.5525  loss_dice_1: 4.199  loss_ce_2: 0.4269  loss_mask_2: 0.5551  loss_dice_2: 4.181  loss_ce_3: 0.4312  loss_mask_3: 0.5569  loss_dice_3: 4.18  loss_ce_4: 0.4313  loss_mask_4: 0.5591  loss_dice_4: 4.176  loss_ce_5: 0.4104  loss_mask_5: 0.5575  loss_dice_5: 4.177  loss_ce_6: 0.4125  loss_mask_6: 0.5557  loss_dice_6: 4.178  loss_ce_7: 0.4152  loss_mask_7: 0.5553  loss_dice_7: 4.17  loss_ce_8: 0.3935  loss_mask_8: 0.559  loss_dice_8: 4.173  time: 1.5366  data_time: 0.0927  lr: 9.6956e-06  max_mem: 21234M
[01/17 04:30:19] d2.utils.events INFO:  eta: 1 day, 12:42:35  iter: 3059  total_loss: 51.8  loss_ce: 0.4015  loss_mask: 0.5364  loss_dice: 4.199  loss_ce_0: 0.7951  loss_mask_0: 0.5129  loss_dice_0: 4.291  loss_ce_1: 0.427  loss_mask_1: 0.5341  loss_dice_1: 4.222  loss_ce_2: 0.4108  loss_mask_2: 0.5379  loss_dice_2: 4.203  loss_ce_3: 0.3917  loss_mask_3: 0.539  loss_dice_3: 4.195  loss_ce_4: 0.402  loss_mask_4: 0.5374  loss_dice_4: 4.197  loss_ce_5: 0.3875  loss_mask_5: 0.5364  loss_dice_5: 4.202  loss_ce_6: 0.3789  loss_mask_6: 0.5367  loss_dice_6: 4.19  loss_ce_7: 0.3873  loss_mask_7: 0.5328  loss_dice_7: 4.2  loss_ce_8: 0.3824  loss_mask_8: 0.5348  loss_dice_8: 4.186  time: 1.5367  data_time: 0.0938  lr: 9.6936e-06  max_mem: 21234M
[01/17 04:30:50] d2.utils.events INFO:  eta: 1 day, 12:43:38  iter: 3079  total_loss: 51.55  loss_ce: 0.37  loss_mask: 0.5461  loss_dice: 4.163  loss_ce_0: 0.7981  loss_mask_0: 0.5289  loss_dice_0: 4.254  loss_ce_1: 0.3983  loss_mask_1: 0.5401  loss_dice_1: 4.193  loss_ce_2: 0.3803  loss_mask_2: 0.5456  loss_dice_2: 4.176  loss_ce_3: 0.3783  loss_mask_3: 0.5412  loss_dice_3: 4.167  loss_ce_4: 0.3709  loss_mask_4: 0.545  loss_dice_4: 4.164  loss_ce_5: 0.37  loss_mask_5: 0.5471  loss_dice_5: 4.167  loss_ce_6: 0.3634  loss_mask_6: 0.546  loss_dice_6: 4.164  loss_ce_7: 0.3686  loss_mask_7: 0.5483  loss_dice_7: 4.164  loss_ce_8: 0.3751  loss_mask_8: 0.5487  loss_dice_8: 4.16  time: 1.5369  data_time: 0.0907  lr: 9.6916e-06  max_mem: 21234M
[01/17 04:31:21] d2.utils.events INFO:  eta: 1 day, 12:42:49  iter: 3099  total_loss: 51.67  loss_ce: 0.4139  loss_mask: 0.5595  loss_dice: 4.151  loss_ce_0: 0.8047  loss_mask_0: 0.5436  loss_dice_0: 4.255  loss_ce_1: 0.4131  loss_mask_1: 0.5493  loss_dice_1: 4.186  loss_ce_2: 0.4273  loss_mask_2: 0.5532  loss_dice_2: 4.166  loss_ce_3: 0.4108  loss_mask_3: 0.5567  loss_dice_3: 4.158  loss_ce_4: 0.4289  loss_mask_4: 0.558  loss_dice_4: 4.158  loss_ce_5: 0.4082  loss_mask_5: 0.5579  loss_dice_5: 4.15  loss_ce_6: 0.3935  loss_mask_6: 0.5581  loss_dice_6: 4.156  loss_ce_7: 0.4164  loss_mask_7: 0.5578  loss_dice_7: 4.157  loss_ce_8: 0.4144  loss_mask_8: 0.5573  loss_dice_8: 4.155  time: 1.5370  data_time: 0.0931  lr: 9.6896e-06  max_mem: 21234M
[01/17 04:31:52] d2.utils.events INFO:  eta: 1 day, 12:42:08  iter: 3119  total_loss: 51.67  loss_ce: 0.417  loss_mask: 0.5558  loss_dice: 4.138  loss_ce_0: 0.8305  loss_mask_0: 0.5328  loss_dice_0: 4.235  loss_ce_1: 0.4748  loss_mask_1: 0.5481  loss_dice_1: 4.16  loss_ce_2: 0.4404  loss_mask_2: 0.5526  loss_dice_2: 4.15  loss_ce_3: 0.4397  loss_mask_3: 0.5571  loss_dice_3: 4.137  loss_ce_4: 0.4282  loss_mask_4: 0.559  loss_dice_4: 4.132  loss_ce_5: 0.425  loss_mask_5: 0.5609  loss_dice_5: 4.13  loss_ce_6: 0.423  loss_mask_6: 0.558  loss_dice_6: 4.135  loss_ce_7: 0.4254  loss_mask_7: 0.5598  loss_dice_7: 4.137  loss_ce_8: 0.4219  loss_mask_8: 0.5577  loss_dice_8: 4.133  time: 1.5370  data_time: 0.0897  lr: 9.6876e-06  max_mem: 21234M
[01/17 04:32:23] d2.utils.events INFO:  eta: 1 day, 12:42:10  iter: 3139  total_loss: 51.51  loss_ce: 0.3758  loss_mask: 0.5585  loss_dice: 4.164  loss_ce_0: 0.8267  loss_mask_0: 0.5289  loss_dice_0: 4.233  loss_ce_1: 0.409  loss_mask_1: 0.5421  loss_dice_1: 4.184  loss_ce_2: 0.3889  loss_mask_2: 0.5482  loss_dice_2: 4.163  loss_ce_3: 0.3889  loss_mask_3: 0.55  loss_dice_3: 4.165  loss_ce_4: 0.3842  loss_mask_4: 0.5525  loss_dice_4: 4.156  loss_ce_5: 0.3832  loss_mask_5: 0.5548  loss_dice_5: 4.162  loss_ce_6: 0.3815  loss_mask_6: 0.5567  loss_dice_6: 4.161  loss_ce_7: 0.3883  loss_mask_7: 0.557  loss_dice_7: 4.155  loss_ce_8: 0.3796  loss_mask_8: 0.5586  loss_dice_8: 4.156  time: 1.5371  data_time: 0.0936  lr: 9.6855e-06  max_mem: 21234M
[01/17 04:32:54] d2.utils.events INFO:  eta: 1 day, 12:42:00  iter: 3159  total_loss: 51.51  loss_ce: 0.3856  loss_mask: 0.5496  loss_dice: 4.163  loss_ce_0: 0.7636  loss_mask_0: 0.5259  loss_dice_0: 4.255  loss_ce_1: 0.4224  loss_mask_1: 0.546  loss_dice_1: 4.195  loss_ce_2: 0.3916  loss_mask_2: 0.5495  loss_dice_2: 4.175  loss_ce_3: 0.3694  loss_mask_3: 0.55  loss_dice_3: 4.164  loss_ce_4: 0.3938  loss_mask_4: 0.5486  loss_dice_4: 4.169  loss_ce_5: 0.394  loss_mask_5: 0.5503  loss_dice_5: 4.16  loss_ce_6: 0.3802  loss_mask_6: 0.5492  loss_dice_6: 4.165  loss_ce_7: 0.3822  loss_mask_7: 0.5498  loss_dice_7: 4.16  loss_ce_8: 0.3747  loss_mask_8: 0.5518  loss_dice_8: 4.167  time: 1.5372  data_time: 0.0929  lr: 9.6835e-06  max_mem: 21234M
[01/17 04:33:25] d2.utils.events INFO:  eta: 1 day, 12:42:27  iter: 3179  total_loss: 51.74  loss_ce: 0.3855  loss_mask: 0.5441  loss_dice: 4.18  loss_ce_0: 0.7927  loss_mask_0: 0.5197  loss_dice_0: 4.261  loss_ce_1: 0.434  loss_mask_1: 0.5381  loss_dice_1: 4.201  loss_ce_2: 0.4081  loss_mask_2: 0.5421  loss_dice_2: 4.182  loss_ce_3: 0.4189  loss_mask_3: 0.5454  loss_dice_3: 4.175  loss_ce_4: 0.4081  loss_mask_4: 0.5464  loss_dice_4: 4.173  loss_ce_5: 0.405  loss_mask_5: 0.5461  loss_dice_5: 4.176  loss_ce_6: 0.3947  loss_mask_6: 0.5474  loss_dice_6: 4.181  loss_ce_7: 0.3903  loss_mask_7: 0.5437  loss_dice_7: 4.18  loss_ce_8: 0.3928  loss_mask_8: 0.5462  loss_dice_8: 4.173  time: 1.5373  data_time: 0.1040  lr: 9.6815e-06  max_mem: 21234M
[01/17 04:33:56] d2.utils.events INFO:  eta: 1 day, 12:41:57  iter: 3199  total_loss: 51.46  loss_ce: 0.3807  loss_mask: 0.5526  loss_dice: 4.137  loss_ce_0: 0.7991  loss_mask_0: 0.5271  loss_dice_0: 4.221  loss_ce_1: 0.4232  loss_mask_1: 0.5435  loss_dice_1: 4.155  loss_ce_2: 0.4039  loss_mask_2: 0.5477  loss_dice_2: 4.14  loss_ce_3: 0.3797  loss_mask_3: 0.5477  loss_dice_3: 4.136  loss_ce_4: 0.3892  loss_mask_4: 0.5514  loss_dice_4: 4.139  loss_ce_5: 0.3806  loss_mask_5: 0.5558  loss_dice_5: 4.14  loss_ce_6: 0.382  loss_mask_6: 0.5557  loss_dice_6: 4.149  loss_ce_7: 0.3885  loss_mask_7: 0.5511  loss_dice_7: 4.148  loss_ce_8: 0.3823  loss_mask_8: 0.5517  loss_dice_8: 4.131  time: 1.5373  data_time: 0.0983  lr: 9.6795e-06  max_mem: 21234M
[01/17 04:34:27] d2.utils.events INFO:  eta: 1 day, 12:42:37  iter: 3219  total_loss: 51.52  loss_ce: 0.3905  loss_mask: 0.5408  loss_dice: 4.176  loss_ce_0: 0.7793  loss_mask_0: 0.5136  loss_dice_0: 4.263  loss_ce_1: 0.3897  loss_mask_1: 0.5362  loss_dice_1: 4.202  loss_ce_2: 0.3811  loss_mask_2: 0.5377  loss_dice_2: 4.182  loss_ce_3: 0.3932  loss_mask_3: 0.5388  loss_dice_3: 4.178  loss_ce_4: 0.393  loss_mask_4: 0.541  loss_dice_4: 4.176  loss_ce_5: 0.3992  loss_mask_5: 0.5412  loss_dice_5: 4.174  loss_ce_6: 0.3867  loss_mask_6: 0.5423  loss_dice_6: 4.173  loss_ce_7: 0.3789  loss_mask_7: 0.5425  loss_dice_7: 4.175  loss_ce_8: 0.376  loss_mask_8: 0.5425  loss_dice_8: 4.179  time: 1.5374  data_time: 0.1005  lr: 9.6775e-06  max_mem: 21234M
[01/17 04:34:58] d2.utils.events INFO:  eta: 1 day, 12:41:29  iter: 3239  total_loss: 51.23  loss_ce: 0.3885  loss_mask: 0.5476  loss_dice: 4.122  loss_ce_0: 0.8391  loss_mask_0: 0.5199  loss_dice_0: 4.246  loss_ce_1: 0.4111  loss_mask_1: 0.5348  loss_dice_1: 4.161  loss_ce_2: 0.3975  loss_mask_2: 0.5379  loss_dice_2: 4.141  loss_ce_3: 0.3946  loss_mask_3: 0.542  loss_dice_3: 4.129  loss_ce_4: 0.3943  loss_mask_4: 0.5454  loss_dice_4: 4.13  loss_ce_5: 0.3899  loss_mask_5: 0.5463  loss_dice_5: 4.13  loss_ce_6: 0.3945  loss_mask_6: 0.5451  loss_dice_6: 4.131  loss_ce_7: 0.3872  loss_mask_7: 0.5466  loss_dice_7: 4.125  loss_ce_8: 0.3951  loss_mask_8: 0.5484  loss_dice_8: 4.127  time: 1.5374  data_time: 0.0974  lr: 9.6755e-06  max_mem: 21234M
[01/17 04:35:29] d2.utils.events INFO:  eta: 1 day, 12:41:06  iter: 3259  total_loss: 51.49  loss_ce: 0.3736  loss_mask: 0.5586  loss_dice: 4.133  loss_ce_0: 0.7867  loss_mask_0: 0.5319  loss_dice_0: 4.236  loss_ce_1: 0.4109  loss_mask_1: 0.5557  loss_dice_1: 4.16  loss_ce_2: 0.4227  loss_mask_2: 0.5601  loss_dice_2: 4.144  loss_ce_3: 0.4077  loss_mask_3: 0.5595  loss_dice_3: 4.131  loss_ce_4: 0.3888  loss_mask_4: 0.5599  loss_dice_4: 4.13  loss_ce_5: 0.3886  loss_mask_5: 0.5586  loss_dice_5: 4.14  loss_ce_6: 0.3798  loss_mask_6: 0.56  loss_dice_6: 4.13  loss_ce_7: 0.3895  loss_mask_7: 0.5594  loss_dice_7: 4.13  loss_ce_8: 0.3791  loss_mask_8: 0.558  loss_dice_8: 4.13  time: 1.5375  data_time: 0.0915  lr: 9.6735e-06  max_mem: 21234M
[01/17 04:36:00] d2.utils.events INFO:  eta: 1 day, 12:42:24  iter: 3279  total_loss: 51.62  loss_ce: 0.3888  loss_mask: 0.5367  loss_dice: 4.177  loss_ce_0: 0.7868  loss_mask_0: 0.5218  loss_dice_0: 4.263  loss_ce_1: 0.4231  loss_mask_1: 0.5377  loss_dice_1: 4.201  loss_ce_2: 0.417  loss_mask_2: 0.5413  loss_dice_2: 4.185  loss_ce_3: 0.3951  loss_mask_3: 0.5412  loss_dice_3: 4.172  loss_ce_4: 0.4014  loss_mask_4: 0.5428  loss_dice_4: 4.174  loss_ce_5: 0.3955  loss_mask_5: 0.5368  loss_dice_5: 4.18  loss_ce_6: 0.4004  loss_mask_6: 0.5383  loss_dice_6: 4.172  loss_ce_7: 0.4005  loss_mask_7: 0.5385  loss_dice_7: 4.185  loss_ce_8: 0.3912  loss_mask_8: 0.5375  loss_dice_8: 4.176  time: 1.5376  data_time: 0.0779  lr: 9.6715e-06  max_mem: 21234M
[01/17 04:36:31] d2.utils.events INFO:  eta: 1 day, 12:42:45  iter: 3299  total_loss: 51.29  loss_ce: 0.4  loss_mask: 0.5456  loss_dice: 4.145  loss_ce_0: 0.7728  loss_mask_0: 0.5291  loss_dice_0: 4.228  loss_ce_1: 0.4117  loss_mask_1: 0.5403  loss_dice_1: 4.159  loss_ce_2: 0.4018  loss_mask_2: 0.5414  loss_dice_2: 4.136  loss_ce_3: 0.3998  loss_mask_3: 0.539  loss_dice_3: 4.146  loss_ce_4: 0.3949  loss_mask_4: 0.5435  loss_dice_4: 4.135  loss_ce_5: 0.3951  loss_mask_5: 0.5431  loss_dice_5: 4.138  loss_ce_6: 0.3983  loss_mask_6: 0.542  loss_dice_6: 4.141  loss_ce_7: 0.3984  loss_mask_7: 0.5433  loss_dice_7: 4.147  loss_ce_8: 0.3947  loss_mask_8: 0.5441  loss_dice_8: 4.144  time: 1.5375  data_time: 0.1007  lr: 9.6695e-06  max_mem: 21234M
[01/17 04:37:02] d2.utils.events INFO:  eta: 1 day, 12:43:06  iter: 3319  total_loss: 51.36  loss_ce: 0.3897  loss_mask: 0.5491  loss_dice: 4.144  loss_ce_0: 0.7852  loss_mask_0: 0.523  loss_dice_0: 4.227  loss_ce_1: 0.4449  loss_mask_1: 0.5401  loss_dice_1: 4.165  loss_ce_2: 0.4095  loss_mask_2: 0.5443  loss_dice_2: 4.149  loss_ce_3: 0.4034  loss_mask_3: 0.5464  loss_dice_3: 4.147  loss_ce_4: 0.4054  loss_mask_4: 0.5442  loss_dice_4: 4.147  loss_ce_5: 0.3963  loss_mask_5: 0.5487  loss_dice_5: 4.145  loss_ce_6: 0.3982  loss_mask_6: 0.5486  loss_dice_6: 4.145  loss_ce_7: 0.3949  loss_mask_7: 0.5484  loss_dice_7: 4.141  loss_ce_8: 0.4025  loss_mask_8: 0.5503  loss_dice_8: 4.141  time: 1.5375  data_time: 0.0844  lr: 9.6675e-06  max_mem: 21234M
[01/17 04:37:32] d2.utils.events INFO:  eta: 1 day, 12:42:23  iter: 3339  total_loss: 50.84  loss_ce: 0.3666  loss_mask: 0.5606  loss_dice: 4.092  loss_ce_0: 0.7807  loss_mask_0: 0.5316  loss_dice_0: 4.181  loss_ce_1: 0.414  loss_mask_1: 0.5516  loss_dice_1: 4.113  loss_ce_2: 0.3862  loss_mask_2: 0.5563  loss_dice_2: 4.096  loss_ce_3: 0.3775  loss_mask_3: 0.557  loss_dice_3: 4.09  loss_ce_4: 0.3864  loss_mask_4: 0.5604  loss_dice_4: 4.093  loss_ce_5: 0.3815  loss_mask_5: 0.5607  loss_dice_5: 4.091  loss_ce_6: 0.3736  loss_mask_6: 0.5599  loss_dice_6: 4.085  loss_ce_7: 0.3758  loss_mask_7: 0.5632  loss_dice_7: 4.089  loss_ce_8: 0.3713  loss_mask_8: 0.5608  loss_dice_8: 4.089  time: 1.5374  data_time: 0.0871  lr: 9.6655e-06  max_mem: 21234M
[01/17 04:38:03] d2.utils.events INFO:  eta: 1 day, 12:42:34  iter: 3359  total_loss: 51.1  loss_ce: 0.4014  loss_mask: 0.555  loss_dice: 4.111  loss_ce_0: 0.757  loss_mask_0: 0.5327  loss_dice_0: 4.209  loss_ce_1: 0.4064  loss_mask_1: 0.5499  loss_dice_1: 4.143  loss_ce_2: 0.4084  loss_mask_2: 0.551  loss_dice_2: 4.121  loss_ce_3: 0.4039  loss_mask_3: 0.5503  loss_dice_3: 4.12  loss_ce_4: 0.4013  loss_mask_4: 0.5522  loss_dice_4: 4.118  loss_ce_5: 0.4024  loss_mask_5: 0.5519  loss_dice_5: 4.118  loss_ce_6: 0.3962  loss_mask_6: 0.5536  loss_dice_6: 4.116  loss_ce_7: 0.3993  loss_mask_7: 0.5551  loss_dice_7: 4.115  loss_ce_8: 0.3983  loss_mask_8: 0.553  loss_dice_8: 4.118  time: 1.5374  data_time: 0.0841  lr: 9.6635e-06  max_mem: 21234M
[01/17 04:38:34] d2.utils.events INFO:  eta: 1 day, 12:42:12  iter: 3379  total_loss: 51.21  loss_ce: 0.4113  loss_mask: 0.5455  loss_dice: 4.146  loss_ce_0: 0.7677  loss_mask_0: 0.5244  loss_dice_0: 4.225  loss_ce_1: 0.4145  loss_mask_1: 0.5433  loss_dice_1: 4.157  loss_ce_2: 0.4036  loss_mask_2: 0.5427  loss_dice_2: 4.15  loss_ce_3: 0.3923  loss_mask_3: 0.5454  loss_dice_3: 4.144  loss_ce_4: 0.3909  loss_mask_4: 0.5471  loss_dice_4: 4.134  loss_ce_5: 0.3706  loss_mask_5: 0.546  loss_dice_5: 4.144  loss_ce_6: 0.3816  loss_mask_6: 0.5454  loss_dice_6: 4.138  loss_ce_7: 0.3892  loss_mask_7: 0.545  loss_dice_7: 4.138  loss_ce_8: 0.3876  loss_mask_8: 0.5502  loss_dice_8: 4.137  time: 1.5373  data_time: 0.0916  lr: 9.6615e-06  max_mem: 21234M
[01/17 04:39:04] d2.utils.events INFO:  eta: 1 day, 12:43:09  iter: 3399  total_loss: 51.44  loss_ce: 0.42  loss_mask: 0.552  loss_dice: 4.144  loss_ce_0: 0.7922  loss_mask_0: 0.536  loss_dice_0: 4.228  loss_ce_1: 0.4278  loss_mask_1: 0.5512  loss_dice_1: 4.166  loss_ce_2: 0.4155  loss_mask_2: 0.5538  loss_dice_2: 4.147  loss_ce_3: 0.4316  loss_mask_3: 0.5573  loss_dice_3: 4.134  loss_ce_4: 0.4299  loss_mask_4: 0.5561  loss_dice_4: 4.135  loss_ce_5: 0.4214  loss_mask_5: 0.5551  loss_dice_5: 4.133  loss_ce_6: 0.4229  loss_mask_6: 0.5519  loss_dice_6: 4.142  loss_ce_7: 0.4179  loss_mask_7: 0.551  loss_dice_7: 4.141  loss_ce_8: 0.4191  loss_mask_8: 0.5516  loss_dice_8: 4.138  time: 1.5373  data_time: 0.0905  lr: 9.6594e-06  max_mem: 21234M
[01/17 04:39:35] d2.utils.events INFO:  eta: 1 day, 12:42:38  iter: 3419  total_loss: 51.14  loss_ce: 0.3685  loss_mask: 0.5546  loss_dice: 4.14  loss_ce_0: 0.7526  loss_mask_0: 0.5234  loss_dice_0: 4.215  loss_ce_1: 0.4033  loss_mask_1: 0.5454  loss_dice_1: 4.165  loss_ce_2: 0.3927  loss_mask_2: 0.5494  loss_dice_2: 4.14  loss_ce_3: 0.3925  loss_mask_3: 0.5518  loss_dice_3: 4.133  loss_ce_4: 0.3875  loss_mask_4: 0.5525  loss_dice_4: 4.129  loss_ce_5: 0.3832  loss_mask_5: 0.5534  loss_dice_5: 4.129  loss_ce_6: 0.3846  loss_mask_6: 0.5546  loss_dice_6: 4.137  loss_ce_7: 0.3758  loss_mask_7: 0.5522  loss_dice_7: 4.132  loss_ce_8: 0.38  loss_mask_8: 0.5548  loss_dice_8: 4.138  time: 1.5372  data_time: 0.0953  lr: 9.6574e-06  max_mem: 21234M
[01/17 04:40:05] d2.utils.events INFO:  eta: 1 day, 12:42:19  iter: 3439  total_loss: 51.28  loss_ce: 0.3833  loss_mask: 0.5649  loss_dice: 4.125  loss_ce_0: 0.7707  loss_mask_0: 0.5354  loss_dice_0: 4.204  loss_ce_1: 0.4295  loss_mask_1: 0.5614  loss_dice_1: 4.154  loss_ce_2: 0.4084  loss_mask_2: 0.5635  loss_dice_2: 4.134  loss_ce_3: 0.4002  loss_mask_3: 0.564  loss_dice_3: 4.131  loss_ce_4: 0.397  loss_mask_4: 0.5643  loss_dice_4: 4.123  loss_ce_5: 0.3971  loss_mask_5: 0.5629  loss_dice_5: 4.127  loss_ce_6: 0.3962  loss_mask_6: 0.5676  loss_dice_6: 4.13  loss_ce_7: 0.3715  loss_mask_7: 0.5665  loss_dice_7: 4.122  loss_ce_8: 0.3871  loss_mask_8: 0.5675  loss_dice_8: 4.128  time: 1.5372  data_time: 0.0889  lr: 9.6554e-06  max_mem: 21234M
[01/17 04:40:36] d2.utils.events INFO:  eta: 1 day, 12:42:42  iter: 3459  total_loss: 51.15  loss_ce: 0.3782  loss_mask: 0.5513  loss_dice: 4.126  loss_ce_0: 0.7781  loss_mask_0: 0.5315  loss_dice_0: 4.224  loss_ce_1: 0.4288  loss_mask_1: 0.5416  loss_dice_1: 4.156  loss_ce_2: 0.4157  loss_mask_2: 0.5466  loss_dice_2: 4.134  loss_ce_3: 0.4059  loss_mask_3: 0.5499  loss_dice_3: 4.14  loss_ce_4: 0.4012  loss_mask_4: 0.5498  loss_dice_4: 4.13  loss_ce_5: 0.3891  loss_mask_5: 0.5519  loss_dice_5: 4.131  loss_ce_6: 0.3924  loss_mask_6: 0.5525  loss_dice_6: 4.132  loss_ce_7: 0.3898  loss_mask_7: 0.553  loss_dice_7: 4.131  loss_ce_8: 0.3954  loss_mask_8: 0.5512  loss_dice_8: 4.133  time: 1.5372  data_time: 0.0872  lr: 9.6534e-06  max_mem: 21234M
[01/17 04:41:06] d2.utils.events INFO:  eta: 1 day, 12:40:38  iter: 3479  total_loss: 50.82  loss_ce: 0.3674  loss_mask: 0.5621  loss_dice: 4.104  loss_ce_0: 0.7263  loss_mask_0: 0.5308  loss_dice_0: 4.198  loss_ce_1: 0.3859  loss_mask_1: 0.5555  loss_dice_1: 4.117  loss_ce_2: 0.3893  loss_mask_2: 0.5565  loss_dice_2: 4.1  loss_ce_3: 0.3853  loss_mask_3: 0.5571  loss_dice_3: 4.102  loss_ce_4: 0.3748  loss_mask_4: 0.5603  loss_dice_4: 4.1  loss_ce_5: 0.3826  loss_mask_5: 0.5625  loss_dice_5: 4.1  loss_ce_6: 0.3679  loss_mask_6: 0.5609  loss_dice_6: 4.104  loss_ce_7: 0.3767  loss_mask_7: 0.5604  loss_dice_7: 4.102  loss_ce_8: 0.3742  loss_mask_8: 0.5612  loss_dice_8: 4.103  time: 1.5370  data_time: 0.0815  lr: 9.6514e-06  max_mem: 21234M
[01/17 04:41:37] d2.utils.events INFO:  eta: 1 day, 12:40:29  iter: 3499  total_loss: 50.78  loss_ce: 0.37  loss_mask: 0.5554  loss_dice: 4.07  loss_ce_0: 0.7621  loss_mask_0: 0.5282  loss_dice_0: 4.156  loss_ce_1: 0.4005  loss_mask_1: 0.5475  loss_dice_1: 4.089  loss_ce_2: 0.3873  loss_mask_2: 0.551  loss_dice_2: 4.068  loss_ce_3: 0.3834  loss_mask_3: 0.5504  loss_dice_3: 4.066  loss_ce_4: 0.3959  loss_mask_4: 0.5526  loss_dice_4: 4.06  loss_ce_5: 0.3717  loss_mask_5: 0.5533  loss_dice_5: 4.069  loss_ce_6: 0.3686  loss_mask_6: 0.5516  loss_dice_6: 4.07  loss_ce_7: 0.3731  loss_mask_7: 0.5538  loss_dice_7: 4.058  loss_ce_8: 0.3698  loss_mask_8: 0.5558  loss_dice_8: 4.069  time: 1.5370  data_time: 0.0885  lr: 9.6494e-06  max_mem: 21234M
[01/17 04:42:08] d2.utils.events INFO:  eta: 1 day, 12:40:17  iter: 3519  total_loss: 51.08  loss_ce: 0.3959  loss_mask: 0.5626  loss_dice: 4.114  loss_ce_0: 0.7846  loss_mask_0: 0.5301  loss_dice_0: 4.187  loss_ce_1: 0.4467  loss_mask_1: 0.5508  loss_dice_1: 4.123  loss_ce_2: 0.4136  loss_mask_2: 0.5541  loss_dice_2: 4.115  loss_ce_3: 0.4017  loss_mask_3: 0.5547  loss_dice_3: 4.112  loss_ce_4: 0.4024  loss_mask_4: 0.5556  loss_dice_4: 4.112  loss_ce_5: 0.4192  loss_mask_5: 0.5604  loss_dice_5: 4.114  loss_ce_6: 0.4077  loss_mask_6: 0.5595  loss_dice_6: 4.113  loss_ce_7: 0.4161  loss_mask_7: 0.5589  loss_dice_7: 4.108  loss_ce_8: 0.3971  loss_mask_8: 0.5609  loss_dice_8: 4.117  time: 1.5370  data_time: 0.0857  lr: 9.6474e-06  max_mem: 21234M
[01/17 04:42:38] d2.utils.events INFO:  eta: 1 day, 12:40:46  iter: 3539  total_loss: 51.07  loss_ce: 0.404  loss_mask: 0.5473  loss_dice: 4.115  loss_ce_0: 0.7322  loss_mask_0: 0.5231  loss_dice_0: 4.213  loss_ce_1: 0.4297  loss_mask_1: 0.5401  loss_dice_1: 4.135  loss_ce_2: 0.404  loss_mask_2: 0.547  loss_dice_2: 4.127  loss_ce_3: 0.4116  loss_mask_3: 0.5486  loss_dice_3: 4.114  loss_ce_4: 0.4036  loss_mask_4: 0.5475  loss_dice_4: 4.114  loss_ce_5: 0.4033  loss_mask_5: 0.5481  loss_dice_5: 4.117  loss_ce_6: 0.4031  loss_mask_6: 0.5484  loss_dice_6: 4.116  loss_ce_7: 0.3962  loss_mask_7: 0.5481  loss_dice_7: 4.117  loss_ce_8: 0.4087  loss_mask_8: 0.5476  loss_dice_8: 4.113  time: 1.5369  data_time: 0.0986  lr: 9.6454e-06  max_mem: 21234M
[01/17 04:43:09] d2.utils.events INFO:  eta: 1 day, 12:41:14  iter: 3559  total_loss: 50.99  loss_ce: 0.3994  loss_mask: 0.5402  loss_dice: 4.11  loss_ce_0: 0.7424  loss_mask_0: 0.5223  loss_dice_0: 4.194  loss_ce_1: 0.4047  loss_mask_1: 0.5394  loss_dice_1: 4.125  loss_ce_2: 0.3933  loss_mask_2: 0.5357  loss_dice_2: 4.117  loss_ce_3: 0.4065  loss_mask_3: 0.5384  loss_dice_3: 4.111  loss_ce_4: 0.4105  loss_mask_4: 0.5406  loss_dice_4: 4.108  loss_ce_5: 0.3912  loss_mask_5: 0.5444  loss_dice_5: 4.099  loss_ce_6: 0.3932  loss_mask_6: 0.5447  loss_dice_6: 4.105  loss_ce_7: 0.3868  loss_mask_7: 0.5424  loss_dice_7: 4.105  loss_ce_8: 0.3964  loss_mask_8: 0.5406  loss_dice_8: 4.101  time: 1.5369  data_time: 0.0997  lr: 9.6434e-06  max_mem: 21234M
[01/17 04:43:40] d2.utils.events INFO:  eta: 1 day, 12:40:24  iter: 3579  total_loss: 50.7  loss_ce: 0.3713  loss_mask: 0.5473  loss_dice: 4.08  loss_ce_0: 0.7261  loss_mask_0: 0.5262  loss_dice_0: 4.174  loss_ce_1: 0.4299  loss_mask_1: 0.5433  loss_dice_1: 4.11  loss_ce_2: 0.3996  loss_mask_2: 0.5421  loss_dice_2: 4.096  loss_ce_3: 0.3821  loss_mask_3: 0.5487  loss_dice_3: 4.077  loss_ce_4: 0.3905  loss_mask_4: 0.5468  loss_dice_4: 4.088  loss_ce_5: 0.3853  loss_mask_5: 0.549  loss_dice_5: 4.089  loss_ce_6: 0.3719  loss_mask_6: 0.5492  loss_dice_6: 4.079  loss_ce_7: 0.3728  loss_mask_7: 0.5485  loss_dice_7: 4.077  loss_ce_8: 0.3729  loss_mask_8: 0.5479  loss_dice_8: 4.085  time: 1.5368  data_time: 0.0969  lr: 9.6414e-06  max_mem: 21234M
[01/17 04:44:10] d2.utils.events INFO:  eta: 1 day, 12:42:17  iter: 3599  total_loss: 50.85  loss_ce: 0.3819  loss_mask: 0.5566  loss_dice: 4.103  loss_ce_0: 0.7248  loss_mask_0: 0.5306  loss_dice_0: 4.193  loss_ce_1: 0.4011  loss_mask_1: 0.5515  loss_dice_1: 4.116  loss_ce_2: 0.3853  loss_mask_2: 0.553  loss_dice_2: 4.108  loss_ce_3: 0.3919  loss_mask_3: 0.5548  loss_dice_3: 4.099  loss_ce_4: 0.393  loss_mask_4: 0.5559  loss_dice_4: 4.099  loss_ce_5: 0.3908  loss_mask_5: 0.5576  loss_dice_5: 4.104  loss_ce_6: 0.3842  loss_mask_6: 0.5568  loss_dice_6: 4.105  loss_ce_7: 0.3906  loss_mask_7: 0.5536  loss_dice_7: 4.097  loss_ce_8: 0.393  loss_mask_8: 0.5549  loss_dice_8: 4.088  time: 1.5368  data_time: 0.0886  lr: 9.6394e-06  max_mem: 21234M
[01/17 04:44:41] d2.utils.events INFO:  eta: 1 day, 12:42:50  iter: 3619  total_loss: 50.82  loss_ce: 0.3715  loss_mask: 0.5445  loss_dice: 4.096  loss_ce_0: 0.7386  loss_mask_0: 0.5315  loss_dice_0: 4.171  loss_ce_1: 0.425  loss_mask_1: 0.5469  loss_dice_1: 4.119  loss_ce_2: 0.3879  loss_mask_2: 0.5535  loss_dice_2: 4.104  loss_ce_3: 0.3787  loss_mask_3: 0.5505  loss_dice_3: 4.095  loss_ce_4: 0.3719  loss_mask_4: 0.5491  loss_dice_4: 4.097  loss_ce_5: 0.3801  loss_mask_5: 0.5479  loss_dice_5: 4.096  loss_ce_6: 0.3802  loss_mask_6: 0.5468  loss_dice_6: 4.1  loss_ce_7: 0.3734  loss_mask_7: 0.5459  loss_dice_7: 4.101  loss_ce_8: 0.3694  loss_mask_8: 0.5475  loss_dice_8: 4.095  time: 1.5369  data_time: 0.0938  lr: 9.6374e-06  max_mem: 21234M
[01/17 04:45:12] d2.utils.events INFO:  eta: 1 day, 12:43:53  iter: 3639  total_loss: 50.4  loss_ce: 0.3731  loss_mask: 0.5544  loss_dice: 4.074  loss_ce_0: 0.7343  loss_mask_0: 0.524  loss_dice_0: 4.17  loss_ce_1: 0.4016  loss_mask_1: 0.5473  loss_dice_1: 4.1  loss_ce_2: 0.372  loss_mask_2: 0.5536  loss_dice_2: 4.082  loss_ce_3: 0.3962  loss_mask_3: 0.5538  loss_dice_3: 4.077  loss_ce_4: 0.3766  loss_mask_4: 0.5547  loss_dice_4: 4.075  loss_ce_5: 0.373  loss_mask_5: 0.5537  loss_dice_5: 4.078  loss_ce_6: 0.37  loss_mask_6: 0.5542  loss_dice_6: 4.079  loss_ce_7: 0.3706  loss_mask_7: 0.553  loss_dice_7: 4.075  loss_ce_8: 0.3805  loss_mask_8: 0.5538  loss_dice_8: 4.07  time: 1.5369  data_time: 0.0839  lr: 9.6354e-06  max_mem: 21234M
[01/17 04:45:43] d2.utils.events INFO:  eta: 1 day, 12:42:25  iter: 3659  total_loss: 50.56  loss_ce: 0.3768  loss_mask: 0.5559  loss_dice: 4.071  loss_ce_0: 0.7557  loss_mask_0: 0.5254  loss_dice_0: 4.166  loss_ce_1: 0.4159  loss_mask_1: 0.5505  loss_dice_1: 4.097  loss_ce_2: 0.4018  loss_mask_2: 0.5539  loss_dice_2: 4.081  loss_ce_3: 0.3984  loss_mask_3: 0.5537  loss_dice_3: 4.077  loss_ce_4: 0.3873  loss_mask_4: 0.5554  loss_dice_4: 4.076  loss_ce_5: 0.397  loss_mask_5: 0.5589  loss_dice_5: 4.076  loss_ce_6: 0.3815  loss_mask_6: 0.5607  loss_dice_6: 4.07  loss_ce_7: 0.3847  loss_mask_7: 0.5569  loss_dice_7: 4.07  loss_ce_8: 0.3885  loss_mask_8: 0.5564  loss_dice_8: 4.068  time: 1.5368  data_time: 0.0962  lr: 9.6333e-06  max_mem: 21234M
[01/17 04:46:13] d2.utils.events INFO:  eta: 1 day, 12:43:56  iter: 3679  total_loss: 51.11  loss_ce: 0.3901  loss_mask: 0.5665  loss_dice: 4.096  loss_ce_0: 0.7504  loss_mask_0: 0.5426  loss_dice_0: 4.186  loss_ce_1: 0.4502  loss_mask_1: 0.5613  loss_dice_1: 4.119  loss_ce_2: 0.4163  loss_mask_2: 0.5654  loss_dice_2: 4.102  loss_ce_3: 0.4126  loss_mask_3: 0.5689  loss_dice_3: 4.099  loss_ce_4: 0.4111  loss_mask_4: 0.5663  loss_dice_4: 4.101  loss_ce_5: 0.4062  loss_mask_5: 0.5672  loss_dice_5: 4.102  loss_ce_6: 0.3992  loss_mask_6: 0.567  loss_dice_6: 4.097  loss_ce_7: 0.3971  loss_mask_7: 0.57  loss_dice_7: 4.095  loss_ce_8: 0.4013  loss_mask_8: 0.5661  loss_dice_8: 4.104  time: 1.5367  data_time: 0.0893  lr: 9.6313e-06  max_mem: 21234M
[01/17 04:46:44] d2.utils.events INFO:  eta: 1 day, 12:43:21  iter: 3699  total_loss: 50.74  loss_ce: 0.4025  loss_mask: 0.5474  loss_dice: 4.071  loss_ce_0: 0.7509  loss_mask_0: 0.5206  loss_dice_0: 4.164  loss_ce_1: 0.4176  loss_mask_1: 0.5389  loss_dice_1: 4.106  loss_ce_2: 0.4042  loss_mask_2: 0.5424  loss_dice_2: 4.08  loss_ce_3: 0.4002  loss_mask_3: 0.5425  loss_dice_3: 4.072  loss_ce_4: 0.4025  loss_mask_4: 0.5434  loss_dice_4: 4.072  loss_ce_5: 0.4011  loss_mask_5: 0.5442  loss_dice_5: 4.074  loss_ce_6: 0.4011  loss_mask_6: 0.5428  loss_dice_6: 4.075  loss_ce_7: 0.4064  loss_mask_7: 0.5448  loss_dice_7: 4.075  loss_ce_8: 0.3983  loss_mask_8: 0.5462  loss_dice_8: 4.08  time: 1.5367  data_time: 0.0862  lr: 9.6293e-06  max_mem: 21234M
[01/17 04:47:14] d2.utils.events INFO:  eta: 1 day, 12:42:57  iter: 3719  total_loss: 50.7  loss_ce: 0.4067  loss_mask: 0.5407  loss_dice: 4.1  loss_ce_0: 0.7318  loss_mask_0: 0.5179  loss_dice_0: 4.193  loss_ce_1: 0.4346  loss_mask_1: 0.5337  loss_dice_1: 4.132  loss_ce_2: 0.3969  loss_mask_2: 0.5357  loss_dice_2: 4.104  loss_ce_3: 0.4007  loss_mask_3: 0.537  loss_dice_3: 4.098  loss_ce_4: 0.3896  loss_mask_4: 0.538  loss_dice_4: 4.095  loss_ce_5: 0.3993  loss_mask_5: 0.5408  loss_dice_5: 4.099  loss_ce_6: 0.3988  loss_mask_6: 0.543  loss_dice_6: 4.096  loss_ce_7: 0.3949  loss_mask_7: 0.5415  loss_dice_7: 4.097  loss_ce_8: 0.4082  loss_mask_8: 0.5418  loss_dice_8: 4.095  time: 1.5366  data_time: 0.0950  lr: 9.6273e-06  max_mem: 21234M
[01/17 04:47:45] d2.utils.events INFO:  eta: 1 day, 12:42:24  iter: 3739  total_loss: 50.7  loss_ce: 0.3827  loss_mask: 0.5501  loss_dice: 4.079  loss_ce_0: 0.7146  loss_mask_0: 0.5237  loss_dice_0: 4.165  loss_ce_1: 0.389  loss_mask_1: 0.5403  loss_dice_1: 4.104  loss_ce_2: 0.397  loss_mask_2: 0.5468  loss_dice_2: 4.082  loss_ce_3: 0.392  loss_mask_3: 0.5503  loss_dice_3: 4.073  loss_ce_4: 0.3621  loss_mask_4: 0.5509  loss_dice_4: 4.075  loss_ce_5: 0.3631  loss_mask_5: 0.5524  loss_dice_5: 4.079  loss_ce_6: 0.3734  loss_mask_6: 0.5496  loss_dice_6: 4.08  loss_ce_7: 0.388  loss_mask_7: 0.5499  loss_dice_7: 4.085  loss_ce_8: 0.3915  loss_mask_8: 0.55  loss_dice_8: 4.078  time: 1.5366  data_time: 0.0922  lr: 9.6253e-06  max_mem: 21234M
[01/17 04:48:16] d2.utils.events INFO:  eta: 1 day, 12:41:56  iter: 3759  total_loss: 50.53  loss_ce: 0.3712  loss_mask: 0.5579  loss_dice: 4.069  loss_ce_0: 0.7512  loss_mask_0: 0.5289  loss_dice_0: 4.169  loss_ce_1: 0.3942  loss_mask_1: 0.5527  loss_dice_1: 4.104  loss_ce_2: 0.4115  loss_mask_2: 0.5534  loss_dice_2: 4.09  loss_ce_3: 0.3883  loss_mask_3: 0.5516  loss_dice_3: 4.084  loss_ce_4: 0.3871  loss_mask_4: 0.5544  loss_dice_4: 4.075  loss_ce_5: 0.3794  loss_mask_5: 0.5563  loss_dice_5: 4.079  loss_ce_6: 0.367  loss_mask_6: 0.5578  loss_dice_6: 4.067  loss_ce_7: 0.3833  loss_mask_7: 0.5559  loss_dice_7: 4.063  loss_ce_8: 0.3762  loss_mask_8: 0.5575  loss_dice_8: 4.082  time: 1.5366  data_time: 0.0987  lr: 9.6233e-06  max_mem: 21234M
[01/17 04:48:46] d2.utils.events INFO:  eta: 1 day, 12:41:23  iter: 3779  total_loss: 50.77  loss_ce: 0.3711  loss_mask: 0.5568  loss_dice: 4.077  loss_ce_0: 0.7097  loss_mask_0: 0.5396  loss_dice_0: 4.158  loss_ce_1: 0.4206  loss_mask_1: 0.5547  loss_dice_1: 4.104  loss_ce_2: 0.4206  loss_mask_2: 0.5544  loss_dice_2: 4.083  loss_ce_3: 0.3905  loss_mask_3: 0.5569  loss_dice_3: 4.067  loss_ce_4: 0.3991  loss_mask_4: 0.5561  loss_dice_4: 4.074  loss_ce_5: 0.3935  loss_mask_5: 0.5569  loss_dice_5: 4.077  loss_ce_6: 0.3792  loss_mask_6: 0.5563  loss_dice_6: 4.079  loss_ce_7: 0.383  loss_mask_7: 0.5534  loss_dice_7: 4.079  loss_ce_8: 0.3685  loss_mask_8: 0.5559  loss_dice_8: 4.085  time: 1.5365  data_time: 0.0895  lr: 9.6213e-06  max_mem: 21234M
[01/17 04:49:17] d2.utils.events INFO:  eta: 1 day, 12:40:52  iter: 3799  total_loss: 50.32  loss_ce: 0.3782  loss_mask: 0.5398  loss_dice: 4.056  loss_ce_0: 0.7224  loss_mask_0: 0.5112  loss_dice_0: 4.169  loss_ce_1: 0.4373  loss_mask_1: 0.5348  loss_dice_1: 4.089  loss_ce_2: 0.4009  loss_mask_2: 0.5398  loss_dice_2: 4.083  loss_ce_3: 0.3849  loss_mask_3: 0.5391  loss_dice_3: 4.066  loss_ce_4: 0.3912  loss_mask_4: 0.5383  loss_dice_4: 4.064  loss_ce_5: 0.3755  loss_mask_5: 0.5404  loss_dice_5: 4.06  loss_ce_6: 0.376  loss_mask_6: 0.5386  loss_dice_6: 4.062  loss_ce_7: 0.3889  loss_mask_7: 0.5417  loss_dice_7: 4.06  loss_ce_8: 0.3723  loss_mask_8: 0.5403  loss_dice_8: 4.061  time: 1.5365  data_time: 0.0823  lr: 9.6193e-06  max_mem: 21234M
[01/17 04:49:48] d2.utils.events INFO:  eta: 1 day, 12:40:27  iter: 3819  total_loss: 50.16  loss_ce: 0.354  loss_mask: 0.5496  loss_dice: 4.048  loss_ce_0: 0.7304  loss_mask_0: 0.5235  loss_dice_0: 4.154  loss_ce_1: 0.3955  loss_mask_1: 0.5394  loss_dice_1: 4.087  loss_ce_2: 0.3827  loss_mask_2: 0.5439  loss_dice_2: 4.068  loss_ce_3: 0.3707  loss_mask_3: 0.55  loss_dice_3: 4.057  loss_ce_4: 0.3652  loss_mask_4: 0.5503  loss_dice_4: 4.05  loss_ce_5: 0.3596  loss_mask_5: 0.5502  loss_dice_5: 4.054  loss_ce_6: 0.3594  loss_mask_6: 0.5491  loss_dice_6: 4.061  loss_ce_7: 0.363  loss_mask_7: 0.5502  loss_dice_7: 4.051  loss_ce_8: 0.3557  loss_mask_8: 0.5466  loss_dice_8: 4.052  time: 1.5364  data_time: 0.1028  lr: 9.6173e-06  max_mem: 21234M
[01/17 04:50:18] d2.utils.events INFO:  eta: 1 day, 12:40:08  iter: 3839  total_loss: 50.39  loss_ce: 0.3801  loss_mask: 0.5647  loss_dice: 4.039  loss_ce_0: 0.7283  loss_mask_0: 0.5282  loss_dice_0: 4.137  loss_ce_1: 0.4231  loss_mask_1: 0.5534  loss_dice_1: 4.074  loss_ce_2: 0.4149  loss_mask_2: 0.5588  loss_dice_2: 4.052  loss_ce_3: 0.3963  loss_mask_3: 0.5594  loss_dice_3: 4.044  loss_ce_4: 0.4078  loss_mask_4: 0.56  loss_dice_4: 4.041  loss_ce_5: 0.3847  loss_mask_5: 0.5634  loss_dice_5: 4.039  loss_ce_6: 0.4014  loss_mask_6: 0.5639  loss_dice_6: 4.038  loss_ce_7: 0.3874  loss_mask_7: 0.5638  loss_dice_7: 4.039  loss_ce_8: 0.3828  loss_mask_8: 0.5644  loss_dice_8: 4.038  time: 1.5362  data_time: 0.0799  lr: 9.6153e-06  max_mem: 21234M
[01/17 04:50:48] d2.utils.events INFO:  eta: 1 day, 12:39:37  iter: 3859  total_loss: 50.12  loss_ce: 0.3805  loss_mask: 0.5689  loss_dice: 4.014  loss_ce_0: 0.7435  loss_mask_0: 0.5331  loss_dice_0: 4.114  loss_ce_1: 0.4022  loss_mask_1: 0.5563  loss_dice_1: 4.045  loss_ce_2: 0.3893  loss_mask_2: 0.5595  loss_dice_2: 4.022  loss_ce_3: 0.3949  loss_mask_3: 0.5589  loss_dice_3: 4.015  loss_ce_4: 0.3799  loss_mask_4: 0.5602  loss_dice_4: 4.007  loss_ce_5: 0.3808  loss_mask_5: 0.5609  loss_dice_5: 4.012  loss_ce_6: 0.371  loss_mask_6: 0.5638  loss_dice_6: 4.015  loss_ce_7: 0.3727  loss_mask_7: 0.565  loss_dice_7: 4.02  loss_ce_8: 0.3734  loss_mask_8: 0.5693  loss_dice_8: 4.014  time: 1.5361  data_time: 0.0838  lr: 9.6133e-06  max_mem: 21234M
[01/17 04:51:19] d2.utils.events INFO:  eta: 1 day, 12:38:52  iter: 3879  total_loss: 50.32  loss_ce: 0.3851  loss_mask: 0.5429  loss_dice: 4.054  loss_ce_0: 0.7229  loss_mask_0: 0.519  loss_dice_0: 4.14  loss_ce_1: 0.4181  loss_mask_1: 0.5371  loss_dice_1: 4.081  loss_ce_2: 0.4048  loss_mask_2: 0.5373  loss_dice_2: 4.072  loss_ce_3: 0.3969  loss_mask_3: 0.5404  loss_dice_3: 4.052  loss_ce_4: 0.3889  loss_mask_4: 0.5424  loss_dice_4: 4.049  loss_ce_5: 0.3876  loss_mask_5: 0.5403  loss_dice_5: 4.051  loss_ce_6: 0.3827  loss_mask_6: 0.5424  loss_dice_6: 4.049  loss_ce_7: 0.3898  loss_mask_7: 0.5441  loss_dice_7: 4.047  loss_ce_8: 0.3778  loss_mask_8: 0.5431  loss_dice_8: 4.05  time: 1.5361  data_time: 0.1068  lr: 9.6113e-06  max_mem: 21234M
[01/17 04:51:49] d2.utils.events INFO:  eta: 1 day, 12:38:21  iter: 3899  total_loss: 50.24  loss_ce: 0.3597  loss_mask: 0.563  loss_dice: 4.039  loss_ce_0: 0.7043  loss_mask_0: 0.5324  loss_dice_0: 4.135  loss_ce_1: 0.3991  loss_mask_1: 0.5523  loss_dice_1: 4.062  loss_ce_2: 0.3694  loss_mask_2: 0.5583  loss_dice_2: 4.047  loss_ce_3: 0.3638  loss_mask_3: 0.5566  loss_dice_3: 4.049  loss_ce_4: 0.3656  loss_mask_4: 0.5588  loss_dice_4: 4.049  loss_ce_5: 0.3646  loss_mask_5: 0.5615  loss_dice_5: 4.041  loss_ce_6: 0.3659  loss_mask_6: 0.5618  loss_dice_6: 4.045  loss_ce_7: 0.3621  loss_mask_7: 0.5584  loss_dice_7: 4.044  loss_ce_8: 0.3571  loss_mask_8: 0.562  loss_dice_8: 4.033  time: 1.5360  data_time: 0.0983  lr: 9.6092e-06  max_mem: 21234M
[01/17 04:52:20] d2.utils.events INFO:  eta: 1 day, 12:37:51  iter: 3919  total_loss: 50.12  loss_ce: 0.3957  loss_mask: 0.5545  loss_dice: 4.026  loss_ce_0: 0.7415  loss_mask_0: 0.5311  loss_dice_0: 4.105  loss_ce_1: 0.4217  loss_mask_1: 0.5513  loss_dice_1: 4.046  loss_ce_2: 0.4017  loss_mask_2: 0.549  loss_dice_2: 4.022  loss_ce_3: 0.3958  loss_mask_3: 0.5531  loss_dice_3: 4.016  loss_ce_4: 0.3985  loss_mask_4: 0.5509  loss_dice_4: 4.02  loss_ce_5: 0.3991  loss_mask_5: 0.5526  loss_dice_5: 4.024  loss_ce_6: 0.3924  loss_mask_6: 0.5558  loss_dice_6: 4.025  loss_ce_7: 0.3912  loss_mask_7: 0.5571  loss_dice_7: 4.032  loss_ce_8: 0.3784  loss_mask_8: 0.5541  loss_dice_8: 4.023  time: 1.5360  data_time: 0.0993  lr: 9.6072e-06  max_mem: 21234M
[01/17 04:52:50] d2.utils.events INFO:  eta: 1 day, 12:37:18  iter: 3939  total_loss: 50.09  loss_ce: 0.3934  loss_mask: 0.5463  loss_dice: 4.006  loss_ce_0: 0.7398  loss_mask_0: 0.519  loss_dice_0: 4.112  loss_ce_1: 0.4394  loss_mask_1: 0.5444  loss_dice_1: 4.03  loss_ce_2: 0.4148  loss_mask_2: 0.5476  loss_dice_2: 4.014  loss_ce_3: 0.4001  loss_mask_3: 0.5486  loss_dice_3: 4.005  loss_ce_4: 0.3689  loss_mask_4: 0.5489  loss_dice_4: 4.011  loss_ce_5: 0.386  loss_mask_5: 0.5463  loss_dice_5: 4.009  loss_ce_6: 0.385  loss_mask_6: 0.5463  loss_dice_6: 4.005  loss_ce_7: 0.3756  loss_mask_7: 0.5462  loss_dice_7: 4.004  loss_ce_8: 0.3691  loss_mask_8: 0.5456  loss_dice_8: 4.006  time: 1.5359  data_time: 0.0892  lr: 9.6052e-06  max_mem: 21234M
[01/17 04:53:20] d2.utils.events INFO:  eta: 1 day, 12:36:43  iter: 3959  total_loss: 50.08  loss_ce: 0.383  loss_mask: 0.5728  loss_dice: 4.011  loss_ce_0: 0.6987  loss_mask_0: 0.5449  loss_dice_0: 4.098  loss_ce_1: 0.4071  loss_mask_1: 0.5653  loss_dice_1: 4.026  loss_ce_2: 0.4009  loss_mask_2: 0.5665  loss_dice_2: 4.018  loss_ce_3: 0.3781  loss_mask_3: 0.5703  loss_dice_3: 4.008  loss_ce_4: 0.3705  loss_mask_4: 0.5716  loss_dice_4: 4.012  loss_ce_5: 0.3863  loss_mask_5: 0.5707  loss_dice_5: 4.012  loss_ce_6: 0.4001  loss_mask_6: 0.5704  loss_dice_6: 4.01  loss_ce_7: 0.3896  loss_mask_7: 0.5712  loss_dice_7: 4.007  loss_ce_8: 0.3824  loss_mask_8: 0.5707  loss_dice_8: 4.002  time: 1.5357  data_time: 0.0883  lr: 9.6032e-06  max_mem: 21234M
[01/17 04:53:51] d2.utils.events INFO:  eta: 1 day, 12:36:22  iter: 3979  total_loss: 50.51  loss_ce: 0.4074  loss_mask: 0.5459  loss_dice: 4.043  loss_ce_0: 0.7285  loss_mask_0: 0.524  loss_dice_0: 4.123  loss_ce_1: 0.4291  loss_mask_1: 0.5492  loss_dice_1: 4.067  loss_ce_2: 0.4095  loss_mask_2: 0.5467  loss_dice_2: 4.049  loss_ce_3: 0.4128  loss_mask_3: 0.5472  loss_dice_3: 4.031  loss_ce_4: 0.4158  loss_mask_4: 0.5497  loss_dice_4: 4.037  loss_ce_5: 0.4118  loss_mask_5: 0.5477  loss_dice_5: 4.049  loss_ce_6: 0.4097  loss_mask_6: 0.5454  loss_dice_6: 4.04  loss_ce_7: 0.4186  loss_mask_7: 0.5458  loss_dice_7: 4.039  loss_ce_8: 0.3985  loss_mask_8: 0.5474  loss_dice_8: 4.033  time: 1.5357  data_time: 0.0966  lr: 9.6012e-06  max_mem: 21234M
[01/17 04:54:21] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 04:54:21] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 04:54:21] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 04:54:22] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 04:54:36] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0086 s/iter. Inference: 0.1881 s/iter. Eval: 0.2020 s/iter. Total: 0.3988 s/iter. ETA=0:07:11
[01/17 04:54:42] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0119 s/iter. Inference: 0.1735 s/iter. Eval: 0.2251 s/iter. Total: 0.4107 s/iter. ETA=0:07:19
[01/17 04:54:47] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0119 s/iter. Inference: 0.1715 s/iter. Eval: 0.2111 s/iter. Total: 0.3946 s/iter. ETA=0:06:56
[01/17 04:54:52] d2.evaluation.evaluator INFO: Inference done 51/1093. Dataloading: 0.0142 s/iter. Inference: 0.1726 s/iter. Eval: 0.2125 s/iter. Total: 0.3994 s/iter. ETA=0:06:56
[01/17 04:54:58] d2.evaluation.evaluator INFO: Inference done 63/1093. Dataloading: 0.0149 s/iter. Inference: 0.1742 s/iter. Eval: 0.2194 s/iter. Total: 0.4087 s/iter. ETA=0:07:00
[01/17 04:55:03] d2.evaluation.evaluator INFO: Inference done 76/1093. Dataloading: 0.0147 s/iter. Inference: 0.1751 s/iter. Eval: 0.2166 s/iter. Total: 0.4065 s/iter. ETA=0:06:53
[01/17 04:55:08] d2.evaluation.evaluator INFO: Inference done 88/1093. Dataloading: 0.0146 s/iter. Inference: 0.1742 s/iter. Eval: 0.2230 s/iter. Total: 0.4120 s/iter. ETA=0:06:54
[01/17 04:55:13] d2.evaluation.evaluator INFO: Inference done 101/1093. Dataloading: 0.0147 s/iter. Inference: 0.1732 s/iter. Eval: 0.2203 s/iter. Total: 0.4084 s/iter. ETA=0:06:45
[01/17 04:55:18] d2.evaluation.evaluator INFO: Inference done 114/1093. Dataloading: 0.0144 s/iter. Inference: 0.1734 s/iter. Eval: 0.2195 s/iter. Total: 0.4073 s/iter. ETA=0:06:38
[01/17 04:55:24] d2.evaluation.evaluator INFO: Inference done 127/1093. Dataloading: 0.0143 s/iter. Inference: 0.1721 s/iter. Eval: 0.2200 s/iter. Total: 0.4064 s/iter. ETA=0:06:32
[01/17 04:55:29] d2.evaluation.evaluator INFO: Inference done 141/1093. Dataloading: 0.0140 s/iter. Inference: 0.1717 s/iter. Eval: 0.2162 s/iter. Total: 0.4020 s/iter. ETA=0:06:22
[01/17 04:55:34] d2.evaluation.evaluator INFO: Inference done 155/1093. Dataloading: 0.0138 s/iter. Inference: 0.1725 s/iter. Eval: 0.2125 s/iter. Total: 0.3989 s/iter. ETA=0:06:14
[01/17 04:55:39] d2.evaluation.evaluator INFO: Inference done 168/1093. Dataloading: 0.0139 s/iter. Inference: 0.1719 s/iter. Eval: 0.2140 s/iter. Total: 0.3999 s/iter. ETA=0:06:09
[01/17 04:55:44] d2.evaluation.evaluator INFO: Inference done 181/1093. Dataloading: 0.0138 s/iter. Inference: 0.1722 s/iter. Eval: 0.2131 s/iter. Total: 0.3993 s/iter. ETA=0:06:04
[01/17 04:55:50] d2.evaluation.evaluator INFO: Inference done 195/1093. Dataloading: 0.0136 s/iter. Inference: 0.1724 s/iter. Eval: 0.2117 s/iter. Total: 0.3978 s/iter. ETA=0:05:57
[01/17 04:55:55] d2.evaluation.evaluator INFO: Inference done 208/1093. Dataloading: 0.0138 s/iter. Inference: 0.1717 s/iter. Eval: 0.2118 s/iter. Total: 0.3974 s/iter. ETA=0:05:51
[01/17 04:56:00] d2.evaluation.evaluator INFO: Inference done 221/1093. Dataloading: 0.0138 s/iter. Inference: 0.1725 s/iter. Eval: 0.2117 s/iter. Total: 0.3982 s/iter. ETA=0:05:47
[01/17 04:56:05] d2.evaluation.evaluator INFO: Inference done 234/1093. Dataloading: 0.0138 s/iter. Inference: 0.1732 s/iter. Eval: 0.2114 s/iter. Total: 0.3985 s/iter. ETA=0:05:42
[01/17 04:56:10] d2.evaluation.evaluator INFO: Inference done 247/1093. Dataloading: 0.0138 s/iter. Inference: 0.1731 s/iter. Eval: 0.2110 s/iter. Total: 0.3980 s/iter. ETA=0:05:36
[01/17 04:56:15] d2.evaluation.evaluator INFO: Inference done 259/1093. Dataloading: 0.0139 s/iter. Inference: 0.1732 s/iter. Eval: 0.2117 s/iter. Total: 0.3990 s/iter. ETA=0:05:32
[01/17 04:56:20] d2.evaluation.evaluator INFO: Inference done 272/1093. Dataloading: 0.0139 s/iter. Inference: 0.1733 s/iter. Eval: 0.2114 s/iter. Total: 0.3987 s/iter. ETA=0:05:27
[01/17 04:56:26] d2.evaluation.evaluator INFO: Inference done 286/1093. Dataloading: 0.0139 s/iter. Inference: 0.1725 s/iter. Eval: 0.2111 s/iter. Total: 0.3976 s/iter. ETA=0:05:20
[01/17 04:56:31] d2.evaluation.evaluator INFO: Inference done 299/1093. Dataloading: 0.0139 s/iter. Inference: 0.1728 s/iter. Eval: 0.2111 s/iter. Total: 0.3980 s/iter. ETA=0:05:16
[01/17 04:56:36] d2.evaluation.evaluator INFO: Inference done 311/1093. Dataloading: 0.0139 s/iter. Inference: 0.1728 s/iter. Eval: 0.2129 s/iter. Total: 0.3998 s/iter. ETA=0:05:12
[01/17 04:56:41] d2.evaluation.evaluator INFO: Inference done 325/1093. Dataloading: 0.0139 s/iter. Inference: 0.1722 s/iter. Eval: 0.2120 s/iter. Total: 0.3982 s/iter. ETA=0:05:05
[01/17 04:56:46] d2.evaluation.evaluator INFO: Inference done 338/1093. Dataloading: 0.0139 s/iter. Inference: 0.1729 s/iter. Eval: 0.2109 s/iter. Total: 0.3978 s/iter. ETA=0:05:00
[01/17 04:56:52] d2.evaluation.evaluator INFO: Inference done 352/1093. Dataloading: 0.0138 s/iter. Inference: 0.1736 s/iter. Eval: 0.2092 s/iter. Total: 0.3966 s/iter. ETA=0:04:53
[01/17 04:56:57] d2.evaluation.evaluator INFO: Inference done 364/1093. Dataloading: 0.0137 s/iter. Inference: 0.1741 s/iter. Eval: 0.2099 s/iter. Total: 0.3978 s/iter. ETA=0:04:50
[01/17 04:57:02] d2.evaluation.evaluator INFO: Inference done 377/1093. Dataloading: 0.0137 s/iter. Inference: 0.1748 s/iter. Eval: 0.2092 s/iter. Total: 0.3978 s/iter. ETA=0:04:44
[01/17 04:57:07] d2.evaluation.evaluator INFO: Inference done 390/1093. Dataloading: 0.0137 s/iter. Inference: 0.1746 s/iter. Eval: 0.2093 s/iter. Total: 0.3976 s/iter. ETA=0:04:39
[01/17 04:57:12] d2.evaluation.evaluator INFO: Inference done 404/1093. Dataloading: 0.0136 s/iter. Inference: 0.1745 s/iter. Eval: 0.2090 s/iter. Total: 0.3972 s/iter. ETA=0:04:33
[01/17 04:57:18] d2.evaluation.evaluator INFO: Inference done 417/1093. Dataloading: 0.0136 s/iter. Inference: 0.1749 s/iter. Eval: 0.2086 s/iter. Total: 0.3972 s/iter. ETA=0:04:28
[01/17 04:57:23] d2.evaluation.evaluator INFO: Inference done 431/1093. Dataloading: 0.0136 s/iter. Inference: 0.1742 s/iter. Eval: 0.2085 s/iter. Total: 0.3965 s/iter. ETA=0:04:22
[01/17 04:57:28] d2.evaluation.evaluator INFO: Inference done 445/1093. Dataloading: 0.0135 s/iter. Inference: 0.1745 s/iter. Eval: 0.2074 s/iter. Total: 0.3956 s/iter. ETA=0:04:16
[01/17 04:57:33] d2.evaluation.evaluator INFO: Inference done 458/1093. Dataloading: 0.0135 s/iter. Inference: 0.1747 s/iter. Eval: 0.2076 s/iter. Total: 0.3959 s/iter. ETA=0:04:11
[01/17 04:57:39] d2.evaluation.evaluator INFO: Inference done 472/1093. Dataloading: 0.0134 s/iter. Inference: 0.1747 s/iter. Eval: 0.2070 s/iter. Total: 0.3952 s/iter. ETA=0:04:05
[01/17 04:57:44] d2.evaluation.evaluator INFO: Inference done 486/1093. Dataloading: 0.0133 s/iter. Inference: 0.1751 s/iter. Eval: 0.2065 s/iter. Total: 0.3950 s/iter. ETA=0:03:59
[01/17 04:57:49] d2.evaluation.evaluator INFO: Inference done 500/1093. Dataloading: 0.0133 s/iter. Inference: 0.1753 s/iter. Eval: 0.2055 s/iter. Total: 0.3942 s/iter. ETA=0:03:53
[01/17 04:57:54] d2.evaluation.evaluator INFO: Inference done 516/1093. Dataloading: 0.0132 s/iter. Inference: 0.1749 s/iter. Eval: 0.2040 s/iter. Total: 0.3923 s/iter. ETA=0:03:46
[01/17 04:58:00] d2.evaluation.evaluator INFO: Inference done 530/1093. Dataloading: 0.0132 s/iter. Inference: 0.1741 s/iter. Eval: 0.2041 s/iter. Total: 0.3916 s/iter. ETA=0:03:40
[01/17 04:58:05] d2.evaluation.evaluator INFO: Inference done 544/1093. Dataloading: 0.0132 s/iter. Inference: 0.1738 s/iter. Eval: 0.2039 s/iter. Total: 0.3910 s/iter. ETA=0:03:34
[01/17 04:58:10] d2.evaluation.evaluator INFO: Inference done 555/1093. Dataloading: 0.0132 s/iter. Inference: 0.1746 s/iter. Eval: 0.2047 s/iter. Total: 0.3926 s/iter. ETA=0:03:31
[01/17 04:58:15] d2.evaluation.evaluator INFO: Inference done 568/1093. Dataloading: 0.0132 s/iter. Inference: 0.1747 s/iter. Eval: 0.2048 s/iter. Total: 0.3928 s/iter. ETA=0:03:26
[01/17 04:58:20] d2.evaluation.evaluator INFO: Inference done 584/1093. Dataloading: 0.0133 s/iter. Inference: 0.1745 s/iter. Eval: 0.2030 s/iter. Total: 0.3909 s/iter. ETA=0:03:18
[01/17 04:58:25] d2.evaluation.evaluator INFO: Inference done 597/1093. Dataloading: 0.0132 s/iter. Inference: 0.1744 s/iter. Eval: 0.2032 s/iter. Total: 0.3910 s/iter. ETA=0:03:13
[01/17 04:58:30] d2.evaluation.evaluator INFO: Inference done 608/1093. Dataloading: 0.0133 s/iter. Inference: 0.1745 s/iter. Eval: 0.2043 s/iter. Total: 0.3922 s/iter. ETA=0:03:10
[01/17 04:58:36] d2.evaluation.evaluator INFO: Inference done 622/1093. Dataloading: 0.0133 s/iter. Inference: 0.1743 s/iter. Eval: 0.2042 s/iter. Total: 0.3920 s/iter. ETA=0:03:04
[01/17 04:58:41] d2.evaluation.evaluator INFO: Inference done 636/1093. Dataloading: 0.0132 s/iter. Inference: 0.1738 s/iter. Eval: 0.2041 s/iter. Total: 0.3913 s/iter. ETA=0:02:58
[01/17 04:58:46] d2.evaluation.evaluator INFO: Inference done 650/1093. Dataloading: 0.0132 s/iter. Inference: 0.1740 s/iter. Eval: 0.2034 s/iter. Total: 0.3907 s/iter. ETA=0:02:53
[01/17 04:58:51] d2.evaluation.evaluator INFO: Inference done 664/1093. Dataloading: 0.0132 s/iter. Inference: 0.1738 s/iter. Eval: 0.2033 s/iter. Total: 0.3905 s/iter. ETA=0:02:47
[01/17 04:58:56] d2.evaluation.evaluator INFO: Inference done 678/1093. Dataloading: 0.0131 s/iter. Inference: 0.1737 s/iter. Eval: 0.2029 s/iter. Total: 0.3899 s/iter. ETA=0:02:41
[01/17 04:59:02] d2.evaluation.evaluator INFO: Inference done 695/1093. Dataloading: 0.0131 s/iter. Inference: 0.1735 s/iter. Eval: 0.2013 s/iter. Total: 0.3880 s/iter. ETA=0:02:34
[01/17 04:59:07] d2.evaluation.evaluator INFO: Inference done 707/1093. Dataloading: 0.0131 s/iter. Inference: 0.1736 s/iter. Eval: 0.2021 s/iter. Total: 0.3890 s/iter. ETA=0:02:30
[01/17 04:59:12] d2.evaluation.evaluator INFO: Inference done 719/1093. Dataloading: 0.0131 s/iter. Inference: 0.1738 s/iter. Eval: 0.2029 s/iter. Total: 0.3899 s/iter. ETA=0:02:25
[01/17 04:59:17] d2.evaluation.evaluator INFO: Inference done 734/1093. Dataloading: 0.0131 s/iter. Inference: 0.1737 s/iter. Eval: 0.2020 s/iter. Total: 0.3889 s/iter. ETA=0:02:19
[01/17 04:59:22] d2.evaluation.evaluator INFO: Inference done 747/1093. Dataloading: 0.0131 s/iter. Inference: 0.1738 s/iter. Eval: 0.2019 s/iter. Total: 0.3889 s/iter. ETA=0:02:14
[01/17 04:59:28] d2.evaluation.evaluator INFO: Inference done 760/1093. Dataloading: 0.0131 s/iter. Inference: 0.1737 s/iter. Eval: 0.2021 s/iter. Total: 0.3890 s/iter. ETA=0:02:09
[01/17 04:59:33] d2.evaluation.evaluator INFO: Inference done 771/1093. Dataloading: 0.0131 s/iter. Inference: 0.1737 s/iter. Eval: 0.2031 s/iter. Total: 0.3900 s/iter. ETA=0:02:05
[01/17 04:59:38] d2.evaluation.evaluator INFO: Inference done 785/1093. Dataloading: 0.0131 s/iter. Inference: 0.1736 s/iter. Eval: 0.2030 s/iter. Total: 0.3898 s/iter. ETA=0:02:00
[01/17 04:59:43] d2.evaluation.evaluator INFO: Inference done 799/1093. Dataloading: 0.0130 s/iter. Inference: 0.1735 s/iter. Eval: 0.2028 s/iter. Total: 0.3894 s/iter. ETA=0:01:54
[01/17 04:59:48] d2.evaluation.evaluator INFO: Inference done 812/1093. Dataloading: 0.0130 s/iter. Inference: 0.1735 s/iter. Eval: 0.2029 s/iter. Total: 0.3895 s/iter. ETA=0:01:49
[01/17 04:59:53] d2.evaluation.evaluator INFO: Inference done 826/1093. Dataloading: 0.0130 s/iter. Inference: 0.1736 s/iter. Eval: 0.2025 s/iter. Total: 0.3892 s/iter. ETA=0:01:43
[01/17 04:59:58] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0129 s/iter. Inference: 0.1736 s/iter. Eval: 0.2025 s/iter. Total: 0.3892 s/iter. ETA=0:01:38
[01/17 05:00:04] d2.evaluation.evaluator INFO: Inference done 853/1093. Dataloading: 0.0129 s/iter. Inference: 0.1735 s/iter. Eval: 0.2025 s/iter. Total: 0.3891 s/iter. ETA=0:01:33
[01/17 05:00:09] d2.evaluation.evaluator INFO: Inference done 866/1093. Dataloading: 0.0129 s/iter. Inference: 0.1733 s/iter. Eval: 0.2027 s/iter. Total: 0.3891 s/iter. ETA=0:01:28
[01/17 05:00:14] d2.evaluation.evaluator INFO: Inference done 880/1093. Dataloading: 0.0129 s/iter. Inference: 0.1732 s/iter. Eval: 0.2027 s/iter. Total: 0.3890 s/iter. ETA=0:01:22
[01/17 05:00:20] d2.evaluation.evaluator INFO: Inference done 894/1093. Dataloading: 0.0130 s/iter. Inference: 0.1728 s/iter. Eval: 0.2029 s/iter. Total: 0.3888 s/iter. ETA=0:01:17
[01/17 05:00:25] d2.evaluation.evaluator INFO: Inference done 908/1093. Dataloading: 0.0129 s/iter. Inference: 0.1729 s/iter. Eval: 0.2025 s/iter. Total: 0.3885 s/iter. ETA=0:01:11
[01/17 05:00:30] d2.evaluation.evaluator INFO: Inference done 921/1093. Dataloading: 0.0129 s/iter. Inference: 0.1728 s/iter. Eval: 0.2028 s/iter. Total: 0.3886 s/iter. ETA=0:01:06
[01/17 05:00:35] d2.evaluation.evaluator INFO: Inference done 934/1093. Dataloading: 0.0129 s/iter. Inference: 0.1729 s/iter. Eval: 0.2030 s/iter. Total: 0.3889 s/iter. ETA=0:01:01
[01/17 05:00:40] d2.evaluation.evaluator INFO: Inference done 947/1093. Dataloading: 0.0129 s/iter. Inference: 0.1728 s/iter. Eval: 0.2031 s/iter. Total: 0.3889 s/iter. ETA=0:00:56
[01/17 05:00:45] d2.evaluation.evaluator INFO: Inference done 961/1093. Dataloading: 0.0129 s/iter. Inference: 0.1725 s/iter. Eval: 0.2031 s/iter. Total: 0.3887 s/iter. ETA=0:00:51
[01/17 05:00:51] d2.evaluation.evaluator INFO: Inference done 975/1093. Dataloading: 0.0129 s/iter. Inference: 0.1724 s/iter. Eval: 0.2030 s/iter. Total: 0.3885 s/iter. ETA=0:00:45
[01/17 05:00:56] d2.evaluation.evaluator INFO: Inference done 990/1093. Dataloading: 0.0129 s/iter. Inference: 0.1723 s/iter. Eval: 0.2024 s/iter. Total: 0.3877 s/iter. ETA=0:00:39
[01/17 05:01:01] d2.evaluation.evaluator INFO: Inference done 1004/1093. Dataloading: 0.0129 s/iter. Inference: 0.1721 s/iter. Eval: 0.2023 s/iter. Total: 0.3873 s/iter. ETA=0:00:34
[01/17 05:01:06] d2.evaluation.evaluator INFO: Inference done 1018/1093. Dataloading: 0.0128 s/iter. Inference: 0.1719 s/iter. Eval: 0.2023 s/iter. Total: 0.3871 s/iter. ETA=0:00:29
[01/17 05:01:11] d2.evaluation.evaluator INFO: Inference done 1031/1093. Dataloading: 0.0128 s/iter. Inference: 0.1718 s/iter. Eval: 0.2026 s/iter. Total: 0.3874 s/iter. ETA=0:00:24
[01/17 05:01:17] d2.evaluation.evaluator INFO: Inference done 1045/1093. Dataloading: 0.0128 s/iter. Inference: 0.1717 s/iter. Eval: 0.2027 s/iter. Total: 0.3873 s/iter. ETA=0:00:18
[01/17 05:01:22] d2.evaluation.evaluator INFO: Inference done 1059/1093. Dataloading: 0.0128 s/iter. Inference: 0.1715 s/iter. Eval: 0.2025 s/iter. Total: 0.3870 s/iter. ETA=0:00:13
[01/17 05:01:27] d2.evaluation.evaluator INFO: Inference done 1073/1093. Dataloading: 0.0128 s/iter. Inference: 0.1716 s/iter. Eval: 0.2022 s/iter. Total: 0.3867 s/iter. ETA=0:00:07
[01/17 05:01:32] d2.evaluation.evaluator INFO: Inference done 1087/1093. Dataloading: 0.0128 s/iter. Inference: 0.1716 s/iter. Eval: 0.2019 s/iter. Total: 0.3864 s/iter. ETA=0:00:02
[01/17 05:01:34] d2.evaluation.evaluator INFO: Total inference time: 0:07:00.288037 (0.386294 s / iter per device, on 4 devices)
[01/17 05:01:34] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:06 (0.171353 s / iter per device, on 4 devices)
[01/17 05:02:03] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 5.239467077917962, 'fwIoU': 18.863409302691615, 'IoU-1': nan, 'IoU-2': 93.29149710641056, 'IoU-3': 35.91307774701401, 'IoU-4': 45.90362826205353, 'IoU-5': 38.748237510545124, 'IoU-6': 31.96949475691134, 'IoU-7': 31.123708339720817, 'IoU-8': 25.852337125257552, 'IoU-9': 9.376802692723262, 'IoU-10': 1.672765282347816, 'IoU-11': 16.32065189067487, 'IoU-12': 16.93424087076864, 'IoU-13': 17.673791536398266, 'IoU-14': 17.286651209737435, 'IoU-15': 10.193757880014562, 'IoU-16': 14.132302201857183, 'IoU-17': 9.759545275367756, 'IoU-18': 4.937024410736394, 'IoU-19': 14.210599117418422, 'IoU-20': 7.381305441930166, 'IoU-21': 8.944397621170355, 'IoU-22': 9.110399691367164, 'IoU-23': 14.824226144607955, 'IoU-24': 9.108436461506694, 'IoU-25': 11.332299976434815, 'IoU-26': 9.125351431030406, 'IoU-27': 12.902091761475598, 'IoU-28': 12.497253474800312, 'IoU-29': 5.618196731914187, 'IoU-30': 13.268737127658403, 'IoU-31': 9.361529303751304, 'IoU-32': 14.191442287404211, 'IoU-33': 9.453913715104767, 'IoU-34': 14.971128126791356, 'IoU-35': 9.198314468155885, 'IoU-36': 13.002549559394033, 'IoU-37': 12.734258408311513, 'IoU-38': 13.488946467873614, 'IoU-39': 11.87094166088798, 'IoU-40': 14.88754183467641, 'IoU-41': 7.872795095530474, 'IoU-42': 14.579959943437357, 'IoU-43': 12.959288086025703, 'IoU-44': 12.238999317553063, 'IoU-45': 11.201172732015397, 'IoU-46': 12.786111191963794, 'IoU-47': 9.730193998433325, 'IoU-48': 11.315493852915862, 'IoU-49': 13.759659357100325, 'IoU-50': 11.103066498872519, 'IoU-51': 10.643906100469534, 'IoU-52': 9.914199686198218, 'IoU-53': 10.019206175764486, 'IoU-54': 13.435034099459259, 'IoU-55': 5.561722330846005, 'IoU-56': 13.657406942566622, 'IoU-57': 12.01001763601493, 'IoU-58': 7.492241996445467, 'IoU-59': 7.221554023482038, 'IoU-60': 13.741200580996772, 'IoU-61': 7.79711926113784, 'IoU-62': 10.050051037492617, 'IoU-63': 11.1592357899369, 'IoU-64': 10.567262965783536, 'IoU-65': 7.353976903293569, 'IoU-66': 10.737906096641312, 'IoU-67': 5.872485464265071, 'IoU-68': 8.615204460278083, 'IoU-69': 7.762529264557188, 'IoU-70': 8.054189148416112, 'IoU-71': 7.7949950529606085, 'IoU-72': 8.247831343872424, 'IoU-73': 6.0812304832967765, 'IoU-74': 5.462469445855399, 'IoU-75': 8.062582349975363, 'IoU-76': 3.441954625054864, 'IoU-77': 10.16766462246603, 'IoU-78': 7.130990092537012, 'IoU-79': 7.598384848637616, 'IoU-80': 7.925429027575818, 'IoU-81': 2.830482251775809, 'IoU-82': 7.73255569889717, 'IoU-83': 4.064349405623356, 'IoU-84': 6.2413050683282245, 'IoU-85': 8.199521332782602, 'IoU-86': 8.077747265293008, 'IoU-87': 2.317737710593023, 'IoU-88': 9.053307814424118, 'IoU-89': 2.2122535268280275, 'IoU-90': 7.736400374026715, 'IoU-91': 3.125218823654923, 'IoU-92': 4.727979431006131, 'IoU-93': 1.00929169265018, 'IoU-94': 9.850370360161092, 'IoU-95': 4.183949945091923, 'IoU-96': 6.32020584018262, 'IoU-97': 5.969347559732459, 'IoU-98': 4.683894355350516, 'IoU-99': 6.731172554171408, 'IoU-100': 5.793303738276841, 'IoU-101': 4.549750882441792, 'IoU-102': 4.926161076896425, 'IoU-103': 7.426363330239977, 'IoU-104': 2.4094728407883275, 'IoU-105': 2.5708183073127597, 'IoU-106': 3.663257077710822, 'IoU-107': 6.52449503701299, 'IoU-108': 5.310277452013805, 'IoU-109': 3.20633076147279, 'IoU-110': 4.395556655038622, 'IoU-111': 4.991612444566922, 'IoU-112': 4.647749340470419, 'IoU-113': 6.444044514065155, 'IoU-114': 2.89290262136994, 'IoU-115': 2.4833890900084943, 'IoU-116': 3.13266680954315, 'IoU-117': 2.9117473291138825, 'IoU-118': 3.3962186631262408, 'IoU-119': 3.7863716565073826, 'IoU-120': 4.411727117868114, 'IoU-121': 3.482760440944769, 'IoU-122': 3.8262221965725076, 'IoU-123': 2.993390559053397, 'IoU-124': 3.7796217388054125, 'IoU-125': 2.653631475906017, 'IoU-126': 2.6096594013263816, 'IoU-127': 3.132619078605383, 'IoU-128': 2.7477971433559993, 'IoU-129': 3.1965153271977313, 'IoU-130': 2.135926572188816, 'IoU-131': 2.1721127294002596, 'IoU-132': 2.8389318365025793, 'IoU-133': 1.9075192958467602, 'IoU-134': 3.766348571447805, 'IoU-135': 1.685424957256222, 'IoU-136': 1.618129273305779, 'IoU-137': 2.04060839159645, 'IoU-138': 2.0023006378400003, 'IoU-139': 2.007614339652893, 'IoU-140': 2.66789513747651, 'IoU-141': 2.5220373964792113, 'IoU-142': 2.8170763617861105, 'IoU-143': 1.013841299887544, 'IoU-144': 2.7074082959385533, 'IoU-145': 1.57212364996812, 'IoU-146': 1.6549939818400659, 'IoU-147': 0.6700409976632783, 'IoU-148': 0.2990808388318195, 'IoU-149': 0.3843994786491335, 'IoU-150': 2.719270159743027, 'IoU-151': 1.910020921741462, 'IoU-152': 0.6785859588413339, 'IoU-153': 0.22061955469506292, 'IoU-154': 0.25282866347472777, 'IoU-155': 1.571697157379558, 'IoU-156': 0.6925287980480317, 'IoU-157': 0.26358710627555404, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 1.5702765766237743, 'IoU-161': 0.0, 'IoU-162': 1.2624172465784416, 'IoU-163': 1.142961381061445, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 1.4922514949774848, 'IoU-167': 0.0, 'IoU-168': 0.0630887805745856, 'IoU-169': 0.012353092025844547, 'IoU-170': 0.004015573441346439, 'IoU-171': 0.21597254063411936, 'IoU-172': 0.0, 'IoU-173': 0.12207843712012138, 'IoU-174': 0.0, 'IoU-175': 0.012076808502073185, 'IoU-176': 0.11598229247092857, 'IoU-177': 0.0, 'IoU-178': 0.682834429311642, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.17455352822709486, 'IoU-182': 0.0006705349080473129, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.09653075043286544, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'IoU-192': 0.0, 'IoU-193': 0.0, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 9.295587905406755, 'pACC': 27.306978275505, 'ACC-1': nan, 'ACC-2': 98.91272438243051, 'ACC-3': 45.998796697315015, 'ACC-4': 67.58366820642819, 'ACC-5': 52.291625599909885, 'ACC-6': 47.633130958599715, 'ACC-7': 48.3196139604751, 'ACC-8': 41.79291962472333, 'ACC-9': 10.874871053458543, 'ACC-10': 1.7374476231423706, 'ACC-11': 27.293165725514353, 'ACC-12': 25.66390107559662, 'ACC-13': 32.46483716728998, 'ACC-14': 37.730345420637775, 'ACC-15': 15.573294179332093, 'ACC-16': 32.72125316577806, 'ACC-17': 14.7929531562155, 'ACC-18': 7.789522208913569, 'ACC-19': 38.274399391948755, 'ACC-20': 11.780520870806559, 'ACC-21': 14.287962959313576, 'ACC-22': 16.115400819558197, 'ACC-23': 28.54102114432609, 'ACC-24': 15.58463345280855, 'ACC-25': 18.731142920011948, 'ACC-26': 18.838045659004422, 'ACC-27': 22.756027377785617, 'ACC-28': 27.01355918536056, 'ACC-29': 8.84455124330238, 'ACC-30': 23.513000324262777, 'ACC-31': 15.596913506548773, 'ACC-32': 22.16583312335411, 'ACC-33': 13.955229185196117, 'ACC-34': 38.506519254029456, 'ACC-35': 15.243380864311865, 'ACC-36': 20.741119045499453, 'ACC-37': 25.172529810244338, 'ACC-38': 22.484581586977512, 'ACC-39': 20.202994237562542, 'ACC-40': 26.740139636899368, 'ACC-41': 12.374483972881272, 'ACC-42': 31.888860058371133, 'ACC-43': 22.754029250788836, 'ACC-44': 20.718654498269512, 'ACC-45': 21.33287262314557, 'ACC-46': 22.224341706270767, 'ACC-47': 15.76551900098971, 'ACC-48': 20.716070999449986, 'ACC-49': 25.627828155390148, 'ACC-50': 20.366542001566117, 'ACC-51': 16.441793904723546, 'ACC-52': 19.22663140819504, 'ACC-53': 15.187314342335487, 'ACC-54': 27.93353364334859, 'ACC-55': 7.123667478682348, 'ACC-56': 25.354391299310304, 'ACC-57': 23.497155757964215, 'ACC-58': 10.899485838897398, 'ACC-59': 13.495060077295474, 'ACC-60': 35.540058946412906, 'ACC-61': 12.657049882430652, 'ACC-62': 17.416209026026753, 'ACC-63': 22.62068329778069, 'ACC-64': 22.222852448178827, 'ACC-65': 11.945787172803172, 'ACC-66': 24.893448545838616, 'ACC-67': 10.013139653585748, 'ACC-68': 17.275918305333153, 'ACC-69': 16.77170303683351, 'ACC-70': 14.092071857769046, 'ACC-71': 14.460919599145832, 'ACC-72': 15.96482068816097, 'ACC-73': 9.911180661536804, 'ACC-74': 8.896502161923017, 'ACC-75': 17.171728944857605, 'ACC-76': 4.228693983766116, 'ACC-77': 20.337269689086355, 'ACC-78': 13.097709580011434, 'ACC-79': 14.444238873607876, 'ACC-80': 18.273509648386806, 'ACC-81': 3.774801285071674, 'ACC-82': 17.32780656763252, 'ACC-83': 7.134952404247626, 'ACC-84': 9.47534239821375, 'ACC-85': 15.544141763202571, 'ACC-86': 16.314801416302476, 'ACC-87': 3.1853085550878784, 'ACC-88': 18.664008812857922, 'ACC-89': 2.912405865633978, 'ACC-90': 17.11202854409314, 'ACC-91': 4.319783401176787, 'ACC-92': 9.058755427075052, 'ACC-93': 1.2241484022735343, 'ACC-94': 25.690790583435756, 'ACC-95': 6.00909315389088, 'ACC-96': 10.124075688845938, 'ACC-97': 10.595351871955907, 'ACC-98': 7.055881964700558, 'ACC-99': 11.857653065000914, 'ACC-100': 12.1212127055778, 'ACC-101': 7.727910784359042, 'ACC-102': 8.006395607758984, 'ACC-103': 18.666425011327593, 'ACC-104': 3.3424482856333237, 'ACC-105': 3.6086667814367575, 'ACC-106': 5.888009120555447, 'ACC-107': 18.72938429599608, 'ACC-108': 9.736019912187947, 'ACC-109': 4.572290666634977, 'ACC-110': 13.136928884182893, 'ACC-111': 8.68266565176298, 'ACC-112': 9.206140874573512, 'ACC-113': 13.828802587053124, 'ACC-114': 3.726317359091863, 'ACC-115': 3.741950155193318, 'ACC-116': 6.285237472373821, 'ACC-117': 5.258917356164448, 'ACC-118': 5.912392803499402, 'ACC-119': 6.642280693136028, 'ACC-120': 11.696747681893962, 'ACC-121': 6.8393615787642705, 'ACC-122': 7.046561474763891, 'ACC-123': 4.245567852661175, 'ACC-124': 7.359439792053589, 'ACC-125': 6.202702997867457, 'ACC-126': 4.899479313394165, 'ACC-127': 9.481472964234646, 'ACC-128': 5.340438654692596, 'ACC-129': 6.521354667876583, 'ACC-130': 3.313024494229583, 'ACC-131': 3.9649884042791945, 'ACC-132': 11.06111265071811, 'ACC-133': 5.0638544567428365, 'ACC-134': 8.375213483298973, 'ACC-135': 2.18271434726411, 'ACC-136': 2.37087395245425, 'ACC-137': 5.219495434215431, 'ACC-138': 3.0330090359077926, 'ACC-139': 4.250958194786405, 'ACC-140': 8.686266809755745, 'ACC-141': 6.032016591379308, 'ACC-142': 5.553835045334509, 'ACC-143': 1.3770445417318378, 'ACC-144': 6.185992219996711, 'ACC-145': 5.247382671480144, 'ACC-146': 3.256781233927413, 'ACC-147': 1.6031619108542186, 'ACC-148': 0.33194581530070577, 'ACC-149': 0.45301986310549586, 'ACC-150': 12.497401297475125, 'ACC-151': 3.192486207654921, 'ACC-152': 0.8088178086995076, 'ACC-153': 0.24013335356396248, 'ACC-154': 0.36747192143498414, 'ACC-155': 2.6119363554065145, 'ACC-156': 0.9431473908145431, 'ACC-157': 0.2906138700899119, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 7.733627834312011, 'ACC-161': 0.0, 'ACC-162': 2.617130564456564, 'ACC-163': 1.8455761809694111, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 12.214962503216052, 'ACC-167': 0.0, 'ACC-168': 0.06653151909002948, 'ACC-169': 0.01388986567269147, 'ACC-170': 0.004035845325350548, 'ACC-171': 0.26832488606840155, 'ACC-172': 0.0, 'ACC-173': 0.13112568063118124, 'ACC-174': 0.0, 'ACC-175': 0.013420792162257377, 'ACC-176': 0.12720035349865683, 'ACC-177': 0.0, 'ACC-178': 1.966526965004457, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.20234345897253547, 'ACC-182': 0.0007105956923689129, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0972585827158279, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0, 'ACC-192': 0.0, 'ACC-193': 0.0, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 05:02:03] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 05:02:03] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 05:02:03] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 05:02:03] d2.evaluation.testing INFO: copypaste: 5.2395,18.8634,9.2956,27.3070
[01/17 05:02:04] d2.utils.events INFO:  eta: 1 day, 12:35:51  iter: 3999  total_loss: 49.95  loss_ce: 0.3665  loss_mask: 0.5542  loss_dice: 3.993  loss_ce_0: 0.7183  loss_mask_0: 0.5228  loss_dice_0: 4.092  loss_ce_1: 0.3994  loss_mask_1: 0.5464  loss_dice_1: 4.024  loss_ce_2: 0.3872  loss_mask_2: 0.5514  loss_dice_2: 3.998  loss_ce_3: 0.3675  loss_mask_3: 0.553  loss_dice_3: 3.991  loss_ce_4: 0.38  loss_mask_4: 0.5518  loss_dice_4: 3.989  loss_ce_5: 0.3769  loss_mask_5: 0.5533  loss_dice_5: 3.988  loss_ce_6: 0.3686  loss_mask_6: 0.5563  loss_dice_6: 3.991  loss_ce_7: 0.371  loss_mask_7: 0.5559  loss_dice_7: 3.99  loss_ce_8: 0.3702  loss_mask_8: 0.5554  loss_dice_8: 3.991  time: 1.5354  data_time: 0.0958  lr: 9.5992e-06  max_mem: 21234M
[01/17 05:02:35] d2.utils.events INFO:  eta: 1 day, 12:35:20  iter: 4019  total_loss: 50.52  loss_ce: 0.3793  loss_mask: 0.5377  loss_dice: 4.046  loss_ce_0: 0.7266  loss_mask_0: 0.5148  loss_dice_0: 4.137  loss_ce_1: 0.4057  loss_mask_1: 0.5398  loss_dice_1: 4.064  loss_ce_2: 0.4073  loss_mask_2: 0.5391  loss_dice_2: 4.045  loss_ce_3: 0.396  loss_mask_3: 0.5407  loss_dice_3: 4.052  loss_ce_4: 0.3897  loss_mask_4: 0.5365  loss_dice_4: 4.043  loss_ce_5: 0.4047  loss_mask_5: 0.5372  loss_dice_5: 4.048  loss_ce_6: 0.3997  loss_mask_6: 0.5364  loss_dice_6: 4.045  loss_ce_7: 0.3824  loss_mask_7: 0.5347  loss_dice_7: 4.042  loss_ce_8: 0.3949  loss_mask_8: 0.5377  loss_dice_8: 4.047  time: 1.5355  data_time: 0.0879  lr: 9.5972e-06  max_mem: 21234M
[01/17 05:03:06] d2.utils.events INFO:  eta: 1 day, 12:36:47  iter: 4039  total_loss: 49.89  loss_ce: 0.3742  loss_mask: 0.55  loss_dice: 4.034  loss_ce_0: 0.7398  loss_mask_0: 0.5249  loss_dice_0: 4.118  loss_ce_1: 0.3972  loss_mask_1: 0.5497  loss_dice_1: 4.062  loss_ce_2: 0.392  loss_mask_2: 0.5468  loss_dice_2: 4.045  loss_ce_3: 0.3875  loss_mask_3: 0.5487  loss_dice_3: 4.039  loss_ce_4: 0.3763  loss_mask_4: 0.5496  loss_dice_4: 4.034  loss_ce_5: 0.3626  loss_mask_5: 0.5499  loss_dice_5: 4.033  loss_ce_6: 0.3643  loss_mask_6: 0.5509  loss_dice_6: 4.032  loss_ce_7: 0.366  loss_mask_7: 0.5518  loss_dice_7: 4.03  loss_ce_8: 0.3622  loss_mask_8: 0.552  loss_dice_8: 4.034  time: 1.5355  data_time: 0.0882  lr: 9.5952e-06  max_mem: 21234M
[01/17 05:03:37] d2.utils.events INFO:  eta: 1 day, 12:35:31  iter: 4059  total_loss: 50.29  loss_ce: 0.3994  loss_mask: 0.5346  loss_dice: 4.022  loss_ce_0: 0.7292  loss_mask_0: 0.515  loss_dice_0: 4.115  loss_ce_1: 0.3986  loss_mask_1: 0.5269  loss_dice_1: 4.039  loss_ce_2: 0.4084  loss_mask_2: 0.5291  loss_dice_2: 4.019  loss_ce_3: 0.4078  loss_mask_3: 0.5333  loss_dice_3: 4.01  loss_ce_4: 0.4083  loss_mask_4: 0.5356  loss_dice_4: 4.01  loss_ce_5: 0.398  loss_mask_5: 0.5393  loss_dice_5: 4.015  loss_ce_6: 0.4012  loss_mask_6: 0.539  loss_dice_6: 4.009  loss_ce_7: 0.3849  loss_mask_7: 0.5426  loss_dice_7: 4.015  loss_ce_8: 0.3937  loss_mask_8: 0.5366  loss_dice_8: 4.019  time: 1.5356  data_time: 0.0968  lr: 9.5932e-06  max_mem: 21234M
[01/17 05:04:07] d2.utils.events INFO:  eta: 1 day, 12:34:30  iter: 4079  total_loss: 49.97  loss_ce: 0.3622  loss_mask: 0.5528  loss_dice: 4.046  loss_ce_0: 0.6804  loss_mask_0: 0.5272  loss_dice_0: 4.138  loss_ce_1: 0.3862  loss_mask_1: 0.5473  loss_dice_1: 4.072  loss_ce_2: 0.3847  loss_mask_2: 0.5483  loss_dice_2: 4.056  loss_ce_3: 0.3702  loss_mask_3: 0.5489  loss_dice_3: 4.049  loss_ce_4: 0.3573  loss_mask_4: 0.5496  loss_dice_4: 4.044  loss_ce_5: 0.3465  loss_mask_5: 0.553  loss_dice_5: 4.045  loss_ce_6: 0.3717  loss_mask_6: 0.553  loss_dice_6: 4.049  loss_ce_7: 0.3425  loss_mask_7: 0.5507  loss_dice_7: 4.053  loss_ce_8: 0.3567  loss_mask_8: 0.5523  loss_dice_8: 4.043  time: 1.5355  data_time: 0.0890  lr: 9.5912e-06  max_mem: 21234M
[01/17 05:04:38] d2.utils.events INFO:  eta: 1 day, 12:33:00  iter: 4099  total_loss: 49.9  loss_ce: 0.3832  loss_mask: 0.5451  loss_dice: 4.029  loss_ce_0: 0.6893  loss_mask_0: 0.5244  loss_dice_0: 4.109  loss_ce_1: 0.4079  loss_mask_1: 0.5412  loss_dice_1: 4.047  loss_ce_2: 0.3999  loss_mask_2: 0.5455  loss_dice_2: 4.02  loss_ce_3: 0.3892  loss_mask_3: 0.5463  loss_dice_3: 4.022  loss_ce_4: 0.3824  loss_mask_4: 0.5458  loss_dice_4: 4.027  loss_ce_5: 0.3825  loss_mask_5: 0.5447  loss_dice_5: 4.03  loss_ce_6: 0.3973  loss_mask_6: 0.5445  loss_dice_6: 4.027  loss_ce_7: 0.3721  loss_mask_7: 0.5436  loss_dice_7: 4.025  loss_ce_8: 0.3766  loss_mask_8: 0.546  loss_dice_8: 4.024  time: 1.5354  data_time: 0.0891  lr: 9.5892e-06  max_mem: 21234M
[01/17 05:05:08] d2.utils.events INFO:  eta: 1 day, 12:32:11  iter: 4119  total_loss: 49.68  loss_ce: 0.4058  loss_mask: 0.5407  loss_dice: 3.974  loss_ce_0: 0.7155  loss_mask_0: 0.5117  loss_dice_0: 4.079  loss_ce_1: 0.3982  loss_mask_1: 0.527  loss_dice_1: 3.998  loss_ce_2: 0.4163  loss_mask_2: 0.5328  loss_dice_2: 3.978  loss_ce_3: 0.4238  loss_mask_3: 0.539  loss_dice_3: 3.975  loss_ce_4: 0.3998  loss_mask_4: 0.5366  loss_dice_4: 3.984  loss_ce_5: 0.3995  loss_mask_5: 0.5379  loss_dice_5: 3.973  loss_ce_6: 0.3918  loss_mask_6: 0.5394  loss_dice_6: 3.978  loss_ce_7: 0.4018  loss_mask_7: 0.5403  loss_dice_7: 3.973  loss_ce_8: 0.3994  loss_mask_8: 0.5423  loss_dice_8: 3.972  time: 1.5354  data_time: 0.0933  lr: 9.5871e-06  max_mem: 21234M
[01/17 05:05:39] d2.utils.events INFO:  eta: 1 day, 12:30:33  iter: 4139  total_loss: 49.78  loss_ce: 0.3737  loss_mask: 0.5445  loss_dice: 4.031  loss_ce_0: 0.6776  loss_mask_0: 0.5211  loss_dice_0: 4.114  loss_ce_1: 0.3935  loss_mask_1: 0.5404  loss_dice_1: 4.045  loss_ce_2: 0.3909  loss_mask_2: 0.541  loss_dice_2: 4.033  loss_ce_3: 0.3947  loss_mask_3: 0.5399  loss_dice_3: 4.039  loss_ce_4: 0.3811  loss_mask_4: 0.5413  loss_dice_4: 4.037  loss_ce_5: 0.374  loss_mask_5: 0.543  loss_dice_5: 4.029  loss_ce_6: 0.3669  loss_mask_6: 0.5464  loss_dice_6: 4.031  loss_ce_7: 0.3775  loss_mask_7: 0.5473  loss_dice_7: 4.032  loss_ce_8: 0.3919  loss_mask_8: 0.5419  loss_dice_8: 4.03  time: 1.5354  data_time: 0.0899  lr: 9.5851e-06  max_mem: 21234M
[01/17 05:06:10] d2.utils.events INFO:  eta: 1 day, 12:28:03  iter: 4159  total_loss: 49.65  loss_ce: 0.3529  loss_mask: 0.5502  loss_dice: 3.968  loss_ce_0: 0.7158  loss_mask_0: 0.5256  loss_dice_0: 4.081  loss_ce_1: 0.4145  loss_mask_1: 0.5419  loss_dice_1: 4.004  loss_ce_2: 0.3842  loss_mask_2: 0.5455  loss_dice_2: 3.981  loss_ce_3: 0.3835  loss_mask_3: 0.5479  loss_dice_3: 3.97  loss_ce_4: 0.3684  loss_mask_4: 0.5494  loss_dice_4: 3.969  loss_ce_5: 0.3615  loss_mask_5: 0.5496  loss_dice_5: 3.974  loss_ce_6: 0.3718  loss_mask_6: 0.5481  loss_dice_6: 3.974  loss_ce_7: 0.3699  loss_mask_7: 0.5489  loss_dice_7: 3.971  loss_ce_8: 0.3545  loss_mask_8: 0.5494  loss_dice_8: 3.976  time: 1.5353  data_time: 0.1020  lr: 9.5831e-06  max_mem: 21234M
[01/17 05:06:40] d2.utils.events INFO:  eta: 1 day, 12:24:53  iter: 4179  total_loss: 49.54  loss_ce: 0.3554  loss_mask: 0.5515  loss_dice: 3.954  loss_ce_0: 0.7075  loss_mask_0: 0.5232  loss_dice_0: 4.065  loss_ce_1: 0.3936  loss_mask_1: 0.5463  loss_dice_1: 3.981  loss_ce_2: 0.3892  loss_mask_2: 0.549  loss_dice_2: 3.963  loss_ce_3: 0.3809  loss_mask_3: 0.5505  loss_dice_3: 3.964  loss_ce_4: 0.3724  loss_mask_4: 0.5502  loss_dice_4: 3.958  loss_ce_5: 0.3686  loss_mask_5: 0.5508  loss_dice_5: 3.962  loss_ce_6: 0.3692  loss_mask_6: 0.5503  loss_dice_6: 3.959  loss_ce_7: 0.3491  loss_mask_7: 0.5512  loss_dice_7: 3.958  loss_ce_8: 0.3576  loss_mask_8: 0.5515  loss_dice_8: 3.962  time: 1.5353  data_time: 0.0967  lr: 9.5811e-06  max_mem: 21234M
[01/17 05:07:11] d2.utils.events INFO:  eta: 1 day, 12:25:39  iter: 4199  total_loss: 49.52  loss_ce: 0.3537  loss_mask: 0.5349  loss_dice: 4.001  loss_ce_0: 0.704  loss_mask_0: 0.5114  loss_dice_0: 4.086  loss_ce_1: 0.398  loss_mask_1: 0.5269  loss_dice_1: 4.023  loss_ce_2: 0.376  loss_mask_2: 0.5312  loss_dice_2: 4.016  loss_ce_3: 0.3677  loss_mask_3: 0.5319  loss_dice_3: 4.012  loss_ce_4: 0.3513  loss_mask_4: 0.5341  loss_dice_4: 4.006  loss_ce_5: 0.3632  loss_mask_5: 0.5325  loss_dice_5: 4.001  loss_ce_6: 0.3571  loss_mask_6: 0.5334  loss_dice_6: 3.998  loss_ce_7: 0.3551  loss_mask_7: 0.534  loss_dice_7: 4.001  loss_ce_8: 0.3516  loss_mask_8: 0.5358  loss_dice_8: 4.004  time: 1.5353  data_time: 0.1035  lr: 9.5791e-06  max_mem: 21234M
[01/17 05:07:42] d2.utils.events INFO:  eta: 1 day, 12:23:25  iter: 4219  total_loss: 49.27  loss_ce: 0.3595  loss_mask: 0.5453  loss_dice: 3.978  loss_ce_0: 0.7229  loss_mask_0: 0.5192  loss_dice_0: 4.075  loss_ce_1: 0.3991  loss_mask_1: 0.5404  loss_dice_1: 4.007  loss_ce_2: 0.3923  loss_mask_2: 0.5407  loss_dice_2: 3.982  loss_ce_3: 0.378  loss_mask_3: 0.5434  loss_dice_3: 3.977  loss_ce_4: 0.3634  loss_mask_4: 0.5455  loss_dice_4: 3.971  loss_ce_5: 0.3629  loss_mask_5: 0.5471  loss_dice_5: 3.977  loss_ce_6: 0.3658  loss_mask_6: 0.5459  loss_dice_6: 3.979  loss_ce_7: 0.3596  loss_mask_7: 0.5445  loss_dice_7: 3.974  loss_ce_8: 0.3544  loss_mask_8: 0.5444  loss_dice_8: 3.973  time: 1.5353  data_time: 0.0977  lr: 9.5771e-06  max_mem: 21234M
[01/17 05:08:12] d2.utils.events INFO:  eta: 1 day, 12:23:32  iter: 4239  total_loss: 49.14  loss_ce: 0.3613  loss_mask: 0.5376  loss_dice: 3.956  loss_ce_0: 0.6834  loss_mask_0: 0.5102  loss_dice_0: 4.067  loss_ce_1: 0.401  loss_mask_1: 0.5281  loss_dice_1: 3.978  loss_ce_2: 0.3713  loss_mask_2: 0.5365  loss_dice_2: 3.961  loss_ce_3: 0.3562  loss_mask_3: 0.5377  loss_dice_3: 3.955  loss_ce_4: 0.356  loss_mask_4: 0.5376  loss_dice_4: 3.961  loss_ce_5: 0.3488  loss_mask_5: 0.5348  loss_dice_5: 3.959  loss_ce_6: 0.3477  loss_mask_6: 0.5379  loss_dice_6: 3.956  loss_ce_7: 0.3563  loss_mask_7: 0.5373  loss_dice_7: 3.96  loss_ce_8: 0.3631  loss_mask_8: 0.5382  loss_dice_8: 3.955  time: 1.5353  data_time: 0.0976  lr: 9.5751e-06  max_mem: 21234M
[01/17 05:08:43] d2.utils.events INFO:  eta: 1 day, 12:21:53  iter: 4259  total_loss: 49.23  loss_ce: 0.372  loss_mask: 0.5346  loss_dice: 3.965  loss_ce_0: 0.7001  loss_mask_0: 0.5006  loss_dice_0: 4.071  loss_ce_1: 0.3983  loss_mask_1: 0.5252  loss_dice_1: 3.99  loss_ce_2: 0.39  loss_mask_2: 0.5273  loss_dice_2: 3.974  loss_ce_3: 0.3773  loss_mask_3: 0.5281  loss_dice_3: 3.961  loss_ce_4: 0.3769  loss_mask_4: 0.5291  loss_dice_4: 3.961  loss_ce_5: 0.3811  loss_mask_5: 0.5303  loss_dice_5: 3.964  loss_ce_6: 0.3611  loss_mask_6: 0.5313  loss_dice_6: 3.959  loss_ce_7: 0.3655  loss_mask_7: 0.5304  loss_dice_7: 3.962  loss_ce_8: 0.3734  loss_mask_8: 0.5332  loss_dice_8: 3.964  time: 1.5352  data_time: 0.0969  lr: 9.5731e-06  max_mem: 21234M
[01/17 05:09:13] d2.utils.events INFO:  eta: 1 day, 12:20:42  iter: 4279  total_loss: 49.2  loss_ce: 0.3778  loss_mask: 0.5514  loss_dice: 3.958  loss_ce_0: 0.7174  loss_mask_0: 0.5145  loss_dice_0: 4.064  loss_ce_1: 0.4129  loss_mask_1: 0.5389  loss_dice_1: 3.981  loss_ce_2: 0.3935  loss_mask_2: 0.5457  loss_dice_2: 3.963  loss_ce_3: 0.3901  loss_mask_3: 0.5461  loss_dice_3: 3.958  loss_ce_4: 0.3938  loss_mask_4: 0.5494  loss_dice_4: 3.947  loss_ce_5: 0.3734  loss_mask_5: 0.5497  loss_dice_5: 3.955  loss_ce_6: 0.3614  loss_mask_6: 0.5498  loss_dice_6: 3.961  loss_ce_7: 0.3719  loss_mask_7: 0.5508  loss_dice_7: 3.95  loss_ce_8: 0.3841  loss_mask_8: 0.5493  loss_dice_8: 3.952  time: 1.5352  data_time: 0.1057  lr: 9.5711e-06  max_mem: 21234M
[01/17 05:09:44] d2.utils.events INFO:  eta: 1 day, 12:19:01  iter: 4299  total_loss: 48.6  loss_ce: 0.3333  loss_mask: 0.5427  loss_dice: 3.928  loss_ce_0: 0.6973  loss_mask_0: 0.5143  loss_dice_0: 4.035  loss_ce_1: 0.3772  loss_mask_1: 0.5338  loss_dice_1: 3.963  loss_ce_2: 0.3641  loss_mask_2: 0.5404  loss_dice_2: 3.937  loss_ce_3: 0.3605  loss_mask_3: 0.5437  loss_dice_3: 3.931  loss_ce_4: 0.3386  loss_mask_4: 0.5457  loss_dice_4: 3.93  loss_ce_5: 0.3431  loss_mask_5: 0.5457  loss_dice_5: 3.928  loss_ce_6: 0.3481  loss_mask_6: 0.5456  loss_dice_6: 3.925  loss_ce_7: 0.3449  loss_mask_7: 0.5423  loss_dice_7: 3.935  loss_ce_8: 0.3325  loss_mask_8: 0.5448  loss_dice_8: 3.928  time: 1.5350  data_time: 0.0932  lr: 9.5691e-06  max_mem: 21234M
[01/17 05:10:14] d2.utils.events INFO:  eta: 1 day, 12:16:30  iter: 4319  total_loss: 49.27  loss_ce: 0.3714  loss_mask: 0.551  loss_dice: 3.971  loss_ce_0: 0.6924  loss_mask_0: 0.5165  loss_dice_0: 4.067  loss_ce_1: 0.403  loss_mask_1: 0.5424  loss_dice_1: 4  loss_ce_2: 0.4027  loss_mask_2: 0.5451  loss_dice_2: 3.986  loss_ce_3: 0.3847  loss_mask_3: 0.5452  loss_dice_3: 3.976  loss_ce_4: 0.3892  loss_mask_4: 0.5472  loss_dice_4: 3.966  loss_ce_5: 0.3888  loss_mask_5: 0.5484  loss_dice_5: 3.968  loss_ce_6: 0.3782  loss_mask_6: 0.5482  loss_dice_6: 3.966  loss_ce_7: 0.3874  loss_mask_7: 0.5503  loss_dice_7: 3.961  loss_ce_8: 0.3884  loss_mask_8: 0.5507  loss_dice_8: 3.963  time: 1.5349  data_time: 0.0917  lr: 9.567e-06  max_mem: 21234M
[01/17 05:10:44] d2.utils.events INFO:  eta: 1 day, 12:16:00  iter: 4339  total_loss: 48.9  loss_ce: 0.3663  loss_mask: 0.5326  loss_dice: 3.907  loss_ce_0: 0.7197  loss_mask_0: 0.5136  loss_dice_0: 4.014  loss_ce_1: 0.4116  loss_mask_1: 0.5308  loss_dice_1: 3.925  loss_ce_2: 0.4002  loss_mask_2: 0.5319  loss_dice_2: 3.916  loss_ce_3: 0.3886  loss_mask_3: 0.5341  loss_dice_3: 3.913  loss_ce_4: 0.3771  loss_mask_4: 0.5371  loss_dice_4: 3.919  loss_ce_5: 0.3848  loss_mask_5: 0.5377  loss_dice_5: 3.906  loss_ce_6: 0.374  loss_mask_6: 0.5346  loss_dice_6: 3.908  loss_ce_7: 0.3854  loss_mask_7: 0.5312  loss_dice_7: 3.909  loss_ce_8: 0.3792  loss_mask_8: 0.5323  loss_dice_8: 3.904  time: 1.5348  data_time: 0.0923  lr: 9.565e-06  max_mem: 21234M
[01/17 05:11:15] d2.utils.events INFO:  eta: 1 day, 12:16:00  iter: 4359  total_loss: 49.42  loss_ce: 0.3725  loss_mask: 0.5408  loss_dice: 3.965  loss_ce_0: 0.7037  loss_mask_0: 0.5091  loss_dice_0: 4.074  loss_ce_1: 0.411  loss_mask_1: 0.5305  loss_dice_1: 3.994  loss_ce_2: 0.3908  loss_mask_2: 0.533  loss_dice_2: 3.975  loss_ce_3: 0.3751  loss_mask_3: 0.5343  loss_dice_3: 3.969  loss_ce_4: 0.3784  loss_mask_4: 0.538  loss_dice_4: 3.972  loss_ce_5: 0.3662  loss_mask_5: 0.5378  loss_dice_5: 3.979  loss_ce_6: 0.3641  loss_mask_6: 0.5381  loss_dice_6: 3.965  loss_ce_7: 0.3743  loss_mask_7: 0.5413  loss_dice_7: 3.973  loss_ce_8: 0.3714  loss_mask_8: 0.5396  loss_dice_8: 3.963  time: 1.5348  data_time: 0.0869  lr: 9.563e-06  max_mem: 21234M
[01/17 05:11:45] d2.utils.events INFO:  eta: 1 day, 12:14:26  iter: 4379  total_loss: 49.43  loss_ce: 0.3881  loss_mask: 0.5561  loss_dice: 3.96  loss_ce_0: 0.6984  loss_mask_0: 0.5225  loss_dice_0: 4.052  loss_ce_1: 0.4166  loss_mask_1: 0.5516  loss_dice_1: 3.974  loss_ce_2: 0.4005  loss_mask_2: 0.5502  loss_dice_2: 3.953  loss_ce_3: 0.3969  loss_mask_3: 0.5523  loss_dice_3: 3.95  loss_ce_4: 0.3902  loss_mask_4: 0.5515  loss_dice_4: 3.951  loss_ce_5: 0.3763  loss_mask_5: 0.5559  loss_dice_5: 3.961  loss_ce_6: 0.3852  loss_mask_6: 0.5555  loss_dice_6: 3.954  loss_ce_7: 0.3869  loss_mask_7: 0.5558  loss_dice_7: 3.957  loss_ce_8: 0.3782  loss_mask_8: 0.5587  loss_dice_8: 3.961  time: 1.5347  data_time: 0.0946  lr: 9.561e-06  max_mem: 21234M
[01/17 05:12:17] d2.utils.events INFO:  eta: 1 day, 12:15:51  iter: 4399  total_loss: 49.26  loss_ce: 0.3706  loss_mask: 0.5372  loss_dice: 3.961  loss_ce_0: 0.6998  loss_mask_0: 0.5098  loss_dice_0: 4.068  loss_ce_1: 0.3962  loss_mask_1: 0.5267  loss_dice_1: 3.991  loss_ce_2: 0.3838  loss_mask_2: 0.5326  loss_dice_2: 3.97  loss_ce_3: 0.3812  loss_mask_3: 0.5352  loss_dice_3: 3.952  loss_ce_4: 0.3699  loss_mask_4: 0.5373  loss_dice_4: 3.963  loss_ce_5: 0.3724  loss_mask_5: 0.5389  loss_dice_5: 3.955  loss_ce_6: 0.3628  loss_mask_6: 0.5381  loss_dice_6: 3.961  loss_ce_7: 0.3704  loss_mask_7: 0.5409  loss_dice_7: 3.961  loss_ce_8: 0.3623  loss_mask_8: 0.5375  loss_dice_8: 3.966  time: 1.5349  data_time: 0.0997  lr: 9.559e-06  max_mem: 21234M
[01/17 05:12:48] d2.utils.events INFO:  eta: 1 day, 12:16:28  iter: 4419  total_loss: 49.6  loss_ce: 0.3727  loss_mask: 0.5367  loss_dice: 3.956  loss_ce_0: 0.7038  loss_mask_0: 0.509  loss_dice_0: 4.069  loss_ce_1: 0.4063  loss_mask_1: 0.5332  loss_dice_1: 3.979  loss_ce_2: 0.4032  loss_mask_2: 0.5358  loss_dice_2: 3.965  loss_ce_3: 0.4196  loss_mask_3: 0.5345  loss_dice_3: 3.955  loss_ce_4: 0.4104  loss_mask_4: 0.5361  loss_dice_4: 3.948  loss_ce_5: 0.3937  loss_mask_5: 0.5383  loss_dice_5: 3.955  loss_ce_6: 0.3969  loss_mask_6: 0.5373  loss_dice_6: 3.952  loss_ce_7: 0.3977  loss_mask_7: 0.5378  loss_dice_7: 3.951  loss_ce_8: 0.3832  loss_mask_8: 0.537  loss_dice_8: 3.954  time: 1.5349  data_time: 0.0963  lr: 9.557e-06  max_mem: 21234M
[01/17 05:13:19] d2.utils.events INFO:  eta: 1 day, 12:16:19  iter: 4439  total_loss: 48.98  loss_ce: 0.373  loss_mask: 0.5433  loss_dice: 3.944  loss_ce_0: 0.6655  loss_mask_0: 0.5199  loss_dice_0: 4.063  loss_ce_1: 0.3866  loss_mask_1: 0.5388  loss_dice_1: 3.981  loss_ce_2: 0.3844  loss_mask_2: 0.5421  loss_dice_2: 3.963  loss_ce_3: 0.37  loss_mask_3: 0.542  loss_dice_3: 3.951  loss_ce_4: 0.3627  loss_mask_4: 0.5417  loss_dice_4: 3.951  loss_ce_5: 0.3682  loss_mask_5: 0.5424  loss_dice_5: 3.944  loss_ce_6: 0.3636  loss_mask_6: 0.5427  loss_dice_6: 3.946  loss_ce_7: 0.3582  loss_mask_7: 0.5416  loss_dice_7: 3.948  loss_ce_8: 0.3604  loss_mask_8: 0.5414  loss_dice_8: 3.95  time: 1.5350  data_time: 0.1116  lr: 9.555e-06  max_mem: 21234M
[01/17 05:13:49] d2.utils.events INFO:  eta: 1 day, 12:15:27  iter: 4459  total_loss: 49.38  loss_ce: 0.3938  loss_mask: 0.5428  loss_dice: 3.95  loss_ce_0: 0.7365  loss_mask_0: 0.5157  loss_dice_0: 4.064  loss_ce_1: 0.4168  loss_mask_1: 0.5418  loss_dice_1: 3.985  loss_ce_2: 0.3977  loss_mask_2: 0.5424  loss_dice_2: 3.96  loss_ce_3: 0.403  loss_mask_3: 0.5451  loss_dice_3: 3.958  loss_ce_4: 0.3963  loss_mask_4: 0.5444  loss_dice_4: 3.955  loss_ce_5: 0.3967  loss_mask_5: 0.543  loss_dice_5: 3.962  loss_ce_6: 0.3876  loss_mask_6: 0.5458  loss_dice_6: 3.959  loss_ce_7: 0.404  loss_mask_7: 0.5445  loss_dice_7: 3.956  loss_ce_8: 0.4106  loss_mask_8: 0.5448  loss_dice_8: 3.954  time: 1.5350  data_time: 0.0960  lr: 9.553e-06  max_mem: 21234M
[01/17 05:14:20] d2.utils.events INFO:  eta: 1 day, 12:16:04  iter: 4479  total_loss: 49.47  loss_ce: 0.4037  loss_mask: 0.5415  loss_dice: 3.922  loss_ce_0: 0.7236  loss_mask_0: 0.5201  loss_dice_0: 4.016  loss_ce_1: 0.4341  loss_mask_1: 0.5376  loss_dice_1: 3.941  loss_ce_2: 0.4058  loss_mask_2: 0.5455  loss_dice_2: 3.92  loss_ce_3: 0.4028  loss_mask_3: 0.5441  loss_dice_3: 3.921  loss_ce_4: 0.4008  loss_mask_4: 0.5412  loss_dice_4: 3.927  loss_ce_5: 0.3901  loss_mask_5: 0.5423  loss_dice_5: 3.918  loss_ce_6: 0.3996  loss_mask_6: 0.5432  loss_dice_6: 3.92  loss_ce_7: 0.3978  loss_mask_7: 0.5448  loss_dice_7: 3.917  loss_ce_8: 0.3796  loss_mask_8: 0.5426  loss_dice_8: 3.923  time: 1.5350  data_time: 0.1036  lr: 9.551e-06  max_mem: 21234M
[01/17 05:14:51] d2.utils.events INFO:  eta: 1 day, 12:15:19  iter: 4499  total_loss: 48.75  loss_ce: 0.3657  loss_mask: 0.5208  loss_dice: 3.937  loss_ce_0: 0.6727  loss_mask_0: 0.5096  loss_dice_0: 4.039  loss_ce_1: 0.4001  loss_mask_1: 0.5253  loss_dice_1: 3.949  loss_ce_2: 0.3962  loss_mask_2: 0.52  loss_dice_2: 3.944  loss_ce_3: 0.362  loss_mask_3: 0.5195  loss_dice_3: 3.927  loss_ce_4: 0.3454  loss_mask_4: 0.5202  loss_dice_4: 3.929  loss_ce_5: 0.3584  loss_mask_5: 0.5181  loss_dice_5: 3.93  loss_ce_6: 0.3626  loss_mask_6: 0.5196  loss_dice_6: 3.933  loss_ce_7: 0.3628  loss_mask_7: 0.5205  loss_dice_7: 3.93  loss_ce_8: 0.3483  loss_mask_8: 0.5212  loss_dice_8: 3.925  time: 1.5351  data_time: 0.1035  lr: 9.549e-06  max_mem: 21234M
[01/17 05:15:22] d2.utils.events INFO:  eta: 1 day, 12:14:35  iter: 4519  total_loss: 48.56  loss_ce: 0.3705  loss_mask: 0.5409  loss_dice: 3.893  loss_ce_0: 0.7136  loss_mask_0: 0.5089  loss_dice_0: 4.01  loss_ce_1: 0.4185  loss_mask_1: 0.5318  loss_dice_1: 3.925  loss_ce_2: 0.3802  loss_mask_2: 0.5387  loss_dice_2: 3.902  loss_ce_3: 0.3921  loss_mask_3: 0.5377  loss_dice_3: 3.888  loss_ce_4: 0.3661  loss_mask_4: 0.5431  loss_dice_4: 3.893  loss_ce_5: 0.3776  loss_mask_5: 0.541  loss_dice_5: 3.895  loss_ce_6: 0.3775  loss_mask_6: 0.5419  loss_dice_6: 3.885  loss_ce_7: 0.3708  loss_mask_7: 0.5396  loss_dice_7: 3.892  loss_ce_8: 0.3643  loss_mask_8: 0.5386  loss_dice_8: 3.893  time: 1.5351  data_time: 0.1019  lr: 9.5469e-06  max_mem: 21234M
[01/17 05:15:53] d2.utils.events INFO:  eta: 1 day, 12:13:47  iter: 4539  total_loss: 48.95  loss_ce: 0.3435  loss_mask: 0.5426  loss_dice: 3.946  loss_ce_0: 0.7029  loss_mask_0: 0.522  loss_dice_0: 4.048  loss_ce_1: 0.3968  loss_mask_1: 0.5393  loss_dice_1: 3.971  loss_ce_2: 0.3846  loss_mask_2: 0.5422  loss_dice_2: 3.947  loss_ce_3: 0.3676  loss_mask_3: 0.5397  loss_dice_3: 3.946  loss_ce_4: 0.3676  loss_mask_4: 0.5358  loss_dice_4: 3.956  loss_ce_5: 0.3546  loss_mask_5: 0.5395  loss_dice_5: 3.946  loss_ce_6: 0.3487  loss_mask_6: 0.5369  loss_dice_6: 3.951  loss_ce_7: 0.3485  loss_mask_7: 0.5397  loss_dice_7: 3.946  loss_ce_8: 0.3602  loss_mask_8: 0.5415  loss_dice_8: 3.948  time: 1.5351  data_time: 0.1038  lr: 9.5449e-06  max_mem: 21234M
[01/17 05:16:25] d2.utils.events INFO:  eta: 1 day, 12:13:00  iter: 4559  total_loss: 49.02  loss_ce: 0.3938  loss_mask: 0.5273  loss_dice: 3.94  loss_ce_0: 0.6969  loss_mask_0: 0.5005  loss_dice_0: 4.051  loss_ce_1: 0.4291  loss_mask_1: 0.5223  loss_dice_1: 3.974  loss_ce_2: 0.4227  loss_mask_2: 0.5261  loss_dice_2: 3.951  loss_ce_3: 0.3929  loss_mask_3: 0.5274  loss_dice_3: 3.944  loss_ce_4: 0.3979  loss_mask_4: 0.5289  loss_dice_4: 3.938  loss_ce_5: 0.3921  loss_mask_5: 0.53  loss_dice_5: 3.945  loss_ce_6: 0.3773  loss_mask_6: 0.5279  loss_dice_6: 3.941  loss_ce_7: 0.3907  loss_mask_7: 0.5276  loss_dice_7: 3.945  loss_ce_8: 0.3871  loss_mask_8: 0.5281  loss_dice_8: 3.942  time: 1.5353  data_time: 0.1047  lr: 9.5429e-06  max_mem: 21234M
[01/17 05:16:56] d2.utils.events INFO:  eta: 1 day, 12:13:17  iter: 4579  total_loss: 49.28  loss_ce: 0.3876  loss_mask: 0.5432  loss_dice: 3.98  loss_ce_0: 0.7011  loss_mask_0: 0.5192  loss_dice_0: 4.071  loss_ce_1: 0.4271  loss_mask_1: 0.5374  loss_dice_1: 4.006  loss_ce_2: 0.4025  loss_mask_2: 0.5395  loss_dice_2: 3.986  loss_ce_3: 0.3902  loss_mask_3: 0.5398  loss_dice_3: 3.984  loss_ce_4: 0.3935  loss_mask_4: 0.5421  loss_dice_4: 3.977  loss_ce_5: 0.3862  loss_mask_5: 0.5434  loss_dice_5: 3.978  loss_ce_6: 0.3968  loss_mask_6: 0.5434  loss_dice_6: 3.979  loss_ce_7: 0.3983  loss_mask_7: 0.543  loss_dice_7: 3.974  loss_ce_8: 0.387  loss_mask_8: 0.5423  loss_dice_8: 3.983  time: 1.5353  data_time: 0.0876  lr: 9.5409e-06  max_mem: 21234M
[01/17 05:17:26] d2.utils.events INFO:  eta: 1 day, 12:11:59  iter: 4599  total_loss: 49.05  loss_ce: 0.3821  loss_mask: 0.5367  loss_dice: 3.927  loss_ce_0: 0.6906  loss_mask_0: 0.5132  loss_dice_0: 4.03  loss_ce_1: 0.4241  loss_mask_1: 0.535  loss_dice_1: 3.954  loss_ce_2: 0.388  loss_mask_2: 0.5356  loss_dice_2: 3.933  loss_ce_3: 0.3747  loss_mask_3: 0.5345  loss_dice_3: 3.931  loss_ce_4: 0.3837  loss_mask_4: 0.5331  loss_dice_4: 3.921  loss_ce_5: 0.3744  loss_mask_5: 0.5363  loss_dice_5: 3.929  loss_ce_6: 0.3797  loss_mask_6: 0.5356  loss_dice_6: 3.922  loss_ce_7: 0.3662  loss_mask_7: 0.5375  loss_dice_7: 3.927  loss_ce_8: 0.3892  loss_mask_8: 0.538  loss_dice_8: 3.923  time: 1.5353  data_time: 0.0950  lr: 9.5389e-06  max_mem: 21234M
[01/17 05:17:57] d2.utils.events INFO:  eta: 1 day, 12:12:02  iter: 4619  total_loss: 48.72  loss_ce: 0.3666  loss_mask: 0.5424  loss_dice: 3.889  loss_ce_0: 0.656  loss_mask_0: 0.5087  loss_dice_0: 4.013  loss_ce_1: 0.3927  loss_mask_1: 0.5313  loss_dice_1: 3.919  loss_ce_2: 0.3682  loss_mask_2: 0.5391  loss_dice_2: 3.901  loss_ce_3: 0.3673  loss_mask_3: 0.5393  loss_dice_3: 3.899  loss_ce_4: 0.3613  loss_mask_4: 0.5414  loss_dice_4: 3.897  loss_ce_5: 0.3816  loss_mask_5: 0.5404  loss_dice_5: 3.894  loss_ce_6: 0.355  loss_mask_6: 0.5432  loss_dice_6: 3.889  loss_ce_7: 0.353  loss_mask_7: 0.5436  loss_dice_7: 3.897  loss_ce_8: 0.3548  loss_mask_8: 0.5445  loss_dice_8: 3.896  time: 1.5353  data_time: 0.0921  lr: 9.5369e-06  max_mem: 21234M
[01/17 05:18:28] d2.utils.events INFO:  eta: 1 day, 12:11:32  iter: 4639  total_loss: 48.85  loss_ce: 0.3642  loss_mask: 0.5196  loss_dice: 3.95  loss_ce_0: 0.6865  loss_mask_0: 0.4864  loss_dice_0: 4.058  loss_ce_1: 0.3856  loss_mask_1: 0.5108  loss_dice_1: 3.972  loss_ce_2: 0.3862  loss_mask_2: 0.5114  loss_dice_2: 3.951  loss_ce_3: 0.3646  loss_mask_3: 0.5142  loss_dice_3: 3.953  loss_ce_4: 0.375  loss_mask_4: 0.5144  loss_dice_4: 3.946  loss_ce_5: 0.376  loss_mask_5: 0.5133  loss_dice_5: 3.948  loss_ce_6: 0.3678  loss_mask_6: 0.5161  loss_dice_6: 3.947  loss_ce_7: 0.3678  loss_mask_7: 0.516  loss_dice_7: 3.948  loss_ce_8: 0.3593  loss_mask_8: 0.5185  loss_dice_8: 3.948  time: 1.5354  data_time: 0.0966  lr: 9.5349e-06  max_mem: 21234M
[01/17 05:18:59] d2.utils.events INFO:  eta: 1 day, 12:11:16  iter: 4659  total_loss: 48.7  loss_ce: 0.3753  loss_mask: 0.5301  loss_dice: 3.906  loss_ce_0: 0.6983  loss_mask_0: 0.515  loss_dice_0: 4.012  loss_ce_1: 0.3875  loss_mask_1: 0.5302  loss_dice_1: 3.922  loss_ce_2: 0.3705  loss_mask_2: 0.5312  loss_dice_2: 3.907  loss_ce_3: 0.3727  loss_mask_3: 0.5299  loss_dice_3: 3.899  loss_ce_4: 0.3631  loss_mask_4: 0.5308  loss_dice_4: 3.898  loss_ce_5: 0.3622  loss_mask_5: 0.5322  loss_dice_5: 3.9  loss_ce_6: 0.3725  loss_mask_6: 0.5317  loss_dice_6: 3.893  loss_ce_7: 0.369  loss_mask_7: 0.5298  loss_dice_7: 3.894  loss_ce_8: 0.3674  loss_mask_8: 0.5295  loss_dice_8: 3.896  time: 1.5354  data_time: 0.0992  lr: 9.5329e-06  max_mem: 21234M
[01/17 05:19:30] d2.utils.events INFO:  eta: 1 day, 12:09:35  iter: 4679  total_loss: 49.02  loss_ce: 0.378  loss_mask: 0.535  loss_dice: 3.951  loss_ce_0: 0.6507  loss_mask_0: 0.5132  loss_dice_0: 4.039  loss_ce_1: 0.3975  loss_mask_1: 0.5314  loss_dice_1: 3.972  loss_ce_2: 0.405  loss_mask_2: 0.5346  loss_dice_2: 3.948  loss_ce_3: 0.3884  loss_mask_3: 0.5334  loss_dice_3: 3.949  loss_ce_4: 0.3916  loss_mask_4: 0.5357  loss_dice_4: 3.946  loss_ce_5: 0.3815  loss_mask_5: 0.5341  loss_dice_5: 3.954  loss_ce_6: 0.3857  loss_mask_6: 0.5341  loss_dice_6: 3.951  loss_ce_7: 0.379  loss_mask_7: 0.5367  loss_dice_7: 3.948  loss_ce_8: 0.3746  loss_mask_8: 0.5368  loss_dice_8: 3.946  time: 1.5354  data_time: 0.1046  lr: 9.5309e-06  max_mem: 21297M
[01/17 05:20:01] d2.utils.events INFO:  eta: 1 day, 12:09:04  iter: 4699  total_loss: 48.59  loss_ce: 0.3452  loss_mask: 0.538  loss_dice: 3.927  loss_ce_0: 0.6546  loss_mask_0: 0.5124  loss_dice_0: 4.03  loss_ce_1: 0.3681  loss_mask_1: 0.535  loss_dice_1: 3.935  loss_ce_2: 0.3714  loss_mask_2: 0.5412  loss_dice_2: 3.927  loss_ce_3: 0.3629  loss_mask_3: 0.5407  loss_dice_3: 3.917  loss_ce_4: 0.3559  loss_mask_4: 0.5393  loss_dice_4: 3.917  loss_ce_5: 0.3719  loss_mask_5: 0.5376  loss_dice_5: 3.915  loss_ce_6: 0.3698  loss_mask_6: 0.5385  loss_dice_6: 3.915  loss_ce_7: 0.3678  loss_mask_7: 0.5386  loss_dice_7: 3.915  loss_ce_8: 0.356  loss_mask_8: 0.5379  loss_dice_8: 3.914  time: 1.5354  data_time: 0.1104  lr: 9.5288e-06  max_mem: 21297M
[01/17 05:20:31] d2.utils.events INFO:  eta: 1 day, 12:08:34  iter: 4719  total_loss: 49.15  loss_ce: 0.378  loss_mask: 0.5309  loss_dice: 3.965  loss_ce_0: 0.693  loss_mask_0: 0.5057  loss_dice_0: 4.061  loss_ce_1: 0.3855  loss_mask_1: 0.5256  loss_dice_1: 3.999  loss_ce_2: 0.3717  loss_mask_2: 0.5274  loss_dice_2: 3.985  loss_ce_3: 0.3887  loss_mask_3: 0.5296  loss_dice_3: 3.969  loss_ce_4: 0.3845  loss_mask_4: 0.5285  loss_dice_4: 3.972  loss_ce_5: 0.3669  loss_mask_5: 0.5296  loss_dice_5: 3.969  loss_ce_6: 0.3758  loss_mask_6: 0.529  loss_dice_6: 3.968  loss_ce_7: 0.3865  loss_mask_7: 0.5297  loss_dice_7: 3.969  loss_ce_8: 0.3803  loss_mask_8: 0.5296  loss_dice_8: 3.976  time: 1.5354  data_time: 0.0958  lr: 9.5268e-06  max_mem: 21297M
[01/17 05:21:02] d2.utils.events INFO:  eta: 1 day, 12:08:59  iter: 4739  total_loss: 48.65  loss_ce: 0.3526  loss_mask: 0.5394  loss_dice: 3.924  loss_ce_0: 0.6436  loss_mask_0: 0.5197  loss_dice_0: 4.027  loss_ce_1: 0.3645  loss_mask_1: 0.5387  loss_dice_1: 3.949  loss_ce_2: 0.3444  loss_mask_2: 0.5426  loss_dice_2: 3.933  loss_ce_3: 0.3481  loss_mask_3: 0.5431  loss_dice_3: 3.927  loss_ce_4: 0.3524  loss_mask_4: 0.5439  loss_dice_4: 3.926  loss_ce_5: 0.345  loss_mask_5: 0.5428  loss_dice_5: 3.929  loss_ce_6: 0.3429  loss_mask_6: 0.5422  loss_dice_6: 3.925  loss_ce_7: 0.3464  loss_mask_7: 0.5441  loss_dice_7: 3.926  loss_ce_8: 0.3513  loss_mask_8: 0.5424  loss_dice_8: 3.926  time: 1.5354  data_time: 0.1112  lr: 9.5248e-06  max_mem: 21297M
[01/17 05:21:32] d2.utils.events INFO:  eta: 1 day, 12:07:11  iter: 4759  total_loss: 48.45  loss_ce: 0.3676  loss_mask: 0.5317  loss_dice: 3.892  loss_ce_0: 0.6572  loss_mask_0: 0.5056  loss_dice_0: 4.015  loss_ce_1: 0.373  loss_mask_1: 0.5248  loss_dice_1: 3.936  loss_ce_2: 0.3783  loss_mask_2: 0.5286  loss_dice_2: 3.91  loss_ce_3: 0.3734  loss_mask_3: 0.5269  loss_dice_3: 3.9  loss_ce_4: 0.3679  loss_mask_4: 0.5278  loss_dice_4: 3.904  loss_ce_5: 0.3709  loss_mask_5: 0.5305  loss_dice_5: 3.896  loss_ce_6: 0.3718  loss_mask_6: 0.5296  loss_dice_6: 3.892  loss_ce_7: 0.3582  loss_mask_7: 0.5311  loss_dice_7: 3.898  loss_ce_8: 0.3815  loss_mask_8: 0.5327  loss_dice_8: 3.892  time: 1.5353  data_time: 0.0896  lr: 9.5228e-06  max_mem: 21297M
[01/17 05:22:03] d2.utils.events INFO:  eta: 1 day, 12:07:58  iter: 4779  total_loss: 48.58  loss_ce: 0.3685  loss_mask: 0.5383  loss_dice: 3.897  loss_ce_0: 0.7048  loss_mask_0: 0.5071  loss_dice_0: 3.998  loss_ce_1: 0.3845  loss_mask_1: 0.536  loss_dice_1: 3.921  loss_ce_2: 0.3964  loss_mask_2: 0.5347  loss_dice_2: 3.902  loss_ce_3: 0.3782  loss_mask_3: 0.5363  loss_dice_3: 3.905  loss_ce_4: 0.3756  loss_mask_4: 0.5376  loss_dice_4: 3.903  loss_ce_5: 0.3724  loss_mask_5: 0.5354  loss_dice_5: 3.898  loss_ce_6: 0.3744  loss_mask_6: 0.5384  loss_dice_6: 3.903  loss_ce_7: 0.3707  loss_mask_7: 0.538  loss_dice_7: 3.9  loss_ce_8: 0.3729  loss_mask_8: 0.5363  loss_dice_8: 3.901  time: 1.5353  data_time: 0.0893  lr: 9.5208e-06  max_mem: 21297M
[01/17 05:22:34] d2.utils.events INFO:  eta: 1 day, 12:07:27  iter: 4799  total_loss: 48.48  loss_ce: 0.3833  loss_mask: 0.5323  loss_dice: 3.943  loss_ce_0: 0.6632  loss_mask_0: 0.5038  loss_dice_0: 4.031  loss_ce_1: 0.4013  loss_mask_1: 0.5263  loss_dice_1: 3.969  loss_ce_2: 0.3903  loss_mask_2: 0.5305  loss_dice_2: 3.944  loss_ce_3: 0.3661  loss_mask_3: 0.5306  loss_dice_3: 3.94  loss_ce_4: 0.386  loss_mask_4: 0.5295  loss_dice_4: 3.936  loss_ce_5: 0.3743  loss_mask_5: 0.5316  loss_dice_5: 3.939  loss_ce_6: 0.3814  loss_mask_6: 0.5297  loss_dice_6: 3.933  loss_ce_7: 0.3851  loss_mask_7: 0.5314  loss_dice_7: 3.935  loss_ce_8: 0.3821  loss_mask_8: 0.5322  loss_dice_8: 3.933  time: 1.5353  data_time: 0.0937  lr: 9.5188e-06  max_mem: 21297M
[01/17 05:23:05] d2.utils.events INFO:  eta: 1 day, 12:07:29  iter: 4819  total_loss: 48.88  loss_ce: 0.3788  loss_mask: 0.5335  loss_dice: 3.928  loss_ce_0: 0.6711  loss_mask_0: 0.5041  loss_dice_0: 4.023  loss_ce_1: 0.4152  loss_mask_1: 0.5263  loss_dice_1: 3.947  loss_ce_2: 0.4067  loss_mask_2: 0.5285  loss_dice_2: 3.926  loss_ce_3: 0.3983  loss_mask_3: 0.5322  loss_dice_3: 3.906  loss_ce_4: 0.3954  loss_mask_4: 0.5334  loss_dice_4: 3.912  loss_ce_5: 0.3699  loss_mask_5: 0.5349  loss_dice_5: 3.919  loss_ce_6: 0.38  loss_mask_6: 0.5336  loss_dice_6: 3.925  loss_ce_7: 0.3743  loss_mask_7: 0.5349  loss_dice_7: 3.921  loss_ce_8: 0.3843  loss_mask_8: 0.5378  loss_dice_8: 3.912  time: 1.5354  data_time: 0.0858  lr: 9.5168e-06  max_mem: 21297M
[01/17 05:23:35] d2.utils.events INFO:  eta: 1 day, 12:06:41  iter: 4839  total_loss: 48.93  loss_ce: 0.4059  loss_mask: 0.5448  loss_dice: 3.879  loss_ce_0: 0.7181  loss_mask_0: 0.5176  loss_dice_0: 3.979  loss_ce_1: 0.4013  loss_mask_1: 0.5414  loss_dice_1: 3.907  loss_ce_2: 0.407  loss_mask_2: 0.5439  loss_dice_2: 3.895  loss_ce_3: 0.4049  loss_mask_3: 0.5435  loss_dice_3: 3.88  loss_ce_4: 0.3935  loss_mask_4: 0.545  loss_dice_4: 3.88  loss_ce_5: 0.404  loss_mask_5: 0.542  loss_dice_5: 3.875  loss_ce_6: 0.3945  loss_mask_6: 0.5448  loss_dice_6: 3.871  loss_ce_7: 0.405  loss_mask_7: 0.543  loss_dice_7: 3.88  loss_ce_8: 0.4042  loss_mask_8: 0.5452  loss_dice_8: 3.883  time: 1.5352  data_time: 0.0921  lr: 9.5148e-06  max_mem: 21297M
[01/17 05:24:06] d2.utils.events INFO:  eta: 1 day, 12:06:28  iter: 4859  total_loss: 48.74  loss_ce: 0.3632  loss_mask: 0.5169  loss_dice: 3.924  loss_ce_0: 0.669  loss_mask_0: 0.4897  loss_dice_0: 4.027  loss_ce_1: 0.3969  loss_mask_1: 0.5139  loss_dice_1: 3.952  loss_ce_2: 0.3875  loss_mask_2: 0.5172  loss_dice_2: 3.928  loss_ce_3: 0.3761  loss_mask_3: 0.5142  loss_dice_3: 3.926  loss_ce_4: 0.3909  loss_mask_4: 0.5163  loss_dice_4: 3.923  loss_ce_5: 0.388  loss_mask_5: 0.5183  loss_dice_5: 3.928  loss_ce_6: 0.3694  loss_mask_6: 0.5207  loss_dice_6: 3.923  loss_ce_7: 0.3798  loss_mask_7: 0.5206  loss_dice_7: 3.926  loss_ce_8: 0.3632  loss_mask_8: 0.5195  loss_dice_8: 3.925  time: 1.5353  data_time: 0.0879  lr: 9.5128e-06  max_mem: 21297M
[01/17 05:24:36] d2.utils.events INFO:  eta: 1 day, 12:05:40  iter: 4879  total_loss: 47.91  loss_ce: 0.3626  loss_mask: 0.5436  loss_dice: 3.837  loss_ce_0: 0.653  loss_mask_0: 0.5159  loss_dice_0: 3.938  loss_ce_1: 0.3822  loss_mask_1: 0.5424  loss_dice_1: 3.869  loss_ce_2: 0.3854  loss_mask_2: 0.5399  loss_dice_2: 3.844  loss_ce_3: 0.3701  loss_mask_3: 0.5439  loss_dice_3: 3.833  loss_ce_4: 0.3658  loss_mask_4: 0.5425  loss_dice_4: 3.836  loss_ce_5: 0.3662  loss_mask_5: 0.5402  loss_dice_5: 3.834  loss_ce_6: 0.3503  loss_mask_6: 0.5428  loss_dice_6: 3.844  loss_ce_7: 0.3603  loss_mask_7: 0.541  loss_dice_7: 3.837  loss_ce_8: 0.3588  loss_mask_8: 0.5381  loss_dice_8: 3.842  time: 1.5352  data_time: 0.0926  lr: 9.5108e-06  max_mem: 21297M
[01/17 05:25:07] d2.utils.events INFO:  eta: 1 day, 12:05:27  iter: 4899  total_loss: 48.07  loss_ce: 0.3425  loss_mask: 0.5271  loss_dice: 3.87  loss_ce_0: 0.6605  loss_mask_0: 0.5003  loss_dice_0: 3.986  loss_ce_1: 0.3699  loss_mask_1: 0.5257  loss_dice_1: 3.902  loss_ce_2: 0.3729  loss_mask_2: 0.5245  loss_dice_2: 3.878  loss_ce_3: 0.3581  loss_mask_3: 0.523  loss_dice_3: 3.875  loss_ce_4: 0.3523  loss_mask_4: 0.5261  loss_dice_4: 3.871  loss_ce_5: 0.3604  loss_mask_5: 0.5239  loss_dice_5: 3.876  loss_ce_6: 0.3483  loss_mask_6: 0.5235  loss_dice_6: 3.87  loss_ce_7: 0.3519  loss_mask_7: 0.5243  loss_dice_7: 3.867  loss_ce_8: 0.3476  loss_mask_8: 0.5275  loss_dice_8: 3.874  time: 1.5353  data_time: 0.0923  lr: 9.5087e-06  max_mem: 21297M
[01/17 05:25:38] d2.utils.events INFO:  eta: 1 day, 12:04:48  iter: 4919  total_loss: 48.08  loss_ce: 0.3501  loss_mask: 0.5396  loss_dice: 3.849  loss_ce_0: 0.6698  loss_mask_0: 0.5102  loss_dice_0: 3.964  loss_ce_1: 0.3771  loss_mask_1: 0.5372  loss_dice_1: 3.88  loss_ce_2: 0.393  loss_mask_2: 0.5382  loss_dice_2: 3.856  loss_ce_3: 0.3629  loss_mask_3: 0.5401  loss_dice_3: 3.846  loss_ce_4: 0.3622  loss_mask_4: 0.537  loss_dice_4: 3.848  loss_ce_5: 0.3633  loss_mask_5: 0.5398  loss_dice_5: 3.843  loss_ce_6: 0.3507  loss_mask_6: 0.5399  loss_dice_6: 3.847  loss_ce_7: 0.3507  loss_mask_7: 0.541  loss_dice_7: 3.842  loss_ce_8: 0.341  loss_mask_8: 0.542  loss_dice_8: 3.841  time: 1.5352  data_time: 0.0918  lr: 9.5067e-06  max_mem: 21297M
[01/17 05:26:08] d2.utils.events INFO:  eta: 1 day, 12:05:03  iter: 4939  total_loss: 48.62  loss_ce: 0.3884  loss_mask: 0.5328  loss_dice: 3.886  loss_ce_0: 0.6656  loss_mask_0: 0.5166  loss_dice_0: 3.985  loss_ce_1: 0.4021  loss_mask_1: 0.5316  loss_dice_1: 3.91  loss_ce_2: 0.3948  loss_mask_2: 0.5291  loss_dice_2: 3.888  loss_ce_3: 0.3783  loss_mask_3: 0.5327  loss_dice_3: 3.879  loss_ce_4: 0.3955  loss_mask_4: 0.5319  loss_dice_4: 3.885  loss_ce_5: 0.3869  loss_mask_5: 0.5329  loss_dice_5: 3.885  loss_ce_6: 0.388  loss_mask_6: 0.5326  loss_dice_6: 3.877  loss_ce_7: 0.3787  loss_mask_7: 0.5329  loss_dice_7: 3.888  loss_ce_8: 0.3855  loss_mask_8: 0.5317  loss_dice_8: 3.883  time: 1.5351  data_time: 0.0932  lr: 9.5047e-06  max_mem: 21297M
[01/17 05:26:39] d2.utils.events INFO:  eta: 1 day, 12:05:55  iter: 4959  total_loss: 48.2  loss_ce: 0.3731  loss_mask: 0.5265  loss_dice: 3.87  loss_ce_0: 0.664  loss_mask_0: 0.499  loss_dice_0: 3.985  loss_ce_1: 0.4081  loss_mask_1: 0.5243  loss_dice_1: 3.898  loss_ce_2: 0.3977  loss_mask_2: 0.5274  loss_dice_2: 3.882  loss_ce_3: 0.3847  loss_mask_3: 0.5246  loss_dice_3: 3.871  loss_ce_4: 0.375  loss_mask_4: 0.5264  loss_dice_4: 3.877  loss_ce_5: 0.372  loss_mask_5: 0.5285  loss_dice_5: 3.87  loss_ce_6: 0.3632  loss_mask_6: 0.5247  loss_dice_6: 3.87  loss_ce_7: 0.3777  loss_mask_7: 0.5239  loss_dice_7: 3.878  loss_ce_8: 0.3721  loss_mask_8: 0.5252  loss_dice_8: 3.87  time: 1.5351  data_time: 0.0940  lr: 9.5027e-06  max_mem: 21297M
[01/17 05:27:10] d2.utils.events INFO:  eta: 1 day, 12:06:28  iter: 4979  total_loss: 48.3  loss_ce: 0.3638  loss_mask: 0.5114  loss_dice: 3.912  loss_ce_0: 0.6598  loss_mask_0: 0.4953  loss_dice_0: 4.015  loss_ce_1: 0.3804  loss_mask_1: 0.5059  loss_dice_1: 3.943  loss_ce_2: 0.3877  loss_mask_2: 0.5047  loss_dice_2: 3.918  loss_ce_3: 0.3796  loss_mask_3: 0.5059  loss_dice_3: 3.909  loss_ce_4: 0.3756  loss_mask_4: 0.5081  loss_dice_4: 3.902  loss_ce_5: 0.3791  loss_mask_5: 0.512  loss_dice_5: 3.902  loss_ce_6: 0.377  loss_mask_6: 0.5135  loss_dice_6: 3.914  loss_ce_7: 0.3766  loss_mask_7: 0.5127  loss_dice_7: 3.913  loss_ce_8: 0.3708  loss_mask_8: 0.5118  loss_dice_8: 3.919  time: 1.5351  data_time: 0.0913  lr: 9.5007e-06  max_mem: 21297M
[01/17 05:27:40] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla/model_0004999.pth
[01/17 05:27:41] d2.utils.events INFO:  eta: 1 day, 12:07:40  iter: 4999  total_loss: 48.58  loss_ce: 0.4085  loss_mask: 0.5336  loss_dice: 3.844  loss_ce_0: 0.6738  loss_mask_0: 0.5148  loss_dice_0: 3.955  loss_ce_1: 0.4249  loss_mask_1: 0.5367  loss_dice_1: 3.874  loss_ce_2: 0.4427  loss_mask_2: 0.5392  loss_dice_2: 3.851  loss_ce_3: 0.4136  loss_mask_3: 0.5373  loss_dice_3: 3.85  loss_ce_4: 0.4221  loss_mask_4: 0.5356  loss_dice_4: 3.841  loss_ce_5: 0.3999  loss_mask_5: 0.5331  loss_dice_5: 3.851  loss_ce_6: 0.4222  loss_mask_6: 0.5339  loss_dice_6: 3.847  loss_ce_7: 0.4089  loss_mask_7: 0.5325  loss_dice_7: 3.847  loss_ce_8: 0.4205  loss_mask_8: 0.5355  loss_dice_8: 3.837  time: 1.5350  data_time: 0.0945  lr: 9.4987e-06  max_mem: 21297M
[01/17 05:28:12] d2.utils.events INFO:  eta: 1 day, 12:06:02  iter: 5019  total_loss: 48.69  loss_ce: 0.4062  loss_mask: 0.5315  loss_dice: 3.89  loss_ce_0: 0.6654  loss_mask_0: 0.5023  loss_dice_0: 4.001  loss_ce_1: 0.4016  loss_mask_1: 0.5248  loss_dice_1: 3.931  loss_ce_2: 0.4153  loss_mask_2: 0.5304  loss_dice_2: 3.898  loss_ce_3: 0.386  loss_mask_3: 0.529  loss_dice_3: 3.894  loss_ce_4: 0.3767  loss_mask_4: 0.5337  loss_dice_4: 3.894  loss_ce_5: 0.3917  loss_mask_5: 0.529  loss_dice_5: 3.895  loss_ce_6: 0.4059  loss_mask_6: 0.5292  loss_dice_6: 3.89  loss_ce_7: 0.4022  loss_mask_7: 0.5303  loss_dice_7: 3.888  loss_ce_8: 0.4076  loss_mask_8: 0.5331  loss_dice_8: 3.891  time: 1.5349  data_time: 0.0973  lr: 9.4967e-06  max_mem: 21297M
[01/17 05:28:42] d2.utils.events INFO:  eta: 1 day, 12:03:53  iter: 5039  total_loss: 48.74  loss_ce: 0.3906  loss_mask: 0.5214  loss_dice: 3.9  loss_ce_0: 0.6729  loss_mask_0: 0.5045  loss_dice_0: 3.999  loss_ce_1: 0.4148  loss_mask_1: 0.5259  loss_dice_1: 3.934  loss_ce_2: 0.4327  loss_mask_2: 0.5273  loss_dice_2: 3.907  loss_ce_3: 0.399  loss_mask_3: 0.5278  loss_dice_3: 3.909  loss_ce_4: 0.4043  loss_mask_4: 0.5214  loss_dice_4: 3.898  loss_ce_5: 0.3889  loss_mask_5: 0.5224  loss_dice_5: 3.906  loss_ce_6: 0.4061  loss_mask_6: 0.5226  loss_dice_6: 3.899  loss_ce_7: 0.4029  loss_mask_7: 0.5232  loss_dice_7: 3.9  loss_ce_8: 0.3907  loss_mask_8: 0.5211  loss_dice_8: 3.903  time: 1.5349  data_time: 0.0919  lr: 9.4947e-06  max_mem: 21297M
[01/17 05:29:13] d2.utils.events INFO:  eta: 1 day, 12:03:23  iter: 5059  total_loss: 48.66  loss_ce: 0.3901  loss_mask: 0.5258  loss_dice: 3.884  loss_ce_0: 0.7081  loss_mask_0: 0.503  loss_dice_0: 4.004  loss_ce_1: 0.4377  loss_mask_1: 0.5168  loss_dice_1: 3.92  loss_ce_2: 0.4013  loss_mask_2: 0.5181  loss_dice_2: 3.897  loss_ce_3: 0.4113  loss_mask_3: 0.5214  loss_dice_3: 3.885  loss_ce_4: 0.4081  loss_mask_4: 0.5227  loss_dice_4: 3.879  loss_ce_5: 0.397  loss_mask_5: 0.5256  loss_dice_5: 3.883  loss_ce_6: 0.4101  loss_mask_6: 0.5242  loss_dice_6: 3.882  loss_ce_7: 0.3966  loss_mask_7: 0.5247  loss_dice_7: 3.884  loss_ce_8: 0.3945  loss_mask_8: 0.5277  loss_dice_8: 3.883  time: 1.5350  data_time: 0.0983  lr: 9.4926e-06  max_mem: 21297M
[01/17 05:29:44] d2.utils.events INFO:  eta: 1 day, 12:03:20  iter: 5079  total_loss: 48.23  loss_ce: 0.3717  loss_mask: 0.5377  loss_dice: 3.859  loss_ce_0: 0.6526  loss_mask_0: 0.5123  loss_dice_0: 3.957  loss_ce_1: 0.3892  loss_mask_1: 0.5369  loss_dice_1: 3.88  loss_ce_2: 0.3742  loss_mask_2: 0.5376  loss_dice_2: 3.861  loss_ce_3: 0.3716  loss_mask_3: 0.5369  loss_dice_3: 3.864  loss_ce_4: 0.381  loss_mask_4: 0.538  loss_dice_4: 3.861  loss_ce_5: 0.3816  loss_mask_5: 0.5408  loss_dice_5: 3.861  loss_ce_6: 0.3755  loss_mask_6: 0.541  loss_dice_6: 3.854  loss_ce_7: 0.368  loss_mask_7: 0.5408  loss_dice_7: 3.857  loss_ce_8: 0.3814  loss_mask_8: 0.5395  loss_dice_8: 3.857  time: 1.5350  data_time: 0.1019  lr: 9.4906e-06  max_mem: 21297M
[01/17 05:30:15] d2.utils.events INFO:  eta: 1 day, 12:04:12  iter: 5099  total_loss: 47.86  loss_ce: 0.3823  loss_mask: 0.5346  loss_dice: 3.863  loss_ce_0: 0.6779  loss_mask_0: 0.5129  loss_dice_0: 3.962  loss_ce_1: 0.3832  loss_mask_1: 0.5339  loss_dice_1: 3.881  loss_ce_2: 0.388  loss_mask_2: 0.5347  loss_dice_2: 3.865  loss_ce_3: 0.3833  loss_mask_3: 0.5356  loss_dice_3: 3.851  loss_ce_4: 0.3657  loss_mask_4: 0.5287  loss_dice_4: 3.858  loss_ce_5: 0.3746  loss_mask_5: 0.5279  loss_dice_5: 3.863  loss_ce_6: 0.3825  loss_mask_6: 0.5311  loss_dice_6: 3.861  loss_ce_7: 0.357  loss_mask_7: 0.53  loss_dice_7: 3.857  loss_ce_8: 0.3624  loss_mask_8: 0.5334  loss_dice_8: 3.854  time: 1.5350  data_time: 0.0858  lr: 9.4886e-06  max_mem: 21297M
[01/17 05:30:45] d2.utils.events INFO:  eta: 1 day, 12:04:05  iter: 5119  total_loss: 48.32  loss_ce: 0.3668  loss_mask: 0.5319  loss_dice: 3.887  loss_ce_0: 0.6749  loss_mask_0: 0.5056  loss_dice_0: 3.973  loss_ce_1: 0.3842  loss_mask_1: 0.5233  loss_dice_1: 3.898  loss_ce_2: 0.367  loss_mask_2: 0.5294  loss_dice_2: 3.878  loss_ce_3: 0.3809  loss_mask_3: 0.5262  loss_dice_3: 3.891  loss_ce_4: 0.3538  loss_mask_4: 0.5318  loss_dice_4: 3.894  loss_ce_5: 0.3483  loss_mask_5: 0.5338  loss_dice_5: 3.888  loss_ce_6: 0.36  loss_mask_6: 0.5313  loss_dice_6: 3.885  loss_ce_7: 0.3648  loss_mask_7: 0.5338  loss_dice_7: 3.887  loss_ce_8: 0.3653  loss_mask_8: 0.5329  loss_dice_8: 3.887  time: 1.5349  data_time: 0.0935  lr: 9.4866e-06  max_mem: 21297M
[01/17 05:31:16] d2.utils.events INFO:  eta: 1 day, 12:02:54  iter: 5139  total_loss: 48.22  loss_ce: 0.4052  loss_mask: 0.522  loss_dice: 3.854  loss_ce_0: 0.6692  loss_mask_0: 0.494  loss_dice_0: 3.963  loss_ce_1: 0.4245  loss_mask_1: 0.5185  loss_dice_1: 3.885  loss_ce_2: 0.398  loss_mask_2: 0.5199  loss_dice_2: 3.857  loss_ce_3: 0.4004  loss_mask_3: 0.5201  loss_dice_3: 3.847  loss_ce_4: 0.3987  loss_mask_4: 0.524  loss_dice_4: 3.856  loss_ce_5: 0.3953  loss_mask_5: 0.5211  loss_dice_5: 3.847  loss_ce_6: 0.3881  loss_mask_6: 0.5211  loss_dice_6: 3.853  loss_ce_7: 0.3888  loss_mask_7: 0.5225  loss_dice_7: 3.848  loss_ce_8: 0.39  loss_mask_8: 0.5212  loss_dice_8: 3.851  time: 1.5349  data_time: 0.0998  lr: 9.4846e-06  max_mem: 21297M
[01/17 05:31:46] d2.utils.events INFO:  eta: 1 day, 12:03:03  iter: 5159  total_loss: 48.33  loss_ce: 0.3669  loss_mask: 0.5417  loss_dice: 3.859  loss_ce_0: 0.6638  loss_mask_0: 0.5138  loss_dice_0: 3.961  loss_ce_1: 0.3942  loss_mask_1: 0.5406  loss_dice_1: 3.889  loss_ce_2: 0.387  loss_mask_2: 0.5427  loss_dice_2: 3.87  loss_ce_3: 0.3811  loss_mask_3: 0.5427  loss_dice_3: 3.868  loss_ce_4: 0.3803  loss_mask_4: 0.5397  loss_dice_4: 3.867  loss_ce_5: 0.3854  loss_mask_5: 0.5406  loss_dice_5: 3.86  loss_ce_6: 0.3627  loss_mask_6: 0.5423  loss_dice_6: 3.852  loss_ce_7: 0.3689  loss_mask_7: 0.5395  loss_dice_7: 3.859  loss_ce_8: 0.3657  loss_mask_8: 0.5413  loss_dice_8: 3.86  time: 1.5349  data_time: 0.0956  lr: 9.4826e-06  max_mem: 21366M
[01/17 05:32:17] d2.utils.events INFO:  eta: 1 day, 12:03:36  iter: 5179  total_loss: 48.35  loss_ce: 0.398  loss_mask: 0.5165  loss_dice: 3.886  loss_ce_0: 0.6949  loss_mask_0: 0.4972  loss_dice_0: 4.001  loss_ce_1: 0.4057  loss_mask_1: 0.5109  loss_dice_1: 3.918  loss_ce_2: 0.4023  loss_mask_2: 0.513  loss_dice_2: 3.901  loss_ce_3: 0.3936  loss_mask_3: 0.5135  loss_dice_3: 3.893  loss_ce_4: 0.383  loss_mask_4: 0.513  loss_dice_4: 3.889  loss_ce_5: 0.3894  loss_mask_5: 0.516  loss_dice_5: 3.888  loss_ce_6: 0.3838  loss_mask_6: 0.518  loss_dice_6: 3.892  loss_ce_7: 0.3784  loss_mask_7: 0.5182  loss_dice_7: 3.887  loss_ce_8: 0.374  loss_mask_8: 0.5179  loss_dice_8: 3.892  time: 1.5348  data_time: 0.0888  lr: 9.4806e-06  max_mem: 21366M
[01/17 05:32:48] d2.utils.events INFO:  eta: 1 day, 12:02:02  iter: 5199  total_loss: 48.12  loss_ce: 0.3755  loss_mask: 0.5246  loss_dice: 3.844  loss_ce_0: 0.689  loss_mask_0: 0.4997  loss_dice_0: 3.961  loss_ce_1: 0.4085  loss_mask_1: 0.5223  loss_dice_1: 3.884  loss_ce_2: 0.3998  loss_mask_2: 0.5237  loss_dice_2: 3.861  loss_ce_3: 0.3953  loss_mask_3: 0.5223  loss_dice_3: 3.852  loss_ce_4: 0.3948  loss_mask_4: 0.5238  loss_dice_4: 3.853  loss_ce_5: 0.3901  loss_mask_5: 0.5245  loss_dice_5: 3.849  loss_ce_6: 0.3837  loss_mask_6: 0.5232  loss_dice_6: 3.849  loss_ce_7: 0.3767  loss_mask_7: 0.5261  loss_dice_7: 3.843  loss_ce_8: 0.3924  loss_mask_8: 0.5241  loss_dice_8: 3.848  time: 1.5348  data_time: 0.1050  lr: 9.4786e-06  max_mem: 21366M
[01/17 05:33:18] d2.utils.events INFO:  eta: 1 day, 12:01:08  iter: 5219  total_loss: 47.99  loss_ce: 0.3603  loss_mask: 0.5319  loss_dice: 3.866  loss_ce_0: 0.676  loss_mask_0: 0.5001  loss_dice_0: 3.974  loss_ce_1: 0.3915  loss_mask_1: 0.5261  loss_dice_1: 3.903  loss_ce_2: 0.3785  loss_mask_2: 0.5305  loss_dice_2: 3.884  loss_ce_3: 0.3656  loss_mask_3: 0.5346  loss_dice_3: 3.872  loss_ce_4: 0.3714  loss_mask_4: 0.5325  loss_dice_4: 3.871  loss_ce_5: 0.3713  loss_mask_5: 0.5292  loss_dice_5: 3.87  loss_ce_6: 0.3604  loss_mask_6: 0.5303  loss_dice_6: 3.863  loss_ce_7: 0.3653  loss_mask_7: 0.527  loss_dice_7: 3.872  loss_ce_8: 0.3597  loss_mask_8: 0.5289  loss_dice_8: 3.868  time: 1.5348  data_time: 0.0981  lr: 9.4766e-06  max_mem: 21366M
[01/17 05:33:49] d2.utils.events INFO:  eta: 1 day, 12:00:38  iter: 5239  total_loss: 48.63  loss_ce: 0.3982  loss_mask: 0.5262  loss_dice: 3.881  loss_ce_0: 0.7031  loss_mask_0: 0.5115  loss_dice_0: 3.985  loss_ce_1: 0.4249  loss_mask_1: 0.5289  loss_dice_1: 3.899  loss_ce_2: 0.4275  loss_mask_2: 0.5306  loss_dice_2: 3.88  loss_ce_3: 0.405  loss_mask_3: 0.5269  loss_dice_3: 3.877  loss_ce_4: 0.4074  loss_mask_4: 0.5268  loss_dice_4: 3.873  loss_ce_5: 0.415  loss_mask_5: 0.5267  loss_dice_5: 3.883  loss_ce_6: 0.403  loss_mask_6: 0.5305  loss_dice_6: 3.88  loss_ce_7: 0.4104  loss_mask_7: 0.5315  loss_dice_7: 3.874  loss_ce_8: 0.4102  loss_mask_8: 0.5273  loss_dice_8: 3.875  time: 1.5348  data_time: 0.0946  lr: 9.4745e-06  max_mem: 21366M
[01/17 05:34:20] d2.utils.events INFO:  eta: 1 day, 12:01:17  iter: 5259  total_loss: 48.24  loss_ce: 0.4015  loss_mask: 0.5142  loss_dice: 3.878  loss_ce_0: 0.6712  loss_mask_0: 0.4922  loss_dice_0: 3.974  loss_ce_1: 0.4019  loss_mask_1: 0.5137  loss_dice_1: 3.898  loss_ce_2: 0.4125  loss_mask_2: 0.5161  loss_dice_2: 3.876  loss_ce_3: 0.3932  loss_mask_3: 0.5142  loss_dice_3: 3.884  loss_ce_4: 0.4036  loss_mask_4: 0.5161  loss_dice_4: 3.875  loss_ce_5: 0.3992  loss_mask_5: 0.5165  loss_dice_5: 3.88  loss_ce_6: 0.4101  loss_mask_6: 0.5142  loss_dice_6: 3.872  loss_ce_7: 0.3767  loss_mask_7: 0.515  loss_dice_7: 3.864  loss_ce_8: 0.3963  loss_mask_8: 0.5134  loss_dice_8: 3.883  time: 1.5348  data_time: 0.0960  lr: 9.4725e-06  max_mem: 21366M
[01/17 05:34:51] d2.utils.events INFO:  eta: 1 day, 12:00:55  iter: 5279  total_loss: 48.56  loss_ce: 0.3965  loss_mask: 0.5179  loss_dice: 3.88  loss_ce_0: 0.6663  loss_mask_0: 0.4968  loss_dice_0: 3.989  loss_ce_1: 0.388  loss_mask_1: 0.5187  loss_dice_1: 3.915  loss_ce_2: 0.4052  loss_mask_2: 0.5186  loss_dice_2: 3.895  loss_ce_3: 0.3886  loss_mask_3: 0.5195  loss_dice_3: 3.884  loss_ce_4: 0.3802  loss_mask_4: 0.5178  loss_dice_4: 3.877  loss_ce_5: 0.3801  loss_mask_5: 0.5175  loss_dice_5: 3.884  loss_ce_6: 0.3877  loss_mask_6: 0.5159  loss_dice_6: 3.881  loss_ce_7: 0.3924  loss_mask_7: 0.5215  loss_dice_7: 3.889  loss_ce_8: 0.388  loss_mask_8: 0.5201  loss_dice_8: 3.884  time: 1.5349  data_time: 0.0968  lr: 9.4705e-06  max_mem: 21366M
[01/17 05:35:21] d2.utils.events INFO:  eta: 1 day, 12:00:24  iter: 5299  total_loss: 48.28  loss_ce: 0.3813  loss_mask: 0.5276  loss_dice: 3.883  loss_ce_0: 0.6679  loss_mask_0: 0.5038  loss_dice_0: 3.972  loss_ce_1: 0.4195  loss_mask_1: 0.5237  loss_dice_1: 3.904  loss_ce_2: 0.4157  loss_mask_2: 0.5251  loss_dice_2: 3.88  loss_ce_3: 0.3969  loss_mask_3: 0.5246  loss_dice_3: 3.876  loss_ce_4: 0.4055  loss_mask_4: 0.5252  loss_dice_4: 3.875  loss_ce_5: 0.4024  loss_mask_5: 0.5266  loss_dice_5: 3.874  loss_ce_6: 0.3792  loss_mask_6: 0.5264  loss_dice_6: 3.878  loss_ce_7: 0.3839  loss_mask_7: 0.5255  loss_dice_7: 3.872  loss_ce_8: 0.3745  loss_mask_8: 0.526  loss_dice_8: 3.873  time: 1.5348  data_time: 0.0925  lr: 9.4685e-06  max_mem: 21366M
[01/17 05:35:52] d2.utils.events INFO:  eta: 1 day, 12:00:29  iter: 5319  total_loss: 48.38  loss_ce: 0.3938  loss_mask: 0.5195  loss_dice: 3.874  loss_ce_0: 0.6866  loss_mask_0: 0.4861  loss_dice_0: 3.985  loss_ce_1: 0.4245  loss_mask_1: 0.5126  loss_dice_1: 3.902  loss_ce_2: 0.4099  loss_mask_2: 0.5142  loss_dice_2: 3.889  loss_ce_3: 0.4038  loss_mask_3: 0.5176  loss_dice_3: 3.875  loss_ce_4: 0.4076  loss_mask_4: 0.5206  loss_dice_4: 3.87  loss_ce_5: 0.3986  loss_mask_5: 0.518  loss_dice_5: 3.874  loss_ce_6: 0.403  loss_mask_6: 0.5202  loss_dice_6: 3.869  loss_ce_7: 0.4137  loss_mask_7: 0.5193  loss_dice_7: 3.871  loss_ce_8: 0.3967  loss_mask_8: 0.5191  loss_dice_8: 3.875  time: 1.5347  data_time: 0.0930  lr: 9.4665e-06  max_mem: 21366M
[01/17 05:36:23] d2.utils.events INFO:  eta: 1 day, 12:03:21  iter: 5339  total_loss: 47.84  loss_ce: 0.3973  loss_mask: 0.5271  loss_dice: 3.841  loss_ce_0: 0.6814  loss_mask_0: 0.5109  loss_dice_0: 3.947  loss_ce_1: 0.4091  loss_mask_1: 0.526  loss_dice_1: 3.873  loss_ce_2: 0.4062  loss_mask_2: 0.5279  loss_dice_2: 3.858  loss_ce_3: 0.3932  loss_mask_3: 0.5263  loss_dice_3: 3.848  loss_ce_4: 0.4016  loss_mask_4: 0.5293  loss_dice_4: 3.848  loss_ce_5: 0.3922  loss_mask_5: 0.5267  loss_dice_5: 3.846  loss_ce_6: 0.3975  loss_mask_6: 0.5286  loss_dice_6: 3.842  loss_ce_7: 0.3919  loss_mask_7: 0.5289  loss_dice_7: 3.85  loss_ce_8: 0.3774  loss_mask_8: 0.5264  loss_dice_8: 3.845  time: 1.5348  data_time: 0.1040  lr: 9.4645e-06  max_mem: 21366M
[01/17 05:36:53] d2.utils.events INFO:  eta: 1 day, 12:01:52  iter: 5359  total_loss: 47.77  loss_ce: 0.3426  loss_mask: 0.5269  loss_dice: 3.827  loss_ce_0: 0.6142  loss_mask_0: 0.5001  loss_dice_0: 3.934  loss_ce_1: 0.3713  loss_mask_1: 0.526  loss_dice_1: 3.853  loss_ce_2: 0.3495  loss_mask_2: 0.5286  loss_dice_2: 3.831  loss_ce_3: 0.3549  loss_mask_3: 0.53  loss_dice_3: 3.818  loss_ce_4: 0.3538  loss_mask_4: 0.5293  loss_dice_4: 3.828  loss_ce_5: 0.353  loss_mask_5: 0.529  loss_dice_5: 3.824  loss_ce_6: 0.3518  loss_mask_6: 0.5289  loss_dice_6: 3.817  loss_ce_7: 0.3601  loss_mask_7: 0.5243  loss_dice_7: 3.82  loss_ce_8: 0.343  loss_mask_8: 0.527  loss_dice_8: 3.817  time: 1.5347  data_time: 0.0859  lr: 9.4625e-06  max_mem: 21366M
[01/17 05:37:23] d2.utils.events INFO:  eta: 1 day, 12:00:06  iter: 5379  total_loss: 47.86  loss_ce: 0.3964  loss_mask: 0.5292  loss_dice: 3.842  loss_ce_0: 0.6468  loss_mask_0: 0.4995  loss_dice_0: 3.955  loss_ce_1: 0.3957  loss_mask_1: 0.522  loss_dice_1: 3.875  loss_ce_2: 0.3924  loss_mask_2: 0.5272  loss_dice_2: 3.848  loss_ce_3: 0.3696  loss_mask_3: 0.5245  loss_dice_3: 3.841  loss_ce_4: 0.3919  loss_mask_4: 0.5268  loss_dice_4: 3.849  loss_ce_5: 0.3782  loss_mask_5: 0.5244  loss_dice_5: 3.852  loss_ce_6: 0.3761  loss_mask_6: 0.5277  loss_dice_6: 3.846  loss_ce_7: 0.3835  loss_mask_7: 0.5287  loss_dice_7: 3.844  loss_ce_8: 0.3817  loss_mask_8: 0.5284  loss_dice_8: 3.842  time: 1.5346  data_time: 0.0865  lr: 9.4605e-06  max_mem: 21366M
[01/17 05:37:55] d2.utils.events INFO:  eta: 1 day, 11:58:47  iter: 5399  total_loss: 48.18  loss_ce: 0.3658  loss_mask: 0.5238  loss_dice: 3.869  loss_ce_0: 0.6953  loss_mask_0: 0.4995  loss_dice_0: 3.976  loss_ce_1: 0.4077  loss_mask_1: 0.5208  loss_dice_1: 3.906  loss_ce_2: 0.3991  loss_mask_2: 0.5212  loss_dice_2: 3.879  loss_ce_3: 0.3982  loss_mask_3: 0.526  loss_dice_3: 3.87  loss_ce_4: 0.3837  loss_mask_4: 0.5259  loss_dice_4: 3.875  loss_ce_5: 0.3773  loss_mask_5: 0.5241  loss_dice_5: 3.869  loss_ce_6: 0.3737  loss_mask_6: 0.5216  loss_dice_6: 3.879  loss_ce_7: 0.3704  loss_mask_7: 0.5235  loss_dice_7: 3.874  loss_ce_8: 0.3661  loss_mask_8: 0.5211  loss_dice_8: 3.873  time: 1.5347  data_time: 0.1005  lr: 9.4584e-06  max_mem: 21366M
[01/17 05:38:26] d2.utils.events INFO:  eta: 1 day, 11:59:05  iter: 5419  total_loss: 47.96  loss_ce: 0.3996  loss_mask: 0.5096  loss_dice: 3.829  loss_ce_0: 0.7019  loss_mask_0: 0.482  loss_dice_0: 3.95  loss_ce_1: 0.4137  loss_mask_1: 0.5092  loss_dice_1: 3.867  loss_ce_2: 0.4137  loss_mask_2: 0.5095  loss_dice_2: 3.843  loss_ce_3: 0.4002  loss_mask_3: 0.5095  loss_dice_3: 3.835  loss_ce_4: 0.3965  loss_mask_4: 0.5071  loss_dice_4: 3.837  loss_ce_5: 0.3873  loss_mask_5: 0.5085  loss_dice_5: 3.846  loss_ce_6: 0.3958  loss_mask_6: 0.5082  loss_dice_6: 3.835  loss_ce_7: 0.393  loss_mask_7: 0.5074  loss_dice_7: 3.828  loss_ce_8: 0.3861  loss_mask_8: 0.5101  loss_dice_8: 3.833  time: 1.5348  data_time: 0.1031  lr: 9.4564e-06  max_mem: 21366M
[01/17 05:38:56] d2.utils.events INFO:  eta: 1 day, 11:55:57  iter: 5439  total_loss: 47.67  loss_ce: 0.3755  loss_mask: 0.5385  loss_dice: 3.813  loss_ce_0: 0.6403  loss_mask_0: 0.5208  loss_dice_0: 3.91  loss_ce_1: 0.3987  loss_mask_1: 0.5414  loss_dice_1: 3.825  loss_ce_2: 0.3926  loss_mask_2: 0.5378  loss_dice_2: 3.81  loss_ce_3: 0.3739  loss_mask_3: 0.5373  loss_dice_3: 3.808  loss_ce_4: 0.3595  loss_mask_4: 0.5359  loss_dice_4: 3.808  loss_ce_5: 0.39  loss_mask_5: 0.5318  loss_dice_5: 3.809  loss_ce_6: 0.3713  loss_mask_6: 0.532  loss_dice_6: 3.808  loss_ce_7: 0.3906  loss_mask_7: 0.5363  loss_dice_7: 3.811  loss_ce_8: 0.3812  loss_mask_8: 0.5364  loss_dice_8: 3.806  time: 1.5346  data_time: 0.0983  lr: 9.4544e-06  max_mem: 21366M
[01/17 05:39:26] d2.utils.events INFO:  eta: 1 day, 11:55:06  iter: 5459  total_loss: 47.62  loss_ce: 0.3805  loss_mask: 0.5282  loss_dice: 3.819  loss_ce_0: 0.633  loss_mask_0: 0.5032  loss_dice_0: 3.926  loss_ce_1: 0.3822  loss_mask_1: 0.527  loss_dice_1: 3.854  loss_ce_2: 0.3782  loss_mask_2: 0.5294  loss_dice_2: 3.832  loss_ce_3: 0.3843  loss_mask_3: 0.5293  loss_dice_3: 3.818  loss_ce_4: 0.3772  loss_mask_4: 0.5319  loss_dice_4: 3.816  loss_ce_5: 0.3715  loss_mask_5: 0.5293  loss_dice_5: 3.817  loss_ce_6: 0.3667  loss_mask_6: 0.5305  loss_dice_6: 3.816  loss_ce_7: 0.3774  loss_mask_7: 0.5298  loss_dice_7: 3.815  loss_ce_8: 0.3704  loss_mask_8: 0.5303  loss_dice_8: 3.813  time: 1.5346  data_time: 0.0994  lr: 9.4524e-06  max_mem: 21366M
[01/17 05:39:57] d2.utils.events INFO:  eta: 1 day, 11:54:00  iter: 5479  total_loss: 47.98  loss_ce: 0.3522  loss_mask: 0.5093  loss_dice: 3.847  loss_ce_0: 0.6518  loss_mask_0: 0.4919  loss_dice_0: 3.949  loss_ce_1: 0.3821  loss_mask_1: 0.5092  loss_dice_1: 3.884  loss_ce_2: 0.3882  loss_mask_2: 0.5088  loss_dice_2: 3.861  loss_ce_3: 0.3786  loss_mask_3: 0.5098  loss_dice_3: 3.855  loss_ce_4: 0.3773  loss_mask_4: 0.5088  loss_dice_4: 3.859  loss_ce_5: 0.377  loss_mask_5: 0.5097  loss_dice_5: 3.858  loss_ce_6: 0.368  loss_mask_6: 0.5099  loss_dice_6: 3.856  loss_ce_7: 0.3753  loss_mask_7: 0.511  loss_dice_7: 3.858  loss_ce_8: 0.3698  loss_mask_8: 0.511  loss_dice_8: 3.849  time: 1.5346  data_time: 0.1011  lr: 9.4504e-06  max_mem: 21366M
[01/17 05:40:27] d2.utils.events INFO:  eta: 1 day, 11:53:34  iter: 5499  total_loss: 47.95  loss_ce: 0.3924  loss_mask: 0.524  loss_dice: 3.851  loss_ce_0: 0.685  loss_mask_0: 0.5043  loss_dice_0: 3.953  loss_ce_1: 0.4037  loss_mask_1: 0.5226  loss_dice_1: 3.872  loss_ce_2: 0.3977  loss_mask_2: 0.5236  loss_dice_2: 3.852  loss_ce_3: 0.4122  loss_mask_3: 0.5241  loss_dice_3: 3.848  loss_ce_4: 0.3956  loss_mask_4: 0.524  loss_dice_4: 3.849  loss_ce_5: 0.3934  loss_mask_5: 0.5236  loss_dice_5: 3.851  loss_ce_6: 0.3944  loss_mask_6: 0.5247  loss_dice_6: 3.848  loss_ce_7: 0.3841  loss_mask_7: 0.5253  loss_dice_7: 3.847  loss_ce_8: 0.3941  loss_mask_8: 0.5249  loss_dice_8: 3.845  time: 1.5345  data_time: 0.0819  lr: 9.4484e-06  max_mem: 21366M
[01/17 05:40:58] d2.utils.events INFO:  eta: 1 day, 11:52:36  iter: 5519  total_loss: 47.14  loss_ce: 0.3513  loss_mask: 0.5261  loss_dice: 3.784  loss_ce_0: 0.6529  loss_mask_0: 0.5036  loss_dice_0: 3.903  loss_ce_1: 0.3898  loss_mask_1: 0.5232  loss_dice_1: 3.821  loss_ce_2: 0.3698  loss_mask_2: 0.5258  loss_dice_2: 3.804  loss_ce_3: 0.3517  loss_mask_3: 0.5287  loss_dice_3: 3.796  loss_ce_4: 0.3605  loss_mask_4: 0.528  loss_dice_4: 3.792  loss_ce_5: 0.3568  loss_mask_5: 0.5269  loss_dice_5: 3.793  loss_ce_6: 0.3677  loss_mask_6: 0.5283  loss_dice_6: 3.794  loss_ce_7: 0.3634  loss_mask_7: 0.5275  loss_dice_7: 3.791  loss_ce_8: 0.3556  loss_mask_8: 0.5286  loss_dice_8: 3.784  time: 1.5344  data_time: 0.0869  lr: 9.4464e-06  max_mem: 21366M
[01/17 05:41:28] d2.utils.events INFO:  eta: 1 day, 11:52:22  iter: 5539  total_loss: 47.42  loss_ce: 0.3777  loss_mask: 0.5167  loss_dice: 3.806  loss_ce_0: 0.6644  loss_mask_0: 0.4874  loss_dice_0: 3.93  loss_ce_1: 0.3893  loss_mask_1: 0.511  loss_dice_1: 3.843  loss_ce_2: 0.385  loss_mask_2: 0.5141  loss_dice_2: 3.813  loss_ce_3: 0.3731  loss_mask_3: 0.518  loss_dice_3: 3.806  loss_ce_4: 0.3851  loss_mask_4: 0.5148  loss_dice_4: 3.806  loss_ce_5: 0.3614  loss_mask_5: 0.5123  loss_dice_5: 3.809  loss_ce_6: 0.3744  loss_mask_6: 0.5154  loss_dice_6: 3.802  loss_ce_7: 0.3715  loss_mask_7: 0.513  loss_dice_7: 3.814  loss_ce_8: 0.3718  loss_mask_8: 0.5174  loss_dice_8: 3.804  time: 1.5344  data_time: 0.0867  lr: 9.4444e-06  max_mem: 21366M
[01/17 05:41:59] d2.utils.events INFO:  eta: 1 day, 11:51:35  iter: 5559  total_loss: 47.51  loss_ce: 0.3638  loss_mask: 0.5056  loss_dice: 3.801  loss_ce_0: 0.6595  loss_mask_0: 0.484  loss_dice_0: 3.931  loss_ce_1: 0.3799  loss_mask_1: 0.5009  loss_dice_1: 3.842  loss_ce_2: 0.3803  loss_mask_2: 0.5041  loss_dice_2: 3.814  loss_ce_3: 0.3603  loss_mask_3: 0.5033  loss_dice_3: 3.803  loss_ce_4: 0.3737  loss_mask_4: 0.5017  loss_dice_4: 3.804  loss_ce_5: 0.3749  loss_mask_5: 0.5064  loss_dice_5: 3.8  loss_ce_6: 0.3681  loss_mask_6: 0.509  loss_dice_6: 3.791  loss_ce_7: 0.362  loss_mask_7: 0.5075  loss_dice_7: 3.802  loss_ce_8: 0.3647  loss_mask_8: 0.5097  loss_dice_8: 3.792  time: 1.5344  data_time: 0.1037  lr: 9.4423e-06  max_mem: 21366M
[01/17 05:42:30] d2.utils.events INFO:  eta: 1 day, 11:51:04  iter: 5579  total_loss: 47.99  loss_ce: 0.3738  loss_mask: 0.5247  loss_dice: 3.832  loss_ce_0: 0.6377  loss_mask_0: 0.4955  loss_dice_0: 3.943  loss_ce_1: 0.3992  loss_mask_1: 0.5215  loss_dice_1: 3.864  loss_ce_2: 0.4011  loss_mask_2: 0.5191  loss_dice_2: 3.843  loss_ce_3: 0.379  loss_mask_3: 0.5238  loss_dice_3: 3.843  loss_ce_4: 0.3922  loss_mask_4: 0.5268  loss_dice_4: 3.842  loss_ce_5: 0.3841  loss_mask_5: 0.5243  loss_dice_5: 3.842  loss_ce_6: 0.3822  loss_mask_6: 0.5212  loss_dice_6: 3.835  loss_ce_7: 0.364  loss_mask_7: 0.5221  loss_dice_7: 3.832  loss_ce_8: 0.3684  loss_mask_8: 0.5218  loss_dice_8: 3.829  time: 1.5344  data_time: 0.0994  lr: 9.4403e-06  max_mem: 21366M
[01/17 05:43:01] d2.utils.events INFO:  eta: 1 day, 11:51:00  iter: 5599  total_loss: 47.73  loss_ce: 0.3575  loss_mask: 0.5276  loss_dice: 3.819  loss_ce_0: 0.6601  loss_mask_0: 0.5043  loss_dice_0: 3.918  loss_ce_1: 0.4086  loss_mask_1: 0.5219  loss_dice_1: 3.842  loss_ce_2: 0.3747  loss_mask_2: 0.5252  loss_dice_2: 3.824  loss_ce_3: 0.3547  loss_mask_3: 0.527  loss_dice_3: 3.82  loss_ce_4: 0.352  loss_mask_4: 0.5275  loss_dice_4: 3.815  loss_ce_5: 0.3683  loss_mask_5: 0.5285  loss_dice_5: 3.818  loss_ce_6: 0.3589  loss_mask_6: 0.5284  loss_dice_6: 3.817  loss_ce_7: 0.3678  loss_mask_7: 0.5293  loss_dice_7: 3.814  loss_ce_8: 0.3751  loss_mask_8: 0.5254  loss_dice_8: 3.814  time: 1.5345  data_time: 0.0998  lr: 9.4383e-06  max_mem: 21366M
[01/17 05:43:31] d2.utils.events INFO:  eta: 1 day, 11:50:03  iter: 5619  total_loss: 47.09  loss_ce: 0.3388  loss_mask: 0.5091  loss_dice: 3.767  loss_ce_0: 0.6412  loss_mask_0: 0.4859  loss_dice_0: 3.903  loss_ce_1: 0.3611  loss_mask_1: 0.5062  loss_dice_1: 3.811  loss_ce_2: 0.3486  loss_mask_2: 0.5084  loss_dice_2: 3.785  loss_ce_3: 0.3427  loss_mask_3: 0.51  loss_dice_3: 3.777  loss_ce_4: 0.3446  loss_mask_4: 0.5094  loss_dice_4: 3.771  loss_ce_5: 0.3545  loss_mask_5: 0.5083  loss_dice_5: 3.77  loss_ce_6: 0.3475  loss_mask_6: 0.5101  loss_dice_6: 3.76  loss_ce_7: 0.3427  loss_mask_7: 0.5097  loss_dice_7: 3.767  loss_ce_8: 0.3512  loss_mask_8: 0.5088  loss_dice_8: 3.764  time: 1.5344  data_time: 0.0899  lr: 9.4363e-06  max_mem: 21366M
[01/17 05:44:02] d2.utils.events INFO:  eta: 1 day, 11:49:14  iter: 5639  total_loss: 47.35  loss_ce: 0.3316  loss_mask: 0.5246  loss_dice: 3.803  loss_ce_0: 0.6546  loss_mask_0: 0.5085  loss_dice_0: 3.916  loss_ce_1: 0.3962  loss_mask_1: 0.5198  loss_dice_1: 3.84  loss_ce_2: 0.3628  loss_mask_2: 0.5258  loss_dice_2: 3.819  loss_ce_3: 0.3549  loss_mask_3: 0.5239  loss_dice_3: 3.813  loss_ce_4: 0.3513  loss_mask_4: 0.5224  loss_dice_4: 3.805  loss_ce_5: 0.3589  loss_mask_5: 0.5237  loss_dice_5: 3.8  loss_ce_6: 0.348  loss_mask_6: 0.5243  loss_dice_6: 3.795  loss_ce_7: 0.338  loss_mask_7: 0.525  loss_dice_7: 3.795  loss_ce_8: 0.3515  loss_mask_8: 0.5234  loss_dice_8: 3.797  time: 1.5344  data_time: 0.0930  lr: 9.4343e-06  max_mem: 21366M
[01/17 05:44:32] d2.utils.events INFO:  eta: 1 day, 11:47:46  iter: 5659  total_loss: 47.26  loss_ce: 0.3599  loss_mask: 0.5172  loss_dice: 3.823  loss_ce_0: 0.6083  loss_mask_0: 0.4883  loss_dice_0: 3.92  loss_ce_1: 0.3801  loss_mask_1: 0.5132  loss_dice_1: 3.847  loss_ce_2: 0.3907  loss_mask_2: 0.5154  loss_dice_2: 3.833  loss_ce_3: 0.3696  loss_mask_3: 0.5181  loss_dice_3: 3.822  loss_ce_4: 0.3742  loss_mask_4: 0.5184  loss_dice_4: 3.82  loss_ce_5: 0.3745  loss_mask_5: 0.5163  loss_dice_5: 3.821  loss_ce_6: 0.3757  loss_mask_6: 0.516  loss_dice_6: 3.818  loss_ce_7: 0.3641  loss_mask_7: 0.5162  loss_dice_7: 3.819  loss_ce_8: 0.3664  loss_mask_8: 0.5176  loss_dice_8: 3.816  time: 1.5343  data_time: 0.0910  lr: 9.4323e-06  max_mem: 21366M
[01/17 05:45:03] d2.utils.events INFO:  eta: 1 day, 11:47:29  iter: 5679  total_loss: 46.97  loss_ce: 0.3603  loss_mask: 0.5101  loss_dice: 3.757  loss_ce_0: 0.6582  loss_mask_0: 0.4817  loss_dice_0: 3.873  loss_ce_1: 0.3808  loss_mask_1: 0.5064  loss_dice_1: 3.788  loss_ce_2: 0.3937  loss_mask_2: 0.5086  loss_dice_2: 3.763  loss_ce_3: 0.3657  loss_mask_3: 0.5085  loss_dice_3: 3.757  loss_ce_4: 0.3713  loss_mask_4: 0.5057  loss_dice_4: 3.761  loss_ce_5: 0.3769  loss_mask_5: 0.5063  loss_dice_5: 3.762  loss_ce_6: 0.3675  loss_mask_6: 0.5059  loss_dice_6: 3.757  loss_ce_7: 0.3692  loss_mask_7: 0.5062  loss_dice_7: 3.751  loss_ce_8: 0.3706  loss_mask_8: 0.5068  loss_dice_8: 3.754  time: 1.5343  data_time: 0.0995  lr: 9.4303e-06  max_mem: 21366M
[01/17 05:45:34] d2.utils.events INFO:  eta: 1 day, 11:47:27  iter: 5699  total_loss: 47.22  loss_ce: 0.3616  loss_mask: 0.5243  loss_dice: 3.76  loss_ce_0: 0.6344  loss_mask_0: 0.4895  loss_dice_0: 3.892  loss_ce_1: 0.3877  loss_mask_1: 0.5139  loss_dice_1: 3.804  loss_ce_2: 0.3893  loss_mask_2: 0.5195  loss_dice_2: 3.761  loss_ce_3: 0.3775  loss_mask_3: 0.5177  loss_dice_3: 3.758  loss_ce_4: 0.3795  loss_mask_4: 0.5214  loss_dice_4: 3.761  loss_ce_5: 0.3581  loss_mask_5: 0.5216  loss_dice_5: 3.756  loss_ce_6: 0.3648  loss_mask_6: 0.5236  loss_dice_6: 3.747  loss_ce_7: 0.3832  loss_mask_7: 0.5244  loss_dice_7: 3.747  loss_ce_8: 0.3736  loss_mask_8: 0.5241  loss_dice_8: 3.758  time: 1.5343  data_time: 0.0895  lr: 9.4283e-06  max_mem: 21366M
[01/17 05:46:04] d2.utils.events INFO:  eta: 1 day, 11:46:28  iter: 5719  total_loss: 47.14  loss_ce: 0.3787  loss_mask: 0.5078  loss_dice: 3.733  loss_ce_0: 0.6493  loss_mask_0: 0.4926  loss_dice_0: 3.857  loss_ce_1: 0.3835  loss_mask_1: 0.5078  loss_dice_1: 3.768  loss_ce_2: 0.4038  loss_mask_2: 0.5112  loss_dice_2: 3.741  loss_ce_3: 0.3841  loss_mask_3: 0.5082  loss_dice_3: 3.737  loss_ce_4: 0.3859  loss_mask_4: 0.5077  loss_dice_4: 3.732  loss_ce_5: 0.3715  loss_mask_5: 0.5075  loss_dice_5: 3.74  loss_ce_6: 0.3888  loss_mask_6: 0.5063  loss_dice_6: 3.74  loss_ce_7: 0.3812  loss_mask_7: 0.5076  loss_dice_7: 3.728  loss_ce_8: 0.3778  loss_mask_8: 0.5098  loss_dice_8: 3.737  time: 1.5343  data_time: 0.1017  lr: 9.4262e-06  max_mem: 21366M
[01/17 05:46:35] d2.utils.events INFO:  eta: 1 day, 11:44:48  iter: 5739  total_loss: 47.05  loss_ce: 0.3574  loss_mask: 0.5183  loss_dice: 3.785  loss_ce_0: 0.6611  loss_mask_0: 0.4866  loss_dice_0: 3.91  loss_ce_1: 0.3876  loss_mask_1: 0.5072  loss_dice_1: 3.827  loss_ce_2: 0.3879  loss_mask_2: 0.5121  loss_dice_2: 3.8  loss_ce_3: 0.3624  loss_mask_3: 0.5132  loss_dice_3: 3.794  loss_ce_4: 0.3657  loss_mask_4: 0.513  loss_dice_4: 3.792  loss_ce_5: 0.3656  loss_mask_5: 0.5129  loss_dice_5: 3.791  loss_ce_6: 0.3536  loss_mask_6: 0.5124  loss_dice_6: 3.78  loss_ce_7: 0.3565  loss_mask_7: 0.5151  loss_dice_7: 3.788  loss_ce_8: 0.3601  loss_mask_8: 0.5145  loss_dice_8: 3.791  time: 1.5342  data_time: 0.0932  lr: 9.4242e-06  max_mem: 21366M
[01/17 05:47:05] d2.utils.events INFO:  eta: 1 day, 11:45:27  iter: 5759  total_loss: 47.31  loss_ce: 0.3706  loss_mask: 0.5242  loss_dice: 3.749  loss_ce_0: 0.6433  loss_mask_0: 0.4963  loss_dice_0: 3.875  loss_ce_1: 0.4017  loss_mask_1: 0.5156  loss_dice_1: 3.805  loss_ce_2: 0.3962  loss_mask_2: 0.5179  loss_dice_2: 3.772  loss_ce_3: 0.3972  loss_mask_3: 0.5216  loss_dice_3: 3.761  loss_ce_4: 0.3819  loss_mask_4: 0.5243  loss_dice_4: 3.757  loss_ce_5: 0.3786  loss_mask_5: 0.5234  loss_dice_5: 3.752  loss_ce_6: 0.389  loss_mask_6: 0.5236  loss_dice_6: 3.749  loss_ce_7: 0.3735  loss_mask_7: 0.5215  loss_dice_7: 3.761  loss_ce_8: 0.3787  loss_mask_8: 0.522  loss_dice_8: 3.749  time: 1.5342  data_time: 0.1052  lr: 9.4222e-06  max_mem: 21366M
[01/17 05:47:36] d2.utils.events INFO:  eta: 1 day, 11:45:03  iter: 5779  total_loss: 46.58  loss_ce: 0.3609  loss_mask: 0.5176  loss_dice: 3.728  loss_ce_0: 0.6736  loss_mask_0: 0.4898  loss_dice_0: 3.857  loss_ce_1: 0.3844  loss_mask_1: 0.5158  loss_dice_1: 3.764  loss_ce_2: 0.3796  loss_mask_2: 0.516  loss_dice_2: 3.737  loss_ce_3: 0.3761  loss_mask_3: 0.5166  loss_dice_3: 3.727  loss_ce_4: 0.3799  loss_mask_4: 0.5133  loss_dice_4: 3.727  loss_ce_5: 0.3705  loss_mask_5: 0.5177  loss_dice_5: 3.727  loss_ce_6: 0.3591  loss_mask_6: 0.5204  loss_dice_6: 3.725  loss_ce_7: 0.3641  loss_mask_7: 0.5179  loss_dice_7: 3.733  loss_ce_8: 0.3758  loss_mask_8: 0.5193  loss_dice_8: 3.724  time: 1.5341  data_time: 0.0969  lr: 9.4202e-06  max_mem: 21366M
[01/17 05:48:06] d2.utils.events INFO:  eta: 1 day, 11:44:18  iter: 5799  total_loss: 47.22  loss_ce: 0.3966  loss_mask: 0.5101  loss_dice: 3.777  loss_ce_0: 0.6576  loss_mask_0: 0.4889  loss_dice_0: 3.9  loss_ce_1: 0.3763  loss_mask_1: 0.5073  loss_dice_1: 3.823  loss_ce_2: 0.3928  loss_mask_2: 0.5102  loss_dice_2: 3.798  loss_ce_3: 0.397  loss_mask_3: 0.5097  loss_dice_3: 3.789  loss_ce_4: 0.3928  loss_mask_4: 0.5078  loss_dice_4: 3.794  loss_ce_5: 0.3823  loss_mask_5: 0.5081  loss_dice_5: 3.789  loss_ce_6: 0.3957  loss_mask_6: 0.5059  loss_dice_6: 3.79  loss_ce_7: 0.3946  loss_mask_7: 0.5078  loss_dice_7: 3.79  loss_ce_8: 0.3947  loss_mask_8: 0.5102  loss_dice_8: 3.781  time: 1.5342  data_time: 0.0996  lr: 9.4182e-06  max_mem: 21366M
[01/17 05:48:37] d2.utils.events INFO:  eta: 1 day, 11:43:55  iter: 5819  total_loss: 47.15  loss_ce: 0.3757  loss_mask: 0.5036  loss_dice: 3.78  loss_ce_0: 0.647  loss_mask_0: 0.4842  loss_dice_0: 3.881  loss_ce_1: 0.3869  loss_mask_1: 0.5067  loss_dice_1: 3.806  loss_ce_2: 0.3868  loss_mask_2: 0.5078  loss_dice_2: 3.783  loss_ce_3: 0.3755  loss_mask_3: 0.5082  loss_dice_3: 3.777  loss_ce_4: 0.3823  loss_mask_4: 0.5097  loss_dice_4: 3.778  loss_ce_5: 0.3652  loss_mask_5: 0.5068  loss_dice_5: 3.779  loss_ce_6: 0.3739  loss_mask_6: 0.5067  loss_dice_6: 3.788  loss_ce_7: 0.3721  loss_mask_7: 0.5058  loss_dice_7: 3.782  loss_ce_8: 0.3708  loss_mask_8: 0.5041  loss_dice_8: 3.778  time: 1.5341  data_time: 0.0824  lr: 9.4162e-06  max_mem: 21366M
[01/17 05:49:08] d2.utils.events INFO:  eta: 1 day, 11:45:07  iter: 5839  total_loss: 46.88  loss_ce: 0.3545  loss_mask: 0.5227  loss_dice: 3.763  loss_ce_0: 0.6576  loss_mask_0: 0.4891  loss_dice_0: 3.874  loss_ce_1: 0.3749  loss_mask_1: 0.5197  loss_dice_1: 3.796  loss_ce_2: 0.3695  loss_mask_2: 0.5223  loss_dice_2: 3.774  loss_ce_3: 0.3579  loss_mask_3: 0.5204  loss_dice_3: 3.759  loss_ce_4: 0.3547  loss_mask_4: 0.5227  loss_dice_4: 3.757  loss_ce_5: 0.3691  loss_mask_5: 0.5227  loss_dice_5: 3.769  loss_ce_6: 0.3611  loss_mask_6: 0.5236  loss_dice_6: 3.76  loss_ce_7: 0.3511  loss_mask_7: 0.5217  loss_dice_7: 3.758  loss_ce_8: 0.3529  loss_mask_8: 0.5237  loss_dice_8: 3.758  time: 1.5342  data_time: 0.1001  lr: 9.4142e-06  max_mem: 21366M
[01/17 05:49:39] d2.utils.events INFO:  eta: 1 day, 11:44:43  iter: 5859  total_loss: 46.88  loss_ce: 0.3727  loss_mask: 0.5093  loss_dice: 3.742  loss_ce_0: 0.6815  loss_mask_0: 0.4882  loss_dice_0: 3.861  loss_ce_1: 0.4005  loss_mask_1: 0.508  loss_dice_1: 3.791  loss_ce_2: 0.3842  loss_mask_2: 0.5099  loss_dice_2: 3.765  loss_ce_3: 0.377  loss_mask_3: 0.5103  loss_dice_3: 3.753  loss_ce_4: 0.3626  loss_mask_4: 0.5114  loss_dice_4: 3.741  loss_ce_5: 0.3467  loss_mask_5: 0.5112  loss_dice_5: 3.745  loss_ce_6: 0.3611  loss_mask_6: 0.5109  loss_dice_6: 3.741  loss_ce_7: 0.3604  loss_mask_7: 0.512  loss_dice_7: 3.741  loss_ce_8: 0.3616  loss_mask_8: 0.5109  loss_dice_8: 3.747  time: 1.5342  data_time: 0.0949  lr: 9.4121e-06  max_mem: 21366M
[01/17 05:50:10] d2.utils.events INFO:  eta: 1 day, 11:44:12  iter: 5879  total_loss: 47.05  loss_ce: 0.3441  loss_mask: 0.5253  loss_dice: 3.747  loss_ce_0: 0.63  loss_mask_0: 0.5025  loss_dice_0: 3.853  loss_ce_1: 0.3831  loss_mask_1: 0.5242  loss_dice_1: 3.779  loss_ce_2: 0.3842  loss_mask_2: 0.5205  loss_dice_2: 3.756  loss_ce_3: 0.374  loss_mask_3: 0.5209  loss_dice_3: 3.751  loss_ce_4: 0.3661  loss_mask_4: 0.5216  loss_dice_4: 3.752  loss_ce_5: 0.3677  loss_mask_5: 0.5216  loss_dice_5: 3.752  loss_ce_6: 0.3631  loss_mask_6: 0.5213  loss_dice_6: 3.748  loss_ce_7: 0.3436  loss_mask_7: 0.5242  loss_dice_7: 3.744  loss_ce_8: 0.3448  loss_mask_8: 0.5247  loss_dice_8: 3.744  time: 1.5342  data_time: 0.0909  lr: 9.4101e-06  max_mem: 21366M
[01/17 05:50:41] d2.utils.events INFO:  eta: 1 day, 11:43:49  iter: 5899  total_loss: 46.99  loss_ce: 0.386  loss_mask: 0.5192  loss_dice: 3.765  loss_ce_0: 0.6569  loss_mask_0: 0.5001  loss_dice_0: 3.884  loss_ce_1: 0.3806  loss_mask_1: 0.5256  loss_dice_1: 3.799  loss_ce_2: 0.3913  loss_mask_2: 0.5228  loss_dice_2: 3.77  loss_ce_3: 0.3737  loss_mask_3: 0.5219  loss_dice_3: 3.769  loss_ce_4: 0.3846  loss_mask_4: 0.5209  loss_dice_4: 3.767  loss_ce_5: 0.3837  loss_mask_5: 0.524  loss_dice_5: 3.774  loss_ce_6: 0.3675  loss_mask_6: 0.5226  loss_dice_6: 3.772  loss_ce_7: 0.3769  loss_mask_7: 0.5203  loss_dice_7: 3.771  loss_ce_8: 0.3763  loss_mask_8: 0.5224  loss_dice_8: 3.766  time: 1.5342  data_time: 0.1009  lr: 9.4081e-06  max_mem: 21366M
[01/17 05:51:12] d2.utils.events INFO:  eta: 1 day, 11:44:33  iter: 5919  total_loss: 47.11  loss_ce: 0.3731  loss_mask: 0.5252  loss_dice: 3.739  loss_ce_0: 0.6459  loss_mask_0: 0.5039  loss_dice_0: 3.864  loss_ce_1: 0.406  loss_mask_1: 0.5267  loss_dice_1: 3.774  loss_ce_2: 0.4193  loss_mask_2: 0.5256  loss_dice_2: 3.759  loss_ce_3: 0.3856  loss_mask_3: 0.5245  loss_dice_3: 3.742  loss_ce_4: 0.3907  loss_mask_4: 0.5266  loss_dice_4: 3.743  loss_ce_5: 0.3871  loss_mask_5: 0.5279  loss_dice_5: 3.747  loss_ce_6: 0.3898  loss_mask_6: 0.5271  loss_dice_6: 3.743  loss_ce_7: 0.3829  loss_mask_7: 0.5272  loss_dice_7: 3.736  loss_ce_8: 0.3813  loss_mask_8: 0.5254  loss_dice_8: 3.743  time: 1.5343  data_time: 0.0962  lr: 9.4061e-06  max_mem: 21366M
[01/17 05:51:42] d2.utils.events INFO:  eta: 1 day, 11:44:23  iter: 5939  total_loss: 47.16  loss_ce: 0.3866  loss_mask: 0.5143  loss_dice: 3.779  loss_ce_0: 0.6482  loss_mask_0: 0.4954  loss_dice_0: 3.894  loss_ce_1: 0.4051  loss_mask_1: 0.512  loss_dice_1: 3.806  loss_ce_2: 0.403  loss_mask_2: 0.5161  loss_dice_2: 3.781  loss_ce_3: 0.3832  loss_mask_3: 0.5157  loss_dice_3: 3.775  loss_ce_4: 0.364  loss_mask_4: 0.5196  loss_dice_4: 3.768  loss_ce_5: 0.3791  loss_mask_5: 0.5174  loss_dice_5: 3.774  loss_ce_6: 0.3621  loss_mask_6: 0.5164  loss_dice_6: 3.772  loss_ce_7: 0.3786  loss_mask_7: 0.5148  loss_dice_7: 3.771  loss_ce_8: 0.3563  loss_mask_8: 0.5141  loss_dice_8: 3.778  time: 1.5343  data_time: 0.0951  lr: 9.4041e-06  max_mem: 21366M
[01/17 05:52:13] d2.utils.events INFO:  eta: 1 day, 11:43:12  iter: 5959  total_loss: 47.13  loss_ce: 0.3703  loss_mask: 0.4997  loss_dice: 3.791  loss_ce_0: 0.656  loss_mask_0: 0.4907  loss_dice_0: 3.903  loss_ce_1: 0.3713  loss_mask_1: 0.5044  loss_dice_1: 3.822  loss_ce_2: 0.375  loss_mask_2: 0.5039  loss_dice_2: 3.804  loss_ce_3: 0.3498  loss_mask_3: 0.5002  loss_dice_3: 3.797  loss_ce_4: 0.3467  loss_mask_4: 0.5041  loss_dice_4: 3.792  loss_ce_5: 0.3606  loss_mask_5: 0.5015  loss_dice_5: 3.797  loss_ce_6: 0.3544  loss_mask_6: 0.5029  loss_dice_6: 3.784  loss_ce_7: 0.3608  loss_mask_7: 0.5031  loss_dice_7: 3.787  loss_ce_8: 0.3625  loss_mask_8: 0.5028  loss_dice_8: 3.785  time: 1.5342  data_time: 0.0983  lr: 9.4021e-06  max_mem: 21366M
[01/17 05:52:43] d2.utils.events INFO:  eta: 1 day, 11:41:33  iter: 5979  total_loss: 46.69  loss_ce: 0.3551  loss_mask: 0.5181  loss_dice: 3.74  loss_ce_0: 0.6065  loss_mask_0: 0.487  loss_dice_0: 3.865  loss_ce_1: 0.3735  loss_mask_1: 0.5103  loss_dice_1: 3.768  loss_ce_2: 0.4024  loss_mask_2: 0.516  loss_dice_2: 3.751  loss_ce_3: 0.3682  loss_mask_3: 0.5133  loss_dice_3: 3.751  loss_ce_4: 0.369  loss_mask_4: 0.5153  loss_dice_4: 3.748  loss_ce_5: 0.342  loss_mask_5: 0.5178  loss_dice_5: 3.744  loss_ce_6: 0.3507  loss_mask_6: 0.518  loss_dice_6: 3.751  loss_ce_7: 0.361  loss_mask_7: 0.5147  loss_dice_7: 3.748  loss_ce_8: 0.3696  loss_mask_8: 0.5178  loss_dice_8: 3.74  time: 1.5342  data_time: 0.0943  lr: 9.4001e-06  max_mem: 21366M
[01/17 05:53:14] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 05:53:14] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 05:53:14] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 05:53:15] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 05:53:30] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0093 s/iter. Inference: 0.1640 s/iter. Eval: 0.1927 s/iter. Total: 0.3660 s/iter. ETA=0:06:36
[01/17 05:53:35] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0125 s/iter. Inference: 0.1830 s/iter. Eval: 0.2211 s/iter. Total: 0.4167 s/iter. ETA=0:07:25
[01/17 05:53:40] d2.evaluation.evaluator INFO: Inference done 36/1093. Dataloading: 0.0118 s/iter. Inference: 0.1757 s/iter. Eval: 0.2228 s/iter. Total: 0.4104 s/iter. ETA=0:07:13
[01/17 05:53:45] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0120 s/iter. Inference: 0.1725 s/iter. Eval: 0.2105 s/iter. Total: 0.3951 s/iter. ETA=0:06:52
[01/17 05:53:50] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0124 s/iter. Inference: 0.1713 s/iter. Eval: 0.2183 s/iter. Total: 0.4020 s/iter. ETA=0:06:54
[01/17 05:53:56] d2.evaluation.evaluator INFO: Inference done 75/1093. Dataloading: 0.0128 s/iter. Inference: 0.1731 s/iter. Eval: 0.2178 s/iter. Total: 0.4037 s/iter. ETA=0:06:50
[01/17 05:54:01] d2.evaluation.evaluator INFO: Inference done 87/1093. Dataloading: 0.0128 s/iter. Inference: 0.1718 s/iter. Eval: 0.2236 s/iter. Total: 0.4082 s/iter. ETA=0:06:50
[01/17 05:54:06] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0128 s/iter. Inference: 0.1721 s/iter. Eval: 0.2232 s/iter. Total: 0.4082 s/iter. ETA=0:06:45
[01/17 05:54:12] d2.evaluation.evaluator INFO: Inference done 113/1093. Dataloading: 0.0130 s/iter. Inference: 0.1723 s/iter. Eval: 0.2231 s/iter. Total: 0.4085 s/iter. ETA=0:06:40
[01/17 05:54:17] d2.evaluation.evaluator INFO: Inference done 126/1093. Dataloading: 0.0128 s/iter. Inference: 0.1711 s/iter. Eval: 0.2231 s/iter. Total: 0.4071 s/iter. ETA=0:06:33
[01/17 05:54:22] d2.evaluation.evaluator INFO: Inference done 140/1093. Dataloading: 0.0126 s/iter. Inference: 0.1704 s/iter. Eval: 0.2198 s/iter. Total: 0.4029 s/iter. ETA=0:06:23
[01/17 05:54:27] d2.evaluation.evaluator INFO: Inference done 154/1093. Dataloading: 0.0126 s/iter. Inference: 0.1710 s/iter. Eval: 0.2149 s/iter. Total: 0.3987 s/iter. ETA=0:06:14
[01/17 05:54:32] d2.evaluation.evaluator INFO: Inference done 166/1093. Dataloading: 0.0128 s/iter. Inference: 0.1706 s/iter. Eval: 0.2175 s/iter. Total: 0.4010 s/iter. ETA=0:06:11
[01/17 05:54:37] d2.evaluation.evaluator INFO: Inference done 179/1093. Dataloading: 0.0127 s/iter. Inference: 0.1700 s/iter. Eval: 0.2182 s/iter. Total: 0.4010 s/iter. ETA=0:06:06
[01/17 05:54:43] d2.evaluation.evaluator INFO: Inference done 192/1093. Dataloading: 0.0129 s/iter. Inference: 0.1712 s/iter. Eval: 0.2173 s/iter. Total: 0.4015 s/iter. ETA=0:06:01
[01/17 05:54:48] d2.evaluation.evaluator INFO: Inference done 205/1093. Dataloading: 0.0128 s/iter. Inference: 0.1717 s/iter. Eval: 0.2180 s/iter. Total: 0.4026 s/iter. ETA=0:05:57
[01/17 05:54:53] d2.evaluation.evaluator INFO: Inference done 218/1093. Dataloading: 0.0129 s/iter. Inference: 0.1722 s/iter. Eval: 0.2179 s/iter. Total: 0.4032 s/iter. ETA=0:05:52
[01/17 05:54:59] d2.evaluation.evaluator INFO: Inference done 230/1093. Dataloading: 0.0130 s/iter. Inference: 0.1728 s/iter. Eval: 0.2183 s/iter. Total: 0.4042 s/iter. ETA=0:05:48
[01/17 05:55:04] d2.evaluation.evaluator INFO: Inference done 240/1093. Dataloading: 0.0131 s/iter. Inference: 0.1768 s/iter. Eval: 0.2196 s/iter. Total: 0.4096 s/iter. ETA=0:05:49
[01/17 05:55:09] d2.evaluation.evaluator INFO: Inference done 251/1093. Dataloading: 0.0132 s/iter. Inference: 0.1784 s/iter. Eval: 0.2209 s/iter. Total: 0.4126 s/iter. ETA=0:05:47
[01/17 05:55:14] d2.evaluation.evaluator INFO: Inference done 263/1093. Dataloading: 0.0133 s/iter. Inference: 0.1776 s/iter. Eval: 0.2221 s/iter. Total: 0.4131 s/iter. ETA=0:05:42
[01/17 05:55:20] d2.evaluation.evaluator INFO: Inference done 275/1093. Dataloading: 0.0132 s/iter. Inference: 0.1781 s/iter. Eval: 0.2236 s/iter. Total: 0.4150 s/iter. ETA=0:05:39
[01/17 05:55:25] d2.evaluation.evaluator INFO: Inference done 287/1093. Dataloading: 0.0133 s/iter. Inference: 0.1799 s/iter. Eval: 0.2229 s/iter. Total: 0.4161 s/iter. ETA=0:05:35
[01/17 05:55:30] d2.evaluation.evaluator INFO: Inference done 296/1093. Dataloading: 0.0136 s/iter. Inference: 0.1822 s/iter. Eval: 0.2247 s/iter. Total: 0.4206 s/iter. ETA=0:05:35
[01/17 05:55:35] d2.evaluation.evaluator INFO: Inference done 305/1093. Dataloading: 0.0137 s/iter. Inference: 0.1846 s/iter. Eval: 0.2268 s/iter. Total: 0.4253 s/iter. ETA=0:05:35
[01/17 05:55:40] d2.evaluation.evaluator INFO: Inference done 317/1093. Dataloading: 0.0138 s/iter. Inference: 0.1844 s/iter. Eval: 0.2276 s/iter. Total: 0.4259 s/iter. ETA=0:05:30
[01/17 05:55:45] d2.evaluation.evaluator INFO: Inference done 331/1093. Dataloading: 0.0137 s/iter. Inference: 0.1830 s/iter. Eval: 0.2262 s/iter. Total: 0.4230 s/iter. ETA=0:05:22
[01/17 05:55:51] d2.evaluation.evaluator INFO: Inference done 345/1093. Dataloading: 0.0135 s/iter. Inference: 0.1831 s/iter. Eval: 0.2240 s/iter. Total: 0.4207 s/iter. ETA=0:05:14
[01/17 05:55:56] d2.evaluation.evaluator INFO: Inference done 359/1093. Dataloading: 0.0135 s/iter. Inference: 0.1821 s/iter. Eval: 0.2233 s/iter. Total: 0.4191 s/iter. ETA=0:05:07
[01/17 05:56:01] d2.evaluation.evaluator INFO: Inference done 371/1093. Dataloading: 0.0135 s/iter. Inference: 0.1814 s/iter. Eval: 0.2241 s/iter. Total: 0.4192 s/iter. ETA=0:05:02
[01/17 05:56:06] d2.evaluation.evaluator INFO: Inference done 383/1093. Dataloading: 0.0135 s/iter. Inference: 0.1816 s/iter. Eval: 0.2243 s/iter. Total: 0.4195 s/iter. ETA=0:04:57
[01/17 05:56:11] d2.evaluation.evaluator INFO: Inference done 394/1093. Dataloading: 0.0135 s/iter. Inference: 0.1821 s/iter. Eval: 0.2251 s/iter. Total: 0.4208 s/iter. ETA=0:04:54
[01/17 05:56:16] d2.evaluation.evaluator INFO: Inference done 406/1093. Dataloading: 0.0135 s/iter. Inference: 0.1821 s/iter. Eval: 0.2256 s/iter. Total: 0.4213 s/iter. ETA=0:04:49
[01/17 05:56:22] d2.evaluation.evaluator INFO: Inference done 417/1093. Dataloading: 0.0134 s/iter. Inference: 0.1822 s/iter. Eval: 0.2267 s/iter. Total: 0.4225 s/iter. ETA=0:04:45
[01/17 05:56:27] d2.evaluation.evaluator INFO: Inference done 429/1093. Dataloading: 0.0135 s/iter. Inference: 0.1820 s/iter. Eval: 0.2269 s/iter. Total: 0.4225 s/iter. ETA=0:04:40
[01/17 05:56:32] d2.evaluation.evaluator INFO: Inference done 442/1093. Dataloading: 0.0135 s/iter. Inference: 0.1822 s/iter. Eval: 0.2258 s/iter. Total: 0.4217 s/iter. ETA=0:04:34
[01/17 05:56:37] d2.evaluation.evaluator INFO: Inference done 454/1093. Dataloading: 0.0135 s/iter. Inference: 0.1822 s/iter. Eval: 0.2266 s/iter. Total: 0.4224 s/iter. ETA=0:04:29
[01/17 05:56:42] d2.evaluation.evaluator INFO: Inference done 466/1093. Dataloading: 0.0134 s/iter. Inference: 0.1823 s/iter. Eval: 0.2267 s/iter. Total: 0.4225 s/iter. ETA=0:04:24
[01/17 05:56:48] d2.evaluation.evaluator INFO: Inference done 480/1093. Dataloading: 0.0133 s/iter. Inference: 0.1822 s/iter. Eval: 0.2256 s/iter. Total: 0.4213 s/iter. ETA=0:04:18
[01/17 05:56:53] d2.evaluation.evaluator INFO: Inference done 492/1093. Dataloading: 0.0133 s/iter. Inference: 0.1824 s/iter. Eval: 0.2254 s/iter. Total: 0.4212 s/iter. ETA=0:04:13
[01/17 05:56:58] d2.evaluation.evaluator INFO: Inference done 505/1093. Dataloading: 0.0133 s/iter. Inference: 0.1828 s/iter. Eval: 0.2241 s/iter. Total: 0.4204 s/iter. ETA=0:04:07
[01/17 05:57:03] d2.evaluation.evaluator INFO: Inference done 518/1093. Dataloading: 0.0133 s/iter. Inference: 0.1827 s/iter. Eval: 0.2239 s/iter. Total: 0.4200 s/iter. ETA=0:04:01
[01/17 05:57:08] d2.evaluation.evaluator INFO: Inference done 529/1093. Dataloading: 0.0133 s/iter. Inference: 0.1830 s/iter. Eval: 0.2251 s/iter. Total: 0.4216 s/iter. ETA=0:03:57
[01/17 05:57:14] d2.evaluation.evaluator INFO: Inference done 541/1093. Dataloading: 0.0134 s/iter. Inference: 0.1834 s/iter. Eval: 0.2250 s/iter. Total: 0.4218 s/iter. ETA=0:03:52
[01/17 05:57:19] d2.evaluation.evaluator INFO: Inference done 554/1093. Dataloading: 0.0134 s/iter. Inference: 0.1829 s/iter. Eval: 0.2253 s/iter. Total: 0.4216 s/iter. ETA=0:03:47
[01/17 05:57:24] d2.evaluation.evaluator INFO: Inference done 566/1093. Dataloading: 0.0133 s/iter. Inference: 0.1826 s/iter. Eval: 0.2254 s/iter. Total: 0.4215 s/iter. ETA=0:03:42
[01/17 05:57:29] d2.evaluation.evaluator INFO: Inference done 580/1093. Dataloading: 0.0134 s/iter. Inference: 0.1834 s/iter. Eval: 0.2237 s/iter. Total: 0.4206 s/iter. ETA=0:03:35
[01/17 05:57:35] d2.evaluation.evaluator INFO: Inference done 591/1093. Dataloading: 0.0134 s/iter. Inference: 0.1850 s/iter. Eval: 0.2233 s/iter. Total: 0.4219 s/iter. ETA=0:03:31
[01/17 05:57:40] d2.evaluation.evaluator INFO: Inference done 600/1093. Dataloading: 0.0136 s/iter. Inference: 0.1865 s/iter. Eval: 0.2244 s/iter. Total: 0.4246 s/iter. ETA=0:03:29
[01/17 05:57:45] d2.evaluation.evaluator INFO: Inference done 610/1093. Dataloading: 0.0136 s/iter. Inference: 0.1872 s/iter. Eval: 0.2251 s/iter. Total: 0.4260 s/iter. ETA=0:03:25
[01/17 05:57:50] d2.evaluation.evaluator INFO: Inference done 624/1093. Dataloading: 0.0136 s/iter. Inference: 0.1867 s/iter. Eval: 0.2243 s/iter. Total: 0.4247 s/iter. ETA=0:03:19
[01/17 05:57:56] d2.evaluation.evaluator INFO: Inference done 638/1093. Dataloading: 0.0135 s/iter. Inference: 0.1862 s/iter. Eval: 0.2238 s/iter. Total: 0.4236 s/iter. ETA=0:03:12
[01/17 05:58:01] d2.evaluation.evaluator INFO: Inference done 652/1093. Dataloading: 0.0135 s/iter. Inference: 0.1860 s/iter. Eval: 0.2230 s/iter. Total: 0.4226 s/iter. ETA=0:03:06
[01/17 05:58:06] d2.evaluation.evaluator INFO: Inference done 666/1093. Dataloading: 0.0134 s/iter. Inference: 0.1854 s/iter. Eval: 0.2225 s/iter. Total: 0.4214 s/iter. ETA=0:02:59
[01/17 05:58:11] d2.evaluation.evaluator INFO: Inference done 678/1093. Dataloading: 0.0134 s/iter. Inference: 0.1858 s/iter. Eval: 0.2220 s/iter. Total: 0.4213 s/iter. ETA=0:02:54
[01/17 05:58:16] d2.evaluation.evaluator INFO: Inference done 693/1093. Dataloading: 0.0134 s/iter. Inference: 0.1855 s/iter. Eval: 0.2208 s/iter. Total: 0.4198 s/iter. ETA=0:02:47
[01/17 05:58:21] d2.evaluation.evaluator INFO: Inference done 706/1093. Dataloading: 0.0133 s/iter. Inference: 0.1850 s/iter. Eval: 0.2207 s/iter. Total: 0.4192 s/iter. ETA=0:02:42
[01/17 05:58:26] d2.evaluation.evaluator INFO: Inference done 716/1093. Dataloading: 0.0134 s/iter. Inference: 0.1852 s/iter. Eval: 0.2216 s/iter. Total: 0.4203 s/iter. ETA=0:02:38
[01/17 05:58:32] d2.evaluation.evaluator INFO: Inference done 731/1093. Dataloading: 0.0133 s/iter. Inference: 0.1848 s/iter. Eval: 0.2205 s/iter. Total: 0.4187 s/iter. ETA=0:02:31
[01/17 05:58:37] d2.evaluation.evaluator INFO: Inference done 744/1093. Dataloading: 0.0133 s/iter. Inference: 0.1848 s/iter. Eval: 0.2203 s/iter. Total: 0.4185 s/iter. ETA=0:02:26
[01/17 05:58:42] d2.evaluation.evaluator INFO: Inference done 758/1093. Dataloading: 0.0132 s/iter. Inference: 0.1844 s/iter. Eval: 0.2198 s/iter. Total: 0.4176 s/iter. ETA=0:02:19
[01/17 05:58:47] d2.evaluation.evaluator INFO: Inference done 770/1093. Dataloading: 0.0132 s/iter. Inference: 0.1842 s/iter. Eval: 0.2205 s/iter. Total: 0.4181 s/iter. ETA=0:02:15
[01/17 05:58:53] d2.evaluation.evaluator INFO: Inference done 784/1093. Dataloading: 0.0132 s/iter. Inference: 0.1840 s/iter. Eval: 0.2202 s/iter. Total: 0.4175 s/iter. ETA=0:02:09
[01/17 05:58:58] d2.evaluation.evaluator INFO: Inference done 795/1093. Dataloading: 0.0132 s/iter. Inference: 0.1851 s/iter. Eval: 0.2200 s/iter. Total: 0.4185 s/iter. ETA=0:02:04
[01/17 05:59:03] d2.evaluation.evaluator INFO: Inference done 806/1093. Dataloading: 0.0132 s/iter. Inference: 0.1861 s/iter. Eval: 0.2198 s/iter. Total: 0.4192 s/iter. ETA=0:02:00
[01/17 05:59:09] d2.evaluation.evaluator INFO: Inference done 822/1093. Dataloading: 0.0131 s/iter. Inference: 0.1854 s/iter. Eval: 0.2188 s/iter. Total: 0.4174 s/iter. ETA=0:01:53
[01/17 05:59:14] d2.evaluation.evaluator INFO: Inference done 837/1093. Dataloading: 0.0131 s/iter. Inference: 0.1851 s/iter. Eval: 0.2179 s/iter. Total: 0.4162 s/iter. ETA=0:01:46
[01/17 05:59:19] d2.evaluation.evaluator INFO: Inference done 850/1093. Dataloading: 0.0130 s/iter. Inference: 0.1849 s/iter. Eval: 0.2177 s/iter. Total: 0.4157 s/iter. ETA=0:01:41
[01/17 05:59:24] d2.evaluation.evaluator INFO: Inference done 861/1093. Dataloading: 0.0131 s/iter. Inference: 0.1848 s/iter. Eval: 0.2184 s/iter. Total: 0.4164 s/iter. ETA=0:01:36
[01/17 05:59:29] d2.evaluation.evaluator INFO: Inference done 874/1093. Dataloading: 0.0131 s/iter. Inference: 0.1846 s/iter. Eval: 0.2184 s/iter. Total: 0.4161 s/iter. ETA=0:01:31
[01/17 05:59:34] d2.evaluation.evaluator INFO: Inference done 887/1093. Dataloading: 0.0131 s/iter. Inference: 0.1843 s/iter. Eval: 0.2185 s/iter. Total: 0.4160 s/iter. ETA=0:01:25
[01/17 05:59:40] d2.evaluation.evaluator INFO: Inference done 902/1093. Dataloading: 0.0131 s/iter. Inference: 0.1838 s/iter. Eval: 0.2178 s/iter. Total: 0.4148 s/iter. ETA=0:01:19
[01/17 05:59:45] d2.evaluation.evaluator INFO: Inference done 916/1093. Dataloading: 0.0130 s/iter. Inference: 0.1832 s/iter. Eval: 0.2176 s/iter. Total: 0.4140 s/iter. ETA=0:01:13
[01/17 05:59:50] d2.evaluation.evaluator INFO: Inference done 929/1093. Dataloading: 0.0130 s/iter. Inference: 0.1829 s/iter. Eval: 0.2177 s/iter. Total: 0.4137 s/iter. ETA=0:01:07
[01/17 05:59:55] d2.evaluation.evaluator INFO: Inference done 941/1093. Dataloading: 0.0130 s/iter. Inference: 0.1828 s/iter. Eval: 0.2179 s/iter. Total: 0.4139 s/iter. ETA=0:01:02
[01/17 06:00:00] d2.evaluation.evaluator INFO: Inference done 953/1093. Dataloading: 0.0130 s/iter. Inference: 0.1829 s/iter. Eval: 0.2182 s/iter. Total: 0.4142 s/iter. ETA=0:00:57
[01/17 06:00:05] d2.evaluation.evaluator INFO: Inference done 966/1093. Dataloading: 0.0130 s/iter. Inference: 0.1828 s/iter. Eval: 0.2182 s/iter. Total: 0.4141 s/iter. ETA=0:00:52
[01/17 06:00:11] d2.evaluation.evaluator INFO: Inference done 980/1093. Dataloading: 0.0129 s/iter. Inference: 0.1826 s/iter. Eval: 0.2179 s/iter. Total: 0.4136 s/iter. ETA=0:00:46
[01/17 06:00:16] d2.evaluation.evaluator INFO: Inference done 995/1093. Dataloading: 0.0129 s/iter. Inference: 0.1824 s/iter. Eval: 0.2172 s/iter. Total: 0.4126 s/iter. ETA=0:00:40
[01/17 06:00:21] d2.evaluation.evaluator INFO: Inference done 1009/1093. Dataloading: 0.0129 s/iter. Inference: 0.1823 s/iter. Eval: 0.2169 s/iter. Total: 0.4122 s/iter. ETA=0:00:34
[01/17 06:00:27] d2.evaluation.evaluator INFO: Inference done 1023/1093. Dataloading: 0.0128 s/iter. Inference: 0.1820 s/iter. Eval: 0.2167 s/iter. Total: 0.4116 s/iter. ETA=0:00:28
[01/17 06:00:32] d2.evaluation.evaluator INFO: Inference done 1036/1093. Dataloading: 0.0128 s/iter. Inference: 0.1820 s/iter. Eval: 0.2164 s/iter. Total: 0.4113 s/iter. ETA=0:00:23
[01/17 06:00:37] d2.evaluation.evaluator INFO: Inference done 1050/1093. Dataloading: 0.0128 s/iter. Inference: 0.1818 s/iter. Eval: 0.2160 s/iter. Total: 0.4107 s/iter. ETA=0:00:17
[01/17 06:00:42] d2.evaluation.evaluator INFO: Inference done 1065/1093. Dataloading: 0.0127 s/iter. Inference: 0.1816 s/iter. Eval: 0.2153 s/iter. Total: 0.4097 s/iter. ETA=0:00:11
[01/17 06:00:47] d2.evaluation.evaluator INFO: Inference done 1082/1093. Dataloading: 0.0127 s/iter. Inference: 0.1809 s/iter. Eval: 0.2142 s/iter. Total: 0.4080 s/iter. ETA=0:00:04
[01/17 06:00:51] d2.evaluation.evaluator INFO: Total inference time: 0:07:23.557224 (0.407681 s / iter per device, on 4 devices)
[01/17 06:00:51] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:16 (0.180617 s / iter per device, on 4 devices)
[01/17 06:01:18] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 6.913522394435604, 'fwIoU': 23.181526986390526, 'IoU-1': nan, 'IoU-2': 94.80679986029085, 'IoU-3': 39.74726418221729, 'IoU-4': 51.51522724706257, 'IoU-5': 42.63581888011155, 'IoU-6': 33.59115344175036, 'IoU-7': 33.49227412289225, 'IoU-8': 29.962324531311946, 'IoU-9': 7.461969878562619, 'IoU-10': 14.71062657782336, 'IoU-11': 12.289705344571425, 'IoU-12': 22.405956576462565, 'IoU-13': 20.17913588522604, 'IoU-14': 18.7841228434514, 'IoU-15': 19.857088160520835, 'IoU-16': 17.707375887277852, 'IoU-17': 14.276181020129993, 'IoU-18': 15.289085559136812, 'IoU-19': 14.688260105016877, 'IoU-20': 16.671611048262967, 'IoU-21': 17.703176044941973, 'IoU-22': 12.924879564940897, 'IoU-23': 18.286100064672222, 'IoU-24': 18.083923788553204, 'IoU-25': 15.669734884899967, 'IoU-26': 15.108028838049531, 'IoU-27': 17.21622738212236, 'IoU-28': 19.237002265883024, 'IoU-29': 15.936340234123076, 'IoU-30': 20.453146918847576, 'IoU-31': 17.270207489477173, 'IoU-32': 21.586330213823814, 'IoU-33': 17.42562376762908, 'IoU-34': 17.952640701755705, 'IoU-35': 21.56230720681934, 'IoU-36': 17.33075049938595, 'IoU-37': 20.442104112056263, 'IoU-38': 21.26115361346476, 'IoU-39': 15.907461280174939, 'IoU-40': 18.54088974806969, 'IoU-41': 18.993159683348317, 'IoU-42': 19.61173072247955, 'IoU-43': 16.793328685591298, 'IoU-44': 17.992536826926052, 'IoU-45': 17.35206745286366, 'IoU-46': 17.034029577570262, 'IoU-47': 17.886934125796753, 'IoU-48': 16.608600653289642, 'IoU-49': 17.32559582709913, 'IoU-50': 16.620665249068548, 'IoU-51': 18.77243558640288, 'IoU-52': 15.217506987069113, 'IoU-53': 16.312954410302837, 'IoU-54': 16.91312399135777, 'IoU-55': 13.824314373385391, 'IoU-56': 14.68139570385874, 'IoU-57': 13.611627418526517, 'IoU-58': 11.490911305180687, 'IoU-59': 13.0119667636572, 'IoU-60': 12.13280456484441, 'IoU-61': 9.548224421718764, 'IoU-62': 11.702021927623719, 'IoU-63': 9.810080390599609, 'IoU-64': 9.345283360909098, 'IoU-65': 10.283070307898749, 'IoU-66': 7.991037366552181, 'IoU-67': 7.3921310047702, 'IoU-68': 7.464158454474941, 'IoU-69': 7.078622985716361, 'IoU-70': 6.902809428672242, 'IoU-71': 5.507134930258187, 'IoU-72': 6.970512015690572, 'IoU-73': 4.853427436000621, 'IoU-74': 7.17384752489801, 'IoU-75': 5.111696275377658, 'IoU-76': 4.616463223140034, 'IoU-77': 5.754940728140723, 'IoU-78': 7.510292723163905, 'IoU-79': 9.30428445205764, 'IoU-80': 6.856283312261477, 'IoU-81': 10.155103686006932, 'IoU-82': 9.332505041311864, 'IoU-83': 4.46766060341747, 'IoU-84': 10.694543301625698, 'IoU-85': 9.919979898286368, 'IoU-86': 6.996740477778146, 'IoU-87': 10.01412217563725, 'IoU-88': 10.986114449899635, 'IoU-89': 9.98812207099274, 'IoU-90': 7.118990248476342, 'IoU-91': 10.797377839451826, 'IoU-92': 10.27470128207641, 'IoU-93': 7.871422637610752, 'IoU-94': 8.066425563185959, 'IoU-95': 10.140820807564289, 'IoU-96': 7.244751428296868, 'IoU-97': 8.103710349388002, 'IoU-98': 9.203835435963521, 'IoU-99': 10.168809703915665, 'IoU-100': 9.49431677523854, 'IoU-101': 5.881075446771423, 'IoU-102': 6.6678760322207165, 'IoU-103': 8.580087845231974, 'IoU-104': 8.603894331972553, 'IoU-105': 3.6953597565196916, 'IoU-106': 8.498223500603384, 'IoU-107': 6.783630896526209, 'IoU-108': 7.535680056417631, 'IoU-109': 4.3210251692333275, 'IoU-110': 7.457649135585396, 'IoU-111': 4.696133133471972, 'IoU-112': 6.580104637431819, 'IoU-113': 5.141489690268446, 'IoU-114': 5.019235310144401, 'IoU-115': 7.5363660439750655, 'IoU-116': 3.5884813944214415, 'IoU-117': 5.025441091272075, 'IoU-118': 6.366557843236595, 'IoU-119': 6.038914113221501, 'IoU-120': 5.392034884169344, 'IoU-121': 2.202297949452823, 'IoU-122': 3.738292260158487, 'IoU-123': 3.895872247655416, 'IoU-124': 3.5666693765883974, 'IoU-125': 4.061543078997222, 'IoU-126': 5.410590411153791, 'IoU-127': 2.930864791724007, 'IoU-128': 3.2257507641516954, 'IoU-129': 1.9236424401305054, 'IoU-130': 3.5013726625226504, 'IoU-131': 2.34081140257486, 'IoU-132': 1.5255734704824409, 'IoU-133': 3.935089768089938, 'IoU-134': 2.696601876672921, 'IoU-135': 3.1378920414308378, 'IoU-136': 1.410975627088027, 'IoU-137': 1.628195069719408, 'IoU-138': 2.3198067301805665, 'IoU-139': 2.0151886315082175, 'IoU-140': 3.1265731049781946, 'IoU-141': 2.327138623905016, 'IoU-142': 2.3835984546279194, 'IoU-143': 1.9545909208406258, 'IoU-144': 1.600934907981872, 'IoU-145': 2.4341711795115475, 'IoU-146': 2.0368560315461504, 'IoU-147': 1.1336900371100742, 'IoU-148': 2.203654555535315, 'IoU-149': 1.1271138181879086, 'IoU-150': 1.331471329033611, 'IoU-151': 1.5681681053384606, 'IoU-152': 1.1964275592910179, 'IoU-153': 1.1259963176337828, 'IoU-154': 0.33408337411485717, 'IoU-155': 0.7845072987453634, 'IoU-156': 1.4166099430359675, 'IoU-157': 0.8649128137906414, 'IoU-158': 0.30190479237501344, 'IoU-159': 0.33043068957401744, 'IoU-160': 0.19085007862533532, 'IoU-161': 0.40276313674188924, 'IoU-162': 1.064554190752272, 'IoU-163': 0.4986695611838525, 'IoU-164': 0.6064188695615038, 'IoU-165': 1.010292405940986, 'IoU-166': 0.45278050041467854, 'IoU-167': 0.0, 'IoU-168': 0.92139581664138, 'IoU-169': 1.049878994758201, 'IoU-170': 1.4181572391389794, 'IoU-171': 0.8739838234568105, 'IoU-172': 0.9573233929595073, 'IoU-173': 0.5355190803720855, 'IoU-174': 0.1683154269794797, 'IoU-175': 0.0, 'IoU-176': 1.1415436125202438, 'IoU-177': 0.033731262033460746, 'IoU-178': 0.8161412277071791, 'IoU-179': 0.0, 'IoU-180': 0.6140644411878964, 'IoU-181': 0.40897799023251874, 'IoU-182': 0.032396458604268, 'IoU-183': 0.0011724511498228426, 'IoU-184': 0.0, 'IoU-185': 0.0025577291085849014, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'IoU-192': 0.0, 'IoU-193': 0.0, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 11.765101349277396, 'pACC': 33.332784596647826, 'ACC-1': nan, 'ACC-2': 98.63752087204418, 'ACC-3': 48.173302211651105, 'ACC-4': 64.30406862144744, 'ACC-5': 57.07427685631797, 'ACC-6': 50.11623264203976, 'ACC-7': 52.01685991063005, 'ACC-8': 46.01988689193476, 'ACC-9': 8.541563259590902, 'ACC-10': 19.30672900925811, 'ACC-11': 16.913772659660285, 'ACC-12': 33.88770419471021, 'ACC-13': 33.06608175166584, 'ACC-14': 34.75662877163415, 'ACC-15': 34.60462016358271, 'ACC-16': 31.474526142326447, 'ACC-17': 21.28981425876182, 'ACC-18': 29.842357303755307, 'ACC-19': 22.65303396589427, 'ACC-20': 32.38247262606379, 'ACC-21': 35.148715271893145, 'ACC-22': 18.035527238451397, 'ACC-23': 30.292378197934834, 'ACC-24': 35.55272090286346, 'ACC-25': 23.699825547406633, 'ACC-26': 26.2262360678386, 'ACC-27': 33.7829224106014, 'ACC-28': 34.9573538794845, 'ACC-29': 26.501992949035163, 'ACC-30': 36.51878452483191, 'ACC-31': 28.968572612834393, 'ACC-32': 36.17295966984456, 'ACC-33': 26.085268394861526, 'ACC-34': 32.733616826645, 'ACC-35': 40.23850267281123, 'ACC-36': 26.34707311840019, 'ACC-37': 33.82890290978851, 'ACC-38': 40.0528507825616, 'ACC-39': 24.594461243850848, 'ACC-40': 30.574372890841257, 'ACC-41': 31.88588279160831, 'ACC-42': 35.41781701391902, 'ACC-43': 25.79160273843159, 'ACC-44': 31.229626767755335, 'ACC-45': 28.57063062897696, 'ACC-46': 28.421766461254983, 'ACC-47': 30.60427447680149, 'ACC-48': 26.725215313375315, 'ACC-49': 31.767806484847167, 'ACC-50': 27.072609698833737, 'ACC-51': 32.4686134300508, 'ACC-52': 23.866970663390997, 'ACC-53': 28.22674850062818, 'ACC-54': 30.166601229884993, 'ACC-55': 21.013723378702245, 'ACC-56': 23.1674195399585, 'ACC-57': 23.38947393315335, 'ACC-58': 17.33706688318647, 'ACC-59': 24.43845938801424, 'ACC-60': 21.318153532578147, 'ACC-61': 15.100761998404202, 'ACC-62': 23.75396985448096, 'ACC-63': 17.119773042565736, 'ACC-64': 15.732533456697759, 'ACC-65': 18.66322420203138, 'ACC-66': 13.25349042750658, 'ACC-67': 12.120962489845795, 'ACC-68': 13.171780606774428, 'ACC-69': 14.68409641520036, 'ACC-70': 11.74079463076045, 'ACC-71': 9.193714551741696, 'ACC-72': 13.541896028848226, 'ACC-73': 9.361444237456201, 'ACC-74': 15.520068117259628, 'ACC-75': 8.195582556405915, 'ACC-76': 7.217132829332375, 'ACC-77': 12.133153630135912, 'ACC-78': 16.37175763627976, 'ACC-79': 20.392686144577375, 'ACC-80': 12.446964196230999, 'ACC-81': 20.917158922513423, 'ACC-82': 21.321282590798845, 'ACC-83': 6.145593519695931, 'ACC-84': 24.020111564962313, 'ACC-85': 19.08357172661535, 'ACC-86': 11.833122480353179, 'ACC-87': 20.508992420788257, 'ACC-88': 22.851706830552153, 'ACC-89': 17.48495496607405, 'ACC-90': 14.503537452846881, 'ACC-91': 19.460271978256745, 'ACC-92': 26.108010530151716, 'ACC-93': 12.85662261199912, 'ACC-94': 16.48947665931389, 'ACC-95': 19.02919284954908, 'ACC-96': 12.57820246860726, 'ACC-97': 14.232783429580229, 'ACC-98': 15.434024861881765, 'ACC-99': 21.676744236847238, 'ACC-100': 19.413984188607436, 'ACC-101': 10.820830768756686, 'ACC-102': 12.111225456565062, 'ACC-103': 18.224580879021296, 'ACC-104': 18.055445357513932, 'ACC-105': 5.618651779346302, 'ACC-106': 17.15411189893836, 'ACC-107': 14.539861551100724, 'ACC-108': 15.565072516742164, 'ACC-109': 6.837971306316253, 'ACC-110': 13.671484642436457, 'ACC-111': 7.276775135501988, 'ACC-112': 11.11354513063947, 'ACC-113': 9.131494075229273, 'ACC-114': 9.582602936153318, 'ACC-115': 14.549797248320607, 'ACC-116': 5.9967976185106675, 'ACC-117': 11.543592033215374, 'ACC-118': 15.527108227981874, 'ACC-119': 11.410141592318418, 'ACC-120': 11.394286258378058, 'ACC-121': 3.010740502570482, 'ACC-122': 7.904290781317634, 'ACC-123': 7.181824185985073, 'ACC-124': 4.978842202700382, 'ACC-125': 8.968244116858923, 'ACC-126': 12.927102787473501, 'ACC-127': 5.7887025382744834, 'ACC-128': 5.456842415123962, 'ACC-129': 3.366910967433476, 'ACC-130': 7.048625121504758, 'ACC-131': 5.140103136132988, 'ACC-132': 3.2132373416060074, 'ACC-133': 9.354150120805729, 'ACC-134': 4.585247028244436, 'ACC-135': 5.128548039638087, 'ACC-136': 2.051693175987686, 'ACC-137': 3.4913110324263883, 'ACC-138': 5.477565929918479, 'ACC-139': 3.1020763002259053, 'ACC-140': 13.666947324581288, 'ACC-141': 5.785696892595003, 'ACC-142': 5.8732889105074735, 'ACC-143': 3.3413940392695234, 'ACC-144': 3.1796652388051254, 'ACC-145': 4.29043321299639, 'ACC-146': 4.072810214897471, 'ACC-147': 2.9168170706632246, 'ACC-148': 4.63888473634217, 'ACC-149': 1.5351917350014401, 'ACC-150': 3.4508741274697816, 'ACC-151': 3.093675377237445, 'ACC-152': 3.149582801777289, 'ACC-153': 2.4970075527682245, 'ACC-154': 0.451066344198318, 'ACC-155': 0.961056375320401, 'ACC-156': 4.696755681440013, 'ACC-157': 1.6411668508858681, 'ACC-158': 0.42382743118928917, 'ACC-159': 0.3963017482157247, 'ACC-160': 0.19890157561804092, 'ACC-161': 0.4640207211690721, 'ACC-162': 1.3061781580950187, 'ACC-163': 0.7948503574818284, 'ACC-164': 0.824865821724422, 'ACC-165': 1.538563985806652, 'ACC-166': 0.534102856489437, 'ACC-167': 0.0, 'ACC-168': 2.065603899476607, 'ACC-169': 2.7946761375624165, 'ACC-170': 13.776445753852041, 'ACC-171': 3.0159930150347116, 'ACC-172': 1.6779663818930253, 'ACC-173': 0.717672333963033, 'ACC-174': 0.19642299428416612, 'ACC-175': 0.0, 'ACC-176': 2.165918809937158, 'ACC-177': 0.03664093336429069, 'ACC-178': 1.8744382867445821, 'ACC-179': 0.0, 'ACC-180': 1.2956474280810175, 'ACC-181': 0.735929086660571, 'ACC-182': 0.03316113231054926, 'ACC-183': 0.0011845842938337649, 'ACC-184': 0.0, 'ACC-185': 0.002594235608477962, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0, 'ACC-192': 0.0, 'ACC-193': 0.0, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 06:01:18] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 06:01:18] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 06:01:18] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 06:01:18] d2.evaluation.testing INFO: copypaste: 6.9135,23.1815,11.7651,33.3328
[01/17 06:01:19] d2.utils.events INFO:  eta: 1 day, 11:41:17  iter: 5999  total_loss: 46.64  loss_ce: 0.3442  loss_mask: 0.5095  loss_dice: 3.771  loss_ce_0: 0.6433  loss_mask_0: 0.4866  loss_dice_0: 3.875  loss_ce_1: 0.3764  loss_mask_1: 0.5113  loss_dice_1: 3.802  loss_ce_2: 0.3778  loss_mask_2: 0.5149  loss_dice_2: 3.776  loss_ce_3: 0.3704  loss_mask_3: 0.5148  loss_dice_3: 3.775  loss_ce_4: 0.3659  loss_mask_4: 0.5124  loss_dice_4: 3.766  loss_ce_5: 0.3683  loss_mask_5: 0.5119  loss_dice_5: 3.767  loss_ce_6: 0.3496  loss_mask_6: 0.511  loss_dice_6: 3.768  loss_ce_7: 0.3592  loss_mask_7: 0.5148  loss_dice_7: 3.761  loss_ce_8: 0.3642  loss_mask_8: 0.5098  loss_dice_8: 3.768  time: 1.5342  data_time: 0.0991  lr: 9.3981e-06  max_mem: 21366M
[01/17 06:01:50] d2.utils.events INFO:  eta: 1 day, 11:41:03  iter: 6019  total_loss: 47.23  loss_ce: 0.3873  loss_mask: 0.5243  loss_dice: 3.776  loss_ce_0: 0.6198  loss_mask_0: 0.4948  loss_dice_0: 3.889  loss_ce_1: 0.4122  loss_mask_1: 0.5245  loss_dice_1: 3.808  loss_ce_2: 0.3912  loss_mask_2: 0.5243  loss_dice_2: 3.784  loss_ce_3: 0.3975  loss_mask_3: 0.5234  loss_dice_3: 3.775  loss_ce_4: 0.3897  loss_mask_4: 0.5243  loss_dice_4: 3.774  loss_ce_5: 0.3881  loss_mask_5: 0.5233  loss_dice_5: 3.777  loss_ce_6: 0.3801  loss_mask_6: 0.5236  loss_dice_6: 3.778  loss_ce_7: 0.3749  loss_mask_7: 0.5235  loss_dice_7: 3.77  loss_ce_8: 0.3701  loss_mask_8: 0.525  loss_dice_8: 3.776  time: 1.5342  data_time: 0.1017  lr: 9.396e-06  max_mem: 21366M
[01/17 06:02:21] d2.utils.events INFO:  eta: 1 day, 11:41:09  iter: 6039  total_loss: 46.97  loss_ce: 0.34  loss_mask: 0.5077  loss_dice: 3.779  loss_ce_0: 0.6293  loss_mask_0: 0.4871  loss_dice_0: 3.895  loss_ce_1: 0.3795  loss_mask_1: 0.5056  loss_dice_1: 3.815  loss_ce_2: 0.3696  loss_mask_2: 0.5141  loss_dice_2: 3.794  loss_ce_3: 0.3787  loss_mask_3: 0.515  loss_dice_3: 3.785  loss_ce_4: 0.3837  loss_mask_4: 0.5139  loss_dice_4: 3.787  loss_ce_5: 0.3731  loss_mask_5: 0.5148  loss_dice_5: 3.782  loss_ce_6: 0.3633  loss_mask_6: 0.5115  loss_dice_6: 3.778  loss_ce_7: 0.3599  loss_mask_7: 0.5061  loss_dice_7: 3.79  loss_ce_8: 0.3732  loss_mask_8: 0.5074  loss_dice_8: 3.785  time: 1.5342  data_time: 0.1003  lr: 9.394e-06  max_mem: 21366M
[01/17 06:02:51] d2.utils.events INFO:  eta: 1 day, 11:39:21  iter: 6059  total_loss: 46.52  loss_ce: 0.361  loss_mask: 0.5161  loss_dice: 3.722  loss_ce_0: 0.623  loss_mask_0: 0.4826  loss_dice_0: 3.847  loss_ce_1: 0.3854  loss_mask_1: 0.5127  loss_dice_1: 3.758  loss_ce_2: 0.378  loss_mask_2: 0.5133  loss_dice_2: 3.736  loss_ce_3: 0.3782  loss_mask_3: 0.513  loss_dice_3: 3.731  loss_ce_4: 0.3709  loss_mask_4: 0.5107  loss_dice_4: 3.725  loss_ce_5: 0.3765  loss_mask_5: 0.5158  loss_dice_5: 3.724  loss_ce_6: 0.3533  loss_mask_6: 0.5159  loss_dice_6: 3.72  loss_ce_7: 0.3689  loss_mask_7: 0.514  loss_dice_7: 3.726  loss_ce_8: 0.3746  loss_mask_8: 0.5148  loss_dice_8: 3.722  time: 1.5341  data_time: 0.0944  lr: 9.392e-06  max_mem: 21366M
[01/17 06:03:21] d2.utils.events INFO:  eta: 1 day, 11:38:46  iter: 6079  total_loss: 46.36  loss_ce: 0.3465  loss_mask: 0.4953  loss_dice: 3.752  loss_ce_0: 0.639  loss_mask_0: 0.472  loss_dice_0: 3.862  loss_ce_1: 0.3639  loss_mask_1: 0.4922  loss_dice_1: 3.78  loss_ce_2: 0.3657  loss_mask_2: 0.4954  loss_dice_2: 3.757  loss_ce_3: 0.3527  loss_mask_3: 0.4949  loss_dice_3: 3.757  loss_ce_4: 0.3532  loss_mask_4: 0.4975  loss_dice_4: 3.754  loss_ce_5: 0.339  loss_mask_5: 0.494  loss_dice_5: 3.75  loss_ce_6: 0.3523  loss_mask_6: 0.4935  loss_dice_6: 3.747  loss_ce_7: 0.3546  loss_mask_7: 0.4944  loss_dice_7: 3.747  loss_ce_8: 0.3491  loss_mask_8: 0.4955  loss_dice_8: 3.747  time: 1.5341  data_time: 0.0905  lr: 9.39e-06  max_mem: 21366M
[01/17 06:03:52] d2.utils.events INFO:  eta: 1 day, 11:37:43  iter: 6099  total_loss: 47.03  loss_ce: 0.3739  loss_mask: 0.5091  loss_dice: 3.787  loss_ce_0: 0.6375  loss_mask_0: 0.4844  loss_dice_0: 3.89  loss_ce_1: 0.3811  loss_mask_1: 0.5068  loss_dice_1: 3.81  loss_ce_2: 0.3761  loss_mask_2: 0.5064  loss_dice_2: 3.791  loss_ce_3: 0.3754  loss_mask_3: 0.507  loss_dice_3: 3.783  loss_ce_4: 0.3648  loss_mask_4: 0.507  loss_dice_4: 3.785  loss_ce_5: 0.3657  loss_mask_5: 0.5108  loss_dice_5: 3.785  loss_ce_6: 0.3666  loss_mask_6: 0.5087  loss_dice_6: 3.784  loss_ce_7: 0.3652  loss_mask_7: 0.5073  loss_dice_7: 3.785  loss_ce_8: 0.3643  loss_mask_8: 0.5109  loss_dice_8: 3.783  time: 1.5340  data_time: 0.1030  lr: 9.388e-06  max_mem: 21366M
[01/17 06:04:22] d2.utils.events INFO:  eta: 1 day, 11:37:13  iter: 6119  total_loss: 46.75  loss_ce: 0.3599  loss_mask: 0.4975  loss_dice: 3.77  loss_ce_0: 0.6134  loss_mask_0: 0.4728  loss_dice_0: 3.886  loss_ce_1: 0.371  loss_mask_1: 0.4879  loss_dice_1: 3.807  loss_ce_2: 0.3737  loss_mask_2: 0.4874  loss_dice_2: 3.781  loss_ce_3: 0.3627  loss_mask_3: 0.494  loss_dice_3: 3.771  loss_ce_4: 0.3478  loss_mask_4: 0.4978  loss_dice_4: 3.771  loss_ce_5: 0.3809  loss_mask_5: 0.4943  loss_dice_5: 3.767  loss_ce_6: 0.365  loss_mask_6: 0.4992  loss_dice_6: 3.763  loss_ce_7: 0.354  loss_mask_7: 0.4979  loss_dice_7: 3.771  loss_ce_8: 0.3589  loss_mask_8: 0.4986  loss_dice_8: 3.772  time: 1.5340  data_time: 0.0940  lr: 9.386e-06  max_mem: 21366M
[01/17 06:04:53] d2.utils.events INFO:  eta: 1 day, 11:37:19  iter: 6139  total_loss: 46.38  loss_ce: 0.3539  loss_mask: 0.5127  loss_dice: 3.719  loss_ce_0: 0.6389  loss_mask_0: 0.4838  loss_dice_0: 3.85  loss_ce_1: 0.3677  loss_mask_1: 0.5104  loss_dice_1: 3.747  loss_ce_2: 0.3583  loss_mask_2: 0.5139  loss_dice_2: 3.736  loss_ce_3: 0.3454  loss_mask_3: 0.5093  loss_dice_3: 3.732  loss_ce_4: 0.3454  loss_mask_4: 0.5128  loss_dice_4: 3.721  loss_ce_5: 0.3472  loss_mask_5: 0.5103  loss_dice_5: 3.722  loss_ce_6: 0.3324  loss_mask_6: 0.5111  loss_dice_6: 3.726  loss_ce_7: 0.3307  loss_mask_7: 0.5113  loss_dice_7: 3.721  loss_ce_8: 0.3316  loss_mask_8: 0.5128  loss_dice_8: 3.726  time: 1.5340  data_time: 0.0946  lr: 9.384e-06  max_mem: 21366M
[01/17 06:05:23] d2.utils.events INFO:  eta: 1 day, 11:36:12  iter: 6159  total_loss: 46.89  loss_ce: 0.3883  loss_mask: 0.5087  loss_dice: 3.735  loss_ce_0: 0.6397  loss_mask_0: 0.4876  loss_dice_0: 3.855  loss_ce_1: 0.4058  loss_mask_1: 0.5138  loss_dice_1: 3.763  loss_ce_2: 0.3976  loss_mask_2: 0.5124  loss_dice_2: 3.738  loss_ce_3: 0.3929  loss_mask_3: 0.5123  loss_dice_3: 3.737  loss_ce_4: 0.3785  loss_mask_4: 0.5118  loss_dice_4: 3.73  loss_ce_5: 0.3729  loss_mask_5: 0.5112  loss_dice_5: 3.738  loss_ce_6: 0.3871  loss_mask_6: 0.508  loss_dice_6: 3.728  loss_ce_7: 0.3798  loss_mask_7: 0.5082  loss_dice_7: 3.728  loss_ce_8: 0.3721  loss_mask_8: 0.5079  loss_dice_8: 3.731  time: 1.5340  data_time: 0.1009  lr: 9.3819e-06  max_mem: 21366M
[01/17 06:05:54] d2.utils.events INFO:  eta: 1 day, 11:34:09  iter: 6179  total_loss: 47.02  loss_ce: 0.398  loss_mask: 0.5175  loss_dice: 3.757  loss_ce_0: 0.6405  loss_mask_0: 0.4947  loss_dice_0: 3.869  loss_ce_1: 0.3876  loss_mask_1: 0.5113  loss_dice_1: 3.786  loss_ce_2: 0.3849  loss_mask_2: 0.5168  loss_dice_2: 3.774  loss_ce_3: 0.4036  loss_mask_3: 0.52  loss_dice_3: 3.758  loss_ce_4: 0.3846  loss_mask_4: 0.5198  loss_dice_4: 3.759  loss_ce_5: 0.3947  loss_mask_5: 0.5211  loss_dice_5: 3.757  loss_ce_6: 0.3879  loss_mask_6: 0.5198  loss_dice_6: 3.756  loss_ce_7: 0.3944  loss_mask_7: 0.5197  loss_dice_7: 3.751  loss_ce_8: 0.3941  loss_mask_8: 0.5185  loss_dice_8: 3.753  time: 1.5339  data_time: 0.0933  lr: 9.3799e-06  max_mem: 21366M
[01/17 06:06:25] d2.utils.events INFO:  eta: 1 day, 11:34:15  iter: 6199  total_loss: 47.01  loss_ce: 0.3596  loss_mask: 0.5056  loss_dice: 3.771  loss_ce_0: 0.6373  loss_mask_0: 0.4894  loss_dice_0: 3.891  loss_ce_1: 0.3936  loss_mask_1: 0.5096  loss_dice_1: 3.8  loss_ce_2: 0.3845  loss_mask_2: 0.5083  loss_dice_2: 3.786  loss_ce_3: 0.3851  loss_mask_3: 0.5058  loss_dice_3: 3.773  loss_ce_4: 0.3834  loss_mask_4: 0.504  loss_dice_4: 3.773  loss_ce_5: 0.3699  loss_mask_5: 0.5037  loss_dice_5: 3.777  loss_ce_6: 0.3803  loss_mask_6: 0.504  loss_dice_6: 3.778  loss_ce_7: 0.3729  loss_mask_7: 0.5056  loss_dice_7: 3.778  loss_ce_8: 0.3713  loss_mask_8: 0.5045  loss_dice_8: 3.773  time: 1.5339  data_time: 0.0986  lr: 9.3779e-06  max_mem: 21366M
[01/17 06:06:55] d2.utils.events INFO:  eta: 1 day, 11:33:08  iter: 6219  total_loss: 46.53  loss_ce: 0.354  loss_mask: 0.5139  loss_dice: 3.724  loss_ce_0: 0.6097  loss_mask_0: 0.4851  loss_dice_0: 3.833  loss_ce_1: 0.3735  loss_mask_1: 0.5125  loss_dice_1: 3.744  loss_ce_2: 0.366  loss_mask_2: 0.5129  loss_dice_2: 3.724  loss_ce_3: 0.3523  loss_mask_3: 0.5147  loss_dice_3: 3.722  loss_ce_4: 0.3582  loss_mask_4: 0.511  loss_dice_4: 3.723  loss_ce_5: 0.3539  loss_mask_5: 0.5151  loss_dice_5: 3.72  loss_ce_6: 0.3601  loss_mask_6: 0.5144  loss_dice_6: 3.708  loss_ce_7: 0.3584  loss_mask_7: 0.5158  loss_dice_7: 3.713  loss_ce_8: 0.3653  loss_mask_8: 0.5145  loss_dice_8: 3.719  time: 1.5338  data_time: 0.0896  lr: 9.3759e-06  max_mem: 21366M
[01/17 06:07:26] d2.utils.events INFO:  eta: 1 day, 11:32:54  iter: 6239  total_loss: 46.74  loss_ce: 0.3693  loss_mask: 0.5041  loss_dice: 3.753  loss_ce_0: 0.6391  loss_mask_0: 0.477  loss_dice_0: 3.865  loss_ce_1: 0.3874  loss_mask_1: 0.4966  loss_dice_1: 3.782  loss_ce_2: 0.3844  loss_mask_2: 0.4993  loss_dice_2: 3.763  loss_ce_3: 0.3759  loss_mask_3: 0.4998  loss_dice_3: 3.76  loss_ce_4: 0.3708  loss_mask_4: 0.5058  loss_dice_4: 3.754  loss_ce_5: 0.3733  loss_mask_5: 0.5062  loss_dice_5: 3.756  loss_ce_6: 0.3565  loss_mask_6: 0.5074  loss_dice_6: 3.757  loss_ce_7: 0.3597  loss_mask_7: 0.5029  loss_dice_7: 3.752  loss_ce_8: 0.3669  loss_mask_8: 0.5039  loss_dice_8: 3.755  time: 1.5338  data_time: 0.1023  lr: 9.3739e-06  max_mem: 21366M
[01/17 06:07:56] d2.utils.events INFO:  eta: 1 day, 11:32:20  iter: 6259  total_loss: 46.41  loss_ce: 0.3678  loss_mask: 0.5015  loss_dice: 3.724  loss_ce_0: 0.6481  loss_mask_0: 0.4865  loss_dice_0: 3.831  loss_ce_1: 0.3895  loss_mask_1: 0.5054  loss_dice_1: 3.754  loss_ce_2: 0.3871  loss_mask_2: 0.5034  loss_dice_2: 3.733  loss_ce_3: 0.3696  loss_mask_3: 0.5031  loss_dice_3: 3.728  loss_ce_4: 0.3772  loss_mask_4: 0.4994  loss_dice_4: 3.726  loss_ce_5: 0.3724  loss_mask_5: 0.5  loss_dice_5: 3.735  loss_ce_6: 0.3814  loss_mask_6: 0.4999  loss_dice_6: 3.725  loss_ce_7: 0.3776  loss_mask_7: 0.5027  loss_dice_7: 3.725  loss_ce_8: 0.3682  loss_mask_8: 0.5021  loss_dice_8: 3.724  time: 1.5338  data_time: 0.0954  lr: 9.3719e-06  max_mem: 21366M
[01/17 06:08:27] d2.utils.events INFO:  eta: 1 day, 11:30:26  iter: 6279  total_loss: 47.07  loss_ce: 0.3887  loss_mask: 0.5046  loss_dice: 3.748  loss_ce_0: 0.6761  loss_mask_0: 0.4893  loss_dice_0: 3.842  loss_ce_1: 0.4182  loss_mask_1: 0.5057  loss_dice_1: 3.776  loss_ce_2: 0.4141  loss_mask_2: 0.5035  loss_dice_2: 3.753  loss_ce_3: 0.4032  loss_mask_3: 0.5039  loss_dice_3: 3.754  loss_ce_4: 0.3788  loss_mask_4: 0.5072  loss_dice_4: 3.754  loss_ce_5: 0.3902  loss_mask_5: 0.5055  loss_dice_5: 3.757  loss_ce_6: 0.3874  loss_mask_6: 0.5067  loss_dice_6: 3.765  loss_ce_7: 0.4005  loss_mask_7: 0.507  loss_dice_7: 3.755  loss_ce_8: 0.3903  loss_mask_8: 0.5078  loss_dice_8: 3.758  time: 1.5338  data_time: 0.1061  lr: 9.3699e-06  max_mem: 21366M
[01/17 06:08:57] d2.utils.events INFO:  eta: 1 day, 11:29:31  iter: 6299  total_loss: 46.81  loss_ce: 0.3542  loss_mask: 0.5229  loss_dice: 3.746  loss_ce_0: 0.6452  loss_mask_0: 0.5078  loss_dice_0: 3.866  loss_ce_1: 0.3866  loss_mask_1: 0.5242  loss_dice_1: 3.795  loss_ce_2: 0.3869  loss_mask_2: 0.5232  loss_dice_2: 3.771  loss_ce_3: 0.3562  loss_mask_3: 0.5262  loss_dice_3: 3.762  loss_ce_4: 0.3761  loss_mask_4: 0.525  loss_dice_4: 3.76  loss_ce_5: 0.3627  loss_mask_5: 0.5242  loss_dice_5: 3.759  loss_ce_6: 0.3712  loss_mask_6: 0.5252  loss_dice_6: 3.751  loss_ce_7: 0.3548  loss_mask_7: 0.5251  loss_dice_7: 3.747  loss_ce_8: 0.3575  loss_mask_8: 0.5224  loss_dice_8: 3.746  time: 1.5338  data_time: 0.1001  lr: 9.3678e-06  max_mem: 21366M
[01/17 06:09:28] d2.utils.events INFO:  eta: 1 day, 11:28:41  iter: 6319  total_loss: 46.77  loss_ce: 0.3695  loss_mask: 0.5028  loss_dice: 3.76  loss_ce_0: 0.6318  loss_mask_0: 0.4834  loss_dice_0: 3.878  loss_ce_1: 0.3834  loss_mask_1: 0.5055  loss_dice_1: 3.797  loss_ce_2: 0.3852  loss_mask_2: 0.5032  loss_dice_2: 3.769  loss_ce_3: 0.37  loss_mask_3: 0.5037  loss_dice_3: 3.761  loss_ce_4: 0.3783  loss_mask_4: 0.5044  loss_dice_4: 3.767  loss_ce_5: 0.3591  loss_mask_5: 0.5035  loss_dice_5: 3.764  loss_ce_6: 0.3654  loss_mask_6: 0.5038  loss_dice_6: 3.757  loss_ce_7: 0.3645  loss_mask_7: 0.5071  loss_dice_7: 3.757  loss_ce_8: 0.3753  loss_mask_8: 0.5047  loss_dice_8: 3.757  time: 1.5337  data_time: 0.0887  lr: 9.3658e-06  max_mem: 21366M
[01/17 06:09:59] d2.utils.events INFO:  eta: 1 day, 11:28:05  iter: 6339  total_loss: 46.64  loss_ce: 0.3708  loss_mask: 0.5091  loss_dice: 3.704  loss_ce_0: 0.6484  loss_mask_0: 0.4868  loss_dice_0: 3.815  loss_ce_1: 0.376  loss_mask_1: 0.5101  loss_dice_1: 3.736  loss_ce_2: 0.3814  loss_mask_2: 0.5101  loss_dice_2: 3.713  loss_ce_3: 0.3915  loss_mask_3: 0.5111  loss_dice_3: 3.709  loss_ce_4: 0.3785  loss_mask_4: 0.5123  loss_dice_4: 3.702  loss_ce_5: 0.3747  loss_mask_5: 0.5121  loss_dice_5: 3.711  loss_ce_6: 0.3798  loss_mask_6: 0.5117  loss_dice_6: 3.698  loss_ce_7: 0.3743  loss_mask_7: 0.5106  loss_dice_7: 3.708  loss_ce_8: 0.3805  loss_mask_8: 0.5087  loss_dice_8: 3.704  time: 1.5337  data_time: 0.1074  lr: 9.3638e-06  max_mem: 21366M
[01/17 06:10:29] d2.utils.events INFO:  eta: 1 day, 11:28:24  iter: 6359  total_loss: 46.52  loss_ce: 0.3516  loss_mask: 0.5123  loss_dice: 3.73  loss_ce_0: 0.6481  loss_mask_0: 0.4894  loss_dice_0: 3.833  loss_ce_1: 0.381  loss_mask_1: 0.5134  loss_dice_1: 3.757  loss_ce_2: 0.3735  loss_mask_2: 0.5166  loss_dice_2: 3.736  loss_ce_3: 0.3737  loss_mask_3: 0.5146  loss_dice_3: 3.723  loss_ce_4: 0.3701  loss_mask_4: 0.5113  loss_dice_4: 3.73  loss_ce_5: 0.3681  loss_mask_5: 0.5103  loss_dice_5: 3.724  loss_ce_6: 0.3678  loss_mask_6: 0.5106  loss_dice_6: 3.72  loss_ce_7: 0.3713  loss_mask_7: 0.5107  loss_dice_7: 3.721  loss_ce_8: 0.361  loss_mask_8: 0.5086  loss_dice_8: 3.729  time: 1.5337  data_time: 0.0858  lr: 9.3618e-06  max_mem: 21366M
[01/17 06:11:00] d2.utils.events INFO:  eta: 1 day, 11:27:10  iter: 6379  total_loss: 46.05  loss_ce: 0.3596  loss_mask: 0.5239  loss_dice: 3.725  loss_ce_0: 0.6308  loss_mask_0: 0.4978  loss_dice_0: 3.839  loss_ce_1: 0.3838  loss_mask_1: 0.5146  loss_dice_1: 3.758  loss_ce_2: 0.3809  loss_mask_2: 0.5144  loss_dice_2: 3.735  loss_ce_3: 0.3673  loss_mask_3: 0.5186  loss_dice_3: 3.739  loss_ce_4: 0.3509  loss_mask_4: 0.5208  loss_dice_4: 3.724  loss_ce_5: 0.3401  loss_mask_5: 0.5214  loss_dice_5: 3.735  loss_ce_6: 0.3388  loss_mask_6: 0.5231  loss_dice_6: 3.726  loss_ce_7: 0.3484  loss_mask_7: 0.5241  loss_dice_7: 3.728  loss_ce_8: 0.3452  loss_mask_8: 0.5229  loss_dice_8: 3.724  time: 1.5337  data_time: 0.0839  lr: 9.3598e-06  max_mem: 21366M
[01/17 06:11:30] d2.utils.events INFO:  eta: 1 day, 11:25:45  iter: 6399  total_loss: 46.48  loss_ce: 0.3994  loss_mask: 0.508  loss_dice: 3.707  loss_ce_0: 0.6515  loss_mask_0: 0.4844  loss_dice_0: 3.813  loss_ce_1: 0.4068  loss_mask_1: 0.4983  loss_dice_1: 3.748  loss_ce_2: 0.4253  loss_mask_2: 0.5033  loss_dice_2: 3.72  loss_ce_3: 0.411  loss_mask_3: 0.5049  loss_dice_3: 3.714  loss_ce_4: 0.412  loss_mask_4: 0.5061  loss_dice_4: 3.708  loss_ce_5: 0.4094  loss_mask_5: 0.509  loss_dice_5: 3.714  loss_ce_6: 0.3948  loss_mask_6: 0.5098  loss_dice_6: 3.707  loss_ce_7: 0.3958  loss_mask_7: 0.5107  loss_dice_7: 3.706  loss_ce_8: 0.4066  loss_mask_8: 0.5098  loss_dice_8: 3.701  time: 1.5336  data_time: 0.0900  lr: 9.3578e-06  max_mem: 21366M
[01/17 06:12:00] d2.utils.events INFO:  eta: 1 day, 11:24:17  iter: 6419  total_loss: 46.34  loss_ce: 0.3775  loss_mask: 0.5071  loss_dice: 3.709  loss_ce_0: 0.6576  loss_mask_0: 0.4778  loss_dice_0: 3.815  loss_ce_1: 0.3947  loss_mask_1: 0.4999  loss_dice_1: 3.737  loss_ce_2: 0.4  loss_mask_2: 0.5056  loss_dice_2: 3.718  loss_ce_3: 0.378  loss_mask_3: 0.5047  loss_dice_3: 3.707  loss_ce_4: 0.3913  loss_mask_4: 0.5054  loss_dice_4: 3.703  loss_ce_5: 0.3814  loss_mask_5: 0.5066  loss_dice_5: 3.71  loss_ce_6: 0.3772  loss_mask_6: 0.5067  loss_dice_6: 3.698  loss_ce_7: 0.3741  loss_mask_7: 0.5078  loss_dice_7: 3.701  loss_ce_8: 0.3762  loss_mask_8: 0.5091  loss_dice_8: 3.697  time: 1.5335  data_time: 0.0974  lr: 9.3557e-06  max_mem: 21366M
[01/17 06:12:31] d2.utils.events INFO:  eta: 1 day, 11:25:10  iter: 6439  total_loss: 46.19  loss_ce: 0.3629  loss_mask: 0.5119  loss_dice: 3.702  loss_ce_0: 0.6537  loss_mask_0: 0.476  loss_dice_0: 3.816  loss_ce_1: 0.3885  loss_mask_1: 0.4971  loss_dice_1: 3.735  loss_ce_2: 0.3893  loss_mask_2: 0.5027  loss_dice_2: 3.712  loss_ce_3: 0.3552  loss_mask_3: 0.503  loss_dice_3: 3.71  loss_ce_4: 0.3786  loss_mask_4: 0.5007  loss_dice_4: 3.708  loss_ce_5: 0.3635  loss_mask_5: 0.5031  loss_dice_5: 3.708  loss_ce_6: 0.3485  loss_mask_6: 0.5063  loss_dice_6: 3.702  loss_ce_7: 0.3661  loss_mask_7: 0.5068  loss_dice_7: 3.695  loss_ce_8: 0.3616  loss_mask_8: 0.5064  loss_dice_8: 3.705  time: 1.5335  data_time: 0.1045  lr: 9.3537e-06  max_mem: 21366M
[01/17 06:13:01] d2.utils.events INFO:  eta: 1 day, 11:24:59  iter: 6459  total_loss: 46.45  loss_ce: 0.3675  loss_mask: 0.51  loss_dice: 3.715  loss_ce_0: 0.6024  loss_mask_0: 0.4928  loss_dice_0: 3.828  loss_ce_1: 0.3896  loss_mask_1: 0.5105  loss_dice_1: 3.753  loss_ce_2: 0.3782  loss_mask_2: 0.508  loss_dice_2: 3.731  loss_ce_3: 0.3648  loss_mask_3: 0.5103  loss_dice_3: 3.722  loss_ce_4: 0.375  loss_mask_4: 0.5081  loss_dice_4: 3.717  loss_ce_5: 0.3649  loss_mask_5: 0.5095  loss_dice_5: 3.722  loss_ce_6: 0.3755  loss_mask_6: 0.5119  loss_dice_6: 3.715  loss_ce_7: 0.3712  loss_mask_7: 0.5096  loss_dice_7: 3.722  loss_ce_8: 0.3633  loss_mask_8: 0.5092  loss_dice_8: 3.726  time: 1.5335  data_time: 0.0906  lr: 9.3517e-06  max_mem: 21366M
[01/17 06:13:32] d2.utils.events INFO:  eta: 1 day, 11:24:09  iter: 6479  total_loss: 46.81  loss_ce: 0.3721  loss_mask: 0.508  loss_dice: 3.74  loss_ce_0: 0.6287  loss_mask_0: 0.48  loss_dice_0: 3.85  loss_ce_1: 0.4032  loss_mask_1: 0.506  loss_dice_1: 3.768  loss_ce_2: 0.3988  loss_mask_2: 0.5095  loss_dice_2: 3.742  loss_ce_3: 0.391  loss_mask_3: 0.5128  loss_dice_3: 3.731  loss_ce_4: 0.404  loss_mask_4: 0.5109  loss_dice_4: 3.733  loss_ce_5: 0.3859  loss_mask_5: 0.5109  loss_dice_5: 3.742  loss_ce_6: 0.3798  loss_mask_6: 0.5077  loss_dice_6: 3.738  loss_ce_7: 0.3743  loss_mask_7: 0.5113  loss_dice_7: 3.736  loss_ce_8: 0.3756  loss_mask_8: 0.5112  loss_dice_8: 3.737  time: 1.5334  data_time: 0.1027  lr: 9.3497e-06  max_mem: 21366M
[01/17 06:14:02] d2.utils.events INFO:  eta: 1 day, 11:22:52  iter: 6499  total_loss: 46.22  loss_ce: 0.3693  loss_mask: 0.5064  loss_dice: 3.698  loss_ce_0: 0.6725  loss_mask_0: 0.489  loss_dice_0: 3.82  loss_ce_1: 0.4093  loss_mask_1: 0.5085  loss_dice_1: 3.747  loss_ce_2: 0.3866  loss_mask_2: 0.5076  loss_dice_2: 3.72  loss_ce_3: 0.3815  loss_mask_3: 0.5058  loss_dice_3: 3.714  loss_ce_4: 0.3668  loss_mask_4: 0.5047  loss_dice_4: 3.712  loss_ce_5: 0.364  loss_mask_5: 0.5044  loss_dice_5: 3.709  loss_ce_6: 0.3681  loss_mask_6: 0.5079  loss_dice_6: 3.704  loss_ce_7: 0.3739  loss_mask_7: 0.5052  loss_dice_7: 3.709  loss_ce_8: 0.3692  loss_mask_8: 0.5053  loss_dice_8: 3.705  time: 1.5334  data_time: 0.1003  lr: 9.3477e-06  max_mem: 21366M
[01/17 06:14:33] d2.utils.events INFO:  eta: 1 day, 11:22:21  iter: 6519  total_loss: 46.68  loss_ce: 0.3758  loss_mask: 0.4909  loss_dice: 3.738  loss_ce_0: 0.6612  loss_mask_0: 0.4605  loss_dice_0: 3.859  loss_ce_1: 0.3891  loss_mask_1: 0.4899  loss_dice_1: 3.772  loss_ce_2: 0.389  loss_mask_2: 0.4903  loss_dice_2: 3.753  loss_ce_3: 0.3735  loss_mask_3: 0.4911  loss_dice_3: 3.751  loss_ce_4: 0.3745  loss_mask_4: 0.4912  loss_dice_4: 3.751  loss_ce_5: 0.3757  loss_mask_5: 0.4937  loss_dice_5: 3.747  loss_ce_6: 0.3627  loss_mask_6: 0.4914  loss_dice_6: 3.752  loss_ce_7: 0.3797  loss_mask_7: 0.4922  loss_dice_7: 3.748  loss_ce_8: 0.3725  loss_mask_8: 0.4928  loss_dice_8: 3.742  time: 1.5334  data_time: 0.1061  lr: 9.3457e-06  max_mem: 21366M
[01/17 06:15:04] d2.utils.events INFO:  eta: 1 day, 11:21:27  iter: 6539  total_loss: 47.13  loss_ce: 0.3697  loss_mask: 0.52  loss_dice: 3.758  loss_ce_0: 0.6204  loss_mask_0: 0.4966  loss_dice_0: 3.86  loss_ce_1: 0.3822  loss_mask_1: 0.5169  loss_dice_1: 3.794  loss_ce_2: 0.3868  loss_mask_2: 0.522  loss_dice_2: 3.764  loss_ce_3: 0.3815  loss_mask_3: 0.5164  loss_dice_3: 3.762  loss_ce_4: 0.3622  loss_mask_4: 0.5196  loss_dice_4: 3.759  loss_ce_5: 0.3819  loss_mask_5: 0.5189  loss_dice_5: 3.764  loss_ce_6: 0.3765  loss_mask_6: 0.5184  loss_dice_6: 3.761  loss_ce_7: 0.362  loss_mask_7: 0.5193  loss_dice_7: 3.762  loss_ce_8: 0.393  loss_mask_8: 0.5179  loss_dice_8: 3.765  time: 1.5334  data_time: 0.1049  lr: 9.3437e-06  max_mem: 21366M
[01/17 06:15:34] d2.utils.events INFO:  eta: 1 day, 11:19:55  iter: 6559  total_loss: 46.41  loss_ce: 0.3855  loss_mask: 0.5209  loss_dice: 3.691  loss_ce_0: 0.6653  loss_mask_0: 0.4966  loss_dice_0: 3.8  loss_ce_1: 0.4102  loss_mask_1: 0.5127  loss_dice_1: 3.725  loss_ce_2: 0.3966  loss_mask_2: 0.5171  loss_dice_2: 3.704  loss_ce_3: 0.3934  loss_mask_3: 0.5179  loss_dice_3: 3.697  loss_ce_4: 0.3684  loss_mask_4: 0.5182  loss_dice_4: 3.699  loss_ce_5: 0.3736  loss_mask_5: 0.5207  loss_dice_5: 3.702  loss_ce_6: 0.3852  loss_mask_6: 0.5208  loss_dice_6: 3.7  loss_ce_7: 0.3871  loss_mask_7: 0.5207  loss_dice_7: 3.698  loss_ce_8: 0.3799  loss_mask_8: 0.5215  loss_dice_8: 3.694  time: 1.5333  data_time: 0.0940  lr: 9.3416e-06  max_mem: 21366M
[01/17 06:16:04] d2.utils.events INFO:  eta: 1 day, 11:17:58  iter: 6579  total_loss: 45.74  loss_ce: 0.3337  loss_mask: 0.5153  loss_dice: 3.68  loss_ce_0: 0.6229  loss_mask_0: 0.4939  loss_dice_0: 3.787  loss_ce_1: 0.3687  loss_mask_1: 0.5148  loss_dice_1: 3.706  loss_ce_2: 0.3422  loss_mask_2: 0.5177  loss_dice_2: 3.696  loss_ce_3: 0.3401  loss_mask_3: 0.5184  loss_dice_3: 3.678  loss_ce_4: 0.3287  loss_mask_4: 0.5144  loss_dice_4: 3.676  loss_ce_5: 0.3355  loss_mask_5: 0.5139  loss_dice_5: 3.679  loss_ce_6: 0.3278  loss_mask_6: 0.5159  loss_dice_6: 3.675  loss_ce_7: 0.3409  loss_mask_7: 0.5132  loss_dice_7: 3.687  loss_ce_8: 0.3378  loss_mask_8: 0.515  loss_dice_8: 3.683  time: 1.5332  data_time: 0.1064  lr: 9.3396e-06  max_mem: 21366M
[01/17 06:16:35] d2.utils.events INFO:  eta: 1 day, 11:16:53  iter: 6599  total_loss: 46.11  loss_ce: 0.3845  loss_mask: 0.5028  loss_dice: 3.701  loss_ce_0: 0.6358  loss_mask_0: 0.4741  loss_dice_0: 3.825  loss_ce_1: 0.394  loss_mask_1: 0.5023  loss_dice_1: 3.728  loss_ce_2: 0.3809  loss_mask_2: 0.5043  loss_dice_2: 3.716  loss_ce_3: 0.3863  loss_mask_3: 0.5052  loss_dice_3: 3.702  loss_ce_4: 0.3741  loss_mask_4: 0.5021  loss_dice_4: 3.7  loss_ce_5: 0.3799  loss_mask_5: 0.4999  loss_dice_5: 3.715  loss_ce_6: 0.3681  loss_mask_6: 0.5009  loss_dice_6: 3.711  loss_ce_7: 0.3588  loss_mask_7: 0.5044  loss_dice_7: 3.699  loss_ce_8: 0.38  loss_mask_8: 0.5029  loss_dice_8: 3.703  time: 1.5332  data_time: 0.1100  lr: 9.3376e-06  max_mem: 21366M
[01/17 06:17:05] d2.utils.events INFO:  eta: 1 day, 11:16:57  iter: 6619  total_loss: 46.61  loss_ce: 0.3559  loss_mask: 0.5173  loss_dice: 3.725  loss_ce_0: 0.6352  loss_mask_0: 0.4916  loss_dice_0: 3.821  loss_ce_1: 0.3815  loss_mask_1: 0.5124  loss_dice_1: 3.748  loss_ce_2: 0.3599  loss_mask_2: 0.5112  loss_dice_2: 3.734  loss_ce_3: 0.3658  loss_mask_3: 0.5098  loss_dice_3: 3.725  loss_ce_4: 0.3477  loss_mask_4: 0.5127  loss_dice_4: 3.725  loss_ce_5: 0.3612  loss_mask_5: 0.5129  loss_dice_5: 3.733  loss_ce_6: 0.3399  loss_mask_6: 0.5123  loss_dice_6: 3.735  loss_ce_7: 0.3706  loss_mask_7: 0.5101  loss_dice_7: 3.736  loss_ce_8: 0.3501  loss_mask_8: 0.5145  loss_dice_8: 3.723  time: 1.5332  data_time: 0.1092  lr: 9.3356e-06  max_mem: 21366M
[01/17 06:17:36] d2.utils.events INFO:  eta: 1 day, 11:16:55  iter: 6639  total_loss: 46.31  loss_ce: 0.368  loss_mask: 0.4876  loss_dice: 3.736  loss_ce_0: 0.6233  loss_mask_0: 0.4553  loss_dice_0: 3.853  loss_ce_1: 0.3941  loss_mask_1: 0.4817  loss_dice_1: 3.762  loss_ce_2: 0.3807  loss_mask_2: 0.4832  loss_dice_2: 3.742  loss_ce_3: 0.3669  loss_mask_3: 0.4871  loss_dice_3: 3.731  loss_ce_4: 0.3742  loss_mask_4: 0.4867  loss_dice_4: 3.736  loss_ce_5: 0.3665  loss_mask_5: 0.4873  loss_dice_5: 3.725  loss_ce_6: 0.3691  loss_mask_6: 0.4901  loss_dice_6: 3.73  loss_ce_7: 0.3727  loss_mask_7: 0.4899  loss_dice_7: 3.726  loss_ce_8: 0.3745  loss_mask_8: 0.4878  loss_dice_8: 3.722  time: 1.5332  data_time: 0.0860  lr: 9.3336e-06  max_mem: 21366M
[01/17 06:18:07] d2.utils.events INFO:  eta: 1 day, 11:17:46  iter: 6659  total_loss: 46.64  loss_ce: 0.343  loss_mask: 0.514  loss_dice: 3.707  loss_ce_0: 0.6489  loss_mask_0: 0.4885  loss_dice_0: 3.82  loss_ce_1: 0.3894  loss_mask_1: 0.513  loss_dice_1: 3.741  loss_ce_2: 0.4008  loss_mask_2: 0.5087  loss_dice_2: 3.722  loss_ce_3: 0.3832  loss_mask_3: 0.5093  loss_dice_3: 3.712  loss_ce_4: 0.37  loss_mask_4: 0.5107  loss_dice_4: 3.707  loss_ce_5: 0.3614  loss_mask_5: 0.5098  loss_dice_5: 3.713  loss_ce_6: 0.3721  loss_mask_6: 0.5108  loss_dice_6: 3.703  loss_ce_7: 0.3732  loss_mask_7: 0.5108  loss_dice_7: 3.709  loss_ce_8: 0.3604  loss_mask_8: 0.5093  loss_dice_8: 3.71  time: 1.5332  data_time: 0.0945  lr: 9.3316e-06  max_mem: 21366M
[01/17 06:18:37] d2.utils.events INFO:  eta: 1 day, 11:15:54  iter: 6679  total_loss: 46.02  loss_ce: 0.3803  loss_mask: 0.4945  loss_dice: 3.678  loss_ce_0: 0.6237  loss_mask_0: 0.4758  loss_dice_0: 3.794  loss_ce_1: 0.3835  loss_mask_1: 0.4946  loss_dice_1: 3.703  loss_ce_2: 0.3887  loss_mask_2: 0.4963  loss_dice_2: 3.68  loss_ce_3: 0.3929  loss_mask_3: 0.497  loss_dice_3: 3.683  loss_ce_4: 0.3772  loss_mask_4: 0.4954  loss_dice_4: 3.685  loss_ce_5: 0.4011  loss_mask_5: 0.4975  loss_dice_5: 3.675  loss_ce_6: 0.3731  loss_mask_6: 0.496  loss_dice_6: 3.682  loss_ce_7: 0.3775  loss_mask_7: 0.4958  loss_dice_7: 3.677  loss_ce_8: 0.375  loss_mask_8: 0.4969  loss_dice_8: 3.674  time: 1.5331  data_time: 0.0951  lr: 9.3296e-06  max_mem: 21366M
[01/17 06:19:08] d2.utils.events INFO:  eta: 1 day, 11:15:10  iter: 6699  total_loss: 46.29  loss_ce: 0.3898  loss_mask: 0.5044  loss_dice: 3.701  loss_ce_0: 0.6632  loss_mask_0: 0.4848  loss_dice_0: 3.792  loss_ce_1: 0.3946  loss_mask_1: 0.5115  loss_dice_1: 3.729  loss_ce_2: 0.4021  loss_mask_2: 0.5107  loss_dice_2: 3.693  loss_ce_3: 0.4066  loss_mask_3: 0.5066  loss_dice_3: 3.681  loss_ce_4: 0.4142  loss_mask_4: 0.5067  loss_dice_4: 3.69  loss_ce_5: 0.3911  loss_mask_5: 0.5077  loss_dice_5: 3.696  loss_ce_6: 0.3924  loss_mask_6: 0.5035  loss_dice_6: 3.691  loss_ce_7: 0.406  loss_mask_7: 0.5057  loss_dice_7: 3.692  loss_ce_8: 0.3987  loss_mask_8: 0.5044  loss_dice_8: 3.695  time: 1.5331  data_time: 0.0883  lr: 9.3275e-06  max_mem: 21366M
[01/17 06:19:39] d2.utils.events INFO:  eta: 1 day, 11:14:58  iter: 6719  total_loss: 45.99  loss_ce: 0.3583  loss_mask: 0.521  loss_dice: 3.679  loss_ce_0: 0.6305  loss_mask_0: 0.4931  loss_dice_0: 3.804  loss_ce_1: 0.367  loss_mask_1: 0.5153  loss_dice_1: 3.72  loss_ce_2: 0.3607  loss_mask_2: 0.5239  loss_dice_2: 3.69  loss_ce_3: 0.3611  loss_mask_3: 0.5212  loss_dice_3: 3.68  loss_ce_4: 0.3495  loss_mask_4: 0.5202  loss_dice_4: 3.681  loss_ce_5: 0.3537  loss_mask_5: 0.5203  loss_dice_5: 3.686  loss_ce_6: 0.3517  loss_mask_6: 0.5178  loss_dice_6: 3.68  loss_ce_7: 0.3531  loss_mask_7: 0.5188  loss_dice_7: 3.685  loss_ce_8: 0.3589  loss_mask_8: 0.5204  loss_dice_8: 3.674  time: 1.5332  data_time: 0.1028  lr: 9.3255e-06  max_mem: 21366M
[01/17 06:20:09] d2.utils.events INFO:  eta: 1 day, 11:14:33  iter: 6739  total_loss: 46.15  loss_ce: 0.3673  loss_mask: 0.5088  loss_dice: 3.656  loss_ce_0: 0.6312  loss_mask_0: 0.4849  loss_dice_0: 3.77  loss_ce_1: 0.369  loss_mask_1: 0.5022  loss_dice_1: 3.694  loss_ce_2: 0.3811  loss_mask_2: 0.5081  loss_dice_2: 3.665  loss_ce_3: 0.3635  loss_mask_3: 0.5094  loss_dice_3: 3.655  loss_ce_4: 0.3699  loss_mask_4: 0.5103  loss_dice_4: 3.65  loss_ce_5: 0.3477  loss_mask_5: 0.5089  loss_dice_5: 3.658  loss_ce_6: 0.3662  loss_mask_6: 0.5104  loss_dice_6: 3.648  loss_ce_7: 0.3547  loss_mask_7: 0.5107  loss_dice_7: 3.652  loss_ce_8: 0.3568  loss_mask_8: 0.5083  loss_dice_8: 3.652  time: 1.5331  data_time: 0.1050  lr: 9.3235e-06  max_mem: 21366M
[01/17 06:20:39] d2.utils.events INFO:  eta: 1 day, 11:13:58  iter: 6759  total_loss: 45.33  loss_ce: 0.3443  loss_mask: 0.5168  loss_dice: 3.633  loss_ce_0: 0.6096  loss_mask_0: 0.4975  loss_dice_0: 3.749  loss_ce_1: 0.3767  loss_mask_1: 0.5185  loss_dice_1: 3.657  loss_ce_2: 0.3562  loss_mask_2: 0.5206  loss_dice_2: 3.644  loss_ce_3: 0.346  loss_mask_3: 0.5184  loss_dice_3: 3.637  loss_ce_4: 0.3411  loss_mask_4: 0.5159  loss_dice_4: 3.637  loss_ce_5: 0.3487  loss_mask_5: 0.5154  loss_dice_5: 3.636  loss_ce_6: 0.3459  loss_mask_6: 0.5148  loss_dice_6: 3.634  loss_ce_7: 0.338  loss_mask_7: 0.5155  loss_dice_7: 3.638  loss_ce_8: 0.3513  loss_mask_8: 0.517  loss_dice_8: 3.635  time: 1.5330  data_time: 0.0955  lr: 9.3215e-06  max_mem: 21366M
[01/17 06:21:10] d2.utils.events INFO:  eta: 1 day, 11:13:32  iter: 6779  total_loss: 45.92  loss_ce: 0.3878  loss_mask: 0.5148  loss_dice: 3.639  loss_ce_0: 0.6425  loss_mask_0: 0.4918  loss_dice_0: 3.761  loss_ce_1: 0.401  loss_mask_1: 0.5078  loss_dice_1: 3.671  loss_ce_2: 0.3809  loss_mask_2: 0.5066  loss_dice_2: 3.66  loss_ce_3: 0.3769  loss_mask_3: 0.5131  loss_dice_3: 3.642  loss_ce_4: 0.3776  loss_mask_4: 0.5134  loss_dice_4: 3.644  loss_ce_5: 0.3911  loss_mask_5: 0.5164  loss_dice_5: 3.652  loss_ce_6: 0.394  loss_mask_6: 0.516  loss_dice_6: 3.644  loss_ce_7: 0.3967  loss_mask_7: 0.513  loss_dice_7: 3.649  loss_ce_8: 0.3982  loss_mask_8: 0.5107  loss_dice_8: 3.643  time: 1.5331  data_time: 0.1058  lr: 9.3195e-06  max_mem: 21366M
[01/17 06:21:41] d2.utils.events INFO:  eta: 1 day, 11:13:02  iter: 6799  total_loss: 45.81  loss_ce: 0.3638  loss_mask: 0.5095  loss_dice: 3.646  loss_ce_0: 0.6431  loss_mask_0: 0.4894  loss_dice_0: 3.77  loss_ce_1: 0.3854  loss_mask_1: 0.5049  loss_dice_1: 3.677  loss_ce_2: 0.3883  loss_mask_2: 0.5097  loss_dice_2: 3.651  loss_ce_3: 0.3651  loss_mask_3: 0.5138  loss_dice_3: 3.641  loss_ce_4: 0.3689  loss_mask_4: 0.5141  loss_dice_4: 3.642  loss_ce_5: 0.374  loss_mask_5: 0.5109  loss_dice_5: 3.643  loss_ce_6: 0.3651  loss_mask_6: 0.5123  loss_dice_6: 3.641  loss_ce_7: 0.3692  loss_mask_7: 0.5114  loss_dice_7: 3.641  loss_ce_8: 0.3599  loss_mask_8: 0.5089  loss_dice_8: 3.641  time: 1.5331  data_time: 0.1044  lr: 9.3175e-06  max_mem: 21366M
[01/17 06:22:12] d2.utils.events INFO:  eta: 1 day, 11:13:19  iter: 6819  total_loss: 46.43  loss_ce: 0.3815  loss_mask: 0.5098  loss_dice: 3.714  loss_ce_0: 0.6336  loss_mask_0: 0.4916  loss_dice_0: 3.823  loss_ce_1: 0.3788  loss_mask_1: 0.5088  loss_dice_1: 3.744  loss_ce_2: 0.3822  loss_mask_2: 0.5093  loss_dice_2: 3.726  loss_ce_3: 0.3686  loss_mask_3: 0.5085  loss_dice_3: 3.715  loss_ce_4: 0.3615  loss_mask_4: 0.5046  loss_dice_4: 3.725  loss_ce_5: 0.3584  loss_mask_5: 0.5069  loss_dice_5: 3.72  loss_ce_6: 0.3701  loss_mask_6: 0.5075  loss_dice_6: 3.723  loss_ce_7: 0.3847  loss_mask_7: 0.5104  loss_dice_7: 3.713  loss_ce_8: 0.3856  loss_mask_8: 0.5072  loss_dice_8: 3.711  time: 1.5332  data_time: 0.0903  lr: 9.3154e-06  max_mem: 21366M
[01/17 06:22:43] d2.utils.events INFO:  eta: 1 day, 11:12:24  iter: 6839  total_loss: 46.39  loss_ce: 0.3613  loss_mask: 0.4831  loss_dice: 3.687  loss_ce_0: 0.6413  loss_mask_0: 0.46  loss_dice_0: 3.799  loss_ce_1: 0.4118  loss_mask_1: 0.4766  loss_dice_1: 3.7  loss_ce_2: 0.3881  loss_mask_2: 0.4803  loss_dice_2: 3.688  loss_ce_3: 0.3799  loss_mask_3: 0.4811  loss_dice_3: 3.68  loss_ce_4: 0.3754  loss_mask_4: 0.4816  loss_dice_4: 3.679  loss_ce_5: 0.3685  loss_mask_5: 0.4814  loss_dice_5: 3.686  loss_ce_6: 0.3755  loss_mask_6: 0.485  loss_dice_6: 3.682  loss_ce_7: 0.3652  loss_mask_7: 0.4845  loss_dice_7: 3.681  loss_ce_8: 0.3674  loss_mask_8: 0.4853  loss_dice_8: 3.685  time: 1.5332  data_time: 0.1108  lr: 9.3134e-06  max_mem: 21366M
[01/17 06:23:15] d2.utils.events INFO:  eta: 1 day, 11:12:41  iter: 6859  total_loss: 46.01  loss_ce: 0.3711  loss_mask: 0.4944  loss_dice: 3.666  loss_ce_0: 0.6301  loss_mask_0: 0.4694  loss_dice_0: 3.768  loss_ce_1: 0.3853  loss_mask_1: 0.4985  loss_dice_1: 3.684  loss_ce_2: 0.3711  loss_mask_2: 0.4962  loss_dice_2: 3.665  loss_ce_3: 0.386  loss_mask_3: 0.494  loss_dice_3: 3.662  loss_ce_4: 0.3857  loss_mask_4: 0.4948  loss_dice_4: 3.668  loss_ce_5: 0.3841  loss_mask_5: 0.4984  loss_dice_5: 3.668  loss_ce_6: 0.3645  loss_mask_6: 0.4984  loss_dice_6: 3.669  loss_ce_7: 0.3676  loss_mask_7: 0.4959  loss_dice_7: 3.674  loss_ce_8: 0.3649  loss_mask_8: 0.4944  loss_dice_8: 3.665  time: 1.5334  data_time: 0.1018  lr: 9.3114e-06  max_mem: 21366M
[01/17 06:23:46] d2.utils.events INFO:  eta: 1 day, 11:12:46  iter: 6879  total_loss: 45.97  loss_ce: 0.3644  loss_mask: 0.5065  loss_dice: 3.66  loss_ce_0: 0.6097  loss_mask_0: 0.488  loss_dice_0: 3.763  loss_ce_1: 0.3763  loss_mask_1: 0.504  loss_dice_1: 3.692  loss_ce_2: 0.3678  loss_mask_2: 0.5029  loss_dice_2: 3.68  loss_ce_3: 0.3695  loss_mask_3: 0.5002  loss_dice_3: 3.671  loss_ce_4: 0.3762  loss_mask_4: 0.504  loss_dice_4: 3.661  loss_ce_5: 0.3589  loss_mask_5: 0.5038  loss_dice_5: 3.661  loss_ce_6: 0.363  loss_mask_6: 0.5011  loss_dice_6: 3.665  loss_ce_7: 0.3941  loss_mask_7: 0.5012  loss_dice_7: 3.664  loss_ce_8: 0.3732  loss_mask_8: 0.5038  loss_dice_8: 3.664  time: 1.5334  data_time: 0.0952  lr: 9.3094e-06  max_mem: 21366M
[01/17 06:24:17] d2.utils.events INFO:  eta: 1 day, 11:12:28  iter: 6899  total_loss: 46.36  loss_ce: 0.3782  loss_mask: 0.5073  loss_dice: 3.707  loss_ce_0: 0.6596  loss_mask_0: 0.4872  loss_dice_0: 3.819  loss_ce_1: 0.3897  loss_mask_1: 0.5115  loss_dice_1: 3.742  loss_ce_2: 0.3997  loss_mask_2: 0.5148  loss_dice_2: 3.717  loss_ce_3: 0.3886  loss_mask_3: 0.5104  loss_dice_3: 3.718  loss_ce_4: 0.3791  loss_mask_4: 0.5086  loss_dice_4: 3.712  loss_ce_5: 0.3971  loss_mask_5: 0.5117  loss_dice_5: 3.705  loss_ce_6: 0.4015  loss_mask_6: 0.5081  loss_dice_6: 3.704  loss_ce_7: 0.3756  loss_mask_7: 0.5068  loss_dice_7: 3.711  loss_ce_8: 0.3866  loss_mask_8: 0.507  loss_dice_8: 3.706  time: 1.5334  data_time: 0.0980  lr: 9.3074e-06  max_mem: 21366M
[01/17 06:24:47] d2.utils.events INFO:  eta: 1 day, 11:11:10  iter: 6919  total_loss: 46.28  loss_ce: 0.366  loss_mask: 0.5007  loss_dice: 3.739  loss_ce_0: 0.6614  loss_mask_0: 0.4778  loss_dice_0: 3.841  loss_ce_1: 0.399  loss_mask_1: 0.4988  loss_dice_1: 3.757  loss_ce_2: 0.3873  loss_mask_2: 0.5022  loss_dice_2: 3.739  loss_ce_3: 0.3656  loss_mask_3: 0.5009  loss_dice_3: 3.726  loss_ce_4: 0.3706  loss_mask_4: 0.5026  loss_dice_4: 3.729  loss_ce_5: 0.3617  loss_mask_5: 0.5017  loss_dice_5: 3.729  loss_ce_6: 0.3692  loss_mask_6: 0.5004  loss_dice_6: 3.726  loss_ce_7: 0.3559  loss_mask_7: 0.5015  loss_dice_7: 3.729  loss_ce_8: 0.3595  loss_mask_8: 0.5017  loss_dice_8: 3.729  time: 1.5334  data_time: 0.0940  lr: 9.3054e-06  max_mem: 21366M
[01/17 06:25:18] d2.utils.events INFO:  eta: 1 day, 11:09:28  iter: 6939  total_loss: 46.56  loss_ce: 0.4003  loss_mask: 0.5017  loss_dice: 3.688  loss_ce_0: 0.6228  loss_mask_0: 0.4836  loss_dice_0: 3.812  loss_ce_1: 0.4106  loss_mask_1: 0.5019  loss_dice_1: 3.72  loss_ce_2: 0.3901  loss_mask_2: 0.5036  loss_dice_2: 3.698  loss_ce_3: 0.397  loss_mask_3: 0.5  loss_dice_3: 3.69  loss_ce_4: 0.3901  loss_mask_4: 0.5031  loss_dice_4: 3.689  loss_ce_5: 0.3996  loss_mask_5: 0.5033  loss_dice_5: 3.694  loss_ce_6: 0.3969  loss_mask_6: 0.5065  loss_dice_6: 3.69  loss_ce_7: 0.3879  loss_mask_7: 0.5042  loss_dice_7: 3.689  loss_ce_8: 0.3951  loss_mask_8: 0.5051  loss_dice_8: 3.687  time: 1.5334  data_time: 0.0956  lr: 9.3033e-06  max_mem: 21366M
[01/17 06:25:49] d2.utils.events INFO:  eta: 1 day, 11:09:21  iter: 6959  total_loss: 45.89  loss_ce: 0.3674  loss_mask: 0.5058  loss_dice: 3.666  loss_ce_0: 0.6546  loss_mask_0: 0.4862  loss_dice_0: 3.777  loss_ce_1: 0.3885  loss_mask_1: 0.5038  loss_dice_1: 3.702  loss_ce_2: 0.3739  loss_mask_2: 0.5012  loss_dice_2: 3.678  loss_ce_3: 0.3672  loss_mask_3: 0.5005  loss_dice_3: 3.671  loss_ce_4: 0.3743  loss_mask_4: 0.5012  loss_dice_4: 3.666  loss_ce_5: 0.3867  loss_mask_5: 0.5019  loss_dice_5: 3.67  loss_ce_6: 0.3816  loss_mask_6: 0.5042  loss_dice_6: 3.67  loss_ce_7: 0.3686  loss_mask_7: 0.5075  loss_dice_7: 3.669  loss_ce_8: 0.3601  loss_mask_8: 0.5043  loss_dice_8: 3.672  time: 1.5333  data_time: 0.1015  lr: 9.3013e-06  max_mem: 21366M
[01/17 06:26:20] d2.utils.events INFO:  eta: 1 day, 11:10:26  iter: 6979  total_loss: 46.93  loss_ce: 0.4273  loss_mask: 0.5039  loss_dice: 3.723  loss_ce_0: 0.6877  loss_mask_0: 0.4954  loss_dice_0: 3.822  loss_ce_1: 0.4661  loss_mask_1: 0.5049  loss_dice_1: 3.75  loss_ce_2: 0.4133  loss_mask_2: 0.5057  loss_dice_2: 3.73  loss_ce_3: 0.4304  loss_mask_3: 0.5043  loss_dice_3: 3.718  loss_ce_4: 0.4294  loss_mask_4: 0.5058  loss_dice_4: 3.718  loss_ce_5: 0.4328  loss_mask_5: 0.5041  loss_dice_5: 3.73  loss_ce_6: 0.4121  loss_mask_6: 0.5037  loss_dice_6: 3.722  loss_ce_7: 0.4333  loss_mask_7: 0.5021  loss_dice_7: 3.717  loss_ce_8: 0.4259  loss_mask_8: 0.503  loss_dice_8: 3.721  time: 1.5334  data_time: 0.0948  lr: 9.2993e-06  max_mem: 21366M
[01/17 06:26:50] d2.utils.events INFO:  eta: 1 day, 11:09:56  iter: 6999  total_loss: 45.75  loss_ce: 0.3657  loss_mask: 0.4926  loss_dice: 3.677  loss_ce_0: 0.6318  loss_mask_0: 0.4724  loss_dice_0: 3.791  loss_ce_1: 0.3727  loss_mask_1: 0.4928  loss_dice_1: 3.709  loss_ce_2: 0.3683  loss_mask_2: 0.4948  loss_dice_2: 3.681  loss_ce_3: 0.3595  loss_mask_3: 0.4904  loss_dice_3: 3.67  loss_ce_4: 0.3633  loss_mask_4: 0.4933  loss_dice_4: 3.68  loss_ce_5: 0.3646  loss_mask_5: 0.4916  loss_dice_5: 3.674  loss_ce_6: 0.353  loss_mask_6: 0.4944  loss_dice_6: 3.669  loss_ce_7: 0.3637  loss_mask_7: 0.4907  loss_dice_7: 3.669  loss_ce_8: 0.3418  loss_mask_8: 0.4914  loss_dice_8: 3.673  time: 1.5334  data_time: 0.1070  lr: 9.2973e-06  max_mem: 21366M
[01/17 06:27:21] d2.utils.events INFO:  eta: 1 day, 11:09:42  iter: 7019  total_loss: 44.99  loss_ce: 0.3554  loss_mask: 0.505  loss_dice: 3.604  loss_ce_0: 0.6559  loss_mask_0: 0.4774  loss_dice_0: 3.74  loss_ce_1: 0.3899  loss_mask_1: 0.4991  loss_dice_1: 3.646  loss_ce_2: 0.3801  loss_mask_2: 0.5009  loss_dice_2: 3.618  loss_ce_3: 0.367  loss_mask_3: 0.5021  loss_dice_3: 3.609  loss_ce_4: 0.3535  loss_mask_4: 0.4996  loss_dice_4: 3.612  loss_ce_5: 0.3722  loss_mask_5: 0.5025  loss_dice_5: 3.611  loss_ce_6: 0.3593  loss_mask_6: 0.5024  loss_dice_6: 3.606  loss_ce_7: 0.3582  loss_mask_7: 0.5013  loss_dice_7: 3.598  loss_ce_8: 0.3753  loss_mask_8: 0.503  loss_dice_8: 3.599  time: 1.5334  data_time: 0.0996  lr: 9.2953e-06  max_mem: 21366M
[01/17 06:27:51] d2.utils.events INFO:  eta: 1 day, 11:09:01  iter: 7039  total_loss: 45.22  loss_ce: 0.3762  loss_mask: 0.4952  loss_dice: 3.599  loss_ce_0: 0.64  loss_mask_0: 0.4625  loss_dice_0: 3.746  loss_ce_1: 0.4084  loss_mask_1: 0.4883  loss_dice_1: 3.646  loss_ce_2: 0.3957  loss_mask_2: 0.4898  loss_dice_2: 3.623  loss_ce_3: 0.375  loss_mask_3: 0.4935  loss_dice_3: 3.617  loss_ce_4: 0.3738  loss_mask_4: 0.4951  loss_dice_4: 3.609  loss_ce_5: 0.373  loss_mask_5: 0.4966  loss_dice_5: 3.621  loss_ce_6: 0.3719  loss_mask_6: 0.4978  loss_dice_6: 3.607  loss_ce_7: 0.3691  loss_mask_7: 0.4966  loss_dice_7: 3.605  loss_ce_8: 0.3772  loss_mask_8: 0.4961  loss_dice_8: 3.606  time: 1.5333  data_time: 0.0929  lr: 9.2933e-06  max_mem: 21366M
[01/17 06:28:22] d2.utils.events INFO:  eta: 1 day, 11:09:36  iter: 7059  total_loss: 45.46  loss_ce: 0.3675  loss_mask: 0.4989  loss_dice: 3.644  loss_ce_0: 0.6377  loss_mask_0: 0.4769  loss_dice_0: 3.763  loss_ce_1: 0.3587  loss_mask_1: 0.4997  loss_dice_1: 3.677  loss_ce_2: 0.3639  loss_mask_2: 0.5004  loss_dice_2: 3.656  loss_ce_3: 0.3675  loss_mask_3: 0.4978  loss_dice_3: 3.649  loss_ce_4: 0.3677  loss_mask_4: 0.4996  loss_dice_4: 3.648  loss_ce_5: 0.3699  loss_mask_5: 0.4988  loss_dice_5: 3.649  loss_ce_6: 0.3696  loss_mask_6: 0.497  loss_dice_6: 3.643  loss_ce_7: 0.3768  loss_mask_7: 0.4986  loss_dice_7: 3.647  loss_ce_8: 0.3588  loss_mask_8: 0.4996  loss_dice_8: 3.65  time: 1.5333  data_time: 0.0930  lr: 9.2912e-06  max_mem: 21366M
[01/17 06:28:53] d2.utils.events INFO:  eta: 1 day, 11:09:26  iter: 7079  total_loss: 45.13  loss_ce: 0.3476  loss_mask: 0.498  loss_dice: 3.597  loss_ce_0: 0.6562  loss_mask_0: 0.4724  loss_dice_0: 3.756  loss_ce_1: 0.3883  loss_mask_1: 0.4964  loss_dice_1: 3.651  loss_ce_2: 0.3713  loss_mask_2: 0.4995  loss_dice_2: 3.613  loss_ce_3: 0.3578  loss_mask_3: 0.4986  loss_dice_3: 3.612  loss_ce_4: 0.3668  loss_mask_4: 0.4983  loss_dice_4: 3.605  loss_ce_5: 0.359  loss_mask_5: 0.5008  loss_dice_5: 3.609  loss_ce_6: 0.3603  loss_mask_6: 0.5019  loss_dice_6: 3.607  loss_ce_7: 0.362  loss_mask_7: 0.5004  loss_dice_7: 3.598  loss_ce_8: 0.3607  loss_mask_8: 0.4977  loss_dice_8: 3.602  time: 1.5333  data_time: 0.1020  lr: 9.2892e-06  max_mem: 21366M
[01/17 06:29:23] d2.utils.events INFO:  eta: 1 day, 11:08:42  iter: 7099  total_loss: 46.05  loss_ce: 0.3394  loss_mask: 0.5092  loss_dice: 3.66  loss_ce_0: 0.6453  loss_mask_0: 0.484  loss_dice_0: 3.768  loss_ce_1: 0.3833  loss_mask_1: 0.5023  loss_dice_1: 3.698  loss_ce_2: 0.3713  loss_mask_2: 0.5022  loss_dice_2: 3.677  loss_ce_3: 0.3595  loss_mask_3: 0.5061  loss_dice_3: 3.674  loss_ce_4: 0.361  loss_mask_4: 0.5072  loss_dice_4: 3.67  loss_ce_5: 0.3478  loss_mask_5: 0.5086  loss_dice_5: 3.664  loss_ce_6: 0.3511  loss_mask_6: 0.5064  loss_dice_6: 3.661  loss_ce_7: 0.3545  loss_mask_7: 0.5076  loss_dice_7: 3.661  loss_ce_8: 0.3631  loss_mask_8: 0.5085  loss_dice_8: 3.66  time: 1.5333  data_time: 0.0944  lr: 9.2872e-06  max_mem: 21366M
[01/17 06:29:54] d2.utils.events INFO:  eta: 1 day, 11:07:19  iter: 7119  total_loss: 45.23  loss_ce: 0.3609  loss_mask: 0.4707  loss_dice: 3.626  loss_ce_0: 0.636  loss_mask_0: 0.4565  loss_dice_0: 3.734  loss_ce_1: 0.3961  loss_mask_1: 0.4665  loss_dice_1: 3.649  loss_ce_2: 0.3776  loss_mask_2: 0.4693  loss_dice_2: 3.636  loss_ce_3: 0.3839  loss_mask_3: 0.4701  loss_dice_3: 3.614  loss_ce_4: 0.3806  loss_mask_4: 0.4694  loss_dice_4: 3.62  loss_ce_5: 0.3538  loss_mask_5: 0.4694  loss_dice_5: 3.624  loss_ce_6: 0.3577  loss_mask_6: 0.4705  loss_dice_6: 3.618  loss_ce_7: 0.3468  loss_mask_7: 0.4686  loss_dice_7: 3.618  loss_ce_8: 0.3606  loss_mask_8: 0.4701  loss_dice_8: 3.623  time: 1.5332  data_time: 0.0971  lr: 9.2852e-06  max_mem: 21366M
[01/17 06:30:25] d2.utils.events INFO:  eta: 1 day, 11:07:09  iter: 7139  total_loss: 45.46  loss_ce: 0.3458  loss_mask: 0.4838  loss_dice: 3.656  loss_ce_0: 0.6404  loss_mask_0: 0.4573  loss_dice_0: 3.785  loss_ce_1: 0.3816  loss_mask_1: 0.4793  loss_dice_1: 3.693  loss_ce_2: 0.3665  loss_mask_2: 0.4867  loss_dice_2: 3.669  loss_ce_3: 0.3704  loss_mask_3: 0.485  loss_dice_3: 3.656  loss_ce_4: 0.3657  loss_mask_4: 0.4863  loss_dice_4: 3.663  loss_ce_5: 0.3721  loss_mask_5: 0.485  loss_dice_5: 3.661  loss_ce_6: 0.3473  loss_mask_6: 0.4869  loss_dice_6: 3.662  loss_ce_7: 0.3573  loss_mask_7: 0.4857  loss_dice_7: 3.654  loss_ce_8: 0.3504  loss_mask_8: 0.4848  loss_dice_8: 3.651  time: 1.5333  data_time: 0.0979  lr: 9.2832e-06  max_mem: 21366M
[01/17 06:30:55] d2.utils.events INFO:  eta: 1 day, 11:07:24  iter: 7159  total_loss: 45.29  loss_ce: 0.342  loss_mask: 0.4981  loss_dice: 3.624  loss_ce_0: 0.6235  loss_mask_0: 0.481  loss_dice_0: 3.733  loss_ce_1: 0.3839  loss_mask_1: 0.4987  loss_dice_1: 3.66  loss_ce_2: 0.3666  loss_mask_2: 0.5001  loss_dice_2: 3.635  loss_ce_3: 0.3451  loss_mask_3: 0.5011  loss_dice_3: 3.63  loss_ce_4: 0.3525  loss_mask_4: 0.4982  loss_dice_4: 3.621  loss_ce_5: 0.3453  loss_mask_5: 0.4975  loss_dice_5: 3.626  loss_ce_6: 0.3382  loss_mask_6: 0.4987  loss_dice_6: 3.623  loss_ce_7: 0.3407  loss_mask_7: 0.4951  loss_dice_7: 3.623  loss_ce_8: 0.3372  loss_mask_8: 0.4975  loss_dice_8: 3.623  time: 1.5333  data_time: 0.0889  lr: 9.2812e-06  max_mem: 21366M
[01/17 06:31:26] d2.utils.events INFO:  eta: 1 day, 11:09:11  iter: 7179  total_loss: 44.97  loss_ce: 0.3514  loss_mask: 0.4967  loss_dice: 3.579  loss_ce_0: 0.6422  loss_mask_0: 0.4615  loss_dice_0: 3.736  loss_ce_1: 0.3765  loss_mask_1: 0.4894  loss_dice_1: 3.629  loss_ce_2: 0.3781  loss_mask_2: 0.4894  loss_dice_2: 3.603  loss_ce_3: 0.3579  loss_mask_3: 0.4903  loss_dice_3: 3.588  loss_ce_4: 0.3471  loss_mask_4: 0.494  loss_dice_4: 3.589  loss_ce_5: 0.3452  loss_mask_5: 0.4944  loss_dice_5: 3.596  loss_ce_6: 0.3659  loss_mask_6: 0.4976  loss_dice_6: 3.584  loss_ce_7: 0.3505  loss_mask_7: 0.5002  loss_dice_7: 3.581  loss_ce_8: 0.3457  loss_mask_8: 0.499  loss_dice_8: 3.582  time: 1.5333  data_time: 0.1012  lr: 9.2791e-06  max_mem: 21366M
[01/17 06:31:57] d2.utils.events INFO:  eta: 1 day, 11:08:17  iter: 7199  total_loss: 44.69  loss_ce: 0.3265  loss_mask: 0.4878  loss_dice: 3.603  loss_ce_0: 0.6016  loss_mask_0: 0.4567  loss_dice_0: 3.74  loss_ce_1: 0.3638  loss_mask_1: 0.4814  loss_dice_1: 3.645  loss_ce_2: 0.3482  loss_mask_2: 0.4842  loss_dice_2: 3.627  loss_ce_3: 0.3386  loss_mask_3: 0.4844  loss_dice_3: 3.617  loss_ce_4: 0.3463  loss_mask_4: 0.4851  loss_dice_4: 3.615  loss_ce_5: 0.3307  loss_mask_5: 0.4841  loss_dice_5: 3.612  loss_ce_6: 0.3317  loss_mask_6: 0.4879  loss_dice_6: 3.606  loss_ce_7: 0.338  loss_mask_7: 0.4885  loss_dice_7: 3.605  loss_ce_8: 0.3354  loss_mask_8: 0.4912  loss_dice_8: 3.605  time: 1.5333  data_time: 0.0900  lr: 9.2771e-06  max_mem: 21366M
[01/17 06:32:28] d2.utils.events INFO:  eta: 1 day, 11:08:18  iter: 7219  total_loss: 44.58  loss_ce: 0.3494  loss_mask: 0.4836  loss_dice: 3.581  loss_ce_0: 0.6098  loss_mask_0: 0.4594  loss_dice_0: 3.714  loss_ce_1: 0.3565  loss_mask_1: 0.4789  loss_dice_1: 3.627  loss_ce_2: 0.3457  loss_mask_2: 0.4798  loss_dice_2: 3.599  loss_ce_3: 0.3614  loss_mask_3: 0.4811  loss_dice_3: 3.589  loss_ce_4: 0.345  loss_mask_4: 0.4846  loss_dice_4: 3.581  loss_ce_5: 0.34  loss_mask_5: 0.4861  loss_dice_5: 3.583  loss_ce_6: 0.3411  loss_mask_6: 0.483  loss_dice_6: 3.585  loss_ce_7: 0.3406  loss_mask_7: 0.4849  loss_dice_7: 3.586  loss_ce_8: 0.3508  loss_mask_8: 0.4844  loss_dice_8: 3.585  time: 1.5333  data_time: 0.1048  lr: 9.2751e-06  max_mem: 21366M
[01/17 06:32:58] d2.utils.events INFO:  eta: 1 day, 11:07:35  iter: 7239  total_loss: 44.93  loss_ce: 0.3593  loss_mask: 0.52  loss_dice: 3.586  loss_ce_0: 0.6302  loss_mask_0: 0.4847  loss_dice_0: 3.715  loss_ce_1: 0.3774  loss_mask_1: 0.5117  loss_dice_1: 3.626  loss_ce_2: 0.3679  loss_mask_2: 0.519  loss_dice_2: 3.594  loss_ce_3: 0.3669  loss_mask_3: 0.5185  loss_dice_3: 3.59  loss_ce_4: 0.3616  loss_mask_4: 0.5184  loss_dice_4: 3.581  loss_ce_5: 0.3618  loss_mask_5: 0.5176  loss_dice_5: 3.584  loss_ce_6: 0.3684  loss_mask_6: 0.5193  loss_dice_6: 3.584  loss_ce_7: 0.3598  loss_mask_7: 0.5219  loss_dice_7: 3.58  loss_ce_8: 0.3566  loss_mask_8: 0.5209  loss_dice_8: 3.58  time: 1.5333  data_time: 0.0969  lr: 9.2731e-06  max_mem: 21366M
[01/17 06:33:29] d2.utils.events INFO:  eta: 1 day, 11:05:45  iter: 7259  total_loss: 44.91  loss_ce: 0.3673  loss_mask: 0.4963  loss_dice: 3.582  loss_ce_0: 0.6268  loss_mask_0: 0.4752  loss_dice_0: 3.701  loss_ce_1: 0.3962  loss_mask_1: 0.4963  loss_dice_1: 3.609  loss_ce_2: 0.3769  loss_mask_2: 0.4984  loss_dice_2: 3.587  loss_ce_3: 0.3756  loss_mask_3: 0.4969  loss_dice_3: 3.58  loss_ce_4: 0.369  loss_mask_4: 0.5004  loss_dice_4: 3.589  loss_ce_5: 0.3651  loss_mask_5: 0.5014  loss_dice_5: 3.58  loss_ce_6: 0.3571  loss_mask_6: 0.5014  loss_dice_6: 3.575  loss_ce_7: 0.355  loss_mask_7: 0.4991  loss_dice_7: 3.587  loss_ce_8: 0.3642  loss_mask_8: 0.4968  loss_dice_8: 3.577  time: 1.5332  data_time: 0.0922  lr: 9.2711e-06  max_mem: 21366M
[01/17 06:34:00] d2.utils.events INFO:  eta: 1 day, 11:05:36  iter: 7279  total_loss: 45.26  loss_ce: 0.3548  loss_mask: 0.4944  loss_dice: 3.611  loss_ce_0: 0.6325  loss_mask_0: 0.471  loss_dice_0: 3.729  loss_ce_1: 0.3634  loss_mask_1: 0.4925  loss_dice_1: 3.653  loss_ce_2: 0.3741  loss_mask_2: 0.4936  loss_dice_2: 3.624  loss_ce_3: 0.3508  loss_mask_3: 0.4943  loss_dice_3: 3.611  loss_ce_4: 0.3651  loss_mask_4: 0.4949  loss_dice_4: 3.61  loss_ce_5: 0.3617  loss_mask_5: 0.4927  loss_dice_5: 3.61  loss_ce_6: 0.3677  loss_mask_6: 0.4933  loss_dice_6: 3.612  loss_ce_7: 0.3565  loss_mask_7: 0.4957  loss_dice_7: 3.609  loss_ce_8: 0.3505  loss_mask_8: 0.4957  loss_dice_8: 3.613  time: 1.5333  data_time: 0.0952  lr: 9.2691e-06  max_mem: 21366M
[01/17 06:34:30] d2.utils.events INFO:  eta: 1 day, 11:06:02  iter: 7299  total_loss: 45.18  loss_ce: 0.3569  loss_mask: 0.5007  loss_dice: 3.621  loss_ce_0: 0.6308  loss_mask_0: 0.47  loss_dice_0: 3.756  loss_ce_1: 0.3788  loss_mask_1: 0.4997  loss_dice_1: 3.662  loss_ce_2: 0.3791  loss_mask_2: 0.4981  loss_dice_2: 3.644  loss_ce_3: 0.3549  loss_mask_3: 0.4992  loss_dice_3: 3.629  loss_ce_4: 0.3816  loss_mask_4: 0.4982  loss_dice_4: 3.62  loss_ce_5: 0.3428  loss_mask_5: 0.4995  loss_dice_5: 3.637  loss_ce_6: 0.3713  loss_mask_6: 0.4998  loss_dice_6: 3.628  loss_ce_7: 0.3574  loss_mask_7: 0.5013  loss_dice_7: 3.635  loss_ce_8: 0.3452  loss_mask_8: 0.5003  loss_dice_8: 3.637  time: 1.5333  data_time: 0.1049  lr: 9.267e-06  max_mem: 21366M
[01/17 06:35:01] d2.utils.events INFO:  eta: 1 day, 11:05:13  iter: 7319  total_loss: 45.29  loss_ce: 0.37  loss_mask: 0.4948  loss_dice: 3.61  loss_ce_0: 0.6273  loss_mask_0: 0.4756  loss_dice_0: 3.719  loss_ce_1: 0.3568  loss_mask_1: 0.4962  loss_dice_1: 3.639  loss_ce_2: 0.3768  loss_mask_2: 0.4995  loss_dice_2: 3.619  loss_ce_3: 0.3686  loss_mask_3: 0.4962  loss_dice_3: 3.621  loss_ce_4: 0.3638  loss_mask_4: 0.498  loss_dice_4: 3.615  loss_ce_5: 0.3502  loss_mask_5: 0.4966  loss_dice_5: 3.619  loss_ce_6: 0.3538  loss_mask_6: 0.4968  loss_dice_6: 3.608  loss_ce_7: 0.3511  loss_mask_7: 0.4931  loss_dice_7: 3.605  loss_ce_8: 0.3634  loss_mask_8: 0.494  loss_dice_8: 3.609  time: 1.5333  data_time: 0.1023  lr: 9.265e-06  max_mem: 21366M
[01/17 06:35:31] d2.utils.events INFO:  eta: 1 day, 11:03:48  iter: 7339  total_loss: 45.29  loss_ce: 0.3784  loss_mask: 0.4912  loss_dice: 3.606  loss_ce_0: 0.6453  loss_mask_0: 0.4706  loss_dice_0: 3.726  loss_ce_1: 0.3952  loss_mask_1: 0.4901  loss_dice_1: 3.645  loss_ce_2: 0.3862  loss_mask_2: 0.4916  loss_dice_2: 3.618  loss_ce_3: 0.394  loss_mask_3: 0.4902  loss_dice_3: 3.611  loss_ce_4: 0.3658  loss_mask_4: 0.4911  loss_dice_4: 3.615  loss_ce_5: 0.397  loss_mask_5: 0.4898  loss_dice_5: 3.616  loss_ce_6: 0.3613  loss_mask_6: 0.4929  loss_dice_6: 3.612  loss_ce_7: 0.3672  loss_mask_7: 0.4919  loss_dice_7: 3.614  loss_ce_8: 0.376  loss_mask_8: 0.4924  loss_dice_8: 3.608  time: 1.5332  data_time: 0.1025  lr: 9.263e-06  max_mem: 21366M
[01/17 06:36:02] d2.utils.events INFO:  eta: 1 day, 11:03:34  iter: 7359  total_loss: 45.38  loss_ce: 0.3511  loss_mask: 0.4994  loss_dice: 3.594  loss_ce_0: 0.597  loss_mask_0: 0.4792  loss_dice_0: 3.713  loss_ce_1: 0.3833  loss_mask_1: 0.5054  loss_dice_1: 3.622  loss_ce_2: 0.3688  loss_mask_2: 0.5039  loss_dice_2: 3.603  loss_ce_3: 0.3652  loss_mask_3: 0.5017  loss_dice_3: 3.59  loss_ce_4: 0.371  loss_mask_4: 0.4986  loss_dice_4: 3.598  loss_ce_5: 0.343  loss_mask_5: 0.4993  loss_dice_5: 3.583  loss_ce_6: 0.3365  loss_mask_6: 0.502  loss_dice_6: 3.576  loss_ce_7: 0.3399  loss_mask_7: 0.5012  loss_dice_7: 3.59  loss_ce_8: 0.336  loss_mask_8: 0.5012  loss_dice_8: 3.583  time: 1.5332  data_time: 0.0929  lr: 9.261e-06  max_mem: 21366M
[01/17 06:36:33] d2.utils.events INFO:  eta: 1 day, 11:04:02  iter: 7379  total_loss: 45.41  loss_ce: 0.3801  loss_mask: 0.4934  loss_dice: 3.622  loss_ce_0: 0.6361  loss_mask_0: 0.4717  loss_dice_0: 3.734  loss_ce_1: 0.3856  loss_mask_1: 0.4928  loss_dice_1: 3.649  loss_ce_2: 0.4032  loss_mask_2: 0.496  loss_dice_2: 3.63  loss_ce_3: 0.383  loss_mask_3: 0.4959  loss_dice_3: 3.619  loss_ce_4: 0.3685  loss_mask_4: 0.4949  loss_dice_4: 3.624  loss_ce_5: 0.3938  loss_mask_5: 0.4943  loss_dice_5: 3.628  loss_ce_6: 0.3964  loss_mask_6: 0.4927  loss_dice_6: 3.626  loss_ce_7: 0.3711  loss_mask_7: 0.4921  loss_dice_7: 3.631  loss_ce_8: 0.3806  loss_mask_8: 0.4914  loss_dice_8: 3.625  time: 1.5332  data_time: 0.1016  lr: 9.259e-06  max_mem: 21366M
[01/17 06:37:03] d2.utils.events INFO:  eta: 1 day, 11:03:51  iter: 7399  total_loss: 44.77  loss_ce: 0.3527  loss_mask: 0.4926  loss_dice: 3.57  loss_ce_0: 0.603  loss_mask_0: 0.4695  loss_dice_0: 3.704  loss_ce_1: 0.3682  loss_mask_1: 0.4943  loss_dice_1: 3.61  loss_ce_2: 0.3667  loss_mask_2: 0.4947  loss_dice_2: 3.588  loss_ce_3: 0.3673  loss_mask_3: 0.4945  loss_dice_3: 3.583  loss_ce_4: 0.3668  loss_mask_4: 0.4933  loss_dice_4: 3.582  loss_ce_5: 0.3709  loss_mask_5: 0.4892  loss_dice_5: 3.594  loss_ce_6: 0.3673  loss_mask_6: 0.4936  loss_dice_6: 3.587  loss_ce_7: 0.3578  loss_mask_7: 0.4935  loss_dice_7: 3.582  loss_ce_8: 0.3758  loss_mask_8: 0.492  loss_dice_8: 3.586  time: 1.5331  data_time: 0.0939  lr: 9.257e-06  max_mem: 21366M
[01/17 06:37:33] d2.utils.events INFO:  eta: 1 day, 11:02:02  iter: 7419  total_loss: 45.09  loss_ce: 0.3787  loss_mask: 0.4987  loss_dice: 3.619  loss_ce_0: 0.6385  loss_mask_0: 0.4781  loss_dice_0: 3.731  loss_ce_1: 0.3987  loss_mask_1: 0.5046  loss_dice_1: 3.646  loss_ce_2: 0.3724  loss_mask_2: 0.4984  loss_dice_2: 3.63  loss_ce_3: 0.367  loss_mask_3: 0.4993  loss_dice_3: 3.612  loss_ce_4: 0.3589  loss_mask_4: 0.5007  loss_dice_4: 3.614  loss_ce_5: 0.3792  loss_mask_5: 0.5014  loss_dice_5: 3.615  loss_ce_6: 0.3783  loss_mask_6: 0.5019  loss_dice_6: 3.611  loss_ce_7: 0.3604  loss_mask_7: 0.4986  loss_dice_7: 3.615  loss_ce_8: 0.392  loss_mask_8: 0.5004  loss_dice_8: 3.61  time: 1.5330  data_time: 0.1006  lr: 9.2549e-06  max_mem: 21366M
[01/17 06:38:04] d2.utils.events INFO:  eta: 1 day, 11:02:29  iter: 7439  total_loss: 44.88  loss_ce: 0.3565  loss_mask: 0.4882  loss_dice: 3.603  loss_ce_0: 0.6234  loss_mask_0: 0.4707  loss_dice_0: 3.714  loss_ce_1: 0.3728  loss_mask_1: 0.4863  loss_dice_1: 3.636  loss_ce_2: 0.3751  loss_mask_2: 0.4879  loss_dice_2: 3.608  loss_ce_3: 0.3741  loss_mask_3: 0.4893  loss_dice_3: 3.597  loss_ce_4: 0.3569  loss_mask_4: 0.4902  loss_dice_4: 3.595  loss_ce_5: 0.357  loss_mask_5: 0.4899  loss_dice_5: 3.594  loss_ce_6: 0.3698  loss_mask_6: 0.4884  loss_dice_6: 3.595  loss_ce_7: 0.359  loss_mask_7: 0.4885  loss_dice_7: 3.597  loss_ce_8: 0.3541  loss_mask_8: 0.4898  loss_dice_8: 3.6  time: 1.5331  data_time: 0.1083  lr: 9.2529e-06  max_mem: 21366M
[01/17 06:38:36] d2.utils.events INFO:  eta: 1 day, 11:02:11  iter: 7459  total_loss: 45.17  loss_ce: 0.3676  loss_mask: 0.4919  loss_dice: 3.602  loss_ce_0: 0.6291  loss_mask_0: 0.4805  loss_dice_0: 3.736  loss_ce_1: 0.3769  loss_mask_1: 0.4999  loss_dice_1: 3.636  loss_ce_2: 0.3673  loss_mask_2: 0.4989  loss_dice_2: 3.611  loss_ce_3: 0.3668  loss_mask_3: 0.4961  loss_dice_3: 3.605  loss_ce_4: 0.3673  loss_mask_4: 0.4936  loss_dice_4: 3.61  loss_ce_5: 0.3705  loss_mask_5: 0.4907  loss_dice_5: 3.606  loss_ce_6: 0.3582  loss_mask_6: 0.4911  loss_dice_6: 3.601  loss_ce_7: 0.3613  loss_mask_7: 0.4874  loss_dice_7: 3.6  loss_ce_8: 0.3585  loss_mask_8: 0.4913  loss_dice_8: 3.604  time: 1.5332  data_time: 0.1058  lr: 9.2509e-06  max_mem: 21366M
[01/17 06:39:07] d2.utils.events INFO:  eta: 1 day, 11:02:34  iter: 7479  total_loss: 44.75  loss_ce: 0.3588  loss_mask: 0.4954  loss_dice: 3.551  loss_ce_0: 0.6254  loss_mask_0: 0.4725  loss_dice_0: 3.69  loss_ce_1: 0.3759  loss_mask_1: 0.4961  loss_dice_1: 3.599  loss_ce_2: 0.3806  loss_mask_2: 0.4984  loss_dice_2: 3.569  loss_ce_3: 0.3587  loss_mask_3: 0.4969  loss_dice_3: 3.553  loss_ce_4: 0.3612  loss_mask_4: 0.4921  loss_dice_4: 3.55  loss_ce_5: 0.3637  loss_mask_5: 0.4908  loss_dice_5: 3.547  loss_ce_6: 0.3523  loss_mask_6: 0.4959  loss_dice_6: 3.549  loss_ce_7: 0.3564  loss_mask_7: 0.4958  loss_dice_7: 3.555  loss_ce_8: 0.366  loss_mask_8: 0.4943  loss_dice_8: 3.546  time: 1.5333  data_time: 0.0973  lr: 9.2489e-06  max_mem: 21366M
[01/17 06:39:38] d2.utils.events INFO:  eta: 1 day, 11:04:14  iter: 7499  total_loss: 45.02  loss_ce: 0.3365  loss_mask: 0.4936  loss_dice: 3.611  loss_ce_0: 0.6504  loss_mask_0: 0.4589  loss_dice_0: 3.731  loss_ce_1: 0.3669  loss_mask_1: 0.4862  loss_dice_1: 3.648  loss_ce_2: 0.3518  loss_mask_2: 0.4916  loss_dice_2: 3.623  loss_ce_3: 0.3526  loss_mask_3: 0.4938  loss_dice_3: 3.617  loss_ce_4: 0.3538  loss_mask_4: 0.493  loss_dice_4: 3.61  loss_ce_5: 0.349  loss_mask_5: 0.4926  loss_dice_5: 3.619  loss_ce_6: 0.3288  loss_mask_6: 0.4947  loss_dice_6: 3.617  loss_ce_7: 0.3428  loss_mask_7: 0.495  loss_dice_7: 3.617  loss_ce_8: 0.3453  loss_mask_8: 0.4934  loss_dice_8: 3.61  time: 1.5334  data_time: 0.1077  lr: 9.2469e-06  max_mem: 21366M
[01/17 06:40:10] d2.utils.events INFO:  eta: 1 day, 11:04:53  iter: 7519  total_loss: 45.19  loss_ce: 0.3407  loss_mask: 0.4945  loss_dice: 3.597  loss_ce_0: 0.6022  loss_mask_0: 0.4779  loss_dice_0: 3.702  loss_ce_1: 0.3548  loss_mask_1: 0.4925  loss_dice_1: 3.626  loss_ce_2: 0.3582  loss_mask_2: 0.4944  loss_dice_2: 3.607  loss_ce_3: 0.3422  loss_mask_3: 0.4948  loss_dice_3: 3.592  loss_ce_4: 0.3426  loss_mask_4: 0.4942  loss_dice_4: 3.596  loss_ce_5: 0.3483  loss_mask_5: 0.492  loss_dice_5: 3.595  loss_ce_6: 0.3404  loss_mask_6: 0.4927  loss_dice_6: 3.594  loss_ce_7: 0.3475  loss_mask_7: 0.4954  loss_dice_7: 3.594  loss_ce_8: 0.3499  loss_mask_8: 0.4933  loss_dice_8: 3.594  time: 1.5335  data_time: 0.1022  lr: 9.2449e-06  max_mem: 21366M
[01/17 06:40:41] d2.utils.events INFO:  eta: 1 day, 11:05:03  iter: 7539  total_loss: 44.36  loss_ce: 0.3712  loss_mask: 0.4864  loss_dice: 3.536  loss_ce_0: 0.662  loss_mask_0: 0.4637  loss_dice_0: 3.665  loss_ce_1: 0.3947  loss_mask_1: 0.4839  loss_dice_1: 3.576  loss_ce_2: 0.371  loss_mask_2: 0.4849  loss_dice_2: 3.55  loss_ce_3: 0.3736  loss_mask_3: 0.486  loss_dice_3: 3.548  loss_ce_4: 0.3829  loss_mask_4: 0.4865  loss_dice_4: 3.55  loss_ce_5: 0.384  loss_mask_5: 0.4881  loss_dice_5: 3.542  loss_ce_6: 0.3767  loss_mask_6: 0.4891  loss_dice_6: 3.54  loss_ce_7: 0.3691  loss_mask_7: 0.4881  loss_dice_7: 3.532  loss_ce_8: 0.3565  loss_mask_8: 0.4861  loss_dice_8: 3.538  time: 1.5335  data_time: 0.0916  lr: 9.2428e-06  max_mem: 21366M
[01/17 06:41:11] d2.utils.events INFO:  eta: 1 day, 11:05:11  iter: 7559  total_loss: 45.08  loss_ce: 0.3531  loss_mask: 0.4948  loss_dice: 3.585  loss_ce_0: 0.6356  loss_mask_0: 0.4706  loss_dice_0: 3.709  loss_ce_1: 0.3877  loss_mask_1: 0.4984  loss_dice_1: 3.615  loss_ce_2: 0.38  loss_mask_2: 0.4995  loss_dice_2: 3.593  loss_ce_3: 0.3701  loss_mask_3: 0.5008  loss_dice_3: 3.576  loss_ce_4: 0.3504  loss_mask_4: 0.4974  loss_dice_4: 3.575  loss_ce_5: 0.341  loss_mask_5: 0.4934  loss_dice_5: 3.59  loss_ce_6: 0.3494  loss_mask_6: 0.4952  loss_dice_6: 3.583  loss_ce_7: 0.357  loss_mask_7: 0.4946  loss_dice_7: 3.583  loss_ce_8: 0.3611  loss_mask_8: 0.4938  loss_dice_8: 3.581  time: 1.5335  data_time: 0.0921  lr: 9.2408e-06  max_mem: 21366M
[01/17 06:41:43] d2.utils.events INFO:  eta: 1 day, 11:06:51  iter: 7579  total_loss: 45.44  loss_ce: 0.3478  loss_mask: 0.4834  loss_dice: 3.643  loss_ce_0: 0.604  loss_mask_0: 0.4662  loss_dice_0: 3.758  loss_ce_1: 0.3576  loss_mask_1: 0.4847  loss_dice_1: 3.675  loss_ce_2: 0.357  loss_mask_2: 0.4859  loss_dice_2: 3.65  loss_ce_3: 0.3584  loss_mask_3: 0.4877  loss_dice_3: 3.649  loss_ce_4: 0.3477  loss_mask_4: 0.4829  loss_dice_4: 3.655  loss_ce_5: 0.3449  loss_mask_5: 0.4834  loss_dice_5: 3.653  loss_ce_6: 0.3484  loss_mask_6: 0.484  loss_dice_6: 3.642  loss_ce_7: 0.3491  loss_mask_7: 0.4842  loss_dice_7: 3.641  loss_ce_8: 0.3429  loss_mask_8: 0.4849  loss_dice_8: 3.647  time: 1.5336  data_time: 0.0974  lr: 9.2388e-06  max_mem: 21366M
[01/17 06:42:14] d2.utils.events INFO:  eta: 1 day, 11:07:24  iter: 7599  total_loss: 45.16  loss_ce: 0.3646  loss_mask: 0.4849  loss_dice: 3.606  loss_ce_0: 0.6185  loss_mask_0: 0.4576  loss_dice_0: 3.725  loss_ce_1: 0.359  loss_mask_1: 0.4752  loss_dice_1: 3.643  loss_ce_2: 0.3622  loss_mask_2: 0.4776  loss_dice_2: 3.619  loss_ce_3: 0.3667  loss_mask_3: 0.4816  loss_dice_3: 3.611  loss_ce_4: 0.3511  loss_mask_4: 0.4841  loss_dice_4: 3.613  loss_ce_5: 0.3546  loss_mask_5: 0.4846  loss_dice_5: 3.607  loss_ce_6: 0.3528  loss_mask_6: 0.4881  loss_dice_6: 3.614  loss_ce_7: 0.3585  loss_mask_7: 0.4825  loss_dice_7: 3.6  loss_ce_8: 0.3474  loss_mask_8: 0.4839  loss_dice_8: 3.597  time: 1.5337  data_time: 0.1001  lr: 9.2368e-06  max_mem: 21366M
[01/17 06:42:45] d2.utils.events INFO:  eta: 1 day, 11:05:50  iter: 7619  total_loss: 45.32  loss_ce: 0.4055  loss_mask: 0.5034  loss_dice: 3.598  loss_ce_0: 0.6264  loss_mask_0: 0.4839  loss_dice_0: 3.719  loss_ce_1: 0.4041  loss_mask_1: 0.506  loss_dice_1: 3.63  loss_ce_2: 0.4054  loss_mask_2: 0.5051  loss_dice_2: 3.615  loss_ce_3: 0.3981  loss_mask_3: 0.5077  loss_dice_3: 3.606  loss_ce_4: 0.4011  loss_mask_4: 0.5068  loss_dice_4: 3.605  loss_ce_5: 0.3995  loss_mask_5: 0.5051  loss_dice_5: 3.602  loss_ce_6: 0.4023  loss_mask_6: 0.5062  loss_dice_6: 3.605  loss_ce_7: 0.4108  loss_mask_7: 0.502  loss_dice_7: 3.598  loss_ce_8: 0.4181  loss_mask_8: 0.5028  loss_dice_8: 3.597  time: 1.5336  data_time: 0.0827  lr: 9.2348e-06  max_mem: 21366M
[01/17 06:43:16] d2.utils.events INFO:  eta: 1 day, 11:06:47  iter: 7639  total_loss: 44.53  loss_ce: 0.3566  loss_mask: 0.4867  loss_dice: 3.571  loss_ce_0: 0.6321  loss_mask_0: 0.4645  loss_dice_0: 3.697  loss_ce_1: 0.3526  loss_mask_1: 0.4831  loss_dice_1: 3.601  loss_ce_2: 0.3623  loss_mask_2: 0.4845  loss_dice_2: 3.587  loss_ce_3: 0.341  loss_mask_3: 0.4838  loss_dice_3: 3.578  loss_ce_4: 0.3344  loss_mask_4: 0.4862  loss_dice_4: 3.581  loss_ce_5: 0.3418  loss_mask_5: 0.4897  loss_dice_5: 3.581  loss_ce_6: 0.3511  loss_mask_6: 0.4916  loss_dice_6: 3.581  loss_ce_7: 0.3435  loss_mask_7: 0.4856  loss_dice_7: 3.573  loss_ce_8: 0.3343  loss_mask_8: 0.4859  loss_dice_8: 3.573  time: 1.5337  data_time: 0.0891  lr: 9.2328e-06  max_mem: 21366M
[01/17 06:43:47] d2.utils.events INFO:  eta: 1 day, 11:07:38  iter: 7659  total_loss: 44.91  loss_ce: 0.3572  loss_mask: 0.4767  loss_dice: 3.592  loss_ce_0: 0.6213  loss_mask_0: 0.4574  loss_dice_0: 3.709  loss_ce_1: 0.3825  loss_mask_1: 0.4767  loss_dice_1: 3.614  loss_ce_2: 0.3699  loss_mask_2: 0.4762  loss_dice_2: 3.595  loss_ce_3: 0.3579  loss_mask_3: 0.4795  loss_dice_3: 3.588  loss_ce_4: 0.3519  loss_mask_4: 0.4764  loss_dice_4: 3.588  loss_ce_5: 0.3561  loss_mask_5: 0.48  loss_dice_5: 3.586  loss_ce_6: 0.3433  loss_mask_6: 0.4809  loss_dice_6: 3.585  loss_ce_7: 0.3603  loss_mask_7: 0.4799  loss_dice_7: 3.587  loss_ce_8: 0.3567  loss_mask_8: 0.4784  loss_dice_8: 3.589  time: 1.5337  data_time: 0.0952  lr: 9.2307e-06  max_mem: 21366M
[01/17 06:44:17] d2.utils.events INFO:  eta: 1 day, 11:09:03  iter: 7679  total_loss: 45.21  loss_ce: 0.3852  loss_mask: 0.4873  loss_dice: 3.572  loss_ce_0: 0.6299  loss_mask_0: 0.467  loss_dice_0: 3.69  loss_ce_1: 0.3904  loss_mask_1: 0.4863  loss_dice_1: 3.609  loss_ce_2: 0.3836  loss_mask_2: 0.4855  loss_dice_2: 3.596  loss_ce_3: 0.3695  loss_mask_3: 0.4891  loss_dice_3: 3.589  loss_ce_4: 0.3795  loss_mask_4: 0.4887  loss_dice_4: 3.585  loss_ce_5: 0.3787  loss_mask_5: 0.4882  loss_dice_5: 3.583  loss_ce_6: 0.3645  loss_mask_6: 0.4897  loss_dice_6: 3.575  loss_ce_7: 0.3683  loss_mask_7: 0.4875  loss_dice_7: 3.576  loss_ce_8: 0.387  loss_mask_8: 0.4868  loss_dice_8: 3.571  time: 1.5337  data_time: 0.0891  lr: 9.2287e-06  max_mem: 21366M
[01/17 06:44:49] d2.utils.events INFO:  eta: 1 day, 11:10:31  iter: 7699  total_loss: 44.78  loss_ce: 0.362  loss_mask: 0.4886  loss_dice: 3.587  loss_ce_0: 0.6274  loss_mask_0: 0.4705  loss_dice_0: 3.703  loss_ce_1: 0.3773  loss_mask_1: 0.4913  loss_dice_1: 3.618  loss_ce_2: 0.3607  loss_mask_2: 0.4898  loss_dice_2: 3.595  loss_ce_3: 0.3638  loss_mask_3: 0.4853  loss_dice_3: 3.588  loss_ce_4: 0.358  loss_mask_4: 0.4865  loss_dice_4: 3.582  loss_ce_5: 0.3594  loss_mask_5: 0.4864  loss_dice_5: 3.593  loss_ce_6: 0.353  loss_mask_6: 0.487  loss_dice_6: 3.588  loss_ce_7: 0.3527  loss_mask_7: 0.4862  loss_dice_7: 3.588  loss_ce_8: 0.3475  loss_mask_8: 0.4875  loss_dice_8: 3.582  time: 1.5338  data_time: 0.0904  lr: 9.2267e-06  max_mem: 21366M
[01/17 06:45:19] d2.utils.events INFO:  eta: 1 day, 11:09:41  iter: 7719  total_loss: 44.74  loss_ce: 0.3388  loss_mask: 0.476  loss_dice: 3.585  loss_ce_0: 0.6213  loss_mask_0: 0.4586  loss_dice_0: 3.711  loss_ce_1: 0.3737  loss_mask_1: 0.4767  loss_dice_1: 3.61  loss_ce_2: 0.3738  loss_mask_2: 0.4736  loss_dice_2: 3.591  loss_ce_3: 0.3424  loss_mask_3: 0.4756  loss_dice_3: 3.571  loss_ce_4: 0.3547  loss_mask_4: 0.4757  loss_dice_4: 3.58  loss_ce_5: 0.3465  loss_mask_5: 0.4766  loss_dice_5: 3.574  loss_ce_6: 0.3588  loss_mask_6: 0.4764  loss_dice_6: 3.574  loss_ce_7: 0.3613  loss_mask_7: 0.4772  loss_dice_7: 3.574  loss_ce_8: 0.3548  loss_mask_8: 0.4765  loss_dice_8: 3.582  time: 1.5338  data_time: 0.1047  lr: 9.2247e-06  max_mem: 21366M
[01/17 06:45:50] d2.utils.events INFO:  eta: 1 day, 11:09:29  iter: 7739  total_loss: 44.77  loss_ce: 0.3519  loss_mask: 0.4894  loss_dice: 3.572  loss_ce_0: 0.6168  loss_mask_0: 0.4686  loss_dice_0: 3.702  loss_ce_1: 0.3638  loss_mask_1: 0.4841  loss_dice_1: 3.608  loss_ce_2: 0.3633  loss_mask_2: 0.4883  loss_dice_2: 3.58  loss_ce_3: 0.3651  loss_mask_3: 0.4897  loss_dice_3: 3.575  loss_ce_4: 0.3642  loss_mask_4: 0.4878  loss_dice_4: 3.583  loss_ce_5: 0.3514  loss_mask_5: 0.4886  loss_dice_5: 3.584  loss_ce_6: 0.3581  loss_mask_6: 0.4905  loss_dice_6: 3.586  loss_ce_7: 0.3545  loss_mask_7: 0.4894  loss_dice_7: 3.586  loss_ce_8: 0.3522  loss_mask_8: 0.489  loss_dice_8: 3.583  time: 1.5337  data_time: 0.0948  lr: 9.2227e-06  max_mem: 21366M
[01/17 06:46:21] d2.utils.events INFO:  eta: 1 day, 11:09:28  iter: 7759  total_loss: 44.61  loss_ce: 0.355  loss_mask: 0.4833  loss_dice: 3.552  loss_ce_0: 0.6148  loss_mask_0: 0.4567  loss_dice_0: 3.689  loss_ce_1: 0.3829  loss_mask_1: 0.4836  loss_dice_1: 3.591  loss_ce_2: 0.375  loss_mask_2: 0.4798  loss_dice_2: 3.575  loss_ce_3: 0.3643  loss_mask_3: 0.4806  loss_dice_3: 3.556  loss_ce_4: 0.3548  loss_mask_4: 0.4814  loss_dice_4: 3.551  loss_ce_5: 0.3594  loss_mask_5: 0.4804  loss_dice_5: 3.553  loss_ce_6: 0.3556  loss_mask_6: 0.4802  loss_dice_6: 3.559  loss_ce_7: 0.357  loss_mask_7: 0.4795  loss_dice_7: 3.558  loss_ce_8: 0.3781  loss_mask_8: 0.4818  loss_dice_8: 3.545  time: 1.5337  data_time: 0.0966  lr: 9.2206e-06  max_mem: 21366M
[01/17 06:46:52] d2.utils.events INFO:  eta: 1 day, 11:08:28  iter: 7779  total_loss: 44.64  loss_ce: 0.3274  loss_mask: 0.4847  loss_dice: 3.613  loss_ce_0: 0.6128  loss_mask_0: 0.458  loss_dice_0: 3.727  loss_ce_1: 0.3314  loss_mask_1: 0.4825  loss_dice_1: 3.629  loss_ce_2: 0.3427  loss_mask_2: 0.4824  loss_dice_2: 3.619  loss_ce_3: 0.3383  loss_mask_3: 0.4856  loss_dice_3: 3.604  loss_ce_4: 0.3449  loss_mask_4: 0.4825  loss_dice_4: 3.609  loss_ce_5: 0.3222  loss_mask_5: 0.4813  loss_dice_5: 3.61  loss_ce_6: 0.3334  loss_mask_6: 0.4839  loss_dice_6: 3.608  loss_ce_7: 0.3274  loss_mask_7: 0.4843  loss_dice_7: 3.61  loss_ce_8: 0.3201  loss_mask_8: 0.4849  loss_dice_8: 3.614  time: 1.5338  data_time: 0.1016  lr: 9.2186e-06  max_mem: 21366M
[01/17 06:47:22] d2.utils.events INFO:  eta: 1 day, 11:06:46  iter: 7799  total_loss: 44.64  loss_ce: 0.3716  loss_mask: 0.4818  loss_dice: 3.567  loss_ce_0: 0.634  loss_mask_0: 0.4585  loss_dice_0: 3.696  loss_ce_1: 0.3845  loss_mask_1: 0.4805  loss_dice_1: 3.598  loss_ce_2: 0.3704  loss_mask_2: 0.4812  loss_dice_2: 3.581  loss_ce_3: 0.3805  loss_mask_3: 0.481  loss_dice_3: 3.573  loss_ce_4: 0.3494  loss_mask_4: 0.48  loss_dice_4: 3.572  loss_ce_5: 0.3525  loss_mask_5: 0.4823  loss_dice_5: 3.566  loss_ce_6: 0.3615  loss_mask_6: 0.4857  loss_dice_6: 3.556  loss_ce_7: 0.3767  loss_mask_7: 0.4827  loss_dice_7: 3.563  loss_ce_8: 0.3645  loss_mask_8: 0.4814  loss_dice_8: 3.567  time: 1.5337  data_time: 0.1125  lr: 9.2166e-06  max_mem: 21366M
[01/17 06:47:53] d2.utils.events INFO:  eta: 1 day, 11:06:15  iter: 7819  total_loss: 44.88  loss_ce: 0.3895  loss_mask: 0.487  loss_dice: 3.563  loss_ce_0: 0.6297  loss_mask_0: 0.4642  loss_dice_0: 3.68  loss_ce_1: 0.3998  loss_mask_1: 0.4909  loss_dice_1: 3.606  loss_ce_2: 0.3995  loss_mask_2: 0.4906  loss_dice_2: 3.575  loss_ce_3: 0.3935  loss_mask_3: 0.4857  loss_dice_3: 3.572  loss_ce_4: 0.3766  loss_mask_4: 0.4872  loss_dice_4: 3.566  loss_ce_5: 0.3826  loss_mask_5: 0.4871  loss_dice_5: 3.575  loss_ce_6: 0.3854  loss_mask_6: 0.4871  loss_dice_6: 3.556  loss_ce_7: 0.3854  loss_mask_7: 0.4861  loss_dice_7: 3.556  loss_ce_8: 0.3848  loss_mask_8: 0.4872  loss_dice_8: 3.561  time: 1.5338  data_time: 0.0890  lr: 9.2146e-06  max_mem: 21366M
[01/17 06:48:24] d2.utils.events INFO:  eta: 1 day, 11:04:57  iter: 7839  total_loss: 44.16  loss_ce: 0.3468  loss_mask: 0.4877  loss_dice: 3.548  loss_ce_0: 0.6094  loss_mask_0: 0.4687  loss_dice_0: 3.658  loss_ce_1: 0.3534  loss_mask_1: 0.4885  loss_dice_1: 3.56  loss_ce_2: 0.3556  loss_mask_2: 0.4919  loss_dice_2: 3.542  loss_ce_3: 0.3509  loss_mask_3: 0.4924  loss_dice_3: 3.534  loss_ce_4: 0.3373  loss_mask_4: 0.4896  loss_dice_4: 3.543  loss_ce_5: 0.3354  loss_mask_5: 0.4898  loss_dice_5: 3.541  loss_ce_6: 0.341  loss_mask_6: 0.4896  loss_dice_6: 3.54  loss_ce_7: 0.3464  loss_mask_7: 0.4887  loss_dice_7: 3.539  loss_ce_8: 0.3338  loss_mask_8: 0.4877  loss_dice_8: 3.541  time: 1.5338  data_time: 0.0936  lr: 9.2126e-06  max_mem: 21366M
[01/17 06:48:55] d2.utils.events INFO:  eta: 1 day, 11:02:31  iter: 7859  total_loss: 44.78  loss_ce: 0.361  loss_mask: 0.4904  loss_dice: 3.577  loss_ce_0: 0.6671  loss_mask_0: 0.4664  loss_dice_0: 3.698  loss_ce_1: 0.3929  loss_mask_1: 0.4889  loss_dice_1: 3.609  loss_ce_2: 0.4007  loss_mask_2: 0.4891  loss_dice_2: 3.588  loss_ce_3: 0.3852  loss_mask_3: 0.4877  loss_dice_3: 3.582  loss_ce_4: 0.3751  loss_mask_4: 0.4908  loss_dice_4: 3.585  loss_ce_5: 0.3744  loss_mask_5: 0.4896  loss_dice_5: 3.588  loss_ce_6: 0.3688  loss_mask_6: 0.4908  loss_dice_6: 3.576  loss_ce_7: 0.3692  loss_mask_7: 0.4892  loss_dice_7: 3.582  loss_ce_8: 0.3722  loss_mask_8: 0.4877  loss_dice_8: 3.583  time: 1.5338  data_time: 0.0968  lr: 9.2106e-06  max_mem: 21366M
[01/17 06:49:26] d2.utils.events INFO:  eta: 1 day, 11:03:43  iter: 7879  total_loss: 45.02  loss_ce: 0.3819  loss_mask: 0.4898  loss_dice: 3.566  loss_ce_0: 0.639  loss_mask_0: 0.4706  loss_dice_0: 3.676  loss_ce_1: 0.405  loss_mask_1: 0.4879  loss_dice_1: 3.599  loss_ce_2: 0.3883  loss_mask_2: 0.4885  loss_dice_2: 3.579  loss_ce_3: 0.3888  loss_mask_3: 0.4869  loss_dice_3: 3.58  loss_ce_4: 0.4044  loss_mask_4: 0.4864  loss_dice_4: 3.562  loss_ce_5: 0.3855  loss_mask_5: 0.4857  loss_dice_5: 3.576  loss_ce_6: 0.391  loss_mask_6: 0.488  loss_dice_6: 3.574  loss_ce_7: 0.3825  loss_mask_7: 0.4883  loss_dice_7: 3.572  loss_ce_8: 0.3813  loss_mask_8: 0.4898  loss_dice_8: 3.574  time: 1.5338  data_time: 0.0905  lr: 9.2085e-06  max_mem: 21366M
[01/17 06:49:57] d2.utils.events INFO:  eta: 1 day, 11:02:44  iter: 7899  total_loss: 44.65  loss_ce: 0.3409  loss_mask: 0.4949  loss_dice: 3.59  loss_ce_0: 0.6206  loss_mask_0: 0.4719  loss_dice_0: 3.709  loss_ce_1: 0.361  loss_mask_1: 0.4948  loss_dice_1: 3.628  loss_ce_2: 0.3644  loss_mask_2: 0.4968  loss_dice_2: 3.6  loss_ce_3: 0.3611  loss_mask_3: 0.4964  loss_dice_3: 3.597  loss_ce_4: 0.3687  loss_mask_4: 0.4962  loss_dice_4: 3.589  loss_ce_5: 0.3449  loss_mask_5: 0.4955  loss_dice_5: 3.595  loss_ce_6: 0.3503  loss_mask_6: 0.4948  loss_dice_6: 3.595  loss_ce_7: 0.3472  loss_mask_7: 0.4942  loss_dice_7: 3.597  loss_ce_8: 0.3441  loss_mask_8: 0.4943  loss_dice_8: 3.596  time: 1.5338  data_time: 0.1039  lr: 9.2065e-06  max_mem: 21366M
[01/17 06:50:27] d2.utils.events INFO:  eta: 1 day, 11:02:41  iter: 7919  total_loss: 44.77  loss_ce: 0.3788  loss_mask: 0.4772  loss_dice: 3.59  loss_ce_0: 0.6088  loss_mask_0: 0.4628  loss_dice_0: 3.72  loss_ce_1: 0.3848  loss_mask_1: 0.477  loss_dice_1: 3.63  loss_ce_2: 0.3887  loss_mask_2: 0.4782  loss_dice_2: 3.604  loss_ce_3: 0.3598  loss_mask_3: 0.4757  loss_dice_3: 3.593  loss_ce_4: 0.3705  loss_mask_4: 0.4753  loss_dice_4: 3.591  loss_ce_5: 0.3895  loss_mask_5: 0.4734  loss_dice_5: 3.589  loss_ce_6: 0.3753  loss_mask_6: 0.4751  loss_dice_6: 3.587  loss_ce_7: 0.3791  loss_mask_7: 0.4779  loss_dice_7: 3.582  loss_ce_8: 0.3839  loss_mask_8: 0.4761  loss_dice_8: 3.596  time: 1.5338  data_time: 0.1093  lr: 9.2045e-06  max_mem: 21366M
[01/17 06:50:58] d2.utils.events INFO:  eta: 1 day, 11:02:14  iter: 7939  total_loss: 44.78  loss_ce: 0.368  loss_mask: 0.4946  loss_dice: 3.544  loss_ce_0: 0.6276  loss_mask_0: 0.4778  loss_dice_0: 3.675  loss_ce_1: 0.3871  loss_mask_1: 0.4911  loss_dice_1: 3.585  loss_ce_2: 0.401  loss_mask_2: 0.4936  loss_dice_2: 3.568  loss_ce_3: 0.3667  loss_mask_3: 0.4953  loss_dice_3: 3.558  loss_ce_4: 0.3889  loss_mask_4: 0.4963  loss_dice_4: 3.554  loss_ce_5: 0.3677  loss_mask_5: 0.4949  loss_dice_5: 3.546  loss_ce_6: 0.3726  loss_mask_6: 0.4948  loss_dice_6: 3.548  loss_ce_7: 0.3734  loss_mask_7: 0.4957  loss_dice_7: 3.546  loss_ce_8: 0.3527  loss_mask_8: 0.4949  loss_dice_8: 3.552  time: 1.5338  data_time: 0.0990  lr: 9.2025e-06  max_mem: 21366M
[01/17 06:51:29] d2.utils.events INFO:  eta: 1 day, 11:01:53  iter: 7959  total_loss: 44.82  loss_ce: 0.3781  loss_mask: 0.498  loss_dice: 3.573  loss_ce_0: 0.6531  loss_mask_0: 0.4753  loss_dice_0: 3.68  loss_ce_1: 0.3846  loss_mask_1: 0.5002  loss_dice_1: 3.607  loss_ce_2: 0.3805  loss_mask_2: 0.4983  loss_dice_2: 3.582  loss_ce_3: 0.3824  loss_mask_3: 0.4985  loss_dice_3: 3.57  loss_ce_4: 0.3836  loss_mask_4: 0.4999  loss_dice_4: 3.578  loss_ce_5: 0.3845  loss_mask_5: 0.4989  loss_dice_5: 3.578  loss_ce_6: 0.3862  loss_mask_6: 0.4989  loss_dice_6: 3.577  loss_ce_7: 0.38  loss_mask_7: 0.4977  loss_dice_7: 3.577  loss_ce_8: 0.3911  loss_mask_8: 0.4973  loss_dice_8: 3.576  time: 1.5338  data_time: 0.0986  lr: 9.2005e-06  max_mem: 21366M
[01/17 06:52:00] d2.utils.events INFO:  eta: 1 day, 11:01:12  iter: 7979  total_loss: 45.08  loss_ce: 0.321  loss_mask: 0.4929  loss_dice: 3.599  loss_ce_0: 0.6042  loss_mask_0: 0.4736  loss_dice_0: 3.706  loss_ce_1: 0.3405  loss_mask_1: 0.4939  loss_dice_1: 3.625  loss_ce_2: 0.3453  loss_mask_2: 0.4936  loss_dice_2: 3.608  loss_ce_3: 0.355  loss_mask_3: 0.4913  loss_dice_3: 3.601  loss_ce_4: 0.3309  loss_mask_4: 0.487  loss_dice_4: 3.605  loss_ce_5: 0.3314  loss_mask_5: 0.4884  loss_dice_5: 3.6  loss_ce_6: 0.3322  loss_mask_6: 0.4864  loss_dice_6: 3.605  loss_ce_7: 0.3362  loss_mask_7: 0.4865  loss_dice_7: 3.602  loss_ce_8: 0.3249  loss_mask_8: 0.4883  loss_dice_8: 3.595  time: 1.5339  data_time: 0.0958  lr: 9.1984e-06  max_mem: 21366M
[01/17 06:52:30] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 06:52:31] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 06:52:31] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 06:52:32] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 06:52:47] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0085 s/iter. Inference: 0.1463 s/iter. Eval: 0.2316 s/iter. Total: 0.3864 s/iter. ETA=0:06:58
[01/17 06:52:52] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0115 s/iter. Inference: 0.1585 s/iter. Eval: 0.2516 s/iter. Total: 0.4217 s/iter. ETA=0:07:31
[01/17 06:52:57] d2.evaluation.evaluator INFO: Inference done 35/1093. Dataloading: 0.0128 s/iter. Inference: 0.1695 s/iter. Eval: 0.2503 s/iter. Total: 0.4327 s/iter. ETA=0:07:37
[01/17 06:53:02] d2.evaluation.evaluator INFO: Inference done 48/1093. Dataloading: 0.0137 s/iter. Inference: 0.1707 s/iter. Eval: 0.2373 s/iter. Total: 0.4218 s/iter. ETA=0:07:20
[01/17 06:53:08] d2.evaluation.evaluator INFO: Inference done 59/1093. Dataloading: 0.0144 s/iter. Inference: 0.1753 s/iter. Eval: 0.2447 s/iter. Total: 0.4345 s/iter. ETA=0:07:29
[01/17 06:53:13] d2.evaluation.evaluator INFO: Inference done 73/1093. Dataloading: 0.0139 s/iter. Inference: 0.1724 s/iter. Eval: 0.2357 s/iter. Total: 0.4222 s/iter. ETA=0:07:10
[01/17 06:53:18] d2.evaluation.evaluator INFO: Inference done 85/1093. Dataloading: 0.0138 s/iter. Inference: 0.1771 s/iter. Eval: 0.2354 s/iter. Total: 0.4264 s/iter. ETA=0:07:09
[01/17 06:53:23] d2.evaluation.evaluator INFO: Inference done 96/1093. Dataloading: 0.0138 s/iter. Inference: 0.1771 s/iter. Eval: 0.2402 s/iter. Total: 0.4312 s/iter. ETA=0:07:09
[01/17 06:53:29] d2.evaluation.evaluator INFO: Inference done 111/1093. Dataloading: 0.0134 s/iter. Inference: 0.1741 s/iter. Eval: 0.2321 s/iter. Total: 0.4198 s/iter. ETA=0:06:52
[01/17 06:53:34] d2.evaluation.evaluator INFO: Inference done 123/1093. Dataloading: 0.0133 s/iter. Inference: 0.1755 s/iter. Eval: 0.2338 s/iter. Total: 0.4227 s/iter. ETA=0:06:50
[01/17 06:53:39] d2.evaluation.evaluator INFO: Inference done 137/1093. Dataloading: 0.0133 s/iter. Inference: 0.1760 s/iter. Eval: 0.2285 s/iter. Total: 0.4179 s/iter. ETA=0:06:39
[01/17 06:53:44] d2.evaluation.evaluator INFO: Inference done 151/1093. Dataloading: 0.0130 s/iter. Inference: 0.1759 s/iter. Eval: 0.2231 s/iter. Total: 0.4122 s/iter. ETA=0:06:28
[01/17 06:53:49] d2.evaluation.evaluator INFO: Inference done 164/1093. Dataloading: 0.0130 s/iter. Inference: 0.1760 s/iter. Eval: 0.2210 s/iter. Total: 0.4102 s/iter. ETA=0:06:21
[01/17 06:53:55] d2.evaluation.evaluator INFO: Inference done 178/1093. Dataloading: 0.0129 s/iter. Inference: 0.1746 s/iter. Eval: 0.2199 s/iter. Total: 0.4075 s/iter. ETA=0:06:12
[01/17 06:54:00] d2.evaluation.evaluator INFO: Inference done 191/1093. Dataloading: 0.0129 s/iter. Inference: 0.1745 s/iter. Eval: 0.2188 s/iter. Total: 0.4063 s/iter. ETA=0:06:06
[01/17 06:54:05] d2.evaluation.evaluator INFO: Inference done 204/1093. Dataloading: 0.0128 s/iter. Inference: 0.1749 s/iter. Eval: 0.2172 s/iter. Total: 0.4050 s/iter. ETA=0:06:00
[01/17 06:54:10] d2.evaluation.evaluator INFO: Inference done 218/1093. Dataloading: 0.0129 s/iter. Inference: 0.1743 s/iter. Eval: 0.2154 s/iter. Total: 0.4027 s/iter. ETA=0:05:52
[01/17 06:54:15] d2.evaluation.evaluator INFO: Inference done 232/1093. Dataloading: 0.0129 s/iter. Inference: 0.1738 s/iter. Eval: 0.2148 s/iter. Total: 0.4016 s/iter. ETA=0:05:45
[01/17 06:54:21] d2.evaluation.evaluator INFO: Inference done 245/1093. Dataloading: 0.0130 s/iter. Inference: 0.1741 s/iter. Eval: 0.2155 s/iter. Total: 0.4027 s/iter. ETA=0:05:41
[01/17 06:54:26] d2.evaluation.evaluator INFO: Inference done 257/1093. Dataloading: 0.0130 s/iter. Inference: 0.1745 s/iter. Eval: 0.2164 s/iter. Total: 0.4041 s/iter. ETA=0:05:37
[01/17 06:54:31] d2.evaluation.evaluator INFO: Inference done 270/1093. Dataloading: 0.0130 s/iter. Inference: 0.1744 s/iter. Eval: 0.2156 s/iter. Total: 0.4032 s/iter. ETA=0:05:31
[01/17 06:54:37] d2.evaluation.evaluator INFO: Inference done 283/1093. Dataloading: 0.0130 s/iter. Inference: 0.1744 s/iter. Eval: 0.2163 s/iter. Total: 0.4039 s/iter. ETA=0:05:27
[01/17 06:54:42] d2.evaluation.evaluator INFO: Inference done 295/1093. Dataloading: 0.0131 s/iter. Inference: 0.1745 s/iter. Eval: 0.2170 s/iter. Total: 0.4047 s/iter. ETA=0:05:22
[01/17 06:54:47] d2.evaluation.evaluator INFO: Inference done 307/1093. Dataloading: 0.0132 s/iter. Inference: 0.1747 s/iter. Eval: 0.2186 s/iter. Total: 0.4066 s/iter. ETA=0:05:19
[01/17 06:54:52] d2.evaluation.evaluator INFO: Inference done 318/1093. Dataloading: 0.0132 s/iter. Inference: 0.1752 s/iter. Eval: 0.2200 s/iter. Total: 0.4086 s/iter. ETA=0:05:16
[01/17 06:54:57] d2.evaluation.evaluator INFO: Inference done 330/1093. Dataloading: 0.0133 s/iter. Inference: 0.1752 s/iter. Eval: 0.2205 s/iter. Total: 0.4090 s/iter. ETA=0:05:12
[01/17 06:55:02] d2.evaluation.evaluator INFO: Inference done 345/1093. Dataloading: 0.0131 s/iter. Inference: 0.1748 s/iter. Eval: 0.2180 s/iter. Total: 0.4061 s/iter. ETA=0:05:03
[01/17 06:55:08] d2.evaluation.evaluator INFO: Inference done 359/1093. Dataloading: 0.0131 s/iter. Inference: 0.1744 s/iter. Eval: 0.2175 s/iter. Total: 0.4051 s/iter. ETA=0:04:57
[01/17 06:55:13] d2.evaluation.evaluator INFO: Inference done 372/1093. Dataloading: 0.0132 s/iter. Inference: 0.1737 s/iter. Eval: 0.2177 s/iter. Total: 0.4046 s/iter. ETA=0:04:51
[01/17 06:55:18] d2.evaluation.evaluator INFO: Inference done 386/1093. Dataloading: 0.0131 s/iter. Inference: 0.1733 s/iter. Eval: 0.2172 s/iter. Total: 0.4037 s/iter. ETA=0:04:45
[01/17 06:55:23] d2.evaluation.evaluator INFO: Inference done 399/1093. Dataloading: 0.0131 s/iter. Inference: 0.1734 s/iter. Eval: 0.2173 s/iter. Total: 0.4039 s/iter. ETA=0:04:40
[01/17 06:55:29] d2.evaluation.evaluator INFO: Inference done 411/1093. Dataloading: 0.0131 s/iter. Inference: 0.1736 s/iter. Eval: 0.2179 s/iter. Total: 0.4047 s/iter. ETA=0:04:36
[01/17 06:55:34] d2.evaluation.evaluator INFO: Inference done 423/1093. Dataloading: 0.0131 s/iter. Inference: 0.1734 s/iter. Eval: 0.2187 s/iter. Total: 0.4052 s/iter. ETA=0:04:31
[01/17 06:55:39] d2.evaluation.evaluator INFO: Inference done 436/1093. Dataloading: 0.0131 s/iter. Inference: 0.1732 s/iter. Eval: 0.2184 s/iter. Total: 0.4049 s/iter. ETA=0:04:26
[01/17 06:55:44] d2.evaluation.evaluator INFO: Inference done 450/1093. Dataloading: 0.0131 s/iter. Inference: 0.1734 s/iter. Eval: 0.2171 s/iter. Total: 0.4036 s/iter. ETA=0:04:19
[01/17 06:55:49] d2.evaluation.evaluator INFO: Inference done 464/1093. Dataloading: 0.0130 s/iter. Inference: 0.1728 s/iter. Eval: 0.2165 s/iter. Total: 0.4025 s/iter. ETA=0:04:13
[01/17 06:55:54] d2.evaluation.evaluator INFO: Inference done 479/1093. Dataloading: 0.0130 s/iter. Inference: 0.1726 s/iter. Eval: 0.2150 s/iter. Total: 0.4006 s/iter. ETA=0:04:05
[01/17 06:55:59] d2.evaluation.evaluator INFO: Inference done 493/1093. Dataloading: 0.0129 s/iter. Inference: 0.1726 s/iter. Eval: 0.2140 s/iter. Total: 0.3996 s/iter. ETA=0:03:59
[01/17 06:56:04] d2.evaluation.evaluator INFO: Inference done 507/1093. Dataloading: 0.0128 s/iter. Inference: 0.1729 s/iter. Eval: 0.2130 s/iter. Total: 0.3989 s/iter. ETA=0:03:53
[01/17 06:56:10] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0129 s/iter. Inference: 0.1729 s/iter. Eval: 0.2123 s/iter. Total: 0.3981 s/iter. ETA=0:03:47
[01/17 06:56:15] d2.evaluation.evaluator INFO: Inference done 534/1093. Dataloading: 0.0129 s/iter. Inference: 0.1727 s/iter. Eval: 0.2129 s/iter. Total: 0.3985 s/iter. ETA=0:03:42
[01/17 06:56:20] d2.evaluation.evaluator INFO: Inference done 548/1093. Dataloading: 0.0128 s/iter. Inference: 0.1725 s/iter. Eval: 0.2126 s/iter. Total: 0.3980 s/iter. ETA=0:03:36
[01/17 06:56:26] d2.evaluation.evaluator INFO: Inference done 561/1093. Dataloading: 0.0128 s/iter. Inference: 0.1728 s/iter. Eval: 0.2124 s/iter. Total: 0.3982 s/iter. ETA=0:03:31
[01/17 06:56:31] d2.evaluation.evaluator INFO: Inference done 576/1093. Dataloading: 0.0128 s/iter. Inference: 0.1727 s/iter. Eval: 0.2115 s/iter. Total: 0.3971 s/iter. ETA=0:03:25
[01/17 06:56:36] d2.evaluation.evaluator INFO: Inference done 589/1093. Dataloading: 0.0128 s/iter. Inference: 0.1736 s/iter. Eval: 0.2105 s/iter. Total: 0.3970 s/iter. ETA=0:03:20
[01/17 06:56:41] d2.evaluation.evaluator INFO: Inference done 601/1093. Dataloading: 0.0128 s/iter. Inference: 0.1737 s/iter. Eval: 0.2108 s/iter. Total: 0.3974 s/iter. ETA=0:03:15
[01/17 06:56:46] d2.evaluation.evaluator INFO: Inference done 613/1093. Dataloading: 0.0129 s/iter. Inference: 0.1736 s/iter. Eval: 0.2115 s/iter. Total: 0.3980 s/iter. ETA=0:03:11
[01/17 06:56:51] d2.evaluation.evaluator INFO: Inference done 627/1093. Dataloading: 0.0128 s/iter. Inference: 0.1736 s/iter. Eval: 0.2109 s/iter. Total: 0.3974 s/iter. ETA=0:03:05
[01/17 06:56:56] d2.evaluation.evaluator INFO: Inference done 640/1093. Dataloading: 0.0128 s/iter. Inference: 0.1736 s/iter. Eval: 0.2106 s/iter. Total: 0.3972 s/iter. ETA=0:02:59
[01/17 06:57:02] d2.evaluation.evaluator INFO: Inference done 655/1093. Dataloading: 0.0127 s/iter. Inference: 0.1733 s/iter. Eval: 0.2098 s/iter. Total: 0.3960 s/iter. ETA=0:02:53
[01/17 06:57:07] d2.evaluation.evaluator INFO: Inference done 667/1093. Dataloading: 0.0127 s/iter. Inference: 0.1739 s/iter. Eval: 0.2098 s/iter. Total: 0.3965 s/iter. ETA=0:02:48
[01/17 06:57:12] d2.evaluation.evaluator INFO: Inference done 681/1093. Dataloading: 0.0127 s/iter. Inference: 0.1738 s/iter. Eval: 0.2091 s/iter. Total: 0.3957 s/iter. ETA=0:02:43
[01/17 06:57:17] d2.evaluation.evaluator INFO: Inference done 696/1093. Dataloading: 0.0127 s/iter. Inference: 0.1740 s/iter. Eval: 0.2078 s/iter. Total: 0.3946 s/iter. ETA=0:02:36
[01/17 06:57:22] d2.evaluation.evaluator INFO: Inference done 709/1093. Dataloading: 0.0127 s/iter. Inference: 0.1742 s/iter. Eval: 0.2079 s/iter. Total: 0.3949 s/iter. ETA=0:02:31
[01/17 06:57:27] d2.evaluation.evaluator INFO: Inference done 722/1093. Dataloading: 0.0127 s/iter. Inference: 0.1743 s/iter. Eval: 0.2080 s/iter. Total: 0.3950 s/iter. ETA=0:02:26
[01/17 06:57:33] d2.evaluation.evaluator INFO: Inference done 736/1093. Dataloading: 0.0127 s/iter. Inference: 0.1745 s/iter. Eval: 0.2072 s/iter. Total: 0.3946 s/iter. ETA=0:02:20
[01/17 06:57:38] d2.evaluation.evaluator INFO: Inference done 751/1093. Dataloading: 0.0126 s/iter. Inference: 0.1747 s/iter. Eval: 0.2063 s/iter. Total: 0.3937 s/iter. ETA=0:02:14
[01/17 06:57:43] d2.evaluation.evaluator INFO: Inference done 764/1093. Dataloading: 0.0126 s/iter. Inference: 0.1747 s/iter. Eval: 0.2066 s/iter. Total: 0.3940 s/iter. ETA=0:02:09
[01/17 06:57:49] d2.evaluation.evaluator INFO: Inference done 777/1093. Dataloading: 0.0126 s/iter. Inference: 0.1744 s/iter. Eval: 0.2071 s/iter. Total: 0.3942 s/iter. ETA=0:02:04
[01/17 06:57:54] d2.evaluation.evaluator INFO: Inference done 791/1093. Dataloading: 0.0126 s/iter. Inference: 0.1742 s/iter. Eval: 0.2066 s/iter. Total: 0.3936 s/iter. ETA=0:01:58
[01/17 06:57:59] d2.evaluation.evaluator INFO: Inference done 805/1093. Dataloading: 0.0126 s/iter. Inference: 0.1744 s/iter. Eval: 0.2062 s/iter. Total: 0.3933 s/iter. ETA=0:01:53
[01/17 06:58:04] d2.evaluation.evaluator INFO: Inference done 820/1093. Dataloading: 0.0125 s/iter. Inference: 0.1741 s/iter. Eval: 0.2055 s/iter. Total: 0.3922 s/iter. ETA=0:01:47
[01/17 06:58:09] d2.evaluation.evaluator INFO: Inference done 835/1093. Dataloading: 0.0125 s/iter. Inference: 0.1742 s/iter. Eval: 0.2048 s/iter. Total: 0.3916 s/iter. ETA=0:01:41
[01/17 06:58:14] d2.evaluation.evaluator INFO: Inference done 849/1093. Dataloading: 0.0125 s/iter. Inference: 0.1741 s/iter. Eval: 0.2046 s/iter. Total: 0.3913 s/iter. ETA=0:01:35
[01/17 06:58:20] d2.evaluation.evaluator INFO: Inference done 860/1093. Dataloading: 0.0125 s/iter. Inference: 0.1744 s/iter. Eval: 0.2055 s/iter. Total: 0.3925 s/iter. ETA=0:01:31
[01/17 06:58:25] d2.evaluation.evaluator INFO: Inference done 873/1093. Dataloading: 0.0125 s/iter. Inference: 0.1746 s/iter. Eval: 0.2056 s/iter. Total: 0.3927 s/iter. ETA=0:01:26
[01/17 06:58:30] d2.evaluation.evaluator INFO: Inference done 885/1093. Dataloading: 0.0126 s/iter. Inference: 0.1748 s/iter. Eval: 0.2060 s/iter. Total: 0.3935 s/iter. ETA=0:01:21
[01/17 06:58:36] d2.evaluation.evaluator INFO: Inference done 898/1093. Dataloading: 0.0126 s/iter. Inference: 0.1748 s/iter. Eval: 0.2061 s/iter. Total: 0.3935 s/iter. ETA=0:01:16
[01/17 06:58:41] d2.evaluation.evaluator INFO: Inference done 912/1093. Dataloading: 0.0125 s/iter. Inference: 0.1747 s/iter. Eval: 0.2058 s/iter. Total: 0.3932 s/iter. ETA=0:01:11
[01/17 06:58:46] d2.evaluation.evaluator INFO: Inference done 925/1093. Dataloading: 0.0126 s/iter. Inference: 0.1747 s/iter. Eval: 0.2059 s/iter. Total: 0.3933 s/iter. ETA=0:01:06
[01/17 06:58:51] d2.evaluation.evaluator INFO: Inference done 938/1093. Dataloading: 0.0126 s/iter. Inference: 0.1749 s/iter. Eval: 0.2060 s/iter. Total: 0.3936 s/iter. ETA=0:01:01
[01/17 06:58:57] d2.evaluation.evaluator INFO: Inference done 951/1093. Dataloading: 0.0126 s/iter. Inference: 0.1747 s/iter. Eval: 0.2061 s/iter. Total: 0.3935 s/iter. ETA=0:00:55
[01/17 06:59:02] d2.evaluation.evaluator INFO: Inference done 963/1093. Dataloading: 0.0126 s/iter. Inference: 0.1748 s/iter. Eval: 0.2064 s/iter. Total: 0.3940 s/iter. ETA=0:00:51
[01/17 06:59:07] d2.evaluation.evaluator INFO: Inference done 978/1093. Dataloading: 0.0126 s/iter. Inference: 0.1747 s/iter. Eval: 0.2059 s/iter. Total: 0.3933 s/iter. ETA=0:00:45
[01/17 06:59:12] d2.evaluation.evaluator INFO: Inference done 992/1093. Dataloading: 0.0125 s/iter. Inference: 0.1748 s/iter. Eval: 0.2054 s/iter. Total: 0.3929 s/iter. ETA=0:00:39
[01/17 06:59:17] d2.evaluation.evaluator INFO: Inference done 1005/1093. Dataloading: 0.0125 s/iter. Inference: 0.1750 s/iter. Eval: 0.2054 s/iter. Total: 0.3930 s/iter. ETA=0:00:34
[01/17 06:59:23] d2.evaluation.evaluator INFO: Inference done 1019/1093. Dataloading: 0.0125 s/iter. Inference: 0.1748 s/iter. Eval: 0.2054 s/iter. Total: 0.3928 s/iter. ETA=0:00:29
[01/17 06:59:28] d2.evaluation.evaluator INFO: Inference done 1033/1093. Dataloading: 0.0125 s/iter. Inference: 0.1747 s/iter. Eval: 0.2052 s/iter. Total: 0.3925 s/iter. ETA=0:00:23
[01/17 06:59:33] d2.evaluation.evaluator INFO: Inference done 1046/1093. Dataloading: 0.0125 s/iter. Inference: 0.1747 s/iter. Eval: 0.2051 s/iter. Total: 0.3925 s/iter. ETA=0:00:18
[01/17 06:59:38] d2.evaluation.evaluator INFO: Inference done 1060/1093. Dataloading: 0.0125 s/iter. Inference: 0.1746 s/iter. Eval: 0.2050 s/iter. Total: 0.3922 s/iter. ETA=0:00:12
[01/17 06:59:43] d2.evaluation.evaluator INFO: Inference done 1077/1093. Dataloading: 0.0124 s/iter. Inference: 0.1743 s/iter. Eval: 0.2040 s/iter. Total: 0.3908 s/iter. ETA=0:00:06
[01/17 06:59:48] d2.evaluation.evaluator INFO: Total inference time: 0:07:04.092460 (0.389791 s / iter per device, on 4 devices)
[01/17 06:59:48] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:08 (0.173642 s / iter per device, on 4 devices)
[01/17 07:00:14] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 8.803069530444898, 'fwIoU': 27.692060045991067, 'IoU-1': nan, 'IoU-2': 94.90962583966365, 'IoU-3': 44.92968622231464, 'IoU-4': 56.08645086257027, 'IoU-5': 50.08681490123459, 'IoU-6': 43.01596429885903, 'IoU-7': 37.46058089137673, 'IoU-8': 30.039964476021314, 'IoU-9': 7.695989009468263, 'IoU-10': 12.590636011622092, 'IoU-11': 16.085042474833227, 'IoU-12': 26.2118310824391, 'IoU-13': 28.584621756854684, 'IoU-14': 25.966558489524584, 'IoU-15': 23.846193749572507, 'IoU-16': 24.413054705310422, 'IoU-17': 23.56043995738696, 'IoU-18': 23.76687999040758, 'IoU-19': 24.929264249330547, 'IoU-20': 23.070892083911513, 'IoU-21': 20.93197296411909, 'IoU-22': 22.86762003407173, 'IoU-23': 25.013397583170942, 'IoU-24': 21.417616705262102, 'IoU-25': 20.69853316323404, 'IoU-26': 22.159518314865462, 'IoU-27': 23.740234981985022, 'IoU-28': 24.612112174363663, 'IoU-29': 25.12134889305096, 'IoU-30': 23.66946666454311, 'IoU-31': 25.566217727442414, 'IoU-32': 26.33695882265424, 'IoU-33': 25.205771636944313, 'IoU-34': 25.717156733986002, 'IoU-35': 24.909115398402935, 'IoU-36': 23.891357151881724, 'IoU-37': 24.325953782354283, 'IoU-38': 25.38217987706302, 'IoU-39': 25.210204830718634, 'IoU-40': 23.376677116441194, 'IoU-41': 23.382805136961878, 'IoU-42': 23.092486995595564, 'IoU-43': 24.078337529368433, 'IoU-44': 23.03521213078092, 'IoU-45': 21.936902037971485, 'IoU-46': 22.085189003445496, 'IoU-47': 20.791875576478628, 'IoU-48': 20.96089498097499, 'IoU-49': 23.235470810787838, 'IoU-50': 22.265251471630144, 'IoU-51': 22.863815200125504, 'IoU-52': 21.769414200264446, 'IoU-53': 19.044849461195906, 'IoU-54': 22.056267757347985, 'IoU-55': 21.414980319227595, 'IoU-56': 18.573794434951264, 'IoU-57': 18.095211840614258, 'IoU-58': 18.928669182214744, 'IoU-59': 15.240161592310963, 'IoU-60': 16.738436428795186, 'IoU-61': 15.31901888941912, 'IoU-62': 14.355685672141396, 'IoU-63': 14.623014203174142, 'IoU-64': 13.874825091336138, 'IoU-65': 12.145203827388784, 'IoU-66': 12.889653838529819, 'IoU-67': 9.251090638566378, 'IoU-68': 14.08790851711968, 'IoU-69': 12.137714817335077, 'IoU-70': 11.683927571058486, 'IoU-71': 10.481956416619656, 'IoU-72': 9.413394060060902, 'IoU-73': 13.229486374165964, 'IoU-74': 10.930162382850451, 'IoU-75': 11.540906980995434, 'IoU-76': 12.81402355810053, 'IoU-77': 6.763304272288793, 'IoU-78': 11.658703689435326, 'IoU-79': 7.471793017420317, 'IoU-80': 11.514436679871718, 'IoU-81': 8.913554403947392, 'IoU-82': 10.272405504090923, 'IoU-83': 11.424142833295793, 'IoU-84': 10.07291100303072, 'IoU-85': 11.026056475416102, 'IoU-86': 12.038182936085052, 'IoU-87': 10.544180467553268, 'IoU-88': 10.331706517773636, 'IoU-89': 11.278620740832164, 'IoU-90': 9.978745195643315, 'IoU-91': 11.06800826758899, 'IoU-92': 10.80929910176383, 'IoU-93': 10.425698373679726, 'IoU-94': 10.901634540691447, 'IoU-95': 8.656501474662786, 'IoU-96': 12.986958274811977, 'IoU-97': 7.617239696450662, 'IoU-98': 8.750082643748827, 'IoU-99': 12.170724634858386, 'IoU-100': 11.763765288960858, 'IoU-101': 7.5946356685532, 'IoU-102': 10.394939713763394, 'IoU-103': 7.4861502013488845, 'IoU-104': 12.011206694838647, 'IoU-105': 8.421447720890885, 'IoU-106': 11.185736226676825, 'IoU-107': 5.30211190077135, 'IoU-108': 8.5320197494882, 'IoU-109': 10.669343087747325, 'IoU-110': 7.536456495558154, 'IoU-111': 9.752355018677413, 'IoU-112': 4.742473838360577, 'IoU-113': 8.255030345361218, 'IoU-114': 6.373487201644059, 'IoU-115': 7.731322088061471, 'IoU-116': 5.590916678535384, 'IoU-117': 5.26839944894165, 'IoU-118': 4.424013192304553, 'IoU-119': 7.596983584445841, 'IoU-120': 3.7967145388181187, 'IoU-121': 3.7397796892683655, 'IoU-122': 5.248943100133492, 'IoU-123': 3.802543455340369, 'IoU-124': 4.517195011269497, 'IoU-125': 4.316655353754606, 'IoU-126': 2.953801384510685, 'IoU-127': 3.972662038719299, 'IoU-128': 3.9735702241919717, 'IoU-129': 3.433815338253994, 'IoU-130': 2.271535578169619, 'IoU-131': 2.2232832583748707, 'IoU-132': 3.2328808547464685, 'IoU-133': 3.364853689182933, 'IoU-134': 3.3233151391499627, 'IoU-135': 4.083246198583127, 'IoU-136': 2.64125435917807, 'IoU-137': 2.6278057878173957, 'IoU-138': 1.1635628859667637, 'IoU-139': 2.9502065268557596, 'IoU-140': 3.0353208901923923, 'IoU-141': 2.1657111928275987, 'IoU-142': 2.537372287983365, 'IoU-143': 1.4224484546001446, 'IoU-144': 3.33209368815002, 'IoU-145': 2.7763170675339377, 'IoU-146': 0.9855920876596891, 'IoU-147': 2.854202252080727, 'IoU-148': 1.8441653171009158, 'IoU-149': 1.0585523672580406, 'IoU-150': 1.6006399535459053, 'IoU-151': 2.4313307705966025, 'IoU-152': 1.7173548252758963, 'IoU-153': 0.7638957702295793, 'IoU-154': 1.1798812905867764, 'IoU-155': 1.0201793240795585, 'IoU-156': 2.0594982699429987, 'IoU-157': 0.697246396357088, 'IoU-158': 1.489612704172897, 'IoU-159': 0.1436952251681991, 'IoU-160': 1.2585953522038098, 'IoU-161': 0.37858756547022054, 'IoU-162': 1.2578144681792962, 'IoU-163': 1.4198939718949766, 'IoU-164': 0.8564360991580656, 'IoU-165': 1.3222221739715736, 'IoU-166': 1.317619778560315, 'IoU-167': 0.8303962137315298, 'IoU-168': 0.9308602258206461, 'IoU-169': 0.6879079347766891, 'IoU-170': 0.15015615573576516, 'IoU-171': 0.4341738967512935, 'IoU-172': 0.5394013111320642, 'IoU-173': 1.1471058989842697, 'IoU-174': 1.186404559979624, 'IoU-175': 0.41498170614288465, 'IoU-176': 0.7960054514716443, 'IoU-177': 0.0, 'IoU-178': 0.5707564193388849, 'IoU-179': 0.2471298043180106, 'IoU-180': 1.0375524689768583, 'IoU-181': 0.5012099595421622, 'IoU-182': 0.3181603581294901, 'IoU-183': 0.030525383475129907, 'IoU-184': 0.07013405167604445, 'IoU-185': 0.3439550958285265, 'IoU-186': 0.018524078107733485, 'IoU-187': 0.10392410875340265, 'IoU-188': 0.009155748436655954, 'IoU-189': 1.3460247839672912, 'IoU-190': 0.056488128597226295, 'IoU-191': 0.0, 'IoU-192': 0.34838533836239105, 'IoU-193': 0.21684826486885597, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 14.555339279157373, 'pACC': 39.664861691943344, 'ACC-1': nan, 'ACC-2': 98.54552976190004, 'ACC-3': 58.00867457725396, 'ACC-4': 70.87470623984693, 'ACC-5': 68.62258410136549, 'ACC-6': 59.02345642577919, 'ACC-7': 49.905576912132204, 'ACC-8': 41.44086986009167, 'ACC-9': 8.490872972249704, 'ACC-10': 15.569478446031221, 'ACC-11': 22.063255659693954, 'ACC-12': 39.45457190451559, 'ACC-13': 51.40817535433795, 'ACC-14': 43.938004241760886, 'ACC-15': 38.37326771698217, 'ACC-16': 43.04364413659938, 'ACC-17': 33.50928118285682, 'ACC-18': 41.777838200628345, 'ACC-19': 41.31966401974943, 'ACC-20': 40.21508505239252, 'ACC-21': 30.916833845050274, 'ACC-22': 38.42409963712874, 'ACC-23': 43.13502279988179, 'ACC-24': 36.273098667086714, 'ACC-25': 30.27734529356375, 'ACC-26': 34.74333687005702, 'ACC-27': 39.51413325345739, 'ACC-28': 37.520215002648904, 'ACC-29': 41.088849010151016, 'ACC-30': 36.83595287581725, 'ACC-31': 39.930001943008314, 'ACC-32': 42.94466396950316, 'ACC-33': 41.660046186275764, 'ACC-34': 42.32850582312272, 'ACC-35': 39.26528043093034, 'ACC-36': 36.509573392525326, 'ACC-37': 35.00044282678699, 'ACC-38': 39.27244009477441, 'ACC-39': 40.604402188421616, 'ACC-40': 35.708453716417985, 'ACC-41': 37.92584886612096, 'ACC-42': 38.24678495049682, 'ACC-43': 39.99171509163085, 'ACC-44': 35.859655696539136, 'ACC-45': 33.27013984212908, 'ACC-46': 34.83074944430395, 'ACC-47': 33.93522647072743, 'ACC-48': 37.6320378753705, 'ACC-49': 40.03054155653108, 'ACC-50': 35.941705350108286, 'ACC-51': 40.01413859096899, 'ACC-52': 36.94910146403503, 'ACC-53': 30.431765778157775, 'ACC-54': 38.93427562298945, 'ACC-55': 39.25373764723386, 'ACC-56': 29.841446293085767, 'ACC-57': 34.886317415097096, 'ACC-58': 33.38954088584364, 'ACC-59': 25.52230826924137, 'ACC-60': 29.061306741283083, 'ACC-61': 27.025083460501463, 'ACC-62': 26.01345643777512, 'ACC-63': 26.82675813849479, 'ACC-64': 23.215827242583746, 'ACC-65': 23.017724521493644, 'ACC-66': 22.78912718663308, 'ACC-67': 13.841382222874527, 'ACC-68': 26.807149690535436, 'ACC-69': 22.804799552462327, 'ACC-70': 22.06474308600992, 'ACC-71': 16.255587952347206, 'ACC-72': 14.598003094976756, 'ACC-73': 26.229000122054764, 'ACC-74': 18.666834028045113, 'ACC-75': 20.59660008450716, 'ACC-76': 27.920258938409308, 'ACC-77': 9.815619944258966, 'ACC-78': 26.01566172285501, 'ACC-79': 11.927186661795082, 'ACC-80': 22.874443940627746, 'ACC-81': 14.157141402226703, 'ACC-82': 17.330886842020703, 'ACC-83': 21.69074900950475, 'ACC-84': 16.196184808169185, 'ACC-85': 18.634031729339632, 'ACC-86': 23.874945435137132, 'ACC-87': 17.848305483203653, 'ACC-88': 16.246443389328846, 'ACC-89': 22.12089417812928, 'ACC-90': 17.182473935013164, 'ACC-91': 20.975220986241638, 'ACC-92': 18.25032161704724, 'ACC-93': 19.34740904206374, 'ACC-94': 19.32059804555481, 'ACC-95': 13.793513456967627, 'ACC-96': 26.346779194696367, 'ACC-97': 10.893459957244696, 'ACC-98': 15.770552433567223, 'ACC-99': 22.11059757918067, 'ACC-100': 20.89762320012157, 'ACC-101': 12.06127493315882, 'ACC-102': 17.980263802699927, 'ACC-103': 12.756932487539647, 'ACC-104': 25.27363747992821, 'ACC-105': 15.384471606699293, 'ACC-106': 26.109122317650822, 'ACC-107': 10.085571478656167, 'ACC-108': 15.222287856437303, 'ACC-109': 23.226777823623358, 'ACC-110': 12.194850903565358, 'ACC-111': 23.637701062487334, 'ACC-112': 7.7833363262371735, 'ACC-113': 18.360816768866208, 'ACC-114': 11.678552417809728, 'ACC-115': 14.015285002133748, 'ACC-116': 13.72190398869394, 'ACC-117': 11.146321831310301, 'ACC-118': 7.426383900694914, 'ACC-119': 21.227007465685183, 'ACC-120': 6.814303182836043, 'ACC-121': 6.054029313683116, 'ACC-122': 11.083712181945444, 'ACC-123': 6.85499164991933, 'ACC-124': 9.535910031993273, 'ACC-125': 7.4500519684343045, 'ACC-126': 5.337119288028776, 'ACC-127': 7.156713631271687, 'ACC-128': 6.653587548710855, 'ACC-129': 6.910064874477218, 'ACC-130': 3.400878160143427, 'ACC-131': 3.3213549580412907, 'ACC-132': 6.223911137871474, 'ACC-133': 6.342820185447685, 'ACC-134': 6.648839535405908, 'ACC-135': 8.434088754847048, 'ACC-136': 4.652955931816886, 'ACC-137': 4.2320134877003355, 'ACC-138': 1.4389219495701857, 'ACC-139': 5.473370434297028, 'ACC-140': 4.452786752727806, 'ACC-141': 3.376998345168937, 'ACC-142': 4.797783010372993, 'ACC-143': 2.453144417633646, 'ACC-144': 8.252482583054546, 'ACC-145': 5.984837545126354, 'ACC-146': 1.3541447706791296, 'ACC-147': 5.9935573781727625, 'ACC-148': 3.155110154942093, 'ACC-149': 1.9033297815410088, 'ACC-150': 3.2203628752146147, 'ACC-151': 3.9534755250367675, 'ACC-152': 2.9886505457308723, 'ACC-153': 0.9016327803628026, 'ACC-154': 1.742084867806299, 'ACC-155': 2.23268014376511, 'ACC-156': 7.506477051148367, 'ACC-157': 1.136212323941386, 'ACC-158': 10.498779828863002, 'ACC-159': 0.15538754443928526, 'ACC-160': 2.603158040982514, 'ACC-161': 0.5283502239514998, 'ACC-162': 2.4257393765955118, 'ACC-163': 4.154554732059709, 'ACC-164': 1.408355970264474, 'ACC-165': 2.41375701977746, 'ACC-166': 3.198264443646828, 'ACC-167': 1.9452535444174899, 'ACC-168': 1.7602190155072288, 'ACC-169': 1.6212462198466842, 'ACC-170': 0.1580998538322106, 'ACC-171': 0.522275224668853, 'ACC-172': 0.9783085878448006, 'ACC-173': 3.1259028780975666, 'ACC-174': 14.486760262348872, 'ACC-175': 0.5623684715768126, 'ACC-176': 1.1046833025501084, 'ACC-177': 0.0, 'ACC-178': 1.4972253441329915, 'ACC-179': 0.37067012290640916, 'ACC-180': 2.4117383303474877, 'ACC-181': 0.8991161231331911, 'ACC-182': 0.39745985726501193, 'ACC-183': 0.031273025357211395, 'ACC-184': 0.07251307236475017, 'ACC-185': 0.41837945176726404, 'ACC-186': 0.020093399664648088, 'ACC-187': 0.1196847122255455, 'ACC-188': 0.009664286836033294, 'ACC-189': 11.411716637746661, 'ACC-190': 0.06520829428965319, 'ACC-191': 0.0, 'ACC-192': 0.8274061990212072, 'ACC-193': 0.3557167962283266, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 07:00:14] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 07:00:14] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 07:00:14] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 07:00:14] d2.evaluation.testing INFO: copypaste: 8.8031,27.6921,14.5553,39.6649
[01/17 07:00:14] d2.utils.events INFO:  eta: 1 day, 11:01:00  iter: 7999  total_loss: 44.17  loss_ce: 0.3346  loss_mask: 0.4819  loss_dice: 3.524  loss_ce_0: 0.6118  loss_mask_0: 0.4661  loss_dice_0: 3.667  loss_ce_1: 0.3735  loss_mask_1: 0.4859  loss_dice_1: 3.567  loss_ce_2: 0.357  loss_mask_2: 0.4868  loss_dice_2: 3.54  loss_ce_3: 0.3352  loss_mask_3: 0.4832  loss_dice_3: 3.531  loss_ce_4: 0.3405  loss_mask_4: 0.4842  loss_dice_4: 3.536  loss_ce_5: 0.3363  loss_mask_5: 0.4831  loss_dice_5: 3.536  loss_ce_6: 0.3511  loss_mask_6: 0.4837  loss_dice_6: 3.533  loss_ce_7: 0.3511  loss_mask_7: 0.483  loss_dice_7: 3.533  loss_ce_8: 0.3358  loss_mask_8: 0.4834  loss_dice_8: 3.531  time: 1.5338  data_time: 0.1029  lr: 9.1964e-06  max_mem: 21366M
[01/17 07:00:45] d2.utils.events INFO:  eta: 1 day, 11:00:03  iter: 8019  total_loss: 44.62  loss_ce: 0.3995  loss_mask: 0.4814  loss_dice: 3.533  loss_ce_0: 0.6382  loss_mask_0: 0.4578  loss_dice_0: 3.668  loss_ce_1: 0.3967  loss_mask_1: 0.4759  loss_dice_1: 3.582  loss_ce_2: 0.3952  loss_mask_2: 0.4761  loss_dice_2: 3.561  loss_ce_3: 0.4038  loss_mask_3: 0.4755  loss_dice_3: 3.534  loss_ce_4: 0.3857  loss_mask_4: 0.4763  loss_dice_4: 3.536  loss_ce_5: 0.3815  loss_mask_5: 0.4771  loss_dice_5: 3.534  loss_ce_6: 0.395  loss_mask_6: 0.4791  loss_dice_6: 3.531  loss_ce_7: 0.3934  loss_mask_7: 0.4796  loss_dice_7: 3.527  loss_ce_8: 0.4106  loss_mask_8: 0.4818  loss_dice_8: 3.533  time: 1.5338  data_time: 0.0924  lr: 9.1944e-06  max_mem: 21366M
[01/17 07:01:15] d2.utils.events INFO:  eta: 1 day, 10:59:32  iter: 8039  total_loss: 44.5  loss_ce: 0.365  loss_mask: 0.4823  loss_dice: 3.539  loss_ce_0: 0.6209  loss_mask_0: 0.4583  loss_dice_0: 3.664  loss_ce_1: 0.3952  loss_mask_1: 0.4853  loss_dice_1: 3.564  loss_ce_2: 0.3776  loss_mask_2: 0.483  loss_dice_2: 3.552  loss_ce_3: 0.3842  loss_mask_3: 0.4836  loss_dice_3: 3.546  loss_ce_4: 0.3654  loss_mask_4: 0.484  loss_dice_4: 3.546  loss_ce_5: 0.3535  loss_mask_5: 0.4874  loss_dice_5: 3.542  loss_ce_6: 0.3595  loss_mask_6: 0.4864  loss_dice_6: 3.541  loss_ce_7: 0.3601  loss_mask_7: 0.4848  loss_dice_7: 3.543  loss_ce_8: 0.3524  loss_mask_8: 0.4847  loss_dice_8: 3.544  time: 1.5338  data_time: 0.0956  lr: 9.1924e-06  max_mem: 21366M
[01/17 07:01:46] d2.utils.events INFO:  eta: 1 day, 10:59:09  iter: 8059  total_loss: 44.42  loss_ce: 0.3714  loss_mask: 0.4779  loss_dice: 3.52  loss_ce_0: 0.6198  loss_mask_0: 0.4619  loss_dice_0: 3.668  loss_ce_1: 0.3936  loss_mask_1: 0.4799  loss_dice_1: 3.556  loss_ce_2: 0.3957  loss_mask_2: 0.4808  loss_dice_2: 3.538  loss_ce_3: 0.3722  loss_mask_3: 0.482  loss_dice_3: 3.514  loss_ce_4: 0.3775  loss_mask_4: 0.4786  loss_dice_4: 3.531  loss_ce_5: 0.3526  loss_mask_5: 0.4795  loss_dice_5: 3.536  loss_ce_6: 0.3688  loss_mask_6: 0.4774  loss_dice_6: 3.525  loss_ce_7: 0.3676  loss_mask_7: 0.4793  loss_dice_7: 3.523  loss_ce_8: 0.3768  loss_mask_8: 0.4821  loss_dice_8: 3.524  time: 1.5338  data_time: 0.0957  lr: 9.1904e-06  max_mem: 21366M
[01/17 07:02:17] d2.utils.events INFO:  eta: 1 day, 10:58:03  iter: 8079  total_loss: 44.47  loss_ce: 0.3663  loss_mask: 0.4664  loss_dice: 3.493  loss_ce_0: 0.6541  loss_mask_0: 0.4513  loss_dice_0: 3.637  loss_ce_1: 0.4025  loss_mask_1: 0.4629  loss_dice_1: 3.53  loss_ce_2: 0.3881  loss_mask_2: 0.465  loss_dice_2: 3.513  loss_ce_3: 0.3903  loss_mask_3: 0.4664  loss_dice_3: 3.503  loss_ce_4: 0.3826  loss_mask_4: 0.4648  loss_dice_4: 3.488  loss_ce_5: 0.3886  loss_mask_5: 0.4653  loss_dice_5: 3.493  loss_ce_6: 0.3976  loss_mask_6: 0.4647  loss_dice_6: 3.489  loss_ce_7: 0.3798  loss_mask_7: 0.4676  loss_dice_7: 3.49  loss_ce_8: 0.3768  loss_mask_8: 0.4664  loss_dice_8: 3.498  time: 1.5339  data_time: 0.0996  lr: 9.1883e-06  max_mem: 21366M
[01/17 07:02:48] d2.utils.events INFO:  eta: 1 day, 10:59:34  iter: 8099  total_loss: 45.23  loss_ce: 0.3764  loss_mask: 0.4914  loss_dice: 3.585  loss_ce_0: 0.6068  loss_mask_0: 0.4727  loss_dice_0: 3.705  loss_ce_1: 0.3992  loss_mask_1: 0.495  loss_dice_1: 3.625  loss_ce_2: 0.4039  loss_mask_2: 0.4944  loss_dice_2: 3.596  loss_ce_3: 0.3885  loss_mask_3: 0.4912  loss_dice_3: 3.586  loss_ce_4: 0.3754  loss_mask_4: 0.4899  loss_dice_4: 3.598  loss_ce_5: 0.3641  loss_mask_5: 0.4924  loss_dice_5: 3.589  loss_ce_6: 0.3932  loss_mask_6: 0.4946  loss_dice_6: 3.593  loss_ce_7: 0.3724  loss_mask_7: 0.4969  loss_dice_7: 3.579  loss_ce_8: 0.3701  loss_mask_8: 0.4929  loss_dice_8: 3.588  time: 1.5339  data_time: 0.0992  lr: 9.1863e-06  max_mem: 21366M
[01/17 07:03:19] d2.utils.events INFO:  eta: 1 day, 11:00:18  iter: 8119  total_loss: 44.66  loss_ce: 0.3619  loss_mask: 0.4837  loss_dice: 3.565  loss_ce_0: 0.6075  loss_mask_0: 0.4695  loss_dice_0: 3.674  loss_ce_1: 0.3521  loss_mask_1: 0.4867  loss_dice_1: 3.587  loss_ce_2: 0.3625  loss_mask_2: 0.4867  loss_dice_2: 3.573  loss_ce_3: 0.3652  loss_mask_3: 0.4824  loss_dice_3: 3.564  loss_ce_4: 0.3658  loss_mask_4: 0.484  loss_dice_4: 3.564  loss_ce_5: 0.3619  loss_mask_5: 0.4831  loss_dice_5: 3.563  loss_ce_6: 0.3584  loss_mask_6: 0.4834  loss_dice_6: 3.56  loss_ce_7: 0.3566  loss_mask_7: 0.4818  loss_dice_7: 3.56  loss_ce_8: 0.3564  loss_mask_8: 0.4835  loss_dice_8: 3.556  time: 1.5339  data_time: 0.0962  lr: 9.1843e-06  max_mem: 21366M
[01/17 07:03:50] d2.utils.events INFO:  eta: 1 day, 10:59:43  iter: 8139  total_loss: 44.88  loss_ce: 0.3647  loss_mask: 0.4875  loss_dice: 3.547  loss_ce_0: 0.6437  loss_mask_0: 0.4659  loss_dice_0: 3.656  loss_ce_1: 0.3832  loss_mask_1: 0.4849  loss_dice_1: 3.581  loss_ce_2: 0.3862  loss_mask_2: 0.4874  loss_dice_2: 3.552  loss_ce_3: 0.3687  loss_mask_3: 0.4893  loss_dice_3: 3.535  loss_ce_4: 0.3669  loss_mask_4: 0.4878  loss_dice_4: 3.548  loss_ce_5: 0.3614  loss_mask_5: 0.4872  loss_dice_5: 3.548  loss_ce_6: 0.3574  loss_mask_6: 0.4869  loss_dice_6: 3.542  loss_ce_7: 0.3442  loss_mask_7: 0.4863  loss_dice_7: 3.547  loss_ce_8: 0.3757  loss_mask_8: 0.4877  loss_dice_8: 3.55  time: 1.5340  data_time: 0.1067  lr: 9.1823e-06  max_mem: 21366M
[01/17 07:04:21] d2.utils.events INFO:  eta: 1 day, 10:59:00  iter: 8159  total_loss: 44.01  loss_ce: 0.3624  loss_mask: 0.4903  loss_dice: 3.506  loss_ce_0: 0.6156  loss_mask_0: 0.4739  loss_dice_0: 3.634  loss_ce_1: 0.3626  loss_mask_1: 0.4967  loss_dice_1: 3.539  loss_ce_2: 0.3858  loss_mask_2: 0.4901  loss_dice_2: 3.523  loss_ce_3: 0.3736  loss_mask_3: 0.4883  loss_dice_3: 3.505  loss_ce_4: 0.3715  loss_mask_4: 0.4937  loss_dice_4: 3.504  loss_ce_5: 0.3594  loss_mask_5: 0.4919  loss_dice_5: 3.516  loss_ce_6: 0.3635  loss_mask_6: 0.4926  loss_dice_6: 3.504  loss_ce_7: 0.3477  loss_mask_7: 0.493  loss_dice_7: 3.512  loss_ce_8: 0.3719  loss_mask_8: 0.4926  loss_dice_8: 3.514  time: 1.5340  data_time: 0.0950  lr: 9.1803e-06  max_mem: 21366M
[01/17 07:04:52] d2.utils.events INFO:  eta: 1 day, 10:56:34  iter: 8179  total_loss: 44.03  loss_ce: 0.3842  loss_mask: 0.4856  loss_dice: 3.535  loss_ce_0: 0.6327  loss_mask_0: 0.4648  loss_dice_0: 3.647  loss_ce_1: 0.3876  loss_mask_1: 0.4843  loss_dice_1: 3.573  loss_ce_2: 0.3873  loss_mask_2: 0.4861  loss_dice_2: 3.544  loss_ce_3: 0.3915  loss_mask_3: 0.4851  loss_dice_3: 3.534  loss_ce_4: 0.3702  loss_mask_4: 0.4848  loss_dice_4: 3.533  loss_ce_5: 0.3777  loss_mask_5: 0.4874  loss_dice_5: 3.537  loss_ce_6: 0.3944  loss_mask_6: 0.4877  loss_dice_6: 3.525  loss_ce_7: 0.3813  loss_mask_7: 0.4853  loss_dice_7: 3.529  loss_ce_8: 0.3799  loss_mask_8: 0.4871  loss_dice_8: 3.529  time: 1.5340  data_time: 0.1116  lr: 9.1783e-06  max_mem: 21366M
[01/17 07:05:23] d2.utils.events INFO:  eta: 1 day, 10:57:29  iter: 8199  total_loss: 44.4  loss_ce: 0.3523  loss_mask: 0.4837  loss_dice: 3.533  loss_ce_0: 0.6235  loss_mask_0: 0.4576  loss_dice_0: 3.652  loss_ce_1: 0.3748  loss_mask_1: 0.4848  loss_dice_1: 3.564  loss_ce_2: 0.3664  loss_mask_2: 0.4829  loss_dice_2: 3.546  loss_ce_3: 0.349  loss_mask_3: 0.4823  loss_dice_3: 3.54  loss_ce_4: 0.3761  loss_mask_4: 0.4859  loss_dice_4: 3.54  loss_ce_5: 0.3573  loss_mask_5: 0.4848  loss_dice_5: 3.534  loss_ce_6: 0.3562  loss_mask_6: 0.4858  loss_dice_6: 3.537  loss_ce_7: 0.3664  loss_mask_7: 0.4894  loss_dice_7: 3.54  loss_ce_8: 0.3658  loss_mask_8: 0.4867  loss_dice_8: 3.535  time: 1.5340  data_time: 0.1011  lr: 9.1762e-06  max_mem: 21366M
[01/17 07:05:54] d2.utils.events INFO:  eta: 1 day, 10:56:40  iter: 8219  total_loss: 44.68  loss_ce: 0.3941  loss_mask: 0.4884  loss_dice: 3.54  loss_ce_0: 0.6272  loss_mask_0: 0.4643  loss_dice_0: 3.653  loss_ce_1: 0.4012  loss_mask_1: 0.4877  loss_dice_1: 3.57  loss_ce_2: 0.4004  loss_mask_2: 0.4893  loss_dice_2: 3.554  loss_ce_3: 0.3814  loss_mask_3: 0.4879  loss_dice_3: 3.546  loss_ce_4: 0.4015  loss_mask_4: 0.4891  loss_dice_4: 3.547  loss_ce_5: 0.3844  loss_mask_5: 0.4946  loss_dice_5: 3.55  loss_ce_6: 0.3773  loss_mask_6: 0.4913  loss_dice_6: 3.551  loss_ce_7: 0.3873  loss_mask_7: 0.4891  loss_dice_7: 3.538  loss_ce_8: 0.38  loss_mask_8: 0.4889  loss_dice_8: 3.545  time: 1.5340  data_time: 0.1069  lr: 9.1742e-06  max_mem: 21366M
[01/17 07:06:25] d2.utils.events INFO:  eta: 1 day, 10:56:49  iter: 8239  total_loss: 44.31  loss_ce: 0.3575  loss_mask: 0.4806  loss_dice: 3.519  loss_ce_0: 0.5978  loss_mask_0: 0.4608  loss_dice_0: 3.644  loss_ce_1: 0.3634  loss_mask_1: 0.48  loss_dice_1: 3.545  loss_ce_2: 0.3683  loss_mask_2: 0.4806  loss_dice_2: 3.527  loss_ce_3: 0.3599  loss_mask_3: 0.4805  loss_dice_3: 3.521  loss_ce_4: 0.3594  loss_mask_4: 0.4804  loss_dice_4: 3.524  loss_ce_5: 0.3643  loss_mask_5: 0.4776  loss_dice_5: 3.525  loss_ce_6: 0.3789  loss_mask_6: 0.4796  loss_dice_6: 3.518  loss_ce_7: 0.353  loss_mask_7: 0.4808  loss_dice_7: 3.521  loss_ce_8: 0.3488  loss_mask_8: 0.4805  loss_dice_8: 3.522  time: 1.5340  data_time: 0.1005  lr: 9.1722e-06  max_mem: 21366M
[01/17 07:06:56] d2.utils.events INFO:  eta: 1 day, 10:57:36  iter: 8259  total_loss: 44.43  loss_ce: 0.3715  loss_mask: 0.4862  loss_dice: 3.553  loss_ce_0: 0.6068  loss_mask_0: 0.4669  loss_dice_0: 3.691  loss_ce_1: 0.3658  loss_mask_1: 0.4889  loss_dice_1: 3.591  loss_ce_2: 0.3782  loss_mask_2: 0.488  loss_dice_2: 3.574  loss_ce_3: 0.3548  loss_mask_3: 0.4865  loss_dice_3: 3.56  loss_ce_4: 0.3501  loss_mask_4: 0.4876  loss_dice_4: 3.562  loss_ce_5: 0.3579  loss_mask_5: 0.487  loss_dice_5: 3.562  loss_ce_6: 0.3543  loss_mask_6: 0.4847  loss_dice_6: 3.552  loss_ce_7: 0.3487  loss_mask_7: 0.4868  loss_dice_7: 3.556  loss_ce_8: 0.3633  loss_mask_8: 0.487  loss_dice_8: 3.56  time: 1.5341  data_time: 0.1026  lr: 9.1702e-06  max_mem: 21366M
[01/17 07:07:27] d2.utils.events INFO:  eta: 1 day, 10:56:55  iter: 8279  total_loss: 44.52  loss_ce: 0.3572  loss_mask: 0.4895  loss_dice: 3.562  loss_ce_0: 0.6429  loss_mask_0: 0.4707  loss_dice_0: 3.675  loss_ce_1: 0.3955  loss_mask_1: 0.493  loss_dice_1: 3.585  loss_ce_2: 0.3842  loss_mask_2: 0.4937  loss_dice_2: 3.572  loss_ce_3: 0.3804  loss_mask_3: 0.4917  loss_dice_3: 3.558  loss_ce_4: 0.384  loss_mask_4: 0.4889  loss_dice_4: 3.564  loss_ce_5: 0.3783  loss_mask_5: 0.486  loss_dice_5: 3.573  loss_ce_6: 0.3786  loss_mask_6: 0.4862  loss_dice_6: 3.567  loss_ce_7: 0.3741  loss_mask_7: 0.4871  loss_dice_7: 3.57  loss_ce_8: 0.3611  loss_mask_8: 0.4876  loss_dice_8: 3.569  time: 1.5341  data_time: 0.0971  lr: 9.1682e-06  max_mem: 21366M
[01/17 07:07:58] d2.utils.events INFO:  eta: 1 day, 10:56:29  iter: 8299  total_loss: 44.65  loss_ce: 0.3593  loss_mask: 0.5013  loss_dice: 3.57  loss_ce_0: 0.6154  loss_mask_0: 0.4781  loss_dice_0: 3.688  loss_ce_1: 0.3585  loss_mask_1: 0.501  loss_dice_1: 3.603  loss_ce_2: 0.3489  loss_mask_2: 0.5038  loss_dice_2: 3.586  loss_ce_3: 0.3607  loss_mask_3: 0.5019  loss_dice_3: 3.57  loss_ce_4: 0.3457  loss_mask_4: 0.4968  loss_dice_4: 3.574  loss_ce_5: 0.3465  loss_mask_5: 0.5013  loss_dice_5: 3.575  loss_ce_6: 0.3533  loss_mask_6: 0.4996  loss_dice_6: 3.571  loss_ce_7: 0.3453  loss_mask_7: 0.4988  loss_dice_7: 3.574  loss_ce_8: 0.348  loss_mask_8: 0.5003  loss_dice_8: 3.567  time: 1.5342  data_time: 0.1024  lr: 9.1661e-06  max_mem: 21366M
[01/17 07:08:28] d2.utils.events INFO:  eta: 1 day, 10:55:59  iter: 8319  total_loss: 44.45  loss_ce: 0.3581  loss_mask: 0.4894  loss_dice: 3.54  loss_ce_0: 0.6219  loss_mask_0: 0.4775  loss_dice_0: 3.659  loss_ce_1: 0.3752  loss_mask_1: 0.4938  loss_dice_1: 3.578  loss_ce_2: 0.3814  loss_mask_2: 0.4905  loss_dice_2: 3.555  loss_ce_3: 0.3716  loss_mask_3: 0.4899  loss_dice_3: 3.545  loss_ce_4: 0.3559  loss_mask_4: 0.4879  loss_dice_4: 3.556  loss_ce_5: 0.358  loss_mask_5: 0.4888  loss_dice_5: 3.547  loss_ce_6: 0.3602  loss_mask_6: 0.4891  loss_dice_6: 3.542  loss_ce_7: 0.3667  loss_mask_7: 0.4902  loss_dice_7: 3.547  loss_ce_8: 0.358  loss_mask_8: 0.4883  loss_dice_8: 3.545  time: 1.5342  data_time: 0.0945  lr: 9.1641e-06  max_mem: 21366M
[01/17 07:08:59] d2.utils.events INFO:  eta: 1 day, 10:55:28  iter: 8339  total_loss: 44.81  loss_ce: 0.3828  loss_mask: 0.5009  loss_dice: 3.553  loss_ce_0: 0.6286  loss_mask_0: 0.4799  loss_dice_0: 3.678  loss_ce_1: 0.3991  loss_mask_1: 0.5058  loss_dice_1: 3.585  loss_ce_2: 0.4171  loss_mask_2: 0.506  loss_dice_2: 3.571  loss_ce_3: 0.373  loss_mask_3: 0.5029  loss_dice_3: 3.563  loss_ce_4: 0.3738  loss_mask_4: 0.5025  loss_dice_4: 3.551  loss_ce_5: 0.3837  loss_mask_5: 0.505  loss_dice_5: 3.554  loss_ce_6: 0.3816  loss_mask_6: 0.502  loss_dice_6: 3.547  loss_ce_7: 0.3761  loss_mask_7: 0.5007  loss_dice_7: 3.547  loss_ce_8: 0.3692  loss_mask_8: 0.5016  loss_dice_8: 3.559  time: 1.5341  data_time: 0.0975  lr: 9.1621e-06  max_mem: 21366M
[01/17 07:09:30] d2.utils.events INFO:  eta: 1 day, 10:55:13  iter: 8359  total_loss: 43.88  loss_ce: 0.3599  loss_mask: 0.4883  loss_dice: 3.476  loss_ce_0: 0.6214  loss_mask_0: 0.4686  loss_dice_0: 3.608  loss_ce_1: 0.3798  loss_mask_1: 0.4814  loss_dice_1: 3.517  loss_ce_2: 0.3797  loss_mask_2: 0.4831  loss_dice_2: 3.495  loss_ce_3: 0.3637  loss_mask_3: 0.4884  loss_dice_3: 3.477  loss_ce_4: 0.3587  loss_mask_4: 0.4903  loss_dice_4: 3.481  loss_ce_5: 0.3675  loss_mask_5: 0.4915  loss_dice_5: 3.495  loss_ce_6: 0.3607  loss_mask_6: 0.491  loss_dice_6: 3.476  loss_ce_7: 0.3548  loss_mask_7: 0.489  loss_dice_7: 3.479  loss_ce_8: 0.3822  loss_mask_8: 0.4877  loss_dice_8: 3.477  time: 1.5341  data_time: 0.1065  lr: 9.1601e-06  max_mem: 21366M
[01/17 07:10:01] d2.utils.events INFO:  eta: 1 day, 10:54:21  iter: 8379  total_loss: 44.54  loss_ce: 0.3689  loss_mask: 0.4666  loss_dice: 3.553  loss_ce_0: 0.6344  loss_mask_0: 0.453  loss_dice_0: 3.67  loss_ce_1: 0.3789  loss_mask_1: 0.475  loss_dice_1: 3.586  loss_ce_2: 0.3938  loss_mask_2: 0.4715  loss_dice_2: 3.558  loss_ce_3: 0.3795  loss_mask_3: 0.4701  loss_dice_3: 3.544  loss_ce_4: 0.3766  loss_mask_4: 0.4657  loss_dice_4: 3.552  loss_ce_5: 0.3653  loss_mask_5: 0.4635  loss_dice_5: 3.557  loss_ce_6: 0.3616  loss_mask_6: 0.4649  loss_dice_6: 3.549  loss_ce_7: 0.3727  loss_mask_7: 0.4672  loss_dice_7: 3.547  loss_ce_8: 0.3647  loss_mask_8: 0.4676  loss_dice_8: 3.55  time: 1.5341  data_time: 0.0967  lr: 9.1581e-06  max_mem: 21366M
[01/17 07:10:31] d2.utils.events INFO:  eta: 1 day, 10:53:07  iter: 8399  total_loss: 44.06  loss_ce: 0.3501  loss_mask: 0.4915  loss_dice: 3.534  loss_ce_0: 0.599  loss_mask_0: 0.4765  loss_dice_0: 3.674  loss_ce_1: 0.3579  loss_mask_1: 0.4964  loss_dice_1: 3.58  loss_ce_2: 0.3642  loss_mask_2: 0.4957  loss_dice_2: 3.56  loss_ce_3: 0.3584  loss_mask_3: 0.4935  loss_dice_3: 3.545  loss_ce_4: 0.3516  loss_mask_4: 0.4924  loss_dice_4: 3.546  loss_ce_5: 0.3487  loss_mask_5: 0.4932  loss_dice_5: 3.542  loss_ce_6: 0.3516  loss_mask_6: 0.4903  loss_dice_6: 3.552  loss_ce_7: 0.3635  loss_mask_7: 0.4891  loss_dice_7: 3.548  loss_ce_8: 0.3686  loss_mask_8: 0.4928  loss_dice_8: 3.542  time: 1.5341  data_time: 0.0934  lr: 9.156e-06  max_mem: 21366M
[01/17 07:11:02] d2.utils.events INFO:  eta: 1 day, 10:53:41  iter: 8419  total_loss: 43.18  loss_ce: 0.3415  loss_mask: 0.4815  loss_dice: 3.478  loss_ce_0: 0.5884  loss_mask_0: 0.4584  loss_dice_0: 3.582  loss_ce_1: 0.3493  loss_mask_1: 0.4791  loss_dice_1: 3.508  loss_ce_2: 0.3461  loss_mask_2: 0.4823  loss_dice_2: 3.488  loss_ce_3: 0.3365  loss_mask_3: 0.4821  loss_dice_3: 3.473  loss_ce_4: 0.3513  loss_mask_4: 0.4824  loss_dice_4: 3.48  loss_ce_5: 0.3401  loss_mask_5: 0.4816  loss_dice_5: 3.481  loss_ce_6: 0.3371  loss_mask_6: 0.4835  loss_dice_6: 3.468  loss_ce_7: 0.341  loss_mask_7: 0.4858  loss_dice_7: 3.468  loss_ce_8: 0.3187  loss_mask_8: 0.4826  loss_dice_8: 3.467  time: 1.5341  data_time: 0.0953  lr: 9.154e-06  max_mem: 21366M
[01/17 07:11:32] d2.utils.events INFO:  eta: 1 day, 10:53:10  iter: 8439  total_loss: 43.48  loss_ce: 0.3477  loss_mask: 0.4719  loss_dice: 3.48  loss_ce_0: 0.6373  loss_mask_0: 0.4439  loss_dice_0: 3.607  loss_ce_1: 0.3776  loss_mask_1: 0.4655  loss_dice_1: 3.527  loss_ce_2: 0.3679  loss_mask_2: 0.4691  loss_dice_2: 3.499  loss_ce_3: 0.3583  loss_mask_3: 0.4679  loss_dice_3: 3.491  loss_ce_4: 0.3483  loss_mask_4: 0.469  loss_dice_4: 3.481  loss_ce_5: 0.3415  loss_mask_5: 0.4674  loss_dice_5: 3.489  loss_ce_6: 0.3434  loss_mask_6: 0.4704  loss_dice_6: 3.479  loss_ce_7: 0.3443  loss_mask_7: 0.4714  loss_dice_7: 3.481  loss_ce_8: 0.3471  loss_mask_8: 0.4705  loss_dice_8: 3.481  time: 1.5341  data_time: 0.0966  lr: 9.152e-06  max_mem: 21366M
[01/17 07:12:03] d2.utils.events INFO:  eta: 1 day, 10:52:17  iter: 8459  total_loss: 44.02  loss_ce: 0.3554  loss_mask: 0.4993  loss_dice: 3.499  loss_ce_0: 0.6135  loss_mask_0: 0.4724  loss_dice_0: 3.628  loss_ce_1: 0.3886  loss_mask_1: 0.5011  loss_dice_1: 3.534  loss_ce_2: 0.3949  loss_mask_2: 0.5004  loss_dice_2: 3.51  loss_ce_3: 0.3659  loss_mask_3: 0.5016  loss_dice_3: 3.503  loss_ce_4: 0.3717  loss_mask_4: 0.5013  loss_dice_4: 3.505  loss_ce_5: 0.3643  loss_mask_5: 0.4987  loss_dice_5: 3.502  loss_ce_6: 0.3544  loss_mask_6: 0.4977  loss_dice_6: 3.5  loss_ce_7: 0.3629  loss_mask_7: 0.4985  loss_dice_7: 3.5  loss_ce_8: 0.3485  loss_mask_8: 0.4981  loss_dice_8: 3.504  time: 1.5340  data_time: 0.0988  lr: 9.15e-06  max_mem: 21366M
[01/17 07:12:33] d2.utils.events INFO:  eta: 1 day, 10:51:00  iter: 8479  total_loss: 43.68  loss_ce: 0.3545  loss_mask: 0.4862  loss_dice: 3.507  loss_ce_0: 0.6127  loss_mask_0: 0.4642  loss_dice_0: 3.625  loss_ce_1: 0.3742  loss_mask_1: 0.4905  loss_dice_1: 3.533  loss_ce_2: 0.3705  loss_mask_2: 0.4877  loss_dice_2: 3.514  loss_ce_3: 0.3505  loss_mask_3: 0.486  loss_dice_3: 3.511  loss_ce_4: 0.3489  loss_mask_4: 0.4885  loss_dice_4: 3.505  loss_ce_5: 0.3394  loss_mask_5: 0.4885  loss_dice_5: 3.505  loss_ce_6: 0.3398  loss_mask_6: 0.4887  loss_dice_6: 3.507  loss_ce_7: 0.3536  loss_mask_7: 0.4879  loss_dice_7: 3.515  loss_ce_8: 0.3469  loss_mask_8: 0.4856  loss_dice_8: 3.51  time: 1.5340  data_time: 0.0858  lr: 9.148e-06  max_mem: 21366M
[01/17 07:13:04] d2.utils.events INFO:  eta: 1 day, 10:50:19  iter: 8499  total_loss: 43.99  loss_ce: 0.3489  loss_mask: 0.4737  loss_dice: 3.537  loss_ce_0: 0.6285  loss_mask_0: 0.4602  loss_dice_0: 3.637  loss_ce_1: 0.3594  loss_mask_1: 0.4747  loss_dice_1: 3.555  loss_ce_2: 0.3602  loss_mask_2: 0.4758  loss_dice_2: 3.543  loss_ce_3: 0.3329  loss_mask_3: 0.4761  loss_dice_3: 3.527  loss_ce_4: 0.3497  loss_mask_4: 0.4766  loss_dice_4: 3.531  loss_ce_5: 0.3414  loss_mask_5: 0.4762  loss_dice_5: 3.527  loss_ce_6: 0.3352  loss_mask_6: 0.4771  loss_dice_6: 3.533  loss_ce_7: 0.3495  loss_mask_7: 0.4764  loss_dice_7: 3.532  loss_ce_8: 0.338  loss_mask_8: 0.4755  loss_dice_8: 3.523  time: 1.5340  data_time: 0.0946  lr: 9.1459e-06  max_mem: 21366M
[01/17 07:13:34] d2.utils.events INFO:  eta: 1 day, 10:48:58  iter: 8519  total_loss: 43.5  loss_ce: 0.3429  loss_mask: 0.4906  loss_dice: 3.494  loss_ce_0: 0.6281  loss_mask_0: 0.4672  loss_dice_0: 3.632  loss_ce_1: 0.3758  loss_mask_1: 0.4902  loss_dice_1: 3.535  loss_ce_2: 0.3537  loss_mask_2: 0.4902  loss_dice_2: 3.515  loss_ce_3: 0.3407  loss_mask_3: 0.4854  loss_dice_3: 3.502  loss_ce_4: 0.3249  loss_mask_4: 0.4878  loss_dice_4: 3.499  loss_ce_5: 0.3493  loss_mask_5: 0.4876  loss_dice_5: 3.505  loss_ce_6: 0.355  loss_mask_6: 0.4871  loss_dice_6: 3.499  loss_ce_7: 0.3418  loss_mask_7: 0.4867  loss_dice_7: 3.491  loss_ce_8: 0.3518  loss_mask_8: 0.4881  loss_dice_8: 3.494  time: 1.5339  data_time: 0.0918  lr: 9.1439e-06  max_mem: 21366M
[01/17 07:14:04] d2.utils.events INFO:  eta: 1 day, 10:47:47  iter: 8539  total_loss: 43.33  loss_ce: 0.3398  loss_mask: 0.4765  loss_dice: 3.472  loss_ce_0: 0.619  loss_mask_0: 0.4499  loss_dice_0: 3.591  loss_ce_1: 0.3546  loss_mask_1: 0.4737  loss_dice_1: 3.516  loss_ce_2: 0.3535  loss_mask_2: 0.4707  loss_dice_2: 3.497  loss_ce_3: 0.3303  loss_mask_3: 0.4734  loss_dice_3: 3.477  loss_ce_4: 0.3465  loss_mask_4: 0.4735  loss_dice_4: 3.478  loss_ce_5: 0.3322  loss_mask_5: 0.4745  loss_dice_5: 3.477  loss_ce_6: 0.3519  loss_mask_6: 0.4745  loss_dice_6: 3.467  loss_ce_7: 0.3334  loss_mask_7: 0.4747  loss_dice_7: 3.468  loss_ce_8: 0.3526  loss_mask_8: 0.475  loss_dice_8: 3.467  time: 1.5339  data_time: 0.1018  lr: 9.1419e-06  max_mem: 21366M
[01/17 07:14:35] d2.utils.events INFO:  eta: 1 day, 10:46:49  iter: 8559  total_loss: 42.9  loss_ce: 0.3235  loss_mask: 0.4863  loss_dice: 3.479  loss_ce_0: 0.5992  loss_mask_0: 0.4618  loss_dice_0: 3.614  loss_ce_1: 0.3601  loss_mask_1: 0.4828  loss_dice_1: 3.517  loss_ce_2: 0.3583  loss_mask_2: 0.4847  loss_dice_2: 3.502  loss_ce_3: 0.3341  loss_mask_3: 0.483  loss_dice_3: 3.489  loss_ce_4: 0.3407  loss_mask_4: 0.4824  loss_dice_4: 3.479  loss_ce_5: 0.3301  loss_mask_5: 0.4804  loss_dice_5: 3.49  loss_ce_6: 0.3278  loss_mask_6: 0.4826  loss_dice_6: 3.477  loss_ce_7: 0.3226  loss_mask_7: 0.4831  loss_dice_7: 3.478  loss_ce_8: 0.3293  loss_mask_8: 0.4847  loss_dice_8: 3.483  time: 1.5338  data_time: 0.0853  lr: 9.1399e-06  max_mem: 21366M
[01/17 07:15:06] d2.utils.events INFO:  eta: 1 day, 10:45:57  iter: 8579  total_loss: 43.6  loss_ce: 0.3297  loss_mask: 0.473  loss_dice: 3.47  loss_ce_0: 0.613  loss_mask_0: 0.4454  loss_dice_0: 3.612  loss_ce_1: 0.3749  loss_mask_1: 0.4664  loss_dice_1: 3.521  loss_ce_2: 0.3631  loss_mask_2: 0.4668  loss_dice_2: 3.499  loss_ce_3: 0.356  loss_mask_3: 0.4667  loss_dice_3: 3.483  loss_ce_4: 0.351  loss_mask_4: 0.4695  loss_dice_4: 3.483  loss_ce_5: 0.3388  loss_mask_5: 0.4679  loss_dice_5: 3.482  loss_ce_6: 0.3412  loss_mask_6: 0.4689  loss_dice_6: 3.48  loss_ce_7: 0.331  loss_mask_7: 0.4718  loss_dice_7: 3.475  loss_ce_8: 0.3389  loss_mask_8: 0.4709  loss_dice_8: 3.474  time: 1.5339  data_time: 0.0987  lr: 9.1379e-06  max_mem: 21366M
[01/17 07:15:36] d2.utils.events INFO:  eta: 1 day, 10:41:54  iter: 8599  total_loss: 43.85  loss_ce: 0.3624  loss_mask: 0.4807  loss_dice: 3.521  loss_ce_0: 0.6333  loss_mask_0: 0.4651  loss_dice_0: 3.652  loss_ce_1: 0.3752  loss_mask_1: 0.4815  loss_dice_1: 3.555  loss_ce_2: 0.3804  loss_mask_2: 0.4814  loss_dice_2: 3.532  loss_ce_3: 0.3655  loss_mask_3: 0.4816  loss_dice_3: 3.524  loss_ce_4: 0.3408  loss_mask_4: 0.4829  loss_dice_4: 3.529  loss_ce_5: 0.3524  loss_mask_5: 0.4842  loss_dice_5: 3.533  loss_ce_6: 0.3615  loss_mask_6: 0.4824  loss_dice_6: 3.531  loss_ce_7: 0.3757  loss_mask_7: 0.4807  loss_dice_7: 3.524  loss_ce_8: 0.3635  loss_mask_8: 0.4808  loss_dice_8: 3.521  time: 1.5339  data_time: 0.1042  lr: 9.1358e-06  max_mem: 21366M
[01/17 07:16:07] d2.utils.events INFO:  eta: 1 day, 10:41:24  iter: 8619  total_loss: 43.28  loss_ce: 0.3532  loss_mask: 0.4812  loss_dice: 3.442  loss_ce_0: 0.6149  loss_mask_0: 0.4438  loss_dice_0: 3.58  loss_ce_1: 0.3964  loss_mask_1: 0.4788  loss_dice_1: 3.487  loss_ce_2: 0.3675  loss_mask_2: 0.4803  loss_dice_2: 3.458  loss_ce_3: 0.3669  loss_mask_3: 0.4794  loss_dice_3: 3.449  loss_ce_4: 0.3556  loss_mask_4: 0.4819  loss_dice_4: 3.442  loss_ce_5: 0.3563  loss_mask_5: 0.4813  loss_dice_5: 3.452  loss_ce_6: 0.3516  loss_mask_6: 0.4813  loss_dice_6: 3.436  loss_ce_7: 0.3674  loss_mask_7: 0.4804  loss_dice_7: 3.446  loss_ce_8: 0.3596  loss_mask_8: 0.4793  loss_dice_8: 3.442  time: 1.5339  data_time: 0.0976  lr: 9.1338e-06  max_mem: 21366M
[01/17 07:16:38] d2.utils.events INFO:  eta: 1 day, 10:39:28  iter: 8639  total_loss: 43.84  loss_ce: 0.3291  loss_mask: 0.4902  loss_dice: 3.507  loss_ce_0: 0.6007  loss_mask_0: 0.4686  loss_dice_0: 3.617  loss_ce_1: 0.3652  loss_mask_1: 0.4901  loss_dice_1: 3.54  loss_ce_2: 0.3477  loss_mask_2: 0.4885  loss_dice_2: 3.519  loss_ce_3: 0.3448  loss_mask_3: 0.4861  loss_dice_3: 3.519  loss_ce_4: 0.3323  loss_mask_4: 0.4865  loss_dice_4: 3.511  loss_ce_5: 0.3324  loss_mask_5: 0.4897  loss_dice_5: 3.515  loss_ce_6: 0.3389  loss_mask_6: 0.4924  loss_dice_6: 3.509  loss_ce_7: 0.3337  loss_mask_7: 0.4932  loss_dice_7: 3.506  loss_ce_8: 0.3287  loss_mask_8: 0.4911  loss_dice_8: 3.506  time: 1.5339  data_time: 0.0969  lr: 9.1318e-06  max_mem: 21366M
[01/17 07:17:09] d2.utils.events INFO:  eta: 1 day, 10:38:57  iter: 8659  total_loss: 43.5  loss_ce: 0.347  loss_mask: 0.4762  loss_dice: 3.497  loss_ce_0: 0.6191  loss_mask_0: 0.4576  loss_dice_0: 3.627  loss_ce_1: 0.3659  loss_mask_1: 0.4758  loss_dice_1: 3.538  loss_ce_2: 0.3714  loss_mask_2: 0.4759  loss_dice_2: 3.509  loss_ce_3: 0.3589  loss_mask_3: 0.4757  loss_dice_3: 3.504  loss_ce_4: 0.3623  loss_mask_4: 0.4757  loss_dice_4: 3.507  loss_ce_5: 0.346  loss_mask_5: 0.4745  loss_dice_5: 3.506  loss_ce_6: 0.3468  loss_mask_6: 0.4744  loss_dice_6: 3.5  loss_ce_7: 0.3514  loss_mask_7: 0.4734  loss_dice_7: 3.49  loss_ce_8: 0.3548  loss_mask_8: 0.4735  loss_dice_8: 3.489  time: 1.5339  data_time: 0.0885  lr: 9.1298e-06  max_mem: 21366M
[01/17 07:17:40] d2.utils.events INFO:  eta: 1 day, 10:38:27  iter: 8679  total_loss: 43.52  loss_ce: 0.3418  loss_mask: 0.4863  loss_dice: 3.476  loss_ce_0: 0.6079  loss_mask_0: 0.4675  loss_dice_0: 3.613  loss_ce_1: 0.3747  loss_mask_1: 0.4902  loss_dice_1: 3.513  loss_ce_2: 0.3686  loss_mask_2: 0.4892  loss_dice_2: 3.49  loss_ce_3: 0.3409  loss_mask_3: 0.4862  loss_dice_3: 3.484  loss_ce_4: 0.3548  loss_mask_4: 0.4882  loss_dice_4: 3.484  loss_ce_5: 0.3538  loss_mask_5: 0.4855  loss_dice_5: 3.488  loss_ce_6: 0.35  loss_mask_6: 0.4865  loss_dice_6: 3.474  loss_ce_7: 0.3327  loss_mask_7: 0.4874  loss_dice_7: 3.478  loss_ce_8: 0.3355  loss_mask_8: 0.4868  loss_dice_8: 3.481  time: 1.5339  data_time: 0.1040  lr: 9.1278e-06  max_mem: 21366M
[01/17 07:18:10] d2.utils.events INFO:  eta: 1 day, 10:36:41  iter: 8699  total_loss: 43.5  loss_ce: 0.3327  loss_mask: 0.4767  loss_dice: 3.49  loss_ce_0: 0.5828  loss_mask_0: 0.4486  loss_dice_0: 3.612  loss_ce_1: 0.3412  loss_mask_1: 0.472  loss_dice_1: 3.512  loss_ce_2: 0.3364  loss_mask_2: 0.4708  loss_dice_2: 3.498  loss_ce_3: 0.3404  loss_mask_3: 0.4745  loss_dice_3: 3.484  loss_ce_4: 0.3531  loss_mask_4: 0.475  loss_dice_4: 3.479  loss_ce_5: 0.325  loss_mask_5: 0.4764  loss_dice_5: 3.49  loss_ce_6: 0.3291  loss_mask_6: 0.476  loss_dice_6: 3.477  loss_ce_7: 0.3491  loss_mask_7: 0.4775  loss_dice_7: 3.475  loss_ce_8: 0.3408  loss_mask_8: 0.4773  loss_dice_8: 3.483  time: 1.5339  data_time: 0.0981  lr: 9.1257e-06  max_mem: 21366M
[01/17 07:18:41] d2.utils.events INFO:  eta: 1 day, 10:36:10  iter: 8719  total_loss: 43.89  loss_ce: 0.3709  loss_mask: 0.4661  loss_dice: 3.508  loss_ce_0: 0.6321  loss_mask_0: 0.4541  loss_dice_0: 3.627  loss_ce_1: 0.3708  loss_mask_1: 0.4716  loss_dice_1: 3.543  loss_ce_2: 0.352  loss_mask_2: 0.4747  loss_dice_2: 3.52  loss_ce_3: 0.3527  loss_mask_3: 0.4712  loss_dice_3: 3.51  loss_ce_4: 0.3583  loss_mask_4: 0.4706  loss_dice_4: 3.513  loss_ce_5: 0.3585  loss_mask_5: 0.4688  loss_dice_5: 3.501  loss_ce_6: 0.3533  loss_mask_6: 0.4672  loss_dice_6: 3.499  loss_ce_7: 0.3692  loss_mask_7: 0.4631  loss_dice_7: 3.506  loss_ce_8: 0.3487  loss_mask_8: 0.4642  loss_dice_8: 3.506  time: 1.5339  data_time: 0.0997  lr: 9.1237e-06  max_mem: 21366M
[01/17 07:19:12] d2.utils.events INFO:  eta: 1 day, 10:36:06  iter: 8739  total_loss: 43.07  loss_ce: 0.3401  loss_mask: 0.4806  loss_dice: 3.467  loss_ce_0: 0.5945  loss_mask_0: 0.4566  loss_dice_0: 3.597  loss_ce_1: 0.3412  loss_mask_1: 0.4761  loss_dice_1: 3.505  loss_ce_2: 0.3352  loss_mask_2: 0.4744  loss_dice_2: 3.489  loss_ce_3: 0.3306  loss_mask_3: 0.4729  loss_dice_3: 3.477  loss_ce_4: 0.3374  loss_mask_4: 0.4753  loss_dice_4: 3.473  loss_ce_5: 0.3349  loss_mask_5: 0.4809  loss_dice_5: 3.47  loss_ce_6: 0.3275  loss_mask_6: 0.4825  loss_dice_6: 3.467  loss_ce_7: 0.3323  loss_mask_7: 0.4839  loss_dice_7: 3.465  loss_ce_8: 0.3372  loss_mask_8: 0.4817  loss_dice_8: 3.469  time: 1.5339  data_time: 0.1030  lr: 9.1217e-06  max_mem: 21366M
[01/17 07:19:42] d2.utils.events INFO:  eta: 1 day, 10:35:11  iter: 8759  total_loss: 43.85  loss_ce: 0.3874  loss_mask: 0.4628  loss_dice: 3.461  loss_ce_0: 0.6278  loss_mask_0: 0.4417  loss_dice_0: 3.598  loss_ce_1: 0.3718  loss_mask_1: 0.4591  loss_dice_1: 3.514  loss_ce_2: 0.3962  loss_mask_2: 0.4658  loss_dice_2: 3.484  loss_ce_3: 0.3981  loss_mask_3: 0.4669  loss_dice_3: 3.471  loss_ce_4: 0.381  loss_mask_4: 0.4627  loss_dice_4: 3.466  loss_ce_5: 0.3719  loss_mask_5: 0.4617  loss_dice_5: 3.477  loss_ce_6: 0.3808  loss_mask_6: 0.4608  loss_dice_6: 3.473  loss_ce_7: 0.3797  loss_mask_7: 0.4625  loss_dice_7: 3.463  loss_ce_8: 0.3685  loss_mask_8: 0.4623  loss_dice_8: 3.466  time: 1.5339  data_time: 0.0974  lr: 9.1197e-06  max_mem: 21366M
[01/17 07:20:13] d2.utils.events INFO:  eta: 1 day, 10:34:57  iter: 8779  total_loss: 44.25  loss_ce: 0.3918  loss_mask: 0.4901  loss_dice: 3.564  loss_ce_0: 0.6216  loss_mask_0: 0.4728  loss_dice_0: 3.655  loss_ce_1: 0.4067  loss_mask_1: 0.4963  loss_dice_1: 3.598  loss_ce_2: 0.3877  loss_mask_2: 0.4911  loss_dice_2: 3.576  loss_ce_3: 0.3858  loss_mask_3: 0.4878  loss_dice_3: 3.575  loss_ce_4: 0.3989  loss_mask_4: 0.4897  loss_dice_4: 3.569  loss_ce_5: 0.3752  loss_mask_5: 0.489  loss_dice_5: 3.571  loss_ce_6: 0.3866  loss_mask_6: 0.4867  loss_dice_6: 3.563  loss_ce_7: 0.3857  loss_mask_7: 0.487  loss_dice_7: 3.57  loss_ce_8: 0.3871  loss_mask_8: 0.4895  loss_dice_8: 3.553  time: 1.5339  data_time: 0.1009  lr: 9.1177e-06  max_mem: 21366M
[01/17 07:20:44] d2.utils.events INFO:  eta: 1 day, 10:35:14  iter: 8799  total_loss: 43.91  loss_ce: 0.3634  loss_mask: 0.4725  loss_dice: 3.5  loss_ce_0: 0.6184  loss_mask_0: 0.4642  loss_dice_0: 3.621  loss_ce_1: 0.3842  loss_mask_1: 0.4803  loss_dice_1: 3.535  loss_ce_2: 0.3662  loss_mask_2: 0.4744  loss_dice_2: 3.514  loss_ce_3: 0.3519  loss_mask_3: 0.4745  loss_dice_3: 3.496  loss_ce_4: 0.3682  loss_mask_4: 0.4748  loss_dice_4: 3.504  loss_ce_5: 0.3587  loss_mask_5: 0.4729  loss_dice_5: 3.51  loss_ce_6: 0.3581  loss_mask_6: 0.4764  loss_dice_6: 3.501  loss_ce_7: 0.3612  loss_mask_7: 0.4761  loss_dice_7: 3.5  loss_ce_8: 0.3447  loss_mask_8: 0.4724  loss_dice_8: 3.499  time: 1.5339  data_time: 0.0987  lr: 9.1156e-06  max_mem: 21366M
[01/17 07:21:16] d2.utils.events INFO:  eta: 1 day, 10:35:15  iter: 8819  total_loss: 43.47  loss_ce: 0.3436  loss_mask: 0.4516  loss_dice: 3.481  loss_ce_0: 0.6158  loss_mask_0: 0.4397  loss_dice_0: 3.602  loss_ce_1: 0.3754  loss_mask_1: 0.4527  loss_dice_1: 3.516  loss_ce_2: 0.357  loss_mask_2: 0.4533  loss_dice_2: 3.49  loss_ce_3: 0.3396  loss_mask_3: 0.4511  loss_dice_3: 3.477  loss_ce_4: 0.3488  loss_mask_4: 0.4511  loss_dice_4: 3.481  loss_ce_5: 0.3424  loss_mask_5: 0.4502  loss_dice_5: 3.488  loss_ce_6: 0.352  loss_mask_6: 0.4504  loss_dice_6: 3.482  loss_ce_7: 0.3364  loss_mask_7: 0.4506  loss_dice_7: 3.492  loss_ce_8: 0.3363  loss_mask_8: 0.452  loss_dice_8: 3.476  time: 1.5340  data_time: 0.1050  lr: 9.1136e-06  max_mem: 21366M
[01/17 07:21:47] d2.utils.events INFO:  eta: 1 day, 10:35:34  iter: 8839  total_loss: 42.87  loss_ce: 0.3353  loss_mask: 0.4749  loss_dice: 3.448  loss_ce_0: 0.6129  loss_mask_0: 0.4524  loss_dice_0: 3.581  loss_ce_1: 0.34  loss_mask_1: 0.4729  loss_dice_1: 3.505  loss_ce_2: 0.3427  loss_mask_2: 0.4747  loss_dice_2: 3.479  loss_ce_3: 0.3331  loss_mask_3: 0.4732  loss_dice_3: 3.46  loss_ce_4: 0.3194  loss_mask_4: 0.4731  loss_dice_4: 3.459  loss_ce_5: 0.3276  loss_mask_5: 0.4729  loss_dice_5: 3.467  loss_ce_6: 0.3208  loss_mask_6: 0.4744  loss_dice_6: 3.458  loss_ce_7: 0.326  loss_mask_7: 0.4741  loss_dice_7: 3.455  loss_ce_8: 0.3289  loss_mask_8: 0.4746  loss_dice_8: 3.461  time: 1.5340  data_time: 0.0985  lr: 9.1116e-06  max_mem: 21366M
[01/17 07:22:18] d2.utils.events INFO:  eta: 1 day, 10:35:15  iter: 8859  total_loss: 43.98  loss_ce: 0.3711  loss_mask: 0.4675  loss_dice: 3.525  loss_ce_0: 0.6413  loss_mask_0: 0.4508  loss_dice_0: 3.643  loss_ce_1: 0.3723  loss_mask_1: 0.4685  loss_dice_1: 3.567  loss_ce_2: 0.3752  loss_mask_2: 0.4661  loss_dice_2: 3.542  loss_ce_3: 0.3658  loss_mask_3: 0.4682  loss_dice_3: 3.526  loss_ce_4: 0.3601  loss_mask_4: 0.4642  loss_dice_4: 3.514  loss_ce_5: 0.3542  loss_mask_5: 0.4651  loss_dice_5: 3.527  loss_ce_6: 0.3631  loss_mask_6: 0.4661  loss_dice_6: 3.517  loss_ce_7: 0.3624  loss_mask_7: 0.4671  loss_dice_7: 3.515  loss_ce_8: 0.3647  loss_mask_8: 0.4676  loss_dice_8: 3.523  time: 1.5341  data_time: 0.1071  lr: 9.1096e-06  max_mem: 21366M
[01/17 07:22:51] d2.utils.events INFO:  eta: 1 day, 10:36:11  iter: 8879  total_loss: 43.56  loss_ce: 0.3386  loss_mask: 0.4664  loss_dice: 3.477  loss_ce_0: 0.6064  loss_mask_0: 0.4511  loss_dice_0: 3.595  loss_ce_1: 0.3526  loss_mask_1: 0.4731  loss_dice_1: 3.503  loss_ce_2: 0.3603  loss_mask_2: 0.4718  loss_dice_2: 3.484  loss_ce_3: 0.3427  loss_mask_3: 0.469  loss_dice_3: 3.477  loss_ce_4: 0.3487  loss_mask_4: 0.4688  loss_dice_4: 3.472  loss_ce_5: 0.3412  loss_mask_5: 0.468  loss_dice_5: 3.479  loss_ce_6: 0.3371  loss_mask_6: 0.4668  loss_dice_6: 3.478  loss_ce_7: 0.3425  loss_mask_7: 0.4674  loss_dice_7: 3.466  loss_ce_8: 0.3352  loss_mask_8: 0.4666  loss_dice_8: 3.476  time: 1.5343  data_time: 0.1040  lr: 9.1076e-06  max_mem: 21366M
[01/17 07:23:22] d2.utils.events INFO:  eta: 1 day, 10:36:30  iter: 8899  total_loss: 44.15  loss_ce: 0.3612  loss_mask: 0.4624  loss_dice: 3.506  loss_ce_0: 0.6142  loss_mask_0: 0.4532  loss_dice_0: 3.612  loss_ce_1: 0.3893  loss_mask_1: 0.4701  loss_dice_1: 3.526  loss_ce_2: 0.3882  loss_mask_2: 0.4688  loss_dice_2: 3.509  loss_ce_3: 0.3758  loss_mask_3: 0.4674  loss_dice_3: 3.51  loss_ce_4: 0.3782  loss_mask_4: 0.4661  loss_dice_4: 3.502  loss_ce_5: 0.3714  loss_mask_5: 0.4669  loss_dice_5: 3.502  loss_ce_6: 0.3692  loss_mask_6: 0.4628  loss_dice_6: 3.5  loss_ce_7: 0.3681  loss_mask_7: 0.463  loss_dice_7: 3.506  loss_ce_8: 0.3718  loss_mask_8: 0.4617  loss_dice_8: 3.503  time: 1.5343  data_time: 0.1025  lr: 9.1055e-06  max_mem: 21366M
[01/17 07:23:52] d2.utils.events INFO:  eta: 1 day, 10:34:08  iter: 8919  total_loss: 43.17  loss_ce: 0.3071  loss_mask: 0.4649  loss_dice: 3.463  loss_ce_0: 0.6059  loss_mask_0: 0.4468  loss_dice_0: 3.603  loss_ce_1: 0.3556  loss_mask_1: 0.4624  loss_dice_1: 3.507  loss_ce_2: 0.3532  loss_mask_2: 0.4589  loss_dice_2: 3.477  loss_ce_3: 0.334  loss_mask_3: 0.4623  loss_dice_3: 3.46  loss_ce_4: 0.3451  loss_mask_4: 0.4623  loss_dice_4: 3.467  loss_ce_5: 0.3222  loss_mask_5: 0.462  loss_dice_5: 3.477  loss_ce_6: 0.3239  loss_mask_6: 0.4631  loss_dice_6: 3.465  loss_ce_7: 0.3213  loss_mask_7: 0.4643  loss_dice_7: 3.46  loss_ce_8: 0.3315  loss_mask_8: 0.4636  loss_dice_8: 3.462  time: 1.5343  data_time: 0.0906  lr: 9.1035e-06  max_mem: 21366M
[01/17 07:24:23] d2.utils.events INFO:  eta: 1 day, 10:33:19  iter: 8939  total_loss: 43.01  loss_ce: 0.3342  loss_mask: 0.4863  loss_dice: 3.429  loss_ce_0: 0.6268  loss_mask_0: 0.4662  loss_dice_0: 3.565  loss_ce_1: 0.3714  loss_mask_1: 0.4873  loss_dice_1: 3.464  loss_ce_2: 0.3377  loss_mask_2: 0.4876  loss_dice_2: 3.445  loss_ce_3: 0.3351  loss_mask_3: 0.4874  loss_dice_3: 3.437  loss_ce_4: 0.3464  loss_mask_4: 0.4867  loss_dice_4: 3.431  loss_ce_5: 0.3359  loss_mask_5: 0.4892  loss_dice_5: 3.428  loss_ce_6: 0.3357  loss_mask_6: 0.4875  loss_dice_6: 3.428  loss_ce_7: 0.3228  loss_mask_7: 0.4873  loss_dice_7: 3.427  loss_ce_8: 0.3246  loss_mask_8: 0.4874  loss_dice_8: 3.433  time: 1.5343  data_time: 0.0833  lr: 9.1015e-06  max_mem: 21366M
[01/17 07:24:53] d2.utils.events INFO:  eta: 1 day, 10:32:30  iter: 8959  total_loss: 43.71  loss_ce: 0.3714  loss_mask: 0.4848  loss_dice: 3.472  loss_ce_0: 0.636  loss_mask_0: 0.4634  loss_dice_0: 3.586  loss_ce_1: 0.3871  loss_mask_1: 0.4846  loss_dice_1: 3.503  loss_ce_2: 0.3797  loss_mask_2: 0.479  loss_dice_2: 3.487  loss_ce_3: 0.4035  loss_mask_3: 0.4835  loss_dice_3: 3.465  loss_ce_4: 0.3881  loss_mask_4: 0.4831  loss_dice_4: 3.473  loss_ce_5: 0.3744  loss_mask_5: 0.4844  loss_dice_5: 3.477  loss_ce_6: 0.3889  loss_mask_6: 0.4835  loss_dice_6: 3.476  loss_ce_7: 0.3816  loss_mask_7: 0.4841  loss_dice_7: 3.472  loss_ce_8: 0.3724  loss_mask_8: 0.4828  loss_dice_8: 3.475  time: 1.5343  data_time: 0.0982  lr: 9.0995e-06  max_mem: 21366M
[01/17 07:25:25] d2.utils.events INFO:  eta: 1 day, 10:32:17  iter: 8979  total_loss: 43.46  loss_ce: 0.3707  loss_mask: 0.4671  loss_dice: 3.487  loss_ce_0: 0.5961  loss_mask_0: 0.4486  loss_dice_0: 3.616  loss_ce_1: 0.3585  loss_mask_1: 0.4696  loss_dice_1: 3.522  loss_ce_2: 0.3623  loss_mask_2: 0.4673  loss_dice_2: 3.502  loss_ce_3: 0.3641  loss_mask_3: 0.4665  loss_dice_3: 3.503  loss_ce_4: 0.3604  loss_mask_4: 0.4685  loss_dice_4: 3.497  loss_ce_5: 0.3471  loss_mask_5: 0.4697  loss_dice_5: 3.49  loss_ce_6: 0.3596  loss_mask_6: 0.4677  loss_dice_6: 3.495  loss_ce_7: 0.3449  loss_mask_7: 0.4655  loss_dice_7: 3.494  loss_ce_8: 0.3522  loss_mask_8: 0.4685  loss_dice_8: 3.489  time: 1.5343  data_time: 0.0924  lr: 9.0974e-06  max_mem: 21366M
[01/17 07:25:55] d2.utils.events INFO:  eta: 1 day, 10:31:35  iter: 8999  total_loss: 43.73  loss_ce: 0.3633  loss_mask: 0.4726  loss_dice: 3.447  loss_ce_0: 0.6023  loss_mask_0: 0.4547  loss_dice_0: 3.578  loss_ce_1: 0.3771  loss_mask_1: 0.47  loss_dice_1: 3.486  loss_ce_2: 0.3637  loss_mask_2: 0.4702  loss_dice_2: 3.471  loss_ce_3: 0.3758  loss_mask_3: 0.4689  loss_dice_3: 3.46  loss_ce_4: 0.3791  loss_mask_4: 0.4719  loss_dice_4: 3.452  loss_ce_5: 0.3538  loss_mask_5: 0.4731  loss_dice_5: 3.464  loss_ce_6: 0.368  loss_mask_6: 0.4714  loss_dice_6: 3.457  loss_ce_7: 0.3451  loss_mask_7: 0.4721  loss_dice_7: 3.458  loss_ce_8: 0.3724  loss_mask_8: 0.4721  loss_dice_8: 3.453  time: 1.5343  data_time: 0.1017  lr: 9.0954e-06  max_mem: 21366M
[01/17 07:26:26] d2.utils.events INFO:  eta: 1 day, 10:31:05  iter: 9019  total_loss: 43.31  loss_ce: 0.3202  loss_mask: 0.4801  loss_dice: 3.473  loss_ce_0: 0.603  loss_mask_0: 0.4635  loss_dice_0: 3.598  loss_ce_1: 0.3519  loss_mask_1: 0.4825  loss_dice_1: 3.505  loss_ce_2: 0.3257  loss_mask_2: 0.4826  loss_dice_2: 3.487  loss_ce_3: 0.331  loss_mask_3: 0.4788  loss_dice_3: 3.477  loss_ce_4: 0.3333  loss_mask_4: 0.4773  loss_dice_4: 3.473  loss_ce_5: 0.3216  loss_mask_5: 0.4772  loss_dice_5: 3.484  loss_ce_6: 0.3188  loss_mask_6: 0.4767  loss_dice_6: 3.474  loss_ce_7: 0.3225  loss_mask_7: 0.4775  loss_dice_7: 3.469  loss_ce_8: 0.3233  loss_mask_8: 0.4767  loss_dice_8: 3.472  time: 1.5343  data_time: 0.1096  lr: 9.0934e-06  max_mem: 21366M
[01/17 07:26:57] d2.utils.events INFO:  eta: 1 day, 10:30:45  iter: 9039  total_loss: 43.45  loss_ce: 0.3512  loss_mask: 0.4792  loss_dice: 3.45  loss_ce_0: 0.6091  loss_mask_0: 0.4624  loss_dice_0: 3.589  loss_ce_1: 0.3886  loss_mask_1: 0.4853  loss_dice_1: 3.493  loss_ce_2: 0.3709  loss_mask_2: 0.4843  loss_dice_2: 3.479  loss_ce_3: 0.3611  loss_mask_3: 0.4858  loss_dice_3: 3.459  loss_ce_4: 0.3602  loss_mask_4: 0.482  loss_dice_4: 3.464  loss_ce_5: 0.3542  loss_mask_5: 0.4834  loss_dice_5: 3.462  loss_ce_6: 0.3568  loss_mask_6: 0.4812  loss_dice_6: 3.453  loss_ce_7: 0.3497  loss_mask_7: 0.483  loss_dice_7: 3.46  loss_ce_8: 0.3589  loss_mask_8: 0.482  loss_dice_8: 3.452  time: 1.5343  data_time: 0.1051  lr: 9.0914e-06  max_mem: 21366M
[01/17 07:27:27] d2.utils.events INFO:  eta: 1 day, 10:30:08  iter: 9059  total_loss: 43.21  loss_ce: 0.358  loss_mask: 0.4863  loss_dice: 3.416  loss_ce_0: 0.5983  loss_mask_0: 0.461  loss_dice_0: 3.556  loss_ce_1: 0.3595  loss_mask_1: 0.4837  loss_dice_1: 3.459  loss_ce_2: 0.3721  loss_mask_2: 0.4828  loss_dice_2: 3.437  loss_ce_3: 0.363  loss_mask_3: 0.483  loss_dice_3: 3.42  loss_ce_4: 0.3621  loss_mask_4: 0.4863  loss_dice_4: 3.424  loss_ce_5: 0.3447  loss_mask_5: 0.4848  loss_dice_5: 3.424  loss_ce_6: 0.3416  loss_mask_6: 0.4859  loss_dice_6: 3.423  loss_ce_7: 0.3594  loss_mask_7: 0.4866  loss_dice_7: 3.415  loss_ce_8: 0.3474  loss_mask_8: 0.4865  loss_dice_8: 3.414  time: 1.5343  data_time: 0.0958  lr: 9.0894e-06  max_mem: 21366M
[01/17 07:27:59] d2.utils.events INFO:  eta: 1 day, 10:29:44  iter: 9079  total_loss: 43.87  loss_ce: 0.3667  loss_mask: 0.4952  loss_dice: 3.508  loss_ce_0: 0.619  loss_mask_0: 0.4665  loss_dice_0: 3.627  loss_ce_1: 0.3751  loss_mask_1: 0.4945  loss_dice_1: 3.541  loss_ce_2: 0.3818  loss_mask_2: 0.4921  loss_dice_2: 3.524  loss_ce_3: 0.3826  loss_mask_3: 0.4983  loss_dice_3: 3.509  loss_ce_4: 0.3773  loss_mask_4: 0.4969  loss_dice_4: 3.514  loss_ce_5: 0.3633  loss_mask_5: 0.4956  loss_dice_5: 3.517  loss_ce_6: 0.3628  loss_mask_6: 0.4951  loss_dice_6: 3.514  loss_ce_7: 0.3714  loss_mask_7: 0.4976  loss_dice_7: 3.514  loss_ce_8: 0.3565  loss_mask_8: 0.4968  loss_dice_8: 3.512  time: 1.5343  data_time: 0.1074  lr: 9.0873e-06  max_mem: 21366M
[01/17 07:28:29] d2.utils.events INFO:  eta: 1 day, 10:29:00  iter: 9099  total_loss: 44.44  loss_ce: 0.3593  loss_mask: 0.4859  loss_dice: 3.528  loss_ce_0: 0.623  loss_mask_0: 0.4703  loss_dice_0: 3.623  loss_ce_1: 0.3877  loss_mask_1: 0.4929  loss_dice_1: 3.551  loss_ce_2: 0.3974  loss_mask_2: 0.4918  loss_dice_2: 3.523  loss_ce_3: 0.3619  loss_mask_3: 0.4875  loss_dice_3: 3.523  loss_ce_4: 0.3723  loss_mask_4: 0.4837  loss_dice_4: 3.522  loss_ce_5: 0.3669  loss_mask_5: 0.4861  loss_dice_5: 3.523  loss_ce_6: 0.365  loss_mask_6: 0.4857  loss_dice_6: 3.525  loss_ce_7: 0.3608  loss_mask_7: 0.4857  loss_dice_7: 3.527  loss_ce_8: 0.3488  loss_mask_8: 0.4873  loss_dice_8: 3.524  time: 1.5343  data_time: 0.0934  lr: 9.0853e-06  max_mem: 21366M
[01/17 07:29:00] d2.utils.events INFO:  eta: 1 day, 10:28:29  iter: 9119  total_loss: 43.38  loss_ce: 0.3524  loss_mask: 0.469  loss_dice: 3.453  loss_ce_0: 0.6193  loss_mask_0: 0.4574  loss_dice_0: 3.559  loss_ce_1: 0.3761  loss_mask_1: 0.4743  loss_dice_1: 3.474  loss_ce_2: 0.3819  loss_mask_2: 0.4716  loss_dice_2: 3.46  loss_ce_3: 0.3659  loss_mask_3: 0.472  loss_dice_3: 3.451  loss_ce_4: 0.3692  loss_mask_4: 0.4705  loss_dice_4: 3.445  loss_ce_5: 0.3436  loss_mask_5: 0.4694  loss_dice_5: 3.453  loss_ce_6: 0.3574  loss_mask_6: 0.4694  loss_dice_6: 3.447  loss_ce_7: 0.3553  loss_mask_7: 0.4691  loss_dice_7: 3.447  loss_ce_8: 0.3579  loss_mask_8: 0.4701  loss_dice_8: 3.451  time: 1.5343  data_time: 0.0957  lr: 9.0833e-06  max_mem: 21366M
[01/17 07:29:31] d2.utils.events INFO:  eta: 1 day, 10:28:03  iter: 9139  total_loss: 42.69  loss_ce: 0.3494  loss_mask: 0.4731  loss_dice: 3.401  loss_ce_0: 0.6307  loss_mask_0: 0.4515  loss_dice_0: 3.541  loss_ce_1: 0.3866  loss_mask_1: 0.468  loss_dice_1: 3.449  loss_ce_2: 0.3881  loss_mask_2: 0.4731  loss_dice_2: 3.413  loss_ce_3: 0.3836  loss_mask_3: 0.4726  loss_dice_3: 3.401  loss_ce_4: 0.3759  loss_mask_4: 0.4709  loss_dice_4: 3.411  loss_ce_5: 0.3515  loss_mask_5: 0.4707  loss_dice_5: 3.408  loss_ce_6: 0.362  loss_mask_6: 0.4731  loss_dice_6: 3.401  loss_ce_7: 0.3569  loss_mask_7: 0.4733  loss_dice_7: 3.398  loss_ce_8: 0.3544  loss_mask_8: 0.472  loss_dice_8: 3.404  time: 1.5344  data_time: 0.1013  lr: 9.0813e-06  max_mem: 21366M
[01/17 07:30:02] d2.utils.events INFO:  eta: 1 day, 10:27:28  iter: 9159  total_loss: 43.69  loss_ce: 0.3435  loss_mask: 0.467  loss_dice: 3.472  loss_ce_0: 0.6257  loss_mask_0: 0.4534  loss_dice_0: 3.602  loss_ce_1: 0.3845  loss_mask_1: 0.4777  loss_dice_1: 3.503  loss_ce_2: 0.4051  loss_mask_2: 0.4749  loss_dice_2: 3.485  loss_ce_3: 0.3729  loss_mask_3: 0.4703  loss_dice_3: 3.471  loss_ce_4: 0.3436  loss_mask_4: 0.4721  loss_dice_4: 3.479  loss_ce_5: 0.3342  loss_mask_5: 0.4688  loss_dice_5: 3.484  loss_ce_6: 0.3626  loss_mask_6: 0.4703  loss_dice_6: 3.465  loss_ce_7: 0.3613  loss_mask_7: 0.4701  loss_dice_7: 3.47  loss_ce_8: 0.3413  loss_mask_8: 0.4697  loss_dice_8: 3.465  time: 1.5344  data_time: 0.0953  lr: 9.0793e-06  max_mem: 21366M
[01/17 07:30:33] d2.utils.events INFO:  eta: 1 day, 10:27:56  iter: 9179  total_loss: 43.56  loss_ce: 0.3489  loss_mask: 0.4563  loss_dice: 3.507  loss_ce_0: 0.5972  loss_mask_0: 0.4358  loss_dice_0: 3.624  loss_ce_1: 0.3573  loss_mask_1: 0.4516  loss_dice_1: 3.536  loss_ce_2: 0.3677  loss_mask_2: 0.4491  loss_dice_2: 3.514  loss_ce_3: 0.3671  loss_mask_3: 0.4499  loss_dice_3: 3.505  loss_ce_4: 0.3514  loss_mask_4: 0.4515  loss_dice_4: 3.505  loss_ce_5: 0.3499  loss_mask_5: 0.455  loss_dice_5: 3.504  loss_ce_6: 0.348  loss_mask_6: 0.4557  loss_dice_6: 3.501  loss_ce_7: 0.3667  loss_mask_7: 0.4551  loss_dice_7: 3.495  loss_ce_8: 0.3541  loss_mask_8: 0.4549  loss_dice_8: 3.501  time: 1.5345  data_time: 0.0874  lr: 9.0772e-06  max_mem: 21366M
[01/17 07:31:03] d2.utils.events INFO:  eta: 1 day, 10:26:21  iter: 9199  total_loss: 43.18  loss_ce: 0.3364  loss_mask: 0.4806  loss_dice: 3.461  loss_ce_0: 0.6109  loss_mask_0: 0.459  loss_dice_0: 3.6  loss_ce_1: 0.3453  loss_mask_1: 0.478  loss_dice_1: 3.505  loss_ce_2: 0.3523  loss_mask_2: 0.48  loss_dice_2: 3.483  loss_ce_3: 0.3596  loss_mask_3: 0.4784  loss_dice_3: 3.469  loss_ce_4: 0.3488  loss_mask_4: 0.4753  loss_dice_4: 3.482  loss_ce_5: 0.3355  loss_mask_5: 0.4795  loss_dice_5: 3.478  loss_ce_6: 0.3446  loss_mask_6: 0.4776  loss_dice_6: 3.461  loss_ce_7: 0.3315  loss_mask_7: 0.4788  loss_dice_7: 3.47  loss_ce_8: 0.3475  loss_mask_8: 0.4796  loss_dice_8: 3.468  time: 1.5344  data_time: 0.0794  lr: 9.0752e-06  max_mem: 21366M
[01/17 07:31:34] d2.utils.events INFO:  eta: 1 day, 10:25:26  iter: 9219  total_loss: 42.86  loss_ce: 0.3577  loss_mask: 0.4657  loss_dice: 3.43  loss_ce_0: 0.6207  loss_mask_0: 0.4514  loss_dice_0: 3.554  loss_ce_1: 0.3533  loss_mask_1: 0.4644  loss_dice_1: 3.463  loss_ce_2: 0.3644  loss_mask_2: 0.4636  loss_dice_2: 3.436  loss_ce_3: 0.3572  loss_mask_3: 0.465  loss_dice_3: 3.43  loss_ce_4: 0.3422  loss_mask_4: 0.4627  loss_dice_4: 3.439  loss_ce_5: 0.341  loss_mask_5: 0.4613  loss_dice_5: 3.439  loss_ce_6: 0.3457  loss_mask_6: 0.4598  loss_dice_6: 3.432  loss_ce_7: 0.3601  loss_mask_7: 0.4603  loss_dice_7: 3.434  loss_ce_8: 0.3538  loss_mask_8: 0.4625  loss_dice_8: 3.436  time: 1.5344  data_time: 0.1052  lr: 9.0732e-06  max_mem: 21366M
[01/17 07:32:05] d2.utils.events INFO:  eta: 1 day, 10:25:40  iter: 9239  total_loss: 43.76  loss_ce: 0.362  loss_mask: 0.4719  loss_dice: 3.522  loss_ce_0: 0.6152  loss_mask_0: 0.4588  loss_dice_0: 3.633  loss_ce_1: 0.3599  loss_mask_1: 0.4769  loss_dice_1: 3.563  loss_ce_2: 0.3726  loss_mask_2: 0.4759  loss_dice_2: 3.545  loss_ce_3: 0.3712  loss_mask_3: 0.4758  loss_dice_3: 3.534  loss_ce_4: 0.3542  loss_mask_4: 0.4761  loss_dice_4: 3.54  loss_ce_5: 0.3609  loss_mask_5: 0.4762  loss_dice_5: 3.534  loss_ce_6: 0.358  loss_mask_6: 0.4751  loss_dice_6: 3.532  loss_ce_7: 0.3559  loss_mask_7: 0.4747  loss_dice_7: 3.53  loss_ce_8: 0.3559  loss_mask_8: 0.4732  loss_dice_8: 3.525  time: 1.5344  data_time: 0.0931  lr: 9.0712e-06  max_mem: 21366M
[01/17 07:32:36] d2.utils.events INFO:  eta: 1 day, 10:23:57  iter: 9259  total_loss: 43.94  loss_ce: 0.3658  loss_mask: 0.4658  loss_dice: 3.496  loss_ce_0: 0.637  loss_mask_0: 0.447  loss_dice_0: 3.615  loss_ce_1: 0.3719  loss_mask_1: 0.4696  loss_dice_1: 3.53  loss_ce_2: 0.3733  loss_mask_2: 0.4676  loss_dice_2: 3.51  loss_ce_3: 0.3688  loss_mask_3: 0.4621  loss_dice_3: 3.503  loss_ce_4: 0.3505  loss_mask_4: 0.4625  loss_dice_4: 3.51  loss_ce_5: 0.3586  loss_mask_5: 0.4639  loss_dice_5: 3.51  loss_ce_6: 0.3489  loss_mask_6: 0.4635  loss_dice_6: 3.512  loss_ce_7: 0.3534  loss_mask_7: 0.4651  loss_dice_7: 3.495  loss_ce_8: 0.362  loss_mask_8: 0.4646  loss_dice_8: 3.505  time: 1.5344  data_time: 0.0957  lr: 9.0691e-06  max_mem: 21366M
[01/17 07:33:08] d2.utils.events INFO:  eta: 1 day, 10:24:56  iter: 9279  total_loss: 43.46  loss_ce: 0.3485  loss_mask: 0.4814  loss_dice: 3.513  loss_ce_0: 0.6014  loss_mask_0: 0.4702  loss_dice_0: 3.631  loss_ce_1: 0.3441  loss_mask_1: 0.4897  loss_dice_1: 3.553  loss_ce_2: 0.3417  loss_mask_2: 0.487  loss_dice_2: 3.52  loss_ce_3: 0.3443  loss_mask_3: 0.4838  loss_dice_3: 3.513  loss_ce_4: 0.3442  loss_mask_4: 0.4823  loss_dice_4: 3.513  loss_ce_5: 0.3295  loss_mask_5: 0.485  loss_dice_5: 3.515  loss_ce_6: 0.3412  loss_mask_6: 0.4808  loss_dice_6: 3.513  loss_ce_7: 0.3499  loss_mask_7: 0.4809  loss_dice_7: 3.517  loss_ce_8: 0.343  loss_mask_8: 0.4822  loss_dice_8: 3.515  time: 1.5345  data_time: 0.0925  lr: 9.0671e-06  max_mem: 21366M
[01/17 07:33:39] d2.utils.events INFO:  eta: 1 day, 10:24:42  iter: 9299  total_loss: 43.25  loss_ce: 0.3351  loss_mask: 0.4597  loss_dice: 3.469  loss_ce_0: 0.6388  loss_mask_0: 0.4422  loss_dice_0: 3.589  loss_ce_1: 0.3572  loss_mask_1: 0.4593  loss_dice_1: 3.507  loss_ce_2: 0.3679  loss_mask_2: 0.4577  loss_dice_2: 3.494  loss_ce_3: 0.3491  loss_mask_3: 0.4616  loss_dice_3: 3.483  loss_ce_4: 0.3372  loss_mask_4: 0.461  loss_dice_4: 3.483  loss_ce_5: 0.3322  loss_mask_5: 0.4607  loss_dice_5: 3.48  loss_ce_6: 0.3305  loss_mask_6: 0.4601  loss_dice_6: 3.475  loss_ce_7: 0.3281  loss_mask_7: 0.4613  loss_dice_7: 3.462  loss_ce_8: 0.3253  loss_mask_8: 0.4615  loss_dice_8: 3.477  time: 1.5345  data_time: 0.0896  lr: 9.0651e-06  max_mem: 21366M
[01/17 07:34:09] d2.utils.events INFO:  eta: 1 day, 10:24:30  iter: 9319  total_loss: 42.82  loss_ce: 0.3468  loss_mask: 0.4718  loss_dice: 3.379  loss_ce_0: 0.6204  loss_mask_0: 0.461  loss_dice_0: 3.5  loss_ce_1: 0.3822  loss_mask_1: 0.4801  loss_dice_1: 3.411  loss_ce_2: 0.3626  loss_mask_2: 0.4767  loss_dice_2: 3.397  loss_ce_3: 0.3685  loss_mask_3: 0.4734  loss_dice_3: 3.378  loss_ce_4: 0.3681  loss_mask_4: 0.472  loss_dice_4: 3.381  loss_ce_5: 0.3696  loss_mask_5: 0.4709  loss_dice_5: 3.38  loss_ce_6: 0.3533  loss_mask_6: 0.471  loss_dice_6: 3.376  loss_ce_7: 0.3601  loss_mask_7: 0.4698  loss_dice_7: 3.374  loss_ce_8: 0.3575  loss_mask_8: 0.4723  loss_dice_8: 3.367  time: 1.5345  data_time: 0.1054  lr: 9.0631e-06  max_mem: 21366M
[01/17 07:34:40] d2.utils.events INFO:  eta: 1 day, 10:24:03  iter: 9339  total_loss: 43.49  loss_ce: 0.3424  loss_mask: 0.4772  loss_dice: 3.477  loss_ce_0: 0.6277  loss_mask_0: 0.4626  loss_dice_0: 3.589  loss_ce_1: 0.3477  loss_mask_1: 0.481  loss_dice_1: 3.5  loss_ce_2: 0.3652  loss_mask_2: 0.4864  loss_dice_2: 3.491  loss_ce_3: 0.3502  loss_mask_3: 0.4844  loss_dice_3: 3.478  loss_ce_4: 0.3533  loss_mask_4: 0.4835  loss_dice_4: 3.476  loss_ce_5: 0.339  loss_mask_5: 0.4857  loss_dice_5: 3.482  loss_ce_6: 0.3217  loss_mask_6: 0.4828  loss_dice_6: 3.48  loss_ce_7: 0.3526  loss_mask_7: 0.4804  loss_dice_7: 3.477  loss_ce_8: 0.3481  loss_mask_8: 0.4774  loss_dice_8: 3.471  time: 1.5345  data_time: 0.0944  lr: 9.0611e-06  max_mem: 21366M
[01/17 07:35:11] d2.utils.events INFO:  eta: 1 day, 10:23:32  iter: 9359  total_loss: 42.77  loss_ce: 0.3462  loss_mask: 0.4673  loss_dice: 3.387  loss_ce_0: 0.6019  loss_mask_0: 0.453  loss_dice_0: 3.524  loss_ce_1: 0.3627  loss_mask_1: 0.4734  loss_dice_1: 3.424  loss_ce_2: 0.3613  loss_mask_2: 0.4706  loss_dice_2: 3.392  loss_ce_3: 0.3651  loss_mask_3: 0.4691  loss_dice_3: 3.392  loss_ce_4: 0.3663  loss_mask_4: 0.4694  loss_dice_4: 3.401  loss_ce_5: 0.3612  loss_mask_5: 0.4674  loss_dice_5: 3.396  loss_ce_6: 0.3516  loss_mask_6: 0.4672  loss_dice_6: 3.391  loss_ce_7: 0.359  loss_mask_7: 0.4676  loss_dice_7: 3.398  loss_ce_8: 0.3592  loss_mask_8: 0.4658  loss_dice_8: 3.398  time: 1.5345  data_time: 0.0976  lr: 9.059e-06  max_mem: 21366M
[01/17 07:35:42] d2.utils.events INFO:  eta: 1 day, 10:24:02  iter: 9379  total_loss: 43.37  loss_ce: 0.3407  loss_mask: 0.4679  loss_dice: 3.431  loss_ce_0: 0.6123  loss_mask_0: 0.4483  loss_dice_0: 3.563  loss_ce_1: 0.3727  loss_mask_1: 0.467  loss_dice_1: 3.474  loss_ce_2: 0.3668  loss_mask_2: 0.4637  loss_dice_2: 3.454  loss_ce_3: 0.3473  loss_mask_3: 0.4633  loss_dice_3: 3.438  loss_ce_4: 0.3727  loss_mask_4: 0.4641  loss_dice_4: 3.438  loss_ce_5: 0.3617  loss_mask_5: 0.4649  loss_dice_5: 3.434  loss_ce_6: 0.3472  loss_mask_6: 0.4643  loss_dice_6: 3.44  loss_ce_7: 0.3415  loss_mask_7: 0.4644  loss_dice_7: 3.428  loss_ce_8: 0.3486  loss_mask_8: 0.4656  loss_dice_8: 3.432  time: 1.5346  data_time: 0.0939  lr: 9.057e-06  max_mem: 21366M
[01/17 07:36:13] d2.utils.events INFO:  eta: 1 day, 10:23:42  iter: 9399  total_loss: 42.26  loss_ce: 0.3328  loss_mask: 0.4672  loss_dice: 3.382  loss_ce_0: 0.6143  loss_mask_0: 0.4451  loss_dice_0: 3.538  loss_ce_1: 0.3705  loss_mask_1: 0.4655  loss_dice_1: 3.43  loss_ce_2: 0.3557  loss_mask_2: 0.4669  loss_dice_2: 3.399  loss_ce_3: 0.3455  loss_mask_3: 0.4653  loss_dice_3: 3.382  loss_ce_4: 0.3448  loss_mask_4: 0.4658  loss_dice_4: 3.387  loss_ce_5: 0.3382  loss_mask_5: 0.4635  loss_dice_5: 3.392  loss_ce_6: 0.3473  loss_mask_6: 0.4684  loss_dice_6: 3.379  loss_ce_7: 0.3279  loss_mask_7: 0.4674  loss_dice_7: 3.382  loss_ce_8: 0.353  loss_mask_8: 0.4675  loss_dice_8: 3.384  time: 1.5346  data_time: 0.1063  lr: 9.055e-06  max_mem: 21366M
[01/17 07:36:44] d2.utils.events INFO:  eta: 1 day, 10:23:21  iter: 9419  total_loss: 42.78  loss_ce: 0.3327  loss_mask: 0.4706  loss_dice: 3.437  loss_ce_0: 0.6147  loss_mask_0: 0.4536  loss_dice_0: 3.575  loss_ce_1: 0.3542  loss_mask_1: 0.4699  loss_dice_1: 3.475  loss_ce_2: 0.3499  loss_mask_2: 0.4702  loss_dice_2: 3.454  loss_ce_3: 0.3404  loss_mask_3: 0.4704  loss_dice_3: 3.438  loss_ce_4: 0.3429  loss_mask_4: 0.4695  loss_dice_4: 3.445  loss_ce_5: 0.3407  loss_mask_5: 0.4701  loss_dice_5: 3.436  loss_ce_6: 0.3322  loss_mask_6: 0.4702  loss_dice_6: 3.428  loss_ce_7: 0.3353  loss_mask_7: 0.4738  loss_dice_7: 3.435  loss_ce_8: 0.3491  loss_mask_8: 0.4706  loss_dice_8: 3.435  time: 1.5346  data_time: 0.0925  lr: 9.053e-06  max_mem: 21366M
[01/17 07:37:14] d2.utils.events INFO:  eta: 1 day, 10:23:10  iter: 9439  total_loss: 43.4  loss_ce: 0.3588  loss_mask: 0.4672  loss_dice: 3.441  loss_ce_0: 0.6169  loss_mask_0: 0.4525  loss_dice_0: 3.571  loss_ce_1: 0.3776  loss_mask_1: 0.4715  loss_dice_1: 3.478  loss_ce_2: 0.3831  loss_mask_2: 0.4713  loss_dice_2: 3.465  loss_ce_3: 0.3566  loss_mask_3: 0.468  loss_dice_3: 3.451  loss_ce_4: 0.3725  loss_mask_4: 0.4653  loss_dice_4: 3.449  loss_ce_5: 0.3628  loss_mask_5: 0.4645  loss_dice_5: 3.449  loss_ce_6: 0.3589  loss_mask_6: 0.462  loss_dice_6: 3.45  loss_ce_7: 0.3586  loss_mask_7: 0.4642  loss_dice_7: 3.45  loss_ce_8: 0.3566  loss_mask_8: 0.4664  loss_dice_8: 3.444  time: 1.5346  data_time: 0.0824  lr: 9.0509e-06  max_mem: 21366M
[01/17 07:37:45] d2.utils.events INFO:  eta: 1 day, 10:22:29  iter: 9459  total_loss: 44.28  loss_ce: 0.3738  loss_mask: 0.4774  loss_dice: 3.539  loss_ce_0: 0.6599  loss_mask_0: 0.4594  loss_dice_0: 3.636  loss_ce_1: 0.3878  loss_mask_1: 0.4858  loss_dice_1: 3.559  loss_ce_2: 0.3918  loss_mask_2: 0.4859  loss_dice_2: 3.532  loss_ce_3: 0.3828  loss_mask_3: 0.4823  loss_dice_3: 3.527  loss_ce_4: 0.3689  loss_mask_4: 0.4805  loss_dice_4: 3.531  loss_ce_5: 0.3745  loss_mask_5: 0.4766  loss_dice_5: 3.529  loss_ce_6: 0.3729  loss_mask_6: 0.4797  loss_dice_6: 3.52  loss_ce_7: 0.3632  loss_mask_7: 0.4771  loss_dice_7: 3.525  loss_ce_8: 0.3684  loss_mask_8: 0.4785  loss_dice_8: 3.527  time: 1.5346  data_time: 0.1028  lr: 9.0489e-06  max_mem: 21366M
[01/17 07:38:16] d2.utils.events INFO:  eta: 1 day, 10:22:33  iter: 9479  total_loss: 43.97  loss_ce: 0.3525  loss_mask: 0.4687  loss_dice: 3.511  loss_ce_0: 0.6035  loss_mask_0: 0.4532  loss_dice_0: 3.626  loss_ce_1: 0.3741  loss_mask_1: 0.4719  loss_dice_1: 3.551  loss_ce_2: 0.3739  loss_mask_2: 0.4725  loss_dice_2: 3.531  loss_ce_3: 0.3565  loss_mask_3: 0.4706  loss_dice_3: 3.522  loss_ce_4: 0.3559  loss_mask_4: 0.4681  loss_dice_4: 3.516  loss_ce_5: 0.3581  loss_mask_5: 0.4687  loss_dice_5: 3.515  loss_ce_6: 0.3597  loss_mask_6: 0.4689  loss_dice_6: 3.511  loss_ce_7: 0.3567  loss_mask_7: 0.4652  loss_dice_7: 3.515  loss_ce_8: 0.3704  loss_mask_8: 0.4677  loss_dice_8: 3.515  time: 1.5346  data_time: 0.0884  lr: 9.0469e-06  max_mem: 21366M
[01/17 07:38:47] d2.utils.events INFO:  eta: 1 day, 10:21:08  iter: 9499  total_loss: 42.68  loss_ce: 0.3416  loss_mask: 0.4766  loss_dice: 3.425  loss_ce_0: 0.5903  loss_mask_0: 0.4581  loss_dice_0: 3.551  loss_ce_1: 0.3512  loss_mask_1: 0.4834  loss_dice_1: 3.454  loss_ce_2: 0.3461  loss_mask_2: 0.479  loss_dice_2: 3.438  loss_ce_3: 0.343  loss_mask_3: 0.4784  loss_dice_3: 3.432  loss_ce_4: 0.3344  loss_mask_4: 0.4757  loss_dice_4: 3.43  loss_ce_5: 0.3449  loss_mask_5: 0.4779  loss_dice_5: 3.423  loss_ce_6: 0.3324  loss_mask_6: 0.4753  loss_dice_6: 3.426  loss_ce_7: 0.3292  loss_mask_7: 0.4742  loss_dice_7: 3.431  loss_ce_8: 0.3405  loss_mask_8: 0.4741  loss_dice_8: 3.43  time: 1.5346  data_time: 0.0892  lr: 9.0449e-06  max_mem: 21366M
[01/17 07:39:17] d2.utils.events INFO:  eta: 1 day, 10:20:47  iter: 9519  total_loss: 43.49  loss_ce: 0.3623  loss_mask: 0.4832  loss_dice: 3.413  loss_ce_0: 0.6153  loss_mask_0: 0.4595  loss_dice_0: 3.532  loss_ce_1: 0.3712  loss_mask_1: 0.4779  loss_dice_1: 3.436  loss_ce_2: 0.356  loss_mask_2: 0.4766  loss_dice_2: 3.424  loss_ce_3: 0.3582  loss_mask_3: 0.4791  loss_dice_3: 3.409  loss_ce_4: 0.3652  loss_mask_4: 0.4784  loss_dice_4: 3.41  loss_ce_5: 0.344  loss_mask_5: 0.482  loss_dice_5: 3.419  loss_ce_6: 0.3523  loss_mask_6: 0.4831  loss_dice_6: 3.406  loss_ce_7: 0.3595  loss_mask_7: 0.4833  loss_dice_7: 3.412  loss_ce_8: 0.3583  loss_mask_8: 0.4817  loss_dice_8: 3.405  time: 1.5345  data_time: 0.0924  lr: 9.0429e-06  max_mem: 21366M
[01/17 07:39:48] d2.utils.events INFO:  eta: 1 day, 10:21:24  iter: 9539  total_loss: 43.73  loss_ce: 0.3698  loss_mask: 0.4745  loss_dice: 3.463  loss_ce_0: 0.6191  loss_mask_0: 0.4669  loss_dice_0: 3.581  loss_ce_1: 0.3859  loss_mask_1: 0.4786  loss_dice_1: 3.496  loss_ce_2: 0.3723  loss_mask_2: 0.4709  loss_dice_2: 3.473  loss_ce_3: 0.3793  loss_mask_3: 0.4728  loss_dice_3: 3.465  loss_ce_4: 0.3715  loss_mask_4: 0.4711  loss_dice_4: 3.465  loss_ce_5: 0.3543  loss_mask_5: 0.4673  loss_dice_5: 3.472  loss_ce_6: 0.3794  loss_mask_6: 0.4691  loss_dice_6: 3.467  loss_ce_7: 0.3656  loss_mask_7: 0.4712  loss_dice_7: 3.465  loss_ce_8: 0.3611  loss_mask_8: 0.4728  loss_dice_8: 3.465  time: 1.5345  data_time: 0.1052  lr: 9.0408e-06  max_mem: 21366M
[01/17 07:40:18] d2.utils.events INFO:  eta: 1 day, 10:22:27  iter: 9559  total_loss: 43.45  loss_ce: 0.3512  loss_mask: 0.474  loss_dice: 3.476  loss_ce_0: 0.6105  loss_mask_0: 0.4589  loss_dice_0: 3.596  loss_ce_1: 0.3524  loss_mask_1: 0.4749  loss_dice_1: 3.514  loss_ce_2: 0.362  loss_mask_2: 0.4754  loss_dice_2: 3.488  loss_ce_3: 0.3505  loss_mask_3: 0.4727  loss_dice_3: 3.485  loss_ce_4: 0.3667  loss_mask_4: 0.475  loss_dice_4: 3.472  loss_ce_5: 0.3596  loss_mask_5: 0.4746  loss_dice_5: 3.473  loss_ce_6: 0.3469  loss_mask_6: 0.4764  loss_dice_6: 3.464  loss_ce_7: 0.3739  loss_mask_7: 0.4746  loss_dice_7: 3.473  loss_ce_8: 0.3532  loss_mask_8: 0.4746  loss_dice_8: 3.474  time: 1.5345  data_time: 0.0963  lr: 9.0388e-06  max_mem: 21366M
[01/17 07:40:49] d2.utils.events INFO:  eta: 1 day, 10:19:35  iter: 9579  total_loss: 42.72  loss_ce: 0.357  loss_mask: 0.4628  loss_dice: 3.406  loss_ce_0: 0.6142  loss_mask_0: 0.449  loss_dice_0: 3.531  loss_ce_1: 0.3731  loss_mask_1: 0.468  loss_dice_1: 3.441  loss_ce_2: 0.3772  loss_mask_2: 0.465  loss_dice_2: 3.424  loss_ce_3: 0.3618  loss_mask_3: 0.463  loss_dice_3: 3.413  loss_ce_4: 0.3497  loss_mask_4: 0.4612  loss_dice_4: 3.416  loss_ce_5: 0.3603  loss_mask_5: 0.4644  loss_dice_5: 3.413  loss_ce_6: 0.36  loss_mask_6: 0.4645  loss_dice_6: 3.409  loss_ce_7: 0.3565  loss_mask_7: 0.466  loss_dice_7: 3.405  loss_ce_8: 0.3474  loss_mask_8: 0.4635  loss_dice_8: 3.41  time: 1.5345  data_time: 0.0934  lr: 9.0368e-06  max_mem: 21366M
[01/17 07:41:20] d2.utils.events INFO:  eta: 1 day, 10:21:34  iter: 9599  total_loss: 43.26  loss_ce: 0.3505  loss_mask: 0.4608  loss_dice: 3.48  loss_ce_0: 0.6247  loss_mask_0: 0.4406  loss_dice_0: 3.594  loss_ce_1: 0.3593  loss_mask_1: 0.4613  loss_dice_1: 3.512  loss_ce_2: 0.3522  loss_mask_2: 0.4595  loss_dice_2: 3.486  loss_ce_3: 0.3692  loss_mask_3: 0.4584  loss_dice_3: 3.48  loss_ce_4: 0.359  loss_mask_4: 0.458  loss_dice_4: 3.469  loss_ce_5: 0.3651  loss_mask_5: 0.4584  loss_dice_5: 3.485  loss_ce_6: 0.3585  loss_mask_6: 0.4607  loss_dice_6: 3.47  loss_ce_7: 0.3619  loss_mask_7: 0.4573  loss_dice_7: 3.475  loss_ce_8: 0.3636  loss_mask_8: 0.4594  loss_dice_8: 3.473  time: 1.5345  data_time: 0.0862  lr: 9.0348e-06  max_mem: 21366M
[01/17 07:41:50] d2.utils.events INFO:  eta: 1 day, 10:21:21  iter: 9619  total_loss: 42.95  loss_ce: 0.3677  loss_mask: 0.4743  loss_dice: 3.417  loss_ce_0: 0.6125  loss_mask_0: 0.4587  loss_dice_0: 3.535  loss_ce_1: 0.3655  loss_mask_1: 0.4794  loss_dice_1: 3.452  loss_ce_2: 0.3846  loss_mask_2: 0.4807  loss_dice_2: 3.431  loss_ce_3: 0.3634  loss_mask_3: 0.4748  loss_dice_3: 3.426  loss_ce_4: 0.3822  loss_mask_4: 0.4727  loss_dice_4: 3.425  loss_ce_5: 0.3858  loss_mask_5: 0.4737  loss_dice_5: 3.43  loss_ce_6: 0.3772  loss_mask_6: 0.4718  loss_dice_6: 3.423  loss_ce_7: 0.37  loss_mask_7: 0.4753  loss_dice_7: 3.414  loss_ce_8: 0.3672  loss_mask_8: 0.4727  loss_dice_8: 3.419  time: 1.5345  data_time: 0.0976  lr: 9.0327e-06  max_mem: 21366M
[01/17 07:42:21] d2.utils.events INFO:  eta: 1 day, 10:20:50  iter: 9639  total_loss: 42.79  loss_ce: 0.3319  loss_mask: 0.477  loss_dice: 3.41  loss_ce_0: 0.5948  loss_mask_0: 0.448  loss_dice_0: 3.522  loss_ce_1: 0.3547  loss_mask_1: 0.4685  loss_dice_1: 3.432  loss_ce_2: 0.3702  loss_mask_2: 0.4662  loss_dice_2: 3.417  loss_ce_3: 0.3543  loss_mask_3: 0.4692  loss_dice_3: 3.408  loss_ce_4: 0.3448  loss_mask_4: 0.4734  loss_dice_4: 3.414  loss_ce_5: 0.345  loss_mask_5: 0.4753  loss_dice_5: 3.41  loss_ce_6: 0.333  loss_mask_6: 0.4779  loss_dice_6: 3.411  loss_ce_7: 0.3255  loss_mask_7: 0.4746  loss_dice_7: 3.414  loss_ce_8: 0.3307  loss_mask_8: 0.4772  loss_dice_8: 3.411  time: 1.5345  data_time: 0.0892  lr: 9.0307e-06  max_mem: 21366M
[01/17 07:42:51] d2.utils.events INFO:  eta: 1 day, 10:19:07  iter: 9659  total_loss: 43.33  loss_ce: 0.3426  loss_mask: 0.4728  loss_dice: 3.47  loss_ce_0: 0.5772  loss_mask_0: 0.4567  loss_dice_0: 3.588  loss_ce_1: 0.3633  loss_mask_1: 0.4745  loss_dice_1: 3.508  loss_ce_2: 0.3669  loss_mask_2: 0.4705  loss_dice_2: 3.496  loss_ce_3: 0.3489  loss_mask_3: 0.4692  loss_dice_3: 3.487  loss_ce_4: 0.3465  loss_mask_4: 0.471  loss_dice_4: 3.49  loss_ce_5: 0.355  loss_mask_5: 0.4724  loss_dice_5: 3.489  loss_ce_6: 0.3471  loss_mask_6: 0.4729  loss_dice_6: 3.477  loss_ce_7: 0.3501  loss_mask_7: 0.4727  loss_dice_7: 3.47  loss_ce_8: 0.3476  loss_mask_8: 0.4728  loss_dice_8: 3.485  time: 1.5344  data_time: 0.0861  lr: 9.0287e-06  max_mem: 21366M
[01/17 07:43:22] d2.utils.events INFO:  eta: 1 day, 10:19:30  iter: 9679  total_loss: 43.2  loss_ce: 0.3498  loss_mask: 0.4628  loss_dice: 3.45  loss_ce_0: 0.6023  loss_mask_0: 0.4392  loss_dice_0: 3.571  loss_ce_1: 0.3686  loss_mask_1: 0.4569  loss_dice_1: 3.493  loss_ce_2: 0.3621  loss_mask_2: 0.4588  loss_dice_2: 3.472  loss_ce_3: 0.3523  loss_mask_3: 0.4596  loss_dice_3: 3.452  loss_ce_4: 0.353  loss_mask_4: 0.46  loss_dice_4: 3.452  loss_ce_5: 0.3599  loss_mask_5: 0.4576  loss_dice_5: 3.455  loss_ce_6: 0.3513  loss_mask_6: 0.4624  loss_dice_6: 3.443  loss_ce_7: 0.3414  loss_mask_7: 0.4597  loss_dice_7: 3.459  loss_ce_8: 0.3499  loss_mask_8: 0.4624  loss_dice_8: 3.448  time: 1.5345  data_time: 0.0967  lr: 9.0267e-06  max_mem: 21366M
[01/17 07:43:53] d2.utils.events INFO:  eta: 1 day, 10:17:32  iter: 9699  total_loss: 42.95  loss_ce: 0.3405  loss_mask: 0.4721  loss_dice: 3.429  loss_ce_0: 0.6008  loss_mask_0: 0.4498  loss_dice_0: 3.545  loss_ce_1: 0.3631  loss_mask_1: 0.4738  loss_dice_1: 3.466  loss_ce_2: 0.3593  loss_mask_2: 0.4744  loss_dice_2: 3.444  loss_ce_3: 0.374  loss_mask_3: 0.4745  loss_dice_3: 3.443  loss_ce_4: 0.348  loss_mask_4: 0.4716  loss_dice_4: 3.431  loss_ce_5: 0.3426  loss_mask_5: 0.4736  loss_dice_5: 3.433  loss_ce_6: 0.3549  loss_mask_6: 0.4736  loss_dice_6: 3.429  loss_ce_7: 0.3526  loss_mask_7: 0.4737  loss_dice_7: 3.436  loss_ce_8: 0.3479  loss_mask_8: 0.4735  loss_dice_8: 3.422  time: 1.5344  data_time: 0.0952  lr: 9.0247e-06  max_mem: 21366M
[01/17 07:44:24] d2.utils.events INFO:  eta: 1 day, 10:16:47  iter: 9719  total_loss: 43.07  loss_ce: 0.3579  loss_mask: 0.4692  loss_dice: 3.442  loss_ce_0: 0.6053  loss_mask_0: 0.4532  loss_dice_0: 3.565  loss_ce_1: 0.3628  loss_mask_1: 0.4725  loss_dice_1: 3.474  loss_ce_2: 0.3686  loss_mask_2: 0.4727  loss_dice_2: 3.452  loss_ce_3: 0.3661  loss_mask_3: 0.4688  loss_dice_3: 3.442  loss_ce_4: 0.3548  loss_mask_4: 0.4701  loss_dice_4: 3.45  loss_ce_5: 0.3493  loss_mask_5: 0.468  loss_dice_5: 3.451  loss_ce_6: 0.3625  loss_mask_6: 0.4701  loss_dice_6: 3.449  loss_ce_7: 0.3486  loss_mask_7: 0.4697  loss_dice_7: 3.446  loss_ce_8: 0.3689  loss_mask_8: 0.4703  loss_dice_8: 3.444  time: 1.5344  data_time: 0.0993  lr: 9.0226e-06  max_mem: 21366M
[01/17 07:44:54] d2.utils.events INFO:  eta: 1 day, 10:15:29  iter: 9739  total_loss: 43.81  loss_ce: 0.373  loss_mask: 0.4654  loss_dice: 3.464  loss_ce_0: 0.6045  loss_mask_0: 0.4512  loss_dice_0: 3.592  loss_ce_1: 0.3919  loss_mask_1: 0.4643  loss_dice_1: 3.504  loss_ce_2: 0.4129  loss_mask_2: 0.464  loss_dice_2: 3.481  loss_ce_3: 0.3871  loss_mask_3: 0.4656  loss_dice_3: 3.471  loss_ce_4: 0.3971  loss_mask_4: 0.4663  loss_dice_4: 3.472  loss_ce_5: 0.3807  loss_mask_5: 0.4674  loss_dice_5: 3.47  loss_ce_6: 0.3704  loss_mask_6: 0.4629  loss_dice_6: 3.469  loss_ce_7: 0.3801  loss_mask_7: 0.467  loss_dice_7: 3.47  loss_ce_8: 0.382  loss_mask_8: 0.4668  loss_dice_8: 3.47  time: 1.5344  data_time: 0.0923  lr: 9.0206e-06  max_mem: 21366M
[01/17 07:45:24] d2.utils.events INFO:  eta: 1 day, 10:15:46  iter: 9759  total_loss: 42.91  loss_ce: 0.3346  loss_mask: 0.4672  loss_dice: 3.446  loss_ce_0: 0.5942  loss_mask_0: 0.4483  loss_dice_0: 3.568  loss_ce_1: 0.3712  loss_mask_1: 0.4677  loss_dice_1: 3.472  loss_ce_2: 0.3468  loss_mask_2: 0.4637  loss_dice_2: 3.449  loss_ce_3: 0.3581  loss_mask_3: 0.4645  loss_dice_3: 3.446  loss_ce_4: 0.3404  loss_mask_4: 0.4652  loss_dice_4: 3.457  loss_ce_5: 0.3377  loss_mask_5: 0.4653  loss_dice_5: 3.445  loss_ce_6: 0.358  loss_mask_6: 0.4647  loss_dice_6: 3.45  loss_ce_7: 0.346  loss_mask_7: 0.4648  loss_dice_7: 3.443  loss_ce_8: 0.3541  loss_mask_8: 0.4656  loss_dice_8: 3.442  time: 1.5344  data_time: 0.1007  lr: 9.0186e-06  max_mem: 21366M
[01/17 07:45:55] d2.utils.events INFO:  eta: 1 day, 10:16:48  iter: 9779  total_loss: 43.59  loss_ce: 0.345  loss_mask: 0.476  loss_dice: 3.451  loss_ce_0: 0.6308  loss_mask_0: 0.4564  loss_dice_0: 3.552  loss_ce_1: 0.3775  loss_mask_1: 0.4801  loss_dice_1: 3.467  loss_ce_2: 0.3678  loss_mask_2: 0.4768  loss_dice_2: 3.455  loss_ce_3: 0.3806  loss_mask_3: 0.4758  loss_dice_3: 3.45  loss_ce_4: 0.3626  loss_mask_4: 0.4732  loss_dice_4: 3.457  loss_ce_5: 0.3558  loss_mask_5: 0.4765  loss_dice_5: 3.447  loss_ce_6: 0.3602  loss_mask_6: 0.4733  loss_dice_6: 3.453  loss_ce_7: 0.3568  loss_mask_7: 0.4755  loss_dice_7: 3.448  loss_ce_8: 0.3563  loss_mask_8: 0.476  loss_dice_8: 3.451  time: 1.5344  data_time: 0.0890  lr: 9.0166e-06  max_mem: 21366M
[01/17 07:46:26] d2.utils.events INFO:  eta: 1 day, 10:14:58  iter: 9799  total_loss: 42.58  loss_ce: 0.3536  loss_mask: 0.4814  loss_dice: 3.416  loss_ce_0: 0.6159  loss_mask_0: 0.4642  loss_dice_0: 3.536  loss_ce_1: 0.3626  loss_mask_1: 0.4786  loss_dice_1: 3.45  loss_ce_2: 0.3831  loss_mask_2: 0.4723  loss_dice_2: 3.431  loss_ce_3: 0.3611  loss_mask_3: 0.4774  loss_dice_3: 3.423  loss_ce_4: 0.3569  loss_mask_4: 0.479  loss_dice_4: 3.42  loss_ce_5: 0.3628  loss_mask_5: 0.4791  loss_dice_5: 3.428  loss_ce_6: 0.3576  loss_mask_6: 0.4802  loss_dice_6: 3.423  loss_ce_7: 0.3723  loss_mask_7: 0.48  loss_dice_7: 3.42  loss_ce_8: 0.3675  loss_mask_8: 0.4822  loss_dice_8: 3.418  time: 1.5343  data_time: 0.0965  lr: 9.0145e-06  max_mem: 21366M
[01/17 07:46:57] d2.utils.events INFO:  eta: 1 day, 10:13:49  iter: 9819  total_loss: 43.1  loss_ce: 0.3516  loss_mask: 0.4628  loss_dice: 3.445  loss_ce_0: 0.593  loss_mask_0: 0.4512  loss_dice_0: 3.57  loss_ce_1: 0.3529  loss_mask_1: 0.4674  loss_dice_1: 3.48  loss_ce_2: 0.3542  loss_mask_2: 0.4648  loss_dice_2: 3.457  loss_ce_3: 0.3419  loss_mask_3: 0.4626  loss_dice_3: 3.452  loss_ce_4: 0.3607  loss_mask_4: 0.4621  loss_dice_4: 3.455  loss_ce_5: 0.3652  loss_mask_5: 0.4619  loss_dice_5: 3.459  loss_ce_6: 0.3417  loss_mask_6: 0.4628  loss_dice_6: 3.452  loss_ce_7: 0.35  loss_mask_7: 0.4602  loss_dice_7: 3.458  loss_ce_8: 0.3523  loss_mask_8: 0.4611  loss_dice_8: 3.455  time: 1.5344  data_time: 0.0977  lr: 9.0125e-06  max_mem: 21366M
[01/17 07:47:28] d2.utils.events INFO:  eta: 1 day, 10:12:09  iter: 9839  total_loss: 42.24  loss_ce: 0.3549  loss_mask: 0.4639  loss_dice: 3.372  loss_ce_0: 0.6265  loss_mask_0: 0.4359  loss_dice_0: 3.497  loss_ce_1: 0.3443  loss_mask_1: 0.4579  loss_dice_1: 3.417  loss_ce_2: 0.3541  loss_mask_2: 0.4605  loss_dice_2: 3.389  loss_ce_3: 0.3379  loss_mask_3: 0.4607  loss_dice_3: 3.374  loss_ce_4: 0.3376  loss_mask_4: 0.4616  loss_dice_4: 3.378  loss_ce_5: 0.3322  loss_mask_5: 0.4613  loss_dice_5: 3.372  loss_ce_6: 0.3485  loss_mask_6: 0.4632  loss_dice_6: 3.368  loss_ce_7: 0.3484  loss_mask_7: 0.464  loss_dice_7: 3.373  loss_ce_8: 0.3355  loss_mask_8: 0.4625  loss_dice_8: 3.371  time: 1.5344  data_time: 0.1004  lr: 9.0105e-06  max_mem: 21366M
[01/17 07:47:59] d2.utils.events INFO:  eta: 1 day, 10:11:29  iter: 9859  total_loss: 42.49  loss_ce: 0.3415  loss_mask: 0.4682  loss_dice: 3.374  loss_ce_0: 0.6094  loss_mask_0: 0.4484  loss_dice_0: 3.487  loss_ce_1: 0.3514  loss_mask_1: 0.4697  loss_dice_1: 3.407  loss_ce_2: 0.3391  loss_mask_2: 0.4668  loss_dice_2: 3.382  loss_ce_3: 0.3498  loss_mask_3: 0.4678  loss_dice_3: 3.363  loss_ce_4: 0.3279  loss_mask_4: 0.4679  loss_dice_4: 3.367  loss_ce_5: 0.3297  loss_mask_5: 0.4702  loss_dice_5: 3.375  loss_ce_6: 0.3372  loss_mask_6: 0.469  loss_dice_6: 3.373  loss_ce_7: 0.3266  loss_mask_7: 0.469  loss_dice_7: 3.372  loss_ce_8: 0.3352  loss_mask_8: 0.4688  loss_dice_8: 3.37  time: 1.5344  data_time: 0.0921  lr: 9.0085e-06  max_mem: 21366M
[01/17 07:48:30] d2.utils.events INFO:  eta: 1 day, 10:10:12  iter: 9879  total_loss: 42.43  loss_ce: 0.3258  loss_mask: 0.4615  loss_dice: 3.398  loss_ce_0: 0.5902  loss_mask_0: 0.4437  loss_dice_0: 3.522  loss_ce_1: 0.3422  loss_mask_1: 0.4637  loss_dice_1: 3.43  loss_ce_2: 0.3355  loss_mask_2: 0.4635  loss_dice_2: 3.407  loss_ce_3: 0.3344  loss_mask_3: 0.4626  loss_dice_3: 3.403  loss_ce_4: 0.3256  loss_mask_4: 0.4595  loss_dice_4: 3.413  loss_ce_5: 0.3254  loss_mask_5: 0.459  loss_dice_5: 3.409  loss_ce_6: 0.3259  loss_mask_6: 0.4628  loss_dice_6: 3.398  loss_ce_7: 0.3119  loss_mask_7: 0.4612  loss_dice_7: 3.4  loss_ce_8: 0.3284  loss_mask_8: 0.4602  loss_dice_8: 3.402  time: 1.5344  data_time: 0.0904  lr: 9.0064e-06  max_mem: 21366M
[01/17 07:49:00] d2.utils.events INFO:  eta: 1 day, 10:09:39  iter: 9899  total_loss: 42.12  loss_ce: 0.3241  loss_mask: 0.4676  loss_dice: 3.399  loss_ce_0: 0.5991  loss_mask_0: 0.448  loss_dice_0: 3.511  loss_ce_1: 0.3596  loss_mask_1: 0.471  loss_dice_1: 3.435  loss_ce_2: 0.3481  loss_mask_2: 0.4642  loss_dice_2: 3.41  loss_ce_3: 0.3278  loss_mask_3: 0.4636  loss_dice_3: 3.399  loss_ce_4: 0.3194  loss_mask_4: 0.4672  loss_dice_4: 3.405  loss_ce_5: 0.3252  loss_mask_5: 0.4691  loss_dice_5: 3.394  loss_ce_6: 0.3195  loss_mask_6: 0.4695  loss_dice_6: 3.39  loss_ce_7: 0.2995  loss_mask_7: 0.4672  loss_dice_7: 3.395  loss_ce_8: 0.3152  loss_mask_8: 0.4689  loss_dice_8: 3.391  time: 1.5344  data_time: 0.0917  lr: 9.0044e-06  max_mem: 21366M
[01/17 07:49:31] d2.utils.events INFO:  eta: 1 day, 10:08:50  iter: 9919  total_loss: 42.13  loss_ce: 0.3282  loss_mask: 0.4598  loss_dice: 3.334  loss_ce_0: 0.5924  loss_mask_0: 0.4384  loss_dice_0: 3.456  loss_ce_1: 0.363  loss_mask_1: 0.4603  loss_dice_1: 3.376  loss_ce_2: 0.3755  loss_mask_2: 0.4612  loss_dice_2: 3.343  loss_ce_3: 0.3377  loss_mask_3: 0.4597  loss_dice_3: 3.344  loss_ce_4: 0.339  loss_mask_4: 0.4602  loss_dice_4: 3.345  loss_ce_5: 0.3414  loss_mask_5: 0.4614  loss_dice_5: 3.343  loss_ce_6: 0.3325  loss_mask_6: 0.4589  loss_dice_6: 3.334  loss_ce_7: 0.3324  loss_mask_7: 0.4597  loss_dice_7: 3.34  loss_ce_8: 0.3224  loss_mask_8: 0.4597  loss_dice_8: 3.33  time: 1.5344  data_time: 0.0986  lr: 9.0024e-06  max_mem: 21366M
[01/17 07:50:01] d2.utils.events INFO:  eta: 1 day, 10:08:29  iter: 9939  total_loss: 42.7  loss_ce: 0.334  loss_mask: 0.4507  loss_dice: 3.42  loss_ce_0: 0.608  loss_mask_0: 0.4358  loss_dice_0: 3.554  loss_ce_1: 0.3374  loss_mask_1: 0.4526  loss_dice_1: 3.461  loss_ce_2: 0.3423  loss_mask_2: 0.4526  loss_dice_2: 3.442  loss_ce_3: 0.3462  loss_mask_3: 0.4526  loss_dice_3: 3.428  loss_ce_4: 0.3268  loss_mask_4: 0.4506  loss_dice_4: 3.428  loss_ce_5: 0.3412  loss_mask_5: 0.4544  loss_dice_5: 3.433  loss_ce_6: 0.3392  loss_mask_6: 0.4506  loss_dice_6: 3.421  loss_ce_7: 0.3472  loss_mask_7: 0.4511  loss_dice_7: 3.423  loss_ce_8: 0.3298  loss_mask_8: 0.4536  loss_dice_8: 3.425  time: 1.5344  data_time: 0.0986  lr: 9.0004e-06  max_mem: 21366M
[01/17 07:50:32] d2.utils.events INFO:  eta: 1 day, 10:08:39  iter: 9959  total_loss: 42.02  loss_ce: 0.3171  loss_mask: 0.4545  loss_dice: 3.372  loss_ce_0: 0.5706  loss_mask_0: 0.431  loss_dice_0: 3.514  loss_ce_1: 0.328  loss_mask_1: 0.4539  loss_dice_1: 3.422  loss_ce_2: 0.3352  loss_mask_2: 0.4528  loss_dice_2: 3.402  loss_ce_3: 0.324  loss_mask_3: 0.4541  loss_dice_3: 3.39  loss_ce_4: 0.3264  loss_mask_4: 0.4542  loss_dice_4: 3.383  loss_ce_5: 0.3158  loss_mask_5: 0.4515  loss_dice_5: 3.385  loss_ce_6: 0.3188  loss_mask_6: 0.4512  loss_dice_6: 3.382  loss_ce_7: 0.3272  loss_mask_7: 0.453  loss_dice_7: 3.379  loss_ce_8: 0.3195  loss_mask_8: 0.4518  loss_dice_8: 3.386  time: 1.5344  data_time: 0.1060  lr: 8.9984e-06  max_mem: 21366M
[01/17 07:51:04] d2.utils.events INFO:  eta: 1 day, 10:08:08  iter: 9979  total_loss: 42.46  loss_ce: 0.3471  loss_mask: 0.4539  loss_dice: 3.393  loss_ce_0: 0.6021  loss_mask_0: 0.4308  loss_dice_0: 3.53  loss_ce_1: 0.367  loss_mask_1: 0.4533  loss_dice_1: 3.423  loss_ce_2: 0.3678  loss_mask_2: 0.4567  loss_dice_2: 3.406  loss_ce_3: 0.3617  loss_mask_3: 0.4535  loss_dice_3: 3.397  loss_ce_4: 0.3525  loss_mask_4: 0.4492  loss_dice_4: 3.395  loss_ce_5: 0.3508  loss_mask_5: 0.4536  loss_dice_5: 3.399  loss_ce_6: 0.3423  loss_mask_6: 0.4538  loss_dice_6: 3.396  loss_ce_7: 0.3432  loss_mask_7: 0.4529  loss_dice_7: 3.395  loss_ce_8: 0.343  loss_mask_8: 0.4533  loss_dice_8: 3.393  time: 1.5345  data_time: 0.1050  lr: 8.9963e-06  max_mem: 21366M
[01/17 07:51:34] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla/model_0009999.pth
[01/17 07:51:35] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 07:51:36] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 07:51:36] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 07:51:37] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 07:51:51] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0117 s/iter. Inference: 0.1823 s/iter. Eval: 0.2008 s/iter. Total: 0.3947 s/iter. ETA=0:07:07
[01/17 07:51:56] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0117 s/iter. Inference: 0.1724 s/iter. Eval: 0.2343 s/iter. Total: 0.4186 s/iter. ETA=0:07:27
[01/17 07:52:02] d2.evaluation.evaluator INFO: Inference done 35/1093. Dataloading: 0.0118 s/iter. Inference: 0.1811 s/iter. Eval: 0.2346 s/iter. Total: 0.4275 s/iter. ETA=0:07:32
[01/17 07:52:07] d2.evaluation.evaluator INFO: Inference done 48/1093. Dataloading: 0.0128 s/iter. Inference: 0.1762 s/iter. Eval: 0.2309 s/iter. Total: 0.4200 s/iter. ETA=0:07:18
[01/17 07:52:12] d2.evaluation.evaluator INFO: Inference done 59/1093. Dataloading: 0.0131 s/iter. Inference: 0.1775 s/iter. Eval: 0.2375 s/iter. Total: 0.4282 s/iter. ETA=0:07:22
[01/17 07:52:17] d2.evaluation.evaluator INFO: Inference done 72/1093. Dataloading: 0.0129 s/iter. Inference: 0.1753 s/iter. Eval: 0.2353 s/iter. Total: 0.4237 s/iter. ETA=0:07:12
[01/17 07:52:22] d2.evaluation.evaluator INFO: Inference done 84/1093. Dataloading: 0.0132 s/iter. Inference: 0.1755 s/iter. Eval: 0.2365 s/iter. Total: 0.4253 s/iter. ETA=0:07:09
[01/17 07:52:28] d2.evaluation.evaluator INFO: Inference done 96/1093. Dataloading: 0.0135 s/iter. Inference: 0.1739 s/iter. Eval: 0.2378 s/iter. Total: 0.4253 s/iter. ETA=0:07:03
[01/17 07:52:33] d2.evaluation.evaluator INFO: Inference done 109/1093. Dataloading: 0.0135 s/iter. Inference: 0.1750 s/iter. Eval: 0.2338 s/iter. Total: 0.4224 s/iter. ETA=0:06:55
[01/17 07:52:38] d2.evaluation.evaluator INFO: Inference done 122/1093. Dataloading: 0.0134 s/iter. Inference: 0.1741 s/iter. Eval: 0.2312 s/iter. Total: 0.4189 s/iter. ETA=0:06:46
[01/17 07:52:43] d2.evaluation.evaluator INFO: Inference done 134/1093. Dataloading: 0.0133 s/iter. Inference: 0.1757 s/iter. Eval: 0.2320 s/iter. Total: 0.4211 s/iter. ETA=0:06:43
[01/17 07:52:48] d2.evaluation.evaluator INFO: Inference done 149/1093. Dataloading: 0.0129 s/iter. Inference: 0.1755 s/iter. Eval: 0.2253 s/iter. Total: 0.4139 s/iter. ETA=0:06:30
[01/17 07:52:54] d2.evaluation.evaluator INFO: Inference done 162/1093. Dataloading: 0.0130 s/iter. Inference: 0.1763 s/iter. Eval: 0.2234 s/iter. Total: 0.4128 s/iter. ETA=0:06:24
[01/17 07:52:59] d2.evaluation.evaluator INFO: Inference done 173/1093. Dataloading: 0.0130 s/iter. Inference: 0.1760 s/iter. Eval: 0.2267 s/iter. Total: 0.4157 s/iter. ETA=0:06:22
[01/17 07:53:04] d2.evaluation.evaluator INFO: Inference done 186/1093. Dataloading: 0.0128 s/iter. Inference: 0.1779 s/iter. Eval: 0.2238 s/iter. Total: 0.4147 s/iter. ETA=0:06:16
[01/17 07:53:09] d2.evaluation.evaluator INFO: Inference done 200/1093. Dataloading: 0.0127 s/iter. Inference: 0.1792 s/iter. Eval: 0.2209 s/iter. Total: 0.4130 s/iter. ETA=0:06:08
[01/17 07:53:14] d2.evaluation.evaluator INFO: Inference done 212/1093. Dataloading: 0.0129 s/iter. Inference: 0.1786 s/iter. Eval: 0.2218 s/iter. Total: 0.4135 s/iter. ETA=0:06:04
[01/17 07:53:20] d2.evaluation.evaluator INFO: Inference done 225/1093. Dataloading: 0.0128 s/iter. Inference: 0.1787 s/iter. Eval: 0.2208 s/iter. Total: 0.4124 s/iter. ETA=0:05:58
[01/17 07:53:25] d2.evaluation.evaluator INFO: Inference done 238/1093. Dataloading: 0.0128 s/iter. Inference: 0.1784 s/iter. Eval: 0.2201 s/iter. Total: 0.4114 s/iter. ETA=0:05:51
[01/17 07:53:30] d2.evaluation.evaluator INFO: Inference done 251/1093. Dataloading: 0.0130 s/iter. Inference: 0.1780 s/iter. Eval: 0.2195 s/iter. Total: 0.4106 s/iter. ETA=0:05:45
[01/17 07:53:35] d2.evaluation.evaluator INFO: Inference done 264/1093. Dataloading: 0.0130 s/iter. Inference: 0.1775 s/iter. Eval: 0.2200 s/iter. Total: 0.4106 s/iter. ETA=0:05:40
[01/17 07:53:40] d2.evaluation.evaluator INFO: Inference done 277/1093. Dataloading: 0.0129 s/iter. Inference: 0.1769 s/iter. Eval: 0.2202 s/iter. Total: 0.4101 s/iter. ETA=0:05:34
[01/17 07:53:46] d2.evaluation.evaluator INFO: Inference done 291/1093. Dataloading: 0.0129 s/iter. Inference: 0.1761 s/iter. Eval: 0.2199 s/iter. Total: 0.4090 s/iter. ETA=0:05:28
[01/17 07:53:51] d2.evaluation.evaluator INFO: Inference done 303/1093. Dataloading: 0.0129 s/iter. Inference: 0.1763 s/iter. Eval: 0.2204 s/iter. Total: 0.4098 s/iter. ETA=0:05:23
[01/17 07:53:56] d2.evaluation.evaluator INFO: Inference done 316/1093. Dataloading: 0.0130 s/iter. Inference: 0.1754 s/iter. Eval: 0.2210 s/iter. Total: 0.4096 s/iter. ETA=0:05:18
[01/17 07:54:01] d2.evaluation.evaluator INFO: Inference done 329/1093. Dataloading: 0.0130 s/iter. Inference: 0.1749 s/iter. Eval: 0.2206 s/iter. Total: 0.4086 s/iter. ETA=0:05:12
[01/17 07:54:06] d2.evaluation.evaluator INFO: Inference done 343/1093. Dataloading: 0.0129 s/iter. Inference: 0.1755 s/iter. Eval: 0.2183 s/iter. Total: 0.4068 s/iter. ETA=0:05:05
[01/17 07:54:12] d2.evaluation.evaluator INFO: Inference done 358/1093. Dataloading: 0.0128 s/iter. Inference: 0.1748 s/iter. Eval: 0.2167 s/iter. Total: 0.4045 s/iter. ETA=0:04:57
[01/17 07:54:17] d2.evaluation.evaluator INFO: Inference done 371/1093. Dataloading: 0.0128 s/iter. Inference: 0.1743 s/iter. Eval: 0.2168 s/iter. Total: 0.4041 s/iter. ETA=0:04:51
[01/17 07:54:22] d2.evaluation.evaluator INFO: Inference done 385/1093. Dataloading: 0.0127 s/iter. Inference: 0.1742 s/iter. Eval: 0.2161 s/iter. Total: 0.4031 s/iter. ETA=0:04:45
[01/17 07:54:27] d2.evaluation.evaluator INFO: Inference done 396/1093. Dataloading: 0.0128 s/iter. Inference: 0.1753 s/iter. Eval: 0.2172 s/iter. Total: 0.4053 s/iter. ETA=0:04:42
[01/17 07:54:33] d2.evaluation.evaluator INFO: Inference done 409/1093. Dataloading: 0.0128 s/iter. Inference: 0.1755 s/iter. Eval: 0.2171 s/iter. Total: 0.4055 s/iter. ETA=0:04:37
[01/17 07:54:38] d2.evaluation.evaluator INFO: Inference done 421/1093. Dataloading: 0.0129 s/iter. Inference: 0.1751 s/iter. Eval: 0.2183 s/iter. Total: 0.4063 s/iter. ETA=0:04:33
[01/17 07:54:43] d2.evaluation.evaluator INFO: Inference done 434/1093. Dataloading: 0.0130 s/iter. Inference: 0.1755 s/iter. Eval: 0.2179 s/iter. Total: 0.4065 s/iter. ETA=0:04:27
[01/17 07:54:48] d2.evaluation.evaluator INFO: Inference done 447/1093. Dataloading: 0.0129 s/iter. Inference: 0.1755 s/iter. Eval: 0.2173 s/iter. Total: 0.4059 s/iter. ETA=0:04:22
[01/17 07:54:53] d2.evaluation.evaluator INFO: Inference done 459/1093. Dataloading: 0.0130 s/iter. Inference: 0.1756 s/iter. Eval: 0.2177 s/iter. Total: 0.4063 s/iter. ETA=0:04:17
[01/17 07:54:58] d2.evaluation.evaluator INFO: Inference done 473/1093. Dataloading: 0.0129 s/iter. Inference: 0.1755 s/iter. Eval: 0.2167 s/iter. Total: 0.4052 s/iter. ETA=0:04:11
[01/17 07:55:04] d2.evaluation.evaluator INFO: Inference done 486/1093. Dataloading: 0.0128 s/iter. Inference: 0.1756 s/iter. Eval: 0.2162 s/iter. Total: 0.4048 s/iter. ETA=0:04:05
[01/17 07:55:09] d2.evaluation.evaluator INFO: Inference done 500/1093. Dataloading: 0.0128 s/iter. Inference: 0.1757 s/iter. Eval: 0.2154 s/iter. Total: 0.4040 s/iter. ETA=0:03:59
[01/17 07:55:14] d2.evaluation.evaluator INFO: Inference done 515/1093. Dataloading: 0.0128 s/iter. Inference: 0.1754 s/iter. Eval: 0.2146 s/iter. Total: 0.4029 s/iter. ETA=0:03:52
[01/17 07:55:20] d2.evaluation.evaluator INFO: Inference done 527/1093. Dataloading: 0.0129 s/iter. Inference: 0.1757 s/iter. Eval: 0.2152 s/iter. Total: 0.4040 s/iter. ETA=0:03:48
[01/17 07:55:25] d2.evaluation.evaluator INFO: Inference done 540/1093. Dataloading: 0.0129 s/iter. Inference: 0.1759 s/iter. Eval: 0.2152 s/iter. Total: 0.4041 s/iter. ETA=0:03:43
[01/17 07:55:30] d2.evaluation.evaluator INFO: Inference done 552/1093. Dataloading: 0.0129 s/iter. Inference: 0.1757 s/iter. Eval: 0.2161 s/iter. Total: 0.4048 s/iter. ETA=0:03:39
[01/17 07:55:35] d2.evaluation.evaluator INFO: Inference done 565/1093. Dataloading: 0.0129 s/iter. Inference: 0.1759 s/iter. Eval: 0.2154 s/iter. Total: 0.4044 s/iter. ETA=0:03:33
[01/17 07:55:40] d2.evaluation.evaluator INFO: Inference done 580/1093. Dataloading: 0.0130 s/iter. Inference: 0.1757 s/iter. Eval: 0.2140 s/iter. Total: 0.4029 s/iter. ETA=0:03:26
[01/17 07:55:46] d2.evaluation.evaluator INFO: Inference done 593/1093. Dataloading: 0.0130 s/iter. Inference: 0.1760 s/iter. Eval: 0.2136 s/iter. Total: 0.4028 s/iter. ETA=0:03:21
[01/17 07:55:51] d2.evaluation.evaluator INFO: Inference done 605/1093. Dataloading: 0.0131 s/iter. Inference: 0.1757 s/iter. Eval: 0.2145 s/iter. Total: 0.4034 s/iter. ETA=0:03:16
[01/17 07:55:56] d2.evaluation.evaluator INFO: Inference done 617/1093. Dataloading: 0.0131 s/iter. Inference: 0.1755 s/iter. Eval: 0.2149 s/iter. Total: 0.4037 s/iter. ETA=0:03:12
[01/17 07:56:01] d2.evaluation.evaluator INFO: Inference done 631/1093. Dataloading: 0.0130 s/iter. Inference: 0.1750 s/iter. Eval: 0.2146 s/iter. Total: 0.4028 s/iter. ETA=0:03:06
[01/17 07:56:06] d2.evaluation.evaluator INFO: Inference done 645/1093. Dataloading: 0.0130 s/iter. Inference: 0.1749 s/iter. Eval: 0.2142 s/iter. Total: 0.4021 s/iter. ETA=0:03:00
[01/17 07:56:11] d2.evaluation.evaluator INFO: Inference done 658/1093. Dataloading: 0.0130 s/iter. Inference: 0.1748 s/iter. Eval: 0.2142 s/iter. Total: 0.4022 s/iter. ETA=0:02:54
[01/17 07:56:17] d2.evaluation.evaluator INFO: Inference done 671/1093. Dataloading: 0.0129 s/iter. Inference: 0.1747 s/iter. Eval: 0.2142 s/iter. Total: 0.4020 s/iter. ETA=0:02:49
[01/17 07:56:22] d2.evaluation.evaluator INFO: Inference done 686/1093. Dataloading: 0.0129 s/iter. Inference: 0.1747 s/iter. Eval: 0.2128 s/iter. Total: 0.4005 s/iter. ETA=0:02:43
[01/17 07:56:27] d2.evaluation.evaluator INFO: Inference done 700/1093. Dataloading: 0.0128 s/iter. Inference: 0.1749 s/iter. Eval: 0.2119 s/iter. Total: 0.3998 s/iter. ETA=0:02:37
[01/17 07:56:32] d2.evaluation.evaluator INFO: Inference done 712/1093. Dataloading: 0.0129 s/iter. Inference: 0.1750 s/iter. Eval: 0.2128 s/iter. Total: 0.4008 s/iter. ETA=0:02:32
[01/17 07:56:37] d2.evaluation.evaluator INFO: Inference done 726/1093. Dataloading: 0.0129 s/iter. Inference: 0.1748 s/iter. Eval: 0.2123 s/iter. Total: 0.4001 s/iter. ETA=0:02:26
[01/17 07:56:42] d2.evaluation.evaluator INFO: Inference done 741/1093. Dataloading: 0.0128 s/iter. Inference: 0.1745 s/iter. Eval: 0.2115 s/iter. Total: 0.3990 s/iter. ETA=0:02:20
[01/17 07:56:48] d2.evaluation.evaluator INFO: Inference done 755/1093. Dataloading: 0.0127 s/iter. Inference: 0.1744 s/iter. Eval: 0.2110 s/iter. Total: 0.3983 s/iter. ETA=0:02:14
[01/17 07:56:53] d2.evaluation.evaluator INFO: Inference done 766/1093. Dataloading: 0.0127 s/iter. Inference: 0.1747 s/iter. Eval: 0.2116 s/iter. Total: 0.3992 s/iter. ETA=0:02:10
[01/17 07:56:58] d2.evaluation.evaluator INFO: Inference done 779/1093. Dataloading: 0.0127 s/iter. Inference: 0.1746 s/iter. Eval: 0.2115 s/iter. Total: 0.3989 s/iter. ETA=0:02:05
[01/17 07:57:03] d2.evaluation.evaluator INFO: Inference done 793/1093. Dataloading: 0.0127 s/iter. Inference: 0.1746 s/iter. Eval: 0.2110 s/iter. Total: 0.3984 s/iter. ETA=0:01:59
[01/17 07:57:08] d2.evaluation.evaluator INFO: Inference done 806/1093. Dataloading: 0.0127 s/iter. Inference: 0.1747 s/iter. Eval: 0.2108 s/iter. Total: 0.3982 s/iter. ETA=0:01:54
[01/17 07:57:13] d2.evaluation.evaluator INFO: Inference done 819/1093. Dataloading: 0.0130 s/iter. Inference: 0.1746 s/iter. Eval: 0.2105 s/iter. Total: 0.3982 s/iter. ETA=0:01:49
[01/17 07:57:18] d2.evaluation.evaluator INFO: Inference done 833/1093. Dataloading: 0.0129 s/iter. Inference: 0.1743 s/iter. Eval: 0.2102 s/iter. Total: 0.3976 s/iter. ETA=0:01:43
[01/17 07:57:23] d2.evaluation.evaluator INFO: Inference done 847/1093. Dataloading: 0.0129 s/iter. Inference: 0.1744 s/iter. Eval: 0.2096 s/iter. Total: 0.3970 s/iter. ETA=0:01:37
[01/17 07:57:28] d2.evaluation.evaluator INFO: Inference done 858/1093. Dataloading: 0.0129 s/iter. Inference: 0.1745 s/iter. Eval: 0.2103 s/iter. Total: 0.3978 s/iter. ETA=0:01:33
[01/17 07:57:33] d2.evaluation.evaluator INFO: Inference done 872/1093. Dataloading: 0.0129 s/iter. Inference: 0.1742 s/iter. Eval: 0.2099 s/iter. Total: 0.3972 s/iter. ETA=0:01:27
[01/17 07:57:38] d2.evaluation.evaluator INFO: Inference done 884/1093. Dataloading: 0.0129 s/iter. Inference: 0.1742 s/iter. Eval: 0.2104 s/iter. Total: 0.3976 s/iter. ETA=0:01:23
[01/17 07:57:44] d2.evaluation.evaluator INFO: Inference done 897/1093. Dataloading: 0.0129 s/iter. Inference: 0.1741 s/iter. Eval: 0.2106 s/iter. Total: 0.3976 s/iter. ETA=0:01:17
[01/17 07:57:49] d2.evaluation.evaluator INFO: Inference done 911/1093. Dataloading: 0.0128 s/iter. Inference: 0.1740 s/iter. Eval: 0.2103 s/iter. Total: 0.3972 s/iter. ETA=0:01:12
[01/17 07:57:54] d2.evaluation.evaluator INFO: Inference done 924/1093. Dataloading: 0.0128 s/iter. Inference: 0.1740 s/iter. Eval: 0.2102 s/iter. Total: 0.3971 s/iter. ETA=0:01:07
[01/17 07:57:59] d2.evaluation.evaluator INFO: Inference done 938/1093. Dataloading: 0.0128 s/iter. Inference: 0.1740 s/iter. Eval: 0.2100 s/iter. Total: 0.3970 s/iter. ETA=0:01:01
[01/17 07:58:04] d2.evaluation.evaluator INFO: Inference done 950/1093. Dataloading: 0.0128 s/iter. Inference: 0.1742 s/iter. Eval: 0.2103 s/iter. Total: 0.3974 s/iter. ETA=0:00:56
[01/17 07:58:09] d2.evaluation.evaluator INFO: Inference done 963/1093. Dataloading: 0.0128 s/iter. Inference: 0.1741 s/iter. Eval: 0.2103 s/iter. Total: 0.3973 s/iter. ETA=0:00:51
[01/17 07:58:14] d2.evaluation.evaluator INFO: Inference done 977/1093. Dataloading: 0.0128 s/iter. Inference: 0.1740 s/iter. Eval: 0.2099 s/iter. Total: 0.3968 s/iter. ETA=0:00:46
[01/17 07:58:20] d2.evaluation.evaluator INFO: Inference done 992/1093. Dataloading: 0.0127 s/iter. Inference: 0.1741 s/iter. Eval: 0.2091 s/iter. Total: 0.3960 s/iter. ETA=0:00:39
[01/17 07:58:25] d2.evaluation.evaluator INFO: Inference done 1005/1093. Dataloading: 0.0127 s/iter. Inference: 0.1740 s/iter. Eval: 0.2093 s/iter. Total: 0.3961 s/iter. ETA=0:00:34
[01/17 07:58:30] d2.evaluation.evaluator INFO: Inference done 1019/1093. Dataloading: 0.0127 s/iter. Inference: 0.1739 s/iter. Eval: 0.2093 s/iter. Total: 0.3959 s/iter. ETA=0:00:29
[01/17 07:58:36] d2.evaluation.evaluator INFO: Inference done 1033/1093. Dataloading: 0.0127 s/iter. Inference: 0.1738 s/iter. Eval: 0.2093 s/iter. Total: 0.3958 s/iter. ETA=0:00:23
[01/17 07:58:41] d2.evaluation.evaluator INFO: Inference done 1045/1093. Dataloading: 0.0126 s/iter. Inference: 0.1739 s/iter. Eval: 0.2095 s/iter. Total: 0.3961 s/iter. ETA=0:00:19
[01/17 07:58:46] d2.evaluation.evaluator INFO: Inference done 1059/1093. Dataloading: 0.0126 s/iter. Inference: 0.1739 s/iter. Eval: 0.2092 s/iter. Total: 0.3959 s/iter. ETA=0:00:13
[01/17 07:58:51] d2.evaluation.evaluator INFO: Inference done 1075/1093. Dataloading: 0.0126 s/iter. Inference: 0.1736 s/iter. Eval: 0.2085 s/iter. Total: 0.3948 s/iter. ETA=0:00:07
[01/17 07:58:57] d2.evaluation.evaluator INFO: Inference done 1092/1093. Dataloading: 0.0125 s/iter. Inference: 0.1730 s/iter. Eval: 0.2078 s/iter. Total: 0.3935 s/iter. ETA=0:00:00
[01/17 07:58:57] d2.evaluation.evaluator INFO: Total inference time: 0:07:08.517770 (0.393858 s / iter per device, on 4 devices)
[01/17 07:58:57] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:08 (0.172975 s / iter per device, on 4 devices)
[01/17 07:59:24] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 9.786435883997674, 'fwIoU': 29.98416121118817, 'IoU-1': nan, 'IoU-2': 94.64077607619427, 'IoU-3': 43.28199000754987, 'IoU-4': 55.9886465440011, 'IoU-5': 49.19263751339141, 'IoU-6': 42.525010952705074, 'IoU-7': 38.93247346538448, 'IoU-8': 32.98616203968298, 'IoU-9': 11.410904096693178, 'IoU-10': 13.112887181336061, 'IoU-11': 17.268038996143538, 'IoU-12': 27.026173249982556, 'IoU-13': 31.95878727034973, 'IoU-14': 31.554894652607558, 'IoU-15': 30.09466648012784, 'IoU-16': 29.752386005617225, 'IoU-17': 28.897717755141368, 'IoU-18': 29.397144549891006, 'IoU-19': 29.94092886822547, 'IoU-20': 27.927218144360012, 'IoU-21': 28.236344306940957, 'IoU-22': 28.644585004611265, 'IoU-23': 31.699331369446398, 'IoU-24': 26.753409879246714, 'IoU-25': 27.53511087667768, 'IoU-26': 27.01610083551802, 'IoU-27': 27.207658265765076, 'IoU-28': 29.03618344036213, 'IoU-29': 25.28952046274756, 'IoU-30': 28.293448587242125, 'IoU-31': 27.421094896589004, 'IoU-32': 29.305597070461292, 'IoU-33': 28.020021431532978, 'IoU-34': 28.51015395745245, 'IoU-35': 27.02979144429924, 'IoU-36': 26.146612777754473, 'IoU-37': 25.363668063248753, 'IoU-38': 22.88627316374686, 'IoU-39': 23.502807658093356, 'IoU-40': 23.785808535376297, 'IoU-41': 24.527828628633397, 'IoU-42': 24.052317919149306, 'IoU-43': 24.420164374529538, 'IoU-44': 25.23752025136332, 'IoU-45': 25.515812272725142, 'IoU-46': 25.15577857253834, 'IoU-47': 26.207144958309858, 'IoU-48': 27.076373068794553, 'IoU-49': 25.799300928164275, 'IoU-50': 25.572342759071226, 'IoU-51': 26.551113640896762, 'IoU-52': 20.539158105390467, 'IoU-53': 20.861543264011605, 'IoU-54': 19.862309665277998, 'IoU-55': 21.618351072567563, 'IoU-56': 18.30870712849001, 'IoU-57': 15.788923586499525, 'IoU-58': 14.523832605307675, 'IoU-59': 15.331746464064459, 'IoU-60': 14.496785754423033, 'IoU-61': 15.121474717815481, 'IoU-62': 15.37806312412766, 'IoU-63': 15.831634770832832, 'IoU-64': 16.34727780754941, 'IoU-65': 18.145125913018937, 'IoU-66': 16.67961490833885, 'IoU-67': 18.006120505319814, 'IoU-68': 17.484228860383897, 'IoU-69': 16.869882966176114, 'IoU-70': 17.30318067252948, 'IoU-71': 14.638895223649026, 'IoU-72': 13.918311209680228, 'IoU-73': 15.270749619707818, 'IoU-74': 15.290630809570944, 'IoU-75': 16.49599947800056, 'IoU-76': 13.147152788336486, 'IoU-77': 13.986588385284715, 'IoU-78': 14.78697225552662, 'IoU-79': 13.697736632608192, 'IoU-80': 14.181809741833593, 'IoU-81': 12.562918951559663, 'IoU-82': 12.281030359612345, 'IoU-83': 11.396507494391969, 'IoU-84': 13.144839267227146, 'IoU-85': 10.912215502732426, 'IoU-86': 9.120754026853396, 'IoU-87': 11.586303976738312, 'IoU-88': 9.764214316260283, 'IoU-89': 8.659409687193588, 'IoU-90': 9.969835187734008, 'IoU-91': 9.941469010859509, 'IoU-92': 9.262391062317024, 'IoU-93': 9.046811423417306, 'IoU-94': 10.053351784271522, 'IoU-95': 8.446283767140468, 'IoU-96': 7.97848200170214, 'IoU-97': 9.385109605099643, 'IoU-98': 8.575812741130393, 'IoU-99': 9.345575751611674, 'IoU-100': 8.104905064211424, 'IoU-101': 9.999540321320236, 'IoU-102': 8.242149832690139, 'IoU-103': 9.30392307457367, 'IoU-104': 7.617928615281853, 'IoU-105': 8.68581527376755, 'IoU-106': 8.814032618831897, 'IoU-107': 9.827066207728551, 'IoU-108': 9.502511648460194, 'IoU-109': 10.503430546776078, 'IoU-110': 9.00458238801081, 'IoU-111': 11.8616782992367, 'IoU-112': 5.367012729381169, 'IoU-113': 7.219032726715315, 'IoU-114': 6.88094267281468, 'IoU-115': 9.612088939728554, 'IoU-116': 6.197403556226863, 'IoU-117': 10.409077681502762, 'IoU-118': 4.370766969374691, 'IoU-119': 7.49632989240913, 'IoU-120': 4.84335652974466, 'IoU-121': 8.475388175529474, 'IoU-122': 5.058242732821851, 'IoU-123': 6.170268626160967, 'IoU-124': 6.539472556738923, 'IoU-125': 5.35572726269213, 'IoU-126': 5.4356455524149325, 'IoU-127': 6.81661832012311, 'IoU-128': 4.515375129565698, 'IoU-129': 3.2582045923796503, 'IoU-130': 6.344001375597122, 'IoU-131': 6.94705209576264, 'IoU-132': 3.809554255190494, 'IoU-133': 3.4291122300601664, 'IoU-134': 3.8751854404652577, 'IoU-135': 3.2834781074673707, 'IoU-136': 3.2706472111930256, 'IoU-137': 3.7723461601929924, 'IoU-138': 2.6867838357206706, 'IoU-139': 4.092166462235492, 'IoU-140': 1.6234341755673742, 'IoU-141': 3.4635559509584684, 'IoU-142': 3.4326199768409316, 'IoU-143': 2.4554436136215556, 'IoU-144': 2.420325520378595, 'IoU-145': 1.8228005568909442, 'IoU-146': 2.6207112010752303, 'IoU-147': 1.6144259628682027, 'IoU-148': 1.998718303907494, 'IoU-149': 3.0961132560717917, 'IoU-150': 1.314406537629311, 'IoU-151': 0.9221916710032209, 'IoU-152': 1.1642392186257087, 'IoU-153': 1.9841221195926246, 'IoU-154': 1.5485308091017376, 'IoU-155': 1.2500106288901738, 'IoU-156': 2.4264669986762537, 'IoU-157': 1.5605757475545392, 'IoU-158': 1.792096543141993, 'IoU-159': 1.8962056287556206, 'IoU-160': 1.0178952736973663, 'IoU-161': 1.2069167355847845, 'IoU-162': 0.9149105161830599, 'IoU-163': 1.471989955720315, 'IoU-164': 3.0384208534049604, 'IoU-165': 1.4725822404589826, 'IoU-166': 0.7371718310969048, 'IoU-167': 0.05035101852917482, 'IoU-168': 0.6889492686194961, 'IoU-169': 0.7064009796281866, 'IoU-170': 0.5618574145213521, 'IoU-171': 1.8614433451301524, 'IoU-172': 0.626465737104642, 'IoU-173': 1.0656409322651128, 'IoU-174': 0.18492353723404256, 'IoU-175': 0.3374199484501533, 'IoU-176': 0.018325254950109494, 'IoU-177': 0.060152695303462636, 'IoU-178': 1.941923831437816, 'IoU-179': 1.1538015559775616, 'IoU-180': 0.5078713017543366, 'IoU-181': 1.0080270481917672, 'IoU-182': 0.31926848461442414, 'IoU-183': 0.04283624431207417, 'IoU-184': 1.5417559625492205, 'IoU-185': 0.0, 'IoU-186': 0.6565842149032046, 'IoU-187': 0.009817924257051958, 'IoU-188': 0.9313401601544267, 'IoU-189': 0.36347123580220175, 'IoU-190': 0.0, 'IoU-191': 0.0, 'IoU-192': 0.00025933139180564664, 'IoU-193': 0.8153123650813696, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 15.98869867397799, 'pACC': 42.69996131508489, 'ACC-1': nan, 'ACC-2': 98.78848410102803, 'ACC-3': 53.8890084549209, 'ACC-4': 75.09641803254065, 'ACC-5': 68.03940263122253, 'ACC-6': 61.42749150546917, 'ACC-7': 56.40947369691827, 'ACC-8': 47.56172265552359, 'ACC-9': 13.755667262957669, 'ACC-10': 16.13755461617485, 'ACC-11': 23.895242203854963, 'ACC-12': 39.07137003016554, 'ACC-13': 54.140115891753574, 'ACC-14': 49.80832400460401, 'ACC-15': 45.69034297646199, 'ACC-16': 46.54772938925022, 'ACC-17': 45.2862117238946, 'ACC-18': 46.35939771961037, 'ACC-19': 44.67997094654017, 'ACC-20': 43.84244189340784, 'ACC-21': 42.59561006479558, 'ACC-22': 43.73384998998509, 'ACC-23': 48.36578129494952, 'ACC-24': 40.70810248737573, 'ACC-25': 43.175923976846875, 'ACC-26': 43.86679442168197, 'ACC-27': 42.88318522578767, 'ACC-28': 42.477329225907575, 'ACC-29': 40.618678056632106, 'ACC-30': 46.86896896642329, 'ACC-31': 43.349491533013875, 'ACC-32': 45.37928534421838, 'ACC-33': 44.09708223722089, 'ACC-34': 44.34454152354722, 'ACC-35': 43.03588920619211, 'ACC-36': 39.1298493153165, 'ACC-37': 37.72989725322436, 'ACC-38': 35.564634222242205, 'ACC-39': 37.76945997228122, 'ACC-40': 38.76612042218256, 'ACC-41': 39.83671378155766, 'ACC-42': 40.75275552573856, 'ACC-43': 39.4048568101215, 'ACC-44': 42.32104604245362, 'ACC-45': 41.00163165403083, 'ACC-46': 40.50416004348924, 'ACC-47': 43.98785599206586, 'ACC-48': 43.315260461545584, 'ACC-49': 43.40159306627566, 'ACC-50': 42.3383221620854, 'ACC-51': 45.90793235139621, 'ACC-52': 39.75274524165487, 'ACC-53': 38.44637067356235, 'ACC-54': 33.24613508056343, 'ACC-55': 39.43972374437818, 'ACC-56': 33.18401908533499, 'ACC-57': 31.279306869296846, 'ACC-58': 22.908067673495843, 'ACC-59': 26.256778730972247, 'ACC-60': 26.39172784643973, 'ACC-61': 23.32852446759099, 'ACC-62': 24.834594961323134, 'ACC-63': 27.191473604000777, 'ACC-64': 25.236343956545028, 'ACC-65': 28.12318700766605, 'ACC-66': 26.610201262484008, 'ACC-67': 30.54051645298036, 'ACC-68': 29.88526883497473, 'ACC-69': 28.43433764305526, 'ACC-70': 29.700894149468084, 'ACC-71': 23.691096426887576, 'ACC-72': 22.380607164534897, 'ACC-73': 25.675120963395155, 'ACC-74': 24.564024345776858, 'ACC-75': 29.46633367780332, 'ACC-76': 18.620600411252553, 'ACC-77': 24.81269267152399, 'ACC-78': 23.000491884989298, 'ACC-79': 25.11985408315814, 'ACC-80': 23.139298560179494, 'ACC-81': 19.821698682694272, 'ACC-82': 19.564396016917133, 'ACC-83': 18.271456117465636, 'ACC-84': 24.56830930479298, 'ACC-85': 16.635214215970873, 'ACC-86': 14.243998327989102, 'ACC-87': 22.60508352074337, 'ACC-88': 19.721999551722686, 'ACC-89': 15.37873819542042, 'ACC-90': 18.317300749926936, 'ACC-91': 17.072815513744814, 'ACC-92': 18.374831707263304, 'ACC-93': 16.03118651259691, 'ACC-94': 20.2431646496249, 'ACC-95': 13.659968592705042, 'ACC-96': 16.173359692006457, 'ACC-97': 17.396049772276463, 'ACC-98': 17.300866024326343, 'ACC-99': 16.630418946854952, 'ACC-100': 15.854723863724896, 'ACC-101': 20.948090535840414, 'ACC-102': 13.012491166168877, 'ACC-103': 18.840598096964207, 'ACC-104': 13.42252290545008, 'ACC-105': 18.070313076412287, 'ACC-106': 15.29197295604118, 'ACC-107': 17.963926926630055, 'ACC-108': 20.679754709345588, 'ACC-109': 22.340508734872916, 'ACC-110': 16.39588764009905, 'ACC-111': 24.595586674451077, 'ACC-112': 8.822986504354127, 'ACC-113': 12.66503574481545, 'ACC-114': 13.120412802280399, 'ACC-115': 20.13774067537246, 'ACC-116': 11.560296483394223, 'ACC-117': 20.749900564541864, 'ACC-118': 6.352871873711654, 'ACC-119': 19.506049678023697, 'ACC-120': 8.438939635099079, 'ACC-121': 17.781523102710338, 'ACC-122': 9.05407682591102, 'ACC-123': 13.149395680602339, 'ACC-124': 12.125371791050963, 'ACC-125': 11.967283593157747, 'ACC-126': 9.569669434162567, 'ACC-127': 22.233979817666828, 'ACC-128': 9.388579465278621, 'ACC-129': 6.0368390432386745, 'ACC-130': 11.853969636518007, 'ACC-131': 16.656433883753866, 'ACC-132': 8.394823464673994, 'ACC-133': 5.395877525442926, 'ACC-134': 6.905087536657889, 'ACC-135': 6.056079276174063, 'ACC-136': 5.450943503791118, 'ACC-137': 6.989392121692135, 'ACC-138': 4.856447443438759, 'ACC-139': 15.60532845140493, 'ACC-140': 2.620448566226113, 'ACC-141': 6.3908946192314895, 'ACC-142': 8.419578583462792, 'ACC-143': 4.837995530369124, 'ACC-144': 4.421518385626853, 'ACC-145': 2.8300541516245485, 'ACC-146': 4.321921330977884, 'ACC-147': 2.198716044869891, 'ACC-148': 3.3158369259310496, 'ACC-149': 7.3075354711891345, 'ACC-150': 1.6195621198922185, 'ACC-151': 1.093485677119977, 'ACC-152': 1.4054395102543689, 'ACC-153': 4.945756625531054, 'ACC-154': 3.002757470822111, 'ACC-155': 2.761760171522438, 'ACC-156': 4.227154066306269, 'ACC-157': 2.6964520028182304, 'ACC-158': 4.71980978124026, 'ACC-159': 3.2769694771442004, 'ACC-160': 1.2874021441107837, 'ACC-161': 2.7111704293534813, 'ACC-162': 1.1904728471650794, 'ACC-163': 4.56261659441656, 'ACC-164': 14.331814353215616, 'ACC-165': 2.3676191261387816, 'ACC-166': 0.9508396674999287, 'ACC-167': 0.05288296077427369, 'ACC-168': 1.2861602280485072, 'ACC-169': 1.3946128419720092, 'ACC-170': 0.696271054390912, 'ACC-171': 3.069054616181836, 'ACC-172': 0.9195292509859315, 'ACC-173': 2.601400155572841, 'ACC-174': 0.2176833375352301, 'ACC-175': 0.4128757588805568, 'ACC-176': 0.01848842347364198, 'ACC-177': 0.0707423960993731, 'ACC-178': 6.909049973333039, 'ACC-179': 4.074667140547525, 'ACC-180': 0.8789139772074801, 'ACC-181': 1.6646770970909952, 'ACC-182': 0.5208666425064131, 'ACC-183': 0.050226374058551626, 'ACC-184': 2.1035853947371517, 'ACC-185': 0.0, 'ACC-186': 0.7841045041549455, 'ACC-187': 0.009914709888506727, 'ACC-188': 1.995675231640875, 'ACC-189': 0.4824764940882127, 'ACC-190': 0.0, 'ACC-191': 0.0, 'ACC-192': 0.0002610114192495922, 'ACC-193': 3.927883159625436, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 07:59:24] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 07:59:24] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 07:59:24] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 07:59:24] d2.evaluation.testing INFO: copypaste: 9.7864,29.9842,15.9887,42.7000
[01/17 07:59:24] d2.utils.events INFO:  eta: 1 day, 10:07:54  iter: 9999  total_loss: 42.63  loss_ce: 0.3308  loss_mask: 0.465  loss_dice: 3.423  loss_ce_0: 0.6153  loss_mask_0: 0.4485  loss_dice_0: 3.54  loss_ce_1: 0.3728  loss_mask_1: 0.4615  loss_dice_1: 3.46  loss_ce_2: 0.3508  loss_mask_2: 0.46  loss_dice_2: 3.439  loss_ce_3: 0.349  loss_mask_3: 0.4613  loss_dice_3: 3.424  loss_ce_4: 0.3506  loss_mask_4: 0.4642  loss_dice_4: 3.426  loss_ce_5: 0.3394  loss_mask_5: 0.465  loss_dice_5: 3.437  loss_ce_6: 0.3431  loss_mask_6: 0.4651  loss_dice_6: 3.434  loss_ce_7: 0.3301  loss_mask_7: 0.4651  loss_dice_7: 3.425  loss_ce_8: 0.3412  loss_mask_8: 0.4652  loss_dice_8: 3.425  time: 1.5344  data_time: 0.0937  lr: 8.9943e-06  max_mem: 21366M
[01/17 07:59:55] d2.utils.events INFO:  eta: 1 day, 10:07:44  iter: 10019  total_loss: 42.51  loss_ce: 0.3611  loss_mask: 0.4441  loss_dice: 3.395  loss_ce_0: 0.6123  loss_mask_0: 0.4304  loss_dice_0: 3.529  loss_ce_1: 0.3733  loss_mask_1: 0.4447  loss_dice_1: 3.43  loss_ce_2: 0.3782  loss_mask_2: 0.4448  loss_dice_2: 3.407  loss_ce_3: 0.3873  loss_mask_3: 0.4419  loss_dice_3: 3.398  loss_ce_4: 0.3557  loss_mask_4: 0.4444  loss_dice_4: 3.4  loss_ce_5: 0.3547  loss_mask_5: 0.4428  loss_dice_5: 3.397  loss_ce_6: 0.3696  loss_mask_6: 0.4428  loss_dice_6: 3.394  loss_ce_7: 0.3791  loss_mask_7: 0.4446  loss_dice_7: 3.401  loss_ce_8: 0.3715  loss_mask_8: 0.4437  loss_dice_8: 3.397  time: 1.5345  data_time: 0.0990  lr: 8.9923e-06  max_mem: 21366M
[01/17 08:00:26] d2.utils.events INFO:  eta: 1 day, 10:08:11  iter: 10039  total_loss: 42  loss_ce: 0.35  loss_mask: 0.455  loss_dice: 3.356  loss_ce_0: 0.6429  loss_mask_0: 0.4402  loss_dice_0: 3.492  loss_ce_1: 0.3627  loss_mask_1: 0.4603  loss_dice_1: 3.4  loss_ce_2: 0.3696  loss_mask_2: 0.4585  loss_dice_2: 3.377  loss_ce_3: 0.3544  loss_mask_3: 0.4597  loss_dice_3: 3.362  loss_ce_4: 0.3402  loss_mask_4: 0.4568  loss_dice_4: 3.367  loss_ce_5: 0.3481  loss_mask_5: 0.4566  loss_dice_5: 3.362  loss_ce_6: 0.3407  loss_mask_6: 0.4561  loss_dice_6: 3.36  loss_ce_7: 0.3253  loss_mask_7: 0.4551  loss_dice_7: 3.361  loss_ce_8: 0.3348  loss_mask_8: 0.4542  loss_dice_8: 3.36  time: 1.5345  data_time: 0.0907  lr: 8.9903e-06  max_mem: 21366M
[01/17 08:00:57] d2.utils.events INFO:  eta: 1 day, 10:07:39  iter: 10059  total_loss: 41.87  loss_ce: 0.3217  loss_mask: 0.4541  loss_dice: 3.353  loss_ce_0: 0.5931  loss_mask_0: 0.4416  loss_dice_0: 3.497  loss_ce_1: 0.3236  loss_mask_1: 0.4549  loss_dice_1: 3.412  loss_ce_2: 0.3343  loss_mask_2: 0.4552  loss_dice_2: 3.365  loss_ce_3: 0.3331  loss_mask_3: 0.4553  loss_dice_3: 3.367  loss_ce_4: 0.3399  loss_mask_4: 0.4545  loss_dice_4: 3.364  loss_ce_5: 0.3158  loss_mask_5: 0.4532  loss_dice_5: 3.364  loss_ce_6: 0.3213  loss_mask_6: 0.4521  loss_dice_6: 3.358  loss_ce_7: 0.3366  loss_mask_7: 0.4525  loss_dice_7: 3.354  loss_ce_8: 0.3277  loss_mask_8: 0.4537  loss_dice_8: 3.351  time: 1.5345  data_time: 0.0995  lr: 8.9882e-06  max_mem: 21366M
[01/17 08:01:27] d2.utils.events INFO:  eta: 1 day, 10:06:00  iter: 10079  total_loss: 41.8  loss_ce: 0.3323  loss_mask: 0.4661  loss_dice: 3.33  loss_ce_0: 0.5737  loss_mask_0: 0.4436  loss_dice_0: 3.465  loss_ce_1: 0.3587  loss_mask_1: 0.4675  loss_dice_1: 3.364  loss_ce_2: 0.3508  loss_mask_2: 0.4657  loss_dice_2: 3.342  loss_ce_3: 0.334  loss_mask_3: 0.4682  loss_dice_3: 3.33  loss_ce_4: 0.3238  loss_mask_4: 0.466  loss_dice_4: 3.33  loss_ce_5: 0.3264  loss_mask_5: 0.4657  loss_dice_5: 3.334  loss_ce_6: 0.3342  loss_mask_6: 0.465  loss_dice_6: 3.333  loss_ce_7: 0.3197  loss_mask_7: 0.4672  loss_dice_7: 3.328  loss_ce_8: 0.327  loss_mask_8: 0.467  loss_dice_8: 3.327  time: 1.5344  data_time: 0.0927  lr: 8.9862e-06  max_mem: 21366M
[01/17 08:01:58] d2.utils.events INFO:  eta: 1 day, 10:05:21  iter: 10099  total_loss: 42.91  loss_ce: 0.3333  loss_mask: 0.4777  loss_dice: 3.425  loss_ce_0: 0.6291  loss_mask_0: 0.4541  loss_dice_0: 3.542  loss_ce_1: 0.3842  loss_mask_1: 0.4783  loss_dice_1: 3.448  loss_ce_2: 0.3707  loss_mask_2: 0.477  loss_dice_2: 3.431  loss_ce_3: 0.3785  loss_mask_3: 0.4763  loss_dice_3: 3.422  loss_ce_4: 0.3498  loss_mask_4: 0.4765  loss_dice_4: 3.423  loss_ce_5: 0.3511  loss_mask_5: 0.4783  loss_dice_5: 3.426  loss_ce_6: 0.3485  loss_mask_6: 0.4772  loss_dice_6: 3.418  loss_ce_7: 0.3366  loss_mask_7: 0.4756  loss_dice_7: 3.422  loss_ce_8: 0.3332  loss_mask_8: 0.4761  loss_dice_8: 3.428  time: 1.5344  data_time: 0.0905  lr: 8.9842e-06  max_mem: 21366M
[01/17 08:02:28] d2.utils.events INFO:  eta: 1 day, 10:04:29  iter: 10119  total_loss: 42.54  loss_ce: 0.3626  loss_mask: 0.4704  loss_dice: 3.396  loss_ce_0: 0.5839  loss_mask_0: 0.4493  loss_dice_0: 3.512  loss_ce_1: 0.3654  loss_mask_1: 0.4735  loss_dice_1: 3.422  loss_ce_2: 0.3529  loss_mask_2: 0.4714  loss_dice_2: 3.41  loss_ce_3: 0.3599  loss_mask_3: 0.4699  loss_dice_3: 3.403  loss_ce_4: 0.3423  loss_mask_4: 0.4677  loss_dice_4: 3.401  loss_ce_5: 0.3532  loss_mask_5: 0.466  loss_dice_5: 3.396  loss_ce_6: 0.3532  loss_mask_6: 0.4689  loss_dice_6: 3.404  loss_ce_7: 0.3493  loss_mask_7: 0.4697  loss_dice_7: 3.406  loss_ce_8: 0.3298  loss_mask_8: 0.4707  loss_dice_8: 3.398  time: 1.5344  data_time: 0.0975  lr: 8.9822e-06  max_mem: 21366M
[01/17 08:03:00] d2.utils.events INFO:  eta: 1 day, 10:04:23  iter: 10139  total_loss: 42.69  loss_ce: 0.332  loss_mask: 0.4578  loss_dice: 3.431  loss_ce_0: 0.5956  loss_mask_0: 0.4414  loss_dice_0: 3.56  loss_ce_1: 0.3583  loss_mask_1: 0.4586  loss_dice_1: 3.475  loss_ce_2: 0.3542  loss_mask_2: 0.4554  loss_dice_2: 3.449  loss_ce_3: 0.3381  loss_mask_3: 0.4564  loss_dice_3: 3.442  loss_ce_4: 0.3294  loss_mask_4: 0.4564  loss_dice_4: 3.437  loss_ce_5: 0.3423  loss_mask_5: 0.4581  loss_dice_5: 3.447  loss_ce_6: 0.3393  loss_mask_6: 0.4571  loss_dice_6: 3.431  loss_ce_7: 0.3375  loss_mask_7: 0.4572  loss_dice_7: 3.438  loss_ce_8: 0.3382  loss_mask_8: 0.4554  loss_dice_8: 3.441  time: 1.5344  data_time: 0.0968  lr: 8.9801e-06  max_mem: 21366M
[01/17 08:03:30] d2.utils.events INFO:  eta: 1 day, 10:03:12  iter: 10159  total_loss: 42.28  loss_ce: 0.3355  loss_mask: 0.4672  loss_dice: 3.358  loss_ce_0: 0.6285  loss_mask_0: 0.4495  loss_dice_0: 3.508  loss_ce_1: 0.3712  loss_mask_1: 0.4722  loss_dice_1: 3.399  loss_ce_2: 0.3643  loss_mask_2: 0.471  loss_dice_2: 3.381  loss_ce_3: 0.3679  loss_mask_3: 0.4691  loss_dice_3: 3.371  loss_ce_4: 0.3527  loss_mask_4: 0.4669  loss_dice_4: 3.357  loss_ce_5: 0.3527  loss_mask_5: 0.4659  loss_dice_5: 3.37  loss_ce_6: 0.34  loss_mask_6: 0.464  loss_dice_6: 3.365  loss_ce_7: 0.3336  loss_mask_7: 0.4656  loss_dice_7: 3.363  loss_ce_8: 0.3383  loss_mask_8: 0.4653  loss_dice_8: 3.358  time: 1.5344  data_time: 0.1040  lr: 8.9781e-06  max_mem: 21366M
[01/17 08:04:01] d2.utils.events INFO:  eta: 1 day, 10:02:33  iter: 10179  total_loss: 42.66  loss_ce: 0.351  loss_mask: 0.4634  loss_dice: 3.372  loss_ce_0: 0.6216  loss_mask_0: 0.4506  loss_dice_0: 3.491  loss_ce_1: 0.3623  loss_mask_1: 0.4621  loss_dice_1: 3.401  loss_ce_2: 0.3724  loss_mask_2: 0.4621  loss_dice_2: 3.381  loss_ce_3: 0.3673  loss_mask_3: 0.4621  loss_dice_3: 3.379  loss_ce_4: 0.3527  loss_mask_4: 0.4644  loss_dice_4: 3.38  loss_ce_5: 0.3479  loss_mask_5: 0.4615  loss_dice_5: 3.377  loss_ce_6: 0.3513  loss_mask_6: 0.4614  loss_dice_6: 3.371  loss_ce_7: 0.3432  loss_mask_7: 0.463  loss_dice_7: 3.377  loss_ce_8: 0.3437  loss_mask_8: 0.4622  loss_dice_8: 3.373  time: 1.5344  data_time: 0.1001  lr: 8.9761e-06  max_mem: 21366M
[01/17 08:04:32] d2.utils.events INFO:  eta: 1 day, 10:02:47  iter: 10199  total_loss: 42.21  loss_ce: 0.33  loss_mask: 0.4565  loss_dice: 3.4  loss_ce_0: 0.6148  loss_mask_0: 0.4395  loss_dice_0: 3.531  loss_ce_1: 0.3749  loss_mask_1: 0.4569  loss_dice_1: 3.428  loss_ce_2: 0.3378  loss_mask_2: 0.4585  loss_dice_2: 3.407  loss_ce_3: 0.337  loss_mask_3: 0.4559  loss_dice_3: 3.404  loss_ce_4: 0.3384  loss_mask_4: 0.4552  loss_dice_4: 3.405  loss_ce_5: 0.3243  loss_mask_5: 0.4542  loss_dice_5: 3.407  loss_ce_6: 0.3313  loss_mask_6: 0.4557  loss_dice_6: 3.396  loss_ce_7: 0.3246  loss_mask_7: 0.4567  loss_dice_7: 3.405  loss_ce_8: 0.3407  loss_mask_8: 0.4557  loss_dice_8: 3.405  time: 1.5345  data_time: 0.0977  lr: 8.9741e-06  max_mem: 21366M
[01/17 08:05:03] d2.utils.events INFO:  eta: 1 day, 10:02:20  iter: 10219  total_loss: 42.42  loss_ce: 0.3455  loss_mask: 0.4701  loss_dice: 3.36  loss_ce_0: 0.5768  loss_mask_0: 0.4443  loss_dice_0: 3.486  loss_ce_1: 0.3318  loss_mask_1: 0.4694  loss_dice_1: 3.41  loss_ce_2: 0.3494  loss_mask_2: 0.4658  loss_dice_2: 3.384  loss_ce_3: 0.3458  loss_mask_3: 0.4683  loss_dice_3: 3.371  loss_ce_4: 0.3472  loss_mask_4: 0.4705  loss_dice_4: 3.361  loss_ce_5: 0.3383  loss_mask_5: 0.4711  loss_dice_5: 3.356  loss_ce_6: 0.3375  loss_mask_6: 0.4701  loss_dice_6: 3.365  loss_ce_7: 0.3341  loss_mask_7: 0.4686  loss_dice_7: 3.366  loss_ce_8: 0.3347  loss_mask_8: 0.4692  loss_dice_8: 3.354  time: 1.5345  data_time: 0.0971  lr: 8.972e-06  max_mem: 21366M
[01/17 08:05:34] d2.utils.events INFO:  eta: 1 day, 10:02:03  iter: 10239  total_loss: 42.96  loss_ce: 0.3576  loss_mask: 0.4635  loss_dice: 3.413  loss_ce_0: 0.597  loss_mask_0: 0.4526  loss_dice_0: 3.546  loss_ce_1: 0.3884  loss_mask_1: 0.472  loss_dice_1: 3.445  loss_ce_2: 0.3712  loss_mask_2: 0.4687  loss_dice_2: 3.424  loss_ce_3: 0.3662  loss_mask_3: 0.4656  loss_dice_3: 3.406  loss_ce_4: 0.3476  loss_mask_4: 0.4637  loss_dice_4: 3.416  loss_ce_5: 0.3407  loss_mask_5: 0.4661  loss_dice_5: 3.416  loss_ce_6: 0.3353  loss_mask_6: 0.464  loss_dice_6: 3.411  loss_ce_7: 0.3455  loss_mask_7: 0.4659  loss_dice_7: 3.408  loss_ce_8: 0.3621  loss_mask_8: 0.4641  loss_dice_8: 3.411  time: 1.5345  data_time: 0.1011  lr: 8.97e-06  max_mem: 21366M
[01/17 08:06:05] d2.utils.events INFO:  eta: 1 day, 10:01:15  iter: 10259  total_loss: 41.69  loss_ce: 0.303  loss_mask: 0.4676  loss_dice: 3.344  loss_ce_0: 0.5595  loss_mask_0: 0.446  loss_dice_0: 3.477  loss_ce_1: 0.3142  loss_mask_1: 0.4687  loss_dice_1: 3.375  loss_ce_2: 0.3369  loss_mask_2: 0.4695  loss_dice_2: 3.35  loss_ce_3: 0.3329  loss_mask_3: 0.469  loss_dice_3: 3.34  loss_ce_4: 0.3348  loss_mask_4: 0.4677  loss_dice_4: 3.337  loss_ce_5: 0.3069  loss_mask_5: 0.4681  loss_dice_5: 3.344  loss_ce_6: 0.326  loss_mask_6: 0.4669  loss_dice_6: 3.336  loss_ce_7: 0.3185  loss_mask_7: 0.4651  loss_dice_7: 3.345  loss_ce_8: 0.3122  loss_mask_8: 0.4654  loss_dice_8: 3.341  time: 1.5345  data_time: 0.1066  lr: 8.968e-06  max_mem: 21366M
[01/17 08:06:35] d2.utils.events INFO:  eta: 1 day, 10:00:07  iter: 10279  total_loss: 42.67  loss_ce: 0.3357  loss_mask: 0.475  loss_dice: 3.429  loss_ce_0: 0.6087  loss_mask_0: 0.4575  loss_dice_0: 3.539  loss_ce_1: 0.3452  loss_mask_1: 0.4798  loss_dice_1: 3.447  loss_ce_2: 0.349  loss_mask_2: 0.4739  loss_dice_2: 3.435  loss_ce_3: 0.364  loss_mask_3: 0.4715  loss_dice_3: 3.431  loss_ce_4: 0.3487  loss_mask_4: 0.474  loss_dice_4: 3.425  loss_ce_5: 0.3397  loss_mask_5: 0.477  loss_dice_5: 3.436  loss_ce_6: 0.3392  loss_mask_6: 0.475  loss_dice_6: 3.432  loss_ce_7: 0.3304  loss_mask_7: 0.4728  loss_dice_7: 3.423  loss_ce_8: 0.3436  loss_mask_8: 0.476  loss_dice_8: 3.422  time: 1.5345  data_time: 0.1009  lr: 8.966e-06  max_mem: 21366M
[01/17 08:07:06] d2.utils.events INFO:  eta: 1 day, 9:58:57  iter: 10299  total_loss: 41.75  loss_ce: 0.3247  loss_mask: 0.464  loss_dice: 3.348  loss_ce_0: 0.6113  loss_mask_0: 0.4515  loss_dice_0: 3.469  loss_ce_1: 0.3483  loss_mask_1: 0.4735  loss_dice_1: 3.38  loss_ce_2: 0.3375  loss_mask_2: 0.4716  loss_dice_2: 3.357  loss_ce_3: 0.328  loss_mask_3: 0.4702  loss_dice_3: 3.349  loss_ce_4: 0.3299  loss_mask_4: 0.4679  loss_dice_4: 3.354  loss_ce_5: 0.3075  loss_mask_5: 0.4671  loss_dice_5: 3.352  loss_ce_6: 0.3115  loss_mask_6: 0.4685  loss_dice_6: 3.352  loss_ce_7: 0.3097  loss_mask_7: 0.4678  loss_dice_7: 3.349  loss_ce_8: 0.3221  loss_mask_8: 0.4667  loss_dice_8: 3.347  time: 1.5345  data_time: 0.0876  lr: 8.9639e-06  max_mem: 21366M
[01/17 08:07:37] d2.utils.events INFO:  eta: 1 day, 9:59:06  iter: 10319  total_loss: 42.27  loss_ce: 0.3584  loss_mask: 0.4578  loss_dice: 3.315  loss_ce_0: 0.6254  loss_mask_0: 0.4413  loss_dice_0: 3.466  loss_ce_1: 0.3844  loss_mask_1: 0.4556  loss_dice_1: 3.371  loss_ce_2: 0.3853  loss_mask_2: 0.4535  loss_dice_2: 3.35  loss_ce_3: 0.3528  loss_mask_3: 0.4554  loss_dice_3: 3.331  loss_ce_4: 0.3448  loss_mask_4: 0.4544  loss_dice_4: 3.319  loss_ce_5: 0.3399  loss_mask_5: 0.4573  loss_dice_5: 3.329  loss_ce_6: 0.3592  loss_mask_6: 0.4554  loss_dice_6: 3.318  loss_ce_7: 0.3394  loss_mask_7: 0.4591  loss_dice_7: 3.325  loss_ce_8: 0.3434  loss_mask_8: 0.4572  loss_dice_8: 3.323  time: 1.5345  data_time: 0.0931  lr: 8.9619e-06  max_mem: 21366M
[01/17 08:08:07] d2.utils.events INFO:  eta: 1 day, 9:58:35  iter: 10339  total_loss: 42.64  loss_ce: 0.3654  loss_mask: 0.4771  loss_dice: 3.389  loss_ce_0: 0.6059  loss_mask_0: 0.4553  loss_dice_0: 3.522  loss_ce_1: 0.3639  loss_mask_1: 0.4866  loss_dice_1: 3.439  loss_ce_2: 0.3767  loss_mask_2: 0.4811  loss_dice_2: 3.408  loss_ce_3: 0.3735  loss_mask_3: 0.4801  loss_dice_3: 3.397  loss_ce_4: 0.3636  loss_mask_4: 0.4745  loss_dice_4: 3.4  loss_ce_5: 0.355  loss_mask_5: 0.4734  loss_dice_5: 3.408  loss_ce_6: 0.3544  loss_mask_6: 0.473  loss_dice_6: 3.402  loss_ce_7: 0.3591  loss_mask_7: 0.4732  loss_dice_7: 3.396  loss_ce_8: 0.3634  loss_mask_8: 0.4742  loss_dice_8: 3.392  time: 1.5345  data_time: 0.0921  lr: 8.9599e-06  max_mem: 21366M
[01/17 08:08:39] d2.utils.events INFO:  eta: 1 day, 9:58:20  iter: 10359  total_loss: 42.97  loss_ce: 0.3416  loss_mask: 0.4609  loss_dice: 3.435  loss_ce_0: 0.6434  loss_mask_0: 0.4431  loss_dice_0: 3.547  loss_ce_1: 0.3617  loss_mask_1: 0.4659  loss_dice_1: 3.466  loss_ce_2: 0.3609  loss_mask_2: 0.4646  loss_dice_2: 3.453  loss_ce_3: 0.3481  loss_mask_3: 0.4622  loss_dice_3: 3.445  loss_ce_4: 0.3365  loss_mask_4: 0.4651  loss_dice_4: 3.441  loss_ce_5: 0.3338  loss_mask_5: 0.4642  loss_dice_5: 3.446  loss_ce_6: 0.3335  loss_mask_6: 0.4645  loss_dice_6: 3.439  loss_ce_7: 0.3365  loss_mask_7: 0.464  loss_dice_7: 3.431  loss_ce_8: 0.335  loss_mask_8: 0.4628  loss_dice_8: 3.44  time: 1.5345  data_time: 0.1102  lr: 8.9579e-06  max_mem: 21366M
[01/17 08:09:10] d2.utils.events INFO:  eta: 1 day, 9:57:16  iter: 10379  total_loss: 42.32  loss_ce: 0.3352  loss_mask: 0.4551  loss_dice: 3.381  loss_ce_0: 0.6038  loss_mask_0: 0.4441  loss_dice_0: 3.51  loss_ce_1: 0.3555  loss_mask_1: 0.4617  loss_dice_1: 3.416  loss_ce_2: 0.3876  loss_mask_2: 0.4597  loss_dice_2: 3.392  loss_ce_3: 0.3443  loss_mask_3: 0.4578  loss_dice_3: 3.385  loss_ce_4: 0.3592  loss_mask_4: 0.4587  loss_dice_4: 3.386  loss_ce_5: 0.3411  loss_mask_5: 0.4581  loss_dice_5: 3.397  loss_ce_6: 0.3348  loss_mask_6: 0.4581  loss_dice_6: 3.391  loss_ce_7: 0.3418  loss_mask_7: 0.4586  loss_dice_7: 3.387  loss_ce_8: 0.3291  loss_mask_8: 0.458  loss_dice_8: 3.389  time: 1.5346  data_time: 0.1028  lr: 8.9558e-06  max_mem: 21366M
[01/17 08:09:41] d2.utils.events INFO:  eta: 1 day, 9:57:03  iter: 10399  total_loss: 43.13  loss_ce: 0.3294  loss_mask: 0.4708  loss_dice: 3.417  loss_ce_0: 0.6152  loss_mask_0: 0.4561  loss_dice_0: 3.538  loss_ce_1: 0.3677  loss_mask_1: 0.4689  loss_dice_1: 3.45  loss_ce_2: 0.3635  loss_mask_2: 0.4679  loss_dice_2: 3.427  loss_ce_3: 0.3552  loss_mask_3: 0.4712  loss_dice_3: 3.42  loss_ce_4: 0.3483  loss_mask_4: 0.4721  loss_dice_4: 3.419  loss_ce_5: 0.3395  loss_mask_5: 0.4747  loss_dice_5: 3.421  loss_ce_6: 0.3393  loss_mask_6: 0.4741  loss_dice_6: 3.424  loss_ce_7: 0.3333  loss_mask_7: 0.4726  loss_dice_7: 3.413  loss_ce_8: 0.3165  loss_mask_8: 0.4706  loss_dice_8: 3.415  time: 1.5346  data_time: 0.0954  lr: 8.9538e-06  max_mem: 21366M
[01/17 08:10:11] d2.utils.events INFO:  eta: 1 day, 9:57:09  iter: 10419  total_loss: 42.55  loss_ce: 0.344  loss_mask: 0.4664  loss_dice: 3.384  loss_ce_0: 0.6052  loss_mask_0: 0.4471  loss_dice_0: 3.502  loss_ce_1: 0.3858  loss_mask_1: 0.465  loss_dice_1: 3.412  loss_ce_2: 0.3683  loss_mask_2: 0.4625  loss_dice_2: 3.393  loss_ce_3: 0.3628  loss_mask_3: 0.4639  loss_dice_3: 3.382  loss_ce_4: 0.3681  loss_mask_4: 0.4639  loss_dice_4: 3.381  loss_ce_5: 0.3447  loss_mask_5: 0.4635  loss_dice_5: 3.387  loss_ce_6: 0.3492  loss_mask_6: 0.4669  loss_dice_6: 3.382  loss_ce_7: 0.3563  loss_mask_7: 0.467  loss_dice_7: 3.377  loss_ce_8: 0.3523  loss_mask_8: 0.4658  loss_dice_8: 3.38  time: 1.5346  data_time: 0.0948  lr: 8.9518e-06  max_mem: 21366M
[01/17 08:10:42] d2.utils.events INFO:  eta: 1 day, 9:56:02  iter: 10439  total_loss: 41.68  loss_ce: 0.3192  loss_mask: 0.458  loss_dice: 3.334  loss_ce_0: 0.5907  loss_mask_0: 0.4377  loss_dice_0: 3.456  loss_ce_1: 0.3405  loss_mask_1: 0.4579  loss_dice_1: 3.359  loss_ce_2: 0.3351  loss_mask_2: 0.457  loss_dice_2: 3.341  loss_ce_3: 0.3246  loss_mask_3: 0.4593  loss_dice_3: 3.341  loss_ce_4: 0.3165  loss_mask_4: 0.4582  loss_dice_4: 3.339  loss_ce_5: 0.3077  loss_mask_5: 0.4584  loss_dice_5: 3.344  loss_ce_6: 0.3287  loss_mask_6: 0.4562  loss_dice_6: 3.335  loss_ce_7: 0.3196  loss_mask_7: 0.4573  loss_dice_7: 3.333  loss_ce_8: 0.2964  loss_mask_8: 0.4564  loss_dice_8: 3.332  time: 1.5346  data_time: 0.1005  lr: 8.9498e-06  max_mem: 21366M
[01/17 08:11:13] d2.utils.events INFO:  eta: 1 day, 9:56:02  iter: 10459  total_loss: 42.75  loss_ce: 0.3558  loss_mask: 0.4597  loss_dice: 3.397  loss_ce_0: 0.6347  loss_mask_0: 0.4499  loss_dice_0: 3.511  loss_ce_1: 0.3736  loss_mask_1: 0.4664  loss_dice_1: 3.425  loss_ce_2: 0.3533  loss_mask_2: 0.4622  loss_dice_2: 3.397  loss_ce_3: 0.3673  loss_mask_3: 0.4599  loss_dice_3: 3.392  loss_ce_4: 0.3626  loss_mask_4: 0.4613  loss_dice_4: 3.406  loss_ce_5: 0.3539  loss_mask_5: 0.4606  loss_dice_5: 3.398  loss_ce_6: 0.3582  loss_mask_6: 0.4636  loss_dice_6: 3.398  loss_ce_7: 0.3709  loss_mask_7: 0.4635  loss_dice_7: 3.394  loss_ce_8: 0.3597  loss_mask_8: 0.4627  loss_dice_8: 3.395  time: 1.5346  data_time: 0.0956  lr: 8.9477e-06  max_mem: 21366M
[01/17 08:11:43] d2.utils.events INFO:  eta: 1 day, 9:55:00  iter: 10479  total_loss: 42.5  loss_ce: 0.3405  loss_mask: 0.4635  loss_dice: 3.38  loss_ce_0: 0.6004  loss_mask_0: 0.4532  loss_dice_0: 3.505  loss_ce_1: 0.346  loss_mask_1: 0.4662  loss_dice_1: 3.417  loss_ce_2: 0.3393  loss_mask_2: 0.4645  loss_dice_2: 3.392  loss_ce_3: 0.3402  loss_mask_3: 0.4675  loss_dice_3: 3.388  loss_ce_4: 0.3553  loss_mask_4: 0.4653  loss_dice_4: 3.389  loss_ce_5: 0.3455  loss_mask_5: 0.4666  loss_dice_5: 3.386  loss_ce_6: 0.3537  loss_mask_6: 0.4664  loss_dice_6: 3.377  loss_ce_7: 0.3378  loss_mask_7: 0.4654  loss_dice_7: 3.387  loss_ce_8: 0.3334  loss_mask_8: 0.4625  loss_dice_8: 3.384  time: 1.5345  data_time: 0.0884  lr: 8.9457e-06  max_mem: 21366M
[01/17 08:12:14] d2.utils.events INFO:  eta: 1 day, 9:54:30  iter: 10499  total_loss: 42.49  loss_ce: 0.3804  loss_mask: 0.4732  loss_dice: 3.365  loss_ce_0: 0.5954  loss_mask_0: 0.4527  loss_dice_0: 3.499  loss_ce_1: 0.3884  loss_mask_1: 0.4743  loss_dice_1: 3.408  loss_ce_2: 0.391  loss_mask_2: 0.4721  loss_dice_2: 3.384  loss_ce_3: 0.3729  loss_mask_3: 0.4709  loss_dice_3: 3.375  loss_ce_4: 0.3681  loss_mask_4: 0.4714  loss_dice_4: 3.377  loss_ce_5: 0.3727  loss_mask_5: 0.4713  loss_dice_5: 3.371  loss_ce_6: 0.3737  loss_mask_6: 0.4728  loss_dice_6: 3.372  loss_ce_7: 0.3754  loss_mask_7: 0.472  loss_dice_7: 3.369  loss_ce_8: 0.3744  loss_mask_8: 0.4714  loss_dice_8: 3.361  time: 1.5345  data_time: 0.0898  lr: 8.9437e-06  max_mem: 21366M
[01/17 08:12:45] d2.utils.events INFO:  eta: 1 day, 9:54:15  iter: 10519  total_loss: 42.92  loss_ce: 0.3781  loss_mask: 0.4609  loss_dice: 3.455  loss_ce_0: 0.6471  loss_mask_0: 0.4472  loss_dice_0: 3.562  loss_ce_1: 0.3806  loss_mask_1: 0.4673  loss_dice_1: 3.486  loss_ce_2: 0.3915  loss_mask_2: 0.4648  loss_dice_2: 3.466  loss_ce_3: 0.3663  loss_mask_3: 0.4609  loss_dice_3: 3.451  loss_ce_4: 0.3692  loss_mask_4: 0.463  loss_dice_4: 3.449  loss_ce_5: 0.3515  loss_mask_5: 0.4622  loss_dice_5: 3.447  loss_ce_6: 0.3645  loss_mask_6: 0.46  loss_dice_6: 3.454  loss_ce_7: 0.361  loss_mask_7: 0.46  loss_dice_7: 3.455  loss_ce_8: 0.3715  loss_mask_8: 0.4598  loss_dice_8: 3.452  time: 1.5345  data_time: 0.0992  lr: 8.9417e-06  max_mem: 21366M
[01/17 08:13:16] d2.utils.events INFO:  eta: 1 day, 9:52:49  iter: 10539  total_loss: 41.65  loss_ce: 0.3067  loss_mask: 0.445  loss_dice: 3.35  loss_ce_0: 0.578  loss_mask_0: 0.4275  loss_dice_0: 3.475  loss_ce_1: 0.3334  loss_mask_1: 0.4478  loss_dice_1: 3.377  loss_ce_2: 0.3324  loss_mask_2: 0.4449  loss_dice_2: 3.357  loss_ce_3: 0.3151  loss_mask_3: 0.4443  loss_dice_3: 3.354  loss_ce_4: 0.314  loss_mask_4: 0.4456  loss_dice_4: 3.356  loss_ce_5: 0.3217  loss_mask_5: 0.4462  loss_dice_5: 3.359  loss_ce_6: 0.3048  loss_mask_6: 0.4465  loss_dice_6: 3.353  loss_ce_7: 0.3119  loss_mask_7: 0.447  loss_dice_7: 3.348  loss_ce_8: 0.309  loss_mask_8: 0.4473  loss_dice_8: 3.338  time: 1.5345  data_time: 0.1026  lr: 8.9396e-06  max_mem: 21366M
[01/17 08:13:46] d2.utils.events INFO:  eta: 1 day, 9:52:18  iter: 10559  total_loss: 41.8  loss_ce: 0.3524  loss_mask: 0.4633  loss_dice: 3.326  loss_ce_0: 0.5812  loss_mask_0: 0.451  loss_dice_0: 3.459  loss_ce_1: 0.3538  loss_mask_1: 0.4636  loss_dice_1: 3.366  loss_ce_2: 0.3577  loss_mask_2: 0.465  loss_dice_2: 3.331  loss_ce_3: 0.3508  loss_mask_3: 0.4612  loss_dice_3: 3.328  loss_ce_4: 0.3498  loss_mask_4: 0.4599  loss_dice_4: 3.326  loss_ce_5: 0.3574  loss_mask_5: 0.4641  loss_dice_5: 3.329  loss_ce_6: 0.3391  loss_mask_6: 0.4621  loss_dice_6: 3.329  loss_ce_7: 0.3508  loss_mask_7: 0.4614  loss_dice_7: 3.328  loss_ce_8: 0.3421  loss_mask_8: 0.4628  loss_dice_8: 3.327  time: 1.5345  data_time: 0.0903  lr: 8.9376e-06  max_mem: 21366M
[01/17 08:14:17] d2.utils.events INFO:  eta: 1 day, 9:52:09  iter: 10579  total_loss: 43.12  loss_ce: 0.3304  loss_mask: 0.4554  loss_dice: 3.375  loss_ce_0: 0.623  loss_mask_0: 0.4428  loss_dice_0: 3.494  loss_ce_1: 0.3608  loss_mask_1: 0.4591  loss_dice_1: 3.425  loss_ce_2: 0.3569  loss_mask_2: 0.4549  loss_dice_2: 3.387  loss_ce_3: 0.3558  loss_mask_3: 0.4538  loss_dice_3: 3.378  loss_ce_4: 0.3566  loss_mask_4: 0.4553  loss_dice_4: 3.375  loss_ce_5: 0.3466  loss_mask_5: 0.4553  loss_dice_5: 3.378  loss_ce_6: 0.3512  loss_mask_6: 0.4564  loss_dice_6: 3.37  loss_ce_7: 0.3424  loss_mask_7: 0.4573  loss_dice_7: 3.371  loss_ce_8: 0.3377  loss_mask_8: 0.4573  loss_dice_8: 3.372  time: 1.5345  data_time: 0.0950  lr: 8.9356e-06  max_mem: 21366M
[01/17 08:14:48] d2.utils.events INFO:  eta: 1 day, 9:51:05  iter: 10599  total_loss: 42.01  loss_ce: 0.3397  loss_mask: 0.4677  loss_dice: 3.367  loss_ce_0: 0.605  loss_mask_0: 0.4601  loss_dice_0: 3.466  loss_ce_1: 0.3492  loss_mask_1: 0.4764  loss_dice_1: 3.388  loss_ce_2: 0.3587  loss_mask_2: 0.472  loss_dice_2: 3.371  loss_ce_3: 0.3556  loss_mask_3: 0.4706  loss_dice_3: 3.356  loss_ce_4: 0.3561  loss_mask_4: 0.4677  loss_dice_4: 3.361  loss_ce_5: 0.3225  loss_mask_5: 0.4668  loss_dice_5: 3.373  loss_ce_6: 0.3452  loss_mask_6: 0.4685  loss_dice_6: 3.353  loss_ce_7: 0.3426  loss_mask_7: 0.469  loss_dice_7: 3.353  loss_ce_8: 0.3181  loss_mask_8: 0.4687  loss_dice_8: 3.355  time: 1.5345  data_time: 0.1087  lr: 8.9336e-06  max_mem: 21366M
[01/17 08:15:18] d2.utils.events INFO:  eta: 1 day, 9:50:34  iter: 10619  total_loss: 42.14  loss_ce: 0.3436  loss_mask: 0.4595  loss_dice: 3.348  loss_ce_0: 0.62  loss_mask_0: 0.4511  loss_dice_0: 3.463  loss_ce_1: 0.3655  loss_mask_1: 0.469  loss_dice_1: 3.378  loss_ce_2: 0.3547  loss_mask_2: 0.4651  loss_dice_2: 3.353  loss_ce_3: 0.3328  loss_mask_3: 0.4634  loss_dice_3: 3.346  loss_ce_4: 0.3307  loss_mask_4: 0.461  loss_dice_4: 3.348  loss_ce_5: 0.3286  loss_mask_5: 0.4605  loss_dice_5: 3.346  loss_ce_6: 0.3441  loss_mask_6: 0.4605  loss_dice_6: 3.35  loss_ce_7: 0.3264  loss_mask_7: 0.4608  loss_dice_7: 3.348  loss_ce_8: 0.334  loss_mask_8: 0.4611  loss_dice_8: 3.35  time: 1.5345  data_time: 0.1011  lr: 8.9315e-06  max_mem: 21366M
[01/17 08:15:50] d2.utils.events INFO:  eta: 1 day, 9:50:44  iter: 10639  total_loss: 42.55  loss_ce: 0.3318  loss_mask: 0.4379  loss_dice: 3.399  loss_ce_0: 0.5905  loss_mask_0: 0.4303  loss_dice_0: 3.524  loss_ce_1: 0.3736  loss_mask_1: 0.4409  loss_dice_1: 3.444  loss_ce_2: 0.3603  loss_mask_2: 0.4376  loss_dice_2: 3.419  loss_ce_3: 0.3439  loss_mask_3: 0.4398  loss_dice_3: 3.408  loss_ce_4: 0.3441  loss_mask_4: 0.442  loss_dice_4: 3.403  loss_ce_5: 0.3415  loss_mask_5: 0.4444  loss_dice_5: 3.409  loss_ce_6: 0.3403  loss_mask_6: 0.4411  loss_dice_6: 3.398  loss_ce_7: 0.3485  loss_mask_7: 0.4416  loss_dice_7: 3.402  loss_ce_8: 0.3501  loss_mask_8: 0.4397  loss_dice_8: 3.404  time: 1.5346  data_time: 0.0997  lr: 8.9295e-06  max_mem: 21366M
[01/17 08:16:20] d2.utils.events INFO:  eta: 1 day, 9:50:40  iter: 10659  total_loss: 42.29  loss_ce: 0.3543  loss_mask: 0.4576  loss_dice: 3.37  loss_ce_0: 0.598  loss_mask_0: 0.4441  loss_dice_0: 3.507  loss_ce_1: 0.3571  loss_mask_1: 0.4585  loss_dice_1: 3.41  loss_ce_2: 0.3672  loss_mask_2: 0.4569  loss_dice_2: 3.387  loss_ce_3: 0.3519  loss_mask_3: 0.4558  loss_dice_3: 3.369  loss_ce_4: 0.3532  loss_mask_4: 0.4553  loss_dice_4: 3.374  loss_ce_5: 0.3495  loss_mask_5: 0.4566  loss_dice_5: 3.381  loss_ce_6: 0.3497  loss_mask_6: 0.4556  loss_dice_6: 3.382  loss_ce_7: 0.3506  loss_mask_7: 0.4561  loss_dice_7: 3.364  loss_ce_8: 0.3479  loss_mask_8: 0.4559  loss_dice_8: 3.37  time: 1.5346  data_time: 0.0948  lr: 8.9275e-06  max_mem: 21366M
[01/17 08:16:51] d2.utils.events INFO:  eta: 1 day, 9:49:53  iter: 10679  total_loss: 42.08  loss_ce: 0.3462  loss_mask: 0.4607  loss_dice: 3.391  loss_ce_0: 0.6325  loss_mask_0: 0.4405  loss_dice_0: 3.51  loss_ce_1: 0.3743  loss_mask_1: 0.4569  loss_dice_1: 3.414  loss_ce_2: 0.3771  loss_mask_2: 0.4557  loss_dice_2: 3.392  loss_ce_3: 0.3716  loss_mask_3: 0.457  loss_dice_3: 3.386  loss_ce_4: 0.3521  loss_mask_4: 0.4583  loss_dice_4: 3.389  loss_ce_5: 0.3736  loss_mask_5: 0.4615  loss_dice_5: 3.388  loss_ce_6: 0.3512  loss_mask_6: 0.463  loss_dice_6: 3.383  loss_ce_7: 0.354  loss_mask_7: 0.4585  loss_dice_7: 3.381  loss_ce_8: 0.3372  loss_mask_8: 0.4591  loss_dice_8: 3.383  time: 1.5346  data_time: 0.0977  lr: 8.9255e-06  max_mem: 21366M
[01/17 08:17:22] d2.utils.events INFO:  eta: 1 day, 9:50:45  iter: 10699  total_loss: 42.36  loss_ce: 0.3631  loss_mask: 0.4465  loss_dice: 3.339  loss_ce_0: 0.6256  loss_mask_0: 0.4358  loss_dice_0: 3.48  loss_ce_1: 0.3688  loss_mask_1: 0.4483  loss_dice_1: 3.395  loss_ce_2: 0.3706  loss_mask_2: 0.4503  loss_dice_2: 3.374  loss_ce_3: 0.372  loss_mask_3: 0.4469  loss_dice_3: 3.356  loss_ce_4: 0.3753  loss_mask_4: 0.4442  loss_dice_4: 3.358  loss_ce_5: 0.3704  loss_mask_5: 0.4439  loss_dice_5: 3.362  loss_ce_6: 0.3715  loss_mask_6: 0.4453  loss_dice_6: 3.352  loss_ce_7: 0.37  loss_mask_7: 0.4459  loss_dice_7: 3.353  loss_ce_8: 0.3713  loss_mask_8: 0.446  loss_dice_8: 3.347  time: 1.5346  data_time: 0.1059  lr: 8.9234e-06  max_mem: 21366M
[01/17 08:17:52] d2.utils.events INFO:  eta: 1 day, 9:50:14  iter: 10719  total_loss: 41.87  loss_ce: 0.3275  loss_mask: 0.4542  loss_dice: 3.33  loss_ce_0: 0.5957  loss_mask_0: 0.4405  loss_dice_0: 3.466  loss_ce_1: 0.3527  loss_mask_1: 0.4626  loss_dice_1: 3.384  loss_ce_2: 0.3508  loss_mask_2: 0.4595  loss_dice_2: 3.344  loss_ce_3: 0.3281  loss_mask_3: 0.4581  loss_dice_3: 3.35  loss_ce_4: 0.3283  loss_mask_4: 0.4551  loss_dice_4: 3.337  loss_ce_5: 0.3254  loss_mask_5: 0.4545  loss_dice_5: 3.342  loss_ce_6: 0.3217  loss_mask_6: 0.4559  loss_dice_6: 3.339  loss_ce_7: 0.3227  loss_mask_7: 0.4535  loss_dice_7: 3.337  loss_ce_8: 0.3305  loss_mask_8: 0.4541  loss_dice_8: 3.337  time: 1.5346  data_time: 0.0897  lr: 8.9214e-06  max_mem: 21366M
[01/17 08:18:24] d2.utils.events INFO:  eta: 1 day, 9:50:12  iter: 10739  total_loss: 42.24  loss_ce: 0.3399  loss_mask: 0.4539  loss_dice: 3.35  loss_ce_0: 0.5906  loss_mask_0: 0.438  loss_dice_0: 3.465  loss_ce_1: 0.3656  loss_mask_1: 0.4587  loss_dice_1: 3.371  loss_ce_2: 0.3667  loss_mask_2: 0.461  loss_dice_2: 3.344  loss_ce_3: 0.3583  loss_mask_3: 0.4604  loss_dice_3: 3.34  loss_ce_4: 0.3527  loss_mask_4: 0.4572  loss_dice_4: 3.35  loss_ce_5: 0.3517  loss_mask_5: 0.4605  loss_dice_5: 3.343  loss_ce_6: 0.3418  loss_mask_6: 0.4574  loss_dice_6: 3.343  loss_ce_7: 0.3362  loss_mask_7: 0.4565  loss_dice_7: 3.338  loss_ce_8: 0.3437  loss_mask_8: 0.4558  loss_dice_8: 3.344  time: 1.5346  data_time: 0.1128  lr: 8.9194e-06  max_mem: 21366M
[01/17 08:18:54] d2.utils.events INFO:  eta: 1 day, 9:49:35  iter: 10759  total_loss: 42.5  loss_ce: 0.3438  loss_mask: 0.4713  loss_dice: 3.337  loss_ce_0: 0.5906  loss_mask_0: 0.4556  loss_dice_0: 3.478  loss_ce_1: 0.3396  loss_mask_1: 0.4709  loss_dice_1: 3.382  loss_ce_2: 0.3483  loss_mask_2: 0.4677  loss_dice_2: 3.363  loss_ce_3: 0.3305  loss_mask_3: 0.4707  loss_dice_3: 3.35  loss_ce_4: 0.3233  loss_mask_4: 0.4707  loss_dice_4: 3.343  loss_ce_5: 0.334  loss_mask_5: 0.4719  loss_dice_5: 3.341  loss_ce_6: 0.3376  loss_mask_6: 0.4705  loss_dice_6: 3.333  loss_ce_7: 0.3419  loss_mask_7: 0.4708  loss_dice_7: 3.347  loss_ce_8: 0.3293  loss_mask_8: 0.4736  loss_dice_8: 3.346  time: 1.5346  data_time: 0.1008  lr: 8.9174e-06  max_mem: 21366M
[01/17 08:19:25] d2.utils.events INFO:  eta: 1 day, 9:48:42  iter: 10779  total_loss: 42.16  loss_ce: 0.3519  loss_mask: 0.4624  loss_dice: 3.362  loss_ce_0: 0.6038  loss_mask_0: 0.4387  loss_dice_0: 3.483  loss_ce_1: 0.3523  loss_mask_1: 0.4561  loss_dice_1: 3.396  loss_ce_2: 0.3606  loss_mask_2: 0.4545  loss_dice_2: 3.371  loss_ce_3: 0.3486  loss_mask_3: 0.4526  loss_dice_3: 3.364  loss_ce_4: 0.3494  loss_mask_4: 0.457  loss_dice_4: 3.372  loss_ce_5: 0.3404  loss_mask_5: 0.4588  loss_dice_5: 3.37  loss_ce_6: 0.322  loss_mask_6: 0.4604  loss_dice_6: 3.363  loss_ce_7: 0.3122  loss_mask_7: 0.4597  loss_dice_7: 3.372  loss_ce_8: 0.3323  loss_mask_8: 0.459  loss_dice_8: 3.369  time: 1.5346  data_time: 0.0925  lr: 8.9153e-06  max_mem: 21366M
[01/17 08:19:55] d2.utils.events INFO:  eta: 1 day, 9:48:25  iter: 10799  total_loss: 41.21  loss_ce: 0.3323  loss_mask: 0.4624  loss_dice: 3.275  loss_ce_0: 0.583  loss_mask_0: 0.4432  loss_dice_0: 3.418  loss_ce_1: 0.3288  loss_mask_1: 0.4618  loss_dice_1: 3.325  loss_ce_2: 0.3385  loss_mask_2: 0.4636  loss_dice_2: 3.297  loss_ce_3: 0.3258  loss_mask_3: 0.4663  loss_dice_3: 3.286  loss_ce_4: 0.3308  loss_mask_4: 0.4642  loss_dice_4: 3.28  loss_ce_5: 0.3358  loss_mask_5: 0.4639  loss_dice_5: 3.288  loss_ce_6: 0.3226  loss_mask_6: 0.4651  loss_dice_6: 3.285  loss_ce_7: 0.3391  loss_mask_7: 0.4661  loss_dice_7: 3.28  loss_ce_8: 0.3277  loss_mask_8: 0.4636  loss_dice_8: 3.281  time: 1.5345  data_time: 0.0856  lr: 8.9133e-06  max_mem: 21366M
[01/17 08:20:27] d2.utils.events INFO:  eta: 1 day, 9:48:03  iter: 10819  total_loss: 43.71  loss_ce: 0.3874  loss_mask: 0.4741  loss_dice: 3.464  loss_ce_0: 0.6451  loss_mask_0: 0.4626  loss_dice_0: 3.553  loss_ce_1: 0.3756  loss_mask_1: 0.4835  loss_dice_1: 3.478  loss_ce_2: 0.4157  loss_mask_2: 0.4764  loss_dice_2: 3.474  loss_ce_3: 0.4031  loss_mask_3: 0.4742  loss_dice_3: 3.477  loss_ce_4: 0.3895  loss_mask_4: 0.4752  loss_dice_4: 3.482  loss_ce_5: 0.3912  loss_mask_5: 0.4747  loss_dice_5: 3.465  loss_ce_6: 0.3849  loss_mask_6: 0.4722  loss_dice_6: 3.478  loss_ce_7: 0.4071  loss_mask_7: 0.4693  loss_dice_7: 3.472  loss_ce_8: 0.3978  loss_mask_8: 0.4734  loss_dice_8: 3.475  time: 1.5346  data_time: 0.1027  lr: 8.9113e-06  max_mem: 21366M
[01/17 08:20:57] d2.utils.events INFO:  eta: 1 day, 9:47:32  iter: 10839  total_loss: 43.07  loss_ce: 0.348  loss_mask: 0.4622  loss_dice: 3.42  loss_ce_0: 0.6119  loss_mask_0: 0.4581  loss_dice_0: 3.522  loss_ce_1: 0.38  loss_mask_1: 0.4699  loss_dice_1: 3.444  loss_ce_2: 0.3577  loss_mask_2: 0.4619  loss_dice_2: 3.431  loss_ce_3: 0.3634  loss_mask_3: 0.46  loss_dice_3: 3.43  loss_ce_4: 0.3618  loss_mask_4: 0.4616  loss_dice_4: 3.429  loss_ce_5: 0.3343  loss_mask_5: 0.4621  loss_dice_5: 3.418  loss_ce_6: 0.3467  loss_mask_6: 0.4598  loss_dice_6: 3.423  loss_ce_7: 0.3479  loss_mask_7: 0.461  loss_dice_7: 3.417  loss_ce_8: 0.3429  loss_mask_8: 0.4626  loss_dice_8: 3.421  time: 1.5346  data_time: 0.0974  lr: 8.9093e-06  max_mem: 21366M
[01/17 08:21:28] d2.utils.events INFO:  eta: 1 day, 9:46:39  iter: 10859  total_loss: 42.57  loss_ce: 0.3575  loss_mask: 0.4718  loss_dice: 3.389  loss_ce_0: 0.6036  loss_mask_0: 0.462  loss_dice_0: 3.501  loss_ce_1: 0.3729  loss_mask_1: 0.4805  loss_dice_1: 3.426  loss_ce_2: 0.3782  loss_mask_2: 0.4771  loss_dice_2: 3.4  loss_ce_3: 0.3626  loss_mask_3: 0.4743  loss_dice_3: 3.391  loss_ce_4: 0.354  loss_mask_4: 0.4714  loss_dice_4: 3.402  loss_ce_5: 0.3455  loss_mask_5: 0.4707  loss_dice_5: 3.411  loss_ce_6: 0.3683  loss_mask_6: 0.4703  loss_dice_6: 3.403  loss_ce_7: 0.347  loss_mask_7: 0.4699  loss_dice_7: 3.401  loss_ce_8: 0.3528  loss_mask_8: 0.4704  loss_dice_8: 3.399  time: 1.5346  data_time: 0.0996  lr: 8.9072e-06  max_mem: 21366M
[01/17 08:21:58] d2.utils.events INFO:  eta: 1 day, 9:45:20  iter: 10879  total_loss: 42.05  loss_ce: 0.3255  loss_mask: 0.4627  loss_dice: 3.357  loss_ce_0: 0.5921  loss_mask_0: 0.4507  loss_dice_0: 3.464  loss_ce_1: 0.3541  loss_mask_1: 0.4685  loss_dice_1: 3.397  loss_ce_2: 0.3475  loss_mask_2: 0.4625  loss_dice_2: 3.374  loss_ce_3: 0.3515  loss_mask_3: 0.4639  loss_dice_3: 3.353  loss_ce_4: 0.3503  loss_mask_4: 0.464  loss_dice_4: 3.367  loss_ce_5: 0.3336  loss_mask_5: 0.464  loss_dice_5: 3.36  loss_ce_6: 0.3316  loss_mask_6: 0.4634  loss_dice_6: 3.355  loss_ce_7: 0.3388  loss_mask_7: 0.4644  loss_dice_7: 3.353  loss_ce_8: 0.3341  loss_mask_8: 0.4622  loss_dice_8: 3.358  time: 1.5345  data_time: 0.0958  lr: 8.9052e-06  max_mem: 21366M
[01/17 08:22:29] d2.utils.events INFO:  eta: 1 day, 9:44:06  iter: 10899  total_loss: 41.79  loss_ce: 0.3389  loss_mask: 0.4573  loss_dice: 3.318  loss_ce_0: 0.6084  loss_mask_0: 0.4427  loss_dice_0: 3.439  loss_ce_1: 0.3559  loss_mask_1: 0.4573  loss_dice_1: 3.364  loss_ce_2: 0.3446  loss_mask_2: 0.4559  loss_dice_2: 3.329  loss_ce_3: 0.3519  loss_mask_3: 0.4553  loss_dice_3: 3.326  loss_ce_4: 0.3603  loss_mask_4: 0.4553  loss_dice_4: 3.325  loss_ce_5: 0.345  loss_mask_5: 0.4537  loss_dice_5: 3.311  loss_ce_6: 0.3358  loss_mask_6: 0.4565  loss_dice_6: 3.313  loss_ce_7: 0.3432  loss_mask_7: 0.4571  loss_dice_7: 3.313  loss_ce_8: 0.3412  loss_mask_8: 0.4574  loss_dice_8: 3.316  time: 1.5345  data_time: 0.0901  lr: 8.9032e-06  max_mem: 21366M
[01/17 08:23:00] d2.utils.events INFO:  eta: 1 day, 9:45:21  iter: 10919  total_loss: 42  loss_ce: 0.361  loss_mask: 0.4425  loss_dice: 3.324  loss_ce_0: 0.6131  loss_mask_0: 0.4293  loss_dice_0: 3.47  loss_ce_1: 0.3626  loss_mask_1: 0.4392  loss_dice_1: 3.367  loss_ce_2: 0.3762  loss_mask_2: 0.4405  loss_dice_2: 3.343  loss_ce_3: 0.3715  loss_mask_3: 0.4411  loss_dice_3: 3.34  loss_ce_4: 0.3585  loss_mask_4: 0.4429  loss_dice_4: 3.332  loss_ce_5: 0.3372  loss_mask_5: 0.4433  loss_dice_5: 3.332  loss_ce_6: 0.3509  loss_mask_6: 0.4448  loss_dice_6: 3.333  loss_ce_7: 0.3501  loss_mask_7: 0.444  loss_dice_7: 3.326  loss_ce_8: 0.3634  loss_mask_8: 0.4424  loss_dice_8: 3.331  time: 1.5345  data_time: 0.0988  lr: 8.9012e-06  max_mem: 21366M
[01/17 08:23:31] d2.utils.events INFO:  eta: 1 day, 9:44:46  iter: 10939  total_loss: 42.13  loss_ce: 0.3552  loss_mask: 0.4613  loss_dice: 3.326  loss_ce_0: 0.6003  loss_mask_0: 0.4416  loss_dice_0: 3.444  loss_ce_1: 0.3629  loss_mask_1: 0.4652  loss_dice_1: 3.363  loss_ce_2: 0.3896  loss_mask_2: 0.4623  loss_dice_2: 3.346  loss_ce_3: 0.3516  loss_mask_3: 0.4615  loss_dice_3: 3.335  loss_ce_4: 0.3708  loss_mask_4: 0.4607  loss_dice_4: 3.337  loss_ce_5: 0.36  loss_mask_5: 0.4592  loss_dice_5: 3.343  loss_ce_6: 0.3485  loss_mask_6: 0.4577  loss_dice_6: 3.326  loss_ce_7: 0.3668  loss_mask_7: 0.4564  loss_dice_7: 3.334  loss_ce_8: 0.351  loss_mask_8: 0.4571  loss_dice_8: 3.333  time: 1.5345  data_time: 0.0905  lr: 8.8991e-06  max_mem: 21366M
[01/17 08:24:01] d2.utils.events INFO:  eta: 1 day, 9:43:50  iter: 10959  total_loss: 42.43  loss_ce: 0.3537  loss_mask: 0.472  loss_dice: 3.384  loss_ce_0: 0.5987  loss_mask_0: 0.4544  loss_dice_0: 3.486  loss_ce_1: 0.3472  loss_mask_1: 0.4762  loss_dice_1: 3.414  loss_ce_2: 0.3438  loss_mask_2: 0.4743  loss_dice_2: 3.403  loss_ce_3: 0.3554  loss_mask_3: 0.4727  loss_dice_3: 3.384  loss_ce_4: 0.3362  loss_mask_4: 0.4742  loss_dice_4: 3.382  loss_ce_5: 0.3449  loss_mask_5: 0.4756  loss_dice_5: 3.398  loss_ce_6: 0.3456  loss_mask_6: 0.4768  loss_dice_6: 3.382  loss_ce_7: 0.3433  loss_mask_7: 0.4756  loss_dice_7: 3.386  loss_ce_8: 0.3502  loss_mask_8: 0.4736  loss_dice_8: 3.394  time: 1.5345  data_time: 0.1058  lr: 8.8971e-06  max_mem: 21366M
[01/17 08:24:32] d2.utils.events INFO:  eta: 1 day, 9:43:08  iter: 10979  total_loss: 41.72  loss_ce: 0.3595  loss_mask: 0.4582  loss_dice: 3.294  loss_ce_0: 0.6232  loss_mask_0: 0.4478  loss_dice_0: 3.419  loss_ce_1: 0.3694  loss_mask_1: 0.4615  loss_dice_1: 3.335  loss_ce_2: 0.3819  loss_mask_2: 0.4581  loss_dice_2: 3.314  loss_ce_3: 0.3808  loss_mask_3: 0.4595  loss_dice_3: 3.289  loss_ce_4: 0.3777  loss_mask_4: 0.4579  loss_dice_4: 3.295  loss_ce_5: 0.3552  loss_mask_5: 0.4599  loss_dice_5: 3.293  loss_ce_6: 0.3475  loss_mask_6: 0.4589  loss_dice_6: 3.295  loss_ce_7: 0.3508  loss_mask_7: 0.4572  loss_dice_7: 3.295  loss_ce_8: 0.357  loss_mask_8: 0.4595  loss_dice_8: 3.29  time: 1.5345  data_time: 0.1035  lr: 8.8951e-06  max_mem: 21366M
[01/17 08:25:02] d2.utils.events INFO:  eta: 1 day, 9:42:37  iter: 10999  total_loss: 41.59  loss_ce: 0.3189  loss_mask: 0.4489  loss_dice: 3.349  loss_ce_0: 0.5848  loss_mask_0: 0.4375  loss_dice_0: 3.446  loss_ce_1: 0.3499  loss_mask_1: 0.4486  loss_dice_1: 3.372  loss_ce_2: 0.3434  loss_mask_2: 0.4488  loss_dice_2: 3.347  loss_ce_3: 0.3333  loss_mask_3: 0.4495  loss_dice_3: 3.343  loss_ce_4: 0.3284  loss_mask_4: 0.4472  loss_dice_4: 3.343  loss_ce_5: 0.3169  loss_mask_5: 0.4465  loss_dice_5: 3.351  loss_ce_6: 0.314  loss_mask_6: 0.4459  loss_dice_6: 3.349  loss_ce_7: 0.3244  loss_mask_7: 0.4462  loss_dice_7: 3.341  loss_ce_8: 0.3167  loss_mask_8: 0.4486  loss_dice_8: 3.347  time: 1.5345  data_time: 0.0878  lr: 8.8931e-06  max_mem: 21366M
[01/17 08:25:33] d2.utils.events INFO:  eta: 1 day, 9:41:40  iter: 11019  total_loss: 41.59  loss_ce: 0.331  loss_mask: 0.4443  loss_dice: 3.328  loss_ce_0: 0.5738  loss_mask_0: 0.4217  loss_dice_0: 3.453  loss_ce_1: 0.3381  loss_mask_1: 0.4434  loss_dice_1: 3.364  loss_ce_2: 0.3532  loss_mask_2: 0.444  loss_dice_2: 3.354  loss_ce_3: 0.3395  loss_mask_3: 0.4427  loss_dice_3: 3.344  loss_ce_4: 0.3348  loss_mask_4: 0.4435  loss_dice_4: 3.332  loss_ce_5: 0.3272  loss_mask_5: 0.4453  loss_dice_5: 3.332  loss_ce_6: 0.3334  loss_mask_6: 0.4458  loss_dice_6: 3.324  loss_ce_7: 0.3376  loss_mask_7: 0.4483  loss_dice_7: 3.335  loss_ce_8: 0.329  loss_mask_8: 0.4454  loss_dice_8: 3.331  time: 1.5344  data_time: 0.1026  lr: 8.891e-06  max_mem: 21366M
[01/17 08:26:03] d2.utils.events INFO:  eta: 1 day, 9:40:09  iter: 11039  total_loss: 42.76  loss_ce: 0.3436  loss_mask: 0.4503  loss_dice: 3.399  loss_ce_0: 0.5959  loss_mask_0: 0.4426  loss_dice_0: 3.518  loss_ce_1: 0.3643  loss_mask_1: 0.4564  loss_dice_1: 3.442  loss_ce_2: 0.384  loss_mask_2: 0.4524  loss_dice_2: 3.416  loss_ce_3: 0.3556  loss_mask_3: 0.4512  loss_dice_3: 3.395  loss_ce_4: 0.3471  loss_mask_4: 0.4506  loss_dice_4: 3.401  loss_ce_5: 0.3457  loss_mask_5: 0.4471  loss_dice_5: 3.401  loss_ce_6: 0.3376  loss_mask_6: 0.4459  loss_dice_6: 3.399  loss_ce_7: 0.3485  loss_mask_7: 0.4484  loss_dice_7: 3.391  loss_ce_8: 0.3394  loss_mask_8: 0.4495  loss_dice_8: 3.399  time: 1.5344  data_time: 0.0908  lr: 8.889e-06  max_mem: 21366M
[01/17 08:26:34] d2.utils.events INFO:  eta: 1 day, 9:40:00  iter: 11059  total_loss: 42.35  loss_ce: 0.3669  loss_mask: 0.4432  loss_dice: 3.366  loss_ce_0: 0.6234  loss_mask_0: 0.4345  loss_dice_0: 3.511  loss_ce_1: 0.3683  loss_mask_1: 0.448  loss_dice_1: 3.402  loss_ce_2: 0.3682  loss_mask_2: 0.4447  loss_dice_2: 3.382  loss_ce_3: 0.3559  loss_mask_3: 0.4412  loss_dice_3: 3.384  loss_ce_4: 0.3757  loss_mask_4: 0.4406  loss_dice_4: 3.377  loss_ce_5: 0.3558  loss_mask_5: 0.4419  loss_dice_5: 3.378  loss_ce_6: 0.3553  loss_mask_6: 0.4413  loss_dice_6: 3.37  loss_ce_7: 0.3629  loss_mask_7: 0.4401  loss_dice_7: 3.374  loss_ce_8: 0.3515  loss_mask_8: 0.4406  loss_dice_8: 3.375  time: 1.5344  data_time: 0.1032  lr: 8.887e-06  max_mem: 21366M
[01/17 08:27:04] d2.utils.events INFO:  eta: 1 day, 9:39:46  iter: 11079  total_loss: 41.72  loss_ce: 0.3499  loss_mask: 0.4759  loss_dice: 3.308  loss_ce_0: 0.5802  loss_mask_0: 0.4556  loss_dice_0: 3.429  loss_ce_1: 0.3398  loss_mask_1: 0.4795  loss_dice_1: 3.347  loss_ce_2: 0.3776  loss_mask_2: 0.4779  loss_dice_2: 3.316  loss_ce_3: 0.3676  loss_mask_3: 0.4748  loss_dice_3: 3.308  loss_ce_4: 0.3536  loss_mask_4: 0.4765  loss_dice_4: 3.313  loss_ce_5: 0.3659  loss_mask_5: 0.4792  loss_dice_5: 3.317  loss_ce_6: 0.3612  loss_mask_6: 0.476  loss_dice_6: 3.316  loss_ce_7: 0.3396  loss_mask_7: 0.477  loss_dice_7: 3.314  loss_ce_8: 0.3302  loss_mask_8: 0.4749  loss_dice_8: 3.308  time: 1.5344  data_time: 0.0914  lr: 8.885e-06  max_mem: 21366M
[01/17 08:27:35] d2.utils.events INFO:  eta: 1 day, 9:39:15  iter: 11099  total_loss: 41.9  loss_ce: 0.349  loss_mask: 0.4584  loss_dice: 3.308  loss_ce_0: 0.5783  loss_mask_0: 0.4521  loss_dice_0: 3.425  loss_ce_1: 0.3513  loss_mask_1: 0.4722  loss_dice_1: 3.329  loss_ce_2: 0.3504  loss_mask_2: 0.4634  loss_dice_2: 3.317  loss_ce_3: 0.3463  loss_mask_3: 0.4596  loss_dice_3: 3.302  loss_ce_4: 0.3426  loss_mask_4: 0.4598  loss_dice_4: 3.305  loss_ce_5: 0.3397  loss_mask_5: 0.4586  loss_dice_5: 3.314  loss_ce_6: 0.3534  loss_mask_6: 0.4589  loss_dice_6: 3.307  loss_ce_7: 0.3369  loss_mask_7: 0.4584  loss_dice_7: 3.302  loss_ce_8: 0.3361  loss_mask_8: 0.4582  loss_dice_8: 3.31  time: 1.5344  data_time: 0.0968  lr: 8.8829e-06  max_mem: 21366M
[01/17 08:28:06] d2.utils.events INFO:  eta: 1 day, 9:39:33  iter: 11119  total_loss: 42.01  loss_ce: 0.3172  loss_mask: 0.4564  loss_dice: 3.363  loss_ce_0: 0.5751  loss_mask_0: 0.4467  loss_dice_0: 3.474  loss_ce_1: 0.3336  loss_mask_1: 0.4604  loss_dice_1: 3.389  loss_ce_2: 0.3309  loss_mask_2: 0.4587  loss_dice_2: 3.367  loss_ce_3: 0.3353  loss_mask_3: 0.4544  loss_dice_3: 3.36  loss_ce_4: 0.318  loss_mask_4: 0.4565  loss_dice_4: 3.365  loss_ce_5: 0.3179  loss_mask_5: 0.4575  loss_dice_5: 3.356  loss_ce_6: 0.3145  loss_mask_6: 0.4549  loss_dice_6: 3.358  loss_ce_7: 0.3079  loss_mask_7: 0.454  loss_dice_7: 3.363  loss_ce_8: 0.3171  loss_mask_8: 0.4562  loss_dice_8: 3.361  time: 1.5344  data_time: 0.0956  lr: 8.8809e-06  max_mem: 21366M
[01/17 08:28:36] d2.utils.events INFO:  eta: 1 day, 9:37:51  iter: 11139  total_loss: 41.76  loss_ce: 0.3197  loss_mask: 0.4467  loss_dice: 3.371  loss_ce_0: 0.6094  loss_mask_0: 0.4335  loss_dice_0: 3.482  loss_ce_1: 0.3281  loss_mask_1: 0.4475  loss_dice_1: 3.398  loss_ce_2: 0.3479  loss_mask_2: 0.4436  loss_dice_2: 3.383  loss_ce_3: 0.338  loss_mask_3: 0.4416  loss_dice_3: 3.374  loss_ce_4: 0.3431  loss_mask_4: 0.4444  loss_dice_4: 3.364  loss_ce_5: 0.3419  loss_mask_5: 0.4478  loss_dice_5: 3.38  loss_ce_6: 0.3294  loss_mask_6: 0.4483  loss_dice_6: 3.369  loss_ce_7: 0.3263  loss_mask_7: 0.4476  loss_dice_7: 3.368  loss_ce_8: 0.3211  loss_mask_8: 0.4456  loss_dice_8: 3.368  time: 1.5344  data_time: 0.0992  lr: 8.8789e-06  max_mem: 21366M
[01/17 08:29:07] d2.utils.events INFO:  eta: 1 day, 9:38:21  iter: 11159  total_loss: 42.29  loss_ce: 0.3559  loss_mask: 0.453  loss_dice: 3.357  loss_ce_0: 0.5947  loss_mask_0: 0.447  loss_dice_0: 3.486  loss_ce_1: 0.374  loss_mask_1: 0.4567  loss_dice_1: 3.397  loss_ce_2: 0.3748  loss_mask_2: 0.4549  loss_dice_2: 3.368  loss_ce_3: 0.3628  loss_mask_3: 0.4551  loss_dice_3: 3.358  loss_ce_4: 0.3596  loss_mask_4: 0.455  loss_dice_4: 3.361  loss_ce_5: 0.3549  loss_mask_5: 0.4558  loss_dice_5: 3.368  loss_ce_6: 0.3545  loss_mask_6: 0.4528  loss_dice_6: 3.363  loss_ce_7: 0.3581  loss_mask_7: 0.4516  loss_dice_7: 3.363  loss_ce_8: 0.3561  loss_mask_8: 0.4544  loss_dice_8: 3.362  time: 1.5343  data_time: 0.0922  lr: 8.8768e-06  max_mem: 21366M
[01/17 08:29:37] d2.utils.events INFO:  eta: 1 day, 9:36:50  iter: 11179  total_loss: 41.87  loss_ce: 0.3549  loss_mask: 0.4641  loss_dice: 3.333  loss_ce_0: 0.5882  loss_mask_0: 0.4506  loss_dice_0: 3.445  loss_ce_1: 0.3769  loss_mask_1: 0.4729  loss_dice_1: 3.358  loss_ce_2: 0.3746  loss_mask_2: 0.4657  loss_dice_2: 3.329  loss_ce_3: 0.3597  loss_mask_3: 0.4674  loss_dice_3: 3.33  loss_ce_4: 0.3678  loss_mask_4: 0.4671  loss_dice_4: 3.329  loss_ce_5: 0.3365  loss_mask_5: 0.466  loss_dice_5: 3.34  loss_ce_6: 0.3672  loss_mask_6: 0.4701  loss_dice_6: 3.327  loss_ce_7: 0.353  loss_mask_7: 0.4678  loss_dice_7: 3.328  loss_ce_8: 0.3534  loss_mask_8: 0.465  loss_dice_8: 3.332  time: 1.5343  data_time: 0.0918  lr: 8.8748e-06  max_mem: 21366M
[01/17 08:30:07] d2.utils.events INFO:  eta: 1 day, 9:35:05  iter: 11199  total_loss: 41.68  loss_ce: 0.3729  loss_mask: 0.4617  loss_dice: 3.291  loss_ce_0: 0.587  loss_mask_0: 0.4409  loss_dice_0: 3.425  loss_ce_1: 0.3758  loss_mask_1: 0.4551  loss_dice_1: 3.328  loss_ce_2: 0.3847  loss_mask_2: 0.4583  loss_dice_2: 3.296  loss_ce_3: 0.3609  loss_mask_3: 0.4589  loss_dice_3: 3.291  loss_ce_4: 0.3791  loss_mask_4: 0.4593  loss_dice_4: 3.289  loss_ce_5: 0.3604  loss_mask_5: 0.4586  loss_dice_5: 3.3  loss_ce_6: 0.3738  loss_mask_6: 0.4553  loss_dice_6: 3.288  loss_ce_7: 0.3609  loss_mask_7: 0.4566  loss_dice_7: 3.292  loss_ce_8: 0.3738  loss_mask_8: 0.4607  loss_dice_8: 3.289  time: 1.5343  data_time: 0.0963  lr: 8.8728e-06  max_mem: 21366M
[01/17 08:30:38] d2.utils.events INFO:  eta: 1 day, 9:33:54  iter: 11219  total_loss: 41.06  loss_ce: 0.3196  loss_mask: 0.4443  loss_dice: 3.259  loss_ce_0: 0.6211  loss_mask_0: 0.4269  loss_dice_0: 3.4  loss_ce_1: 0.3487  loss_mask_1: 0.4474  loss_dice_1: 3.292  loss_ce_2: 0.3606  loss_mask_2: 0.4459  loss_dice_2: 3.27  loss_ce_3: 0.3378  loss_mask_3: 0.4457  loss_dice_3: 3.274  loss_ce_4: 0.3332  loss_mask_4: 0.4455  loss_dice_4: 3.267  loss_ce_5: 0.3249  loss_mask_5: 0.4436  loss_dice_5: 3.274  loss_ce_6: 0.3231  loss_mask_6: 0.4439  loss_dice_6: 3.26  loss_ce_7: 0.3163  loss_mask_7: 0.4437  loss_dice_7: 3.266  loss_ce_8: 0.3193  loss_mask_8: 0.4465  loss_dice_8: 3.257  time: 1.5342  data_time: 0.0943  lr: 8.8708e-06  max_mem: 21366M
[01/17 08:31:08] d2.utils.events INFO:  eta: 1 day, 9:32:49  iter: 11239  total_loss: 40.89  loss_ce: 0.3538  loss_mask: 0.4459  loss_dice: 3.26  loss_ce_0: 0.5872  loss_mask_0: 0.4284  loss_dice_0: 3.398  loss_ce_1: 0.3543  loss_mask_1: 0.4447  loss_dice_1: 3.285  loss_ce_2: 0.3803  loss_mask_2: 0.4464  loss_dice_2: 3.269  loss_ce_3: 0.3629  loss_mask_3: 0.4431  loss_dice_3: 3.255  loss_ce_4: 0.3644  loss_mask_4: 0.4427  loss_dice_4: 3.257  loss_ce_5: 0.3446  loss_mask_5: 0.4432  loss_dice_5: 3.261  loss_ce_6: 0.3378  loss_mask_6: 0.444  loss_dice_6: 3.256  loss_ce_7: 0.3378  loss_mask_7: 0.4449  loss_dice_7: 3.259  loss_ce_8: 0.3526  loss_mask_8: 0.4457  loss_dice_8: 3.26  time: 1.5342  data_time: 0.0974  lr: 8.8687e-06  max_mem: 21366M
[01/17 08:31:39] d2.utils.events INFO:  eta: 1 day, 9:32:38  iter: 11259  total_loss: 41.49  loss_ce: 0.3173  loss_mask: 0.4527  loss_dice: 3.327  loss_ce_0: 0.5692  loss_mask_0: 0.4412  loss_dice_0: 3.455  loss_ce_1: 0.3416  loss_mask_1: 0.4546  loss_dice_1: 3.366  loss_ce_2: 0.3388  loss_mask_2: 0.4564  loss_dice_2: 3.35  loss_ce_3: 0.3361  loss_mask_3: 0.4519  loss_dice_3: 3.34  loss_ce_4: 0.3361  loss_mask_4: 0.4533  loss_dice_4: 3.343  loss_ce_5: 0.3237  loss_mask_5: 0.4533  loss_dice_5: 3.341  loss_ce_6: 0.3156  loss_mask_6: 0.4523  loss_dice_6: 3.336  loss_ce_7: 0.3108  loss_mask_7: 0.4513  loss_dice_7: 3.336  loss_ce_8: 0.3245  loss_mask_8: 0.4514  loss_dice_8: 3.33  time: 1.5342  data_time: 0.0992  lr: 8.8667e-06  max_mem: 21366M
[01/17 08:32:11] d2.utils.events INFO:  eta: 1 day, 9:32:22  iter: 11279  total_loss: 41.48  loss_ce: 0.3206  loss_mask: 0.4284  loss_dice: 3.351  loss_ce_0: 0.6052  loss_mask_0: 0.4139  loss_dice_0: 3.478  loss_ce_1: 0.3674  loss_mask_1: 0.4302  loss_dice_1: 3.386  loss_ce_2: 0.3466  loss_mask_2: 0.427  loss_dice_2: 3.37  loss_ce_3: 0.3428  loss_mask_3: 0.4266  loss_dice_3: 3.358  loss_ce_4: 0.3371  loss_mask_4: 0.4266  loss_dice_4: 3.356  loss_ce_5: 0.3296  loss_mask_5: 0.427  loss_dice_5: 3.35  loss_ce_6: 0.3159  loss_mask_6: 0.4268  loss_dice_6: 3.359  loss_ce_7: 0.3283  loss_mask_7: 0.4275  loss_dice_7: 3.351  loss_ce_8: 0.3199  loss_mask_8: 0.4271  loss_dice_8: 3.35  time: 1.5343  data_time: 0.0989  lr: 8.8647e-06  max_mem: 21366M
[01/17 08:32:42] d2.utils.events INFO:  eta: 1 day, 9:31:51  iter: 11299  total_loss: 41.49  loss_ce: 0.3135  loss_mask: 0.4521  loss_dice: 3.299  loss_ce_0: 0.5763  loss_mask_0: 0.4352  loss_dice_0: 3.44  loss_ce_1: 0.3217  loss_mask_1: 0.4559  loss_dice_1: 3.341  loss_ce_2: 0.3198  loss_mask_2: 0.46  loss_dice_2: 3.32  loss_ce_3: 0.321  loss_mask_3: 0.4537  loss_dice_3: 3.303  loss_ce_4: 0.3143  loss_mask_4: 0.456  loss_dice_4: 3.305  loss_ce_5: 0.3032  loss_mask_5: 0.4553  loss_dice_5: 3.319  loss_ce_6: 0.3023  loss_mask_6: 0.4558  loss_dice_6: 3.308  loss_ce_7: 0.3179  loss_mask_7: 0.4525  loss_dice_7: 3.305  loss_ce_8: 0.3151  loss_mask_8: 0.4539  loss_dice_8: 3.308  time: 1.5343  data_time: 0.0973  lr: 8.8627e-06  max_mem: 21366M
[01/17 08:33:12] d2.utils.events INFO:  eta: 1 day, 9:31:21  iter: 11319  total_loss: 40.98  loss_ce: 0.3174  loss_mask: 0.4534  loss_dice: 3.259  loss_ce_0: 0.5952  loss_mask_0: 0.4367  loss_dice_0: 3.404  loss_ce_1: 0.3214  loss_mask_1: 0.4537  loss_dice_1: 3.299  loss_ce_2: 0.3233  loss_mask_2: 0.4537  loss_dice_2: 3.269  loss_ce_3: 0.3105  loss_mask_3: 0.4541  loss_dice_3: 3.255  loss_ce_4: 0.3028  loss_mask_4: 0.4545  loss_dice_4: 3.254  loss_ce_5: 0.3139  loss_mask_5: 0.4512  loss_dice_5: 3.263  loss_ce_6: 0.3076  loss_mask_6: 0.4525  loss_dice_6: 3.258  loss_ce_7: 0.2929  loss_mask_7: 0.4526  loss_dice_7: 3.253  loss_ce_8: 0.3034  loss_mask_8: 0.4524  loss_dice_8: 3.253  time: 1.5343  data_time: 0.0974  lr: 8.8606e-06  max_mem: 21366M
[01/17 08:33:43] d2.utils.events INFO:  eta: 1 day, 9:30:50  iter: 11339  total_loss: 41.68  loss_ce: 0.3292  loss_mask: 0.4587  loss_dice: 3.317  loss_ce_0: 0.6059  loss_mask_0: 0.4466  loss_dice_0: 3.441  loss_ce_1: 0.3507  loss_mask_1: 0.4682  loss_dice_1: 3.35  loss_ce_2: 0.3406  loss_mask_2: 0.4635  loss_dice_2: 3.328  loss_ce_3: 0.3349  loss_mask_3: 0.4588  loss_dice_3: 3.311  loss_ce_4: 0.3407  loss_mask_4: 0.4568  loss_dice_4: 3.331  loss_ce_5: 0.3314  loss_mask_5: 0.4578  loss_dice_5: 3.323  loss_ce_6: 0.3297  loss_mask_6: 0.4567  loss_dice_6: 3.319  loss_ce_7: 0.3385  loss_mask_7: 0.4575  loss_dice_7: 3.326  loss_ce_8: 0.3217  loss_mask_8: 0.457  loss_dice_8: 3.324  time: 1.5343  data_time: 0.0956  lr: 8.8586e-06  max_mem: 21366M
[01/17 08:34:14] d2.utils.events INFO:  eta: 1 day, 9:29:20  iter: 11359  total_loss: 41.39  loss_ce: 0.3326  loss_mask: 0.4612  loss_dice: 3.323  loss_ce_0: 0.5557  loss_mask_0: 0.4457  loss_dice_0: 3.446  loss_ce_1: 0.333  loss_mask_1: 0.4738  loss_dice_1: 3.365  loss_ce_2: 0.3471  loss_mask_2: 0.4682  loss_dice_2: 3.331  loss_ce_3: 0.3374  loss_mask_3: 0.4621  loss_dice_3: 3.333  loss_ce_4: 0.3206  loss_mask_4: 0.4632  loss_dice_4: 3.332  loss_ce_5: 0.3259  loss_mask_5: 0.4649  loss_dice_5: 3.332  loss_ce_6: 0.3329  loss_mask_6: 0.4623  loss_dice_6: 3.325  loss_ce_7: 0.3323  loss_mask_7: 0.4623  loss_dice_7: 3.327  loss_ce_8: 0.3327  loss_mask_8: 0.4627  loss_dice_8: 3.333  time: 1.5343  data_time: 0.0904  lr: 8.8566e-06  max_mem: 21366M
[01/17 08:34:44] d2.utils.events INFO:  eta: 1 day, 9:28:22  iter: 11379  total_loss: 41.39  loss_ce: 0.328  loss_mask: 0.4586  loss_dice: 3.306  loss_ce_0: 0.5802  loss_mask_0: 0.4433  loss_dice_0: 3.447  loss_ce_1: 0.348  loss_mask_1: 0.462  loss_dice_1: 3.357  loss_ce_2: 0.366  loss_mask_2: 0.4573  loss_dice_2: 3.329  loss_ce_3: 0.3469  loss_mask_3: 0.4573  loss_dice_3: 3.321  loss_ce_4: 0.329  loss_mask_4: 0.4592  loss_dice_4: 3.319  loss_ce_5: 0.3242  loss_mask_5: 0.4579  loss_dice_5: 3.327  loss_ce_6: 0.3233  loss_mask_6: 0.4606  loss_dice_6: 3.316  loss_ce_7: 0.3326  loss_mask_7: 0.4602  loss_dice_7: 3.322  loss_ce_8: 0.3378  loss_mask_8: 0.461  loss_dice_8: 3.318  time: 1.5342  data_time: 0.0975  lr: 8.8545e-06  max_mem: 21366M
[01/17 08:35:15] d2.utils.events INFO:  eta: 1 day, 9:27:20  iter: 11399  total_loss: 41.69  loss_ce: 0.3242  loss_mask: 0.4604  loss_dice: 3.29  loss_ce_0: 0.5888  loss_mask_0: 0.4284  loss_dice_0: 3.432  loss_ce_1: 0.3475  loss_mask_1: 0.4525  loss_dice_1: 3.343  loss_ce_2: 0.343  loss_mask_2: 0.4485  loss_dice_2: 3.313  loss_ce_3: 0.3547  loss_mask_3: 0.4483  loss_dice_3: 3.312  loss_ce_4: 0.3504  loss_mask_4: 0.4502  loss_dice_4: 3.309  loss_ce_5: 0.3402  loss_mask_5: 0.4555  loss_dice_5: 3.314  loss_ce_6: 0.3454  loss_mask_6: 0.4549  loss_dice_6: 3.294  loss_ce_7: 0.3352  loss_mask_7: 0.4523  loss_dice_7: 3.3  loss_ce_8: 0.3435  loss_mask_8: 0.454  loss_dice_8: 3.295  time: 1.5342  data_time: 0.0998  lr: 8.8525e-06  max_mem: 21366M
[01/17 08:35:45] d2.utils.events INFO:  eta: 1 day, 9:26:19  iter: 11419  total_loss: 41.45  loss_ce: 0.2989  loss_mask: 0.4642  loss_dice: 3.326  loss_ce_0: 0.5765  loss_mask_0: 0.4485  loss_dice_0: 3.448  loss_ce_1: 0.3359  loss_mask_1: 0.4661  loss_dice_1: 3.352  loss_ce_2: 0.3381  loss_mask_2: 0.4649  loss_dice_2: 3.34  loss_ce_3: 0.3152  loss_mask_3: 0.4649  loss_dice_3: 3.33  loss_ce_4: 0.3124  loss_mask_4: 0.4656  loss_dice_4: 3.337  loss_ce_5: 0.3106  loss_mask_5: 0.4633  loss_dice_5: 3.335  loss_ce_6: 0.3149  loss_mask_6: 0.4621  loss_dice_6: 3.328  loss_ce_7: 0.3034  loss_mask_7: 0.4632  loss_dice_7: 3.327  loss_ce_8: 0.3162  loss_mask_8: 0.4627  loss_dice_8: 3.328  time: 1.5342  data_time: 0.0868  lr: 8.8505e-06  max_mem: 21366M
[01/17 08:36:15] d2.utils.events INFO:  eta: 1 day, 9:25:58  iter: 11439  total_loss: 41.63  loss_ce: 0.3228  loss_mask: 0.4492  loss_dice: 3.32  loss_ce_0: 0.5939  loss_mask_0: 0.438  loss_dice_0: 3.447  loss_ce_1: 0.3621  loss_mask_1: 0.4523  loss_dice_1: 3.369  loss_ce_2: 0.3621  loss_mask_2: 0.4499  loss_dice_2: 3.35  loss_ce_3: 0.3312  loss_mask_3: 0.4491  loss_dice_3: 3.341  loss_ce_4: 0.3335  loss_mask_4: 0.4494  loss_dice_4: 3.339  loss_ce_5: 0.3049  loss_mask_5: 0.4507  loss_dice_5: 3.35  loss_ce_6: 0.3286  loss_mask_6: 0.4494  loss_dice_6: 3.332  loss_ce_7: 0.3282  loss_mask_7: 0.4499  loss_dice_7: 3.331  loss_ce_8: 0.316  loss_mask_8: 0.449  loss_dice_8: 3.324  time: 1.5342  data_time: 0.1088  lr: 8.8485e-06  max_mem: 21366M
[01/17 08:36:46] d2.utils.events INFO:  eta: 1 day, 9:25:01  iter: 11459  total_loss: 41.03  loss_ce: 0.3165  loss_mask: 0.4432  loss_dice: 3.279  loss_ce_0: 0.5824  loss_mask_0: 0.4322  loss_dice_0: 3.419  loss_ce_1: 0.3477  loss_mask_1: 0.4444  loss_dice_1: 3.342  loss_ce_2: 0.3465  loss_mask_2: 0.4421  loss_dice_2: 3.3  loss_ce_3: 0.3283  loss_mask_3: 0.4409  loss_dice_3: 3.292  loss_ce_4: 0.329  loss_mask_4: 0.4419  loss_dice_4: 3.292  loss_ce_5: 0.3231  loss_mask_5: 0.4428  loss_dice_5: 3.293  loss_ce_6: 0.3268  loss_mask_6: 0.4446  loss_dice_6: 3.287  loss_ce_7: 0.3196  loss_mask_7: 0.4446  loss_dice_7: 3.283  loss_ce_8: 0.3337  loss_mask_8: 0.4441  loss_dice_8: 3.287  time: 1.5342  data_time: 0.0954  lr: 8.8464e-06  max_mem: 21366M
[01/17 08:37:17] d2.utils.events INFO:  eta: 1 day, 9:24:58  iter: 11479  total_loss: 40.89  loss_ce: 0.3149  loss_mask: 0.4364  loss_dice: 3.279  loss_ce_0: 0.6156  loss_mask_0: 0.4221  loss_dice_0: 3.41  loss_ce_1: 0.3368  loss_mask_1: 0.4377  loss_dice_1: 3.319  loss_ce_2: 0.3377  loss_mask_2: 0.4391  loss_dice_2: 3.291  loss_ce_3: 0.3207  loss_mask_3: 0.4393  loss_dice_3: 3.29  loss_ce_4: 0.3205  loss_mask_4: 0.438  loss_dice_4: 3.291  loss_ce_5: 0.3134  loss_mask_5: 0.4358  loss_dice_5: 3.285  loss_ce_6: 0.3167  loss_mask_6: 0.4361  loss_dice_6: 3.283  loss_ce_7: 0.3115  loss_mask_7: 0.4365  loss_dice_7: 3.28  loss_ce_8: 0.3212  loss_mask_8: 0.4359  loss_dice_8: 3.283  time: 1.5342  data_time: 0.0849  lr: 8.8444e-06  max_mem: 21366M
[01/17 08:37:48] d2.utils.events INFO:  eta: 1 day, 9:24:57  iter: 11499  total_loss: 41.44  loss_ce: 0.3538  loss_mask: 0.4574  loss_dice: 3.301  loss_ce_0: 0.5903  loss_mask_0: 0.443  loss_dice_0: 3.444  loss_ce_1: 0.3433  loss_mask_1: 0.4597  loss_dice_1: 3.343  loss_ce_2: 0.3586  loss_mask_2: 0.4559  loss_dice_2: 3.317  loss_ce_3: 0.3579  loss_mask_3: 0.4556  loss_dice_3: 3.304  loss_ce_4: 0.3466  loss_mask_4: 0.4554  loss_dice_4: 3.303  loss_ce_5: 0.3307  loss_mask_5: 0.4555  loss_dice_5: 3.306  loss_ce_6: 0.3422  loss_mask_6: 0.4527  loss_dice_6: 3.304  loss_ce_7: 0.3589  loss_mask_7: 0.4529  loss_dice_7: 3.298  loss_ce_8: 0.3319  loss_mask_8: 0.4558  loss_dice_8: 3.302  time: 1.5342  data_time: 0.1013  lr: 8.8424e-06  max_mem: 21366M
[01/17 08:38:18] d2.utils.events INFO:  eta: 1 day, 9:24:26  iter: 11519  total_loss: 41.52  loss_ce: 0.3542  loss_mask: 0.4461  loss_dice: 3.283  loss_ce_0: 0.5897  loss_mask_0: 0.4357  loss_dice_0: 3.42  loss_ce_1: 0.3762  loss_mask_1: 0.4604  loss_dice_1: 3.309  loss_ce_2: 0.3776  loss_mask_2: 0.4549  loss_dice_2: 3.285  loss_ce_3: 0.3642  loss_mask_3: 0.4542  loss_dice_3: 3.28  loss_ce_4: 0.3539  loss_mask_4: 0.4477  loss_dice_4: 3.276  loss_ce_5: 0.3554  loss_mask_5: 0.448  loss_dice_5: 3.285  loss_ce_6: 0.374  loss_mask_6: 0.4486  loss_dice_6: 3.282  loss_ce_7: 0.3407  loss_mask_7: 0.4475  loss_dice_7: 3.282  loss_ce_8: 0.3489  loss_mask_8: 0.4463  loss_dice_8: 3.285  time: 1.5341  data_time: 0.0952  lr: 8.8404e-06  max_mem: 21366M
[01/17 08:38:49] d2.utils.events INFO:  eta: 1 day, 9:23:45  iter: 11539  total_loss: 40.9  loss_ce: 0.3235  loss_mask: 0.4547  loss_dice: 3.278  loss_ce_0: 0.5901  loss_mask_0: 0.4414  loss_dice_0: 3.403  loss_ce_1: 0.3361  loss_mask_1: 0.4588  loss_dice_1: 3.314  loss_ce_2: 0.3473  loss_mask_2: 0.4576  loss_dice_2: 3.284  loss_ce_3: 0.3327  loss_mask_3: 0.4582  loss_dice_3: 3.278  loss_ce_4: 0.319  loss_mask_4: 0.4575  loss_dice_4: 3.278  loss_ce_5: 0.3255  loss_mask_5: 0.4554  loss_dice_5: 3.283  loss_ce_6: 0.3102  loss_mask_6: 0.454  loss_dice_6: 3.28  loss_ce_7: 0.3234  loss_mask_7: 0.453  loss_dice_7: 3.273  loss_ce_8: 0.3341  loss_mask_8: 0.4538  loss_dice_8: 3.275  time: 1.5341  data_time: 0.0977  lr: 8.8383e-06  max_mem: 21366M
[01/17 08:39:19] d2.utils.events INFO:  eta: 1 day, 9:22:54  iter: 11559  total_loss: 40.6  loss_ce: 0.3145  loss_mask: 0.437  loss_dice: 3.255  loss_ce_0: 0.5926  loss_mask_0: 0.42  loss_dice_0: 3.382  loss_ce_1: 0.3378  loss_mask_1: 0.4359  loss_dice_1: 3.302  loss_ce_2: 0.3509  loss_mask_2: 0.4363  loss_dice_2: 3.277  loss_ce_3: 0.3423  loss_mask_3: 0.4335  loss_dice_3: 3.267  loss_ce_4: 0.3251  loss_mask_4: 0.4346  loss_dice_4: 3.258  loss_ce_5: 0.3102  loss_mask_5: 0.4367  loss_dice_5: 3.265  loss_ce_6: 0.3241  loss_mask_6: 0.4378  loss_dice_6: 3.258  loss_ce_7: 0.3225  loss_mask_7: 0.4374  loss_dice_7: 3.258  loss_ce_8: 0.3088  loss_mask_8: 0.4375  loss_dice_8: 3.25  time: 1.5341  data_time: 0.0953  lr: 8.8363e-06  max_mem: 21366M
[01/17 08:39:50] d2.utils.events INFO:  eta: 1 day, 9:22:25  iter: 11579  total_loss: 42.42  loss_ce: 0.3607  loss_mask: 0.4655  loss_dice: 3.337  loss_ce_0: 0.5966  loss_mask_0: 0.448  loss_dice_0: 3.443  loss_ce_1: 0.3411  loss_mask_1: 0.4732  loss_dice_1: 3.368  loss_ce_2: 0.3658  loss_mask_2: 0.4693  loss_dice_2: 3.343  loss_ce_3: 0.3495  loss_mask_3: 0.4632  loss_dice_3: 3.33  loss_ce_4: 0.3567  loss_mask_4: 0.4655  loss_dice_4: 3.333  loss_ce_5: 0.3477  loss_mask_5: 0.4642  loss_dice_5: 3.347  loss_ce_6: 0.3549  loss_mask_6: 0.4636  loss_dice_6: 3.331  loss_ce_7: 0.361  loss_mask_7: 0.4647  loss_dice_7: 3.341  loss_ce_8: 0.3667  loss_mask_8: 0.4655  loss_dice_8: 3.329  time: 1.5341  data_time: 0.1052  lr: 8.8343e-06  max_mem: 21366M
[01/17 08:40:21] d2.utils.events INFO:  eta: 1 day, 9:22:09  iter: 11599  total_loss: 41.1  loss_ce: 0.3113  loss_mask: 0.4307  loss_dice: 3.298  loss_ce_0: 0.5773  loss_mask_0: 0.419  loss_dice_0: 3.423  loss_ce_1: 0.3357  loss_mask_1: 0.4392  loss_dice_1: 3.322  loss_ce_2: 0.3363  loss_mask_2: 0.4334  loss_dice_2: 3.307  loss_ce_3: 0.3194  loss_mask_3: 0.4322  loss_dice_3: 3.298  loss_ce_4: 0.3242  loss_mask_4: 0.4318  loss_dice_4: 3.293  loss_ce_5: 0.3081  loss_mask_5: 0.4302  loss_dice_5: 3.289  loss_ce_6: 0.3145  loss_mask_6: 0.4322  loss_dice_6: 3.291  loss_ce_7: 0.3116  loss_mask_7: 0.4317  loss_dice_7: 3.298  loss_ce_8: 0.3083  loss_mask_8: 0.431  loss_dice_8: 3.29  time: 1.5342  data_time: 0.1007  lr: 8.8322e-06  max_mem: 21366M
[01/17 08:40:52] d2.utils.events INFO:  eta: 1 day, 9:21:22  iter: 11619  total_loss: 40.92  loss_ce: 0.3211  loss_mask: 0.4438  loss_dice: 3.259  loss_ce_0: 0.5702  loss_mask_0: 0.4328  loss_dice_0: 3.393  loss_ce_1: 0.3266  loss_mask_1: 0.4432  loss_dice_1: 3.295  loss_ce_2: 0.3375  loss_mask_2: 0.4414  loss_dice_2: 3.269  loss_ce_3: 0.318  loss_mask_3: 0.4432  loss_dice_3: 3.262  loss_ce_4: 0.3238  loss_mask_4: 0.4441  loss_dice_4: 3.249  loss_ce_5: 0.3184  loss_mask_5: 0.445  loss_dice_5: 3.265  loss_ce_6: 0.3258  loss_mask_6: 0.4433  loss_dice_6: 3.256  loss_ce_7: 0.3224  loss_mask_7: 0.4429  loss_dice_7: 3.252  loss_ce_8: 0.3101  loss_mask_8: 0.445  loss_dice_8: 3.259  time: 1.5342  data_time: 0.0996  lr: 8.8302e-06  max_mem: 21366M
[01/17 08:41:22] d2.utils.events INFO:  eta: 1 day, 9:20:25  iter: 11639  total_loss: 41.76  loss_ce: 0.3213  loss_mask: 0.4495  loss_dice: 3.313  loss_ce_0: 0.5975  loss_mask_0: 0.4368  loss_dice_0: 3.445  loss_ce_1: 0.3435  loss_mask_1: 0.4512  loss_dice_1: 3.368  loss_ce_2: 0.3537  loss_mask_2: 0.4468  loss_dice_2: 3.339  loss_ce_3: 0.3523  loss_mask_3: 0.4464  loss_dice_3: 3.322  loss_ce_4: 0.3388  loss_mask_4: 0.446  loss_dice_4: 3.324  loss_ce_5: 0.3365  loss_mask_5: 0.4443  loss_dice_5: 3.323  loss_ce_6: 0.3344  loss_mask_6: 0.4436  loss_dice_6: 3.318  loss_ce_7: 0.3316  loss_mask_7: 0.4468  loss_dice_7: 3.313  loss_ce_8: 0.3128  loss_mask_8: 0.4478  loss_dice_8: 3.31  time: 1.5342  data_time: 0.0972  lr: 8.8282e-06  max_mem: 21366M
[01/17 08:41:53] d2.utils.events INFO:  eta: 1 day, 9:20:14  iter: 11659  total_loss: 40.89  loss_ce: 0.3263  loss_mask: 0.4516  loss_dice: 3.275  loss_ce_0: 0.583  loss_mask_0: 0.4298  loss_dice_0: 3.397  loss_ce_1: 0.3505  loss_mask_1: 0.4505  loss_dice_1: 3.313  loss_ce_2: 0.3595  loss_mask_2: 0.4493  loss_dice_2: 3.288  loss_ce_3: 0.3351  loss_mask_3: 0.4494  loss_dice_3: 3.27  loss_ce_4: 0.3325  loss_mask_4: 0.4564  loss_dice_4: 3.276  loss_ce_5: 0.3256  loss_mask_5: 0.453  loss_dice_5: 3.277  loss_ce_6: 0.3197  loss_mask_6: 0.4525  loss_dice_6: 3.277  loss_ce_7: 0.3276  loss_mask_7: 0.4551  loss_dice_7: 3.275  loss_ce_8: 0.3339  loss_mask_8: 0.451  loss_dice_8: 3.271  time: 1.5341  data_time: 0.1045  lr: 8.8262e-06  max_mem: 21366M
[01/17 08:42:23] d2.utils.events INFO:  eta: 1 day, 9:19:36  iter: 11679  total_loss: 40.72  loss_ce: 0.3545  loss_mask: 0.4387  loss_dice: 3.226  loss_ce_0: 0.6133  loss_mask_0: 0.4236  loss_dice_0: 3.344  loss_ce_1: 0.3764  loss_mask_1: 0.4406  loss_dice_1: 3.26  loss_ce_2: 0.375  loss_mask_2: 0.4383  loss_dice_2: 3.243  loss_ce_3: 0.3677  loss_mask_3: 0.438  loss_dice_3: 3.221  loss_ce_4: 0.3591  loss_mask_4: 0.439  loss_dice_4: 3.227  loss_ce_5: 0.3479  loss_mask_5: 0.4371  loss_dice_5: 3.235  loss_ce_6: 0.3556  loss_mask_6: 0.4376  loss_dice_6: 3.23  loss_ce_7: 0.3409  loss_mask_7: 0.4378  loss_dice_7: 3.23  loss_ce_8: 0.3556  loss_mask_8: 0.4383  loss_dice_8: 3.224  time: 1.5341  data_time: 0.0971  lr: 8.8241e-06  max_mem: 21366M
[01/17 08:42:54] d2.utils.events INFO:  eta: 1 day, 9:19:05  iter: 11699  total_loss: 41.6  loss_ce: 0.3245  loss_mask: 0.455  loss_dice: 3.282  loss_ce_0: 0.6072  loss_mask_0: 0.4355  loss_dice_0: 3.409  loss_ce_1: 0.3382  loss_mask_1: 0.4506  loss_dice_1: 3.322  loss_ce_2: 0.3454  loss_mask_2: 0.4509  loss_dice_2: 3.291  loss_ce_3: 0.3406  loss_mask_3: 0.454  loss_dice_3: 3.281  loss_ce_4: 0.3443  loss_mask_4: 0.4528  loss_dice_4: 3.283  loss_ce_5: 0.3319  loss_mask_5: 0.4563  loss_dice_5: 3.273  loss_ce_6: 0.3262  loss_mask_6: 0.4543  loss_dice_6: 3.278  loss_ce_7: 0.3242  loss_mask_7: 0.4541  loss_dice_7: 3.278  loss_ce_8: 0.3241  loss_mask_8: 0.4546  loss_dice_8: 3.273  time: 1.5341  data_time: 0.1033  lr: 8.8221e-06  max_mem: 21366M
[01/17 08:43:25] d2.utils.events INFO:  eta: 1 day, 9:18:42  iter: 11719  total_loss: 41.04  loss_ce: 0.3219  loss_mask: 0.445  loss_dice: 3.279  loss_ce_0: 0.5764  loss_mask_0: 0.4214  loss_dice_0: 3.398  loss_ce_1: 0.3518  loss_mask_1: 0.4477  loss_dice_1: 3.307  loss_ce_2: 0.3575  loss_mask_2: 0.4474  loss_dice_2: 3.281  loss_ce_3: 0.3473  loss_mask_3: 0.4473  loss_dice_3: 3.268  loss_ce_4: 0.3469  loss_mask_4: 0.4461  loss_dice_4: 3.274  loss_ce_5: 0.331  loss_mask_5: 0.4467  loss_dice_5: 3.274  loss_ce_6: 0.3325  loss_mask_6: 0.4458  loss_dice_6: 3.268  loss_ce_7: 0.3342  loss_mask_7: 0.4447  loss_dice_7: 3.273  loss_ce_8: 0.3198  loss_mask_8: 0.446  loss_dice_8: 3.28  time: 1.5341  data_time: 0.1001  lr: 8.8201e-06  max_mem: 21366M
[01/17 08:43:56] d2.utils.events INFO:  eta: 1 day, 9:18:04  iter: 11739  total_loss: 41.13  loss_ce: 0.3238  loss_mask: 0.4386  loss_dice: 3.323  loss_ce_0: 0.5945  loss_mask_0: 0.4342  loss_dice_0: 3.45  loss_ce_1: 0.3399  loss_mask_1: 0.4436  loss_dice_1: 3.368  loss_ce_2: 0.3433  loss_mask_2: 0.4397  loss_dice_2: 3.341  loss_ce_3: 0.3377  loss_mask_3: 0.4365  loss_dice_3: 3.334  loss_ce_4: 0.3278  loss_mask_4: 0.4388  loss_dice_4: 3.33  loss_ce_5: 0.3273  loss_mask_5: 0.4375  loss_dice_5: 3.334  loss_ce_6: 0.3151  loss_mask_6: 0.4369  loss_dice_6: 3.323  loss_ce_7: 0.3177  loss_mask_7: 0.4375  loss_dice_7: 3.324  loss_ce_8: 0.3107  loss_mask_8: 0.4358  loss_dice_8: 3.326  time: 1.5341  data_time: 0.0952  lr: 8.8181e-06  max_mem: 21366M
[01/17 08:44:26] d2.utils.events INFO:  eta: 1 day, 9:14:45  iter: 11759  total_loss: 41.73  loss_ce: 0.3259  loss_mask: 0.4523  loss_dice: 3.315  loss_ce_0: 0.6041  loss_mask_0: 0.4364  loss_dice_0: 3.437  loss_ce_1: 0.3435  loss_mask_1: 0.459  loss_dice_1: 3.357  loss_ce_2: 0.3576  loss_mask_2: 0.4533  loss_dice_2: 3.342  loss_ce_3: 0.3439  loss_mask_3: 0.4464  loss_dice_3: 3.336  loss_ce_4: 0.3464  loss_mask_4: 0.4417  loss_dice_4: 3.338  loss_ce_5: 0.347  loss_mask_5: 0.4413  loss_dice_5: 3.337  loss_ce_6: 0.3265  loss_mask_6: 0.4441  loss_dice_6: 3.335  loss_ce_7: 0.34  loss_mask_7: 0.4485  loss_dice_7: 3.332  loss_ce_8: 0.3339  loss_mask_8: 0.4496  loss_dice_8: 3.331  time: 1.5341  data_time: 0.0851  lr: 8.816e-06  max_mem: 21366M
[01/17 08:44:57] d2.utils.events INFO:  eta: 1 day, 9:14:15  iter: 11779  total_loss: 41.06  loss_ce: 0.3652  loss_mask: 0.427  loss_dice: 3.256  loss_ce_0: 0.5983  loss_mask_0: 0.4305  loss_dice_0: 3.393  loss_ce_1: 0.3749  loss_mask_1: 0.4358  loss_dice_1: 3.292  loss_ce_2: 0.3817  loss_mask_2: 0.4293  loss_dice_2: 3.277  loss_ce_3: 0.3582  loss_mask_3: 0.4257  loss_dice_3: 3.268  loss_ce_4: 0.3601  loss_mask_4: 0.426  loss_dice_4: 3.26  loss_ce_5: 0.3596  loss_mask_5: 0.4261  loss_dice_5: 3.268  loss_ce_6: 0.347  loss_mask_6: 0.4287  loss_dice_6: 3.269  loss_ce_7: 0.3499  loss_mask_7: 0.4278  loss_dice_7: 3.259  loss_ce_8: 0.3566  loss_mask_8: 0.427  loss_dice_8: 3.26  time: 1.5341  data_time: 0.0975  lr: 8.814e-06  max_mem: 21366M
[01/17 08:45:28] d2.utils.events INFO:  eta: 1 day, 9:13:44  iter: 11799  total_loss: 41.22  loss_ce: 0.3073  loss_mask: 0.4468  loss_dice: 3.285  loss_ce_0: 0.6162  loss_mask_0: 0.4365  loss_dice_0: 3.396  loss_ce_1: 0.35  loss_mask_1: 0.456  loss_dice_1: 3.306  loss_ce_2: 0.3209  loss_mask_2: 0.454  loss_dice_2: 3.293  loss_ce_3: 0.3239  loss_mask_3: 0.4534  loss_dice_3: 3.287  loss_ce_4: 0.3251  loss_mask_4: 0.4514  loss_dice_4: 3.278  loss_ce_5: 0.3013  loss_mask_5: 0.4523  loss_dice_5: 3.285  loss_ce_6: 0.306  loss_mask_6: 0.4518  loss_dice_6: 3.284  loss_ce_7: 0.3095  loss_mask_7: 0.4501  loss_dice_7: 3.282  loss_ce_8: 0.2974  loss_mask_8: 0.449  loss_dice_8: 3.284  time: 1.5341  data_time: 0.1108  lr: 8.812e-06  max_mem: 21366M
[01/17 08:45:59] d2.utils.events INFO:  eta: 1 day, 9:13:30  iter: 11819  total_loss: 41.62  loss_ce: 0.3294  loss_mask: 0.448  loss_dice: 3.293  loss_ce_0: 0.5994  loss_mask_0: 0.431  loss_dice_0: 3.426  loss_ce_1: 0.3647  loss_mask_1: 0.4498  loss_dice_1: 3.338  loss_ce_2: 0.3514  loss_mask_2: 0.4532  loss_dice_2: 3.31  loss_ce_3: 0.3442  loss_mask_3: 0.4472  loss_dice_3: 3.309  loss_ce_4: 0.3444  loss_mask_4: 0.4481  loss_dice_4: 3.301  loss_ce_5: 0.3353  loss_mask_5: 0.4498  loss_dice_5: 3.308  loss_ce_6: 0.3488  loss_mask_6: 0.4481  loss_dice_6: 3.309  loss_ce_7: 0.3347  loss_mask_7: 0.4469  loss_dice_7: 3.295  loss_ce_8: 0.3217  loss_mask_8: 0.4473  loss_dice_8: 3.295  time: 1.5342  data_time: 0.0958  lr: 8.8099e-06  max_mem: 21366M
[01/17 08:46:32] d2.utils.events INFO:  eta: 1 day, 9:16:06  iter: 11839  total_loss: 41.13  loss_ce: 0.333  loss_mask: 0.445  loss_dice: 3.29  loss_ce_0: 0.6231  loss_mask_0: 0.4264  loss_dice_0: 3.424  loss_ce_1: 0.3567  loss_mask_1: 0.4398  loss_dice_1: 3.326  loss_ce_2: 0.3668  loss_mask_2: 0.4436  loss_dice_2: 3.305  loss_ce_3: 0.3459  loss_mask_3: 0.4476  loss_dice_3: 3.296  loss_ce_4: 0.3288  loss_mask_4: 0.4484  loss_dice_4: 3.29  loss_ce_5: 0.3347  loss_mask_5: 0.4495  loss_dice_5: 3.299  loss_ce_6: 0.3289  loss_mask_6: 0.4506  loss_dice_6: 3.294  loss_ce_7: 0.335  loss_mask_7: 0.4476  loss_dice_7: 3.289  loss_ce_8: 0.3258  loss_mask_8: 0.4464  loss_dice_8: 3.286  time: 1.5343  data_time: 0.1224  lr: 8.8079e-06  max_mem: 21366M
[01/17 08:47:03] d2.utils.events INFO:  eta: 1 day, 9:16:09  iter: 11859  total_loss: 40.95  loss_ce: 0.3273  loss_mask: 0.4278  loss_dice: 3.26  loss_ce_0: 0.5596  loss_mask_0: 0.4156  loss_dice_0: 3.395  loss_ce_1: 0.3389  loss_mask_1: 0.4314  loss_dice_1: 3.294  loss_ce_2: 0.3359  loss_mask_2: 0.4323  loss_dice_2: 3.275  loss_ce_3: 0.3427  loss_mask_3: 0.4305  loss_dice_3: 3.258  loss_ce_4: 0.3278  loss_mask_4: 0.4285  loss_dice_4: 3.259  loss_ce_5: 0.3191  loss_mask_5: 0.4295  loss_dice_5: 3.26  loss_ce_6: 0.3181  loss_mask_6: 0.4286  loss_dice_6: 3.263  loss_ce_7: 0.3358  loss_mask_7: 0.4292  loss_dice_7: 3.256  loss_ce_8: 0.323  loss_mask_8: 0.4278  loss_dice_8: 3.258  time: 1.5343  data_time: 0.1108  lr: 8.8059e-06  max_mem: 21366M
[01/17 08:47:34] d2.utils.events INFO:  eta: 1 day, 9:17:28  iter: 11879  total_loss: 40.98  loss_ce: 0.3354  loss_mask: 0.444  loss_dice: 3.287  loss_ce_0: 0.5995  loss_mask_0: 0.4325  loss_dice_0: 3.421  loss_ce_1: 0.3353  loss_mask_1: 0.4497  loss_dice_1: 3.323  loss_ce_2: 0.3568  loss_mask_2: 0.4476  loss_dice_2: 3.302  loss_ce_3: 0.326  loss_mask_3: 0.4449  loss_dice_3: 3.29  loss_ce_4: 0.3295  loss_mask_4: 0.4435  loss_dice_4: 3.288  loss_ce_5: 0.3407  loss_mask_5: 0.4448  loss_dice_5: 3.294  loss_ce_6: 0.3368  loss_mask_6: 0.4436  loss_dice_6: 3.282  loss_ce_7: 0.3263  loss_mask_7: 0.4438  loss_dice_7: 3.283  loss_ce_8: 0.3415  loss_mask_8: 0.4442  loss_dice_8: 3.284  time: 1.5344  data_time: 0.1035  lr: 8.8039e-06  max_mem: 21366M
[01/17 08:48:06] d2.utils.events INFO:  eta: 1 day, 9:17:41  iter: 11899  total_loss: 41.27  loss_ce: 0.333  loss_mask: 0.4518  loss_dice: 3.268  loss_ce_0: 0.5997  loss_mask_0: 0.4392  loss_dice_0: 3.371  loss_ce_1: 0.334  loss_mask_1: 0.4556  loss_dice_1: 3.301  loss_ce_2: 0.3527  loss_mask_2: 0.4497  loss_dice_2: 3.28  loss_ce_3: 0.3382  loss_mask_3: 0.4483  loss_dice_3: 3.273  loss_ce_4: 0.334  loss_mask_4: 0.45  loss_dice_4: 3.266  loss_ce_5: 0.3259  loss_mask_5: 0.4487  loss_dice_5: 3.276  loss_ce_6: 0.3197  loss_mask_6: 0.447  loss_dice_6: 3.271  loss_ce_7: 0.3421  loss_mask_7: 0.4483  loss_dice_7: 3.264  loss_ce_8: 0.3531  loss_mask_8: 0.4496  loss_dice_8: 3.263  time: 1.5345  data_time: 0.1046  lr: 8.8018e-06  max_mem: 21366M
[01/17 08:48:37] d2.utils.events INFO:  eta: 1 day, 9:15:40  iter: 11919  total_loss: 40.37  loss_ce: 0.319  loss_mask: 0.4426  loss_dice: 3.23  loss_ce_0: 0.6083  loss_mask_0: 0.4345  loss_dice_0: 3.351  loss_ce_1: 0.3543  loss_mask_1: 0.4506  loss_dice_1: 3.277  loss_ce_2: 0.3533  loss_mask_2: 0.4491  loss_dice_2: 3.244  loss_ce_3: 0.3331  loss_mask_3: 0.4458  loss_dice_3: 3.229  loss_ce_4: 0.3397  loss_mask_4: 0.4457  loss_dice_4: 3.232  loss_ce_5: 0.3401  loss_mask_5: 0.4478  loss_dice_5: 3.241  loss_ce_6: 0.3406  loss_mask_6: 0.4461  loss_dice_6: 3.233  loss_ce_7: 0.3239  loss_mask_7: 0.4457  loss_dice_7: 3.228  loss_ce_8: 0.3257  loss_mask_8: 0.4425  loss_dice_8: 3.226  time: 1.5345  data_time: 0.0978  lr: 8.7998e-06  max_mem: 21366M
[01/17 08:49:07] d2.utils.events INFO:  eta: 1 day, 9:15:36  iter: 11939  total_loss: 40.27  loss_ce: 0.32  loss_mask: 0.4402  loss_dice: 3.213  loss_ce_0: 0.5795  loss_mask_0: 0.4267  loss_dice_0: 3.32  loss_ce_1: 0.3238  loss_mask_1: 0.4436  loss_dice_1: 3.25  loss_ce_2: 0.3282  loss_mask_2: 0.4433  loss_dice_2: 3.22  loss_ce_3: 0.3307  loss_mask_3: 0.4432  loss_dice_3: 3.222  loss_ce_4: 0.3258  loss_mask_4: 0.4423  loss_dice_4: 3.21  loss_ce_5: 0.329  loss_mask_5: 0.442  loss_dice_5: 3.221  loss_ce_6: 0.324  loss_mask_6: 0.4388  loss_dice_6: 3.212  loss_ce_7: 0.328  loss_mask_7: 0.4414  loss_dice_7: 3.207  loss_ce_8: 0.3176  loss_mask_8: 0.4401  loss_dice_8: 3.213  time: 1.5344  data_time: 0.0994  lr: 8.7978e-06  max_mem: 21366M
[01/17 08:49:38] d2.utils.events INFO:  eta: 1 day, 9:13:36  iter: 11959  total_loss: 40.94  loss_ce: 0.3458  loss_mask: 0.4405  loss_dice: 3.248  loss_ce_0: 0.5861  loss_mask_0: 0.43  loss_dice_0: 3.372  loss_ce_1: 0.3655  loss_mask_1: 0.4436  loss_dice_1: 3.292  loss_ce_2: 0.3723  loss_mask_2: 0.4439  loss_dice_2: 3.257  loss_ce_3: 0.3605  loss_mask_3: 0.4437  loss_dice_3: 3.253  loss_ce_4: 0.3675  loss_mask_4: 0.4416  loss_dice_4: 3.246  loss_ce_5: 0.3415  loss_mask_5: 0.4415  loss_dice_5: 3.255  loss_ce_6: 0.3451  loss_mask_6: 0.4394  loss_dice_6: 3.254  loss_ce_7: 0.3562  loss_mask_7: 0.4402  loss_dice_7: 3.249  loss_ce_8: 0.3511  loss_mask_8: 0.4407  loss_dice_8: 3.24  time: 1.5344  data_time: 0.0954  lr: 8.7957e-06  max_mem: 21366M
[01/17 08:50:09] d2.utils.events INFO:  eta: 1 day, 9:12:53  iter: 11979  total_loss: 41.22  loss_ce: 0.3197  loss_mask: 0.4429  loss_dice: 3.329  loss_ce_0: 0.6273  loss_mask_0: 0.4211  loss_dice_0: 3.445  loss_ce_1: 0.3206  loss_mask_1: 0.4329  loss_dice_1: 3.369  loss_ce_2: 0.3458  loss_mask_2: 0.4396  loss_dice_2: 3.35  loss_ce_3: 0.328  loss_mask_3: 0.4392  loss_dice_3: 3.334  loss_ce_4: 0.3231  loss_mask_4: 0.4385  loss_dice_4: 3.333  loss_ce_5: 0.3169  loss_mask_5: 0.4389  loss_dice_5: 3.33  loss_ce_6: 0.3192  loss_mask_6: 0.4388  loss_dice_6: 3.336  loss_ce_7: 0.3226  loss_mask_7: 0.4413  loss_dice_7: 3.331  loss_ce_8: 0.3227  loss_mask_8: 0.4429  loss_dice_8: 3.326  time: 1.5345  data_time: 0.0975  lr: 8.7937e-06  max_mem: 21366M
[01/17 08:50:39] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 08:50:40] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 08:50:40] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 08:50:41] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 08:50:55] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0086 s/iter. Inference: 0.1851 s/iter. Eval: 0.2202 s/iter. Total: 0.4139 s/iter. ETA=0:07:27
[01/17 08:51:00] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0113 s/iter. Inference: 0.1835 s/iter. Eval: 0.2364 s/iter. Total: 0.4313 s/iter. ETA=0:07:41
[01/17 08:51:05] d2.evaluation.evaluator INFO: Inference done 37/1093. Dataloading: 0.0103 s/iter. Inference: 0.1732 s/iter. Eval: 0.2168 s/iter. Total: 0.4004 s/iter. ETA=0:07:02
[01/17 08:51:10] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0108 s/iter. Inference: 0.1747 s/iter. Eval: 0.2132 s/iter. Total: 0.3988 s/iter. ETA=0:06:55
[01/17 08:51:15] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0114 s/iter. Inference: 0.1721 s/iter. Eval: 0.2213 s/iter. Total: 0.4050 s/iter. ETA=0:06:57
[01/17 08:51:20] d2.evaluation.evaluator INFO: Inference done 76/1093. Dataloading: 0.0114 s/iter. Inference: 0.1714 s/iter. Eval: 0.2162 s/iter. Total: 0.3991 s/iter. ETA=0:06:45
[01/17 08:51:26] d2.evaluation.evaluator INFO: Inference done 87/1093. Dataloading: 0.0116 s/iter. Inference: 0.1741 s/iter. Eval: 0.2244 s/iter. Total: 0.4102 s/iter. ETA=0:06:52
[01/17 08:51:31] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0117 s/iter. Inference: 0.1736 s/iter. Eval: 0.2231 s/iter. Total: 0.4085 s/iter. ETA=0:06:45
[01/17 08:51:36] d2.evaluation.evaluator INFO: Inference done 113/1093. Dataloading: 0.0115 s/iter. Inference: 0.1749 s/iter. Eval: 0.2214 s/iter. Total: 0.4079 s/iter. ETA=0:06:39
[01/17 08:51:41] d2.evaluation.evaluator INFO: Inference done 126/1093. Dataloading: 0.0115 s/iter. Inference: 0.1750 s/iter. Eval: 0.2200 s/iter. Total: 0.4066 s/iter. ETA=0:06:33
[01/17 08:51:46] d2.evaluation.evaluator INFO: Inference done 139/1093. Dataloading: 0.0116 s/iter. Inference: 0.1750 s/iter. Eval: 0.2185 s/iter. Total: 0.4052 s/iter. ETA=0:06:26
[01/17 08:51:52] d2.evaluation.evaluator INFO: Inference done 154/1093. Dataloading: 0.0115 s/iter. Inference: 0.1752 s/iter. Eval: 0.2134 s/iter. Total: 0.4001 s/iter. ETA=0:06:15
[01/17 08:51:57] d2.evaluation.evaluator INFO: Inference done 167/1093. Dataloading: 0.0115 s/iter. Inference: 0.1742 s/iter. Eval: 0.2155 s/iter. Total: 0.4013 s/iter. ETA=0:06:11
[01/17 08:52:02] d2.evaluation.evaluator INFO: Inference done 180/1093. Dataloading: 0.0115 s/iter. Inference: 0.1736 s/iter. Eval: 0.2150 s/iter. Total: 0.4003 s/iter. ETA=0:06:05
[01/17 08:52:07] d2.evaluation.evaluator INFO: Inference done 194/1093. Dataloading: 0.0115 s/iter. Inference: 0.1733 s/iter. Eval: 0.2129 s/iter. Total: 0.3978 s/iter. ETA=0:05:57
[01/17 08:52:12] d2.evaluation.evaluator INFO: Inference done 208/1093. Dataloading: 0.0114 s/iter. Inference: 0.1728 s/iter. Eval: 0.2115 s/iter. Total: 0.3958 s/iter. ETA=0:05:50
[01/17 08:52:18] d2.evaluation.evaluator INFO: Inference done 222/1093. Dataloading: 0.0115 s/iter. Inference: 0.1718 s/iter. Eval: 0.2113 s/iter. Total: 0.3947 s/iter. ETA=0:05:43
[01/17 08:52:23] d2.evaluation.evaluator INFO: Inference done 234/1093. Dataloading: 0.0116 s/iter. Inference: 0.1724 s/iter. Eval: 0.2131 s/iter. Total: 0.3972 s/iter. ETA=0:05:41
[01/17 08:52:28] d2.evaluation.evaluator INFO: Inference done 247/1093. Dataloading: 0.0117 s/iter. Inference: 0.1721 s/iter. Eval: 0.2137 s/iter. Total: 0.3976 s/iter. ETA=0:05:36
[01/17 08:52:33] d2.evaluation.evaluator INFO: Inference done 260/1093. Dataloading: 0.0117 s/iter. Inference: 0.1714 s/iter. Eval: 0.2145 s/iter. Total: 0.3977 s/iter. ETA=0:05:31
[01/17 08:52:39] d2.evaluation.evaluator INFO: Inference done 274/1093. Dataloading: 0.0117 s/iter. Inference: 0.1708 s/iter. Eval: 0.2143 s/iter. Total: 0.3970 s/iter. ETA=0:05:25
[01/17 08:52:44] d2.evaluation.evaluator INFO: Inference done 289/1093. Dataloading: 0.0115 s/iter. Inference: 0.1708 s/iter. Eval: 0.2120 s/iter. Total: 0.3944 s/iter. ETA=0:05:17
[01/17 08:52:49] d2.evaluation.evaluator INFO: Inference done 302/1093. Dataloading: 0.0117 s/iter. Inference: 0.1706 s/iter. Eval: 0.2124 s/iter. Total: 0.3948 s/iter. ETA=0:05:12
[01/17 08:52:55] d2.evaluation.evaluator INFO: Inference done 314/1093. Dataloading: 0.0117 s/iter. Inference: 0.1714 s/iter. Eval: 0.2142 s/iter. Total: 0.3974 s/iter. ETA=0:05:09
[01/17 08:53:00] d2.evaluation.evaluator INFO: Inference done 327/1093. Dataloading: 0.0117 s/iter. Inference: 0.1706 s/iter. Eval: 0.2147 s/iter. Total: 0.3971 s/iter. ETA=0:05:04
[01/17 08:53:05] d2.evaluation.evaluator INFO: Inference done 342/1093. Dataloading: 0.0116 s/iter. Inference: 0.1708 s/iter. Eval: 0.2127 s/iter. Total: 0.3952 s/iter. ETA=0:04:56
[01/17 08:53:10] d2.evaluation.evaluator INFO: Inference done 356/1093. Dataloading: 0.0116 s/iter. Inference: 0.1705 s/iter. Eval: 0.2118 s/iter. Total: 0.3940 s/iter. ETA=0:04:50
[01/17 08:53:15] d2.evaluation.evaluator INFO: Inference done 369/1093. Dataloading: 0.0117 s/iter. Inference: 0.1701 s/iter. Eval: 0.2121 s/iter. Total: 0.3940 s/iter. ETA=0:04:45
[01/17 08:53:21] d2.evaluation.evaluator INFO: Inference done 383/1093. Dataloading: 0.0116 s/iter. Inference: 0.1702 s/iter. Eval: 0.2108 s/iter. Total: 0.3927 s/iter. ETA=0:04:38
[01/17 08:53:26] d2.evaluation.evaluator INFO: Inference done 396/1093. Dataloading: 0.0116 s/iter. Inference: 0.1699 s/iter. Eval: 0.2112 s/iter. Total: 0.3928 s/iter. ETA=0:04:33
[01/17 08:53:31] d2.evaluation.evaluator INFO: Inference done 411/1093. Dataloading: 0.0115 s/iter. Inference: 0.1692 s/iter. Eval: 0.2110 s/iter. Total: 0.3918 s/iter. ETA=0:04:27
[01/17 08:53:36] d2.evaluation.evaluator INFO: Inference done 422/1093. Dataloading: 0.0116 s/iter. Inference: 0.1694 s/iter. Eval: 0.2127 s/iter. Total: 0.3937 s/iter. ETA=0:04:24
[01/17 08:53:41] d2.evaluation.evaluator INFO: Inference done 434/1093. Dataloading: 0.0116 s/iter. Inference: 0.1696 s/iter. Eval: 0.2134 s/iter. Total: 0.3947 s/iter. ETA=0:04:20
[01/17 08:53:47] d2.evaluation.evaluator INFO: Inference done 448/1093. Dataloading: 0.0116 s/iter. Inference: 0.1700 s/iter. Eval: 0.2124 s/iter. Total: 0.3941 s/iter. ETA=0:04:14
[01/17 08:53:52] d2.evaluation.evaluator INFO: Inference done 460/1093. Dataloading: 0.0116 s/iter. Inference: 0.1701 s/iter. Eval: 0.2130 s/iter. Total: 0.3949 s/iter. ETA=0:04:09
[01/17 08:53:57] d2.evaluation.evaluator INFO: Inference done 475/1093. Dataloading: 0.0117 s/iter. Inference: 0.1698 s/iter. Eval: 0.2119 s/iter. Total: 0.3935 s/iter. ETA=0:04:03
[01/17 08:54:02] d2.evaluation.evaluator INFO: Inference done 489/1093. Dataloading: 0.0116 s/iter. Inference: 0.1696 s/iter. Eval: 0.2115 s/iter. Total: 0.3928 s/iter. ETA=0:03:57
[01/17 08:54:07] d2.evaluation.evaluator INFO: Inference done 504/1093. Dataloading: 0.0116 s/iter. Inference: 0.1697 s/iter. Eval: 0.2101 s/iter. Total: 0.3915 s/iter. ETA=0:03:50
[01/17 08:54:13] d2.evaluation.evaluator INFO: Inference done 517/1093. Dataloading: 0.0116 s/iter. Inference: 0.1704 s/iter. Eval: 0.2098 s/iter. Total: 0.3919 s/iter. ETA=0:03:45
[01/17 08:54:18] d2.evaluation.evaluator INFO: Inference done 529/1093. Dataloading: 0.0116 s/iter. Inference: 0.1704 s/iter. Eval: 0.2107 s/iter. Total: 0.3929 s/iter. ETA=0:03:41
[01/17 08:54:23] d2.evaluation.evaluator INFO: Inference done 545/1093. Dataloading: 0.0116 s/iter. Inference: 0.1699 s/iter. Eval: 0.2095 s/iter. Total: 0.3911 s/iter. ETA=0:03:34
[01/17 08:54:28] d2.evaluation.evaluator INFO: Inference done 558/1093. Dataloading: 0.0116 s/iter. Inference: 0.1696 s/iter. Eval: 0.2100 s/iter. Total: 0.3913 s/iter. ETA=0:03:29
[01/17 08:54:34] d2.evaluation.evaluator INFO: Inference done 572/1093. Dataloading: 0.0115 s/iter. Inference: 0.1698 s/iter. Eval: 0.2093 s/iter. Total: 0.3907 s/iter. ETA=0:03:23
[01/17 08:54:39] d2.evaluation.evaluator INFO: Inference done 587/1093. Dataloading: 0.0116 s/iter. Inference: 0.1703 s/iter. Eval: 0.2076 s/iter. Total: 0.3897 s/iter. ETA=0:03:17
[01/17 08:54:44] d2.evaluation.evaluator INFO: Inference done 599/1093. Dataloading: 0.0116 s/iter. Inference: 0.1705 s/iter. Eval: 0.2086 s/iter. Total: 0.3908 s/iter. ETA=0:03:13
[01/17 08:54:49] d2.evaluation.evaluator INFO: Inference done 612/1093. Dataloading: 0.0117 s/iter. Inference: 0.1705 s/iter. Eval: 0.2087 s/iter. Total: 0.3910 s/iter. ETA=0:03:08
[01/17 08:54:54] d2.evaluation.evaluator INFO: Inference done 626/1093. Dataloading: 0.0117 s/iter. Inference: 0.1700 s/iter. Eval: 0.2085 s/iter. Total: 0.3904 s/iter. ETA=0:03:02
[01/17 08:55:00] d2.evaluation.evaluator INFO: Inference done 640/1093. Dataloading: 0.0117 s/iter. Inference: 0.1700 s/iter. Eval: 0.2081 s/iter. Total: 0.3899 s/iter. ETA=0:02:56
[01/17 08:55:05] d2.evaluation.evaluator INFO: Inference done 653/1093. Dataloading: 0.0117 s/iter. Inference: 0.1702 s/iter. Eval: 0.2078 s/iter. Total: 0.3899 s/iter. ETA=0:02:51
[01/17 08:55:10] d2.evaluation.evaluator INFO: Inference done 666/1093. Dataloading: 0.0117 s/iter. Inference: 0.1702 s/iter. Eval: 0.2084 s/iter. Total: 0.3905 s/iter. ETA=0:02:46
[01/17 08:55:15] d2.evaluation.evaluator INFO: Inference done 680/1093. Dataloading: 0.0117 s/iter. Inference: 0.1702 s/iter. Eval: 0.2078 s/iter. Total: 0.3898 s/iter. ETA=0:02:41
[01/17 08:55:20] d2.evaluation.evaluator INFO: Inference done 694/1093. Dataloading: 0.0117 s/iter. Inference: 0.1706 s/iter. Eval: 0.2070 s/iter. Total: 0.3894 s/iter. ETA=0:02:35
[01/17 08:55:26] d2.evaluation.evaluator INFO: Inference done 707/1093. Dataloading: 0.0117 s/iter. Inference: 0.1705 s/iter. Eval: 0.2072 s/iter. Total: 0.3895 s/iter. ETA=0:02:30
[01/17 08:55:31] d2.evaluation.evaluator INFO: Inference done 719/1093. Dataloading: 0.0117 s/iter. Inference: 0.1705 s/iter. Eval: 0.2076 s/iter. Total: 0.3900 s/iter. ETA=0:02:25
[01/17 08:55:36] d2.evaluation.evaluator INFO: Inference done 735/1093. Dataloading: 0.0117 s/iter. Inference: 0.1704 s/iter. Eval: 0.2065 s/iter. Total: 0.3888 s/iter. ETA=0:02:19
[01/17 08:55:41] d2.evaluation.evaluator INFO: Inference done 749/1093. Dataloading: 0.0117 s/iter. Inference: 0.1705 s/iter. Eval: 0.2063 s/iter. Total: 0.3886 s/iter. ETA=0:02:13
[01/17 08:55:47] d2.evaluation.evaluator INFO: Inference done 762/1093. Dataloading: 0.0117 s/iter. Inference: 0.1707 s/iter. Eval: 0.2066 s/iter. Total: 0.3891 s/iter. ETA=0:02:08
[01/17 08:55:52] d2.evaluation.evaluator INFO: Inference done 775/1093. Dataloading: 0.0117 s/iter. Inference: 0.1704 s/iter. Eval: 0.2069 s/iter. Total: 0.3891 s/iter. ETA=0:02:03
[01/17 08:55:57] d2.evaluation.evaluator INFO: Inference done 790/1093. Dataloading: 0.0117 s/iter. Inference: 0.1700 s/iter. Eval: 0.2062 s/iter. Total: 0.3881 s/iter. ETA=0:01:57
[01/17 08:56:02] d2.evaluation.evaluator INFO: Inference done 803/1093. Dataloading: 0.0117 s/iter. Inference: 0.1700 s/iter. Eval: 0.2066 s/iter. Total: 0.3884 s/iter. ETA=0:01:52
[01/17 08:56:07] d2.evaluation.evaluator INFO: Inference done 818/1093. Dataloading: 0.0116 s/iter. Inference: 0.1697 s/iter. Eval: 0.2062 s/iter. Total: 0.3876 s/iter. ETA=0:01:46
[01/17 08:56:12] d2.evaluation.evaluator INFO: Inference done 832/1093. Dataloading: 0.0116 s/iter. Inference: 0.1698 s/iter. Eval: 0.2059 s/iter. Total: 0.3875 s/iter. ETA=0:01:41
[01/17 08:56:18] d2.evaluation.evaluator INFO: Inference done 846/1093. Dataloading: 0.0116 s/iter. Inference: 0.1702 s/iter. Eval: 0.2054 s/iter. Total: 0.3872 s/iter. ETA=0:01:35
[01/17 08:56:23] d2.evaluation.evaluator INFO: Inference done 859/1093. Dataloading: 0.0116 s/iter. Inference: 0.1700 s/iter. Eval: 0.2059 s/iter. Total: 0.3876 s/iter. ETA=0:01:30
[01/17 08:56:28] d2.evaluation.evaluator INFO: Inference done 873/1093. Dataloading: 0.0116 s/iter. Inference: 0.1698 s/iter. Eval: 0.2057 s/iter. Total: 0.3872 s/iter. ETA=0:01:25
[01/17 08:56:33] d2.evaluation.evaluator INFO: Inference done 885/1093. Dataloading: 0.0116 s/iter. Inference: 0.1700 s/iter. Eval: 0.2062 s/iter. Total: 0.3879 s/iter. ETA=0:01:20
[01/17 08:56:39] d2.evaluation.evaluator INFO: Inference done 897/1093. Dataloading: 0.0117 s/iter. Inference: 0.1700 s/iter. Eval: 0.2067 s/iter. Total: 0.3885 s/iter. ETA=0:01:16
[01/17 08:56:44] d2.evaluation.evaluator INFO: Inference done 910/1093. Dataloading: 0.0116 s/iter. Inference: 0.1704 s/iter. Eval: 0.2064 s/iter. Total: 0.3886 s/iter. ETA=0:01:11
[01/17 08:56:49] d2.evaluation.evaluator INFO: Inference done 923/1093. Dataloading: 0.0116 s/iter. Inference: 0.1704 s/iter. Eval: 0.2067 s/iter. Total: 0.3888 s/iter. ETA=0:01:06
[01/17 08:56:54] d2.evaluation.evaluator INFO: Inference done 935/1093. Dataloading: 0.0116 s/iter. Inference: 0.1705 s/iter. Eval: 0.2070 s/iter. Total: 0.3892 s/iter. ETA=0:01:01
[01/17 08:56:59] d2.evaluation.evaluator INFO: Inference done 947/1093. Dataloading: 0.0116 s/iter. Inference: 0.1706 s/iter. Eval: 0.2074 s/iter. Total: 0.3898 s/iter. ETA=0:00:56
[01/17 08:57:05] d2.evaluation.evaluator INFO: Inference done 961/1093. Dataloading: 0.0116 s/iter. Inference: 0.1706 s/iter. Eval: 0.2073 s/iter. Total: 0.3897 s/iter. ETA=0:00:51
[01/17 08:57:10] d2.evaluation.evaluator INFO: Inference done 975/1093. Dataloading: 0.0116 s/iter. Inference: 0.1705 s/iter. Eval: 0.2072 s/iter. Total: 0.3894 s/iter. ETA=0:00:45
[01/17 08:57:15] d2.evaluation.evaluator INFO: Inference done 990/1093. Dataloading: 0.0116 s/iter. Inference: 0.1704 s/iter. Eval: 0.2065 s/iter. Total: 0.3887 s/iter. ETA=0:00:40
[01/17 08:57:20] d2.evaluation.evaluator INFO: Inference done 1003/1093. Dataloading: 0.0116 s/iter. Inference: 0.1705 s/iter. Eval: 0.2067 s/iter. Total: 0.3889 s/iter. ETA=0:00:35
[01/17 08:57:25] d2.evaluation.evaluator INFO: Inference done 1016/1093. Dataloading: 0.0115 s/iter. Inference: 0.1706 s/iter. Eval: 0.2067 s/iter. Total: 0.3890 s/iter. ETA=0:00:29
[01/17 08:57:31] d2.evaluation.evaluator INFO: Inference done 1031/1093. Dataloading: 0.0115 s/iter. Inference: 0.1705 s/iter. Eval: 0.2063 s/iter. Total: 0.3885 s/iter. ETA=0:00:24
[01/17 08:57:36] d2.evaluation.evaluator INFO: Inference done 1045/1093. Dataloading: 0.0115 s/iter. Inference: 0.1705 s/iter. Eval: 0.2063 s/iter. Total: 0.3884 s/iter. ETA=0:00:18
[01/17 08:57:41] d2.evaluation.evaluator INFO: Inference done 1058/1093. Dataloading: 0.0115 s/iter. Inference: 0.1706 s/iter. Eval: 0.2063 s/iter. Total: 0.3885 s/iter. ETA=0:00:13
[01/17 08:57:46] d2.evaluation.evaluator INFO: Inference done 1072/1093. Dataloading: 0.0115 s/iter. Inference: 0.1708 s/iter. Eval: 0.2057 s/iter. Total: 0.3881 s/iter. ETA=0:00:08
[01/17 08:57:51] d2.evaluation.evaluator INFO: Inference done 1087/1093. Dataloading: 0.0114 s/iter. Inference: 0.1707 s/iter. Eval: 0.2052 s/iter. Total: 0.3875 s/iter. ETA=0:00:02
[01/17 08:57:53] d2.evaluation.evaluator INFO: Total inference time: 0:07:01.405918 (0.387322 s / iter per device, on 4 devices)
[01/17 08:57:53] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:05 (0.170512 s / iter per device, on 4 devices)
[01/17 08:58:19] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 11.197018976567245, 'fwIoU': 32.955544299652544, 'IoU-1': nan, 'IoU-2': 95.62983727198616, 'IoU-3': 43.429111409798224, 'IoU-4': 55.75881023829881, 'IoU-5': 49.20590378788032, 'IoU-6': 42.861446184901446, 'IoU-7': 40.85330694746101, 'IoU-8': 36.15071811091276, 'IoU-9': 10.665192355631465, 'IoU-10': 10.171976862830777, 'IoU-11': 16.89795127965433, 'IoU-12': 29.112176311673455, 'IoU-13': 33.01477820230847, 'IoU-14': 33.67946253469563, 'IoU-15': 33.33493855283957, 'IoU-16': 35.09053690131017, 'IoU-17': 34.00302753733348, 'IoU-18': 28.051504809863058, 'IoU-19': 30.460909375030383, 'IoU-20': 32.06069516641225, 'IoU-21': 29.228055040941296, 'IoU-22': 32.87387748324077, 'IoU-23': 33.63232151109629, 'IoU-24': 32.18443864432917, 'IoU-25': 31.37840134280025, 'IoU-26': 30.038914059452555, 'IoU-27': 29.207905579935407, 'IoU-28': 32.87813252208011, 'IoU-29': 33.16644291023239, 'IoU-30': 31.3181095984631, 'IoU-31': 31.1085331954559, 'IoU-32': 32.47716432277997, 'IoU-33': 31.018448974838186, 'IoU-34': 28.543251638834732, 'IoU-35': 29.386577338842486, 'IoU-36': 29.054314314138622, 'IoU-37': 28.86358267249445, 'IoU-38': 28.418370801796677, 'IoU-39': 28.214305883499286, 'IoU-40': 31.351099064277548, 'IoU-41': 31.54396625244768, 'IoU-42': 29.170602279722104, 'IoU-43': 28.520261302016547, 'IoU-44': 28.42922235235605, 'IoU-45': 27.48737722318908, 'IoU-46': 29.552228224749783, 'IoU-47': 30.160685247370328, 'IoU-48': 26.11543160165129, 'IoU-49': 29.153939287842434, 'IoU-50': 28.291635948968068, 'IoU-51': 28.4165327645834, 'IoU-52': 24.06857213405103, 'IoU-53': 27.002170641265895, 'IoU-54': 26.98474012979295, 'IoU-55': 25.61385074427077, 'IoU-56': 25.20910941200784, 'IoU-57': 25.312128831139596, 'IoU-58': 21.614566260988532, 'IoU-59': 23.35200616513526, 'IoU-60': 20.816342073370805, 'IoU-61': 21.021366579893222, 'IoU-62': 20.338547611661852, 'IoU-63': 21.09847215279385, 'IoU-64': 19.631349704981975, 'IoU-65': 19.173886242746946, 'IoU-66': 18.96055585990443, 'IoU-67': 18.402398565217464, 'IoU-68': 18.0958554091406, 'IoU-69': 16.661467736459784, 'IoU-70': 18.055978172414875, 'IoU-71': 17.57870636043643, 'IoU-72': 14.8397895184364, 'IoU-73': 15.58697248180031, 'IoU-74': 16.542402844075784, 'IoU-75': 14.807469413495841, 'IoU-76': 16.91896203451729, 'IoU-77': 16.271499240579367, 'IoU-78': 13.614197050286474, 'IoU-79': 17.012821349139045, 'IoU-80': 16.41926631638829, 'IoU-81': 14.069713916429336, 'IoU-82': 14.12822637550529, 'IoU-83': 14.896445664296296, 'IoU-84': 15.084449145793155, 'IoU-85': 14.492167034859033, 'IoU-86': 15.575436550633556, 'IoU-87': 13.423612197922253, 'IoU-88': 15.192532827494695, 'IoU-89': 12.27578142016626, 'IoU-90': 13.141155377156547, 'IoU-91': 12.833939637397279, 'IoU-92': 13.998180636696578, 'IoU-93': 13.874892949229842, 'IoU-94': 14.195739565500734, 'IoU-95': 11.856668776232423, 'IoU-96': 13.334054891151952, 'IoU-97': 15.1899649653157, 'IoU-98': 11.572108567988558, 'IoU-99': 12.445994780770983, 'IoU-100': 11.23116090378992, 'IoU-101': 11.944172262009694, 'IoU-102': 14.446143877093649, 'IoU-103': 10.8169534244873, 'IoU-104': 10.869804432972105, 'IoU-105': 8.836505508033445, 'IoU-106': 12.432917591476771, 'IoU-107': 9.786752754537215, 'IoU-108': 12.190541163084736, 'IoU-109': 10.334063343534226, 'IoU-110': 12.50628815133471, 'IoU-111': 7.683341920633785, 'IoU-112': 7.880935366008682, 'IoU-113': 11.52029866429893, 'IoU-114': 5.71654947792185, 'IoU-115': 10.783490389808438, 'IoU-116': 6.9902962531123976, 'IoU-117': 8.484547518161618, 'IoU-118': 9.622887446198131, 'IoU-119': 6.80446218358695, 'IoU-120': 6.048508024851262, 'IoU-121': 7.244813156052878, 'IoU-122': 5.913103351460477, 'IoU-123': 4.547820997737576, 'IoU-124': 5.809173154341592, 'IoU-125': 5.70995591752466, 'IoU-126': 7.676441258372845, 'IoU-127': 4.6208440461411255, 'IoU-128': 5.444514140265745, 'IoU-129': 4.245520954303552, 'IoU-130': 5.457605884308291, 'IoU-131': 6.017272733132835, 'IoU-132': 5.287126414861431, 'IoU-133': 3.6000952013739873, 'IoU-134': 5.2430989423162, 'IoU-135': 5.593099894631958, 'IoU-136': 4.879078686301205, 'IoU-137': 3.0315236632630334, 'IoU-138': 2.0996816915730947, 'IoU-139': 6.439651771980548, 'IoU-140': 2.4036807916392005, 'IoU-141': 4.408092424574154, 'IoU-142': 2.796051172457007, 'IoU-143': 2.237712508249643, 'IoU-144': 3.498579772677161, 'IoU-145': 3.49683939194926, 'IoU-146': 4.928068658822352, 'IoU-147': 2.0186037643046504, 'IoU-148': 2.938747937840727, 'IoU-149': 3.3750121643721918, 'IoU-150': 2.3836529302789597, 'IoU-151': 1.9842827529814293, 'IoU-152': 1.973181836161279, 'IoU-153': 3.3579199408635843, 'IoU-154': 2.643466449268407, 'IoU-155': 1.8339409253919983, 'IoU-156': 2.734492190087314, 'IoU-157': 1.75996012574061, 'IoU-158': 1.3756068118818214, 'IoU-159': 0.16051736073869674, 'IoU-160': 1.8306799583395328, 'IoU-161': 1.5672240078003683, 'IoU-162': 0.8022167389149845, 'IoU-163': 1.8106876600689894, 'IoU-164': 0.2912142208281426, 'IoU-165': 2.800378482330513, 'IoU-166': 1.4888090759194965, 'IoU-167': 2.1662339737188963, 'IoU-168': 0.34865500465471744, 'IoU-169': 1.2443492384357384, 'IoU-170': 1.5364974028202123, 'IoU-171': 1.1780845781978138, 'IoU-172': 1.3421150009495635, 'IoU-173': 1.0572765975161653, 'IoU-174': 0.9861492829736864, 'IoU-175': 0.2160877124507682, 'IoU-176': 0.24868001652630306, 'IoU-177': 0.4094428848653842, 'IoU-178': 1.7612093956403847, 'IoU-179': 0.5873524526647708, 'IoU-180': 0.14361916649786396, 'IoU-181': 0.7324050079372986, 'IoU-182': 1.1260010669752496, 'IoU-183': 0.0658339017445984, 'IoU-184': 0.9149289863408528, 'IoU-185': 1.4352556240057042, 'IoU-186': 0.6039186394979186, 'IoU-187': 0.37454610263896504, 'IoU-188': 1.1393292336992888, 'IoU-189': 0.6213013134017488, 'IoU-190': 0.07020504776044147, 'IoU-191': 0.8168104691228207, 'IoU-192': 0.0044361636057137785, 'IoU-193': 0.018503669894529083, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 17.937073901457428, 'pACC': 46.38533788707169, 'ACC-1': nan, 'ACC-2': 98.49239005994998, 'ACC-3': 54.01865747807874, 'ACC-4': 69.1713017416482, 'ACC-5': 65.36835372255902, 'ACC-6': 60.853982447529596, 'ACC-7': 60.24055036156714, 'ACC-8': 53.38831945918155, 'ACC-9': 11.995553912146459, 'ACC-10': 11.64086664356309, 'ACC-11': 22.78271662816026, 'ACC-12': 42.48838101929188, 'ACC-13': 50.06466830854331, 'ACC-14': 51.131440370052616, 'ACC-15': 54.4493957870483, 'ACC-16': 55.97084112997626, 'ACC-17': 49.39750175769095, 'ACC-18': 48.068181977999316, 'ACC-19': 45.44535978131407, 'ACC-20': 49.98099693610744, 'ACC-21': 44.54700919021287, 'ACC-22': 53.06060161993662, 'ACC-23': 49.631022486224516, 'ACC-24': 50.53135057061719, 'ACC-25': 49.091304007957056, 'ACC-26': 43.53970470817213, 'ACC-27': 47.114794054614165, 'ACC-28': 50.35361980992724, 'ACC-29': 50.494881982609286, 'ACC-30': 47.339776364604056, 'ACC-31': 44.42092651942799, 'ACC-32': 48.176481034554115, 'ACC-33': 44.754955426189035, 'ACC-34': 40.62885423441652, 'ACC-35': 46.506757015150384, 'ACC-36': 45.25232584958069, 'ACC-37': 44.06657867859172, 'ACC-38': 45.45042766223641, 'ACC-39': 45.065123043399176, 'ACC-40': 49.71018759538659, 'ACC-41': 46.6176961600628, 'ACC-42': 47.03884491078023, 'ACC-43': 42.886568695044566, 'ACC-44': 43.86833313024193, 'ACC-45': 45.53238628183073, 'ACC-46': 48.42548844510567, 'ACC-47': 47.20323209211504, 'ACC-48': 39.309435473849604, 'ACC-49': 47.97779645297487, 'ACC-50': 43.5051386499511, 'ACC-51': 43.58326392447764, 'ACC-52': 42.93129183323539, 'ACC-53': 40.23879678155057, 'ACC-54': 43.87649671333221, 'ACC-55': 39.10217371040975, 'ACC-56': 38.862386739110086, 'ACC-57': 42.55004903470981, 'ACC-58': 33.62291474753056, 'ACC-59': 40.53193487763582, 'ACC-60': 33.129921104486556, 'ACC-61': 35.355946053556806, 'ACC-62': 34.81780237670413, 'ACC-63': 38.07123158897543, 'ACC-64': 31.88269459778918, 'ACC-65': 30.06685267336046, 'ACC-66': 32.10287221788359, 'ACC-67': 29.63346539761761, 'ACC-68': 33.21272443296218, 'ACC-69': 28.175003540885115, 'ACC-70': 30.269719254416806, 'ACC-71': 30.277032811600492, 'ACC-72': 26.16290691860167, 'ACC-73': 26.56358162122577, 'ACC-74': 26.816773322714138, 'ACC-75': 23.016099605600566, 'ACC-76': 27.802616247642614, 'ACC-77': 28.432915678023722, 'ACC-78': 21.39889645722729, 'ACC-79': 27.741924892846203, 'ACC-80': 32.13182518013489, 'ACC-81': 27.103366042512906, 'ACC-82': 22.650775553258278, 'ACC-83': 27.463839356938102, 'ACC-84': 27.75192071210504, 'ACC-85': 23.614516616908695, 'ACC-86': 28.30460548273297, 'ACC-87': 21.176529879573362, 'ACC-88': 31.52900491245379, 'ACC-89': 18.47371301206594, 'ACC-90': 24.11815452551416, 'ACC-91': 20.010301575093635, 'ACC-92': 24.585027600417046, 'ACC-93': 27.827701471432437, 'ACC-94': 26.88906793268209, 'ACC-95': 20.62107256763759, 'ACC-96': 22.648417063043606, 'ACC-97': 28.15626785530209, 'ACC-98': 18.630865584320308, 'ACC-99': 21.94618752639145, 'ACC-100': 19.580154997620348, 'ACC-101': 21.40359178589414, 'ACC-102': 27.418234403022158, 'ACC-103': 19.87745808790213, 'ACC-104': 23.489350146405968, 'ACC-105': 13.727306263987604, 'ACC-106': 22.786499269196085, 'ACC-107': 15.075348685485418, 'ACC-108': 22.763196828670765, 'ACC-109': 17.406735026020975, 'ACC-110': 22.5039015823202, 'ACC-111': 11.355339038754126, 'ACC-112': 12.68737380052719, 'ACC-113': 25.766997055131696, 'ACC-114': 7.568492616269696, 'ACC-115': 20.585080814592725, 'ACC-116': 11.857531610361884, 'ACC-117': 13.933658863275852, 'ACC-118': 19.934751877922764, 'ACC-119': 10.409174634724737, 'ACC-120': 9.143281878005796, 'ACC-121': 14.50855128840349, 'ACC-122': 9.642505292676734, 'ACC-123': 7.368309320954457, 'ACC-124': 11.085886446575575, 'ACC-125': 10.136801376164629, 'ACC-126': 17.750877509754037, 'ACC-127': 8.830795362434996, 'ACC-128': 8.760186059116522, 'ACC-129': 7.488678373100814, 'ACC-130': 10.344141635170082, 'ACC-131': 10.836323627667726, 'ACC-132': 9.50719505978881, 'ACC-133': 6.3766658959227644, 'ACC-134': 9.918553144583479, 'ACC-135': 11.17159844894442, 'ACC-136': 9.403255230602587, 'ACC-137': 4.706135855385762, 'ACC-138': 3.1108587356140243, 'ACC-139': 17.274792496890626, 'ACC-140': 5.62622104639218, 'ACC-141': 9.621790762431525, 'ACC-142': 5.039477294033888, 'ACC-143': 3.8197964999244585, 'ACC-144': 5.8173002184475395, 'ACC-145': 7.549458483754512, 'ACC-146': 12.8650461772401, 'ACC-147': 3.40545109775879, 'ACC-148': 9.775270361742013, 'ACC-149': 8.472117796570698, 'ACC-150': 3.9641242144312416, 'ACC-151': 3.6776807388715347, 'ACC-152': 3.1367934834159845, 'ACC-153': 7.7061838289837485, 'ACC-154': 4.983029187049964, 'ACC-155': 4.9374931751494415, 'ACC-156': 6.920399444859402, 'ACC-157': 5.459481471426632, 'ACC-158': 2.810408679071173, 'ACC-159': 0.1858722943020333, 'ACC-160': 2.616484304780159, 'ACC-161': 3.067881056870108, 'ACC-162': 1.492373840489557, 'ACC-163': 4.01456704978563, 'ACC-164': 0.39057304938952897, 'ACC-165': 9.544360554162083, 'ACC-166': 2.6873489249651397, 'ACC-167': 4.552635334211918, 'ACC-168': 0.47232167207778103, 'ACC-169': 6.526302834235882, 'ACC-170': 2.954414249692486, 'ACC-171': 2.112171160043727, 'ACC-172': 3.7774908203207516, 'ACC-173': 1.5992517687150425, 'ACC-174': 1.7045903526959245, 'ACC-175': 0.340552601117281, 'ACC-176': 0.2926717435877525, 'ACC-177': 0.9854959948920362, 'ACC-178': 10.475225561897803, 'ACC-179': 1.459694694530346, 'ACC-180': 0.15967343030062023, 'ACC-181': 1.1981865284974091, 'ACC-182': 6.859143353206326, 'ACC-183': 0.09192374120150015, 'ACC-184': 1.3631515876360503, 'ACC-185': 1.9511010171761982, 'ACC-186': 1.0718789407313998, 'ACC-187': 0.6498856267395005, 'ACC-188': 2.5958274441585427, 'ACC-189': 1.1044247609055324, 'ACC-190': 0.1029469527958698, 'ACC-191': 1.5956276135279877, 'ACC-192': 0.004437194127243067, 'ACC-193': 0.018677254217239106, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 08:58:19] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 08:58:19] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 08:58:19] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 08:58:19] d2.evaluation.testing INFO: copypaste: 11.1970,32.9555,17.9371,46.3853
[01/17 08:58:19] d2.utils.events INFO:  eta: 1 day, 9:12:13  iter: 11999  total_loss: 41.26  loss_ce: 0.3435  loss_mask: 0.4465  loss_dice: 3.252  loss_ce_0: 0.6058  loss_mask_0: 0.4327  loss_dice_0: 3.379  loss_ce_1: 0.3454  loss_mask_1: 0.4535  loss_dice_1: 3.286  loss_ce_2: 0.3695  loss_mask_2: 0.4473  loss_dice_2: 3.271  loss_ce_3: 0.3515  loss_mask_3: 0.4451  loss_dice_3: 3.257  loss_ce_4: 0.3507  loss_mask_4: 0.4476  loss_dice_4: 3.263  loss_ce_5: 0.3524  loss_mask_5: 0.4471  loss_dice_5: 3.254  loss_ce_6: 0.3425  loss_mask_6: 0.4483  loss_dice_6: 3.25  loss_ce_7: 0.3455  loss_mask_7: 0.4447  loss_dice_7: 3.259  loss_ce_8: 0.3449  loss_mask_8: 0.4468  loss_dice_8: 3.259  time: 1.5344  data_time: 0.0930  lr: 8.7917e-06  max_mem: 21366M
[01/17 08:58:50] d2.utils.events INFO:  eta: 1 day, 9:11:30  iter: 12019  total_loss: 41.01  loss_ce: 0.3304  loss_mask: 0.4384  loss_dice: 3.252  loss_ce_0: 0.5694  loss_mask_0: 0.425  loss_dice_0: 3.376  loss_ce_1: 0.348  loss_mask_1: 0.4406  loss_dice_1: 3.287  loss_ce_2: 0.3625  loss_mask_2: 0.4369  loss_dice_2: 3.267  loss_ce_3: 0.3404  loss_mask_3: 0.4392  loss_dice_3: 3.259  loss_ce_4: 0.3438  loss_mask_4: 0.436  loss_dice_4: 3.255  loss_ce_5: 0.3404  loss_mask_5: 0.4356  loss_dice_5: 3.254  loss_ce_6: 0.3317  loss_mask_6: 0.4373  loss_dice_6: 3.255  loss_ce_7: 0.3433  loss_mask_7: 0.437  loss_dice_7: 3.244  loss_ce_8: 0.335  loss_mask_8: 0.4373  loss_dice_8: 3.255  time: 1.5344  data_time: 0.0899  lr: 8.7897e-06  max_mem: 21366M
[01/17 08:59:20] d2.utils.events INFO:  eta: 1 day, 9:10:33  iter: 12039  total_loss: 39.99  loss_ce: 0.3347  loss_mask: 0.4481  loss_dice: 3.167  loss_ce_0: 0.5912  loss_mask_0: 0.4249  loss_dice_0: 3.302  loss_ce_1: 0.3384  loss_mask_1: 0.4545  loss_dice_1: 3.207  loss_ce_2: 0.3425  loss_mask_2: 0.4528  loss_dice_2: 3.179  loss_ce_3: 0.3222  loss_mask_3: 0.4522  loss_dice_3: 3.168  loss_ce_4: 0.3435  loss_mask_4: 0.4493  loss_dice_4: 3.164  loss_ce_5: 0.3466  loss_mask_5: 0.4519  loss_dice_5: 3.181  loss_ce_6: 0.3208  loss_mask_6: 0.451  loss_dice_6: 3.172  loss_ce_7: 0.3275  loss_mask_7: 0.4473  loss_dice_7: 3.175  loss_ce_8: 0.3354  loss_mask_8: 0.4495  loss_dice_8: 3.169  time: 1.5344  data_time: 0.0967  lr: 8.7876e-06  max_mem: 21366M
[01/17 08:59:51] d2.utils.events INFO:  eta: 1 day, 9:10:15  iter: 12059  total_loss: 41.36  loss_ce: 0.3304  loss_mask: 0.4289  loss_dice: 3.316  loss_ce_0: 0.5667  loss_mask_0: 0.4234  loss_dice_0: 3.444  loss_ce_1: 0.3403  loss_mask_1: 0.44  loss_dice_1: 3.361  loss_ce_2: 0.3486  loss_mask_2: 0.4357  loss_dice_2: 3.329  loss_ce_3: 0.3497  loss_mask_3: 0.4328  loss_dice_3: 3.322  loss_ce_4: 0.3541  loss_mask_4: 0.4277  loss_dice_4: 3.326  loss_ce_5: 0.3388  loss_mask_5: 0.4303  loss_dice_5: 3.322  loss_ce_6: 0.3257  loss_mask_6: 0.4302  loss_dice_6: 3.314  loss_ce_7: 0.3254  loss_mask_7: 0.431  loss_dice_7: 3.324  loss_ce_8: 0.3369  loss_mask_8: 0.4291  loss_dice_8: 3.317  time: 1.5344  data_time: 0.1020  lr: 8.7856e-06  max_mem: 21366M
[01/17 09:00:21] d2.utils.events INFO:  eta: 1 day, 9:09:58  iter: 12079  total_loss: 41  loss_ce: 0.3771  loss_mask: 0.4445  loss_dice: 3.239  loss_ce_0: 0.6099  loss_mask_0: 0.4349  loss_dice_0: 3.349  loss_ce_1: 0.3737  loss_mask_1: 0.4497  loss_dice_1: 3.27  loss_ce_2: 0.3851  loss_mask_2: 0.4444  loss_dice_2: 3.241  loss_ce_3: 0.376  loss_mask_3: 0.4461  loss_dice_3: 3.22  loss_ce_4: 0.3998  loss_mask_4: 0.4463  loss_dice_4: 3.229  loss_ce_5: 0.3685  loss_mask_5: 0.4456  loss_dice_5: 3.233  loss_ce_6: 0.384  loss_mask_6: 0.4457  loss_dice_6: 3.232  loss_ce_7: 0.3738  loss_mask_7: 0.4435  loss_dice_7: 3.236  loss_ce_8: 0.3808  loss_mask_8: 0.4449  loss_dice_8: 3.229  time: 1.5343  data_time: 0.0939  lr: 8.7836e-06  max_mem: 21366M
[01/17 09:00:52] d2.utils.events INFO:  eta: 1 day, 9:09:27  iter: 12099  total_loss: 40.59  loss_ce: 0.3351  loss_mask: 0.4477  loss_dice: 3.241  loss_ce_0: 0.6082  loss_mask_0: 0.4242  loss_dice_0: 3.366  loss_ce_1: 0.3288  loss_mask_1: 0.4468  loss_dice_1: 3.266  loss_ce_2: 0.3388  loss_mask_2: 0.447  loss_dice_2: 3.243  loss_ce_3: 0.3447  loss_mask_3: 0.4443  loss_dice_3: 3.235  loss_ce_4: 0.3377  loss_mask_4: 0.4435  loss_dice_4: 3.245  loss_ce_5: 0.3339  loss_mask_5: 0.4439  loss_dice_5: 3.251  loss_ce_6: 0.3445  loss_mask_6: 0.4466  loss_dice_6: 3.247  loss_ce_7: 0.3375  loss_mask_7: 0.4473  loss_dice_7: 3.246  loss_ce_8: 0.3301  loss_mask_8: 0.4478  loss_dice_8: 3.244  time: 1.5343  data_time: 0.0919  lr: 8.7815e-06  max_mem: 21366M
[01/17 09:01:22] d2.utils.events INFO:  eta: 1 day, 9:06:50  iter: 12119  total_loss: 41.22  loss_ce: 0.327  loss_mask: 0.4568  loss_dice: 3.262  loss_ce_0: 0.5819  loss_mask_0: 0.4467  loss_dice_0: 3.393  loss_ce_1: 0.3336  loss_mask_1: 0.4623  loss_dice_1: 3.306  loss_ce_2: 0.3486  loss_mask_2: 0.4598  loss_dice_2: 3.287  loss_ce_3: 0.3423  loss_mask_3: 0.459  loss_dice_3: 3.273  loss_ce_4: 0.3266  loss_mask_4: 0.455  loss_dice_4: 3.274  loss_ce_5: 0.3265  loss_mask_5: 0.4548  loss_dice_5: 3.268  loss_ce_6: 0.3189  loss_mask_6: 0.4567  loss_dice_6: 3.259  loss_ce_7: 0.3243  loss_mask_7: 0.457  loss_dice_7: 3.273  loss_ce_8: 0.3214  loss_mask_8: 0.4568  loss_dice_8: 3.269  time: 1.5343  data_time: 0.0881  lr: 8.7795e-06  max_mem: 21366M
[01/17 09:01:52] d2.utils.events INFO:  eta: 1 day, 9:06:19  iter: 12139  total_loss: 41.1  loss_ce: 0.3129  loss_mask: 0.4429  loss_dice: 3.29  loss_ce_0: 0.5635  loss_mask_0: 0.4297  loss_dice_0: 3.402  loss_ce_1: 0.3562  loss_mask_1: 0.4466  loss_dice_1: 3.322  loss_ce_2: 0.3494  loss_mask_2: 0.4445  loss_dice_2: 3.307  loss_ce_3: 0.3396  loss_mask_3: 0.4414  loss_dice_3: 3.297  loss_ce_4: 0.3391  loss_mask_4: 0.4436  loss_dice_4: 3.296  loss_ce_5: 0.3199  loss_mask_5: 0.4413  loss_dice_5: 3.305  loss_ce_6: 0.3235  loss_mask_6: 0.4401  loss_dice_6: 3.296  loss_ce_7: 0.3307  loss_mask_7: 0.4406  loss_dice_7: 3.299  loss_ce_8: 0.3165  loss_mask_8: 0.4408  loss_dice_8: 3.289  time: 1.5343  data_time: 0.0970  lr: 8.7775e-06  max_mem: 21366M
[01/17 09:02:23] d2.utils.events INFO:  eta: 1 day, 9:07:20  iter: 12159  total_loss: 40.97  loss_ce: 0.3318  loss_mask: 0.4474  loss_dice: 3.269  loss_ce_0: 0.5931  loss_mask_0: 0.4224  loss_dice_0: 3.385  loss_ce_1: 0.3422  loss_mask_1: 0.443  loss_dice_1: 3.304  loss_ce_2: 0.3485  loss_mask_2: 0.4401  loss_dice_2: 3.279  loss_ce_3: 0.348  loss_mask_3: 0.4417  loss_dice_3: 3.27  loss_ce_4: 0.3383  loss_mask_4: 0.4452  loss_dice_4: 3.267  loss_ce_5: 0.3368  loss_mask_5: 0.4465  loss_dice_5: 3.27  loss_ce_6: 0.3283  loss_mask_6: 0.4466  loss_dice_6: 3.262  loss_ce_7: 0.3318  loss_mask_7: 0.4452  loss_dice_7: 3.266  loss_ce_8: 0.3346  loss_mask_8: 0.4467  loss_dice_8: 3.269  time: 1.5343  data_time: 0.1145  lr: 8.7754e-06  max_mem: 21366M
[01/17 09:02:54] d2.utils.events INFO:  eta: 1 day, 9:06:59  iter: 12179  total_loss: 41.43  loss_ce: 0.3555  loss_mask: 0.4478  loss_dice: 3.291  loss_ce_0: 0.585  loss_mask_0: 0.441  loss_dice_0: 3.404  loss_ce_1: 0.3682  loss_mask_1: 0.4557  loss_dice_1: 3.323  loss_ce_2: 0.366  loss_mask_2: 0.4534  loss_dice_2: 3.298  loss_ce_3: 0.3599  loss_mask_3: 0.4497  loss_dice_3: 3.294  loss_ce_4: 0.3664  loss_mask_4: 0.4461  loss_dice_4: 3.294  loss_ce_5: 0.3517  loss_mask_5: 0.4459  loss_dice_5: 3.292  loss_ce_6: 0.3551  loss_mask_6: 0.4437  loss_dice_6: 3.297  loss_ce_7: 0.348  loss_mask_7: 0.4465  loss_dice_7: 3.289  loss_ce_8: 0.3569  loss_mask_8: 0.4461  loss_dice_8: 3.288  time: 1.5343  data_time: 0.1036  lr: 8.7734e-06  max_mem: 21366M
[01/17 09:03:25] d2.utils.events INFO:  eta: 1 day, 9:06:19  iter: 12199  total_loss: 40.81  loss_ce: 0.3169  loss_mask: 0.4382  loss_dice: 3.266  loss_ce_0: 0.5888  loss_mask_0: 0.4299  loss_dice_0: 3.383  loss_ce_1: 0.3367  loss_mask_1: 0.4478  loss_dice_1: 3.291  loss_ce_2: 0.3426  loss_mask_2: 0.4439  loss_dice_2: 3.279  loss_ce_3: 0.336  loss_mask_3: 0.4451  loss_dice_3: 3.27  loss_ce_4: 0.3187  loss_mask_4: 0.4419  loss_dice_4: 3.269  loss_ce_5: 0.3004  loss_mask_5: 0.4416  loss_dice_5: 3.275  loss_ce_6: 0.3099  loss_mask_6: 0.4369  loss_dice_6: 3.27  loss_ce_7: 0.319  loss_mask_7: 0.4383  loss_dice_7: 3.281  loss_ce_8: 0.3198  loss_mask_8: 0.4392  loss_dice_8: 3.261  time: 1.5343  data_time: 0.0982  lr: 8.7714e-06  max_mem: 21366M
[01/17 09:03:55] d2.utils.events INFO:  eta: 1 day, 9:06:10  iter: 12219  total_loss: 40.93  loss_ce: 0.3376  loss_mask: 0.4273  loss_dice: 3.291  loss_ce_0: 0.6336  loss_mask_0: 0.4197  loss_dice_0: 3.425  loss_ce_1: 0.3587  loss_mask_1: 0.4296  loss_dice_1: 3.333  loss_ce_2: 0.3502  loss_mask_2: 0.424  loss_dice_2: 3.322  loss_ce_3: 0.3518  loss_mask_3: 0.4262  loss_dice_3: 3.307  loss_ce_4: 0.348  loss_mask_4: 0.4265  loss_dice_4: 3.299  loss_ce_5: 0.337  loss_mask_5: 0.4292  loss_dice_5: 3.307  loss_ce_6: 0.3354  loss_mask_6: 0.4288  loss_dice_6: 3.297  loss_ce_7: 0.3452  loss_mask_7: 0.4274  loss_dice_7: 3.291  loss_ce_8: 0.3436  loss_mask_8: 0.4277  loss_dice_8: 3.294  time: 1.5343  data_time: 0.1079  lr: 8.7694e-06  max_mem: 21366M
[01/17 09:04:26] d2.utils.events INFO:  eta: 1 day, 9:05:39  iter: 12239  total_loss: 41.22  loss_ce: 0.3124  loss_mask: 0.4419  loss_dice: 3.269  loss_ce_0: 0.5822  loss_mask_0: 0.4314  loss_dice_0: 3.398  loss_ce_1: 0.3386  loss_mask_1: 0.4459  loss_dice_1: 3.309  loss_ce_2: 0.353  loss_mask_2: 0.444  loss_dice_2: 3.289  loss_ce_3: 0.3425  loss_mask_3: 0.4442  loss_dice_3: 3.282  loss_ce_4: 0.3304  loss_mask_4: 0.4426  loss_dice_4: 3.277  loss_ce_5: 0.3325  loss_mask_5: 0.4432  loss_dice_5: 3.28  loss_ce_6: 0.3232  loss_mask_6: 0.4449  loss_dice_6: 3.27  loss_ce_7: 0.3226  loss_mask_7: 0.445  loss_dice_7: 3.266  loss_ce_8: 0.324  loss_mask_8: 0.4409  loss_dice_8: 3.269  time: 1.5343  data_time: 0.0900  lr: 8.7673e-06  max_mem: 21366M
[01/17 09:04:57] d2.utils.events INFO:  eta: 1 day, 9:05:04  iter: 12259  total_loss: 40.92  loss_ce: 0.3197  loss_mask: 0.4495  loss_dice: 3.272  loss_ce_0: 0.5806  loss_mask_0: 0.4363  loss_dice_0: 3.401  loss_ce_1: 0.3496  loss_mask_1: 0.4593  loss_dice_1: 3.309  loss_ce_2: 0.3446  loss_mask_2: 0.4549  loss_dice_2: 3.276  loss_ce_3: 0.3323  loss_mask_3: 0.452  loss_dice_3: 3.279  loss_ce_4: 0.331  loss_mask_4: 0.4501  loss_dice_4: 3.276  loss_ce_5: 0.3292  loss_mask_5: 0.449  loss_dice_5: 3.282  loss_ce_6: 0.3225  loss_mask_6: 0.4496  loss_dice_6: 3.275  loss_ce_7: 0.322  loss_mask_7: 0.449  loss_dice_7: 3.279  loss_ce_8: 0.3144  loss_mask_8: 0.4458  loss_dice_8: 3.278  time: 1.5343  data_time: 0.1001  lr: 8.7653e-06  max_mem: 21366M
[01/17 09:05:28] d2.utils.events INFO:  eta: 1 day, 9:04:16  iter: 12279  total_loss: 40.97  loss_ce: 0.3296  loss_mask: 0.4465  loss_dice: 3.218  loss_ce_0: 0.5853  loss_mask_0: 0.4293  loss_dice_0: 3.362  loss_ce_1: 0.3435  loss_mask_1: 0.4468  loss_dice_1: 3.267  loss_ce_2: 0.3436  loss_mask_2: 0.4457  loss_dice_2: 3.24  loss_ce_3: 0.3381  loss_mask_3: 0.445  loss_dice_3: 3.228  loss_ce_4: 0.3374  loss_mask_4: 0.444  loss_dice_4: 3.231  loss_ce_5: 0.3204  loss_mask_5: 0.4428  loss_dice_5: 3.236  loss_ce_6: 0.3274  loss_mask_6: 0.4448  loss_dice_6: 3.221  loss_ce_7: 0.3201  loss_mask_7: 0.4448  loss_dice_7: 3.224  loss_ce_8: 0.3133  loss_mask_8: 0.4456  loss_dice_8: 3.226  time: 1.5343  data_time: 0.1045  lr: 8.7633e-06  max_mem: 21366M
[01/17 09:05:58] d2.utils.events INFO:  eta: 1 day, 9:03:27  iter: 12299  total_loss: 41.27  loss_ce: 0.3268  loss_mask: 0.4549  loss_dice: 3.282  loss_ce_0: 0.5821  loss_mask_0: 0.4486  loss_dice_0: 3.41  loss_ce_1: 0.3268  loss_mask_1: 0.4669  loss_dice_1: 3.318  loss_ce_2: 0.3202  loss_mask_2: 0.4614  loss_dice_2: 3.295  loss_ce_3: 0.3398  loss_mask_3: 0.4603  loss_dice_3: 3.275  loss_ce_4: 0.343  loss_mask_4: 0.462  loss_dice_4: 3.275  loss_ce_5: 0.3332  loss_mask_5: 0.4592  loss_dice_5: 3.283  loss_ce_6: 0.3269  loss_mask_6: 0.4601  loss_dice_6: 3.27  loss_ce_7: 0.327  loss_mask_7: 0.4599  loss_dice_7: 3.276  loss_ce_8: 0.3225  loss_mask_8: 0.4577  loss_dice_8: 3.277  time: 1.5343  data_time: 0.1009  lr: 8.7612e-06  max_mem: 21366M
[01/17 09:06:29] d2.utils.events INFO:  eta: 1 day, 9:00:51  iter: 12319  total_loss: 40.77  loss_ce: 0.3162  loss_mask: 0.4441  loss_dice: 3.234  loss_ce_0: 0.5722  loss_mask_0: 0.4303  loss_dice_0: 3.367  loss_ce_1: 0.3401  loss_mask_1: 0.4492  loss_dice_1: 3.274  loss_ce_2: 0.3319  loss_mask_2: 0.4469  loss_dice_2: 3.254  loss_ce_3: 0.3475  loss_mask_3: 0.4479  loss_dice_3: 3.241  loss_ce_4: 0.3355  loss_mask_4: 0.4442  loss_dice_4: 3.242  loss_ce_5: 0.3233  loss_mask_5: 0.4453  loss_dice_5: 3.249  loss_ce_6: 0.3217  loss_mask_6: 0.4459  loss_dice_6: 3.239  loss_ce_7: 0.3236  loss_mask_7: 0.4434  loss_dice_7: 3.239  loss_ce_8: 0.3172  loss_mask_8: 0.4457  loss_dice_8: 3.232  time: 1.5342  data_time: 0.0832  lr: 8.7592e-06  max_mem: 21366M
[01/17 09:06:59] d2.utils.events INFO:  eta: 1 day, 8:59:47  iter: 12339  total_loss: 40.63  loss_ce: 0.3342  loss_mask: 0.4353  loss_dice: 3.233  loss_ce_0: 0.6127  loss_mask_0: 0.4261  loss_dice_0: 3.348  loss_ce_1: 0.3606  loss_mask_1: 0.4365  loss_dice_1: 3.268  loss_ce_2: 0.3619  loss_mask_2: 0.4327  loss_dice_2: 3.242  loss_ce_3: 0.3546  loss_mask_3: 0.4341  loss_dice_3: 3.234  loss_ce_4: 0.3445  loss_mask_4: 0.4346  loss_dice_4: 3.239  loss_ce_5: 0.34  loss_mask_5: 0.4339  loss_dice_5: 3.238  loss_ce_6: 0.3418  loss_mask_6: 0.4343  loss_dice_6: 3.222  loss_ce_7: 0.3509  loss_mask_7: 0.4342  loss_dice_7: 3.216  loss_ce_8: 0.3322  loss_mask_8: 0.4332  loss_dice_8: 3.234  time: 1.5342  data_time: 0.1049  lr: 8.7572e-06  max_mem: 21366M
[01/17 09:07:30] d2.utils.events INFO:  eta: 1 day, 8:59:17  iter: 12359  total_loss: 40.55  loss_ce: 0.3318  loss_mask: 0.4374  loss_dice: 3.246  loss_ce_0: 0.606  loss_mask_0: 0.4243  loss_dice_0: 3.385  loss_ce_1: 0.364  loss_mask_1: 0.4402  loss_dice_1: 3.282  loss_ce_2: 0.3496  loss_mask_2: 0.4379  loss_dice_2: 3.263  loss_ce_3: 0.3421  loss_mask_3: 0.4375  loss_dice_3: 3.241  loss_ce_4: 0.3353  loss_mask_4: 0.4367  loss_dice_4: 3.245  loss_ce_5: 0.3344  loss_mask_5: 0.4366  loss_dice_5: 3.246  loss_ce_6: 0.3258  loss_mask_6: 0.4362  loss_dice_6: 3.234  loss_ce_7: 0.3412  loss_mask_7: 0.4333  loss_dice_7: 3.236  loss_ce_8: 0.336  loss_mask_8: 0.4352  loss_dice_8: 3.238  time: 1.5342  data_time: 0.0981  lr: 8.7552e-06  max_mem: 21366M
[01/17 09:08:01] d2.utils.events INFO:  eta: 1 day, 9:00:14  iter: 12379  total_loss: 40.86  loss_ce: 0.3346  loss_mask: 0.4378  loss_dice: 3.27  loss_ce_0: 0.6164  loss_mask_0: 0.4217  loss_dice_0: 3.393  loss_ce_1: 0.3348  loss_mask_1: 0.438  loss_dice_1: 3.308  loss_ce_2: 0.3475  loss_mask_2: 0.4393  loss_dice_2: 3.285  loss_ce_3: 0.3382  loss_mask_3: 0.4402  loss_dice_3: 3.268  loss_ce_4: 0.3236  loss_mask_4: 0.4397  loss_dice_4: 3.275  loss_ce_5: 0.3324  loss_mask_5: 0.4396  loss_dice_5: 3.271  loss_ce_6: 0.3298  loss_mask_6: 0.4389  loss_dice_6: 3.269  loss_ce_7: 0.3282  loss_mask_7: 0.4413  loss_dice_7: 3.275  loss_ce_8: 0.3367  loss_mask_8: 0.4388  loss_dice_8: 3.268  time: 1.5342  data_time: 0.0967  lr: 8.7531e-06  max_mem: 21366M
[01/17 09:08:31] d2.utils.events INFO:  eta: 1 day, 8:58:49  iter: 12399  total_loss: 41.33  loss_ce: 0.3304  loss_mask: 0.449  loss_dice: 3.291  loss_ce_0: 0.5718  loss_mask_0: 0.4397  loss_dice_0: 3.427  loss_ce_1: 0.3388  loss_mask_1: 0.4649  loss_dice_1: 3.341  loss_ce_2: 0.3449  loss_mask_2: 0.4556  loss_dice_2: 3.318  loss_ce_3: 0.3521  loss_mask_3: 0.449  loss_dice_3: 3.301  loss_ce_4: 0.3504  loss_mask_4: 0.4487  loss_dice_4: 3.298  loss_ce_5: 0.3388  loss_mask_5: 0.4494  loss_dice_5: 3.306  loss_ce_6: 0.3396  loss_mask_6: 0.4491  loss_dice_6: 3.298  loss_ce_7: 0.339  loss_mask_7: 0.4479  loss_dice_7: 3.301  loss_ce_8: 0.3287  loss_mask_8: 0.4484  loss_dice_8: 3.303  time: 1.5342  data_time: 0.0930  lr: 8.7511e-06  max_mem: 21366M
[01/17 09:09:02] d2.utils.events INFO:  eta: 1 day, 8:58:24  iter: 12419  total_loss: 40.18  loss_ce: 0.3093  loss_mask: 0.4479  loss_dice: 3.229  loss_ce_0: 0.5296  loss_mask_0: 0.4384  loss_dice_0: 3.365  loss_ce_1: 0.316  loss_mask_1: 0.4529  loss_dice_1: 3.276  loss_ce_2: 0.3321  loss_mask_2: 0.4539  loss_dice_2: 3.251  loss_ce_3: 0.3202  loss_mask_3: 0.4538  loss_dice_3: 3.224  loss_ce_4: 0.3194  loss_mask_4: 0.4504  loss_dice_4: 3.24  loss_ce_5: 0.3179  loss_mask_5: 0.4484  loss_dice_5: 3.24  loss_ce_6: 0.3219  loss_mask_6: 0.4466  loss_dice_6: 3.234  loss_ce_7: 0.3119  loss_mask_7: 0.447  loss_dice_7: 3.228  loss_ce_8: 0.3172  loss_mask_8: 0.4473  loss_dice_8: 3.222  time: 1.5342  data_time: 0.0957  lr: 8.7491e-06  max_mem: 21366M
[01/17 09:09:33] d2.utils.events INFO:  eta: 1 day, 8:58:42  iter: 12439  total_loss: 40.58  loss_ce: 0.344  loss_mask: 0.4267  loss_dice: 3.239  loss_ce_0: 0.6103  loss_mask_0: 0.4277  loss_dice_0: 3.362  loss_ce_1: 0.3425  loss_mask_1: 0.4403  loss_dice_1: 3.275  loss_ce_2: 0.3686  loss_mask_2: 0.4362  loss_dice_2: 3.256  loss_ce_3: 0.3402  loss_mask_3: 0.4329  loss_dice_3: 3.245  loss_ce_4: 0.3351  loss_mask_4: 0.4311  loss_dice_4: 3.251  loss_ce_5: 0.3478  loss_mask_5: 0.4312  loss_dice_5: 3.242  loss_ce_6: 0.3435  loss_mask_6: 0.431  loss_dice_6: 3.248  loss_ce_7: 0.3432  loss_mask_7: 0.4311  loss_dice_7: 3.242  loss_ce_8: 0.3311  loss_mask_8: 0.4304  loss_dice_8: 3.241  time: 1.5342  data_time: 0.1007  lr: 8.747e-06  max_mem: 21366M
[01/17 09:10:03] d2.utils.events INFO:  eta: 1 day, 8:58:56  iter: 12459  total_loss: 40.35  loss_ce: 0.2997  loss_mask: 0.4423  loss_dice: 3.236  loss_ce_0: 0.5733  loss_mask_0: 0.4289  loss_dice_0: 3.375  loss_ce_1: 0.3155  loss_mask_1: 0.4429  loss_dice_1: 3.281  loss_ce_2: 0.3219  loss_mask_2: 0.4387  loss_dice_2: 3.267  loss_ce_3: 0.3149  loss_mask_3: 0.4398  loss_dice_3: 3.255  loss_ce_4: 0.3126  loss_mask_4: 0.4409  loss_dice_4: 3.255  loss_ce_5: 0.32  loss_mask_5: 0.4414  loss_dice_5: 3.245  loss_ce_6: 0.3101  loss_mask_6: 0.4393  loss_dice_6: 3.242  loss_ce_7: 0.314  loss_mask_7: 0.4384  loss_dice_7: 3.239  loss_ce_8: 0.296  loss_mask_8: 0.4392  loss_dice_8: 3.242  time: 1.5342  data_time: 0.0966  lr: 8.745e-06  max_mem: 21366M
[01/17 09:10:34] d2.utils.events INFO:  eta: 1 day, 8:58:04  iter: 12479  total_loss: 40.36  loss_ce: 0.3017  loss_mask: 0.4439  loss_dice: 3.207  loss_ce_0: 0.5684  loss_mask_0: 0.429  loss_dice_0: 3.325  loss_ce_1: 0.3132  loss_mask_1: 0.4486  loss_dice_1: 3.246  loss_ce_2: 0.3252  loss_mask_2: 0.4458  loss_dice_2: 3.226  loss_ce_3: 0.312  loss_mask_3: 0.4487  loss_dice_3: 3.213  loss_ce_4: 0.3125  loss_mask_4: 0.4494  loss_dice_4: 3.213  loss_ce_5: 0.301  loss_mask_5: 0.4501  loss_dice_5: 3.217  loss_ce_6: 0.3009  loss_mask_6: 0.4472  loss_dice_6: 3.2  loss_ce_7: 0.3026  loss_mask_7: 0.4444  loss_dice_7: 3.203  loss_ce_8: 0.287  loss_mask_8: 0.4448  loss_dice_8: 3.21  time: 1.5342  data_time: 0.1087  lr: 8.743e-06  max_mem: 21366M
[01/17 09:11:04] d2.utils.events INFO:  eta: 1 day, 8:56:11  iter: 12499  total_loss: 40.79  loss_ce: 0.3325  loss_mask: 0.4368  loss_dice: 3.281  loss_ce_0: 0.5778  loss_mask_0: 0.4254  loss_dice_0: 3.398  loss_ce_1: 0.3445  loss_mask_1: 0.439  loss_dice_1: 3.328  loss_ce_2: 0.3633  loss_mask_2: 0.4383  loss_dice_2: 3.293  loss_ce_3: 0.344  loss_mask_3: 0.4362  loss_dice_3: 3.286  loss_ce_4: 0.3543  loss_mask_4: 0.4375  loss_dice_4: 3.287  loss_ce_5: 0.3315  loss_mask_5: 0.4344  loss_dice_5: 3.288  loss_ce_6: 0.3404  loss_mask_6: 0.4366  loss_dice_6: 3.294  loss_ce_7: 0.3352  loss_mask_7: 0.4354  loss_dice_7: 3.282  loss_ce_8: 0.338  loss_mask_8: 0.436  loss_dice_8: 3.286  time: 1.5342  data_time: 0.0860  lr: 8.7409e-06  max_mem: 21366M
[01/17 09:11:36] d2.utils.events INFO:  eta: 1 day, 8:57:03  iter: 12519  total_loss: 40.48  loss_ce: 0.3251  loss_mask: 0.4337  loss_dice: 3.251  loss_ce_0: 0.583  loss_mask_0: 0.4174  loss_dice_0: 3.39  loss_ce_1: 0.3465  loss_mask_1: 0.4313  loss_dice_1: 3.286  loss_ce_2: 0.3605  loss_mask_2: 0.4303  loss_dice_2: 3.262  loss_ce_3: 0.3382  loss_mask_3: 0.4317  loss_dice_3: 3.252  loss_ce_4: 0.343  loss_mask_4: 0.4313  loss_dice_4: 3.253  loss_ce_5: 0.3285  loss_mask_5: 0.4316  loss_dice_5: 3.26  loss_ce_6: 0.318  loss_mask_6: 0.4301  loss_dice_6: 3.245  loss_ce_7: 0.3242  loss_mask_7: 0.4311  loss_dice_7: 3.244  loss_ce_8: 0.3061  loss_mask_8: 0.4335  loss_dice_8: 3.251  time: 1.5342  data_time: 0.1071  lr: 8.7389e-06  max_mem: 21366M
[01/17 09:12:06] d2.utils.events INFO:  eta: 1 day, 8:56:09  iter: 12539  total_loss: 40.8  loss_ce: 0.3298  loss_mask: 0.4314  loss_dice: 3.293  loss_ce_0: 0.5859  loss_mask_0: 0.4264  loss_dice_0: 3.419  loss_ce_1: 0.3502  loss_mask_1: 0.4387  loss_dice_1: 3.341  loss_ce_2: 0.3606  loss_mask_2: 0.4358  loss_dice_2: 3.31  loss_ce_3: 0.3411  loss_mask_3: 0.4313  loss_dice_3: 3.291  loss_ce_4: 0.3374  loss_mask_4: 0.4281  loss_dice_4: 3.29  loss_ce_5: 0.3266  loss_mask_5: 0.431  loss_dice_5: 3.297  loss_ce_6: 0.3337  loss_mask_6: 0.4309  loss_dice_6: 3.299  loss_ce_7: 0.3415  loss_mask_7: 0.4312  loss_dice_7: 3.291  loss_ce_8: 0.3347  loss_mask_8: 0.4306  loss_dice_8: 3.294  time: 1.5342  data_time: 0.0878  lr: 8.7369e-06  max_mem: 21366M
[01/17 09:12:37] d2.utils.events INFO:  eta: 1 day, 8:56:51  iter: 12559  total_loss: 41.11  loss_ce: 0.3355  loss_mask: 0.4413  loss_dice: 3.274  loss_ce_0: 0.6254  loss_mask_0: 0.4272  loss_dice_0: 3.381  loss_ce_1: 0.3612  loss_mask_1: 0.4375  loss_dice_1: 3.298  loss_ce_2: 0.3711  loss_mask_2: 0.4371  loss_dice_2: 3.275  loss_ce_3: 0.3629  loss_mask_3: 0.442  loss_dice_3: 3.27  loss_ce_4: 0.3737  loss_mask_4: 0.4447  loss_dice_4: 3.273  loss_ce_5: 0.3567  loss_mask_5: 0.4418  loss_dice_5: 3.264  loss_ce_6: 0.3481  loss_mask_6: 0.4402  loss_dice_6: 3.273  loss_ce_7: 0.3403  loss_mask_7: 0.4427  loss_dice_7: 3.265  loss_ce_8: 0.3491  loss_mask_8: 0.4427  loss_dice_8: 3.266  time: 1.5342  data_time: 0.0980  lr: 8.7349e-06  max_mem: 21366M
[01/17 09:13:08] d2.utils.events INFO:  eta: 1 day, 8:54:55  iter: 12579  total_loss: 40.82  loss_ce: 0.3117  loss_mask: 0.4354  loss_dice: 3.285  loss_ce_0: 0.5963  loss_mask_0: 0.4231  loss_dice_0: 3.39  loss_ce_1: 0.326  loss_mask_1: 0.4402  loss_dice_1: 3.312  loss_ce_2: 0.3493  loss_mask_2: 0.4367  loss_dice_2: 3.298  loss_ce_3: 0.3282  loss_mask_3: 0.4353  loss_dice_3: 3.286  loss_ce_4: 0.3256  loss_mask_4: 0.4361  loss_dice_4: 3.278  loss_ce_5: 0.3172  loss_mask_5: 0.4345  loss_dice_5: 3.28  loss_ce_6: 0.3306  loss_mask_6: 0.4351  loss_dice_6: 3.281  loss_ce_7: 0.3123  loss_mask_7: 0.4363  loss_dice_7: 3.277  loss_ce_8: 0.3132  loss_mask_8: 0.4348  loss_dice_8: 3.273  time: 1.5342  data_time: 0.1059  lr: 8.7328e-06  max_mem: 21366M
[01/17 09:13:38] d2.utils.events INFO:  eta: 1 day, 8:52:58  iter: 12599  total_loss: 40.93  loss_ce: 0.3283  loss_mask: 0.4525  loss_dice: 3.235  loss_ce_0: 0.5936  loss_mask_0: 0.4417  loss_dice_0: 3.355  loss_ce_1: 0.3384  loss_mask_1: 0.4627  loss_dice_1: 3.279  loss_ce_2: 0.3595  loss_mask_2: 0.4571  loss_dice_2: 3.256  loss_ce_3: 0.3475  loss_mask_3: 0.4508  loss_dice_3: 3.241  loss_ce_4: 0.3378  loss_mask_4: 0.4495  loss_dice_4: 3.239  loss_ce_5: 0.3296  loss_mask_5: 0.4488  loss_dice_5: 3.253  loss_ce_6: 0.3417  loss_mask_6: 0.4503  loss_dice_6: 3.232  loss_ce_7: 0.3334  loss_mask_7: 0.4504  loss_dice_7: 3.241  loss_ce_8: 0.3394  loss_mask_8: 0.4521  loss_dice_8: 3.231  time: 1.5342  data_time: 0.0962  lr: 8.7308e-06  max_mem: 21366M
[01/17 09:14:09] d2.utils.events INFO:  eta: 1 day, 8:53:08  iter: 12619  total_loss: 39.65  loss_ce: 0.3169  loss_mask: 0.4352  loss_dice: 3.171  loss_ce_0: 0.5996  loss_mask_0: 0.4192  loss_dice_0: 3.303  loss_ce_1: 0.3532  loss_mask_1: 0.4371  loss_dice_1: 3.215  loss_ce_2: 0.3378  loss_mask_2: 0.4339  loss_dice_2: 3.185  loss_ce_3: 0.3183  loss_mask_3: 0.4313  loss_dice_3: 3.18  loss_ce_4: 0.3276  loss_mask_4: 0.4315  loss_dice_4: 3.181  loss_ce_5: 0.3206  loss_mask_5: 0.4306  loss_dice_5: 3.183  loss_ce_6: 0.3279  loss_mask_6: 0.4318  loss_dice_6: 3.178  loss_ce_7: 0.3071  loss_mask_7: 0.4328  loss_dice_7: 3.176  loss_ce_8: 0.3193  loss_mask_8: 0.4316  loss_dice_8: 3.176  time: 1.5342  data_time: 0.1078  lr: 8.7288e-06  max_mem: 21366M
[01/17 09:14:40] d2.utils.events INFO:  eta: 1 day, 8:53:59  iter: 12639  total_loss: 39.91  loss_ce: 0.3023  loss_mask: 0.4173  loss_dice: 3.212  loss_ce_0: 0.5839  loss_mask_0: 0.404  loss_dice_0: 3.356  loss_ce_1: 0.3375  loss_mask_1: 0.4157  loss_dice_1: 3.249  loss_ce_2: 0.3416  loss_mask_2: 0.4162  loss_dice_2: 3.226  loss_ce_3: 0.3201  loss_mask_3: 0.415  loss_dice_3: 3.21  loss_ce_4: 0.3085  loss_mask_4: 0.4146  loss_dice_4: 3.211  loss_ce_5: 0.3101  loss_mask_5: 0.4153  loss_dice_5: 3.213  loss_ce_6: 0.3139  loss_mask_6: 0.4165  loss_dice_6: 3.225  loss_ce_7: 0.303  loss_mask_7: 0.4159  loss_dice_7: 3.215  loss_ce_8: 0.3041  loss_mask_8: 0.4182  loss_dice_8: 3.202  time: 1.5342  data_time: 0.1004  lr: 8.7267e-06  max_mem: 21366M
[01/17 09:15:11] d2.utils.events INFO:  eta: 1 day, 8:52:53  iter: 12659  total_loss: 40.57  loss_ce: 0.327  loss_mask: 0.4367  loss_dice: 3.224  loss_ce_0: 0.5733  loss_mask_0: 0.4239  loss_dice_0: 3.364  loss_ce_1: 0.3388  loss_mask_1: 0.4404  loss_dice_1: 3.265  loss_ce_2: 0.3312  loss_mask_2: 0.437  loss_dice_2: 3.237  loss_ce_3: 0.3235  loss_mask_3: 0.4332  loss_dice_3: 3.229  loss_ce_4: 0.3187  loss_mask_4: 0.4333  loss_dice_4: 3.226  loss_ce_5: 0.313  loss_mask_5: 0.4361  loss_dice_5: 3.239  loss_ce_6: 0.3185  loss_mask_6: 0.4359  loss_dice_6: 3.223  loss_ce_7: 0.3109  loss_mask_7: 0.4373  loss_dice_7: 3.224  loss_ce_8: 0.3112  loss_mask_8: 0.4392  loss_dice_8: 3.224  time: 1.5342  data_time: 0.0885  lr: 8.7247e-06  max_mem: 21366M
[01/17 09:15:41] d2.utils.events INFO:  eta: 1 day, 8:52:22  iter: 12679  total_loss: 40.26  loss_ce: 0.3004  loss_mask: 0.4425  loss_dice: 3.223  loss_ce_0: 0.5844  loss_mask_0: 0.4263  loss_dice_0: 3.348  loss_ce_1: 0.3306  loss_mask_1: 0.4414  loss_dice_1: 3.265  loss_ce_2: 0.3297  loss_mask_2: 0.4417  loss_dice_2: 3.235  loss_ce_3: 0.3199  loss_mask_3: 0.4429  loss_dice_3: 3.22  loss_ce_4: 0.3117  loss_mask_4: 0.4434  loss_dice_4: 3.218  loss_ce_5: 0.3193  loss_mask_5: 0.4423  loss_dice_5: 3.223  loss_ce_6: 0.3143  loss_mask_6: 0.4416  loss_dice_6: 3.23  loss_ce_7: 0.3138  loss_mask_7: 0.4415  loss_dice_7: 3.22  loss_ce_8: 0.2979  loss_mask_8: 0.4406  loss_dice_8: 3.211  time: 1.5342  data_time: 0.0940  lr: 8.7227e-06  max_mem: 21366M
[01/17 09:16:12] d2.utils.events INFO:  eta: 1 day, 8:50:56  iter: 12699  total_loss: 40.59  loss_ce: 0.3268  loss_mask: 0.4386  loss_dice: 3.218  loss_ce_0: 0.5824  loss_mask_0: 0.4221  loss_dice_0: 3.365  loss_ce_1: 0.3585  loss_mask_1: 0.4378  loss_dice_1: 3.268  loss_ce_2: 0.352  loss_mask_2: 0.4379  loss_dice_2: 3.233  loss_ce_3: 0.3366  loss_mask_3: 0.4393  loss_dice_3: 3.233  loss_ce_4: 0.3376  loss_mask_4: 0.437  loss_dice_4: 3.225  loss_ce_5: 0.3274  loss_mask_5: 0.4369  loss_dice_5: 3.23  loss_ce_6: 0.3264  loss_mask_6: 0.4384  loss_dice_6: 3.227  loss_ce_7: 0.3245  loss_mask_7: 0.4365  loss_dice_7: 3.228  loss_ce_8: 0.3302  loss_mask_8: 0.4375  loss_dice_8: 3.216  time: 1.5341  data_time: 0.0947  lr: 8.7206e-06  max_mem: 21366M
[01/17 09:16:42] d2.utils.events INFO:  eta: 1 day, 8:50:06  iter: 12719  total_loss: 39.62  loss_ce: 0.2977  loss_mask: 0.4265  loss_dice: 3.187  loss_ce_0: 0.5939  loss_mask_0: 0.4137  loss_dice_0: 3.315  loss_ce_1: 0.3194  loss_mask_1: 0.429  loss_dice_1: 3.22  loss_ce_2: 0.3242  loss_mask_2: 0.4282  loss_dice_2: 3.191  loss_ce_3: 0.3137  loss_mask_3: 0.4277  loss_dice_3: 3.189  loss_ce_4: 0.3183  loss_mask_4: 0.4267  loss_dice_4: 3.18  loss_ce_5: 0.3045  loss_mask_5: 0.4272  loss_dice_5: 3.185  loss_ce_6: 0.294  loss_mask_6: 0.4281  loss_dice_6: 3.186  loss_ce_7: 0.2981  loss_mask_7: 0.4285  loss_dice_7: 3.184  loss_ce_8: 0.3027  loss_mask_8: 0.4266  loss_dice_8: 3.18  time: 1.5341  data_time: 0.0890  lr: 8.7186e-06  max_mem: 21366M
[01/17 09:17:12] d2.utils.events INFO:  eta: 1 day, 8:49:03  iter: 12739  total_loss: 40.23  loss_ce: 0.3178  loss_mask: 0.4268  loss_dice: 3.227  loss_ce_0: 0.5799  loss_mask_0: 0.4121  loss_dice_0: 3.372  loss_ce_1: 0.343  loss_mask_1: 0.4284  loss_dice_1: 3.272  loss_ce_2: 0.3327  loss_mask_2: 0.426  loss_dice_2: 3.252  loss_ce_3: 0.3361  loss_mask_3: 0.4275  loss_dice_3: 3.234  loss_ce_4: 0.329  loss_mask_4: 0.4255  loss_dice_4: 3.232  loss_ce_5: 0.3189  loss_mask_5: 0.4267  loss_dice_5: 3.241  loss_ce_6: 0.3127  loss_mask_6: 0.4264  loss_dice_6: 3.232  loss_ce_7: 0.3203  loss_mask_7: 0.4273  loss_dice_7: 3.229  loss_ce_8: 0.3115  loss_mask_8: 0.4284  loss_dice_8: 3.233  time: 1.5341  data_time: 0.0907  lr: 8.7166e-06  max_mem: 21366M
[01/17 09:17:43] d2.utils.events INFO:  eta: 1 day, 8:49:18  iter: 12759  total_loss: 39.91  loss_ce: 0.3038  loss_mask: 0.4284  loss_dice: 3.21  loss_ce_0: 0.5909  loss_mask_0: 0.4162  loss_dice_0: 3.341  loss_ce_1: 0.3375  loss_mask_1: 0.43  loss_dice_1: 3.243  loss_ce_2: 0.3364  loss_mask_2: 0.4285  loss_dice_2: 3.229  loss_ce_3: 0.327  loss_mask_3: 0.4273  loss_dice_3: 3.214  loss_ce_4: 0.3175  loss_mask_4: 0.4279  loss_dice_4: 3.22  loss_ce_5: 0.3171  loss_mask_5: 0.4301  loss_dice_5: 3.22  loss_ce_6: 0.3123  loss_mask_6: 0.4284  loss_dice_6: 3.208  loss_ce_7: 0.3118  loss_mask_7: 0.4293  loss_dice_7: 3.203  loss_ce_8: 0.3012  loss_mask_8: 0.4282  loss_dice_8: 3.208  time: 1.5341  data_time: 0.0988  lr: 8.7145e-06  max_mem: 21366M
[01/17 09:18:14] d2.utils.events INFO:  eta: 1 day, 8:48:48  iter: 12779  total_loss: 40.36  loss_ce: 0.3471  loss_mask: 0.4286  loss_dice: 3.19  loss_ce_0: 0.5906  loss_mask_0: 0.4163  loss_dice_0: 3.342  loss_ce_1: 0.3626  loss_mask_1: 0.4306  loss_dice_1: 3.256  loss_ce_2: 0.3645  loss_mask_2: 0.4313  loss_dice_2: 3.227  loss_ce_3: 0.3438  loss_mask_3: 0.43  loss_dice_3: 3.219  loss_ce_4: 0.3518  loss_mask_4: 0.4323  loss_dice_4: 3.206  loss_ce_5: 0.3481  loss_mask_5: 0.4305  loss_dice_5: 3.218  loss_ce_6: 0.3544  loss_mask_6: 0.4286  loss_dice_6: 3.207  loss_ce_7: 0.3536  loss_mask_7: 0.4281  loss_dice_7: 3.199  loss_ce_8: 0.35  loss_mask_8: 0.4299  loss_dice_8: 3.2  time: 1.5341  data_time: 0.1060  lr: 8.7125e-06  max_mem: 21366M
[01/17 09:18:45] d2.utils.events INFO:  eta: 1 day, 8:48:32  iter: 12799  total_loss: 39.79  loss_ce: 0.3189  loss_mask: 0.4185  loss_dice: 3.17  loss_ce_0: 0.5716  loss_mask_0: 0.4066  loss_dice_0: 3.328  loss_ce_1: 0.3332  loss_mask_1: 0.421  loss_dice_1: 3.216  loss_ce_2: 0.3364  loss_mask_2: 0.4204  loss_dice_2: 3.201  loss_ce_3: 0.3184  loss_mask_3: 0.4209  loss_dice_3: 3.176  loss_ce_4: 0.3256  loss_mask_4: 0.4175  loss_dice_4: 3.182  loss_ce_5: 0.3143  loss_mask_5: 0.4186  loss_dice_5: 3.185  loss_ce_6: 0.3158  loss_mask_6: 0.4201  loss_dice_6: 3.186  loss_ce_7: 0.3128  loss_mask_7: 0.4191  loss_dice_7: 3.178  loss_ce_8: 0.3064  loss_mask_8: 0.4189  loss_dice_8: 3.188  time: 1.5341  data_time: 0.1030  lr: 8.7105e-06  max_mem: 21366M
[01/17 09:19:15] d2.utils.events INFO:  eta: 1 day, 8:46:29  iter: 12819  total_loss: 40.31  loss_ce: 0.3086  loss_mask: 0.4293  loss_dice: 3.19  loss_ce_0: 0.5878  loss_mask_0: 0.413  loss_dice_0: 3.33  loss_ce_1: 0.3566  loss_mask_1: 0.4296  loss_dice_1: 3.236  loss_ce_2: 0.3687  loss_mask_2: 0.4286  loss_dice_2: 3.208  loss_ce_3: 0.331  loss_mask_3: 0.4283  loss_dice_3: 3.201  loss_ce_4: 0.3419  loss_mask_4: 0.4277  loss_dice_4: 3.201  loss_ce_5: 0.3116  loss_mask_5: 0.4295  loss_dice_5: 3.2  loss_ce_6: 0.3321  loss_mask_6: 0.4294  loss_dice_6: 3.195  loss_ce_7: 0.3361  loss_mask_7: 0.4288  loss_dice_7: 3.183  loss_ce_8: 0.3378  loss_mask_8: 0.4301  loss_dice_8: 3.186  time: 1.5341  data_time: 0.1029  lr: 8.7085e-06  max_mem: 21366M
[01/17 09:19:46] d2.utils.events INFO:  eta: 1 day, 8:44:45  iter: 12839  total_loss: 40.11  loss_ce: 0.3224  loss_mask: 0.4388  loss_dice: 3.146  loss_ce_0: 0.5965  loss_mask_0: 0.4192  loss_dice_0: 3.303  loss_ce_1: 0.351  loss_mask_1: 0.4399  loss_dice_1: 3.218  loss_ce_2: 0.3407  loss_mask_2: 0.4389  loss_dice_2: 3.167  loss_ce_3: 0.3226  loss_mask_3: 0.4377  loss_dice_3: 3.163  loss_ce_4: 0.3239  loss_mask_4: 0.439  loss_dice_4: 3.158  loss_ce_5: 0.3133  loss_mask_5: 0.4409  loss_dice_5: 3.161  loss_ce_6: 0.316  loss_mask_6: 0.4372  loss_dice_6: 3.156  loss_ce_7: 0.322  loss_mask_7: 0.438  loss_dice_7: 3.154  loss_ce_8: 0.3252  loss_mask_8: 0.4397  loss_dice_8: 3.153  time: 1.5341  data_time: 0.1064  lr: 8.7064e-06  max_mem: 21366M
[01/17 09:20:16] d2.utils.events INFO:  eta: 1 day, 8:42:34  iter: 12859  total_loss: 40.33  loss_ce: 0.3145  loss_mask: 0.4311  loss_dice: 3.2  loss_ce_0: 0.5985  loss_mask_0: 0.4157  loss_dice_0: 3.344  loss_ce_1: 0.3254  loss_mask_1: 0.4364  loss_dice_1: 3.246  loss_ce_2: 0.3223  loss_mask_2: 0.4323  loss_dice_2: 3.212  loss_ce_3: 0.3194  loss_mask_3: 0.4313  loss_dice_3: 3.2  loss_ce_4: 0.3215  loss_mask_4: 0.4303  loss_dice_4: 3.209  loss_ce_5: 0.3114  loss_mask_5: 0.4283  loss_dice_5: 3.214  loss_ce_6: 0.3264  loss_mask_6: 0.4315  loss_dice_6: 3.196  loss_ce_7: 0.3302  loss_mask_7: 0.431  loss_dice_7: 3.206  loss_ce_8: 0.3363  loss_mask_8: 0.4314  loss_dice_8: 3.207  time: 1.5341  data_time: 0.0983  lr: 8.7044e-06  max_mem: 21366M
[01/17 09:20:48] d2.utils.events INFO:  eta: 1 day, 8:42:03  iter: 12879  total_loss: 40.57  loss_ce: 0.3177  loss_mask: 0.4204  loss_dice: 3.234  loss_ce_0: 0.5927  loss_mask_0: 0.4099  loss_dice_0: 3.374  loss_ce_1: 0.3355  loss_mask_1: 0.4213  loss_dice_1: 3.289  loss_ce_2: 0.3396  loss_mask_2: 0.4172  loss_dice_2: 3.258  loss_ce_3: 0.3314  loss_mask_3: 0.4188  loss_dice_3: 3.247  loss_ce_4: 0.3335  loss_mask_4: 0.4206  loss_dice_4: 3.245  loss_ce_5: 0.3176  loss_mask_5: 0.4198  loss_dice_5: 3.244  loss_ce_6: 0.3273  loss_mask_6: 0.4204  loss_dice_6: 3.238  loss_ce_7: 0.3124  loss_mask_7: 0.421  loss_dice_7: 3.238  loss_ce_8: 0.3224  loss_mask_8: 0.4188  loss_dice_8: 3.24  time: 1.5341  data_time: 0.1034  lr: 8.7024e-06  max_mem: 21366M
[01/17 09:21:19] d2.utils.events INFO:  eta: 1 day, 8:41:08  iter: 12899  total_loss: 40.58  loss_ce: 0.3209  loss_mask: 0.4405  loss_dice: 3.223  loss_ce_0: 0.5766  loss_mask_0: 0.4303  loss_dice_0: 3.353  loss_ce_1: 0.3319  loss_mask_1: 0.4461  loss_dice_1: 3.258  loss_ce_2: 0.3457  loss_mask_2: 0.4436  loss_dice_2: 3.234  loss_ce_3: 0.3361  loss_mask_3: 0.4372  loss_dice_3: 3.234  loss_ce_4: 0.3206  loss_mask_4: 0.4368  loss_dice_4: 3.234  loss_ce_5: 0.3127  loss_mask_5: 0.4406  loss_dice_5: 3.241  loss_ce_6: 0.3251  loss_mask_6: 0.4412  loss_dice_6: 3.238  loss_ce_7: 0.3157  loss_mask_7: 0.4378  loss_dice_7: 3.228  loss_ce_8: 0.3055  loss_mask_8: 0.4407  loss_dice_8: 3.227  time: 1.5341  data_time: 0.0983  lr: 8.7003e-06  max_mem: 21366M
[01/17 09:21:50] d2.utils.events INFO:  eta: 1 day, 8:41:24  iter: 12919  total_loss: 40.09  loss_ce: 0.3039  loss_mask: 0.4315  loss_dice: 3.221  loss_ce_0: 0.5768  loss_mask_0: 0.4245  loss_dice_0: 3.329  loss_ce_1: 0.3469  loss_mask_1: 0.4407  loss_dice_1: 3.252  loss_ce_2: 0.3488  loss_mask_2: 0.4343  loss_dice_2: 3.237  loss_ce_3: 0.3322  loss_mask_3: 0.434  loss_dice_3: 3.228  loss_ce_4: 0.3137  loss_mask_4: 0.4336  loss_dice_4: 3.229  loss_ce_5: 0.2996  loss_mask_5: 0.4326  loss_dice_5: 3.232  loss_ce_6: 0.3056  loss_mask_6: 0.4292  loss_dice_6: 3.224  loss_ce_7: 0.3133  loss_mask_7: 0.432  loss_dice_7: 3.223  loss_ce_8: 0.3146  loss_mask_8: 0.4335  loss_dice_8: 3.224  time: 1.5341  data_time: 0.1007  lr: 8.6983e-06  max_mem: 21366M
[01/17 09:22:21] d2.utils.events INFO:  eta: 1 day, 8:41:33  iter: 12939  total_loss: 40.28  loss_ce: 0.3004  loss_mask: 0.4268  loss_dice: 3.28  loss_ce_0: 0.5723  loss_mask_0: 0.4114  loss_dice_0: 3.394  loss_ce_1: 0.322  loss_mask_1: 0.4311  loss_dice_1: 3.316  loss_ce_2: 0.3194  loss_mask_2: 0.4309  loss_dice_2: 3.283  loss_ce_3: 0.3165  loss_mask_3: 0.4304  loss_dice_3: 3.273  loss_ce_4: 0.3153  loss_mask_4: 0.4285  loss_dice_4: 3.28  loss_ce_5: 0.3017  loss_mask_5: 0.4278  loss_dice_5: 3.284  loss_ce_6: 0.2975  loss_mask_6: 0.4269  loss_dice_6: 3.266  loss_ce_7: 0.2928  loss_mask_7: 0.427  loss_dice_7: 3.271  loss_ce_8: 0.3126  loss_mask_8: 0.4269  loss_dice_8: 3.283  time: 1.5342  data_time: 0.0997  lr: 8.6963e-06  max_mem: 21366M
[01/17 09:22:52] d2.utils.events INFO:  eta: 1 day, 8:41:02  iter: 12959  total_loss: 40.71  loss_ce: 0.3064  loss_mask: 0.454  loss_dice: 3.239  loss_ce_0: 0.5911  loss_mask_0: 0.4411  loss_dice_0: 3.356  loss_ce_1: 0.3115  loss_mask_1: 0.4641  loss_dice_1: 3.277  loss_ce_2: 0.3229  loss_mask_2: 0.4583  loss_dice_2: 3.26  loss_ce_3: 0.3158  loss_mask_3: 0.4583  loss_dice_3: 3.254  loss_ce_4: 0.3191  loss_mask_4: 0.4583  loss_dice_4: 3.245  loss_ce_5: 0.3198  loss_mask_5: 0.4576  loss_dice_5: 3.242  loss_ce_6: 0.3173  loss_mask_6: 0.4548  loss_dice_6: 3.243  loss_ce_7: 0.3164  loss_mask_7: 0.454  loss_dice_7: 3.232  loss_ce_8: 0.3149  loss_mask_8: 0.4534  loss_dice_8: 3.229  time: 1.5342  data_time: 0.0901  lr: 8.6942e-06  max_mem: 21366M
[01/17 09:23:23] d2.utils.events INFO:  eta: 1 day, 8:41:04  iter: 12979  total_loss: 40.39  loss_ce: 0.3281  loss_mask: 0.4382  loss_dice: 3.263  loss_ce_0: 0.5894  loss_mask_0: 0.4328  loss_dice_0: 3.364  loss_ce_1: 0.3472  loss_mask_1: 0.4462  loss_dice_1: 3.274  loss_ce_2: 0.3537  loss_mask_2: 0.4425  loss_dice_2: 3.257  loss_ce_3: 0.3222  loss_mask_3: 0.4414  loss_dice_3: 3.258  loss_ce_4: 0.3264  loss_mask_4: 0.4408  loss_dice_4: 3.251  loss_ce_5: 0.3274  loss_mask_5: 0.4434  loss_dice_5: 3.247  loss_ce_6: 0.3212  loss_mask_6: 0.4397  loss_dice_6: 3.258  loss_ce_7: 0.3176  loss_mask_7: 0.4398  loss_dice_7: 3.249  loss_ce_8: 0.3197  loss_mask_8: 0.439  loss_dice_8: 3.254  time: 1.5342  data_time: 0.0931  lr: 8.6922e-06  max_mem: 21366M
[01/17 09:23:53] d2.utils.events INFO:  eta: 1 day, 8:40:33  iter: 12999  total_loss: 39.72  loss_ce: 0.3311  loss_mask: 0.4345  loss_dice: 3.198  loss_ce_0: 0.6238  loss_mask_0: 0.4284  loss_dice_0: 3.3  loss_ce_1: 0.3312  loss_mask_1: 0.4341  loss_dice_1: 3.219  loss_ce_2: 0.3432  loss_mask_2: 0.4348  loss_dice_2: 3.2  loss_ce_3: 0.344  loss_mask_3: 0.4329  loss_dice_3: 3.204  loss_ce_4: 0.3205  loss_mask_4: 0.4329  loss_dice_4: 3.206  loss_ce_5: 0.3304  loss_mask_5: 0.4321  loss_dice_5: 3.206  loss_ce_6: 0.312  loss_mask_6: 0.4317  loss_dice_6: 3.201  loss_ce_7: 0.3254  loss_mask_7: 0.4325  loss_dice_7: 3.198  loss_ce_8: 0.3225  loss_mask_8: 0.433  loss_dice_8: 3.193  time: 1.5342  data_time: 0.0933  lr: 8.6902e-06  max_mem: 21366M
[01/17 09:24:24] d2.utils.events INFO:  eta: 1 day, 8:40:38  iter: 13019  total_loss: 39.93  loss_ce: 0.3057  loss_mask: 0.431  loss_dice: 3.203  loss_ce_0: 0.5872  loss_mask_0: 0.4277  loss_dice_0: 3.327  loss_ce_1: 0.3328  loss_mask_1: 0.4405  loss_dice_1: 3.235  loss_ce_2: 0.3086  loss_mask_2: 0.4318  loss_dice_2: 3.211  loss_ce_3: 0.301  loss_mask_3: 0.4328  loss_dice_3: 3.204  loss_ce_4: 0.309  loss_mask_4: 0.4325  loss_dice_4: 3.207  loss_ce_5: 0.3084  loss_mask_5: 0.4304  loss_dice_5: 3.211  loss_ce_6: 0.3074  loss_mask_6: 0.4316  loss_dice_6: 3.203  loss_ce_7: 0.3051  loss_mask_7: 0.431  loss_dice_7: 3.2  loss_ce_8: 0.3114  loss_mask_8: 0.4317  loss_dice_8: 3.195  time: 1.5342  data_time: 0.1003  lr: 8.6881e-06  max_mem: 21366M
[01/17 09:24:55] d2.utils.events INFO:  eta: 1 day, 8:40:31  iter: 13039  total_loss: 39.85  loss_ce: 0.3304  loss_mask: 0.4267  loss_dice: 3.186  loss_ce_0: 0.592  loss_mask_0: 0.4189  loss_dice_0: 3.31  loss_ce_1: 0.3388  loss_mask_1: 0.4287  loss_dice_1: 3.215  loss_ce_2: 0.3419  loss_mask_2: 0.4271  loss_dice_2: 3.202  loss_ce_3: 0.3337  loss_mask_3: 0.4255  loss_dice_3: 3.192  loss_ce_4: 0.348  loss_mask_4: 0.4268  loss_dice_4: 3.187  loss_ce_5: 0.3358  loss_mask_5: 0.4249  loss_dice_5: 3.186  loss_ce_6: 0.3324  loss_mask_6: 0.4296  loss_dice_6: 3.19  loss_ce_7: 0.3265  loss_mask_7: 0.428  loss_dice_7: 3.18  loss_ce_8: 0.3255  loss_mask_8: 0.4251  loss_dice_8: 3.184  time: 1.5342  data_time: 0.1017  lr: 8.6861e-06  max_mem: 21366M
[01/17 09:25:26] d2.utils.events INFO:  eta: 1 day, 8:39:28  iter: 13059  total_loss: 39.78  loss_ce: 0.31  loss_mask: 0.4353  loss_dice: 3.154  loss_ce_0: 0.5812  loss_mask_0: 0.4293  loss_dice_0: 3.272  loss_ce_1: 0.3125  loss_mask_1: 0.4359  loss_dice_1: 3.181  loss_ce_2: 0.3275  loss_mask_2: 0.4355  loss_dice_2: 3.169  loss_ce_3: 0.3116  loss_mask_3: 0.4354  loss_dice_3: 3.152  loss_ce_4: 0.331  loss_mask_4: 0.4374  loss_dice_4: 3.16  loss_ce_5: 0.3136  loss_mask_5: 0.437  loss_dice_5: 3.163  loss_ce_6: 0.2929  loss_mask_6: 0.4352  loss_dice_6: 3.144  loss_ce_7: 0.2998  loss_mask_7: 0.4367  loss_dice_7: 3.154  loss_ce_8: 0.3026  loss_mask_8: 0.4369  loss_dice_8: 3.149  time: 1.5342  data_time: 0.0933  lr: 8.6841e-06  max_mem: 21366M
[01/17 09:25:56] d2.utils.events INFO:  eta: 1 day, 8:38:42  iter: 13079  total_loss: 40.03  loss_ce: 0.3173  loss_mask: 0.4269  loss_dice: 3.181  loss_ce_0: 0.5831  loss_mask_0: 0.418  loss_dice_0: 3.338  loss_ce_1: 0.3146  loss_mask_1: 0.4332  loss_dice_1: 3.237  loss_ce_2: 0.3421  loss_mask_2: 0.4295  loss_dice_2: 3.216  loss_ce_3: 0.3286  loss_mask_3: 0.4293  loss_dice_3: 3.189  loss_ce_4: 0.3219  loss_mask_4: 0.4293  loss_dice_4: 3.193  loss_ce_5: 0.3078  loss_mask_5: 0.429  loss_dice_5: 3.204  loss_ce_6: 0.3165  loss_mask_6: 0.4244  loss_dice_6: 3.195  loss_ce_7: 0.313  loss_mask_7: 0.4236  loss_dice_7: 3.192  loss_ce_8: 0.3166  loss_mask_8: 0.4254  loss_dice_8: 3.19  time: 1.5342  data_time: 0.1052  lr: 8.682e-06  max_mem: 21366M
[01/17 09:26:27] d2.utils.events INFO:  eta: 1 day, 8:38:27  iter: 13099  total_loss: 39.57  loss_ce: 0.314  loss_mask: 0.4354  loss_dice: 3.162  loss_ce_0: 0.5755  loss_mask_0: 0.4222  loss_dice_0: 3.274  loss_ce_1: 0.3288  loss_mask_1: 0.434  loss_dice_1: 3.197  loss_ce_2: 0.3466  loss_mask_2: 0.4316  loss_dice_2: 3.174  loss_ce_3: 0.3322  loss_mask_3: 0.4326  loss_dice_3: 3.163  loss_ce_4: 0.3079  loss_mask_4: 0.4348  loss_dice_4: 3.168  loss_ce_5: 0.3176  loss_mask_5: 0.4351  loss_dice_5: 3.175  loss_ce_6: 0.3081  loss_mask_6: 0.4359  loss_dice_6: 3.165  loss_ce_7: 0.3125  loss_mask_7: 0.4384  loss_dice_7: 3.162  loss_ce_8: 0.3138  loss_mask_8: 0.4358  loss_dice_8: 3.165  time: 1.5342  data_time: 0.1072  lr: 8.68e-06  max_mem: 21366M
[01/17 09:26:58] d2.utils.events INFO:  eta: 1 day, 8:38:42  iter: 13119  total_loss: 39.87  loss_ce: 0.3203  loss_mask: 0.4295  loss_dice: 3.152  loss_ce_0: 0.5851  loss_mask_0: 0.4167  loss_dice_0: 3.304  loss_ce_1: 0.3442  loss_mask_1: 0.4342  loss_dice_1: 3.205  loss_ce_2: 0.3605  loss_mask_2: 0.4342  loss_dice_2: 3.172  loss_ce_3: 0.3291  loss_mask_3: 0.4342  loss_dice_3: 3.16  loss_ce_4: 0.3393  loss_mask_4: 0.4324  loss_dice_4: 3.157  loss_ce_5: 0.3287  loss_mask_5: 0.4341  loss_dice_5: 3.159  loss_ce_6: 0.3152  loss_mask_6: 0.4327  loss_dice_6: 3.151  loss_ce_7: 0.3243  loss_mask_7: 0.4317  loss_dice_7: 3.155  loss_ce_8: 0.3263  loss_mask_8: 0.4313  loss_dice_8: 3.147  time: 1.5342  data_time: 0.1042  lr: 8.678e-06  max_mem: 21366M
[01/17 09:27:28] d2.utils.events INFO:  eta: 1 day, 8:37:48  iter: 13139  total_loss: 39.8  loss_ce: 0.3182  loss_mask: 0.4279  loss_dice: 3.186  loss_ce_0: 0.569  loss_mask_0: 0.4238  loss_dice_0: 3.319  loss_ce_1: 0.3259  loss_mask_1: 0.4348  loss_dice_1: 3.226  loss_ce_2: 0.336  loss_mask_2: 0.4309  loss_dice_2: 3.205  loss_ce_3: 0.3276  loss_mask_3: 0.4307  loss_dice_3: 3.195  loss_ce_4: 0.3349  loss_mask_4: 0.427  loss_dice_4: 3.194  loss_ce_5: 0.3179  loss_mask_5: 0.4286  loss_dice_5: 3.2  loss_ce_6: 0.3185  loss_mask_6: 0.4254  loss_dice_6: 3.187  loss_ce_7: 0.3039  loss_mask_7: 0.4266  loss_dice_7: 3.197  loss_ce_8: 0.3212  loss_mask_8: 0.4268  loss_dice_8: 3.187  time: 1.5341  data_time: 0.0948  lr: 8.676e-06  max_mem: 21366M
[01/17 09:27:59] d2.utils.events INFO:  eta: 1 day, 8:36:52  iter: 13159  total_loss: 40.48  loss_ce: 0.3259  loss_mask: 0.4459  loss_dice: 3.212  loss_ce_0: 0.5904  loss_mask_0: 0.4362  loss_dice_0: 3.324  loss_ce_1: 0.3215  loss_mask_1: 0.4544  loss_dice_1: 3.242  loss_ce_2: 0.3405  loss_mask_2: 0.4535  loss_dice_2: 3.216  loss_ce_3: 0.3404  loss_mask_3: 0.4505  loss_dice_3: 3.212  loss_ce_4: 0.3436  loss_mask_4: 0.4504  loss_dice_4: 3.222  loss_ce_5: 0.3248  loss_mask_5: 0.4496  loss_dice_5: 3.217  loss_ce_6: 0.3211  loss_mask_6: 0.449  loss_dice_6: 3.213  loss_ce_7: 0.3203  loss_mask_7: 0.4503  loss_dice_7: 3.208  loss_ce_8: 0.3115  loss_mask_8: 0.4488  loss_dice_8: 3.21  time: 1.5342  data_time: 0.0948  lr: 8.6739e-06  max_mem: 21366M
[01/17 09:28:29] d2.utils.events INFO:  eta: 1 day, 8:36:25  iter: 13179  total_loss: 39.78  loss_ce: 0.3094  loss_mask: 0.4201  loss_dice: 3.169  loss_ce_0: 0.5829  loss_mask_0: 0.4081  loss_dice_0: 3.311  loss_ce_1: 0.3418  loss_mask_1: 0.4191  loss_dice_1: 3.22  loss_ce_2: 0.3392  loss_mask_2: 0.4184  loss_dice_2: 3.192  loss_ce_3: 0.3213  loss_mask_3: 0.4196  loss_dice_3: 3.186  loss_ce_4: 0.3178  loss_mask_4: 0.4204  loss_dice_4: 3.182  loss_ce_5: 0.305  loss_mask_5: 0.4177  loss_dice_5: 3.181  loss_ce_6: 0.3055  loss_mask_6: 0.4191  loss_dice_6: 3.166  loss_ce_7: 0.2992  loss_mask_7: 0.4194  loss_dice_7: 3.175  loss_ce_8: 0.304  loss_mask_8: 0.4203  loss_dice_8: 3.176  time: 1.5341  data_time: 0.0916  lr: 8.6719e-06  max_mem: 21366M
[01/17 09:29:00] d2.utils.events INFO:  eta: 1 day, 8:36:13  iter: 13199  total_loss: 40.35  loss_ce: 0.3115  loss_mask: 0.4213  loss_dice: 3.178  loss_ce_0: 0.573  loss_mask_0: 0.4128  loss_dice_0: 3.299  loss_ce_1: 0.3481  loss_mask_1: 0.4289  loss_dice_1: 3.215  loss_ce_2: 0.3624  loss_mask_2: 0.4222  loss_dice_2: 3.195  loss_ce_3: 0.3255  loss_mask_3: 0.4209  loss_dice_3: 3.188  loss_ce_4: 0.3302  loss_mask_4: 0.4192  loss_dice_4: 3.187  loss_ce_5: 0.33  loss_mask_5: 0.4215  loss_dice_5: 3.187  loss_ce_6: 0.3304  loss_mask_6: 0.4219  loss_dice_6: 3.184  loss_ce_7: 0.3309  loss_mask_7: 0.4254  loss_dice_7: 3.189  loss_ce_8: 0.3219  loss_mask_8: 0.4218  loss_dice_8: 3.18  time: 1.5341  data_time: 0.0979  lr: 8.6699e-06  max_mem: 21366M
[01/17 09:29:31] d2.utils.events INFO:  eta: 1 day, 8:36:15  iter: 13219  total_loss: 40.13  loss_ce: 0.3315  loss_mask: 0.4351  loss_dice: 3.2  loss_ce_0: 0.6013  loss_mask_0: 0.419  loss_dice_0: 3.334  loss_ce_1: 0.3552  loss_mask_1: 0.4353  loss_dice_1: 3.25  loss_ce_2: 0.3474  loss_mask_2: 0.4356  loss_dice_2: 3.21  loss_ce_3: 0.3406  loss_mask_3: 0.4361  loss_dice_3: 3.212  loss_ce_4: 0.3406  loss_mask_4: 0.4392  loss_dice_4: 3.2  loss_ce_5: 0.3363  loss_mask_5: 0.4395  loss_dice_5: 3.216  loss_ce_6: 0.3212  loss_mask_6: 0.4383  loss_dice_6: 3.201  loss_ce_7: 0.3356  loss_mask_7: 0.434  loss_dice_7: 3.201  loss_ce_8: 0.3157  loss_mask_8: 0.4337  loss_dice_8: 3.204  time: 1.5342  data_time: 0.1044  lr: 8.6678e-06  max_mem: 21366M
[01/17 09:30:02] d2.utils.events INFO:  eta: 1 day, 8:35:50  iter: 13239  total_loss: 40.13  loss_ce: 0.3213  loss_mask: 0.4488  loss_dice: 3.189  loss_ce_0: 0.5732  loss_mask_0: 0.435  loss_dice_0: 3.331  loss_ce_1: 0.3216  loss_mask_1: 0.4574  loss_dice_1: 3.235  loss_ce_2: 0.3211  loss_mask_2: 0.4528  loss_dice_2: 3.213  loss_ce_3: 0.3254  loss_mask_3: 0.4493  loss_dice_3: 3.201  loss_ce_4: 0.3283  loss_mask_4: 0.4462  loss_dice_4: 3.202  loss_ce_5: 0.3089  loss_mask_5: 0.445  loss_dice_5: 3.204  loss_ce_6: 0.2982  loss_mask_6: 0.4483  loss_dice_6: 3.193  loss_ce_7: 0.3148  loss_mask_7: 0.4476  loss_dice_7: 3.197  loss_ce_8: 0.3215  loss_mask_8: 0.4472  loss_dice_8: 3.197  time: 1.5342  data_time: 0.1020  lr: 8.6658e-06  max_mem: 21366M
[01/17 09:30:34] d2.utils.events INFO:  eta: 1 day, 8:35:25  iter: 13259  total_loss: 39.6  loss_ce: 0.3265  loss_mask: 0.4183  loss_dice: 3.199  loss_ce_0: 0.5679  loss_mask_0: 0.4078  loss_dice_0: 3.315  loss_ce_1: 0.3542  loss_mask_1: 0.4189  loss_dice_1: 3.24  loss_ce_2: 0.3475  loss_mask_2: 0.4186  loss_dice_2: 3.221  loss_ce_3: 0.3356  loss_mask_3: 0.4178  loss_dice_3: 3.2  loss_ce_4: 0.3264  loss_mask_4: 0.4171  loss_dice_4: 3.196  loss_ce_5: 0.3133  loss_mask_5: 0.4166  loss_dice_5: 3.202  loss_ce_6: 0.3188  loss_mask_6: 0.4181  loss_dice_6: 3.197  loss_ce_7: 0.3336  loss_mask_7: 0.4166  loss_dice_7: 3.2  loss_ce_8: 0.3249  loss_mask_8: 0.4175  loss_dice_8: 3.196  time: 1.5342  data_time: 0.1094  lr: 8.6638e-06  max_mem: 21366M
[01/17 09:31:04] d2.utils.events INFO:  eta: 1 day, 8:34:37  iter: 13279  total_loss: 40.72  loss_ce: 0.3006  loss_mask: 0.4293  loss_dice: 3.248  loss_ce_0: 0.587  loss_mask_0: 0.422  loss_dice_0: 3.378  loss_ce_1: 0.3222  loss_mask_1: 0.4332  loss_dice_1: 3.281  loss_ce_2: 0.3254  loss_mask_2: 0.4306  loss_dice_2: 3.264  loss_ce_3: 0.3175  loss_mask_3: 0.4287  loss_dice_3: 3.258  loss_ce_4: 0.3128  loss_mask_4: 0.4284  loss_dice_4: 3.254  loss_ce_5: 0.3045  loss_mask_5: 0.4306  loss_dice_5: 3.263  loss_ce_6: 0.303  loss_mask_6: 0.4291  loss_dice_6: 3.255  loss_ce_7: 0.3129  loss_mask_7: 0.4293  loss_dice_7: 3.256  loss_ce_8: 0.3045  loss_mask_8: 0.4296  loss_dice_8: 3.253  time: 1.5342  data_time: 0.1071  lr: 8.6617e-06  max_mem: 21366M
[01/17 09:31:35] d2.utils.events INFO:  eta: 1 day, 8:34:29  iter: 13299  total_loss: 40.05  loss_ce: 0.3308  loss_mask: 0.4314  loss_dice: 3.211  loss_ce_0: 0.6245  loss_mask_0: 0.4227  loss_dice_0: 3.321  loss_ce_1: 0.3528  loss_mask_1: 0.438  loss_dice_1: 3.24  loss_ce_2: 0.3438  loss_mask_2: 0.4344  loss_dice_2: 3.222  loss_ce_3: 0.3372  loss_mask_3: 0.4308  loss_dice_3: 3.21  loss_ce_4: 0.3464  loss_mask_4: 0.4304  loss_dice_4: 3.215  loss_ce_5: 0.3199  loss_mask_5: 0.431  loss_dice_5: 3.22  loss_ce_6: 0.3268  loss_mask_6: 0.4319  loss_dice_6: 3.212  loss_ce_7: 0.3302  loss_mask_7: 0.4308  loss_dice_7: 3.207  loss_ce_8: 0.3256  loss_mask_8: 0.43  loss_dice_8: 3.211  time: 1.5342  data_time: 0.0936  lr: 8.6597e-06  max_mem: 21366M
[01/17 09:32:06] d2.utils.events INFO:  eta: 1 day, 8:34:41  iter: 13319  total_loss: 39.88  loss_ce: 0.3222  loss_mask: 0.4215  loss_dice: 3.185  loss_ce_0: 0.5816  loss_mask_0: 0.4086  loss_dice_0: 3.319  loss_ce_1: 0.327  loss_mask_1: 0.4223  loss_dice_1: 3.22  loss_ce_2: 0.3438  loss_mask_2: 0.4203  loss_dice_2: 3.209  loss_ce_3: 0.3221  loss_mask_3: 0.419  loss_dice_3: 3.191  loss_ce_4: 0.3247  loss_mask_4: 0.4191  loss_dice_4: 3.193  loss_ce_5: 0.3146  loss_mask_5: 0.4212  loss_dice_5: 3.199  loss_ce_6: 0.3224  loss_mask_6: 0.422  loss_dice_6: 3.193  loss_ce_7: 0.3212  loss_mask_7: 0.4208  loss_dice_7: 3.193  loss_ce_8: 0.3182  loss_mask_8: 0.4193  loss_dice_8: 3.191  time: 1.5342  data_time: 0.0994  lr: 8.6577e-06  max_mem: 21366M
[01/17 09:32:37] d2.utils.events INFO:  eta: 1 day, 8:35:04  iter: 13339  total_loss: 40.85  loss_ce: 0.3528  loss_mask: 0.4255  loss_dice: 3.24  loss_ce_0: 0.5987  loss_mask_0: 0.4275  loss_dice_0: 3.372  loss_ce_1: 0.3374  loss_mask_1: 0.4332  loss_dice_1: 3.287  loss_ce_2: 0.3557  loss_mask_2: 0.4257  loss_dice_2: 3.264  loss_ce_3: 0.3423  loss_mask_3: 0.4267  loss_dice_3: 3.246  loss_ce_4: 0.3509  loss_mask_4: 0.4262  loss_dice_4: 3.246  loss_ce_5: 0.3591  loss_mask_5: 0.4244  loss_dice_5: 3.26  loss_ce_6: 0.3493  loss_mask_6: 0.4268  loss_dice_6: 3.256  loss_ce_7: 0.3465  loss_mask_7: 0.4249  loss_dice_7: 3.257  loss_ce_8: 0.3555  loss_mask_8: 0.4246  loss_dice_8: 3.257  time: 1.5343  data_time: 0.1050  lr: 8.6556e-06  max_mem: 21366M
[01/17 09:33:08] d2.utils.events INFO:  eta: 1 day, 8:35:02  iter: 13359  total_loss: 40.2  loss_ce: 0.3238  loss_mask: 0.4349  loss_dice: 3.189  loss_ce_0: 0.5761  loss_mask_0: 0.4174  loss_dice_0: 3.31  loss_ce_1: 0.3198  loss_mask_1: 0.4321  loss_dice_1: 3.228  loss_ce_2: 0.3274  loss_mask_2: 0.4304  loss_dice_2: 3.2  loss_ce_3: 0.3364  loss_mask_3: 0.4323  loss_dice_3: 3.185  loss_ce_4: 0.3374  loss_mask_4: 0.4336  loss_dice_4: 3.183  loss_ce_5: 0.3141  loss_mask_5: 0.4317  loss_dice_5: 3.195  loss_ce_6: 0.3112  loss_mask_6: 0.4331  loss_dice_6: 3.185  loss_ce_7: 0.3201  loss_mask_7: 0.4331  loss_dice_7: 3.19  loss_ce_8: 0.317  loss_mask_8: 0.4327  loss_dice_8: 3.19  time: 1.5343  data_time: 0.0982  lr: 8.6536e-06  max_mem: 21366M
[01/17 09:33:39] d2.utils.events INFO:  eta: 1 day, 8:35:16  iter: 13379  total_loss: 39.35  loss_ce: 0.3126  loss_mask: 0.4279  loss_dice: 3.146  loss_ce_0: 0.5731  loss_mask_0: 0.4176  loss_dice_0: 3.282  loss_ce_1: 0.3259  loss_mask_1: 0.4312  loss_dice_1: 3.196  loss_ce_2: 0.3312  loss_mask_2: 0.4297  loss_dice_2: 3.169  loss_ce_3: 0.325  loss_mask_3: 0.4284  loss_dice_3: 3.158  loss_ce_4: 0.3064  loss_mask_4: 0.429  loss_dice_4: 3.153  loss_ce_5: 0.3244  loss_mask_5: 0.429  loss_dice_5: 3.157  loss_ce_6: 0.3279  loss_mask_6: 0.4272  loss_dice_6: 3.149  loss_ce_7: 0.3084  loss_mask_7: 0.426  loss_dice_7: 3.144  loss_ce_8: 0.3038  loss_mask_8: 0.4256  loss_dice_8: 3.149  time: 1.5343  data_time: 0.0959  lr: 8.6516e-06  max_mem: 21366M
[01/17 09:34:10] d2.utils.events INFO:  eta: 1 day, 8:35:42  iter: 13399  total_loss: 40.13  loss_ce: 0.3019  loss_mask: 0.4284  loss_dice: 3.155  loss_ce_0: 0.5872  loss_mask_0: 0.4213  loss_dice_0: 3.298  loss_ce_1: 0.3227  loss_mask_1: 0.4324  loss_dice_1: 3.21  loss_ce_2: 0.3293  loss_mask_2: 0.4298  loss_dice_2: 3.182  loss_ce_3: 0.3345  loss_mask_3: 0.4293  loss_dice_3: 3.165  loss_ce_4: 0.3219  loss_mask_4: 0.431  loss_dice_4: 3.164  loss_ce_5: 0.3027  loss_mask_5: 0.4298  loss_dice_5: 3.174  loss_ce_6: 0.3115  loss_mask_6: 0.4297  loss_dice_6: 3.167  loss_ce_7: 0.3166  loss_mask_7: 0.4263  loss_dice_7: 3.161  loss_ce_8: 0.3122  loss_mask_8: 0.4276  loss_dice_8: 3.161  time: 1.5344  data_time: 0.1030  lr: 8.6495e-06  max_mem: 21366M
[01/17 09:34:41] d2.utils.events INFO:  eta: 1 day, 8:35:51  iter: 13419  total_loss: 39.27  loss_ce: 0.3133  loss_mask: 0.4245  loss_dice: 3.161  loss_ce_0: 0.5684  loss_mask_0: 0.4134  loss_dice_0: 3.3  loss_ce_1: 0.3274  loss_mask_1: 0.4307  loss_dice_1: 3.205  loss_ce_2: 0.3283  loss_mask_2: 0.4272  loss_dice_2: 3.182  loss_ce_3: 0.3209  loss_mask_3: 0.4264  loss_dice_3: 3.168  loss_ce_4: 0.3212  loss_mask_4: 0.4232  loss_dice_4: 3.167  loss_ce_5: 0.3142  loss_mask_5: 0.4228  loss_dice_5: 3.17  loss_ce_6: 0.3026  loss_mask_6: 0.4244  loss_dice_6: 3.169  loss_ce_7: 0.3094  loss_mask_7: 0.4248  loss_dice_7: 3.17  loss_ce_8: 0.3094  loss_mask_8: 0.426  loss_dice_8: 3.16  time: 1.5344  data_time: 0.0942  lr: 8.6475e-06  max_mem: 21366M
[01/17 09:35:12] d2.utils.events INFO:  eta: 1 day, 8:34:41  iter: 13439  total_loss: 39.68  loss_ce: 0.2962  loss_mask: 0.4204  loss_dice: 3.178  loss_ce_0: 0.5884  loss_mask_0: 0.4118  loss_dice_0: 3.3  loss_ce_1: 0.3476  loss_mask_1: 0.4271  loss_dice_1: 3.205  loss_ce_2: 0.3362  loss_mask_2: 0.4221  loss_dice_2: 3.186  loss_ce_3: 0.3257  loss_mask_3: 0.4207  loss_dice_3: 3.189  loss_ce_4: 0.3212  loss_mask_4: 0.4206  loss_dice_4: 3.174  loss_ce_5: 0.3118  loss_mask_5: 0.4182  loss_dice_5: 3.177  loss_ce_6: 0.3078  loss_mask_6: 0.4189  loss_dice_6: 3.184  loss_ce_7: 0.2802  loss_mask_7: 0.4192  loss_dice_7: 3.182  loss_ce_8: 0.2947  loss_mask_8: 0.4198  loss_dice_8: 3.176  time: 1.5344  data_time: 0.0944  lr: 8.6455e-06  max_mem: 21366M
[01/17 09:35:43] d2.utils.events INFO:  eta: 1 day, 8:33:51  iter: 13459  total_loss: 39.78  loss_ce: 0.3077  loss_mask: 0.423  loss_dice: 3.194  loss_ce_0: 0.5866  loss_mask_0: 0.4173  loss_dice_0: 3.322  loss_ce_1: 0.3404  loss_mask_1: 0.422  loss_dice_1: 3.224  loss_ce_2: 0.3482  loss_mask_2: 0.4191  loss_dice_2: 3.212  loss_ce_3: 0.3569  loss_mask_3: 0.4198  loss_dice_3: 3.199  loss_ce_4: 0.3189  loss_mask_4: 0.4218  loss_dice_4: 3.208  loss_ce_5: 0.3084  loss_mask_5: 0.422  loss_dice_5: 3.208  loss_ce_6: 0.3182  loss_mask_6: 0.4235  loss_dice_6: 3.2  loss_ce_7: 0.3037  loss_mask_7: 0.4205  loss_dice_7: 3.198  loss_ce_8: 0.3112  loss_mask_8: 0.4217  loss_dice_8: 3.199  time: 1.5344  data_time: 0.0996  lr: 8.6434e-06  max_mem: 21366M
[01/17 09:36:14] d2.utils.events INFO:  eta: 1 day, 8:34:11  iter: 13479  total_loss: 39.79  loss_ce: 0.3012  loss_mask: 0.4348  loss_dice: 3.166  loss_ce_0: 0.5798  loss_mask_0: 0.4164  loss_dice_0: 3.298  loss_ce_1: 0.3327  loss_mask_1: 0.4317  loss_dice_1: 3.202  loss_ce_2: 0.3254  loss_mask_2: 0.4332  loss_dice_2: 3.177  loss_ce_3: 0.3195  loss_mask_3: 0.4339  loss_dice_3: 3.169  loss_ce_4: 0.3194  loss_mask_4: 0.4328  loss_dice_4: 3.17  loss_ce_5: 0.3058  loss_mask_5: 0.4321  loss_dice_5: 3.176  loss_ce_6: 0.3104  loss_mask_6: 0.4312  loss_dice_6: 3.164  loss_ce_7: 0.3101  loss_mask_7: 0.4303  loss_dice_7: 3.166  loss_ce_8: 0.3049  loss_mask_8: 0.4323  loss_dice_8: 3.162  time: 1.5344  data_time: 0.0907  lr: 8.6414e-06  max_mem: 21366M
[01/17 09:36:45] d2.utils.events INFO:  eta: 1 day, 8:33:40  iter: 13499  total_loss: 39.85  loss_ce: 0.3015  loss_mask: 0.4101  loss_dice: 3.214  loss_ce_0: 0.5949  loss_mask_0: 0.4065  loss_dice_0: 3.329  loss_ce_1: 0.3336  loss_mask_1: 0.4147  loss_dice_1: 3.247  loss_ce_2: 0.357  loss_mask_2: 0.4116  loss_dice_2: 3.233  loss_ce_3: 0.3329  loss_mask_3: 0.4083  loss_dice_3: 3.222  loss_ce_4: 0.3167  loss_mask_4: 0.4073  loss_dice_4: 3.225  loss_ce_5: 0.3118  loss_mask_5: 0.4099  loss_dice_5: 3.226  loss_ce_6: 0.3033  loss_mask_6: 0.4089  loss_dice_6: 3.211  loss_ce_7: 0.302  loss_mask_7: 0.4109  loss_dice_7: 3.212  loss_ce_8: 0.3124  loss_mask_8: 0.4089  loss_dice_8: 3.227  time: 1.5344  data_time: 0.0978  lr: 8.6394e-06  max_mem: 21366M
[01/17 09:37:16] d2.utils.events INFO:  eta: 1 day, 8:32:49  iter: 13519  total_loss: 39.26  loss_ce: 0.3243  loss_mask: 0.4137  loss_dice: 3.151  loss_ce_0: 0.5889  loss_mask_0: 0.4051  loss_dice_0: 3.278  loss_ce_1: 0.3277  loss_mask_1: 0.4134  loss_dice_1: 3.199  loss_ce_2: 0.3419  loss_mask_2: 0.411  loss_dice_2: 3.168  loss_ce_3: 0.3253  loss_mask_3: 0.4129  loss_dice_3: 3.152  loss_ce_4: 0.3246  loss_mask_4: 0.4139  loss_dice_4: 3.154  loss_ce_5: 0.299  loss_mask_5: 0.4151  loss_dice_5: 3.147  loss_ce_6: 0.3169  loss_mask_6: 0.4144  loss_dice_6: 3.146  loss_ce_7: 0.3069  loss_mask_7: 0.4142  loss_dice_7: 3.15  loss_ce_8: 0.3098  loss_mask_8: 0.4149  loss_dice_8: 3.151  time: 1.5344  data_time: 0.1078  lr: 8.6373e-06  max_mem: 21366M
[01/17 09:37:46] d2.utils.events INFO:  eta: 1 day, 8:32:39  iter: 13539  total_loss: 39.94  loss_ce: 0.316  loss_mask: 0.4304  loss_dice: 3.232  loss_ce_0: 0.5943  loss_mask_0: 0.4186  loss_dice_0: 3.361  loss_ce_1: 0.3395  loss_mask_1: 0.432  loss_dice_1: 3.275  loss_ce_2: 0.3446  loss_mask_2: 0.432  loss_dice_2: 3.251  loss_ce_3: 0.3236  loss_mask_3: 0.4325  loss_dice_3: 3.241  loss_ce_4: 0.3343  loss_mask_4: 0.4319  loss_dice_4: 3.242  loss_ce_5: 0.3194  loss_mask_5: 0.43  loss_dice_5: 3.243  loss_ce_6: 0.3195  loss_mask_6: 0.4312  loss_dice_6: 3.242  loss_ce_7: 0.3238  loss_mask_7: 0.4292  loss_dice_7: 3.235  loss_ce_8: 0.3272  loss_mask_8: 0.4286  loss_dice_8: 3.235  time: 1.5344  data_time: 0.0929  lr: 8.6353e-06  max_mem: 21366M
[01/17 09:38:17] d2.utils.events INFO:  eta: 1 day, 8:30:58  iter: 13559  total_loss: 39.39  loss_ce: 0.3182  loss_mask: 0.4314  loss_dice: 3.116  loss_ce_0: 0.6091  loss_mask_0: 0.4162  loss_dice_0: 3.233  loss_ce_1: 0.3618  loss_mask_1: 0.427  loss_dice_1: 3.161  loss_ce_2: 0.3374  loss_mask_2: 0.431  loss_dice_2: 3.135  loss_ce_3: 0.3307  loss_mask_3: 0.4285  loss_dice_3: 3.129  loss_ce_4: 0.3217  loss_mask_4: 0.4298  loss_dice_4: 3.116  loss_ce_5: 0.3241  loss_mask_5: 0.4301  loss_dice_5: 3.126  loss_ce_6: 0.3218  loss_mask_6: 0.4297  loss_dice_6: 3.117  loss_ce_7: 0.3152  loss_mask_7: 0.4306  loss_dice_7: 3.118  loss_ce_8: 0.3175  loss_mask_8: 0.4301  loss_dice_8: 3.116  time: 1.5344  data_time: 0.0954  lr: 8.6333e-06  max_mem: 21366M
[01/17 09:38:47] d2.utils.events INFO:  eta: 1 day, 8:31:43  iter: 13579  total_loss: 40.18  loss_ce: 0.3266  loss_mask: 0.4345  loss_dice: 3.191  loss_ce_0: 0.6071  loss_mask_0: 0.4209  loss_dice_0: 3.319  loss_ce_1: 0.3509  loss_mask_1: 0.436  loss_dice_1: 3.223  loss_ce_2: 0.356  loss_mask_2: 0.4313  loss_dice_2: 3.2  loss_ce_3: 0.3333  loss_mask_3: 0.4308  loss_dice_3: 3.194  loss_ce_4: 0.3413  loss_mask_4: 0.434  loss_dice_4: 3.191  loss_ce_5: 0.311  loss_mask_5: 0.4352  loss_dice_5: 3.194  loss_ce_6: 0.3238  loss_mask_6: 0.4348  loss_dice_6: 3.183  loss_ce_7: 0.3169  loss_mask_7: 0.4353  loss_dice_7: 3.188  loss_ce_8: 0.32  loss_mask_8: 0.4352  loss_dice_8: 3.196  time: 1.5344  data_time: 0.0870  lr: 8.6312e-06  max_mem: 21366M
[01/17 09:39:18] d2.utils.events INFO:  eta: 1 day, 8:31:16  iter: 13599  total_loss: 39.33  loss_ce: 0.3058  loss_mask: 0.4272  loss_dice: 3.157  loss_ce_0: 0.5739  loss_mask_0: 0.4097  loss_dice_0: 3.287  loss_ce_1: 0.3226  loss_mask_1: 0.4286  loss_dice_1: 3.192  loss_ce_2: 0.3258  loss_mask_2: 0.4246  loss_dice_2: 3.173  loss_ce_3: 0.3206  loss_mask_3: 0.4237  loss_dice_3: 3.162  loss_ce_4: 0.3129  loss_mask_4: 0.4225  loss_dice_4: 3.159  loss_ce_5: 0.319  loss_mask_5: 0.4246  loss_dice_5: 3.173  loss_ce_6: 0.3178  loss_mask_6: 0.4269  loss_dice_6: 3.155  loss_ce_7: 0.3141  loss_mask_7: 0.4258  loss_dice_7: 3.157  loss_ce_8: 0.3163  loss_mask_8: 0.4254  loss_dice_8: 3.155  time: 1.5344  data_time: 0.0960  lr: 8.6292e-06  max_mem: 21366M
[01/17 09:39:48] d2.utils.events INFO:  eta: 1 day, 8:29:26  iter: 13619  total_loss: 39.69  loss_ce: 0.3177  loss_mask: 0.4263  loss_dice: 3.193  loss_ce_0: 0.5741  loss_mask_0: 0.422  loss_dice_0: 3.311  loss_ce_1: 0.3071  loss_mask_1: 0.4353  loss_dice_1: 3.228  loss_ce_2: 0.3257  loss_mask_2: 0.4266  loss_dice_2: 3.197  loss_ce_3: 0.3092  loss_mask_3: 0.426  loss_dice_3: 3.199  loss_ce_4: 0.3281  loss_mask_4: 0.4264  loss_dice_4: 3.193  loss_ce_5: 0.3024  loss_mask_5: 0.4265  loss_dice_5: 3.206  loss_ce_6: 0.3287  loss_mask_6: 0.4262  loss_dice_6: 3.193  loss_ce_7: 0.327  loss_mask_7: 0.4257  loss_dice_7: 3.197  loss_ce_8: 0.319  loss_mask_8: 0.4257  loss_dice_8: 3.186  time: 1.5343  data_time: 0.0904  lr: 8.6272e-06  max_mem: 21366M
[01/17 09:40:19] d2.utils.events INFO:  eta: 1 day, 8:27:39  iter: 13639  total_loss: 39.69  loss_ce: 0.3241  loss_mask: 0.4354  loss_dice: 3.169  loss_ce_0: 0.5754  loss_mask_0: 0.4252  loss_dice_0: 3.278  loss_ce_1: 0.3402  loss_mask_1: 0.4411  loss_dice_1: 3.205  loss_ce_2: 0.3215  loss_mask_2: 0.4431  loss_dice_2: 3.18  loss_ce_3: 0.3142  loss_mask_3: 0.4385  loss_dice_3: 3.167  loss_ce_4: 0.3199  loss_mask_4: 0.4357  loss_dice_4: 3.172  loss_ce_5: 0.3209  loss_mask_5: 0.436  loss_dice_5: 3.166  loss_ce_6: 0.333  loss_mask_6: 0.4373  loss_dice_6: 3.16  loss_ce_7: 0.3037  loss_mask_7: 0.4377  loss_dice_7: 3.17  loss_ce_8: 0.3139  loss_mask_8: 0.4352  loss_dice_8: 3.175  time: 1.5343  data_time: 0.0955  lr: 8.6251e-06  max_mem: 21366M
[01/17 09:40:50] d2.utils.events INFO:  eta: 1 day, 8:27:22  iter: 13659  total_loss: 39.63  loss_ce: 0.3144  loss_mask: 0.4309  loss_dice: 3.178  loss_ce_0: 0.5812  loss_mask_0: 0.4199  loss_dice_0: 3.298  loss_ce_1: 0.3415  loss_mask_1: 0.4314  loss_dice_1: 3.227  loss_ce_2: 0.3568  loss_mask_2: 0.4294  loss_dice_2: 3.195  loss_ce_3: 0.3188  loss_mask_3: 0.4285  loss_dice_3: 3.191  loss_ce_4: 0.3168  loss_mask_4: 0.4295  loss_dice_4: 3.189  loss_ce_5: 0.3142  loss_mask_5: 0.4296  loss_dice_5: 3.183  loss_ce_6: 0.2935  loss_mask_6: 0.4307  loss_dice_6: 3.184  loss_ce_7: 0.2975  loss_mask_7: 0.4278  loss_dice_7: 3.187  loss_ce_8: 0.3124  loss_mask_8: 0.4306  loss_dice_8: 3.19  time: 1.5343  data_time: 0.1070  lr: 8.6231e-06  max_mem: 21366M
[01/17 09:41:20] d2.utils.events INFO:  eta: 1 day, 8:26:51  iter: 13679  total_loss: 39.54  loss_ce: 0.3073  loss_mask: 0.4398  loss_dice: 3.169  loss_ce_0: 0.5766  loss_mask_0: 0.429  loss_dice_0: 3.296  loss_ce_1: 0.33  loss_mask_1: 0.4469  loss_dice_1: 3.206  loss_ce_2: 0.3248  loss_mask_2: 0.4426  loss_dice_2: 3.17  loss_ce_3: 0.3131  loss_mask_3: 0.4398  loss_dice_3: 3.179  loss_ce_4: 0.3159  loss_mask_4: 0.4392  loss_dice_4: 3.181  loss_ce_5: 0.3053  loss_mask_5: 0.4389  loss_dice_5: 3.185  loss_ce_6: 0.3151  loss_mask_6: 0.4395  loss_dice_6: 3.177  loss_ce_7: 0.2971  loss_mask_7: 0.4388  loss_dice_7: 3.174  loss_ce_8: 0.3025  loss_mask_8: 0.4411  loss_dice_8: 3.165  time: 1.5343  data_time: 0.0885  lr: 8.6211e-06  max_mem: 21366M
[01/17 09:41:51] d2.utils.events INFO:  eta: 1 day, 8:27:05  iter: 13699  total_loss: 39.3  loss_ce: 0.289  loss_mask: 0.429  loss_dice: 3.143  loss_ce_0: 0.5604  loss_mask_0: 0.422  loss_dice_0: 3.262  loss_ce_1: 0.3063  loss_mask_1: 0.4295  loss_dice_1: 3.161  loss_ce_2: 0.3302  loss_mask_2: 0.427  loss_dice_2: 3.151  loss_ce_3: 0.2979  loss_mask_3: 0.4294  loss_dice_3: 3.13  loss_ce_4: 0.2982  loss_mask_4: 0.4303  loss_dice_4: 3.133  loss_ce_5: 0.2934  loss_mask_5: 0.4267  loss_dice_5: 3.149  loss_ce_6: 0.304  loss_mask_6: 0.428  loss_dice_6: 3.132  loss_ce_7: 0.2937  loss_mask_7: 0.4272  loss_dice_7: 3.137  loss_ce_8: 0.2932  loss_mask_8: 0.4284  loss_dice_8: 3.137  time: 1.5343  data_time: 0.1049  lr: 8.619e-06  max_mem: 21366M
[01/17 09:42:22] d2.utils.events INFO:  eta: 1 day, 8:26:25  iter: 13719  total_loss: 39.13  loss_ce: 0.2851  loss_mask: 0.4217  loss_dice: 3.147  loss_ce_0: 0.5466  loss_mask_0: 0.411  loss_dice_0: 3.261  loss_ce_1: 0.3165  loss_mask_1: 0.427  loss_dice_1: 3.168  loss_ce_2: 0.3271  loss_mask_2: 0.4218  loss_dice_2: 3.152  loss_ce_3: 0.3102  loss_mask_3: 0.4216  loss_dice_3: 3.147  loss_ce_4: 0.3011  loss_mask_4: 0.4209  loss_dice_4: 3.146  loss_ce_5: 0.3029  loss_mask_5: 0.421  loss_dice_5: 3.146  loss_ce_6: 0.2994  loss_mask_6: 0.4213  loss_dice_6: 3.137  loss_ce_7: 0.2927  loss_mask_7: 0.4192  loss_dice_7: 3.141  loss_ce_8: 0.2978  loss_mask_8: 0.4199  loss_dice_8: 3.145  time: 1.5343  data_time: 0.0935  lr: 8.617e-06  max_mem: 21366M
[01/17 09:42:52] d2.utils.events INFO:  eta: 1 day, 8:26:04  iter: 13739  total_loss: 39.47  loss_ce: 0.2776  loss_mask: 0.421  loss_dice: 3.177  loss_ce_0: 0.5444  loss_mask_0: 0.4133  loss_dice_0: 3.305  loss_ce_1: 0.3171  loss_mask_1: 0.4272  loss_dice_1: 3.217  loss_ce_2: 0.2939  loss_mask_2: 0.4248  loss_dice_2: 3.195  loss_ce_3: 0.3084  loss_mask_3: 0.4248  loss_dice_3: 3.175  loss_ce_4: 0.2874  loss_mask_4: 0.4253  loss_dice_4: 3.18  loss_ce_5: 0.3047  loss_mask_5: 0.4231  loss_dice_5: 3.183  loss_ce_6: 0.2966  loss_mask_6: 0.4227  loss_dice_6: 3.186  loss_ce_7: 0.3011  loss_mask_7: 0.4223  loss_dice_7: 3.177  loss_ce_8: 0.2939  loss_mask_8: 0.4233  loss_dice_8: 3.19  time: 1.5343  data_time: 0.1006  lr: 8.615e-06  max_mem: 21366M
[01/17 09:43:23] d2.utils.events INFO:  eta: 1 day, 8:25:24  iter: 13759  total_loss: 38.86  loss_ce: 0.2933  loss_mask: 0.4325  loss_dice: 3.122  loss_ce_0: 0.564  loss_mask_0: 0.4205  loss_dice_0: 3.256  loss_ce_1: 0.3002  loss_mask_1: 0.4332  loss_dice_1: 3.17  loss_ce_2: 0.2944  loss_mask_2: 0.4336  loss_dice_2: 3.138  loss_ce_3: 0.2765  loss_mask_3: 0.4331  loss_dice_3: 3.126  loss_ce_4: 0.3055  loss_mask_4: 0.4332  loss_dice_4: 3.13  loss_ce_5: 0.2937  loss_mask_5: 0.4345  loss_dice_5: 3.137  loss_ce_6: 0.2703  loss_mask_6: 0.4353  loss_dice_6: 3.128  loss_ce_7: 0.3066  loss_mask_7: 0.4341  loss_dice_7: 3.119  loss_ce_8: 0.287  loss_mask_8: 0.4331  loss_dice_8: 3.126  time: 1.5343  data_time: 0.1026  lr: 8.6129e-06  max_mem: 21366M
[01/17 09:43:54] d2.utils.events INFO:  eta: 1 day, 8:25:03  iter: 13779  total_loss: 40.18  loss_ce: 0.3168  loss_mask: 0.4251  loss_dice: 3.223  loss_ce_0: 0.5864  loss_mask_0: 0.4055  loss_dice_0: 3.338  loss_ce_1: 0.332  loss_mask_1: 0.419  loss_dice_1: 3.256  loss_ce_2: 0.3406  loss_mask_2: 0.418  loss_dice_2: 3.234  loss_ce_3: 0.3284  loss_mask_3: 0.4212  loss_dice_3: 3.224  loss_ce_4: 0.3323  loss_mask_4: 0.4207  loss_dice_4: 3.225  loss_ce_5: 0.327  loss_mask_5: 0.4205  loss_dice_5: 3.232  loss_ce_6: 0.3327  loss_mask_6: 0.4205  loss_dice_6: 3.227  loss_ce_7: 0.3225  loss_mask_7: 0.4225  loss_dice_7: 3.22  loss_ce_8: 0.3216  loss_mask_8: 0.4236  loss_dice_8: 3.217  time: 1.5343  data_time: 0.0978  lr: 8.6109e-06  max_mem: 21366M
[01/17 09:44:24] d2.utils.events INFO:  eta: 1 day, 8:24:32  iter: 13799  total_loss: 39.19  loss_ce: 0.3027  loss_mask: 0.4198  loss_dice: 3.14  loss_ce_0: 0.5951  loss_mask_0: 0.4092  loss_dice_0: 3.267  loss_ce_1: 0.3171  loss_mask_1: 0.4233  loss_dice_1: 3.178  loss_ce_2: 0.3255  loss_mask_2: 0.4258  loss_dice_2: 3.161  loss_ce_3: 0.315  loss_mask_3: 0.4239  loss_dice_3: 3.154  loss_ce_4: 0.3146  loss_mask_4: 0.4251  loss_dice_4: 3.145  loss_ce_5: 0.3072  loss_mask_5: 0.4229  loss_dice_5: 3.147  loss_ce_6: 0.3127  loss_mask_6: 0.4245  loss_dice_6: 3.141  loss_ce_7: 0.311  loss_mask_7: 0.4223  loss_dice_7: 3.142  loss_ce_8: 0.3013  loss_mask_8: 0.4218  loss_dice_8: 3.147  time: 1.5343  data_time: 0.0878  lr: 8.6089e-06  max_mem: 21366M
[01/17 09:44:55] d2.utils.events INFO:  eta: 1 day, 8:24:39  iter: 13819  total_loss: 39.69  loss_ce: 0.3131  loss_mask: 0.4142  loss_dice: 3.149  loss_ce_0: 0.5966  loss_mask_0: 0.4082  loss_dice_0: 3.284  loss_ce_1: 0.3438  loss_mask_1: 0.4215  loss_dice_1: 3.206  loss_ce_2: 0.3222  loss_mask_2: 0.4167  loss_dice_2: 3.17  loss_ce_3: 0.3198  loss_mask_3: 0.4151  loss_dice_3: 3.164  loss_ce_4: 0.3153  loss_mask_4: 0.4145  loss_dice_4: 3.155  loss_ce_5: 0.3004  loss_mask_5: 0.414  loss_dice_5: 3.158  loss_ce_6: 0.2999  loss_mask_6: 0.4145  loss_dice_6: 3.143  loss_ce_7: 0.2949  loss_mask_7: 0.4135  loss_dice_7: 3.163  loss_ce_8: 0.2963  loss_mask_8: 0.4145  loss_dice_8: 3.156  time: 1.5343  data_time: 0.1047  lr: 8.6068e-06  max_mem: 21366M
[01/17 09:45:25] d2.utils.events INFO:  eta: 1 day, 8:23:31  iter: 13839  total_loss: 39.55  loss_ce: 0.3083  loss_mask: 0.4299  loss_dice: 3.155  loss_ce_0: 0.5951  loss_mask_0: 0.4171  loss_dice_0: 3.294  loss_ce_1: 0.3136  loss_mask_1: 0.4308  loss_dice_1: 3.214  loss_ce_2: 0.33  loss_mask_2: 0.4261  loss_dice_2: 3.185  loss_ce_3: 0.3193  loss_mask_3: 0.4291  loss_dice_3: 3.17  loss_ce_4: 0.3329  loss_mask_4: 0.4315  loss_dice_4: 3.166  loss_ce_5: 0.3178  loss_mask_5: 0.4332  loss_dice_5: 3.155  loss_ce_6: 0.312  loss_mask_6: 0.4331  loss_dice_6: 3.15  loss_ce_7: 0.3013  loss_mask_7: 0.4335  loss_dice_7: 3.154  loss_ce_8: 0.3082  loss_mask_8: 0.4324  loss_dice_8: 3.156  time: 1.5342  data_time: 0.0838  lr: 8.6048e-06  max_mem: 21366M
[01/17 09:45:55] d2.utils.events INFO:  eta: 1 day, 8:23:11  iter: 13859  total_loss: 39.46  loss_ce: 0.3288  loss_mask: 0.4261  loss_dice: 3.148  loss_ce_0: 0.588  loss_mask_0: 0.416  loss_dice_0: 3.293  loss_ce_1: 0.3468  loss_mask_1: 0.4291  loss_dice_1: 3.197  loss_ce_2: 0.3292  loss_mask_2: 0.4299  loss_dice_2: 3.176  loss_ce_3: 0.3415  loss_mask_3: 0.4298  loss_dice_3: 3.159  loss_ce_4: 0.3134  loss_mask_4: 0.4283  loss_dice_4: 3.157  loss_ce_5: 0.3247  loss_mask_5: 0.4266  loss_dice_5: 3.156  loss_ce_6: 0.3222  loss_mask_6: 0.4298  loss_dice_6: 3.157  loss_ce_7: 0.3221  loss_mask_7: 0.4289  loss_dice_7: 3.153  loss_ce_8: 0.3148  loss_mask_8: 0.4297  loss_dice_8: 3.149  time: 1.5342  data_time: 0.1042  lr: 8.6028e-06  max_mem: 21366M
[01/17 09:46:25] d2.utils.events INFO:  eta: 1 day, 8:21:54  iter: 13879  total_loss: 39.39  loss_ce: 0.3127  loss_mask: 0.4356  loss_dice: 3.137  loss_ce_0: 0.5559  loss_mask_0: 0.4231  loss_dice_0: 3.24  loss_ce_1: 0.322  loss_mask_1: 0.4399  loss_dice_1: 3.162  loss_ce_2: 0.3225  loss_mask_2: 0.4351  loss_dice_2: 3.15  loss_ce_3: 0.313  loss_mask_3: 0.4336  loss_dice_3: 3.144  loss_ce_4: 0.3053  loss_mask_4: 0.4333  loss_dice_4: 3.142  loss_ce_5: 0.2971  loss_mask_5: 0.4328  loss_dice_5: 3.146  loss_ce_6: 0.3123  loss_mask_6: 0.4355  loss_dice_6: 3.143  loss_ce_7: 0.3034  loss_mask_7: 0.4352  loss_dice_7: 3.131  loss_ce_8: 0.2971  loss_mask_8: 0.4338  loss_dice_8: 3.146  time: 1.5341  data_time: 0.0848  lr: 8.6007e-06  max_mem: 21366M
[01/17 09:46:56] d2.utils.events INFO:  eta: 1 day, 8:21:15  iter: 13899  total_loss: 39.91  loss_ce: 0.3225  loss_mask: 0.4357  loss_dice: 3.171  loss_ce_0: 0.575  loss_mask_0: 0.4203  loss_dice_0: 3.282  loss_ce_1: 0.3237  loss_mask_1: 0.43  loss_dice_1: 3.206  loss_ce_2: 0.3419  loss_mask_2: 0.4318  loss_dice_2: 3.186  loss_ce_3: 0.3329  loss_mask_3: 0.4301  loss_dice_3: 3.175  loss_ce_4: 0.3319  loss_mask_4: 0.4322  loss_dice_4: 3.184  loss_ce_5: 0.321  loss_mask_5: 0.4351  loss_dice_5: 3.165  loss_ce_6: 0.3332  loss_mask_6: 0.4359  loss_dice_6: 3.16  loss_ce_7: 0.3229  loss_mask_7: 0.4339  loss_dice_7: 3.172  loss_ce_8: 0.3293  loss_mask_8: 0.4344  loss_dice_8: 3.169  time: 1.5341  data_time: 0.0916  lr: 8.5987e-06  max_mem: 21366M
[01/17 09:47:27] d2.utils.events INFO:  eta: 1 day, 8:20:29  iter: 13919  total_loss: 39.99  loss_ce: 0.3136  loss_mask: 0.4302  loss_dice: 3.22  loss_ce_0: 0.5778  loss_mask_0: 0.4216  loss_dice_0: 3.359  loss_ce_1: 0.3254  loss_mask_1: 0.4333  loss_dice_1: 3.269  loss_ce_2: 0.3441  loss_mask_2: 0.4317  loss_dice_2: 3.239  loss_ce_3: 0.3285  loss_mask_3: 0.4309  loss_dice_3: 3.225  loss_ce_4: 0.3377  loss_mask_4: 0.428  loss_dice_4: 3.227  loss_ce_5: 0.3273  loss_mask_5: 0.429  loss_dice_5: 3.218  loss_ce_6: 0.3263  loss_mask_6: 0.4286  loss_dice_6: 3.228  loss_ce_7: 0.3261  loss_mask_7: 0.4282  loss_dice_7: 3.215  loss_ce_8: 0.3136  loss_mask_8: 0.4283  loss_dice_8: 3.222  time: 1.5341  data_time: 0.1022  lr: 8.5967e-06  max_mem: 21366M
[01/17 09:47:58] d2.utils.events INFO:  eta: 1 day, 8:19:38  iter: 13939  total_loss: 39.68  loss_ce: 0.3268  loss_mask: 0.4301  loss_dice: 3.163  loss_ce_0: 0.5691  loss_mask_0: 0.4182  loss_dice_0: 3.29  loss_ce_1: 0.3524  loss_mask_1: 0.4335  loss_dice_1: 3.2  loss_ce_2: 0.364  loss_mask_2: 0.4309  loss_dice_2: 3.176  loss_ce_3: 0.3322  loss_mask_3: 0.4298  loss_dice_3: 3.163  loss_ce_4: 0.3406  loss_mask_4: 0.4301  loss_dice_4: 3.164  loss_ce_5: 0.3361  loss_mask_5: 0.4305  loss_dice_5: 3.175  loss_ce_6: 0.3301  loss_mask_6: 0.432  loss_dice_6: 3.167  loss_ce_7: 0.33  loss_mask_7: 0.4308  loss_dice_7: 3.162  loss_ce_8: 0.3354  loss_mask_8: 0.4308  loss_dice_8: 3.167  time: 1.5342  data_time: 0.1037  lr: 8.5946e-06  max_mem: 21366M
[01/17 09:48:28] d2.utils.events INFO:  eta: 1 day, 8:19:18  iter: 13959  total_loss: 39.71  loss_ce: 0.3072  loss_mask: 0.4238  loss_dice: 3.189  loss_ce_0: 0.6174  loss_mask_0: 0.4206  loss_dice_0: 3.312  loss_ce_1: 0.3413  loss_mask_1: 0.4309  loss_dice_1: 3.224  loss_ce_2: 0.3308  loss_mask_2: 0.4234  loss_dice_2: 3.194  loss_ce_3: 0.3146  loss_mask_3: 0.4252  loss_dice_3: 3.196  loss_ce_4: 0.3068  loss_mask_4: 0.4248  loss_dice_4: 3.185  loss_ce_5: 0.3117  loss_mask_5: 0.4241  loss_dice_5: 3.193  loss_ce_6: 0.2936  loss_mask_6: 0.4228  loss_dice_6: 3.192  loss_ce_7: 0.3109  loss_mask_7: 0.4225  loss_dice_7: 3.196  loss_ce_8: 0.3072  loss_mask_8: 0.4243  loss_dice_8: 3.188  time: 1.5341  data_time: 0.0978  lr: 8.5926e-06  max_mem: 21366M
[01/17 09:48:59] d2.utils.events INFO:  eta: 1 day, 8:17:34  iter: 13979  total_loss: 39.49  loss_ce: 0.308  loss_mask: 0.4186  loss_dice: 3.149  loss_ce_0: 0.6108  loss_mask_0: 0.4068  loss_dice_0: 3.282  loss_ce_1: 0.3335  loss_mask_1: 0.4199  loss_dice_1: 3.195  loss_ce_2: 0.3407  loss_mask_2: 0.417  loss_dice_2: 3.165  loss_ce_3: 0.3188  loss_mask_3: 0.4202  loss_dice_3: 3.154  loss_ce_4: 0.312  loss_mask_4: 0.4188  loss_dice_4: 3.155  loss_ce_5: 0.3185  loss_mask_5: 0.4189  loss_dice_5: 3.16  loss_ce_6: 0.31  loss_mask_6: 0.4189  loss_dice_6: 3.151  loss_ce_7: 0.325  loss_mask_7: 0.419  loss_dice_7: 3.146  loss_ce_8: 0.2932  loss_mask_8: 0.4189  loss_dice_8: 3.153  time: 1.5341  data_time: 0.0953  lr: 8.5906e-06  max_mem: 21366M
[01/17 09:49:30] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 09:49:30] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 09:49:30] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 09:49:31] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 09:49:45] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0117 s/iter. Inference: 0.1726 s/iter. Eval: 0.2476 s/iter. Total: 0.4319 s/iter. ETA=0:07:47
[01/17 09:49:51] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0123 s/iter. Inference: 0.1753 s/iter. Eval: 0.2478 s/iter. Total: 0.4355 s/iter. ETA=0:07:46
[01/17 09:49:56] d2.evaluation.evaluator INFO: Inference done 37/1093. Dataloading: 0.0110 s/iter. Inference: 0.1779 s/iter. Eval: 0.2186 s/iter. Total: 0.4076 s/iter. ETA=0:07:10
[01/17 09:50:01] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0116 s/iter. Inference: 0.1800 s/iter. Eval: 0.2143 s/iter. Total: 0.4059 s/iter. ETA=0:07:03
[01/17 09:50:06] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0120 s/iter. Inference: 0.1790 s/iter. Eval: 0.2223 s/iter. Total: 0.4134 s/iter. ETA=0:07:06
[01/17 09:50:12] d2.evaluation.evaluator INFO: Inference done 75/1093. Dataloading: 0.0122 s/iter. Inference: 0.1821 s/iter. Eval: 0.2196 s/iter. Total: 0.4139 s/iter. ETA=0:07:01
[01/17 09:50:17] d2.evaluation.evaluator INFO: Inference done 87/1093. Dataloading: 0.0122 s/iter. Inference: 0.1807 s/iter. Eval: 0.2237 s/iter. Total: 0.4167 s/iter. ETA=0:06:59
[01/17 09:50:22] d2.evaluation.evaluator INFO: Inference done 99/1093. Dataloading: 0.0122 s/iter. Inference: 0.1835 s/iter. Eval: 0.2232 s/iter. Total: 0.4191 s/iter. ETA=0:06:56
[01/17 09:50:28] d2.evaluation.evaluator INFO: Inference done 113/1093. Dataloading: 0.0119 s/iter. Inference: 0.1839 s/iter. Eval: 0.2193 s/iter. Total: 0.4152 s/iter. ETA=0:06:46
[01/17 09:50:33] d2.evaluation.evaluator INFO: Inference done 126/1093. Dataloading: 0.0119 s/iter. Inference: 0.1835 s/iter. Eval: 0.2189 s/iter. Total: 0.4145 s/iter. ETA=0:06:40
[01/17 09:50:38] d2.evaluation.evaluator INFO: Inference done 140/1093. Dataloading: 0.0118 s/iter. Inference: 0.1824 s/iter. Eval: 0.2157 s/iter. Total: 0.4100 s/iter. ETA=0:06:30
[01/17 09:50:43] d2.evaluation.evaluator INFO: Inference done 155/1093. Dataloading: 0.0116 s/iter. Inference: 0.1813 s/iter. Eval: 0.2100 s/iter. Total: 0.4030 s/iter. ETA=0:06:18
[01/17 09:50:48] d2.evaluation.evaluator INFO: Inference done 169/1093. Dataloading: 0.0118 s/iter. Inference: 0.1788 s/iter. Eval: 0.2095 s/iter. Total: 0.4001 s/iter. ETA=0:06:09
[01/17 09:50:53] d2.evaluation.evaluator INFO: Inference done 182/1093. Dataloading: 0.0116 s/iter. Inference: 0.1783 s/iter. Eval: 0.2096 s/iter. Total: 0.3996 s/iter. ETA=0:06:04
[01/17 09:50:59] d2.evaluation.evaluator INFO: Inference done 195/1093. Dataloading: 0.0117 s/iter. Inference: 0.1790 s/iter. Eval: 0.2091 s/iter. Total: 0.4000 s/iter. ETA=0:05:59
[01/17 09:51:04] d2.evaluation.evaluator INFO: Inference done 209/1093. Dataloading: 0.0117 s/iter. Inference: 0.1772 s/iter. Eval: 0.2095 s/iter. Total: 0.3985 s/iter. ETA=0:05:52
[01/17 09:51:09] d2.evaluation.evaluator INFO: Inference done 222/1093. Dataloading: 0.0118 s/iter. Inference: 0.1763 s/iter. Eval: 0.2100 s/iter. Total: 0.3982 s/iter. ETA=0:05:46
[01/17 09:51:15] d2.evaluation.evaluator INFO: Inference done 234/1093. Dataloading: 0.0118 s/iter. Inference: 0.1775 s/iter. Eval: 0.2115 s/iter. Total: 0.4010 s/iter. ETA=0:05:44
[01/17 09:51:20] d2.evaluation.evaluator INFO: Inference done 246/1093. Dataloading: 0.0119 s/iter. Inference: 0.1775 s/iter. Eval: 0.2125 s/iter. Total: 0.4020 s/iter. ETA=0:05:40
[01/17 09:51:25] d2.evaluation.evaluator INFO: Inference done 258/1093. Dataloading: 0.0119 s/iter. Inference: 0.1777 s/iter. Eval: 0.2146 s/iter. Total: 0.4043 s/iter. ETA=0:05:37
[01/17 09:51:30] d2.evaluation.evaluator INFO: Inference done 272/1093. Dataloading: 0.0119 s/iter. Inference: 0.1773 s/iter. Eval: 0.2136 s/iter. Total: 0.4029 s/iter. ETA=0:05:30
[01/17 09:51:35] d2.evaluation.evaluator INFO: Inference done 285/1093. Dataloading: 0.0120 s/iter. Inference: 0.1774 s/iter. Eval: 0.2128 s/iter. Total: 0.4022 s/iter. ETA=0:05:25
[01/17 09:51:41] d2.evaluation.evaluator INFO: Inference done 298/1093. Dataloading: 0.0120 s/iter. Inference: 0.1783 s/iter. Eval: 0.2123 s/iter. Total: 0.4027 s/iter. ETA=0:05:20
[01/17 09:51:46] d2.evaluation.evaluator INFO: Inference done 309/1093. Dataloading: 0.0121 s/iter. Inference: 0.1785 s/iter. Eval: 0.2145 s/iter. Total: 0.4052 s/iter. ETA=0:05:17
[01/17 09:51:51] d2.evaluation.evaluator INFO: Inference done 321/1093. Dataloading: 0.0121 s/iter. Inference: 0.1782 s/iter. Eval: 0.2158 s/iter. Total: 0.4063 s/iter. ETA=0:05:13
[01/17 09:51:56] d2.evaluation.evaluator INFO: Inference done 334/1093. Dataloading: 0.0121 s/iter. Inference: 0.1784 s/iter. Eval: 0.2149 s/iter. Total: 0.4055 s/iter. ETA=0:05:07
[01/17 09:52:02] d2.evaluation.evaluator INFO: Inference done 350/1093. Dataloading: 0.0119 s/iter. Inference: 0.1782 s/iter. Eval: 0.2121 s/iter. Total: 0.4023 s/iter. ETA=0:04:58
[01/17 09:52:07] d2.evaluation.evaluator INFO: Inference done 363/1093. Dataloading: 0.0119 s/iter. Inference: 0.1777 s/iter. Eval: 0.2121 s/iter. Total: 0.4018 s/iter. ETA=0:04:53
[01/17 09:52:12] d2.evaluation.evaluator INFO: Inference done 376/1093. Dataloading: 0.0119 s/iter. Inference: 0.1778 s/iter. Eval: 0.2124 s/iter. Total: 0.4023 s/iter. ETA=0:04:48
[01/17 09:52:17] d2.evaluation.evaluator INFO: Inference done 389/1093. Dataloading: 0.0119 s/iter. Inference: 0.1777 s/iter. Eval: 0.2123 s/iter. Total: 0.4020 s/iter. ETA=0:04:43
[01/17 09:52:23] d2.evaluation.evaluator INFO: Inference done 403/1093. Dataloading: 0.0119 s/iter. Inference: 0.1774 s/iter. Eval: 0.2123 s/iter. Total: 0.4018 s/iter. ETA=0:04:37
[01/17 09:52:28] d2.evaluation.evaluator INFO: Inference done 416/1093. Dataloading: 0.0120 s/iter. Inference: 0.1772 s/iter. Eval: 0.2129 s/iter. Total: 0.4022 s/iter. ETA=0:04:32
[01/17 09:52:33] d2.evaluation.evaluator INFO: Inference done 427/1093. Dataloading: 0.0120 s/iter. Inference: 0.1773 s/iter. Eval: 0.2144 s/iter. Total: 0.4038 s/iter. ETA=0:04:28
[01/17 09:52:38] d2.evaluation.evaluator INFO: Inference done 441/1093. Dataloading: 0.0120 s/iter. Inference: 0.1776 s/iter. Eval: 0.2129 s/iter. Total: 0.4026 s/iter. ETA=0:04:22
[01/17 09:52:44] d2.evaluation.evaluator INFO: Inference done 455/1093. Dataloading: 0.0121 s/iter. Inference: 0.1774 s/iter. Eval: 0.2124 s/iter. Total: 0.4020 s/iter. ETA=0:04:16
[01/17 09:52:49] d2.evaluation.evaluator INFO: Inference done 471/1093. Dataloading: 0.0120 s/iter. Inference: 0.1766 s/iter. Eval: 0.2107 s/iter. Total: 0.3994 s/iter. ETA=0:04:08
[01/17 09:52:54] d2.evaluation.evaluator INFO: Inference done 486/1093. Dataloading: 0.0119 s/iter. Inference: 0.1763 s/iter. Eval: 0.2096 s/iter. Total: 0.3979 s/iter. ETA=0:04:01
[01/17 09:52:59] d2.evaluation.evaluator INFO: Inference done 499/1093. Dataloading: 0.0119 s/iter. Inference: 0.1766 s/iter. Eval: 0.2091 s/iter. Total: 0.3977 s/iter. ETA=0:03:56
[01/17 09:53:04] d2.evaluation.evaluator INFO: Inference done 514/1093. Dataloading: 0.0118 s/iter. Inference: 0.1768 s/iter. Eval: 0.2076 s/iter. Total: 0.3963 s/iter. ETA=0:03:49
[01/17 09:53:10] d2.evaluation.evaluator INFO: Inference done 526/1093. Dataloading: 0.0118 s/iter. Inference: 0.1767 s/iter. Eval: 0.2085 s/iter. Total: 0.3971 s/iter. ETA=0:03:45
[01/17 09:53:15] d2.evaluation.evaluator INFO: Inference done 540/1093. Dataloading: 0.0118 s/iter. Inference: 0.1762 s/iter. Eval: 0.2084 s/iter. Total: 0.3964 s/iter. ETA=0:03:39
[01/17 09:53:20] d2.evaluation.evaluator INFO: Inference done 550/1093. Dataloading: 0.0118 s/iter. Inference: 0.1768 s/iter. Eval: 0.2097 s/iter. Total: 0.3984 s/iter. ETA=0:03:36
[01/17 09:53:25] d2.evaluation.evaluator INFO: Inference done 563/1093. Dataloading: 0.0119 s/iter. Inference: 0.1769 s/iter. Eval: 0.2092 s/iter. Total: 0.3981 s/iter. ETA=0:03:30
[01/17 09:53:30] d2.evaluation.evaluator INFO: Inference done 579/1093. Dataloading: 0.0119 s/iter. Inference: 0.1767 s/iter. Eval: 0.2075 s/iter. Total: 0.3963 s/iter. ETA=0:03:23
[01/17 09:53:36] d2.evaluation.evaluator INFO: Inference done 593/1093. Dataloading: 0.0119 s/iter. Inference: 0.1767 s/iter. Eval: 0.2073 s/iter. Total: 0.3961 s/iter. ETA=0:03:18
[01/17 09:53:41] d2.evaluation.evaluator INFO: Inference done 605/1093. Dataloading: 0.0119 s/iter. Inference: 0.1766 s/iter. Eval: 0.2081 s/iter. Total: 0.3968 s/iter. ETA=0:03:13
[01/17 09:53:46] d2.evaluation.evaluator INFO: Inference done 618/1093. Dataloading: 0.0119 s/iter. Inference: 0.1766 s/iter. Eval: 0.2080 s/iter. Total: 0.3967 s/iter. ETA=0:03:08
[01/17 09:53:51] d2.evaluation.evaluator INFO: Inference done 633/1093. Dataloading: 0.0119 s/iter. Inference: 0.1758 s/iter. Eval: 0.2076 s/iter. Total: 0.3954 s/iter. ETA=0:03:01
[01/17 09:53:56] d2.evaluation.evaluator INFO: Inference done 647/1093. Dataloading: 0.0119 s/iter. Inference: 0.1757 s/iter. Eval: 0.2071 s/iter. Total: 0.3948 s/iter. ETA=0:02:56
[01/17 09:54:02] d2.evaluation.evaluator INFO: Inference done 660/1093. Dataloading: 0.0119 s/iter. Inference: 0.1762 s/iter. Eval: 0.2069 s/iter. Total: 0.3951 s/iter. ETA=0:02:51
[01/17 09:54:07] d2.evaluation.evaluator INFO: Inference done 672/1093. Dataloading: 0.0118 s/iter. Inference: 0.1766 s/iter. Eval: 0.2071 s/iter. Total: 0.3956 s/iter. ETA=0:02:46
[01/17 09:54:12] d2.evaluation.evaluator INFO: Inference done 688/1093. Dataloading: 0.0118 s/iter. Inference: 0.1763 s/iter. Eval: 0.2057 s/iter. Total: 0.3939 s/iter. ETA=0:02:39
[01/17 09:54:17] d2.evaluation.evaluator INFO: Inference done 701/1093. Dataloading: 0.0118 s/iter. Inference: 0.1764 s/iter. Eval: 0.2056 s/iter. Total: 0.3939 s/iter. ETA=0:02:34
[01/17 09:54:22] d2.evaluation.evaluator INFO: Inference done 713/1093. Dataloading: 0.0118 s/iter. Inference: 0.1763 s/iter. Eval: 0.2065 s/iter. Total: 0.3947 s/iter. ETA=0:02:30
[01/17 09:54:27] d2.evaluation.evaluator INFO: Inference done 728/1093. Dataloading: 0.0118 s/iter. Inference: 0.1761 s/iter. Eval: 0.2057 s/iter. Total: 0.3937 s/iter. ETA=0:02:23
[01/17 09:54:33] d2.evaluation.evaluator INFO: Inference done 743/1093. Dataloading: 0.0118 s/iter. Inference: 0.1761 s/iter. Eval: 0.2050 s/iter. Total: 0.3931 s/iter. ETA=0:02:17
[01/17 09:54:38] d2.evaluation.evaluator INFO: Inference done 756/1093. Dataloading: 0.0118 s/iter. Inference: 0.1762 s/iter. Eval: 0.2050 s/iter. Total: 0.3931 s/iter. ETA=0:02:12
[01/17 09:54:43] d2.evaluation.evaluator INFO: Inference done 769/1093. Dataloading: 0.0118 s/iter. Inference: 0.1764 s/iter. Eval: 0.2053 s/iter. Total: 0.3936 s/iter. ETA=0:02:07
[01/17 09:54:49] d2.evaluation.evaluator INFO: Inference done 783/1093. Dataloading: 0.0118 s/iter. Inference: 0.1766 s/iter. Eval: 0.2047 s/iter. Total: 0.3932 s/iter. ETA=0:02:01
[01/17 09:54:54] d2.evaluation.evaluator INFO: Inference done 797/1093. Dataloading: 0.0117 s/iter. Inference: 0.1767 s/iter. Eval: 0.2040 s/iter. Total: 0.3926 s/iter. ETA=0:01:56
[01/17 09:54:59] d2.evaluation.evaluator INFO: Inference done 810/1093. Dataloading: 0.0117 s/iter. Inference: 0.1769 s/iter. Eval: 0.2037 s/iter. Total: 0.3925 s/iter. ETA=0:01:51
[01/17 09:55:04] d2.evaluation.evaluator INFO: Inference done 825/1093. Dataloading: 0.0117 s/iter. Inference: 0.1769 s/iter. Eval: 0.2029 s/iter. Total: 0.3917 s/iter. ETA=0:01:44
[01/17 09:55:09] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0117 s/iter. Inference: 0.1771 s/iter. Eval: 0.2022 s/iter. Total: 0.3912 s/iter. ETA=0:01:39
[01/17 09:55:14] d2.evaluation.evaluator INFO: Inference done 853/1093. Dataloading: 0.0117 s/iter. Inference: 0.1771 s/iter. Eval: 0.2021 s/iter. Total: 0.3911 s/iter. ETA=0:01:33
[01/17 09:55:20] d2.evaluation.evaluator INFO: Inference done 866/1093. Dataloading: 0.0118 s/iter. Inference: 0.1771 s/iter. Eval: 0.2026 s/iter. Total: 0.3916 s/iter. ETA=0:01:28
[01/17 09:55:25] d2.evaluation.evaluator INFO: Inference done 880/1093. Dataloading: 0.0118 s/iter. Inference: 0.1772 s/iter. Eval: 0.2022 s/iter. Total: 0.3913 s/iter. ETA=0:01:23
[01/17 09:55:30] d2.evaluation.evaluator INFO: Inference done 890/1093. Dataloading: 0.0119 s/iter. Inference: 0.1774 s/iter. Eval: 0.2032 s/iter. Total: 0.3925 s/iter. ETA=0:01:19
[01/17 09:55:35] d2.evaluation.evaluator INFO: Inference done 904/1093. Dataloading: 0.0118 s/iter. Inference: 0.1772 s/iter. Eval: 0.2029 s/iter. Total: 0.3921 s/iter. ETA=0:01:14
[01/17 09:55:40] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0119 s/iter. Inference: 0.1772 s/iter. Eval: 0.2029 s/iter. Total: 0.3921 s/iter. ETA=0:01:09
[01/17 09:55:46] d2.evaluation.evaluator INFO: Inference done 931/1093. Dataloading: 0.0118 s/iter. Inference: 0.1772 s/iter. Eval: 0.2026 s/iter. Total: 0.3918 s/iter. ETA=0:01:03
[01/17 09:55:51] d2.evaluation.evaluator INFO: Inference done 944/1093. Dataloading: 0.0119 s/iter. Inference: 0.1772 s/iter. Eval: 0.2027 s/iter. Total: 0.3919 s/iter. ETA=0:00:58
[01/17 09:55:56] d2.evaluation.evaluator INFO: Inference done 957/1093. Dataloading: 0.0119 s/iter. Inference: 0.1770 s/iter. Eval: 0.2028 s/iter. Total: 0.3919 s/iter. ETA=0:00:53
[01/17 09:56:01] d2.evaluation.evaluator INFO: Inference done 970/1093. Dataloading: 0.0119 s/iter. Inference: 0.1771 s/iter. Eval: 0.2029 s/iter. Total: 0.3920 s/iter. ETA=0:00:48
[01/17 09:56:06] d2.evaluation.evaluator INFO: Inference done 985/1093. Dataloading: 0.0119 s/iter. Inference: 0.1769 s/iter. Eval: 0.2025 s/iter. Total: 0.3914 s/iter. ETA=0:00:42
[01/17 09:56:11] d2.evaluation.evaluator INFO: Inference done 999/1093. Dataloading: 0.0118 s/iter. Inference: 0.1769 s/iter. Eval: 0.2023 s/iter. Total: 0.3911 s/iter. ETA=0:00:36
[01/17 09:56:17] d2.evaluation.evaluator INFO: Inference done 1013/1093. Dataloading: 0.0118 s/iter. Inference: 0.1767 s/iter. Eval: 0.2020 s/iter. Total: 0.3907 s/iter. ETA=0:00:31
[01/17 09:56:22] d2.evaluation.evaluator INFO: Inference done 1025/1093. Dataloading: 0.0118 s/iter. Inference: 0.1769 s/iter. Eval: 0.2023 s/iter. Total: 0.3911 s/iter. ETA=0:00:26
[01/17 09:56:27] d2.evaluation.evaluator INFO: Inference done 1038/1093. Dataloading: 0.0118 s/iter. Inference: 0.1769 s/iter. Eval: 0.2023 s/iter. Total: 0.3911 s/iter. ETA=0:00:21
[01/17 09:56:32] d2.evaluation.evaluator INFO: Inference done 1052/1093. Dataloading: 0.0118 s/iter. Inference: 0.1768 s/iter. Eval: 0.2023 s/iter. Total: 0.3911 s/iter. ETA=0:00:16
[01/17 09:56:37] d2.evaluation.evaluator INFO: Inference done 1069/1093. Dataloading: 0.0118 s/iter. Inference: 0.1763 s/iter. Eval: 0.2015 s/iter. Total: 0.3897 s/iter. ETA=0:00:09
[01/17 09:56:43] d2.evaluation.evaluator INFO: Inference done 1086/1093. Dataloading: 0.0117 s/iter. Inference: 0.1757 s/iter. Eval: 0.2008 s/iter. Total: 0.3884 s/iter. ETA=0:00:02
[01/17 09:56:45] d2.evaluation.evaluator INFO: Total inference time: 0:07:02.462988 (0.388293 s / iter per device, on 4 devices)
[01/17 09:56:45] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:11 (0.175577 s / iter per device, on 4 devices)
[01/17 09:57:10] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 12.054668662058452, 'fwIoU': 35.89808063588868, 'IoU-1': nan, 'IoU-2': 94.99265190657133, 'IoU-3': 41.60665049593181, 'IoU-4': 56.72375366784442, 'IoU-5': 50.03478560288836, 'IoU-6': 45.250601941621156, 'IoU-7': 42.12976391261222, 'IoU-8': 33.23720488455987, 'IoU-9': 18.770081902303424, 'IoU-10': 27.392004787300607, 'IoU-11': 31.86600116743299, 'IoU-12': 39.88303561138369, 'IoU-13': 43.81594134350038, 'IoU-14': 40.11863865639919, 'IoU-15': 37.98357635789559, 'IoU-16': 39.83924247484433, 'IoU-17': 40.080839105123985, 'IoU-18': 37.109005099564044, 'IoU-19': 37.218329481354154, 'IoU-20': 36.784448965549444, 'IoU-21': 35.559598460581725, 'IoU-22': 34.87767045798535, 'IoU-23': 35.742535216588514, 'IoU-24': 33.71616277856699, 'IoU-25': 37.17364659949738, 'IoU-26': 35.85845606221236, 'IoU-27': 32.277277094920755, 'IoU-28': 35.353648348722295, 'IoU-29': 34.50505150923439, 'IoU-30': 35.8896267198439, 'IoU-31': 37.14277881377976, 'IoU-32': 38.18128291382315, 'IoU-33': 37.789347149521625, 'IoU-34': 35.76778927414786, 'IoU-35': 34.59334432596613, 'IoU-36': 35.899781109162014, 'IoU-37': 34.61399591531083, 'IoU-38': 35.11350694395624, 'IoU-39': 34.37662515360421, 'IoU-40': 33.19701786163042, 'IoU-41': 35.65148249094344, 'IoU-42': 33.228978466534606, 'IoU-43': 33.26915821319106, 'IoU-44': 32.597330081030776, 'IoU-45': 31.758817337434614, 'IoU-46': 29.04101412393924, 'IoU-47': 28.670253636748782, 'IoU-48': 28.403460525454804, 'IoU-49': 27.436877787804402, 'IoU-50': 24.5156977108388, 'IoU-51': 26.281376628445262, 'IoU-52': 24.561903871838854, 'IoU-53': 25.681008912194326, 'IoU-54': 26.09110672624448, 'IoU-55': 26.478003784206887, 'IoU-56': 26.74736874296304, 'IoU-57': 25.310207508718552, 'IoU-58': 24.813177579035692, 'IoU-59': 22.822604026968328, 'IoU-60': 22.6720979552901, 'IoU-61': 21.232893175919642, 'IoU-62': 21.988876616147337, 'IoU-63': 20.207472717559437, 'IoU-64': 20.244222515659313, 'IoU-65': 18.606762308919084, 'IoU-66': 17.99387425616539, 'IoU-67': 17.359594790205076, 'IoU-68': 17.291568546439425, 'IoU-69': 18.185220472726638, 'IoU-70': 18.61876173406259, 'IoU-71': 18.39339571955379, 'IoU-72': 17.318383618151127, 'IoU-73': 15.823644602326786, 'IoU-74': 18.567828784943917, 'IoU-75': 16.55335393780672, 'IoU-76': 17.20250601905851, 'IoU-77': 14.495699679932466, 'IoU-78': 17.62806538947615, 'IoU-79': 16.0528065070914, 'IoU-80': 15.7588625501106, 'IoU-81': 15.840582188557335, 'IoU-82': 17.585624211332348, 'IoU-83': 14.80592132214595, 'IoU-84': 16.10687771982267, 'IoU-85': 16.148747495806038, 'IoU-86': 16.017474523394345, 'IoU-87': 15.751606651766279, 'IoU-88': 12.71451344811729, 'IoU-89': 15.469387107740747, 'IoU-90': 11.363837492876918, 'IoU-91': 13.596711099548948, 'IoU-92': 12.365727752216362, 'IoU-93': 12.63444608032421, 'IoU-94': 13.189640629714782, 'IoU-95': 11.418606902322072, 'IoU-96': 12.444053356483437, 'IoU-97': 12.529937539996327, 'IoU-98': 11.827751150784147, 'IoU-99': 10.988748452459426, 'IoU-100': 12.437122159384213, 'IoU-101': 10.287975194382415, 'IoU-102': 8.777323576356396, 'IoU-103': 10.947222730457923, 'IoU-104': 10.179103127290407, 'IoU-105': 9.215441168632108, 'IoU-106': 10.862119903048812, 'IoU-107': 7.925425946399709, 'IoU-108': 8.281941078442296, 'IoU-109': 11.782573909991584, 'IoU-110': 9.043828082792515, 'IoU-111': 8.24577602357835, 'IoU-112': 7.596884083453872, 'IoU-113': 9.097459435887506, 'IoU-114': 9.123616265651421, 'IoU-115': 9.468543882585106, 'IoU-116': 9.178640859298676, 'IoU-117': 7.975823876246986, 'IoU-118': 7.481937062057523, 'IoU-119': 7.664932595092032, 'IoU-120': 8.574470454292381, 'IoU-121': 9.061259718071087, 'IoU-122': 7.71717064835716, 'IoU-123': 7.144201957578895, 'IoU-124': 7.764724063587973, 'IoU-125': 7.409726193755232, 'IoU-126': 5.3965087344849945, 'IoU-127': 4.862871476386469, 'IoU-128': 7.396424887850584, 'IoU-129': 7.000739636514952, 'IoU-130': 5.1990711656932715, 'IoU-131': 6.375835608746286, 'IoU-132': 4.378431579661615, 'IoU-133': 5.597906434746007, 'IoU-134': 5.6824204247278844, 'IoU-135': 6.558131576896551, 'IoU-136': 4.50983769079129, 'IoU-137': 4.992655137580341, 'IoU-138': 2.70187265917603, 'IoU-139': 5.314407613200624, 'IoU-140': 4.190908055066039, 'IoU-141': 4.354300323014074, 'IoU-142': 4.249032291417604, 'IoU-143': 2.0234063157527395, 'IoU-144': 2.9109281242498457, 'IoU-145': 3.936378769597579, 'IoU-146': 2.663320068635683, 'IoU-147': 2.334773962340607, 'IoU-148': 2.026081138508231, 'IoU-149': 3.739965693474984, 'IoU-150': 1.9695148269699287, 'IoU-151': 1.6918260861711998, 'IoU-152': 1.7443857258395097, 'IoU-153': 1.8198435497034582, 'IoU-154': 1.7202123539156597, 'IoU-155': 1.1411949980147311, 'IoU-156': 1.1077061808400073, 'IoU-157': 2.178972852990948, 'IoU-158': 1.047396505784715, 'IoU-159': 0.7801412292885016, 'IoU-160': 1.0655804234883088, 'IoU-161': 0.6289703329376695, 'IoU-162': 1.1332261332470817, 'IoU-163': 1.33185864875198, 'IoU-164': 1.1204027616492358, 'IoU-165': 1.5452450941447526, 'IoU-166': 0.7950312350871541, 'IoU-167': 1.9479128311842695, 'IoU-168': 1.3566114260181217, 'IoU-169': 0.7013111286450552, 'IoU-170': 1.033337692264433, 'IoU-171': 0.7506714345449413, 'IoU-172': 0.3769038442059697, 'IoU-173': 0.8747649163944256, 'IoU-174': 0.45786034254843055, 'IoU-175': 0.03958427474017171, 'IoU-176': 0.763171095408566, 'IoU-177': 0.3159551445996459, 'IoU-178': 1.9125195828033879, 'IoU-179': 0.3680771377673303, 'IoU-180': 0.07282340968422295, 'IoU-181': 0.07017195233275167, 'IoU-182': 0.1630990094353739, 'IoU-183': 0.0, 'IoU-184': 0.10888648991459289, 'IoU-185': 0.26926147659952276, 'IoU-186': 0.08313064529021502, 'IoU-187': 0.14140456062339832, 'IoU-188': 1.5359887832975891, 'IoU-189': 1.0559373201078355, 'IoU-190': 0.7660225839459255, 'IoU-191': 0.43220997597852195, 'IoU-192': 0.0, 'IoU-193': 1.0036455492723255, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 18.869302951691626, 'pACC': 49.78236320263833, 'ACC-1': nan, 'ACC-2': 98.88034770295044, 'ACC-3': 49.923083294874985, 'ACC-4': 71.33963872989317, 'ACC-5': 66.63998165573118, 'ACC-6': 65.81466219937725, 'ACC-7': 60.87158977436763, 'ACC-8': 45.80676095985064, 'ACC-9': 22.898129733823247, 'ACC-10': 35.79846639201827, 'ACC-11': 47.73174602976693, 'ACC-12': 54.56493514276216, 'ACC-13': 68.4471886716997, 'ACC-14': 62.49324433786653, 'ACC-15': 51.10209906810137, 'ACC-16': 56.61828824371613, 'ACC-17': 55.874035110494006, 'ACC-18': 56.6093686150692, 'ACC-19': 56.59499957542072, 'ACC-20': 54.834312935985764, 'ACC-21': 53.12723547715799, 'ACC-22': 51.79764883749859, 'ACC-23': 51.426133949420375, 'ACC-24': 45.98239948287417, 'ACC-25': 50.89226690994222, 'ACC-26': 53.92308628109076, 'ACC-27': 48.43740596773691, 'ACC-28': 54.78261347651806, 'ACC-29': 49.81333367301572, 'ACC-30': 55.30524493291696, 'ACC-31': 55.07856908304962, 'ACC-32': 55.891292401615786, 'ACC-33': 53.81598444454634, 'ACC-34': 51.31740351418627, 'ACC-35': 47.79131096760282, 'ACC-36': 55.745065477163635, 'ACC-37': 51.468466786785264, 'ACC-38': 52.40185712717462, 'ACC-39': 52.90565585549911, 'ACC-40': 50.2244078486537, 'ACC-41': 53.56366158209633, 'ACC-42': 51.690474191887795, 'ACC-43': 50.35264771156274, 'ACC-44': 48.36578530094196, 'ACC-45': 50.992397134667534, 'ACC-46': 46.82570138072508, 'ACC-47': 44.42699500618312, 'ACC-48': 46.95002875729833, 'ACC-49': 44.3141457185636, 'ACC-50': 38.238167171037205, 'ACC-51': 40.02768285438476, 'ACC-52': 38.557412263078234, 'ACC-53': 38.77376052277854, 'ACC-54': 37.70149563178873, 'ACC-55': 39.73551905368804, 'ACC-56': 41.81827983216778, 'ACC-57': 41.4017862035793, 'ACC-58': 38.74542565850368, 'ACC-59': 33.8974714256385, 'ACC-60': 37.118088340996344, 'ACC-61': 32.35433604656927, 'ACC-62': 38.538063525684855, 'ACC-63': 30.49645010981246, 'ACC-64': 34.3521689118697, 'ACC-65': 30.526442812641065, 'ACC-66': 28.935336148855008, 'ACC-67': 29.097535967450945, 'ACC-68': 30.37674710622224, 'ACC-69': 32.11511064227719, 'ACC-70': 30.216356483645495, 'ACC-71': 31.056444408108074, 'ACC-72': 28.645691317140294, 'ACC-73': 27.430159285917288, 'ACC-74': 33.792858913916, 'ACC-75': 27.332369892139535, 'ACC-76': 29.615010390515202, 'ACC-77': 22.459521240519724, 'ACC-78': 31.725883171529446, 'ACC-79': 28.80350230590582, 'ACC-80': 25.800386338674198, 'ACC-81': 28.75194717463261, 'ACC-82': 32.047207975029835, 'ACC-83': 22.67001560984962, 'ACC-84': 27.566290736007996, 'ACC-85': 28.448353200794873, 'ACC-86': 29.91448303061696, 'ACC-87': 29.824930466083277, 'ACC-88': 23.78049974111496, 'ACC-89': 28.19479953738139, 'ACC-90': 19.069021491994427, 'ACC-91': 26.02195063997949, 'ACC-92': 20.99218348786646, 'ACC-93': 24.613009017264236, 'ACC-94': 23.658884154491712, 'ACC-95': 18.671253688715026, 'ACC-96': 22.37223336239923, 'ACC-97': 22.061613667175752, 'ACC-98': 21.347123841495787, 'ACC-99': 16.922880004852686, 'ACC-100': 23.416315246677936, 'ACC-101': 17.365366821636023, 'ACC-102': 15.118510895488255, 'ACC-103': 19.926053466243772, 'ACC-104': 17.32256541040899, 'ACC-105': 14.669535918939525, 'ACC-106': 18.638730538818162, 'ACC-107': 12.338755333959, 'ACC-108': 13.62672371404079, 'ACC-109': 21.933121768105313, 'ACC-110': 17.12707464247898, 'ACC-111': 13.597983119799084, 'ACC-112': 11.72588385464704, 'ACC-113': 16.476047670430997, 'ACC-114': 17.815518577663113, 'ACC-115': 17.428043835324893, 'ACC-116': 18.977643468194188, 'ACC-117': 15.77612322765139, 'ACC-118': 13.840011508228947, 'ACC-119': 13.081009045231195, 'ACC-120': 16.183187313776276, 'ACC-121': 18.03092836033833, 'ACC-122': 14.310689322033204, 'ACC-123': 12.870918131468953, 'ACC-124': 17.601826833692215, 'ACC-125': 14.64668123878257, 'ACC-126': 9.28594037393282, 'ACC-127': 7.921121777065365, 'ACC-128': 13.905664476819707, 'ACC-129': 14.93838815159576, 'ACC-130': 8.397743666848859, 'ACC-131': 12.478104854208642, 'ACC-132': 8.319460255876038, 'ACC-133': 10.874191137785552, 'ACC-134': 10.739077479978498, 'ACC-135': 15.431072813442482, 'ACC-136': 10.953765463770594, 'ACC-137': 9.030673453224003, 'ACC-138': 4.404766538672602, 'ACC-139': 11.44741350864279, 'ACC-140': 8.02105016343473, 'ACC-141': 11.034277696885969, 'ACC-142': 8.853483780731446, 'ACC-143': 4.658529108929568, 'ACC-144': 6.941168330408419, 'ACC-145': 7.87942238267148, 'ACC-146': 6.525280082291643, 'ACC-147': 4.95999634461173, 'ACC-148': 5.055511554035672, 'ACC-149': 8.07841064739257, 'ACC-150': 3.618712280783596, 'ACC-151': 3.124110780699368, 'ACC-152': 4.027356351974831, 'ACC-153': 5.352160462607054, 'ACC-154': 3.1379972150460804, 'ACC-155': 2.1024649305421015, 'ACC-156': 3.2158466510379307, 'ACC-157': 6.686704544720019, 'ACC-158': 2.2005272663675104, 'ACC-159': 1.016299461859485, 'ACC-160': 2.1031396110432197, 'ACC-161': 0.9199825815500159, 'ACC-162': 1.8990274575442951, 'ACC-163': 2.176577873912901, 'ACC-164': 1.8908015843049253, 'ACC-165': 2.405353898255639, 'ACC-166': 1.0504181661674505, 'ACC-167': 5.479849913120851, 'ACC-168': 4.530396913493391, 'ACC-169': 1.7627821928405654, 'ACC-170': 5.352232787558366, 'ACC-171': 1.1373638854579269, 'ACC-172': 0.5194623894902546, 'ACC-173': 1.8185353928214247, 'ACC-174': 0.600181371423841, 'ACC-175': 0.040821576160199526, 'ACC-176': 1.853649337467345, 'ACC-177': 0.4369703389830508, 'ACC-178': 4.789718548209992, 'ACC-179': 0.3859296016503416, 'ACC-180': 0.08014104824491104, 'ACC-181': 0.07175149852687189, 'ACC-182': 0.1807281710924935, 'ACC-183': 0.0, 'ACC-184': 0.10994676881278678, 'ACC-185': 0.2839508793279515, 'ACC-186': 0.08406893652795291, 'ACC-187': 0.15344193875069934, 'ACC-188': 4.647797146619312, 'ACC-189': 4.205338767604213, 'ACC-190': 1.1896662824340665, 'ACC-191': 0.6411331314126414, 'ACC-192': 0.0, 'ACC-193': 2.473604228983137, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 09:57:10] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 09:57:10] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 09:57:10] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 09:57:10] d2.evaluation.testing INFO: copypaste: 12.0547,35.8981,18.8693,49.7824
[01/17 09:57:10] d2.utils.events INFO:  eta: 1 day, 8:17:19  iter: 13999  total_loss: 39.52  loss_ce: 0.3072  loss_mask: 0.4121  loss_dice: 3.174  loss_ce_0: 0.5979  loss_mask_0: 0.3968  loss_dice_0: 3.314  loss_ce_1: 0.3318  loss_mask_1: 0.4157  loss_dice_1: 3.218  loss_ce_2: 0.3363  loss_mask_2: 0.4128  loss_dice_2: 3.185  loss_ce_3: 0.3327  loss_mask_3: 0.4119  loss_dice_3: 3.184  loss_ce_4: 0.3205  loss_mask_4: 0.4128  loss_dice_4: 3.174  loss_ce_5: 0.318  loss_mask_5: 0.4139  loss_dice_5: 3.177  loss_ce_6: 0.3013  loss_mask_6: 0.4126  loss_dice_6: 3.174  loss_ce_7: 0.3228  loss_mask_7: 0.4129  loss_dice_7: 3.163  loss_ce_8: 0.3033  loss_mask_8: 0.4133  loss_dice_8: 3.171  time: 1.5342  data_time: 0.1048  lr: 8.5885e-06  max_mem: 21366M
[01/17 09:57:41] d2.utils.events INFO:  eta: 1 day, 8:16:22  iter: 14019  total_loss: 39.17  loss_ce: 0.3095  loss_mask: 0.4213  loss_dice: 3.148  loss_ce_0: 0.5715  loss_mask_0: 0.41  loss_dice_0: 3.28  loss_ce_1: 0.3205  loss_mask_1: 0.4233  loss_dice_1: 3.191  loss_ce_2: 0.3321  loss_mask_2: 0.4178  loss_dice_2: 3.173  loss_ce_3: 0.3247  loss_mask_3: 0.4205  loss_dice_3: 3.164  loss_ce_4: 0.3358  loss_mask_4: 0.4219  loss_dice_4: 3.155  loss_ce_5: 0.3161  loss_mask_5: 0.4194  loss_dice_5: 3.162  loss_ce_6: 0.324  loss_mask_6: 0.4208  loss_dice_6: 3.152  loss_ce_7: 0.324  loss_mask_7: 0.4197  loss_dice_7: 3.155  loss_ce_8: 0.3261  loss_mask_8: 0.4203  loss_dice_8: 3.154  time: 1.5342  data_time: 0.0983  lr: 8.5865e-06  max_mem: 21366M
[01/17 09:58:12] d2.utils.events INFO:  eta: 1 day, 8:16:35  iter: 14039  total_loss: 39.46  loss_ce: 0.3049  loss_mask: 0.4386  loss_dice: 3.18  loss_ce_0: 0.5511  loss_mask_0: 0.4289  loss_dice_0: 3.289  loss_ce_1: 0.3261  loss_mask_1: 0.4452  loss_dice_1: 3.211  loss_ce_2: 0.3268  loss_mask_2: 0.4397  loss_dice_2: 3.19  loss_ce_3: 0.3283  loss_mask_3: 0.4371  loss_dice_3: 3.177  loss_ce_4: 0.3226  loss_mask_4: 0.437  loss_dice_4: 3.172  loss_ce_5: 0.311  loss_mask_5: 0.4368  loss_dice_5: 3.186  loss_ce_6: 0.307  loss_mask_6: 0.4361  loss_dice_6: 3.18  loss_ce_7: 0.321  loss_mask_7: 0.4379  loss_dice_7: 3.168  loss_ce_8: 0.3208  loss_mask_8: 0.4374  loss_dice_8: 3.169  time: 1.5342  data_time: 0.1046  lr: 8.5845e-06  max_mem: 21366M
[01/17 09:58:42] d2.utils.events INFO:  eta: 1 day, 8:15:47  iter: 14059  total_loss: 38.89  loss_ce: 0.3119  loss_mask: 0.4241  loss_dice: 3.081  loss_ce_0: 0.6004  loss_mask_0: 0.4081  loss_dice_0: 3.206  loss_ce_1: 0.3433  loss_mask_1: 0.4266  loss_dice_1: 3.12  loss_ce_2: 0.3442  loss_mask_2: 0.4252  loss_dice_2: 3.094  loss_ce_3: 0.3142  loss_mask_3: 0.4244  loss_dice_3: 3.086  loss_ce_4: 0.3309  loss_mask_4: 0.428  loss_dice_4: 3.083  loss_ce_5: 0.322  loss_mask_5: 0.4263  loss_dice_5: 3.089  loss_ce_6: 0.3106  loss_mask_6: 0.4262  loss_dice_6: 3.087  loss_ce_7: 0.318  loss_mask_7: 0.4247  loss_dice_7: 3.078  loss_ce_8: 0.3034  loss_mask_8: 0.4267  loss_dice_8: 3.079  time: 1.5341  data_time: 0.1103  lr: 8.5824e-06  max_mem: 21366M
[01/17 09:59:12] d2.utils.events INFO:  eta: 1 day, 8:15:16  iter: 14079  total_loss: 38.86  loss_ce: 0.2938  loss_mask: 0.416  loss_dice: 3.115  loss_ce_0: 0.5604  loss_mask_0: 0.4146  loss_dice_0: 3.253  loss_ce_1: 0.3034  loss_mask_1: 0.4263  loss_dice_1: 3.157  loss_ce_2: 0.2995  loss_mask_2: 0.4185  loss_dice_2: 3.129  loss_ce_3: 0.2946  loss_mask_3: 0.417  loss_dice_3: 3.116  loss_ce_4: 0.2929  loss_mask_4: 0.4159  loss_dice_4: 3.112  loss_ce_5: 0.2936  loss_mask_5: 0.4176  loss_dice_5: 3.124  loss_ce_6: 0.2999  loss_mask_6: 0.4161  loss_dice_6: 3.121  loss_ce_7: 0.2814  loss_mask_7: 0.4165  loss_dice_7: 3.115  loss_ce_8: 0.2937  loss_mask_8: 0.4144  loss_dice_8: 3.116  time: 1.5341  data_time: 0.0936  lr: 8.5804e-06  max_mem: 21366M
[01/17 09:59:43] d2.utils.events INFO:  eta: 1 day, 8:14:31  iter: 14099  total_loss: 39.93  loss_ce: 0.3201  loss_mask: 0.4298  loss_dice: 3.162  loss_ce_0: 0.6005  loss_mask_0: 0.4067  loss_dice_0: 3.273  loss_ce_1: 0.3383  loss_mask_1: 0.4268  loss_dice_1: 3.205  loss_ce_2: 0.3375  loss_mask_2: 0.4269  loss_dice_2: 3.183  loss_ce_3: 0.3158  loss_mask_3: 0.4266  loss_dice_3: 3.165  loss_ce_4: 0.3237  loss_mask_4: 0.4254  loss_dice_4: 3.168  loss_ce_5: 0.3169  loss_mask_5: 0.426  loss_dice_5: 3.173  loss_ce_6: 0.3019  loss_mask_6: 0.43  loss_dice_6: 3.167  loss_ce_7: 0.3176  loss_mask_7: 0.4285  loss_dice_7: 3.161  loss_ce_8: 0.3159  loss_mask_8: 0.4289  loss_dice_8: 3.16  time: 1.5341  data_time: 0.1110  lr: 8.5784e-06  max_mem: 21366M
[01/17 10:00:14] d2.utils.events INFO:  eta: 1 day, 8:14:00  iter: 14119  total_loss: 39.22  loss_ce: 0.2892  loss_mask: 0.426  loss_dice: 3.147  loss_ce_0: 0.5931  loss_mask_0: 0.422  loss_dice_0: 3.28  loss_ce_1: 0.3288  loss_mask_1: 0.4272  loss_dice_1: 3.193  loss_ce_2: 0.3236  loss_mask_2: 0.4182  loss_dice_2: 3.16  loss_ce_3: 0.3072  loss_mask_3: 0.4236  loss_dice_3: 3.153  loss_ce_4: 0.318  loss_mask_4: 0.4263  loss_dice_4: 3.147  loss_ce_5: 0.2949  loss_mask_5: 0.4244  loss_dice_5: 3.15  loss_ce_6: 0.2988  loss_mask_6: 0.4238  loss_dice_6: 3.144  loss_ce_7: 0.2943  loss_mask_7: 0.4238  loss_dice_7: 3.143  loss_ce_8: 0.2871  loss_mask_8: 0.4265  loss_dice_8: 3.147  time: 1.5341  data_time: 0.1092  lr: 8.5763e-06  max_mem: 21366M
[01/17 10:00:44] d2.utils.events INFO:  eta: 1 day, 8:14:10  iter: 14139  total_loss: 39.19  loss_ce: 0.2899  loss_mask: 0.4236  loss_dice: 3.161  loss_ce_0: 0.5611  loss_mask_0: 0.4128  loss_dice_0: 3.283  loss_ce_1: 0.3062  loss_mask_1: 0.432  loss_dice_1: 3.2  loss_ce_2: 0.3089  loss_mask_2: 0.4288  loss_dice_2: 3.175  loss_ce_3: 0.2995  loss_mask_3: 0.4249  loss_dice_3: 3.168  loss_ce_4: 0.289  loss_mask_4: 0.4253  loss_dice_4: 3.166  loss_ce_5: 0.2864  loss_mask_5: 0.4257  loss_dice_5: 3.166  loss_ce_6: 0.2798  loss_mask_6: 0.4261  loss_dice_6: 3.163  loss_ce_7: 0.2834  loss_mask_7: 0.4237  loss_dice_7: 3.161  loss_ce_8: 0.2815  loss_mask_8: 0.4252  loss_dice_8: 3.167  time: 1.5341  data_time: 0.0976  lr: 8.5743e-06  max_mem: 21366M
[01/17 10:01:15] d2.utils.events INFO:  eta: 1 day, 8:14:01  iter: 14159  total_loss: 38.51  loss_ce: 0.3047  loss_mask: 0.4096  loss_dice: 3.104  loss_ce_0: 0.5707  loss_mask_0: 0.3989  loss_dice_0: 3.242  loss_ce_1: 0.3015  loss_mask_1: 0.4093  loss_dice_1: 3.151  loss_ce_2: 0.3227  loss_mask_2: 0.4073  loss_dice_2: 3.119  loss_ce_3: 0.3081  loss_mask_3: 0.4087  loss_dice_3: 3.11  loss_ce_4: 0.3176  loss_mask_4: 0.4111  loss_dice_4: 3.107  loss_ce_5: 0.2971  loss_mask_5: 0.4078  loss_dice_5: 3.105  loss_ce_6: 0.2913  loss_mask_6: 0.4088  loss_dice_6: 3.111  loss_ce_7: 0.2991  loss_mask_7: 0.4098  loss_dice_7: 3.106  loss_ce_8: 0.314  loss_mask_8: 0.4091  loss_dice_8: 3.103  time: 1.5341  data_time: 0.0928  lr: 8.5723e-06  max_mem: 21366M
[01/17 10:01:46] d2.utils.events INFO:  eta: 1 day, 8:13:52  iter: 14179  total_loss: 39.74  loss_ce: 0.2997  loss_mask: 0.4143  loss_dice: 3.193  loss_ce_0: 0.6041  loss_mask_0: 0.4012  loss_dice_0: 3.332  loss_ce_1: 0.3383  loss_mask_1: 0.4118  loss_dice_1: 3.231  loss_ce_2: 0.3268  loss_mask_2: 0.4147  loss_dice_2: 3.21  loss_ce_3: 0.3181  loss_mask_3: 0.4143  loss_dice_3: 3.201  loss_ce_4: 0.3258  loss_mask_4: 0.4151  loss_dice_4: 3.193  loss_ce_5: 0.2988  loss_mask_5: 0.415  loss_dice_5: 3.198  loss_ce_6: 0.3044  loss_mask_6: 0.4149  loss_dice_6: 3.192  loss_ce_7: 0.3152  loss_mask_7: 0.4143  loss_dice_7: 3.193  loss_ce_8: 0.3051  loss_mask_8: 0.4149  loss_dice_8: 3.195  time: 1.5341  data_time: 0.1027  lr: 8.5702e-06  max_mem: 21366M
[01/17 10:02:16] d2.utils.events INFO:  eta: 1 day, 8:13:41  iter: 14199  total_loss: 38.64  loss_ce: 0.2906  loss_mask: 0.4169  loss_dice: 3.104  loss_ce_0: 0.5803  loss_mask_0: 0.4137  loss_dice_0: 3.238  loss_ce_1: 0.3072  loss_mask_1: 0.423  loss_dice_1: 3.15  loss_ce_2: 0.3054  loss_mask_2: 0.4207  loss_dice_2: 3.124  loss_ce_3: 0.3069  loss_mask_3: 0.4197  loss_dice_3: 3.109  loss_ce_4: 0.3017  loss_mask_4: 0.4174  loss_dice_4: 3.111  loss_ce_5: 0.3013  loss_mask_5: 0.4186  loss_dice_5: 3.117  loss_ce_6: 0.3011  loss_mask_6: 0.4182  loss_dice_6: 3.102  loss_ce_7: 0.2952  loss_mask_7: 0.4171  loss_dice_7: 3.105  loss_ce_8: 0.2953  loss_mask_8: 0.417  loss_dice_8: 3.113  time: 1.5341  data_time: 0.1007  lr: 8.5682e-06  max_mem: 21366M
[01/17 10:02:47] d2.utils.events INFO:  eta: 1 day, 8:12:08  iter: 14219  total_loss: 39.41  loss_ce: 0.3095  loss_mask: 0.4234  loss_dice: 3.163  loss_ce_0: 0.5704  loss_mask_0: 0.4118  loss_dice_0: 3.295  loss_ce_1: 0.3114  loss_mask_1: 0.4296  loss_dice_1: 3.208  loss_ce_2: 0.3246  loss_mask_2: 0.4273  loss_dice_2: 3.181  loss_ce_3: 0.3117  loss_mask_3: 0.4235  loss_dice_3: 3.174  loss_ce_4: 0.3145  loss_mask_4: 0.4234  loss_dice_4: 3.175  loss_ce_5: 0.3118  loss_mask_5: 0.426  loss_dice_5: 3.174  loss_ce_6: 0.306  loss_mask_6: 0.4268  loss_dice_6: 3.162  loss_ce_7: 0.3127  loss_mask_7: 0.4255  loss_dice_7: 3.158  loss_ce_8: 0.3008  loss_mask_8: 0.4234  loss_dice_8: 3.164  time: 1.5341  data_time: 0.0884  lr: 8.5662e-06  max_mem: 21366M
[01/17 10:03:18] d2.utils.events INFO:  eta: 1 day, 8:11:16  iter: 14239  total_loss: 39.65  loss_ce: 0.3357  loss_mask: 0.4151  loss_dice: 3.158  loss_ce_0: 0.6456  loss_mask_0: 0.4019  loss_dice_0: 3.276  loss_ce_1: 0.3447  loss_mask_1: 0.4176  loss_dice_1: 3.195  loss_ce_2: 0.3527  loss_mask_2: 0.4152  loss_dice_2: 3.177  loss_ce_3: 0.3487  loss_mask_3: 0.4147  loss_dice_3: 3.153  loss_ce_4: 0.3247  loss_mask_4: 0.4172  loss_dice_4: 3.152  loss_ce_5: 0.3282  loss_mask_5: 0.4151  loss_dice_5: 3.159  loss_ce_6: 0.3269  loss_mask_6: 0.4153  loss_dice_6: 3.147  loss_ce_7: 0.3246  loss_mask_7: 0.4145  loss_dice_7: 3.14  loss_ce_8: 0.3277  loss_mask_8: 0.4152  loss_dice_8: 3.149  time: 1.5341  data_time: 0.0950  lr: 8.5641e-06  max_mem: 21366M
[01/17 10:03:48] d2.utils.events INFO:  eta: 1 day, 8:10:33  iter: 14259  total_loss: 38.86  loss_ce: 0.3136  loss_mask: 0.4095  loss_dice: 3.082  loss_ce_0: 0.5725  loss_mask_0: 0.3996  loss_dice_0: 3.213  loss_ce_1: 0.3149  loss_mask_1: 0.4149  loss_dice_1: 3.118  loss_ce_2: 0.3402  loss_mask_2: 0.4104  loss_dice_2: 3.084  loss_ce_3: 0.3258  loss_mask_3: 0.4117  loss_dice_3: 3.081  loss_ce_4: 0.3299  loss_mask_4: 0.41  loss_dice_4: 3.073  loss_ce_5: 0.3104  loss_mask_5: 0.4084  loss_dice_5: 3.087  loss_ce_6: 0.3261  loss_mask_6: 0.4098  loss_dice_6: 3.079  loss_ce_7: 0.3146  loss_mask_7: 0.4095  loss_dice_7: 3.074  loss_ce_8: 0.3017  loss_mask_8: 0.4086  loss_dice_8: 3.069  time: 1.5341  data_time: 0.0953  lr: 8.5621e-06  max_mem: 21366M
[01/17 10:04:19] d2.utils.events INFO:  eta: 1 day, 8:10:36  iter: 14279  total_loss: 39.59  loss_ce: 0.3164  loss_mask: 0.4273  loss_dice: 3.159  loss_ce_0: 0.5705  loss_mask_0: 0.4122  loss_dice_0: 3.287  loss_ce_1: 0.319  loss_mask_1: 0.432  loss_dice_1: 3.188  loss_ce_2: 0.3315  loss_mask_2: 0.4311  loss_dice_2: 3.16  loss_ce_3: 0.332  loss_mask_3: 0.4272  loss_dice_3: 3.157  loss_ce_4: 0.3279  loss_mask_4: 0.4246  loss_dice_4: 3.164  loss_ce_5: 0.3167  loss_mask_5: 0.4259  loss_dice_5: 3.164  loss_ce_6: 0.3264  loss_mask_6: 0.4247  loss_dice_6: 3.16  loss_ce_7: 0.3163  loss_mask_7: 0.4253  loss_dice_7: 3.155  loss_ce_8: 0.3105  loss_mask_8: 0.4273  loss_dice_8: 3.161  time: 1.5341  data_time: 0.0966  lr: 8.5601e-06  max_mem: 21366M
[01/17 10:04:50] d2.utils.events INFO:  eta: 1 day, 8:09:44  iter: 14299  total_loss: 38.85  loss_ce: 0.3142  loss_mask: 0.4276  loss_dice: 3.104  loss_ce_0: 0.5449  loss_mask_0: 0.4165  loss_dice_0: 3.235  loss_ce_1: 0.3346  loss_mask_1: 0.4249  loss_dice_1: 3.14  loss_ce_2: 0.3177  loss_mask_2: 0.4296  loss_dice_2: 3.125  loss_ce_3: 0.3115  loss_mask_3: 0.4257  loss_dice_3: 3.114  loss_ce_4: 0.3257  loss_mask_4: 0.4244  loss_dice_4: 3.11  loss_ce_5: 0.3057  loss_mask_5: 0.4247  loss_dice_5: 3.116  loss_ce_6: 0.3062  loss_mask_6: 0.4274  loss_dice_6: 3.107  loss_ce_7: 0.3112  loss_mask_7: 0.4268  loss_dice_7: 3.1  loss_ce_8: 0.3181  loss_mask_8: 0.4284  loss_dice_8: 3.104  time: 1.5341  data_time: 0.0938  lr: 8.558e-06  max_mem: 21366M
[01/17 10:05:20] d2.utils.events INFO:  eta: 1 day, 8:09:08  iter: 14319  total_loss: 39.46  loss_ce: 0.3088  loss_mask: 0.4186  loss_dice: 3.167  loss_ce_0: 0.5867  loss_mask_0: 0.4097  loss_dice_0: 3.283  loss_ce_1: 0.3365  loss_mask_1: 0.4252  loss_dice_1: 3.196  loss_ce_2: 0.3493  loss_mask_2: 0.4265  loss_dice_2: 3.176  loss_ce_3: 0.3282  loss_mask_3: 0.4236  loss_dice_3: 3.167  loss_ce_4: 0.3169  loss_mask_4: 0.4223  loss_dice_4: 3.157  loss_ce_5: 0.3215  loss_mask_5: 0.4219  loss_dice_5: 3.165  loss_ce_6: 0.318  loss_mask_6: 0.417  loss_dice_6: 3.164  loss_ce_7: 0.3273  loss_mask_7: 0.4165  loss_dice_7: 3.166  loss_ce_8: 0.3063  loss_mask_8: 0.418  loss_dice_8: 3.155  time: 1.5341  data_time: 0.0896  lr: 8.556e-06  max_mem: 21366M
[01/17 10:05:51] d2.utils.events INFO:  eta: 1 day, 8:07:59  iter: 14339  total_loss: 39.44  loss_ce: 0.3031  loss_mask: 0.4276  loss_dice: 3.174  loss_ce_0: 0.5537  loss_mask_0: 0.4175  loss_dice_0: 3.276  loss_ce_1: 0.3241  loss_mask_1: 0.4322  loss_dice_1: 3.198  loss_ce_2: 0.3161  loss_mask_2: 0.4309  loss_dice_2: 3.175  loss_ce_3: 0.3166  loss_mask_3: 0.4253  loss_dice_3: 3.171  loss_ce_4: 0.3156  loss_mask_4: 0.4262  loss_dice_4: 3.176  loss_ce_5: 0.3157  loss_mask_5: 0.427  loss_dice_5: 3.169  loss_ce_6: 0.3111  loss_mask_6: 0.4262  loss_dice_6: 3.169  loss_ce_7: 0.2878  loss_mask_7: 0.4244  loss_dice_7: 3.178  loss_ce_8: 0.3  loss_mask_8: 0.4276  loss_dice_8: 3.168  time: 1.5340  data_time: 0.0837  lr: 8.5539e-06  max_mem: 21366M
[01/17 10:06:21] d2.utils.events INFO:  eta: 1 day, 8:06:48  iter: 14359  total_loss: 40.64  loss_ce: 0.3147  loss_mask: 0.44  loss_dice: 3.233  loss_ce_0: 0.5598  loss_mask_0: 0.447  loss_dice_0: 3.327  loss_ce_1: 0.3338  loss_mask_1: 0.4469  loss_dice_1: 3.259  loss_ce_2: 0.3482  loss_mask_2: 0.4422  loss_dice_2: 3.244  loss_ce_3: 0.336  loss_mask_3: 0.4424  loss_dice_3: 3.233  loss_ce_4: 0.3216  loss_mask_4: 0.4424  loss_dice_4: 3.229  loss_ce_5: 0.3115  loss_mask_5: 0.4451  loss_dice_5: 3.23  loss_ce_6: 0.3205  loss_mask_6: 0.4429  loss_dice_6: 3.225  loss_ce_7: 0.3298  loss_mask_7: 0.4441  loss_dice_7: 3.228  loss_ce_8: 0.3321  loss_mask_8: 0.4427  loss_dice_8: 3.222  time: 1.5340  data_time: 0.1114  lr: 8.5519e-06  max_mem: 21366M
[01/17 10:06:53] d2.utils.events INFO:  eta: 1 day, 8:05:40  iter: 14379  total_loss: 40.21  loss_ce: 0.3054  loss_mask: 0.4258  loss_dice: 3.202  loss_ce_0: 0.5847  loss_mask_0: 0.4195  loss_dice_0: 3.314  loss_ce_1: 0.3379  loss_mask_1: 0.4317  loss_dice_1: 3.233  loss_ce_2: 0.3291  loss_mask_2: 0.431  loss_dice_2: 3.219  loss_ce_3: 0.3281  loss_mask_3: 0.4266  loss_dice_3: 3.208  loss_ce_4: 0.321  loss_mask_4: 0.4244  loss_dice_4: 3.207  loss_ce_5: 0.3019  loss_mask_5: 0.4253  loss_dice_5: 3.216  loss_ce_6: 0.3231  loss_mask_6: 0.427  loss_dice_6: 3.214  loss_ce_7: 0.3194  loss_mask_7: 0.4265  loss_dice_7: 3.208  loss_ce_8: 0.3134  loss_mask_8: 0.4267  loss_dice_8: 3.207  time: 1.5341  data_time: 0.0938  lr: 8.5499e-06  max_mem: 21366M
[01/17 10:07:24] d2.utils.events INFO:  eta: 1 day, 8:05:38  iter: 14399  total_loss: 38.67  loss_ce: 0.3113  loss_mask: 0.4154  loss_dice: 3.081  loss_ce_0: 0.5565  loss_mask_0: 0.3964  loss_dice_0: 3.224  loss_ce_1: 0.3356  loss_mask_1: 0.411  loss_dice_1: 3.137  loss_ce_2: 0.3279  loss_mask_2: 0.4142  loss_dice_2: 3.095  loss_ce_3: 0.3077  loss_mask_3: 0.4139  loss_dice_3: 3.09  loss_ce_4: 0.3029  loss_mask_4: 0.4154  loss_dice_4: 3.089  loss_ce_5: 0.3131  loss_mask_5: 0.4134  loss_dice_5: 3.1  loss_ce_6: 0.3007  loss_mask_6: 0.4121  loss_dice_6: 3.088  loss_ce_7: 0.301  loss_mask_7: 0.4141  loss_dice_7: 3.088  loss_ce_8: 0.2953  loss_mask_8: 0.4147  loss_dice_8: 3.077  time: 1.5341  data_time: 0.1045  lr: 8.5478e-06  max_mem: 21366M
[01/17 10:07:55] d2.utils.events INFO:  eta: 1 day, 8:05:17  iter: 14419  total_loss: 39.03  loss_ce: 0.2998  loss_mask: 0.4124  loss_dice: 3.124  loss_ce_0: 0.6128  loss_mask_0: 0.396  loss_dice_0: 3.26  loss_ce_1: 0.3369  loss_mask_1: 0.4155  loss_dice_1: 3.178  loss_ce_2: 0.3348  loss_mask_2: 0.4137  loss_dice_2: 3.152  loss_ce_3: 0.3126  loss_mask_3: 0.4118  loss_dice_3: 3.137  loss_ce_4: 0.3107  loss_mask_4: 0.4114  loss_dice_4: 3.131  loss_ce_5: 0.3134  loss_mask_5: 0.4102  loss_dice_5: 3.134  loss_ce_6: 0.2981  loss_mask_6: 0.4134  loss_dice_6: 3.135  loss_ce_7: 0.3064  loss_mask_7: 0.4122  loss_dice_7: 3.133  loss_ce_8: 0.3045  loss_mask_8: 0.4124  loss_dice_8: 3.126  time: 1.5341  data_time: 0.1015  lr: 8.5458e-06  max_mem: 21366M
[01/17 10:08:26] d2.utils.events INFO:  eta: 1 day, 8:05:27  iter: 14439  total_loss: 39.51  loss_ce: 0.3216  loss_mask: 0.4201  loss_dice: 3.178  loss_ce_0: 0.5532  loss_mask_0: 0.4134  loss_dice_0: 3.291  loss_ce_1: 0.3253  loss_mask_1: 0.427  loss_dice_1: 3.221  loss_ce_2: 0.3283  loss_mask_2: 0.4221  loss_dice_2: 3.197  loss_ce_3: 0.3313  loss_mask_3: 0.4223  loss_dice_3: 3.187  loss_ce_4: 0.3195  loss_mask_4: 0.4202  loss_dice_4: 3.192  loss_ce_5: 0.3184  loss_mask_5: 0.4198  loss_dice_5: 3.192  loss_ce_6: 0.3195  loss_mask_6: 0.4191  loss_dice_6: 3.184  loss_ce_7: 0.3167  loss_mask_7: 0.4222  loss_dice_7: 3.18  loss_ce_8: 0.3095  loss_mask_8: 0.4225  loss_dice_8: 3.184  time: 1.5341  data_time: 0.0962  lr: 8.5438e-06  max_mem: 21366M
[01/17 10:08:57] d2.utils.events INFO:  eta: 1 day, 8:05:31  iter: 14459  total_loss: 39.61  loss_ce: 0.2956  loss_mask: 0.4231  loss_dice: 3.174  loss_ce_0: 0.5905  loss_mask_0: 0.4226  loss_dice_0: 3.286  loss_ce_1: 0.3149  loss_mask_1: 0.4288  loss_dice_1: 3.206  loss_ce_2: 0.3185  loss_mask_2: 0.4232  loss_dice_2: 3.185  loss_ce_3: 0.3022  loss_mask_3: 0.4219  loss_dice_3: 3.169  loss_ce_4: 0.289  loss_mask_4: 0.4216  loss_dice_4: 3.178  loss_ce_5: 0.2966  loss_mask_5: 0.4183  loss_dice_5: 3.177  loss_ce_6: 0.2837  loss_mask_6: 0.4162  loss_dice_6: 3.171  loss_ce_7: 0.2987  loss_mask_7: 0.4178  loss_dice_7: 3.179  loss_ce_8: 0.2901  loss_mask_8: 0.4199  loss_dice_8: 3.177  time: 1.5341  data_time: 0.1060  lr: 8.5417e-06  max_mem: 21366M
[01/17 10:09:28] d2.utils.events INFO:  eta: 1 day, 8:05:04  iter: 14479  total_loss: 39.32  loss_ce: 0.3061  loss_mask: 0.4106  loss_dice: 3.127  loss_ce_0: 0.5963  loss_mask_0: 0.4115  loss_dice_0: 3.228  loss_ce_1: 0.3525  loss_mask_1: 0.4101  loss_dice_1: 3.165  loss_ce_2: 0.3542  loss_mask_2: 0.4092  loss_dice_2: 3.149  loss_ce_3: 0.3519  loss_mask_3: 0.4081  loss_dice_3: 3.135  loss_ce_4: 0.338  loss_mask_4: 0.4052  loss_dice_4: 3.127  loss_ce_5: 0.3176  loss_mask_5: 0.4098  loss_dice_5: 3.128  loss_ce_6: 0.328  loss_mask_6: 0.4119  loss_dice_6: 3.12  loss_ce_7: 0.3208  loss_mask_7: 0.4114  loss_dice_7: 3.125  loss_ce_8: 0.3162  loss_mask_8: 0.4084  loss_dice_8: 3.126  time: 1.5342  data_time: 0.1001  lr: 8.5397e-06  max_mem: 21366M
[01/17 10:09:59] d2.utils.events INFO:  eta: 1 day, 8:04:37  iter: 14499  total_loss: 38.94  loss_ce: 0.3074  loss_mask: 0.4241  loss_dice: 3.102  loss_ce_0: 0.5791  loss_mask_0: 0.4249  loss_dice_0: 3.234  loss_ce_1: 0.3235  loss_mask_1: 0.4284  loss_dice_1: 3.139  loss_ce_2: 0.3325  loss_mask_2: 0.4232  loss_dice_2: 3.116  loss_ce_3: 0.3115  loss_mask_3: 0.4235  loss_dice_3: 3.107  loss_ce_4: 0.3141  loss_mask_4: 0.4222  loss_dice_4: 3.103  loss_ce_5: 0.3013  loss_mask_5: 0.4237  loss_dice_5: 3.095  loss_ce_6: 0.301  loss_mask_6: 0.4259  loss_dice_6: 3.096  loss_ce_7: 0.2953  loss_mask_7: 0.4247  loss_dice_7: 3.099  loss_ce_8: 0.2989  loss_mask_8: 0.4251  loss_dice_8: 3.096  time: 1.5342  data_time: 0.0971  lr: 8.5377e-06  max_mem: 21366M
[01/17 10:10:30] d2.utils.events INFO:  eta: 1 day, 8:04:06  iter: 14519  total_loss: 39.49  loss_ce: 0.3086  loss_mask: 0.4248  loss_dice: 3.156  loss_ce_0: 0.5859  loss_mask_0: 0.4198  loss_dice_0: 3.272  loss_ce_1: 0.3503  loss_mask_1: 0.4293  loss_dice_1: 3.182  loss_ce_2: 0.3357  loss_mask_2: 0.427  loss_dice_2: 3.168  loss_ce_3: 0.3278  loss_mask_3: 0.4257  loss_dice_3: 3.165  loss_ce_4: 0.3239  loss_mask_4: 0.4254  loss_dice_4: 3.164  loss_ce_5: 0.3162  loss_mask_5: 0.4233  loss_dice_5: 3.159  loss_ce_6: 0.3134  loss_mask_6: 0.4244  loss_dice_6: 3.159  loss_ce_7: 0.3215  loss_mask_7: 0.425  loss_dice_7: 3.161  loss_ce_8: 0.3171  loss_mask_8: 0.4254  loss_dice_8: 3.162  time: 1.5342  data_time: 0.0920  lr: 8.5356e-06  max_mem: 21366M
[01/17 10:11:01] d2.utils.events INFO:  eta: 1 day, 8:03:52  iter: 14539  total_loss: 39.5  loss_ce: 0.31  loss_mask: 0.4083  loss_dice: 3.14  loss_ce_0: 0.5948  loss_mask_0: 0.4036  loss_dice_0: 3.27  loss_ce_1: 0.3407  loss_mask_1: 0.4096  loss_dice_1: 3.193  loss_ce_2: 0.3308  loss_mask_2: 0.4089  loss_dice_2: 3.15  loss_ce_3: 0.3123  loss_mask_3: 0.4077  loss_dice_3: 3.152  loss_ce_4: 0.3229  loss_mask_4: 0.4068  loss_dice_4: 3.15  loss_ce_5: 0.3147  loss_mask_5: 0.4076  loss_dice_5: 3.151  loss_ce_6: 0.3151  loss_mask_6: 0.4067  loss_dice_6: 3.145  loss_ce_7: 0.3169  loss_mask_7: 0.4061  loss_dice_7: 3.154  loss_ce_8: 0.3038  loss_mask_8: 0.4077  loss_dice_8: 3.148  time: 1.5342  data_time: 0.0999  lr: 8.5336e-06  max_mem: 21366M
[01/17 10:11:32] d2.utils.events INFO:  eta: 1 day, 8:03:52  iter: 14559  total_loss: 39.16  loss_ce: 0.3084  loss_mask: 0.4312  loss_dice: 3.122  loss_ce_0: 0.5713  loss_mask_0: 0.4284  loss_dice_0: 3.233  loss_ce_1: 0.3405  loss_mask_1: 0.4483  loss_dice_1: 3.149  loss_ce_2: 0.3447  loss_mask_2: 0.4359  loss_dice_2: 3.131  loss_ce_3: 0.3217  loss_mask_3: 0.4387  loss_dice_3: 3.131  loss_ce_4: 0.3209  loss_mask_4: 0.4386  loss_dice_4: 3.118  loss_ce_5: 0.3203  loss_mask_5: 0.4358  loss_dice_5: 3.125  loss_ce_6: 0.3296  loss_mask_6: 0.4364  loss_dice_6: 3.114  loss_ce_7: 0.3093  loss_mask_7: 0.4375  loss_dice_7: 3.12  loss_ce_8: 0.3134  loss_mask_8: 0.4346  loss_dice_8: 3.118  time: 1.5342  data_time: 0.1131  lr: 8.5316e-06  max_mem: 21366M
[01/17 10:12:02] d2.utils.events INFO:  eta: 1 day, 8:02:51  iter: 14579  total_loss: 38.91  loss_ce: 0.2786  loss_mask: 0.4236  loss_dice: 3.119  loss_ce_0: 0.5937  loss_mask_0: 0.4154  loss_dice_0: 3.254  loss_ce_1: 0.3302  loss_mask_1: 0.4251  loss_dice_1: 3.177  loss_ce_2: 0.3256  loss_mask_2: 0.4235  loss_dice_2: 3.14  loss_ce_3: 0.3087  loss_mask_3: 0.4214  loss_dice_3: 3.13  loss_ce_4: 0.3109  loss_mask_4: 0.4209  loss_dice_4: 3.128  loss_ce_5: 0.2896  loss_mask_5: 0.42  loss_dice_5: 3.128  loss_ce_6: 0.2798  loss_mask_6: 0.4215  loss_dice_6: 3.123  loss_ce_7: 0.2903  loss_mask_7: 0.4211  loss_dice_7: 3.122  loss_ce_8: 0.2903  loss_mask_8: 0.4233  loss_dice_8: 3.124  time: 1.5342  data_time: 0.0960  lr: 8.5295e-06  max_mem: 21366M
[01/17 10:12:34] d2.utils.events INFO:  eta: 1 day, 8:04:05  iter: 14599  total_loss: 39.25  loss_ce: 0.3205  loss_mask: 0.4095  loss_dice: 3.151  loss_ce_0: 0.5781  loss_mask_0: 0.3915  loss_dice_0: 3.264  loss_ce_1: 0.3441  loss_mask_1: 0.4096  loss_dice_1: 3.179  loss_ce_2: 0.3572  loss_mask_2: 0.4109  loss_dice_2: 3.158  loss_ce_3: 0.3385  loss_mask_3: 0.4096  loss_dice_3: 3.149  loss_ce_4: 0.3514  loss_mask_4: 0.409  loss_dice_4: 3.15  loss_ce_5: 0.3457  loss_mask_5: 0.4075  loss_dice_5: 3.15  loss_ce_6: 0.3486  loss_mask_6: 0.4093  loss_dice_6: 3.147  loss_ce_7: 0.336  loss_mask_7: 0.4094  loss_dice_7: 3.15  loss_ce_8: 0.3244  loss_mask_8: 0.4105  loss_dice_8: 3.146  time: 1.5343  data_time: 0.1001  lr: 8.5275e-06  max_mem: 21366M
[01/17 10:13:04] d2.utils.events INFO:  eta: 1 day, 8:03:12  iter: 14619  total_loss: 38.97  loss_ce: 0.3178  loss_mask: 0.4179  loss_dice: 3.086  loss_ce_0: 0.5849  loss_mask_0: 0.4068  loss_dice_0: 3.206  loss_ce_1: 0.3341  loss_mask_1: 0.4195  loss_dice_1: 3.133  loss_ce_2: 0.3292  loss_mask_2: 0.4188  loss_dice_2: 3.096  loss_ce_3: 0.3153  loss_mask_3: 0.4176  loss_dice_3: 3.095  loss_ce_4: 0.3255  loss_mask_4: 0.4192  loss_dice_4: 3.094  loss_ce_5: 0.302  loss_mask_5: 0.4182  loss_dice_5: 3.086  loss_ce_6: 0.3018  loss_mask_6: 0.4191  loss_dice_6: 3.087  loss_ce_7: 0.3028  loss_mask_7: 0.4181  loss_dice_7: 3.08  loss_ce_8: 0.3148  loss_mask_8: 0.4172  loss_dice_8: 3.079  time: 1.5342  data_time: 0.0983  lr: 8.5255e-06  max_mem: 21366M
[01/17 10:13:35] d2.utils.events INFO:  eta: 1 day, 8:03:04  iter: 14639  total_loss: 38.97  loss_ce: 0.307  loss_mask: 0.4151  loss_dice: 3.125  loss_ce_0: 0.5681  loss_mask_0: 0.409  loss_dice_0: 3.259  loss_ce_1: 0.3547  loss_mask_1: 0.4203  loss_dice_1: 3.157  loss_ce_2: 0.3387  loss_mask_2: 0.4146  loss_dice_2: 3.132  loss_ce_3: 0.337  loss_mask_3: 0.4126  loss_dice_3: 3.126  loss_ce_4: 0.316  loss_mask_4: 0.4119  loss_dice_4: 3.126  loss_ce_5: 0.3138  loss_mask_5: 0.4127  loss_dice_5: 3.13  loss_ce_6: 0.3117  loss_mask_6: 0.413  loss_dice_6: 3.125  loss_ce_7: 0.3023  loss_mask_7: 0.4149  loss_dice_7: 3.126  loss_ce_8: 0.3086  loss_mask_8: 0.4146  loss_dice_8: 3.12  time: 1.5342  data_time: 0.0840  lr: 8.5234e-06  max_mem: 21366M
[01/17 10:14:06] d2.utils.events INFO:  eta: 1 day, 8:02:44  iter: 14659  total_loss: 38.81  loss_ce: 0.2939  loss_mask: 0.4136  loss_dice: 3.114  loss_ce_0: 0.5777  loss_mask_0: 0.4017  loss_dice_0: 3.241  loss_ce_1: 0.3104  loss_mask_1: 0.4177  loss_dice_1: 3.164  loss_ce_2: 0.3322  loss_mask_2: 0.416  loss_dice_2: 3.136  loss_ce_3: 0.32  loss_mask_3: 0.4139  loss_dice_3: 3.126  loss_ce_4: 0.3105  loss_mask_4: 0.4138  loss_dice_4: 3.121  loss_ce_5: 0.3085  loss_mask_5: 0.4116  loss_dice_5: 3.12  loss_ce_6: 0.3144  loss_mask_6: 0.4132  loss_dice_6: 3.119  loss_ce_7: 0.3125  loss_mask_7: 0.4126  loss_dice_7: 3.109  loss_ce_8: 0.2995  loss_mask_8: 0.4143  loss_dice_8: 3.122  time: 1.5342  data_time: 0.1018  lr: 8.5214e-06  max_mem: 21366M
[01/17 10:14:37] d2.utils.events INFO:  eta: 1 day, 8:02:45  iter: 14679  total_loss: 39.58  loss_ce: 0.3204  loss_mask: 0.4268  loss_dice: 3.19  loss_ce_0: 0.5984  loss_mask_0: 0.415  loss_dice_0: 3.283  loss_ce_1: 0.3505  loss_mask_1: 0.4295  loss_dice_1: 3.212  loss_ce_2: 0.3452  loss_mask_2: 0.4277  loss_dice_2: 3.195  loss_ce_3: 0.3349  loss_mask_3: 0.424  loss_dice_3: 3.197  loss_ce_4: 0.3415  loss_mask_4: 0.4269  loss_dice_4: 3.196  loss_ce_5: 0.3292  loss_mask_5: 0.4292  loss_dice_5: 3.192  loss_ce_6: 0.3174  loss_mask_6: 0.4275  loss_dice_6: 3.191  loss_ce_7: 0.3158  loss_mask_7: 0.4263  loss_dice_7: 3.187  loss_ce_8: 0.3314  loss_mask_8: 0.4259  loss_dice_8: 3.186  time: 1.5342  data_time: 0.0857  lr: 8.5193e-06  max_mem: 21366M
[01/17 10:15:07] d2.utils.events INFO:  eta: 1 day, 8:02:17  iter: 14699  total_loss: 39.42  loss_ce: 0.3223  loss_mask: 0.4148  loss_dice: 3.125  loss_ce_0: 0.5905  loss_mask_0: 0.4088  loss_dice_0: 3.237  loss_ce_1: 0.35  loss_mask_1: 0.4191  loss_dice_1: 3.152  loss_ce_2: 0.3448  loss_mask_2: 0.4183  loss_dice_2: 3.133  loss_ce_3: 0.33  loss_mask_3: 0.413  loss_dice_3: 3.129  loss_ce_4: 0.344  loss_mask_4: 0.4115  loss_dice_4: 3.12  loss_ce_5: 0.3139  loss_mask_5: 0.4132  loss_dice_5: 3.137  loss_ce_6: 0.3146  loss_mask_6: 0.4121  loss_dice_6: 3.134  loss_ce_7: 0.3201  loss_mask_7: 0.411  loss_dice_7: 3.129  loss_ce_8: 0.3171  loss_mask_8: 0.4117  loss_dice_8: 3.124  time: 1.5343  data_time: 0.0929  lr: 8.5173e-06  max_mem: 21366M
[01/17 10:15:38] d2.utils.events INFO:  eta: 1 day, 8:02:34  iter: 14719  total_loss: 38.85  loss_ce: 0.3066  loss_mask: 0.4117  loss_dice: 3.099  loss_ce_0: 0.5926  loss_mask_0: 0.3983  loss_dice_0: 3.237  loss_ce_1: 0.333  loss_mask_1: 0.4102  loss_dice_1: 3.135  loss_ce_2: 0.3256  loss_mask_2: 0.4103  loss_dice_2: 3.121  loss_ce_3: 0.3076  loss_mask_3: 0.4097  loss_dice_3: 3.113  loss_ce_4: 0.316  loss_mask_4: 0.4113  loss_dice_4: 3.111  loss_ce_5: 0.309  loss_mask_5: 0.4113  loss_dice_5: 3.108  loss_ce_6: 0.3034  loss_mask_6: 0.4117  loss_dice_6: 3.108  loss_ce_7: 0.2958  loss_mask_7: 0.4109  loss_dice_7: 3.11  loss_ce_8: 0.2998  loss_mask_8: 0.411  loss_dice_8: 3.098  time: 1.5343  data_time: 0.0967  lr: 8.5153e-06  max_mem: 21366M
[01/17 10:16:09] d2.utils.events INFO:  eta: 1 day, 8:02:04  iter: 14739  total_loss: 38.84  loss_ce: 0.2789  loss_mask: 0.4145  loss_dice: 3.099  loss_ce_0: 0.539  loss_mask_0: 0.4001  loss_dice_0: 3.225  loss_ce_1: 0.3108  loss_mask_1: 0.4167  loss_dice_1: 3.133  loss_ce_2: 0.3031  loss_mask_2: 0.414  loss_dice_2: 3.125  loss_ce_3: 0.3011  loss_mask_3: 0.4167  loss_dice_3: 3.112  loss_ce_4: 0.2905  loss_mask_4: 0.4161  loss_dice_4: 3.118  loss_ce_5: 0.295  loss_mask_5: 0.4154  loss_dice_5: 3.106  loss_ce_6: 0.2816  loss_mask_6: 0.4162  loss_dice_6: 3.112  loss_ce_7: 0.283  loss_mask_7: 0.4142  loss_dice_7: 3.11  loss_ce_8: 0.2871  loss_mask_8: 0.4163  loss_dice_8: 3.116  time: 1.5343  data_time: 0.0948  lr: 8.5132e-06  max_mem: 21410M
[01/17 10:16:40] d2.utils.events INFO:  eta: 1 day, 8:01:33  iter: 14759  total_loss: 38.83  loss_ce: 0.2904  loss_mask: 0.4242  loss_dice: 3.087  loss_ce_0: 0.5544  loss_mask_0: 0.4186  loss_dice_0: 3.213  loss_ce_1: 0.3085  loss_mask_1: 0.4324  loss_dice_1: 3.133  loss_ce_2: 0.3044  loss_mask_2: 0.4267  loss_dice_2: 3.117  loss_ce_3: 0.3045  loss_mask_3: 0.4273  loss_dice_3: 3.109  loss_ce_4: 0.304  loss_mask_4: 0.4255  loss_dice_4: 3.106  loss_ce_5: 0.2899  loss_mask_5: 0.4246  loss_dice_5: 3.11  loss_ce_6: 0.2921  loss_mask_6: 0.4239  loss_dice_6: 3.098  loss_ce_7: 0.2953  loss_mask_7: 0.4231  loss_dice_7: 3.097  loss_ce_8: 0.3003  loss_mask_8: 0.4229  loss_dice_8: 3.098  time: 1.5343  data_time: 0.0920  lr: 8.5112e-06  max_mem: 21410M
[01/17 10:17:11] d2.utils.events INFO:  eta: 1 day, 8:01:02  iter: 14779  total_loss: 39.5  loss_ce: 0.3094  loss_mask: 0.4289  loss_dice: 3.162  loss_ce_0: 0.5791  loss_mask_0: 0.4289  loss_dice_0: 3.289  loss_ce_1: 0.3296  loss_mask_1: 0.4443  loss_dice_1: 3.203  loss_ce_2: 0.3415  loss_mask_2: 0.4382  loss_dice_2: 3.182  loss_ce_3: 0.3138  loss_mask_3: 0.4315  loss_dice_3: 3.164  loss_ce_4: 0.3226  loss_mask_4: 0.4307  loss_dice_4: 3.161  loss_ce_5: 0.3217  loss_mask_5: 0.4323  loss_dice_5: 3.166  loss_ce_6: 0.3171  loss_mask_6: 0.4296  loss_dice_6: 3.159  loss_ce_7: 0.3165  loss_mask_7: 0.4295  loss_dice_7: 3.162  loss_ce_8: 0.306  loss_mask_8: 0.4303  loss_dice_8: 3.165  time: 1.5343  data_time: 0.1014  lr: 8.5092e-06  max_mem: 21410M
[01/17 10:17:42] d2.utils.events INFO:  eta: 1 day, 8:00:39  iter: 14799  total_loss: 38.65  loss_ce: 0.3113  loss_mask: 0.4109  loss_dice: 3.118  loss_ce_0: 0.6282  loss_mask_0: 0.4033  loss_dice_0: 3.236  loss_ce_1: 0.3386  loss_mask_1: 0.4149  loss_dice_1: 3.158  loss_ce_2: 0.3255  loss_mask_2: 0.4126  loss_dice_2: 3.131  loss_ce_3: 0.3224  loss_mask_3: 0.4122  loss_dice_3: 3.115  loss_ce_4: 0.3187  loss_mask_4: 0.4103  loss_dice_4: 3.121  loss_ce_5: 0.3116  loss_mask_5: 0.4126  loss_dice_5: 3.133  loss_ce_6: 0.3138  loss_mask_6: 0.4134  loss_dice_6: 3.119  loss_ce_7: 0.3101  loss_mask_7: 0.4122  loss_dice_7: 3.12  loss_ce_8: 0.3205  loss_mask_8: 0.4112  loss_dice_8: 3.125  time: 1.5343  data_time: 0.0988  lr: 8.5071e-06  max_mem: 21410M
[01/17 10:18:13] d2.utils.events INFO:  eta: 1 day, 8:00:44  iter: 14819  total_loss: 38.69  loss_ce: 0.3334  loss_mask: 0.4108  loss_dice: 3.078  loss_ce_0: 0.5581  loss_mask_0: 0.3954  loss_dice_0: 3.201  loss_ce_1: 0.3265  loss_mask_1: 0.4108  loss_dice_1: 3.121  loss_ce_2: 0.3444  loss_mask_2: 0.4094  loss_dice_2: 3.094  loss_ce_3: 0.3366  loss_mask_3: 0.408  loss_dice_3: 3.084  loss_ce_4: 0.3191  loss_mask_4: 0.4071  loss_dice_4: 3.085  loss_ce_5: 0.3173  loss_mask_5: 0.4062  loss_dice_5: 3.085  loss_ce_6: 0.3209  loss_mask_6: 0.4086  loss_dice_6: 3.084  loss_ce_7: 0.3362  loss_mask_7: 0.4087  loss_dice_7: 3.084  loss_ce_8: 0.3325  loss_mask_8: 0.4085  loss_dice_8: 3.083  time: 1.5343  data_time: 0.0960  lr: 8.5051e-06  max_mem: 21410M
[01/17 10:18:44] d2.utils.events INFO:  eta: 1 day, 8:01:58  iter: 14839  total_loss: 39.07  loss_ce: 0.305  loss_mask: 0.4123  loss_dice: 3.104  loss_ce_0: 0.5648  loss_mask_0: 0.3977  loss_dice_0: 3.233  loss_ce_1: 0.3171  loss_mask_1: 0.4149  loss_dice_1: 3.148  loss_ce_2: 0.3262  loss_mask_2: 0.4115  loss_dice_2: 3.122  loss_ce_3: 0.3239  loss_mask_3: 0.4165  loss_dice_3: 3.107  loss_ce_4: 0.3161  loss_mask_4: 0.4152  loss_dice_4: 3.116  loss_ce_5: 0.3164  loss_mask_5: 0.4142  loss_dice_5: 3.113  loss_ce_6: 0.3106  loss_mask_6: 0.4131  loss_dice_6: 3.116  loss_ce_7: 0.3052  loss_mask_7: 0.414  loss_dice_7: 3.106  loss_ce_8: 0.3116  loss_mask_8: 0.4133  loss_dice_8: 3.105  time: 1.5343  data_time: 0.0996  lr: 8.5031e-06  max_mem: 21410M
[01/17 10:19:15] d2.utils.events INFO:  eta: 1 day, 8:02:28  iter: 14859  total_loss: 38.64  loss_ce: 0.2956  loss_mask: 0.4244  loss_dice: 3.09  loss_ce_0: 0.6109  loss_mask_0: 0.4102  loss_dice_0: 3.204  loss_ce_1: 0.3425  loss_mask_1: 0.426  loss_dice_1: 3.124  loss_ce_2: 0.3411  loss_mask_2: 0.4266  loss_dice_2: 3.109  loss_ce_3: 0.3267  loss_mask_3: 0.422  loss_dice_3: 3.099  loss_ce_4: 0.3124  loss_mask_4: 0.423  loss_dice_4: 3.095  loss_ce_5: 0.2881  loss_mask_5: 0.4237  loss_dice_5: 3.105  loss_ce_6: 0.3039  loss_mask_6: 0.4257  loss_dice_6: 3.096  loss_ce_7: 0.3031  loss_mask_7: 0.4244  loss_dice_7: 3.092  loss_ce_8: 0.311  loss_mask_8: 0.4251  loss_dice_8: 3.097  time: 1.5343  data_time: 0.0925  lr: 8.501e-06  max_mem: 21410M
[01/17 10:19:45] d2.utils.events INFO:  eta: 1 day, 8:01:58  iter: 14879  total_loss: 38.97  loss_ce: 0.3237  loss_mask: 0.413  loss_dice: 3.081  loss_ce_0: 0.5848  loss_mask_0: 0.4097  loss_dice_0: 3.194  loss_ce_1: 0.3317  loss_mask_1: 0.4228  loss_dice_1: 3.12  loss_ce_2: 0.3625  loss_mask_2: 0.4169  loss_dice_2: 3.098  loss_ce_3: 0.3137  loss_mask_3: 0.4156  loss_dice_3: 3.081  loss_ce_4: 0.3376  loss_mask_4: 0.416  loss_dice_4: 3.087  loss_ce_5: 0.3229  loss_mask_5: 0.4164  loss_dice_5: 3.086  loss_ce_6: 0.3207  loss_mask_6: 0.417  loss_dice_6: 3.079  loss_ce_7: 0.3098  loss_mask_7: 0.4142  loss_dice_7: 3.089  loss_ce_8: 0.3178  loss_mask_8: 0.4144  loss_dice_8: 3.083  time: 1.5343  data_time: 0.0876  lr: 8.499e-06  max_mem: 21410M
[01/17 10:20:16] d2.utils.events INFO:  eta: 1 day, 8:01:54  iter: 14899  total_loss: 39.44  loss_ce: 0.2967  loss_mask: 0.4121  loss_dice: 3.148  loss_ce_0: 0.5699  loss_mask_0: 0.4045  loss_dice_0: 3.273  loss_ce_1: 0.3422  loss_mask_1: 0.415  loss_dice_1: 3.186  loss_ce_2: 0.3356  loss_mask_2: 0.4178  loss_dice_2: 3.176  loss_ce_3: 0.3212  loss_mask_3: 0.4136  loss_dice_3: 3.16  loss_ce_4: 0.3252  loss_mask_4: 0.4098  loss_dice_4: 3.159  loss_ce_5: 0.2965  loss_mask_5: 0.4119  loss_dice_5: 3.157  loss_ce_6: 0.3042  loss_mask_6: 0.4121  loss_dice_6: 3.157  loss_ce_7: 0.3107  loss_mask_7: 0.4108  loss_dice_7: 3.155  loss_ce_8: 0.311  loss_mask_8: 0.4122  loss_dice_8: 3.151  time: 1.5343  data_time: 0.0994  lr: 8.4969e-06  max_mem: 21410M
[01/17 10:20:47] d2.utils.events INFO:  eta: 1 day, 8:01:18  iter: 14919  total_loss: 38.94  loss_ce: 0.3262  loss_mask: 0.4136  loss_dice: 3.104  loss_ce_0: 0.5771  loss_mask_0: 0.4029  loss_dice_0: 3.22  loss_ce_1: 0.3496  loss_mask_1: 0.414  loss_dice_1: 3.139  loss_ce_2: 0.3499  loss_mask_2: 0.4083  loss_dice_2: 3.122  loss_ce_3: 0.3577  loss_mask_3: 0.4074  loss_dice_3: 3.108  loss_ce_4: 0.3421  loss_mask_4: 0.4086  loss_dice_4: 3.11  loss_ce_5: 0.3525  loss_mask_5: 0.4092  loss_dice_5: 3.108  loss_ce_6: 0.3287  loss_mask_6: 0.4112  loss_dice_6: 3.103  loss_ce_7: 0.3411  loss_mask_7: 0.4114  loss_dice_7: 3.103  loss_ce_8: 0.3286  loss_mask_8: 0.4126  loss_dice_8: 3.103  time: 1.5343  data_time: 0.0966  lr: 8.4949e-06  max_mem: 21410M
[01/17 10:21:18] d2.utils.events INFO:  eta: 1 day, 8:00:52  iter: 14939  total_loss: 39.51  loss_ce: 0.3311  loss_mask: 0.4133  loss_dice: 3.154  loss_ce_0: 0.5719  loss_mask_0: 0.4151  loss_dice_0: 3.273  loss_ce_1: 0.3396  loss_mask_1: 0.4262  loss_dice_1: 3.201  loss_ce_2: 0.3656  loss_mask_2: 0.4187  loss_dice_2: 3.172  loss_ce_3: 0.3587  loss_mask_3: 0.4135  loss_dice_3: 3.151  loss_ce_4: 0.3409  loss_mask_4: 0.4128  loss_dice_4: 3.152  loss_ce_5: 0.334  loss_mask_5: 0.4121  loss_dice_5: 3.163  loss_ce_6: 0.3174  loss_mask_6: 0.4132  loss_dice_6: 3.155  loss_ce_7: 0.3429  loss_mask_7: 0.4144  loss_dice_7: 3.15  loss_ce_8: 0.3352  loss_mask_8: 0.4131  loss_dice_8: 3.157  time: 1.5343  data_time: 0.0864  lr: 8.4929e-06  max_mem: 21410M
[01/17 10:21:49] d2.utils.events INFO:  eta: 1 day, 8:00:24  iter: 14959  total_loss: 38.47  loss_ce: 0.2992  loss_mask: 0.4209  loss_dice: 3.063  loss_ce_0: 0.5797  loss_mask_0: 0.4123  loss_dice_0: 3.181  loss_ce_1: 0.3281  loss_mask_1: 0.4284  loss_dice_1: 3.091  loss_ce_2: 0.3269  loss_mask_2: 0.4242  loss_dice_2: 3.076  loss_ce_3: 0.3131  loss_mask_3: 0.4258  loss_dice_3: 3.074  loss_ce_4: 0.2921  loss_mask_4: 0.4253  loss_dice_4: 3.07  loss_ce_5: 0.3035  loss_mask_5: 0.4233  loss_dice_5: 3.067  loss_ce_6: 0.3075  loss_mask_6: 0.4202  loss_dice_6: 3.07  loss_ce_7: 0.3146  loss_mask_7: 0.4192  loss_dice_7: 3.068  loss_ce_8: 0.3014  loss_mask_8: 0.4184  loss_dice_8: 3.068  time: 1.5344  data_time: 0.0983  lr: 8.4908e-06  max_mem: 21410M
[01/17 10:22:20] d2.utils.events INFO:  eta: 1 day, 8:01:08  iter: 14979  total_loss: 39.12  loss_ce: 0.3027  loss_mask: 0.4156  loss_dice: 3.152  loss_ce_0: 0.5853  loss_mask_0: 0.4159  loss_dice_0: 3.283  loss_ce_1: 0.302  loss_mask_1: 0.4226  loss_dice_1: 3.199  loss_ce_2: 0.3201  loss_mask_2: 0.4192  loss_dice_2: 3.161  loss_ce_3: 0.3077  loss_mask_3: 0.4194  loss_dice_3: 3.156  loss_ce_4: 0.3197  loss_mask_4: 0.4174  loss_dice_4: 3.157  loss_ce_5: 0.303  loss_mask_5: 0.417  loss_dice_5: 3.159  loss_ce_6: 0.3025  loss_mask_6: 0.4169  loss_dice_6: 3.158  loss_ce_7: 0.3105  loss_mask_7: 0.418  loss_dice_7: 3.152  loss_ce_8: 0.3076  loss_mask_8: 0.4176  loss_dice_8: 3.152  time: 1.5344  data_time: 0.0979  lr: 8.4888e-06  max_mem: 21410M
[01/17 10:22:51] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla/model_0014999.pth
[01/17 10:22:53] d2.utils.events INFO:  eta: 1 day, 8:01:57  iter: 14999  total_loss: 39.91  loss_ce: 0.3244  loss_mask: 0.4059  loss_dice: 3.204  loss_ce_0: 0.6232  loss_mask_0: 0.4147  loss_dice_0: 3.326  loss_ce_1: 0.3374  loss_mask_1: 0.4137  loss_dice_1: 3.242  loss_ce_2: 0.3416  loss_mask_2: 0.4049  loss_dice_2: 3.222  loss_ce_3: 0.3298  loss_mask_3: 0.4051  loss_dice_3: 3.205  loss_ce_4: 0.3227  loss_mask_4: 0.4048  loss_dice_4: 3.208  loss_ce_5: 0.3342  loss_mask_5: 0.4043  loss_dice_5: 3.2  loss_ce_6: 0.3239  loss_mask_6: 0.4036  loss_dice_6: 3.203  loss_ce_7: 0.3295  loss_mask_7: 0.4049  loss_dice_7: 3.196  loss_ce_8: 0.3173  loss_mask_8: 0.4041  loss_dice_8: 3.202  time: 1.5344  data_time: 0.1018  lr: 8.4868e-06  max_mem: 21410M
[01/17 10:23:23] d2.utils.events INFO:  eta: 1 day, 8:00:45  iter: 15019  total_loss: 38.91  loss_ce: 0.313  loss_mask: 0.4079  loss_dice: 3.111  loss_ce_0: 0.6017  loss_mask_0: 0.4006  loss_dice_0: 3.226  loss_ce_1: 0.3272  loss_mask_1: 0.41  loss_dice_1: 3.148  loss_ce_2: 0.3489  loss_mask_2: 0.4079  loss_dice_2: 3.123  loss_ce_3: 0.307  loss_mask_3: 0.4067  loss_dice_3: 3.112  loss_ce_4: 0.3031  loss_mask_4: 0.4089  loss_dice_4: 3.119  loss_ce_5: 0.3143  loss_mask_5: 0.4074  loss_dice_5: 3.112  loss_ce_6: 0.3162  loss_mask_6: 0.4072  loss_dice_6: 3.111  loss_ce_7: 0.3074  loss_mask_7: 0.4076  loss_dice_7: 3.116  loss_ce_8: 0.3052  loss_mask_8: 0.4079  loss_dice_8: 3.112  time: 1.5344  data_time: 0.0927  lr: 8.4847e-06  max_mem: 21410M
[01/17 10:23:55] d2.utils.events INFO:  eta: 1 day, 8:00:15  iter: 15039  total_loss: 38.85  loss_ce: 0.2992  loss_mask: 0.407  loss_dice: 3.108  loss_ce_0: 0.5899  loss_mask_0: 0.3967  loss_dice_0: 3.253  loss_ce_1: 0.3239  loss_mask_1: 0.4098  loss_dice_1: 3.149  loss_ce_2: 0.3466  loss_mask_2: 0.4061  loss_dice_2: 3.12  loss_ce_3: 0.3297  loss_mask_3: 0.408  loss_dice_3: 3.115  loss_ce_4: 0.313  loss_mask_4: 0.4083  loss_dice_4: 3.119  loss_ce_5: 0.3108  loss_mask_5: 0.4072  loss_dice_5: 3.123  loss_ce_6: 0.313  loss_mask_6: 0.4075  loss_dice_6: 3.114  loss_ce_7: 0.3013  loss_mask_7: 0.4068  loss_dice_7: 3.115  loss_ce_8: 0.3148  loss_mask_8: 0.4057  loss_dice_8: 3.115  time: 1.5345  data_time: 0.0985  lr: 8.4827e-06  max_mem: 21410M
[01/17 10:24:25] d2.utils.events INFO:  eta: 1 day, 8:00:59  iter: 15059  total_loss: 39.18  loss_ce: 0.3303  loss_mask: 0.4212  loss_dice: 3.102  loss_ce_0: 0.5743  loss_mask_0: 0.4134  loss_dice_0: 3.197  loss_ce_1: 0.3317  loss_mask_1: 0.4218  loss_dice_1: 3.128  loss_ce_2: 0.3358  loss_mask_2: 0.4218  loss_dice_2: 3.114  loss_ce_3: 0.3594  loss_mask_3: 0.4226  loss_dice_3: 3.098  loss_ce_4: 0.3213  loss_mask_4: 0.4217  loss_dice_4: 3.104  loss_ce_5: 0.3104  loss_mask_5: 0.4216  loss_dice_5: 3.1  loss_ce_6: 0.3219  loss_mask_6: 0.4216  loss_dice_6: 3.102  loss_ce_7: 0.3189  loss_mask_7: 0.425  loss_dice_7: 3.1  loss_ce_8: 0.314  loss_mask_8: 0.4203  loss_dice_8: 3.101  time: 1.5345  data_time: 0.0947  lr: 8.4807e-06  max_mem: 21410M
[01/17 10:24:57] d2.utils.events INFO:  eta: 1 day, 8:00:58  iter: 15079  total_loss: 39.36  loss_ce: 0.3318  loss_mask: 0.4177  loss_dice: 3.126  loss_ce_0: 0.5843  loss_mask_0: 0.4065  loss_dice_0: 3.247  loss_ce_1: 0.3292  loss_mask_1: 0.4132  loss_dice_1: 3.178  loss_ce_2: 0.3382  loss_mask_2: 0.4136  loss_dice_2: 3.152  loss_ce_3: 0.3332  loss_mask_3: 0.4151  loss_dice_3: 3.148  loss_ce_4: 0.322  loss_mask_4: 0.4174  loss_dice_4: 3.14  loss_ce_5: 0.321  loss_mask_5: 0.4164  loss_dice_5: 3.141  loss_ce_6: 0.3369  loss_mask_6: 0.4168  loss_dice_6: 3.13  loss_ce_7: 0.3181  loss_mask_7: 0.4158  loss_dice_7: 3.132  loss_ce_8: 0.3333  loss_mask_8: 0.4172  loss_dice_8: 3.123  time: 1.5345  data_time: 0.1011  lr: 8.4786e-06  max_mem: 21410M
[01/17 10:25:27] d2.utils.events INFO:  eta: 1 day, 8:00:12  iter: 15099  total_loss: 38.91  loss_ce: 0.3266  loss_mask: 0.4194  loss_dice: 3.103  loss_ce_0: 0.5638  loss_mask_0: 0.4187  loss_dice_0: 3.217  loss_ce_1: 0.3234  loss_mask_1: 0.4294  loss_dice_1: 3.128  loss_ce_2: 0.3331  loss_mask_2: 0.4247  loss_dice_2: 3.119  loss_ce_3: 0.3242  loss_mask_3: 0.4202  loss_dice_3: 3.11  loss_ce_4: 0.3302  loss_mask_4: 0.4189  loss_dice_4: 3.101  loss_ce_5: 0.3185  loss_mask_5: 0.4173  loss_dice_5: 3.11  loss_ce_6: 0.3188  loss_mask_6: 0.4184  loss_dice_6: 3.1  loss_ce_7: 0.3237  loss_mask_7: 0.418  loss_dice_7: 3.1  loss_ce_8: 0.3332  loss_mask_8: 0.42  loss_dice_8: 3.098  time: 1.5345  data_time: 0.0997  lr: 8.4766e-06  max_mem: 21410M
[01/17 10:25:58] d2.utils.events INFO:  eta: 1 day, 7:59:27  iter: 15119  total_loss: 38.12  loss_ce: 0.2847  loss_mask: 0.4117  loss_dice: 3.018  loss_ce_0: 0.5372  loss_mask_0: 0.4013  loss_dice_0: 3.15  loss_ce_1: 0.3131  loss_mask_1: 0.4159  loss_dice_1: 3.05  loss_ce_2: 0.3008  loss_mask_2: 0.41  loss_dice_2: 3.031  loss_ce_3: 0.3003  loss_mask_3: 0.4107  loss_dice_3: 3.015  loss_ce_4: 0.2972  loss_mask_4: 0.4114  loss_dice_4: 3.023  loss_ce_5: 0.2998  loss_mask_5: 0.4125  loss_dice_5: 3.023  loss_ce_6: 0.291  loss_mask_6: 0.4107  loss_dice_6: 3.02  loss_ce_7: 0.3  loss_mask_7: 0.4125  loss_dice_7: 3.021  loss_ce_8: 0.3062  loss_mask_8: 0.413  loss_dice_8: 3.02  time: 1.5345  data_time: 0.0927  lr: 8.4745e-06  max_mem: 21410M
[01/17 10:26:29] d2.utils.events INFO:  eta: 1 day, 7:58:49  iter: 15139  total_loss: 39.05  loss_ce: 0.3208  loss_mask: 0.424  loss_dice: 3.095  loss_ce_0: 0.5879  loss_mask_0: 0.419  loss_dice_0: 3.202  loss_ce_1: 0.3333  loss_mask_1: 0.427  loss_dice_1: 3.123  loss_ce_2: 0.3257  loss_mask_2: 0.4226  loss_dice_2: 3.116  loss_ce_3: 0.3189  loss_mask_3: 0.4254  loss_dice_3: 3.1  loss_ce_4: 0.3255  loss_mask_4: 0.4265  loss_dice_4: 3.097  loss_ce_5: 0.3097  loss_mask_5: 0.4284  loss_dice_5: 3.094  loss_ce_6: 0.3098  loss_mask_6: 0.4288  loss_dice_6: 3.087  loss_ce_7: 0.31  loss_mask_7: 0.428  loss_dice_7: 3.095  loss_ce_8: 0.3202  loss_mask_8: 0.4257  loss_dice_8: 3.098  time: 1.5345  data_time: 0.1063  lr: 8.4725e-06  max_mem: 21410M
[01/17 10:26:59] d2.utils.events INFO:  eta: 1 day, 7:57:41  iter: 15159  total_loss: 39.06  loss_ce: 0.3228  loss_mask: 0.4254  loss_dice: 3.088  loss_ce_0: 0.5773  loss_mask_0: 0.4208  loss_dice_0: 3.21  loss_ce_1: 0.3399  loss_mask_1: 0.429  loss_dice_1: 3.133  loss_ce_2: 0.3512  loss_mask_2: 0.4255  loss_dice_2: 3.106  loss_ce_3: 0.3298  loss_mask_3: 0.4262  loss_dice_3: 3.098  loss_ce_4: 0.3315  loss_mask_4: 0.4234  loss_dice_4: 3.099  loss_ce_5: 0.3143  loss_mask_5: 0.4233  loss_dice_5: 3.095  loss_ce_6: 0.3058  loss_mask_6: 0.4238  loss_dice_6: 3.092  loss_ce_7: 0.3182  loss_mask_7: 0.4233  loss_dice_7: 3.087  loss_ce_8: 0.3115  loss_mask_8: 0.4248  loss_dice_8: 3.09  time: 1.5345  data_time: 0.0996  lr: 8.4705e-06  max_mem: 21410M
[01/17 10:27:30] d2.utils.events INFO:  eta: 1 day, 7:56:53  iter: 15179  total_loss: 38.79  loss_ce: 0.3479  loss_mask: 0.4105  loss_dice: 3.078  loss_ce_0: 0.5937  loss_mask_0: 0.3979  loss_dice_0: 3.218  loss_ce_1: 0.3486  loss_mask_1: 0.4138  loss_dice_1: 3.12  loss_ce_2: 0.3573  loss_mask_2: 0.4113  loss_dice_2: 3.101  loss_ce_3: 0.3461  loss_mask_3: 0.4119  loss_dice_3: 3.087  loss_ce_4: 0.3387  loss_mask_4: 0.4105  loss_dice_4: 3.08  loss_ce_5: 0.3186  loss_mask_5: 0.4095  loss_dice_5: 3.089  loss_ce_6: 0.3155  loss_mask_6: 0.4111  loss_dice_6: 3.083  loss_ce_7: 0.3222  loss_mask_7: 0.4108  loss_dice_7: 3.076  loss_ce_8: 0.317  loss_mask_8: 0.4116  loss_dice_8: 3.075  time: 1.5345  data_time: 0.0941  lr: 8.4684e-06  max_mem: 21410M
[01/17 10:28:00] d2.utils.events INFO:  eta: 1 day, 7:56:14  iter: 15199  total_loss: 38.55  loss_ce: 0.3011  loss_mask: 0.4159  loss_dice: 3.077  loss_ce_0: 0.5896  loss_mask_0: 0.4059  loss_dice_0: 3.194  loss_ce_1: 0.3366  loss_mask_1: 0.4179  loss_dice_1: 3.105  loss_ce_2: 0.3304  loss_mask_2: 0.4181  loss_dice_2: 3.077  loss_ce_3: 0.3074  loss_mask_3: 0.4176  loss_dice_3: 3.067  loss_ce_4: 0.3085  loss_mask_4: 0.4171  loss_dice_4: 3.066  loss_ce_5: 0.2939  loss_mask_5: 0.4155  loss_dice_5: 3.081  loss_ce_6: 0.3075  loss_mask_6: 0.4158  loss_dice_6: 3.068  loss_ce_7: 0.297  loss_mask_7: 0.4152  loss_dice_7: 3.077  loss_ce_8: 0.3168  loss_mask_8: 0.4158  loss_dice_8: 3.076  time: 1.5344  data_time: 0.0913  lr: 8.4664e-06  max_mem: 21410M
[01/17 10:28:31] d2.utils.events INFO:  eta: 1 day, 7:54:59  iter: 15219  total_loss: 38.81  loss_ce: 0.2784  loss_mask: 0.4168  loss_dice: 3.121  loss_ce_0: 0.5782  loss_mask_0: 0.4075  loss_dice_0: 3.243  loss_ce_1: 0.323  loss_mask_1: 0.4164  loss_dice_1: 3.161  loss_ce_2: 0.3134  loss_mask_2: 0.4122  loss_dice_2: 3.133  loss_ce_3: 0.3074  loss_mask_3: 0.4129  loss_dice_3: 3.125  loss_ce_4: 0.2903  loss_mask_4: 0.4148  loss_dice_4: 3.116  loss_ce_5: 0.2798  loss_mask_5: 0.4172  loss_dice_5: 3.121  loss_ce_6: 0.2793  loss_mask_6: 0.4179  loss_dice_6: 3.117  loss_ce_7: 0.3035  loss_mask_7: 0.4158  loss_dice_7: 3.114  loss_ce_8: 0.288  loss_mask_8: 0.4168  loss_dice_8: 3.117  time: 1.5344  data_time: 0.0847  lr: 8.4644e-06  max_mem: 21410M
[01/17 10:29:01] d2.utils.events INFO:  eta: 1 day, 7:54:53  iter: 15239  total_loss: 37.67  loss_ce: 0.2824  loss_mask: 0.4224  loss_dice: 3.032  loss_ce_0: 0.5504  loss_mask_0: 0.4108  loss_dice_0: 3.152  loss_ce_1: 0.297  loss_mask_1: 0.4266  loss_dice_1: 3.073  loss_ce_2: 0.3102  loss_mask_2: 0.4237  loss_dice_2: 3.049  loss_ce_3: 0.3003  loss_mask_3: 0.4244  loss_dice_3: 3.036  loss_ce_4: 0.2902  loss_mask_4: 0.424  loss_dice_4: 3.039  loss_ce_5: 0.2818  loss_mask_5: 0.4242  loss_dice_5: 3.034  loss_ce_6: 0.2841  loss_mask_6: 0.423  loss_dice_6: 3.04  loss_ce_7: 0.294  loss_mask_7: 0.4217  loss_dice_7: 3.032  loss_ce_8: 0.2816  loss_mask_8: 0.4212  loss_dice_8: 3.032  time: 1.5344  data_time: 0.0998  lr: 8.4623e-06  max_mem: 21410M
[01/17 10:29:33] d2.utils.events INFO:  eta: 1 day, 7:55:04  iter: 15259  total_loss: 39.01  loss_ce: 0.2979  loss_mask: 0.4138  loss_dice: 3.15  loss_ce_0: 0.591  loss_mask_0: 0.4059  loss_dice_0: 3.269  loss_ce_1: 0.3276  loss_mask_1: 0.418  loss_dice_1: 3.186  loss_ce_2: 0.3313  loss_mask_2: 0.4163  loss_dice_2: 3.167  loss_ce_3: 0.3167  loss_mask_3: 0.4156  loss_dice_3: 3.145  loss_ce_4: 0.3322  loss_mask_4: 0.413  loss_dice_4: 3.154  loss_ce_5: 0.3119  loss_mask_5: 0.4125  loss_dice_5: 3.149  loss_ce_6: 0.3008  loss_mask_6: 0.4117  loss_dice_6: 3.149  loss_ce_7: 0.2926  loss_mask_7: 0.4117  loss_dice_7: 3.152  loss_ce_8: 0.2922  loss_mask_8: 0.4144  loss_dice_8: 3.147  time: 1.5345  data_time: 0.0984  lr: 8.4603e-06  max_mem: 21410M
[01/17 10:30:04] d2.utils.events INFO:  eta: 1 day, 7:54:19  iter: 15279  total_loss: 38.5  loss_ce: 0.317  loss_mask: 0.4178  loss_dice: 3.071  loss_ce_0: 0.5948  loss_mask_0: 0.4083  loss_dice_0: 3.194  loss_ce_1: 0.3377  loss_mask_1: 0.423  loss_dice_1: 3.107  loss_ce_2: 0.3208  loss_mask_2: 0.4227  loss_dice_2: 3.09  loss_ce_3: 0.3183  loss_mask_3: 0.4203  loss_dice_3: 3.082  loss_ce_4: 0.3016  loss_mask_4: 0.4195  loss_dice_4: 3.08  loss_ce_5: 0.3087  loss_mask_5: 0.4194  loss_dice_5: 3.082  loss_ce_6: 0.3153  loss_mask_6: 0.4184  loss_dice_6: 3.078  loss_ce_7: 0.3118  loss_mask_7: 0.4174  loss_dice_7: 3.074  loss_ce_8: 0.3093  loss_mask_8: 0.4176  loss_dice_8: 3.079  time: 1.5345  data_time: 0.0879  lr: 8.4582e-06  max_mem: 21410M
[01/17 10:30:34] d2.utils.events INFO:  eta: 1 day, 7:54:18  iter: 15299  total_loss: 38.34  loss_ce: 0.3046  loss_mask: 0.4176  loss_dice: 3.048  loss_ce_0: 0.561  loss_mask_0: 0.4055  loss_dice_0: 3.168  loss_ce_1: 0.3175  loss_mask_1: 0.42  loss_dice_1: 3.082  loss_ce_2: 0.3367  loss_mask_2: 0.4171  loss_dice_2: 3.063  loss_ce_3: 0.2995  loss_mask_3: 0.4169  loss_dice_3: 3.05  loss_ce_4: 0.3102  loss_mask_4: 0.417  loss_dice_4: 3.048  loss_ce_5: 0.3099  loss_mask_5: 0.419  loss_dice_5: 3.05  loss_ce_6: 0.3035  loss_mask_6: 0.4192  loss_dice_6: 3.046  loss_ce_7: 0.2943  loss_mask_7: 0.4174  loss_dice_7: 3.041  loss_ce_8: 0.3074  loss_mask_8: 0.4195  loss_dice_8: 3.045  time: 1.5345  data_time: 0.1026  lr: 8.4562e-06  max_mem: 21410M
[01/17 10:31:06] d2.utils.events INFO:  eta: 1 day, 7:54:49  iter: 15319  total_loss: 38.46  loss_ce: 0.2902  loss_mask: 0.4196  loss_dice: 3.062  loss_ce_0: 0.581  loss_mask_0: 0.4159  loss_dice_0: 3.187  loss_ce_1: 0.3181  loss_mask_1: 0.4242  loss_dice_1: 3.103  loss_ce_2: 0.3221  loss_mask_2: 0.4213  loss_dice_2: 3.082  loss_ce_3: 0.2995  loss_mask_3: 0.42  loss_dice_3: 3.07  loss_ce_4: 0.3152  loss_mask_4: 0.4193  loss_dice_4: 3.065  loss_ce_5: 0.3  loss_mask_5: 0.4159  loss_dice_5: 3.073  loss_ce_6: 0.3045  loss_mask_6: 0.4137  loss_dice_6: 3.069  loss_ce_7: 0.2925  loss_mask_7: 0.4149  loss_dice_7: 3.077  loss_ce_8: 0.2969  loss_mask_8: 0.4172  loss_dice_8: 3.064  time: 1.5345  data_time: 0.0999  lr: 8.4542e-06  max_mem: 21410M
[01/17 10:31:37] d2.utils.events INFO:  eta: 1 day, 7:54:43  iter: 15339  total_loss: 38.54  loss_ce: 0.2703  loss_mask: 0.4096  loss_dice: 3.089  loss_ce_0: 0.5612  loss_mask_0: 0.3972  loss_dice_0: 3.23  loss_ce_1: 0.3126  loss_mask_1: 0.4096  loss_dice_1: 3.137  loss_ce_2: 0.3253  loss_mask_2: 0.4081  loss_dice_2: 3.117  loss_ce_3: 0.3113  loss_mask_3: 0.4078  loss_dice_3: 3.104  loss_ce_4: 0.3123  loss_mask_4: 0.407  loss_dice_4: 3.092  loss_ce_5: 0.2825  loss_mask_5: 0.4089  loss_dice_5: 3.102  loss_ce_6: 0.3017  loss_mask_6: 0.4097  loss_dice_6: 3.1  loss_ce_7: 0.2774  loss_mask_7: 0.4073  loss_dice_7: 3.094  loss_ce_8: 0.2791  loss_mask_8: 0.4095  loss_dice_8: 3.093  time: 1.5345  data_time: 0.0953  lr: 8.4521e-06  max_mem: 21410M
[01/17 10:32:07] d2.utils.events INFO:  eta: 1 day, 7:54:33  iter: 15359  total_loss: 38.7  loss_ce: 0.3084  loss_mask: 0.4298  loss_dice: 3.079  loss_ce_0: 0.5854  loss_mask_0: 0.4206  loss_dice_0: 3.197  loss_ce_1: 0.345  loss_mask_1: 0.4304  loss_dice_1: 3.112  loss_ce_2: 0.3338  loss_mask_2: 0.4279  loss_dice_2: 3.094  loss_ce_3: 0.3205  loss_mask_3: 0.4243  loss_dice_3: 3.088  loss_ce_4: 0.3222  loss_mask_4: 0.4238  loss_dice_4: 3.084  loss_ce_5: 0.3137  loss_mask_5: 0.4274  loss_dice_5: 3.086  loss_ce_6: 0.3107  loss_mask_6: 0.4276  loss_dice_6: 3.078  loss_ce_7: 0.3139  loss_mask_7: 0.4266  loss_dice_7: 3.076  loss_ce_8: 0.318  loss_mask_8: 0.4277  loss_dice_8: 3.084  time: 1.5345  data_time: 0.0913  lr: 8.4501e-06  max_mem: 21410M
[01/17 10:32:38] d2.utils.events INFO:  eta: 1 day, 7:53:22  iter: 15379  total_loss: 38.54  loss_ce: 0.2945  loss_mask: 0.4166  loss_dice: 3.064  loss_ce_0: 0.5779  loss_mask_0: 0.4173  loss_dice_0: 3.178  loss_ce_1: 0.3192  loss_mask_1: 0.4253  loss_dice_1: 3.109  loss_ce_2: 0.3243  loss_mask_2: 0.4219  loss_dice_2: 3.092  loss_ce_3: 0.2967  loss_mask_3: 0.4218  loss_dice_3: 3.076  loss_ce_4: 0.3023  loss_mask_4: 0.42  loss_dice_4: 3.074  loss_ce_5: 0.3009  loss_mask_5: 0.4188  loss_dice_5: 3.085  loss_ce_6: 0.294  loss_mask_6: 0.419  loss_dice_6: 3.063  loss_ce_7: 0.2901  loss_mask_7: 0.4182  loss_dice_7: 3.078  loss_ce_8: 0.2921  loss_mask_8: 0.4165  loss_dice_8: 3.069  time: 1.5345  data_time: 0.0976  lr: 8.4481e-06  max_mem: 21410M
[01/17 10:33:09] d2.utils.events INFO:  eta: 1 day, 7:52:45  iter: 15399  total_loss: 38.52  loss_ce: 0.2933  loss_mask: 0.408  loss_dice: 3.089  loss_ce_0: 0.5854  loss_mask_0: 0.3955  loss_dice_0: 3.226  loss_ce_1: 0.3026  loss_mask_1: 0.405  loss_dice_1: 3.137  loss_ce_2: 0.3093  loss_mask_2: 0.4031  loss_dice_2: 3.104  loss_ce_3: 0.3028  loss_mask_3: 0.4086  loss_dice_3: 3.098  loss_ce_4: 0.3025  loss_mask_4: 0.4058  loss_dice_4: 3.101  loss_ce_5: 0.298  loss_mask_5: 0.407  loss_dice_5: 3.099  loss_ce_6: 0.2998  loss_mask_6: 0.4066  loss_dice_6: 3.093  loss_ce_7: 0.2966  loss_mask_7: 0.4071  loss_dice_7: 3.09  loss_ce_8: 0.3037  loss_mask_8: 0.4065  loss_dice_8: 3.098  time: 1.5345  data_time: 0.0947  lr: 8.446e-06  max_mem: 21410M
[01/17 10:33:40] d2.utils.events INFO:  eta: 1 day, 7:51:38  iter: 15419  total_loss: 37.96  loss_ce: 0.2859  loss_mask: 0.4097  loss_dice: 3.057  loss_ce_0: 0.5454  loss_mask_0: 0.4034  loss_dice_0: 3.171  loss_ce_1: 0.3312  loss_mask_1: 0.4075  loss_dice_1: 3.088  loss_ce_2: 0.3199  loss_mask_2: 0.4094  loss_dice_2: 3.075  loss_ce_3: 0.2977  loss_mask_3: 0.4079  loss_dice_3: 3.066  loss_ce_4: 0.3058  loss_mask_4: 0.4057  loss_dice_4: 3.069  loss_ce_5: 0.2991  loss_mask_5: 0.4064  loss_dice_5: 3.065  loss_ce_6: 0.3  loss_mask_6: 0.4075  loss_dice_6: 3.065  loss_ce_7: 0.3069  loss_mask_7: 0.409  loss_dice_7: 3.06  loss_ce_8: 0.3032  loss_mask_8: 0.4069  loss_dice_8: 3.059  time: 1.5345  data_time: 0.0963  lr: 8.444e-06  max_mem: 21410M
[01/17 10:34:10] d2.utils.events INFO:  eta: 1 day, 7:50:34  iter: 15439  total_loss: 38.98  loss_ce: 0.3012  loss_mask: 0.4219  loss_dice: 3.122  loss_ce_0: 0.5766  loss_mask_0: 0.4143  loss_dice_0: 3.237  loss_ce_1: 0.3555  loss_mask_1: 0.4313  loss_dice_1: 3.139  loss_ce_2: 0.345  loss_mask_2: 0.4274  loss_dice_2: 3.126  loss_ce_3: 0.3206  loss_mask_3: 0.4281  loss_dice_3: 3.115  loss_ce_4: 0.3162  loss_mask_4: 0.4268  loss_dice_4: 3.119  loss_ce_5: 0.3006  loss_mask_5: 0.4251  loss_dice_5: 3.119  loss_ce_6: 0.3001  loss_mask_6: 0.4237  loss_dice_6: 3.125  loss_ce_7: 0.3132  loss_mask_7: 0.4218  loss_dice_7: 3.125  loss_ce_8: 0.2965  loss_mask_8: 0.4215  loss_dice_8: 3.121  time: 1.5345  data_time: 0.1016  lr: 8.4419e-06  max_mem: 21410M
[01/17 10:34:42] d2.utils.events INFO:  eta: 1 day, 7:50:03  iter: 15459  total_loss: 38.39  loss_ce: 0.3202  loss_mask: 0.4059  loss_dice: 3.074  loss_ce_0: 0.5845  loss_mask_0: 0.4004  loss_dice_0: 3.208  loss_ce_1: 0.3179  loss_mask_1: 0.4064  loss_dice_1: 3.112  loss_ce_2: 0.3357  loss_mask_2: 0.4022  loss_dice_2: 3.095  loss_ce_3: 0.3225  loss_mask_3: 0.4036  loss_dice_3: 3.089  loss_ce_4: 0.3209  loss_mask_4: 0.404  loss_dice_4: 3.09  loss_ce_5: 0.3129  loss_mask_5: 0.4044  loss_dice_5: 3.094  loss_ce_6: 0.3215  loss_mask_6: 0.4074  loss_dice_6: 3.082  loss_ce_7: 0.305  loss_mask_7: 0.4073  loss_dice_7: 3.089  loss_ce_8: 0.3191  loss_mask_8: 0.4052  loss_dice_8: 3.082  time: 1.5345  data_time: 0.0954  lr: 8.4399e-06  max_mem: 21410M
[01/17 10:35:12] d2.utils.events INFO:  eta: 1 day, 7:48:45  iter: 15479  total_loss: 38.17  loss_ce: 0.3086  loss_mask: 0.4061  loss_dice: 3.031  loss_ce_0: 0.5895  loss_mask_0: 0.3958  loss_dice_0: 3.154  loss_ce_1: 0.3277  loss_mask_1: 0.4097  loss_dice_1: 3.067  loss_ce_2: 0.3325  loss_mask_2: 0.406  loss_dice_2: 3.036  loss_ce_3: 0.3118  loss_mask_3: 0.4067  loss_dice_3: 3.022  loss_ce_4: 0.3044  loss_mask_4: 0.4076  loss_dice_4: 3.024  loss_ce_5: 0.2996  loss_mask_5: 0.4091  loss_dice_5: 3.035  loss_ce_6: 0.3091  loss_mask_6: 0.4067  loss_dice_6: 3.026  loss_ce_7: 0.285  loss_mask_7: 0.4064  loss_dice_7: 3.025  loss_ce_8: 0.2952  loss_mask_8: 0.4077  loss_dice_8: 3.023  time: 1.5345  data_time: 0.1025  lr: 8.4379e-06  max_mem: 21410M
[01/17 10:35:43] d2.utils.events INFO:  eta: 1 day, 7:48:33  iter: 15499  total_loss: 38.06  loss_ce: 0.2968  loss_mask: 0.3987  loss_dice: 3.033  loss_ce_0: 0.5966  loss_mask_0: 0.3965  loss_dice_0: 3.167  loss_ce_1: 0.3433  loss_mask_1: 0.4023  loss_dice_1: 3.071  loss_ce_2: 0.3405  loss_mask_2: 0.4001  loss_dice_2: 3.051  loss_ce_3: 0.3297  loss_mask_3: 0.3985  loss_dice_3: 3.035  loss_ce_4: 0.323  loss_mask_4: 0.3998  loss_dice_4: 3.036  loss_ce_5: 0.321  loss_mask_5: 0.397  loss_dice_5: 3.038  loss_ce_6: 0.3099  loss_mask_6: 0.3968  loss_dice_6: 3.028  loss_ce_7: 0.3089  loss_mask_7: 0.3988  loss_dice_7: 3.025  loss_ce_8: 0.312  loss_mask_8: 0.3986  loss_dice_8: 3.025  time: 1.5346  data_time: 0.0937  lr: 8.4358e-06  max_mem: 21410M
[01/17 10:36:14] d2.utils.events INFO:  eta: 1 day, 7:48:16  iter: 15519  total_loss: 38.59  loss_ce: 0.2995  loss_mask: 0.4102  loss_dice: 3.072  loss_ce_0: 0.5689  loss_mask_0: 0.3956  loss_dice_0: 3.211  loss_ce_1: 0.3163  loss_mask_1: 0.4138  loss_dice_1: 3.119  loss_ce_2: 0.3143  loss_mask_2: 0.4086  loss_dice_2: 3.091  loss_ce_3: 0.3149  loss_mask_3: 0.4043  loss_dice_3: 3.085  loss_ce_4: 0.3379  loss_mask_4: 0.4045  loss_dice_4: 3.08  loss_ce_5: 0.3008  loss_mask_5: 0.4062  loss_dice_5: 3.078  loss_ce_6: 0.3128  loss_mask_6: 0.409  loss_dice_6: 3.071  loss_ce_7: 0.2903  loss_mask_7: 0.4064  loss_dice_7: 3.075  loss_ce_8: 0.2969  loss_mask_8: 0.4083  loss_dice_8: 3.081  time: 1.5346  data_time: 0.0926  lr: 8.4338e-06  max_mem: 21410M
[01/17 10:36:45] d2.utils.events INFO:  eta: 1 day, 7:46:55  iter: 15539  total_loss: 38.26  loss_ce: 0.2787  loss_mask: 0.4081  loss_dice: 3.09  loss_ce_0: 0.6  loss_mask_0: 0.4109  loss_dice_0: 3.201  loss_ce_1: 0.2918  loss_mask_1: 0.4242  loss_dice_1: 3.124  loss_ce_2: 0.3015  loss_mask_2: 0.4164  loss_dice_2: 3.104  loss_ce_3: 0.2832  loss_mask_3: 0.4133  loss_dice_3: 3.097  loss_ce_4: 0.2776  loss_mask_4: 0.4142  loss_dice_4: 3.095  loss_ce_5: 0.2702  loss_mask_5: 0.4104  loss_dice_5: 3.093  loss_ce_6: 0.282  loss_mask_6: 0.4102  loss_dice_6: 3.084  loss_ce_7: 0.2897  loss_mask_7: 0.4104  loss_dice_7: 3.09  loss_ce_8: 0.2756  loss_mask_8: 0.4091  loss_dice_8: 3.086  time: 1.5346  data_time: 0.0918  lr: 8.4317e-06  max_mem: 21410M
[01/17 10:37:15] d2.utils.events INFO:  eta: 1 day, 7:46:55  iter: 15559  total_loss: 38.23  loss_ce: 0.3065  loss_mask: 0.4015  loss_dice: 3.029  loss_ce_0: 0.5667  loss_mask_0: 0.3941  loss_dice_0: 3.16  loss_ce_1: 0.3468  loss_mask_1: 0.4069  loss_dice_1: 3.079  loss_ce_2: 0.348  loss_mask_2: 0.4034  loss_dice_2: 3.046  loss_ce_3: 0.338  loss_mask_3: 0.4004  loss_dice_3: 3.035  loss_ce_4: 0.3298  loss_mask_4: 0.3987  loss_dice_4: 3.039  loss_ce_5: 0.3212  loss_mask_5: 0.3992  loss_dice_5: 3.037  loss_ce_6: 0.3161  loss_mask_6: 0.4005  loss_dice_6: 3.027  loss_ce_7: 0.3045  loss_mask_7: 0.3995  loss_dice_7: 3.038  loss_ce_8: 0.3096  loss_mask_8: 0.4014  loss_dice_8: 3.025  time: 1.5346  data_time: 0.1037  lr: 8.4297e-06  max_mem: 21410M
[01/17 10:37:46] d2.utils.events INFO:  eta: 1 day, 7:46:59  iter: 15579  total_loss: 37.92  loss_ce: 0.2909  loss_mask: 0.413  loss_dice: 3.011  loss_ce_0: 0.5714  loss_mask_0: 0.3978  loss_dice_0: 3.158  loss_ce_1: 0.324  loss_mask_1: 0.4143  loss_dice_1: 3.056  loss_ce_2: 0.3349  loss_mask_2: 0.4118  loss_dice_2: 3.033  loss_ce_3: 0.3238  loss_mask_3: 0.4129  loss_dice_3: 3.013  loss_ce_4: 0.3291  loss_mask_4: 0.4127  loss_dice_4: 3.027  loss_ce_5: 0.3091  loss_mask_5: 0.4121  loss_dice_5: 3.012  loss_ce_6: 0.3047  loss_mask_6: 0.4121  loss_dice_6: 3.023  loss_ce_7: 0.2813  loss_mask_7: 0.412  loss_dice_7: 3.019  loss_ce_8: 0.3019  loss_mask_8: 0.4135  loss_dice_8: 3.011  time: 1.5346  data_time: 0.0811  lr: 8.4277e-06  max_mem: 21410M
[01/17 10:38:17] d2.utils.events INFO:  eta: 1 day, 7:45:01  iter: 15599  total_loss: 38.59  loss_ce: 0.3199  loss_mask: 0.4138  loss_dice: 3.069  loss_ce_0: 0.5968  loss_mask_0: 0.4028  loss_dice_0: 3.198  loss_ce_1: 0.3328  loss_mask_1: 0.4149  loss_dice_1: 3.117  loss_ce_2: 0.3461  loss_mask_2: 0.4107  loss_dice_2: 3.09  loss_ce_3: 0.3382  loss_mask_3: 0.4126  loss_dice_3: 3.086  loss_ce_4: 0.3194  loss_mask_4: 0.4128  loss_dice_4: 3.074  loss_ce_5: 0.3061  loss_mask_5: 0.4123  loss_dice_5: 3.079  loss_ce_6: 0.3062  loss_mask_6: 0.4141  loss_dice_6: 3.079  loss_ce_7: 0.3169  loss_mask_7: 0.4122  loss_dice_7: 3.076  loss_ce_8: 0.3121  loss_mask_8: 0.4126  loss_dice_8: 3.071  time: 1.5346  data_time: 0.1001  lr: 8.4256e-06  max_mem: 21410M
[01/17 10:38:48] d2.utils.events INFO:  eta: 1 day, 7:47:07  iter: 15619  total_loss: 38.68  loss_ce: 0.3081  loss_mask: 0.4143  loss_dice: 3.091  loss_ce_0: 0.5813  loss_mask_0: 0.4181  loss_dice_0: 3.213  loss_ce_1: 0.3383  loss_mask_1: 0.4247  loss_dice_1: 3.131  loss_ce_2: 0.352  loss_mask_2: 0.4161  loss_dice_2: 3.106  loss_ce_3: 0.3065  loss_mask_3: 0.4167  loss_dice_3: 3.095  loss_ce_4: 0.3107  loss_mask_4: 0.416  loss_dice_4: 3.093  loss_ce_5: 0.3076  loss_mask_5: 0.4151  loss_dice_5: 3.098  loss_ce_6: 0.2957  loss_mask_6: 0.4125  loss_dice_6: 3.092  loss_ce_7: 0.3133  loss_mask_7: 0.4145  loss_dice_7: 3.088  loss_ce_8: 0.2996  loss_mask_8: 0.4137  loss_dice_8: 3.093  time: 1.5346  data_time: 0.1052  lr: 8.4236e-06  max_mem: 21410M
[01/17 10:39:18] d2.utils.events INFO:  eta: 1 day, 7:45:36  iter: 15639  total_loss: 38.24  loss_ce: 0.3075  loss_mask: 0.403  loss_dice: 3.065  loss_ce_0: 0.5671  loss_mask_0: 0.3989  loss_dice_0: 3.195  loss_ce_1: 0.3217  loss_mask_1: 0.4122  loss_dice_1: 3.114  loss_ce_2: 0.3028  loss_mask_2: 0.4061  loss_dice_2: 3.071  loss_ce_3: 0.3066  loss_mask_3: 0.4081  loss_dice_3: 3.068  loss_ce_4: 0.2967  loss_mask_4: 0.4065  loss_dice_4: 3.074  loss_ce_5: 0.2967  loss_mask_5: 0.4042  loss_dice_5: 3.079  loss_ce_6: 0.3052  loss_mask_6: 0.4035  loss_dice_6: 3.063  loss_ce_7: 0.2962  loss_mask_7: 0.4061  loss_dice_7: 3.072  loss_ce_8: 0.2952  loss_mask_8: 0.4054  loss_dice_8: 3.071  time: 1.5345  data_time: 0.0954  lr: 8.4216e-06  max_mem: 21410M
[01/17 10:39:49] d2.utils.events INFO:  eta: 1 day, 7:43:57  iter: 15659  total_loss: 38.27  loss_ce: 0.31  loss_mask: 0.4054  loss_dice: 3.045  loss_ce_0: 0.5673  loss_mask_0: 0.3951  loss_dice_0: 3.175  loss_ce_1: 0.3354  loss_mask_1: 0.4113  loss_dice_1: 3.089  loss_ce_2: 0.3298  loss_mask_2: 0.4049  loss_dice_2: 3.067  loss_ce_3: 0.3078  loss_mask_3: 0.4004  loss_dice_3: 3.06  loss_ce_4: 0.3086  loss_mask_4: 0.4019  loss_dice_4: 3.053  loss_ce_5: 0.3228  loss_mask_5: 0.4033  loss_dice_5: 3.056  loss_ce_6: 0.3121  loss_mask_6: 0.404  loss_dice_6: 3.059  loss_ce_7: 0.3065  loss_mask_7: 0.4075  loss_dice_7: 3.045  loss_ce_8: 0.2963  loss_mask_8: 0.4054  loss_dice_8: 3.054  time: 1.5345  data_time: 0.0940  lr: 8.4195e-06  max_mem: 21410M
[01/17 10:40:19] d2.utils.events INFO:  eta: 1 day, 7:43:03  iter: 15679  total_loss: 37.56  loss_ce: 0.2815  loss_mask: 0.4031  loss_dice: 3.006  loss_ce_0: 0.5762  loss_mask_0: 0.3944  loss_dice_0: 3.15  loss_ce_1: 0.3074  loss_mask_1: 0.4042  loss_dice_1: 3.049  loss_ce_2: 0.2977  loss_mask_2: 0.4025  loss_dice_2: 3.023  loss_ce_3: 0.3017  loss_mask_3: 0.4031  loss_dice_3: 3.004  loss_ce_4: 0.2968  loss_mask_4: 0.4011  loss_dice_4: 3.004  loss_ce_5: 0.2788  loss_mask_5: 0.4023  loss_dice_5: 3.017  loss_ce_6: 0.2811  loss_mask_6: 0.4013  loss_dice_6: 2.999  loss_ce_7: 0.2691  loss_mask_7: 0.4015  loss_dice_7: 3.006  loss_ce_8: 0.2779  loss_mask_8: 0.401  loss_dice_8: 3.005  time: 1.5345  data_time: 0.0947  lr: 8.4175e-06  max_mem: 21410M
[01/17 10:40:50] d2.utils.events INFO:  eta: 1 day, 7:42:01  iter: 15699  total_loss: 37.62  loss_ce: 0.294  loss_mask: 0.4169  loss_dice: 3.02  loss_ce_0: 0.5648  loss_mask_0: 0.4053  loss_dice_0: 3.126  loss_ce_1: 0.3242  loss_mask_1: 0.4281  loss_dice_1: 3.047  loss_ce_2: 0.3255  loss_mask_2: 0.4249  loss_dice_2: 3.026  loss_ce_3: 0.3058  loss_mask_3: 0.4235  loss_dice_3: 3.018  loss_ce_4: 0.3079  loss_mask_4: 0.4237  loss_dice_4: 3.027  loss_ce_5: 0.3033  loss_mask_5: 0.4193  loss_dice_5: 3.026  loss_ce_6: 0.3059  loss_mask_6: 0.4182  loss_dice_6: 3.021  loss_ce_7: 0.2962  loss_mask_7: 0.4174  loss_dice_7: 3.024  loss_ce_8: 0.3055  loss_mask_8: 0.4179  loss_dice_8: 3.023  time: 1.5345  data_time: 0.0914  lr: 8.4154e-06  max_mem: 21410M
[01/17 10:41:20] d2.utils.events INFO:  eta: 1 day, 7:42:48  iter: 15719  total_loss: 38.89  loss_ce: 0.299  loss_mask: 0.423  loss_dice: 3.092  loss_ce_0: 0.5947  loss_mask_0: 0.4321  loss_dice_0: 3.217  loss_ce_1: 0.3142  loss_mask_1: 0.4335  loss_dice_1: 3.132  loss_ce_2: 0.3009  loss_mask_2: 0.4293  loss_dice_2: 3.112  loss_ce_3: 0.2917  loss_mask_3: 0.425  loss_dice_3: 3.096  loss_ce_4: 0.2812  loss_mask_4: 0.425  loss_dice_4: 3.108  loss_ce_5: 0.3009  loss_mask_5: 0.4257  loss_dice_5: 3.102  loss_ce_6: 0.2958  loss_mask_6: 0.4214  loss_dice_6: 3.105  loss_ce_7: 0.2869  loss_mask_7: 0.4213  loss_dice_7: 3.094  loss_ce_8: 0.2883  loss_mask_8: 0.4233  loss_dice_8: 3.094  time: 1.5345  data_time: 0.0929  lr: 8.4134e-06  max_mem: 21410M
[01/17 10:41:51] d2.utils.events INFO:  eta: 1 day, 7:42:52  iter: 15739  total_loss: 37.99  loss_ce: 0.3047  loss_mask: 0.4082  loss_dice: 3.002  loss_ce_0: 0.594  loss_mask_0: 0.3994  loss_dice_0: 3.129  loss_ce_1: 0.3192  loss_mask_1: 0.4124  loss_dice_1: 3.048  loss_ce_2: 0.3268  loss_mask_2: 0.4089  loss_dice_2: 3.025  loss_ce_3: 0.3213  loss_mask_3: 0.4087  loss_dice_3: 3.007  loss_ce_4: 0.3261  loss_mask_4: 0.4102  loss_dice_4: 3.013  loss_ce_5: 0.3073  loss_mask_5: 0.4093  loss_dice_5: 3.005  loss_ce_6: 0.3048  loss_mask_6: 0.4081  loss_dice_6: 3.005  loss_ce_7: 0.3076  loss_mask_7: 0.4087  loss_dice_7: 3.005  loss_ce_8: 0.309  loss_mask_8: 0.4091  loss_dice_8: 3.004  time: 1.5345  data_time: 0.0909  lr: 8.4114e-06  max_mem: 21410M
[01/17 10:42:22] d2.utils.events INFO:  eta: 1 day, 7:42:42  iter: 15759  total_loss: 37.53  loss_ce: 0.2963  loss_mask: 0.4094  loss_dice: 3.044  loss_ce_0: 0.5513  loss_mask_0: 0.4077  loss_dice_0: 3.172  loss_ce_1: 0.3241  loss_mask_1: 0.4115  loss_dice_1: 3.079  loss_ce_2: 0.3409  loss_mask_2: 0.4098  loss_dice_2: 3.063  loss_ce_3: 0.2989  loss_mask_3: 0.4082  loss_dice_3: 3.05  loss_ce_4: 0.3039  loss_mask_4: 0.4073  loss_dice_4: 3.044  loss_ce_5: 0.299  loss_mask_5: 0.4084  loss_dice_5: 3.052  loss_ce_6: 0.307  loss_mask_6: 0.41  loss_dice_6: 3.041  loss_ce_7: 0.3008  loss_mask_7: 0.4086  loss_dice_7: 3.041  loss_ce_8: 0.2934  loss_mask_8: 0.4109  loss_dice_8: 3.043  time: 1.5345  data_time: 0.0928  lr: 8.4093e-06  max_mem: 21410M
[01/17 10:42:53] d2.utils.events INFO:  eta: 1 day, 7:42:01  iter: 15779  total_loss: 38  loss_ce: 0.271  loss_mask: 0.4169  loss_dice: 3.041  loss_ce_0: 0.5372  loss_mask_0: 0.4078  loss_dice_0: 3.167  loss_ce_1: 0.3075  loss_mask_1: 0.4216  loss_dice_1: 3.07  loss_ce_2: 0.3098  loss_mask_2: 0.4172  loss_dice_2: 3.051  loss_ce_3: 0.2804  loss_mask_3: 0.4176  loss_dice_3: 3.04  loss_ce_4: 0.2828  loss_mask_4: 0.4188  loss_dice_4: 3.042  loss_ce_5: 0.278  loss_mask_5: 0.4187  loss_dice_5: 3.031  loss_ce_6: 0.2709  loss_mask_6: 0.4157  loss_dice_6: 3.046  loss_ce_7: 0.2776  loss_mask_7: 0.4171  loss_dice_7: 3.038  loss_ce_8: 0.2693  loss_mask_8: 0.4164  loss_dice_8: 3.026  time: 1.5345  data_time: 0.0904  lr: 8.4073e-06  max_mem: 21410M
[01/17 10:43:23] d2.utils.events INFO:  eta: 1 day, 7:40:59  iter: 15799  total_loss: 37.29  loss_ce: 0.2925  loss_mask: 0.4068  loss_dice: 2.964  loss_ce_0: 0.556  loss_mask_0: 0.3879  loss_dice_0: 3.08  loss_ce_1: 0.3151  loss_mask_1: 0.4085  loss_dice_1: 3  loss_ce_2: 0.3307  loss_mask_2: 0.4091  loss_dice_2: 2.971  loss_ce_3: 0.3072  loss_mask_3: 0.4083  loss_dice_3: 2.969  loss_ce_4: 0.2982  loss_mask_4: 0.4074  loss_dice_4: 2.965  loss_ce_5: 0.2945  loss_mask_5: 0.4081  loss_dice_5: 2.961  loss_ce_6: 0.2877  loss_mask_6: 0.4076  loss_dice_6: 2.961  loss_ce_7: 0.2935  loss_mask_7: 0.4089  loss_dice_7: 2.96  loss_ce_8: 0.2893  loss_mask_8: 0.4081  loss_dice_8: 2.955  time: 1.5345  data_time: 0.0864  lr: 8.4052e-06  max_mem: 21410M
[01/17 10:43:54] d2.utils.events INFO:  eta: 1 day, 7:38:12  iter: 15819  total_loss: 38.21  loss_ce: 0.2949  loss_mask: 0.4186  loss_dice: 3.043  loss_ce_0: 0.5763  loss_mask_0: 0.4051  loss_dice_0: 3.164  loss_ce_1: 0.3193  loss_mask_1: 0.4169  loss_dice_1: 3.09  loss_ce_2: 0.3458  loss_mask_2: 0.4169  loss_dice_2: 3.058  loss_ce_3: 0.3155  loss_mask_3: 0.4141  loss_dice_3: 3.048  loss_ce_4: 0.3136  loss_mask_4: 0.4171  loss_dice_4: 3.035  loss_ce_5: 0.294  loss_mask_5: 0.4158  loss_dice_5: 3.046  loss_ce_6: 0.3  loss_mask_6: 0.4161  loss_dice_6: 3.042  loss_ce_7: 0.3101  loss_mask_7: 0.4158  loss_dice_7: 3.042  loss_ce_8: 0.303  loss_mask_8: 0.4168  loss_dice_8: 3.038  time: 1.5345  data_time: 0.0944  lr: 8.4032e-06  max_mem: 21410M
[01/17 10:44:24] d2.utils.events INFO:  eta: 1 day, 7:36:40  iter: 15839  total_loss: 38.64  loss_ce: 0.2993  loss_mask: 0.3981  loss_dice: 3.077  loss_ce_0: 0.5877  loss_mask_0: 0.4016  loss_dice_0: 3.202  loss_ce_1: 0.321  loss_mask_1: 0.4091  loss_dice_1: 3.119  loss_ce_2: 0.3321  loss_mask_2: 0.4017  loss_dice_2: 3.102  loss_ce_3: 0.3125  loss_mask_3: 0.3989  loss_dice_3: 3.085  loss_ce_4: 0.3117  loss_mask_4: 0.3982  loss_dice_4: 3.082  loss_ce_5: 0.3099  loss_mask_5: 0.3993  loss_dice_5: 3.086  loss_ce_6: 0.3034  loss_mask_6: 0.4003  loss_dice_6: 3.077  loss_ce_7: 0.293  loss_mask_7: 0.3992  loss_dice_7: 3.084  loss_ce_8: 0.3113  loss_mask_8: 0.3969  loss_dice_8: 3.076  time: 1.5344  data_time: 0.0950  lr: 8.4012e-06  max_mem: 21410M
[01/17 10:44:55] d2.utils.events INFO:  eta: 1 day, 7:36:09  iter: 15859  total_loss: 37.84  loss_ce: 0.2812  loss_mask: 0.3984  loss_dice: 3.024  loss_ce_0: 0.5701  loss_mask_0: 0.393  loss_dice_0: 3.165  loss_ce_1: 0.3107  loss_mask_1: 0.403  loss_dice_1: 3.078  loss_ce_2: 0.3167  loss_mask_2: 0.4013  loss_dice_2: 3.051  loss_ce_3: 0.3007  loss_mask_3: 0.4002  loss_dice_3: 3.03  loss_ce_4: 0.2867  loss_mask_4: 0.3976  loss_dice_4: 3.032  loss_ce_5: 0.2794  loss_mask_5: 0.3991  loss_dice_5: 3.029  loss_ce_6: 0.2884  loss_mask_6: 0.3994  loss_dice_6: 3.027  loss_ce_7: 0.2929  loss_mask_7: 0.3988  loss_dice_7: 3.026  loss_ce_8: 0.2831  loss_mask_8: 0.3977  loss_dice_8: 3.03  time: 1.5344  data_time: 0.1048  lr: 8.3991e-06  max_mem: 21410M
[01/17 10:45:25] d2.utils.events INFO:  eta: 1 day, 7:35:12  iter: 15879  total_loss: 37.67  loss_ce: 0.281  loss_mask: 0.4117  loss_dice: 2.998  loss_ce_0: 0.5695  loss_mask_0: 0.4054  loss_dice_0: 3.115  loss_ce_1: 0.2996  loss_mask_1: 0.4221  loss_dice_1: 3.037  loss_ce_2: 0.3122  loss_mask_2: 0.4145  loss_dice_2: 3.01  loss_ce_3: 0.3133  loss_mask_3: 0.4123  loss_dice_3: 3.005  loss_ce_4: 0.2939  loss_mask_4: 0.4101  loss_dice_4: 3.002  loss_ce_5: 0.292  loss_mask_5: 0.4125  loss_dice_5: 3.004  loss_ce_6: 0.2959  loss_mask_6: 0.4132  loss_dice_6: 2.994  loss_ce_7: 0.2876  loss_mask_7: 0.4108  loss_dice_7: 2.991  loss_ce_8: 0.2818  loss_mask_8: 0.4127  loss_dice_8: 2.999  time: 1.5344  data_time: 0.0996  lr: 8.3971e-06  max_mem: 21410M
[01/17 10:45:55] d2.utils.events INFO:  eta: 1 day, 7:34:42  iter: 15899  total_loss: 38.39  loss_ce: 0.3064  loss_mask: 0.4022  loss_dice: 3.066  loss_ce_0: 0.5643  loss_mask_0: 0.4031  loss_dice_0: 3.163  loss_ce_1: 0.3256  loss_mask_1: 0.4143  loss_dice_1: 3.096  loss_ce_2: 0.333  loss_mask_2: 0.4088  loss_dice_2: 3.066  loss_ce_3: 0.3109  loss_mask_3: 0.4052  loss_dice_3: 3.064  loss_ce_4: 0.3093  loss_mask_4: 0.4034  loss_dice_4: 3.071  loss_ce_5: 0.3089  loss_mask_5: 0.4017  loss_dice_5: 3.075  loss_ce_6: 0.3015  loss_mask_6: 0.3995  loss_dice_6: 3.064  loss_ce_7: 0.3131  loss_mask_7: 0.3995  loss_dice_7: 3.073  loss_ce_8: 0.3008  loss_mask_8: 0.4013  loss_dice_8: 3.063  time: 1.5344  data_time: 0.0928  lr: 8.3951e-06  max_mem: 21410M
[01/17 10:46:26] d2.utils.events INFO:  eta: 1 day, 7:34:26  iter: 15919  total_loss: 37.9  loss_ce: 0.2862  loss_mask: 0.3937  loss_dice: 3.037  loss_ce_0: 0.5647  loss_mask_0: 0.3866  loss_dice_0: 3.17  loss_ce_1: 0.3227  loss_mask_1: 0.3985  loss_dice_1: 3.071  loss_ce_2: 0.311  loss_mask_2: 0.3947  loss_dice_2: 3.06  loss_ce_3: 0.2985  loss_mask_3: 0.3904  loss_dice_3: 3.046  loss_ce_4: 0.2979  loss_mask_4: 0.3935  loss_dice_4: 3.043  loss_ce_5: 0.2886  loss_mask_5: 0.3937  loss_dice_5: 3.039  loss_ce_6: 0.2815  loss_mask_6: 0.3931  loss_dice_6: 3.043  loss_ce_7: 0.2871  loss_mask_7: 0.3939  loss_dice_7: 3.042  loss_ce_8: 0.2994  loss_mask_8: 0.3942  loss_dice_8: 3.037  time: 1.5344  data_time: 0.0848  lr: 8.393e-06  max_mem: 21410M
[01/17 10:46:56] d2.utils.events INFO:  eta: 1 day, 7:33:29  iter: 15939  total_loss: 38.07  loss_ce: 0.2844  loss_mask: 0.3973  loss_dice: 3.057  loss_ce_0: 0.5915  loss_mask_0: 0.3922  loss_dice_0: 3.164  loss_ce_1: 0.304  loss_mask_1: 0.4018  loss_dice_1: 3.096  loss_ce_2: 0.3134  loss_mask_2: 0.4019  loss_dice_2: 3.068  loss_ce_3: 0.2889  loss_mask_3: 0.398  loss_dice_3: 3.062  loss_ce_4: 0.2824  loss_mask_4: 0.3993  loss_dice_4: 3.053  loss_ce_5: 0.2702  loss_mask_5: 0.3995  loss_dice_5: 3.065  loss_ce_6: 0.2837  loss_mask_6: 0.3995  loss_dice_6: 3.057  loss_ce_7: 0.2864  loss_mask_7: 0.3986  loss_dice_7: 3.055  loss_ce_8: 0.2763  loss_mask_8: 0.398  loss_dice_8: 3.062  time: 1.5344  data_time: 0.0957  lr: 8.391e-06  max_mem: 21410M
[01/17 10:47:28] d2.utils.events INFO:  eta: 1 day, 7:32:42  iter: 15959  total_loss: 38.09  loss_ce: 0.3022  loss_mask: 0.4019  loss_dice: 3.042  loss_ce_0: 0.5748  loss_mask_0: 0.3854  loss_dice_0: 3.179  loss_ce_1: 0.3271  loss_mask_1: 0.4047  loss_dice_1: 3.087  loss_ce_2: 0.3402  loss_mask_2: 0.4012  loss_dice_2: 3.065  loss_ce_3: 0.3238  loss_mask_3: 0.4017  loss_dice_3: 3.047  loss_ce_4: 0.3034  loss_mask_4: 0.4009  loss_dice_4: 3.056  loss_ce_5: 0.3126  loss_mask_5: 0.4012  loss_dice_5: 3.052  loss_ce_6: 0.3068  loss_mask_6: 0.4011  loss_dice_6: 3.048  loss_ce_7: 0.3058  loss_mask_7: 0.4012  loss_dice_7: 3.046  loss_ce_8: 0.3153  loss_mask_8: 0.3997  loss_dice_8: 3.04  time: 1.5344  data_time: 0.0911  lr: 8.3889e-06  max_mem: 21410M
[01/17 10:47:59] d2.utils.events INFO:  eta: 1 day, 7:32:28  iter: 15979  total_loss: 37.93  loss_ce: 0.2953  loss_mask: 0.4046  loss_dice: 3.056  loss_ce_0: 0.5608  loss_mask_0: 0.3973  loss_dice_0: 3.186  loss_ce_1: 0.296  loss_mask_1: 0.4108  loss_dice_1: 3.105  loss_ce_2: 0.3222  loss_mask_2: 0.4063  loss_dice_2: 3.081  loss_ce_3: 0.2934  loss_mask_3: 0.406  loss_dice_3: 3.068  loss_ce_4: 0.3082  loss_mask_4: 0.4057  loss_dice_4: 3.076  loss_ce_5: 0.2957  loss_mask_5: 0.4051  loss_dice_5: 3.068  loss_ce_6: 0.2916  loss_mask_6: 0.4045  loss_dice_6: 3.071  loss_ce_7: 0.2919  loss_mask_7: 0.4049  loss_dice_7: 3.056  loss_ce_8: 0.2909  loss_mask_8: 0.406  loss_dice_8: 3.059  time: 1.5344  data_time: 0.0995  lr: 8.3869e-06  max_mem: 21410M
[01/17 10:48:29] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 10:48:30] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 10:48:30] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 10:48:30] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 10:48:45] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0080 s/iter. Inference: 0.1808 s/iter. Eval: 0.2006 s/iter. Total: 0.3895 s/iter. ETA=0:07:01
[01/17 10:48:50] d2.evaluation.evaluator INFO: Inference done 22/1093. Dataloading: 0.0132 s/iter. Inference: 0.1856 s/iter. Eval: 0.2388 s/iter. Total: 0.4376 s/iter. ETA=0:07:48
[01/17 10:48:55] d2.evaluation.evaluator INFO: Inference done 36/1093. Dataloading: 0.0114 s/iter. Inference: 0.1806 s/iter. Eval: 0.2139 s/iter. Total: 0.4061 s/iter. ETA=0:07:09
[01/17 10:49:00] d2.evaluation.evaluator INFO: Inference done 49/1093. Dataloading: 0.0124 s/iter. Inference: 0.1785 s/iter. Eval: 0.2148 s/iter. Total: 0.4060 s/iter. ETA=0:07:03
[01/17 10:49:06] d2.evaluation.evaluator INFO: Inference done 61/1093. Dataloading: 0.0128 s/iter. Inference: 0.1820 s/iter. Eval: 0.2192 s/iter. Total: 0.4141 s/iter. ETA=0:07:07
[01/17 10:49:11] d2.evaluation.evaluator INFO: Inference done 73/1093. Dataloading: 0.0127 s/iter. Inference: 0.1823 s/iter. Eval: 0.2234 s/iter. Total: 0.4185 s/iter. ETA=0:07:06
[01/17 10:49:16] d2.evaluation.evaluator INFO: Inference done 85/1093. Dataloading: 0.0127 s/iter. Inference: 0.1806 s/iter. Eval: 0.2278 s/iter. Total: 0.4213 s/iter. ETA=0:07:04
[01/17 10:49:21] d2.evaluation.evaluator INFO: Inference done 96/1093. Dataloading: 0.0128 s/iter. Inference: 0.1824 s/iter. Eval: 0.2327 s/iter. Total: 0.4281 s/iter. ETA=0:07:06
[01/17 10:49:27] d2.evaluation.evaluator INFO: Inference done 109/1093. Dataloading: 0.0126 s/iter. Inference: 0.1817 s/iter. Eval: 0.2288 s/iter. Total: 0.4232 s/iter. ETA=0:06:56
[01/17 10:49:32] d2.evaluation.evaluator INFO: Inference done 121/1093. Dataloading: 0.0126 s/iter. Inference: 0.1814 s/iter. Eval: 0.2299 s/iter. Total: 0.4241 s/iter. ETA=0:06:52
[01/17 10:49:37] d2.evaluation.evaluator INFO: Inference done 134/1093. Dataloading: 0.0125 s/iter. Inference: 0.1821 s/iter. Eval: 0.2289 s/iter. Total: 0.4235 s/iter. ETA=0:06:46
[01/17 10:49:42] d2.evaluation.evaluator INFO: Inference done 148/1093. Dataloading: 0.0123 s/iter. Inference: 0.1816 s/iter. Eval: 0.2234 s/iter. Total: 0.4173 s/iter. ETA=0:06:34
[01/17 10:49:47] d2.evaluation.evaluator INFO: Inference done 162/1093. Dataloading: 0.0121 s/iter. Inference: 0.1815 s/iter. Eval: 0.2195 s/iter. Total: 0.4132 s/iter. ETA=0:06:24
[01/17 10:49:52] d2.evaluation.evaluator INFO: Inference done 174/1093. Dataloading: 0.0121 s/iter. Inference: 0.1813 s/iter. Eval: 0.2204 s/iter. Total: 0.4139 s/iter. ETA=0:06:20
[01/17 10:49:58] d2.evaluation.evaluator INFO: Inference done 187/1093. Dataloading: 0.0121 s/iter. Inference: 0.1815 s/iter. Eval: 0.2197 s/iter. Total: 0.4135 s/iter. ETA=0:06:14
[01/17 10:50:03] d2.evaluation.evaluator INFO: Inference done 200/1093. Dataloading: 0.0123 s/iter. Inference: 0.1812 s/iter. Eval: 0.2191 s/iter. Total: 0.4127 s/iter. ETA=0:06:08
[01/17 10:50:08] d2.evaluation.evaluator INFO: Inference done 212/1093. Dataloading: 0.0125 s/iter. Inference: 0.1813 s/iter. Eval: 0.2203 s/iter. Total: 0.4143 s/iter. ETA=0:06:04
[01/17 10:50:13] d2.evaluation.evaluator INFO: Inference done 227/1093. Dataloading: 0.0125 s/iter. Inference: 0.1794 s/iter. Eval: 0.2176 s/iter. Total: 0.4096 s/iter. ETA=0:05:54
[01/17 10:50:19] d2.evaluation.evaluator INFO: Inference done 240/1093. Dataloading: 0.0125 s/iter. Inference: 0.1794 s/iter. Eval: 0.2178 s/iter. Total: 0.4098 s/iter. ETA=0:05:49
[01/17 10:50:24] d2.evaluation.evaluator INFO: Inference done 252/1093. Dataloading: 0.0125 s/iter. Inference: 0.1798 s/iter. Eval: 0.2186 s/iter. Total: 0.4110 s/iter. ETA=0:05:45
[01/17 10:50:29] d2.evaluation.evaluator INFO: Inference done 264/1093. Dataloading: 0.0126 s/iter. Inference: 0.1806 s/iter. Eval: 0.2192 s/iter. Total: 0.4125 s/iter. ETA=0:05:41
[01/17 10:50:35] d2.evaluation.evaluator INFO: Inference done 276/1093. Dataloading: 0.0126 s/iter. Inference: 0.1812 s/iter. Eval: 0.2196 s/iter. Total: 0.4135 s/iter. ETA=0:05:37
[01/17 10:50:40] d2.evaluation.evaluator INFO: Inference done 289/1093. Dataloading: 0.0127 s/iter. Inference: 0.1822 s/iter. Eval: 0.2185 s/iter. Total: 0.4135 s/iter. ETA=0:05:32
[01/17 10:50:45] d2.evaluation.evaluator INFO: Inference done 301/1093. Dataloading: 0.0127 s/iter. Inference: 0.1829 s/iter. Eval: 0.2189 s/iter. Total: 0.4147 s/iter. ETA=0:05:28
[01/17 10:50:51] d2.evaluation.evaluator INFO: Inference done 312/1093. Dataloading: 0.0129 s/iter. Inference: 0.1836 s/iter. Eval: 0.2209 s/iter. Total: 0.4176 s/iter. ETA=0:05:26
[01/17 10:50:56] d2.evaluation.evaluator INFO: Inference done 324/1093. Dataloading: 0.0129 s/iter. Inference: 0.1842 s/iter. Eval: 0.2212 s/iter. Total: 0.4184 s/iter. ETA=0:05:21
[01/17 10:51:01] d2.evaluation.evaluator INFO: Inference done 337/1093. Dataloading: 0.0128 s/iter. Inference: 0.1852 s/iter. Eval: 0.2194 s/iter. Total: 0.4175 s/iter. ETA=0:05:15
[01/17 10:51:06] d2.evaluation.evaluator INFO: Inference done 352/1093. Dataloading: 0.0127 s/iter. Inference: 0.1845 s/iter. Eval: 0.2166 s/iter. Total: 0.4140 s/iter. ETA=0:05:06
[01/17 10:51:12] d2.evaluation.evaluator INFO: Inference done 365/1093. Dataloading: 0.0127 s/iter. Inference: 0.1847 s/iter. Eval: 0.2168 s/iter. Total: 0.4142 s/iter. ETA=0:05:01
[01/17 10:51:17] d2.evaluation.evaluator INFO: Inference done 379/1093. Dataloading: 0.0127 s/iter. Inference: 0.1846 s/iter. Eval: 0.2158 s/iter. Total: 0.4132 s/iter. ETA=0:04:55
[01/17 10:51:22] d2.evaluation.evaluator INFO: Inference done 390/1093. Dataloading: 0.0128 s/iter. Inference: 0.1848 s/iter. Eval: 0.2176 s/iter. Total: 0.4153 s/iter. ETA=0:04:51
[01/17 10:51:28] d2.evaluation.evaluator INFO: Inference done 402/1093. Dataloading: 0.0127 s/iter. Inference: 0.1855 s/iter. Eval: 0.2177 s/iter. Total: 0.4161 s/iter. ETA=0:04:47
[01/17 10:51:33] d2.evaluation.evaluator INFO: Inference done 414/1093. Dataloading: 0.0127 s/iter. Inference: 0.1856 s/iter. Eval: 0.2179 s/iter. Total: 0.4164 s/iter. ETA=0:04:42
[01/17 10:51:38] d2.evaluation.evaluator INFO: Inference done 425/1093. Dataloading: 0.0128 s/iter. Inference: 0.1861 s/iter. Eval: 0.2189 s/iter. Total: 0.4178 s/iter. ETA=0:04:39
[01/17 10:51:43] d2.evaluation.evaluator INFO: Inference done 436/1093. Dataloading: 0.0128 s/iter. Inference: 0.1887 s/iter. Eval: 0.2180 s/iter. Total: 0.4196 s/iter. ETA=0:04:35
[01/17 10:51:49] d2.evaluation.evaluator INFO: Inference done 448/1093. Dataloading: 0.0128 s/iter. Inference: 0.1898 s/iter. Eval: 0.2173 s/iter. Total: 0.4201 s/iter. ETA=0:04:30
[01/17 10:51:54] d2.evaluation.evaluator INFO: Inference done 460/1093. Dataloading: 0.0128 s/iter. Inference: 0.1903 s/iter. Eval: 0.2172 s/iter. Total: 0.4204 s/iter. ETA=0:04:26
[01/17 10:51:59] d2.evaluation.evaluator INFO: Inference done 474/1093. Dataloading: 0.0127 s/iter. Inference: 0.1901 s/iter. Eval: 0.2165 s/iter. Total: 0.4194 s/iter. ETA=0:04:19
[01/17 10:52:04] d2.evaluation.evaluator INFO: Inference done 487/1093. Dataloading: 0.0127 s/iter. Inference: 0.1898 s/iter. Eval: 0.2161 s/iter. Total: 0.4186 s/iter. ETA=0:04:13
[01/17 10:52:09] d2.evaluation.evaluator INFO: Inference done 501/1093. Dataloading: 0.0126 s/iter. Inference: 0.1891 s/iter. Eval: 0.2150 s/iter. Total: 0.4169 s/iter. ETA=0:04:06
[01/17 10:52:15] d2.evaluation.evaluator INFO: Inference done 516/1093. Dataloading: 0.0125 s/iter. Inference: 0.1882 s/iter. Eval: 0.2143 s/iter. Total: 0.4151 s/iter. ETA=0:03:59
[01/17 10:52:20] d2.evaluation.evaluator INFO: Inference done 527/1093. Dataloading: 0.0126 s/iter. Inference: 0.1883 s/iter. Eval: 0.2156 s/iter. Total: 0.4166 s/iter. ETA=0:03:55
[01/17 10:52:25] d2.evaluation.evaluator INFO: Inference done 539/1093. Dataloading: 0.0126 s/iter. Inference: 0.1881 s/iter. Eval: 0.2160 s/iter. Total: 0.4168 s/iter. ETA=0:03:50
[01/17 10:52:30] d2.evaluation.evaluator INFO: Inference done 551/1093. Dataloading: 0.0127 s/iter. Inference: 0.1875 s/iter. Eval: 0.2168 s/iter. Total: 0.4171 s/iter. ETA=0:03:46
[01/17 10:52:35] d2.evaluation.evaluator INFO: Inference done 564/1093. Dataloading: 0.0126 s/iter. Inference: 0.1872 s/iter. Eval: 0.2168 s/iter. Total: 0.4168 s/iter. ETA=0:03:40
[01/17 10:52:41] d2.evaluation.evaluator INFO: Inference done 578/1093. Dataloading: 0.0127 s/iter. Inference: 0.1870 s/iter. Eval: 0.2159 s/iter. Total: 0.4157 s/iter. ETA=0:03:34
[01/17 10:52:46] d2.evaluation.evaluator INFO: Inference done 592/1093. Dataloading: 0.0127 s/iter. Inference: 0.1868 s/iter. Eval: 0.2149 s/iter. Total: 0.4146 s/iter. ETA=0:03:27
[01/17 10:52:51] d2.evaluation.evaluator INFO: Inference done 603/1093. Dataloading: 0.0127 s/iter. Inference: 0.1864 s/iter. Eval: 0.2161 s/iter. Total: 0.4153 s/iter. ETA=0:03:23
[01/17 10:52:56] d2.evaluation.evaluator INFO: Inference done 615/1093. Dataloading: 0.0128 s/iter. Inference: 0.1864 s/iter. Eval: 0.2162 s/iter. Total: 0.4156 s/iter. ETA=0:03:18
[01/17 10:53:01] d2.evaluation.evaluator INFO: Inference done 627/1093. Dataloading: 0.0128 s/iter. Inference: 0.1864 s/iter. Eval: 0.2166 s/iter. Total: 0.4160 s/iter. ETA=0:03:13
[01/17 10:53:06] d2.evaluation.evaluator INFO: Inference done 640/1093. Dataloading: 0.0128 s/iter. Inference: 0.1861 s/iter. Eval: 0.2164 s/iter. Total: 0.4153 s/iter. ETA=0:03:08
[01/17 10:53:11] d2.evaluation.evaluator INFO: Inference done 652/1093. Dataloading: 0.0128 s/iter. Inference: 0.1860 s/iter. Eval: 0.2165 s/iter. Total: 0.4154 s/iter. ETA=0:03:03
[01/17 10:53:16] d2.evaluation.evaluator INFO: Inference done 664/1093. Dataloading: 0.0128 s/iter. Inference: 0.1858 s/iter. Eval: 0.2169 s/iter. Total: 0.4156 s/iter. ETA=0:02:58
[01/17 10:53:22] d2.evaluation.evaluator INFO: Inference done 676/1093. Dataloading: 0.0128 s/iter. Inference: 0.1863 s/iter. Eval: 0.2166 s/iter. Total: 0.4158 s/iter. ETA=0:02:53
[01/17 10:53:27] d2.evaluation.evaluator INFO: Inference done 690/1093. Dataloading: 0.0127 s/iter. Inference: 0.1864 s/iter. Eval: 0.2154 s/iter. Total: 0.4147 s/iter. ETA=0:02:47
[01/17 10:53:32] d2.evaluation.evaluator INFO: Inference done 701/1093. Dataloading: 0.0127 s/iter. Inference: 0.1871 s/iter. Eval: 0.2159 s/iter. Total: 0.4159 s/iter. ETA=0:02:43
[01/17 10:53:37] d2.evaluation.evaluator INFO: Inference done 709/1093. Dataloading: 0.0128 s/iter. Inference: 0.1882 s/iter. Eval: 0.2174 s/iter. Total: 0.4186 s/iter. ETA=0:02:40
[01/17 10:53:42] d2.evaluation.evaluator INFO: Inference done 718/1093. Dataloading: 0.0129 s/iter. Inference: 0.1894 s/iter. Eval: 0.2182 s/iter. Total: 0.4206 s/iter. ETA=0:02:37
[01/17 10:53:48] d2.evaluation.evaluator INFO: Inference done 732/1093. Dataloading: 0.0129 s/iter. Inference: 0.1893 s/iter. Eval: 0.2173 s/iter. Total: 0.4196 s/iter. ETA=0:02:31
[01/17 10:53:53] d2.evaluation.evaluator INFO: Inference done 746/1093. Dataloading: 0.0128 s/iter. Inference: 0.1889 s/iter. Eval: 0.2169 s/iter. Total: 0.4188 s/iter. ETA=0:02:25
[01/17 10:53:58] d2.evaluation.evaluator INFO: Inference done 760/1093. Dataloading: 0.0128 s/iter. Inference: 0.1883 s/iter. Eval: 0.2167 s/iter. Total: 0.4180 s/iter. ETA=0:02:19
[01/17 10:54:03] d2.evaluation.evaluator INFO: Inference done 773/1093. Dataloading: 0.0128 s/iter. Inference: 0.1878 s/iter. Eval: 0.2171 s/iter. Total: 0.4178 s/iter. ETA=0:02:13
[01/17 10:54:08] d2.evaluation.evaluator INFO: Inference done 786/1093. Dataloading: 0.0128 s/iter. Inference: 0.1876 s/iter. Eval: 0.2168 s/iter. Total: 0.4174 s/iter. ETA=0:02:08
[01/17 10:54:14] d2.evaluation.evaluator INFO: Inference done 800/1093. Dataloading: 0.0128 s/iter. Inference: 0.1875 s/iter. Eval: 0.2165 s/iter. Total: 0.4169 s/iter. ETA=0:02:02
[01/17 10:54:19] d2.evaluation.evaluator INFO: Inference done 814/1093. Dataloading: 0.0127 s/iter. Inference: 0.1871 s/iter. Eval: 0.2162 s/iter. Total: 0.4161 s/iter. ETA=0:01:56
[01/17 10:54:24] d2.evaluation.evaluator INFO: Inference done 829/1093. Dataloading: 0.0127 s/iter. Inference: 0.1866 s/iter. Eval: 0.2152 s/iter. Total: 0.4146 s/iter. ETA=0:01:49
[01/17 10:54:29] d2.evaluation.evaluator INFO: Inference done 843/1093. Dataloading: 0.0126 s/iter. Inference: 0.1865 s/iter. Eval: 0.2146 s/iter. Total: 0.4138 s/iter. ETA=0:01:43
[01/17 10:54:34] d2.evaluation.evaluator INFO: Inference done 854/1093. Dataloading: 0.0127 s/iter. Inference: 0.1867 s/iter. Eval: 0.2151 s/iter. Total: 0.4146 s/iter. ETA=0:01:39
[01/17 10:54:40] d2.evaluation.evaluator INFO: Inference done 864/1093. Dataloading: 0.0127 s/iter. Inference: 0.1872 s/iter. Eval: 0.2159 s/iter. Total: 0.4159 s/iter. ETA=0:01:35
[01/17 10:54:45] d2.evaluation.evaluator INFO: Inference done 875/1093. Dataloading: 0.0127 s/iter. Inference: 0.1878 s/iter. Eval: 0.2160 s/iter. Total: 0.4166 s/iter. ETA=0:01:30
[01/17 10:54:50] d2.evaluation.evaluator INFO: Inference done 888/1093. Dataloading: 0.0127 s/iter. Inference: 0.1875 s/iter. Eval: 0.2163 s/iter. Total: 0.4166 s/iter. ETA=0:01:25
[01/17 10:54:56] d2.evaluation.evaluator INFO: Inference done 901/1093. Dataloading: 0.0127 s/iter. Inference: 0.1873 s/iter. Eval: 0.2163 s/iter. Total: 0.4164 s/iter. ETA=0:01:19
[01/17 10:55:01] d2.evaluation.evaluator INFO: Inference done 912/1093. Dataloading: 0.0127 s/iter. Inference: 0.1880 s/iter. Eval: 0.2166 s/iter. Total: 0.4174 s/iter. ETA=0:01:15
[01/17 10:55:06] d2.evaluation.evaluator INFO: Inference done 922/1093. Dataloading: 0.0127 s/iter. Inference: 0.1885 s/iter. Eval: 0.2170 s/iter. Total: 0.4183 s/iter. ETA=0:01:11
[01/17 10:55:11] d2.evaluation.evaluator INFO: Inference done 931/1093. Dataloading: 0.0127 s/iter. Inference: 0.1894 s/iter. Eval: 0.2175 s/iter. Total: 0.4197 s/iter. ETA=0:01:07
[01/17 10:55:16] d2.evaluation.evaluator INFO: Inference done 943/1093. Dataloading: 0.0127 s/iter. Inference: 0.1895 s/iter. Eval: 0.2176 s/iter. Total: 0.4198 s/iter. ETA=0:01:02
[01/17 10:55:22] d2.evaluation.evaluator INFO: Inference done 956/1093. Dataloading: 0.0127 s/iter. Inference: 0.1894 s/iter. Eval: 0.2174 s/iter. Total: 0.4196 s/iter. ETA=0:00:57
[01/17 10:55:27] d2.evaluation.evaluator INFO: Inference done 970/1093. Dataloading: 0.0127 s/iter. Inference: 0.1890 s/iter. Eval: 0.2171 s/iter. Total: 0.4189 s/iter. ETA=0:00:51
[01/17 10:55:32] d2.evaluation.evaluator INFO: Inference done 985/1093. Dataloading: 0.0126 s/iter. Inference: 0.1886 s/iter. Eval: 0.2162 s/iter. Total: 0.4176 s/iter. ETA=0:00:45
[01/17 10:55:37] d2.evaluation.evaluator INFO: Inference done 996/1093. Dataloading: 0.0127 s/iter. Inference: 0.1889 s/iter. Eval: 0.2163 s/iter. Total: 0.4180 s/iter. ETA=0:00:40
[01/17 10:55:42] d2.evaluation.evaluator INFO: Inference done 1010/1093. Dataloading: 0.0126 s/iter. Inference: 0.1888 s/iter. Eval: 0.2159 s/iter. Total: 0.4174 s/iter. ETA=0:00:34
[01/17 10:55:47] d2.evaluation.evaluator INFO: Inference done 1024/1093. Dataloading: 0.0126 s/iter. Inference: 0.1885 s/iter. Eval: 0.2157 s/iter. Total: 0.4170 s/iter. ETA=0:00:28
[01/17 10:55:53] d2.evaluation.evaluator INFO: Inference done 1037/1093. Dataloading: 0.0126 s/iter. Inference: 0.1884 s/iter. Eval: 0.2157 s/iter. Total: 0.4168 s/iter. ETA=0:00:23
[01/17 10:55:58] d2.evaluation.evaluator INFO: Inference done 1049/1093. Dataloading: 0.0126 s/iter. Inference: 0.1883 s/iter. Eval: 0.2159 s/iter. Total: 0.4170 s/iter. ETA=0:00:18
[01/17 10:56:03] d2.evaluation.evaluator INFO: Inference done 1064/1093. Dataloading: 0.0126 s/iter. Inference: 0.1880 s/iter. Eval: 0.2154 s/iter. Total: 0.4160 s/iter. ETA=0:00:12
[01/17 10:56:08] d2.evaluation.evaluator INFO: Inference done 1080/1093. Dataloading: 0.0125 s/iter. Inference: 0.1874 s/iter. Eval: 0.2145 s/iter. Total: 0.4145 s/iter. ETA=0:00:05
[01/17 10:56:13] d2.evaluation.evaluator INFO: Total inference time: 0:07:30.343562 (0.413919 s / iter per device, on 4 devices)
[01/17 10:56:13] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:23 (0.186897 s / iter per device, on 4 devices)
[01/17 10:56:37] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 12.39630266828517, 'fwIoU': 37.47282787668374, 'IoU-1': nan, 'IoU-2': 94.61404669486184, 'IoU-3': 43.606674163149734, 'IoU-4': 56.82316276827702, 'IoU-5': 50.1339591939686, 'IoU-6': 42.30103418776622, 'IoU-7': 37.62575232935368, 'IoU-8': 31.583657914033438, 'IoU-9': 14.309047059722966, 'IoU-10': 18.309096731919432, 'IoU-11': 27.121098907938222, 'IoU-12': 40.58412813937754, 'IoU-13': 43.00658372837647, 'IoU-14': 44.682747826392344, 'IoU-15': 44.38599194189593, 'IoU-16': 45.68930222447161, 'IoU-17': 46.310820645808235, 'IoU-18': 38.76438286595387, 'IoU-19': 42.64412411489784, 'IoU-20': 40.80980537276425, 'IoU-21': 42.4341185559031, 'IoU-22': 39.87355733873924, 'IoU-23': 41.02431314109428, 'IoU-24': 39.32658328354767, 'IoU-25': 41.965452488701786, 'IoU-26': 40.4151470255566, 'IoU-27': 37.48589592799188, 'IoU-28': 39.27248670639667, 'IoU-29': 40.44481833961202, 'IoU-30': 41.762702551182, 'IoU-31': 40.87510795678085, 'IoU-32': 40.10457014400038, 'IoU-33': 40.83553357023834, 'IoU-34': 38.94301866111011, 'IoU-35': 37.217252172273554, 'IoU-36': 37.96654530728474, 'IoU-37': 36.984533954575525, 'IoU-38': 34.93269125420953, 'IoU-39': 32.73097175616902, 'IoU-40': 32.083433584458994, 'IoU-41': 33.68742860999892, 'IoU-42': 30.594689194436697, 'IoU-43': 32.04235042883127, 'IoU-44': 31.43894071159636, 'IoU-45': 30.718739342713498, 'IoU-46': 31.86975890513322, 'IoU-47': 29.96618682343225, 'IoU-48': 31.269178847711327, 'IoU-49': 29.581594642009968, 'IoU-50': 30.587660738411582, 'IoU-51': 32.167163756943026, 'IoU-52': 29.753722567646403, 'IoU-53': 30.303376839363505, 'IoU-54': 30.61418067285952, 'IoU-55': 29.14805602873197, 'IoU-56': 27.78139927989282, 'IoU-57': 26.94050897681431, 'IoU-58': 26.48242125023885, 'IoU-59': 22.78744921803447, 'IoU-60': 22.545788370207866, 'IoU-61': 21.51385160713773, 'IoU-62': 20.75853006135292, 'IoU-63': 19.960928328977893, 'IoU-64': 19.35536679089012, 'IoU-65': 19.446122440000075, 'IoU-66': 19.016075480016532, 'IoU-67': 19.79963938099856, 'IoU-68': 18.697042028395327, 'IoU-69': 16.845388243892508, 'IoU-70': 17.516669543891158, 'IoU-71': 17.093515113142775, 'IoU-72': 14.558232606439987, 'IoU-73': 15.016531448743581, 'IoU-74': 13.649546666477782, 'IoU-75': 11.174659242114837, 'IoU-76': 13.042543678843433, 'IoU-77': 13.224800945291108, 'IoU-78': 12.91759672845377, 'IoU-79': 13.152709641304755, 'IoU-80': 13.755368618231193, 'IoU-81': 13.09133573911965, 'IoU-82': 13.333879325395367, 'IoU-83': 14.268962508143135, 'IoU-84': 13.742283152648127, 'IoU-85': 12.489768070940308, 'IoU-86': 10.243354368998219, 'IoU-87': 11.205278623225169, 'IoU-88': 11.11191672244581, 'IoU-89': 10.344839603821784, 'IoU-90': 8.092336451842355, 'IoU-91': 8.464789968545535, 'IoU-92': 8.284339781969827, 'IoU-93': 8.11109003633187, 'IoU-94': 9.072188715564197, 'IoU-95': 8.518066532342706, 'IoU-96': 8.68483682686382, 'IoU-97': 9.856915507028434, 'IoU-98': 10.85485929502436, 'IoU-99': 10.282070397901004, 'IoU-100': 11.136981962112442, 'IoU-101': 13.387794251310098, 'IoU-102': 13.016394206856507, 'IoU-103': 13.741153348286076, 'IoU-104': 10.023029287283014, 'IoU-105': 12.88732349320489, 'IoU-106': 12.126041339386678, 'IoU-107': 12.407540951281003, 'IoU-108': 12.320070517998289, 'IoU-109': 12.676990520871387, 'IoU-110': 12.894552502968756, 'IoU-111': 9.819251802262977, 'IoU-112': 9.192199819538335, 'IoU-113': 9.381334031355031, 'IoU-114': 10.858897635635637, 'IoU-115': 9.6930258137334, 'IoU-116': 9.957790667629805, 'IoU-117': 11.183630245931251, 'IoU-118': 5.422058441083834, 'IoU-119': 8.151013091230935, 'IoU-120': 9.835968400329161, 'IoU-121': 6.341150539067277, 'IoU-122': 8.94838682652362, 'IoU-123': 7.902956193288263, 'IoU-124': 4.592188718128718, 'IoU-125': 7.379191610242061, 'IoU-126': 9.277415822099812, 'IoU-127': 5.501741786979291, 'IoU-128': 6.529728921047368, 'IoU-129': 4.1201166018103965, 'IoU-130': 4.508038995256534, 'IoU-131': 6.3701587104388615, 'IoU-132': 5.712169572276824, 'IoU-133': 3.586164290459173, 'IoU-134': 5.814863745129915, 'IoU-135': 5.569604136161881, 'IoU-136': 5.724488275481343, 'IoU-137': 4.747497595248369, 'IoU-138': 3.8493253888113683, 'IoU-139': 4.705389207547464, 'IoU-140': 3.8435342154248002, 'IoU-141': 3.622880832090198, 'IoU-142': 5.506528892951348, 'IoU-143': 3.1532231416472905, 'IoU-144': 4.0106532042387535, 'IoU-145': 5.5414137222163244, 'IoU-146': 3.3623281528988818, 'IoU-147': 4.571034667950362, 'IoU-148': 2.5474169006775744, 'IoU-149': 4.096464832828019, 'IoU-150': 4.029160769427928, 'IoU-151': 2.482510800565156, 'IoU-152': 1.4398107799390638, 'IoU-153': 2.3655830278670194, 'IoU-154': 1.9492207572332796, 'IoU-155': 2.240262850895237, 'IoU-156': 2.2178035714954185, 'IoU-157': 1.5196553278245084, 'IoU-158': 2.0310749288592604, 'IoU-159': 2.258414485970742, 'IoU-160': 1.8636309573904992, 'IoU-161': 1.4156186529944772, 'IoU-162': 3.480441504398188, 'IoU-163': 2.488930772140023, 'IoU-164': 2.3922757267927275, 'IoU-165': 2.868901216098273, 'IoU-166': 1.085715929908021, 'IoU-167': 1.3754669722207755, 'IoU-168': 1.752688145682051, 'IoU-169': 1.5684679088118423, 'IoU-170': 1.4094156630061319, 'IoU-171': 1.63720143320197, 'IoU-172': 1.0403080023623343, 'IoU-173': 0.8847708173265366, 'IoU-174': 1.077042245596773, 'IoU-175': 0.9922651182567764, 'IoU-176': 1.4874057103658866, 'IoU-177': 1.1498473522306458, 'IoU-178': 1.5148355294340696, 'IoU-179': 1.540112564802137, 'IoU-180': 0.5811060241399902, 'IoU-181': 1.653012879448426, 'IoU-182': 0.5292609549606002, 'IoU-183': 0.3430902795997065, 'IoU-184': 0.0, 'IoU-185': 0.5948371751103654, 'IoU-186': 0.9372119809696204, 'IoU-187': 0.9951256214472657, 'IoU-188': 1.0835788312997376, 'IoU-189': 1.5058020093620135, 'IoU-190': 1.1120404078169364, 'IoU-191': 1.3292999916964734, 'IoU-192': 0.07559827776108942, 'IoU-193': 0.0, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 19.25701216491176, 'pACC': 51.30439096220041, 'ACC-1': nan, 'ACC-2': 98.76643558750652, 'ACC-3': 57.42615996307998, 'ACC-4': 70.2781738228219, 'ACC-5': 65.56668101488863, 'ACC-6': 59.61984903171028, 'ACC-7': 53.46571536483529, 'ACC-8': 47.35187838877525, 'ACC-9': 17.145864771860396, 'ACC-10': 22.06399258214588, 'ACC-11': 38.25025466596678, 'ACC-12': 56.363011687849266, 'ACC-13': 61.22890609811695, 'ACC-14': 67.61548078471462, 'ACC-15': 60.66183065080896, 'ACC-16': 64.41113460106736, 'ACC-17': 65.76518022247657, 'ACC-18': 54.6195805977018, 'ACC-19': 59.47967623330579, 'ACC-20': 56.86639809003032, 'ACC-21': 59.954237996657746, 'ACC-22': 56.82288125457271, 'ACC-23': 55.96943612615004, 'ACC-24': 56.01334644516833, 'ACC-25': 61.29168567776605, 'ACC-26': 59.03567233044817, 'ACC-27': 53.18349091854554, 'ACC-28': 54.517392980102606, 'ACC-29': 64.69404766080116, 'ACC-30': 59.0244222936868, 'ACC-31': 58.33896000145008, 'ACC-32': 55.90062490865249, 'ACC-33': 58.5928755636792, 'ACC-34': 57.033421415701426, 'ACC-35': 51.47778101838669, 'ACC-36': 53.148948608446524, 'ACC-37': 49.346466033979716, 'ACC-38': 48.27307490579552, 'ACC-39': 51.75816268677347, 'ACC-40': 50.0802378896447, 'ACC-41': 51.260737061532794, 'ACC-42': 49.53617423152853, 'ACC-43': 48.21633905114326, 'ACC-44': 47.7051405522977, 'ACC-45': 46.11446071668876, 'ACC-46': 49.88134946662223, 'ACC-47': 46.312069316729804, 'ACC-48': 48.82151309184473, 'ACC-49': 48.23534429106127, 'ACC-50': 47.980941599334905, 'ACC-51': 49.87338750975908, 'ACC-52': 48.43362550483447, 'ACC-53': 46.00518012321084, 'ACC-54': 47.11030355864537, 'ACC-55': 46.871526974874115, 'ACC-56': 38.86476554177704, 'ACC-57': 41.99252953366993, 'ACC-58': 39.75200431346425, 'ACC-59': 34.86376181091618, 'ACC-60': 35.65067467628687, 'ACC-61': 32.640326207181594, 'ACC-62': 35.56532196888756, 'ACC-63': 32.35520037181314, 'ACC-64': 31.284788472804493, 'ACC-65': 32.27676700574328, 'ACC-66': 31.79435843842905, 'ACC-67': 32.52595735704086, 'ACC-68': 32.68320981483758, 'ACC-69': 27.622700973142024, 'ACC-70': 27.792989733439295, 'ACC-71': 28.99454135476577, 'ACC-72': 28.24051067667747, 'ACC-73': 25.476291976369485, 'ACC-74': 22.144451727218044, 'ACC-75': 17.30073362630166, 'ACC-76': 23.535221280299297, 'ACC-77': 22.048294492956966, 'ACC-78': 26.655188622501523, 'ACC-79': 22.960672598198713, 'ACC-80': 25.024623716311623, 'ACC-81': 24.68907407098193, 'ACC-82': 21.773628640790147, 'ACC-83': 24.38751536063658, 'ACC-84': 23.914202646409674, 'ACC-85': 21.24429214292868, 'ACC-86': 16.4338604426385, 'ACC-87': 19.578023546962, 'ACC-88': 21.600543316018868, 'ACC-89': 17.5342106262017, 'ACC-90': 12.903601927420747, 'ACC-91': 15.579191339381925, 'ACC-92': 15.14660484797386, 'ACC-93': 15.378678030071956, 'ACC-94': 18.644204380405107, 'ACC-95': 17.152661713510003, 'ACC-96': 15.653258580472126, 'ACC-97': 20.226552819054007, 'ACC-98': 19.599495208642473, 'ACC-99': 22.95902349780176, 'ACC-100': 20.312795287282466, 'ACC-101': 26.305136370603016, 'ACC-102': 22.684740986053292, 'ACC-103': 25.273470774807432, 'ACC-104': 17.966019646736562, 'ACC-105': 27.252453210693297, 'ACC-106': 23.331020334047494, 'ACC-107': 23.0998938780455, 'ACC-108': 21.491760548977627, 'ACC-109': 22.77638874571741, 'ACC-110': 27.021572454616198, 'ACC-111': 17.306586927675166, 'ACC-112': 14.143523371758457, 'ACC-113': 15.330431840612757, 'ACC-114': 18.671916856960056, 'ACC-115': 16.08365461040956, 'ACC-116': 18.715175980635365, 'ACC-117': 22.654345054402594, 'ACC-118': 7.690663306349537, 'ACC-119': 15.513392341679063, 'ACC-120': 16.369116166717493, 'ACC-121': 10.74366515916606, 'ACC-122': 16.592002187473174, 'ACC-123': 13.040891807485824, 'ACC-124': 6.920247735701217, 'ACC-125': 16.145694877366747, 'ACC-126': 25.910875247317982, 'ACC-127': 9.646205794223992, 'ACC-128': 11.114544103766972, 'ACC-129': 6.305106410486272, 'ACC-130': 7.453567768749069, 'ACC-131': 13.095102968452931, 'ACC-132': 9.35912695643621, 'ACC-133': 5.94464718360125, 'ACC-134': 11.407458817285514, 'ACC-135': 11.036691081430417, 'ACC-136': 12.621144746593696, 'ACC-137': 7.440874836936265, 'ACC-138': 5.775379192912319, 'ACC-139': 9.571874127471634, 'ACC-140': 8.07310988372236, 'ACC-141': 7.281505150848039, 'ACC-142': 11.183914875138647, 'ACC-143': 9.092092037018116, 'ACC-144': 8.446578299200343, 'ACC-145': 12.43673285198556, 'ACC-146': 6.009883941948613, 'ACC-147': 7.6535605766375, 'ACC-148': 4.530805035919787, 'ACC-149': 10.2357205116482, 'ACC-150': 8.305909627013932, 'ACC-151': 3.236054696172331, 'ACC-152': 1.9385409306147505, 'ACC-153': 3.5223422010924534, 'ACC-154': 3.0059638322431708, 'ACC-155': 4.6674345724326525, 'ACC-156': 8.762692185385406, 'ACC-157': 2.305907295726761, 'ACC-158': 3.0445073244294187, 'ACC-159': 6.907053973531899, 'ACC-160': 3.798552257384026, 'ACC-161': 2.063492512328643, 'ACC-162': 7.3354920424514924, 'ACC-163': 7.088047937971026, 'ACC-164': 4.325348114472298, 'ACC-165': 4.320472857917484, 'ACC-166': 1.6180314903105515, 'ACC-167': 1.9507936641176522, 'ACC-168': 4.967628854876169, 'ACC-169': 4.399219354384978, 'ACC-170': 2.8914199683098407, 'ACC-171': 3.5719863140111023, 'ACC-172': 2.1257914545865333, 'ACC-173': 1.576101048264622, 'ACC-174': 3.0974250525864244, 'ACC-175': 2.666450442792941, 'ACC-176': 3.303326622035612, 'ACC-177': 2.8741365799860694, 'ACC-178': 3.9812205533071645, 'ACC-179': 12.379879894381228, 'ACC-180': 1.0424422933730455, 'ACC-181': 2.2109621050492736, 'ACC-182': 0.7442305551410413, 'ACC-183': 0.8829891326236884, 'ACC-184': 0.0, 'ACC-185': 0.7289802059823073, 'ACC-186': 2.087865896188721, 'ACC-187': 2.6101654104099734, 'ACC-188': 1.8804286111211779, 'ACC-189': 4.04846454945321, 'ACC-190': 2.3518629290846964, 'ACC-191': 3.3136128782082857, 'ACC-192': 0.07882544861337684, 'ACC-193': 0.0, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 10:56:37] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 10:56:37] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 10:56:37] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 10:56:37] d2.evaluation.testing INFO: copypaste: 12.3963,37.4728,19.2570,51.3044
[01/17 10:56:37] d2.utils.events INFO:  eta: 1 day, 7:30:47  iter: 15999  total_loss: 38.13  loss_ce: 0.2881  loss_mask: 0.4118  loss_dice: 3.04  loss_ce_0: 0.5603  loss_mask_0: 0.4099  loss_dice_0: 3.162  loss_ce_1: 0.3344  loss_mask_1: 0.4189  loss_dice_1: 3.085  loss_ce_2: 0.3301  loss_mask_2: 0.4146  loss_dice_2: 3.072  loss_ce_3: 0.3126  loss_mask_3: 0.4126  loss_dice_3: 3.058  loss_ce_4: 0.3003  loss_mask_4: 0.412  loss_dice_4: 3.049  loss_ce_5: 0.3026  loss_mask_5: 0.412  loss_dice_5: 3.051  loss_ce_6: 0.304  loss_mask_6: 0.4121  loss_dice_6: 3.051  loss_ce_7: 0.2949  loss_mask_7: 0.4131  loss_dice_7: 3.044  loss_ce_8: 0.2981  loss_mask_8: 0.4113  loss_dice_8: 3.055  time: 1.5344  data_time: 0.0978  lr: 8.3849e-06  max_mem: 21410M
[01/17 10:57:08] d2.utils.events INFO:  eta: 1 day, 7:30:59  iter: 16019  total_loss: 38.68  loss_ce: 0.2845  loss_mask: 0.4099  loss_dice: 3.106  loss_ce_0: 0.5771  loss_mask_0: 0.4077  loss_dice_0: 3.216  loss_ce_1: 0.3085  loss_mask_1: 0.416  loss_dice_1: 3.126  loss_ce_2: 0.3179  loss_mask_2: 0.4126  loss_dice_2: 3.108  loss_ce_3: 0.3097  loss_mask_3: 0.4089  loss_dice_3: 3.097  loss_ce_4: 0.303  loss_mask_4: 0.412  loss_dice_4: 3.1  loss_ce_5: 0.2967  loss_mask_5: 0.4107  loss_dice_5: 3.102  loss_ce_6: 0.2996  loss_mask_6: 0.4092  loss_dice_6: 3.079  loss_ce_7: 0.2916  loss_mask_7: 0.4091  loss_dice_7: 3.096  loss_ce_8: 0.3063  loss_mask_8: 0.4091  loss_dice_8: 3.092  time: 1.5344  data_time: 0.1007  lr: 8.3828e-06  max_mem: 21410M
[01/17 10:57:39] d2.utils.events INFO:  eta: 1 day, 7:30:43  iter: 16039  total_loss: 37.49  loss_ce: 0.2876  loss_mask: 0.4096  loss_dice: 3.015  loss_ce_0: 0.5483  loss_mask_0: 0.393  loss_dice_0: 3.144  loss_ce_1: 0.3113  loss_mask_1: 0.4117  loss_dice_1: 3.058  loss_ce_2: 0.3175  loss_mask_2: 0.4063  loss_dice_2: 3.033  loss_ce_3: 0.3053  loss_mask_3: 0.4058  loss_dice_3: 3.018  loss_ce_4: 0.305  loss_mask_4: 0.4077  loss_dice_4: 3.017  loss_ce_5: 0.2863  loss_mask_5: 0.4104  loss_dice_5: 3.026  loss_ce_6: 0.2815  loss_mask_6: 0.4078  loss_dice_6: 3.015  loss_ce_7: 0.2892  loss_mask_7: 0.4094  loss_dice_7: 3.015  loss_ce_8: 0.2943  loss_mask_8: 0.4089  loss_dice_8: 3.016  time: 1.5344  data_time: 0.1151  lr: 8.3808e-06  max_mem: 21410M
[01/17 10:58:11] d2.utils.events INFO:  eta: 1 day, 7:29:58  iter: 16059  total_loss: 38.04  loss_ce: 0.2792  loss_mask: 0.4228  loss_dice: 3.043  loss_ce_0: 0.5703  loss_mask_0: 0.4072  loss_dice_0: 3.176  loss_ce_1: 0.3215  loss_mask_1: 0.4244  loss_dice_1: 3.088  loss_ce_2: 0.3234  loss_mask_2: 0.4242  loss_dice_2: 3.072  loss_ce_3: 0.3068  loss_mask_3: 0.4258  loss_dice_3: 3.055  loss_ce_4: 0.3051  loss_mask_4: 0.4214  loss_dice_4: 3.061  loss_ce_5: 0.2834  loss_mask_5: 0.4205  loss_dice_5: 3.059  loss_ce_6: 0.2923  loss_mask_6: 0.4214  loss_dice_6: 3.05  loss_ce_7: 0.2885  loss_mask_7: 0.4225  loss_dice_7: 3.041  loss_ce_8: 0.2859  loss_mask_8: 0.4223  loss_dice_8: 3.041  time: 1.5344  data_time: 0.0925  lr: 8.3787e-06  max_mem: 21410M
[01/17 10:58:42] d2.utils.events INFO:  eta: 1 day, 7:29:42  iter: 16079  total_loss: 37.77  loss_ce: 0.293  loss_mask: 0.4009  loss_dice: 2.99  loss_ce_0: 0.5748  loss_mask_0: 0.3967  loss_dice_0: 3.111  loss_ce_1: 0.3288  loss_mask_1: 0.4059  loss_dice_1: 3.034  loss_ce_2: 0.325  loss_mask_2: 0.4016  loss_dice_2: 3.007  loss_ce_3: 0.3141  loss_mask_3: 0.4013  loss_dice_3: 2.993  loss_ce_4: 0.3168  loss_mask_4: 0.4012  loss_dice_4: 2.998  loss_ce_5: 0.2983  loss_mask_5: 0.4  loss_dice_5: 3.006  loss_ce_6: 0.2991  loss_mask_6: 0.4018  loss_dice_6: 2.995  loss_ce_7: 0.2966  loss_mask_7: 0.3982  loss_dice_7: 2.993  loss_ce_8: 0.294  loss_mask_8: 0.4003  loss_dice_8: 2.995  time: 1.5345  data_time: 0.0992  lr: 8.3767e-06  max_mem: 21410M
[01/17 10:59:12] d2.utils.events INFO:  eta: 1 day, 7:29:35  iter: 16099  total_loss: 37.78  loss_ce: 0.2868  loss_mask: 0.404  loss_dice: 3.029  loss_ce_0: 0.5725  loss_mask_0: 0.3974  loss_dice_0: 3.167  loss_ce_1: 0.3003  loss_mask_1: 0.4071  loss_dice_1: 3.08  loss_ce_2: 0.3211  loss_mask_2: 0.4072  loss_dice_2: 3.049  loss_ce_3: 0.2998  loss_mask_3: 0.4053  loss_dice_3: 3.036  loss_ce_4: 0.2953  loss_mask_4: 0.4061  loss_dice_4: 3.034  loss_ce_5: 0.2793  loss_mask_5: 0.4068  loss_dice_5: 3.038  loss_ce_6: 0.2971  loss_mask_6: 0.4058  loss_dice_6: 3.039  loss_ce_7: 0.2822  loss_mask_7: 0.4042  loss_dice_7: 3.037  loss_ce_8: 0.2757  loss_mask_8: 0.4047  loss_dice_8: 3.032  time: 1.5345  data_time: 0.1155  lr: 8.3747e-06  max_mem: 21410M
[01/17 10:59:44] d2.utils.events INFO:  eta: 1 day, 7:29:54  iter: 16119  total_loss: 38.14  loss_ce: 0.2817  loss_mask: 0.4013  loss_dice: 3.042  loss_ce_0: 0.5975  loss_mask_0: 0.3977  loss_dice_0: 3.167  loss_ce_1: 0.3305  loss_mask_1: 0.4053  loss_dice_1: 3.094  loss_ce_2: 0.3047  loss_mask_2: 0.401  loss_dice_2: 3.066  loss_ce_3: 0.3001  loss_mask_3: 0.4012  loss_dice_3: 3.056  loss_ce_4: 0.2973  loss_mask_4: 0.4037  loss_dice_4: 3.052  loss_ce_5: 0.2875  loss_mask_5: 0.4032  loss_dice_5: 3.046  loss_ce_6: 0.2887  loss_mask_6: 0.404  loss_dice_6: 3.047  loss_ce_7: 0.291  loss_mask_7: 0.4021  loss_dice_7: 3.046  loss_ce_8: 0.286  loss_mask_8: 0.402  loss_dice_8: 3.049  time: 1.5345  data_time: 0.1072  lr: 8.3726e-06  max_mem: 21410M
[01/17 11:00:14] d2.utils.events INFO:  eta: 1 day, 7:29:35  iter: 16139  total_loss: 38.05  loss_ce: 0.2863  loss_mask: 0.4076  loss_dice: 3.045  loss_ce_0: 0.5714  loss_mask_0: 0.4063  loss_dice_0: 3.166  loss_ce_1: 0.3133  loss_mask_1: 0.4109  loss_dice_1: 3.08  loss_ce_2: 0.3252  loss_mask_2: 0.4068  loss_dice_2: 3.057  loss_ce_3: 0.3076  loss_mask_3: 0.408  loss_dice_3: 3.048  loss_ce_4: 0.2963  loss_mask_4: 0.4083  loss_dice_4: 3.055  loss_ce_5: 0.2858  loss_mask_5: 0.4065  loss_dice_5: 3.044  loss_ce_6: 0.2924  loss_mask_6: 0.4073  loss_dice_6: 3.041  loss_ce_7: 0.2901  loss_mask_7: 0.4069  loss_dice_7: 3.041  loss_ce_8: 0.2956  loss_mask_8: 0.4074  loss_dice_8: 3.043  time: 1.5345  data_time: 0.1094  lr: 8.3706e-06  max_mem: 21410M
[01/17 11:00:45] d2.utils.events INFO:  eta: 1 day, 7:30:33  iter: 16159  total_loss: 38.28  loss_ce: 0.3151  loss_mask: 0.4034  loss_dice: 3.076  loss_ce_0: 0.6154  loss_mask_0: 0.3985  loss_dice_0: 3.208  loss_ce_1: 0.3521  loss_mask_1: 0.4051  loss_dice_1: 3.109  loss_ce_2: 0.3324  loss_mask_2: 0.4054  loss_dice_2: 3.091  loss_ce_3: 0.3043  loss_mask_3: 0.4047  loss_dice_3: 3.079  loss_ce_4: 0.3099  loss_mask_4: 0.4023  loss_dice_4: 3.08  loss_ce_5: 0.3071  loss_mask_5: 0.4001  loss_dice_5: 3.087  loss_ce_6: 0.3096  loss_mask_6: 0.4029  loss_dice_6: 3.077  loss_ce_7: 0.3137  loss_mask_7: 0.4022  loss_dice_7: 3.084  loss_ce_8: 0.3178  loss_mask_8: 0.4022  loss_dice_8: 3.075  time: 1.5345  data_time: 0.0938  lr: 8.3685e-06  max_mem: 21410M
[01/17 11:01:17] d2.utils.events INFO:  eta: 1 day, 7:31:35  iter: 16179  total_loss: 37.51  loss_ce: 0.2812  loss_mask: 0.4022  loss_dice: 3.017  loss_ce_0: 0.5588  loss_mask_0: 0.3954  loss_dice_0: 3.137  loss_ce_1: 0.3182  loss_mask_1: 0.4081  loss_dice_1: 3.051  loss_ce_2: 0.3229  loss_mask_2: 0.4018  loss_dice_2: 3.038  loss_ce_3: 0.2964  loss_mask_3: 0.4021  loss_dice_3: 3.024  loss_ce_4: 0.2884  loss_mask_4: 0.4024  loss_dice_4: 3.021  loss_ce_5: 0.2788  loss_mask_5: 0.4025  loss_dice_5: 3.022  loss_ce_6: 0.2832  loss_mask_6: 0.4026  loss_dice_6: 3.013  loss_ce_7: 0.2786  loss_mask_7: 0.4041  loss_dice_7: 3.02  loss_ce_8: 0.2838  loss_mask_8: 0.4016  loss_dice_8: 3.009  time: 1.5345  data_time: 0.1010  lr: 8.3665e-06  max_mem: 21410M
[01/17 11:01:48] d2.utils.events INFO:  eta: 1 day, 7:31:41  iter: 16199  total_loss: 38.59  loss_ce: 0.31  loss_mask: 0.3991  loss_dice: 3.073  loss_ce_0: 0.5892  loss_mask_0: 0.3926  loss_dice_0: 3.198  loss_ce_1: 0.3332  loss_mask_1: 0.4048  loss_dice_1: 3.114  loss_ce_2: 0.3189  loss_mask_2: 0.4002  loss_dice_2: 3.098  loss_ce_3: 0.3233  loss_mask_3: 0.3984  loss_dice_3: 3.078  loss_ce_4: 0.3091  loss_mask_4: 0.3992  loss_dice_4: 3.072  loss_ce_5: 0.3125  loss_mask_5: 0.3974  loss_dice_5: 3.078  loss_ce_6: 0.3041  loss_mask_6: 0.3997  loss_dice_6: 3.071  loss_ce_7: 0.2982  loss_mask_7: 0.3975  loss_dice_7: 3.078  loss_ce_8: 0.3031  loss_mask_8: 0.4002  loss_dice_8: 3.075  time: 1.5346  data_time: 0.1017  lr: 8.3645e-06  max_mem: 21410M
[01/17 11:02:18] d2.utils.events INFO:  eta: 1 day, 7:31:01  iter: 16219  total_loss: 38.02  loss_ce: 0.2868  loss_mask: 0.3973  loss_dice: 3.05  loss_ce_0: 0.5765  loss_mask_0: 0.3881  loss_dice_0: 3.182  loss_ce_1: 0.3106  loss_mask_1: 0.4003  loss_dice_1: 3.094  loss_ce_2: 0.3212  loss_mask_2: 0.3962  loss_dice_2: 3.063  loss_ce_3: 0.3145  loss_mask_3: 0.3995  loss_dice_3: 3.059  loss_ce_4: 0.3019  loss_mask_4: 0.3987  loss_dice_4: 3.061  loss_ce_5: 0.3045  loss_mask_5: 0.3981  loss_dice_5: 3.049  loss_ce_6: 0.2892  loss_mask_6: 0.4  loss_dice_6: 3.045  loss_ce_7: 0.2937  loss_mask_7: 0.3989  loss_dice_7: 3.054  loss_ce_8: 0.2897  loss_mask_8: 0.3965  loss_dice_8: 3.054  time: 1.5346  data_time: 0.1216  lr: 8.3624e-06  max_mem: 21410M
[01/17 11:02:49] d2.utils.events INFO:  eta: 1 day, 7:30:40  iter: 16239  total_loss: 37.92  loss_ce: 0.2751  loss_mask: 0.4193  loss_dice: 3.037  loss_ce_0: 0.5633  loss_mask_0: 0.4076  loss_dice_0: 3.148  loss_ce_1: 0.3114  loss_mask_1: 0.4211  loss_dice_1: 3.075  loss_ce_2: 0.321  loss_mask_2: 0.4209  loss_dice_2: 3.048  loss_ce_3: 0.2877  loss_mask_3: 0.4195  loss_dice_3: 3.036  loss_ce_4: 0.299  loss_mask_4: 0.4192  loss_dice_4: 3.042  loss_ce_5: 0.2911  loss_mask_5: 0.4194  loss_dice_5: 3.05  loss_ce_6: 0.2765  loss_mask_6: 0.4191  loss_dice_6: 3.038  loss_ce_7: 0.2803  loss_mask_7: 0.4195  loss_dice_7: 3.033  loss_ce_8: 0.2718  loss_mask_8: 0.4196  loss_dice_8: 3.041  time: 1.5346  data_time: 0.1019  lr: 8.3604e-06  max_mem: 21410M
[01/17 11:03:20] d2.utils.events INFO:  eta: 1 day, 7:29:35  iter: 16259  total_loss: 38.83  loss_ce: 0.3187  loss_mask: 0.4117  loss_dice: 3.064  loss_ce_0: 0.5806  loss_mask_0: 0.4109  loss_dice_0: 3.191  loss_ce_1: 0.3634  loss_mask_1: 0.415  loss_dice_1: 3.112  loss_ce_2: 0.3747  loss_mask_2: 0.4111  loss_dice_2: 3.097  loss_ce_3: 0.3383  loss_mask_3: 0.4145  loss_dice_3: 3.08  loss_ce_4: 0.3345  loss_mask_4: 0.4135  loss_dice_4: 3.082  loss_ce_5: 0.3091  loss_mask_5: 0.411  loss_dice_5: 3.086  loss_ce_6: 0.3359  loss_mask_6: 0.4123  loss_dice_6: 3.081  loss_ce_7: 0.3128  loss_mask_7: 0.4136  loss_dice_7: 3.083  loss_ce_8: 0.3218  loss_mask_8: 0.4105  loss_dice_8: 3.069  time: 1.5346  data_time: 0.0956  lr: 8.3583e-06  max_mem: 21410M
[01/17 11:03:51] d2.utils.events INFO:  eta: 1 day, 7:29:29  iter: 16279  total_loss: 38.07  loss_ce: 0.305  loss_mask: 0.4058  loss_dice: 3.051  loss_ce_0: 0.5637  loss_mask_0: 0.3965  loss_dice_0: 3.168  loss_ce_1: 0.3015  loss_mask_1: 0.4053  loss_dice_1: 3.084  loss_ce_2: 0.3233  loss_mask_2: 0.404  loss_dice_2: 3.05  loss_ce_3: 0.2977  loss_mask_3: 0.4  loss_dice_3: 3.047  loss_ce_4: 0.2958  loss_mask_4: 0.4017  loss_dice_4: 3.051  loss_ce_5: 0.289  loss_mask_5: 0.4027  loss_dice_5: 3.047  loss_ce_6: 0.2948  loss_mask_6: 0.4046  loss_dice_6: 3.044  loss_ce_7: 0.2896  loss_mask_7: 0.4058  loss_dice_7: 3.047  loss_ce_8: 0.2983  loss_mask_8: 0.4056  loss_dice_8: 3.046  time: 1.5346  data_time: 0.1057  lr: 8.3563e-06  max_mem: 21410M
[01/17 11:04:22] d2.utils.events INFO:  eta: 1 day, 7:28:34  iter: 16299  total_loss: 37.68  loss_ce: 0.298  loss_mask: 0.3967  loss_dice: 3.012  loss_ce_0: 0.5643  loss_mask_0: 0.3926  loss_dice_0: 3.134  loss_ce_1: 0.3195  loss_mask_1: 0.4026  loss_dice_1: 3.057  loss_ce_2: 0.3104  loss_mask_2: 0.3997  loss_dice_2: 3.03  loss_ce_3: 0.3088  loss_mask_3: 0.3981  loss_dice_3: 3.024  loss_ce_4: 0.298  loss_mask_4: 0.3978  loss_dice_4: 3.01  loss_ce_5: 0.2924  loss_mask_5: 0.3983  loss_dice_5: 3.016  loss_ce_6: 0.2872  loss_mask_6: 0.3988  loss_dice_6: 3.002  loss_ce_7: 0.2868  loss_mask_7: 0.3975  loss_dice_7: 3.016  loss_ce_8: 0.2993  loss_mask_8: 0.3958  loss_dice_8: 3.012  time: 1.5346  data_time: 0.0878  lr: 8.3543e-06  max_mem: 21410M
[01/17 11:04:53] d2.utils.events INFO:  eta: 1 day, 7:27:39  iter: 16319  total_loss: 38.16  loss_ce: 0.3135  loss_mask: 0.3964  loss_dice: 3.029  loss_ce_0: 0.5924  loss_mask_0: 0.3899  loss_dice_0: 3.147  loss_ce_1: 0.3538  loss_mask_1: 0.4007  loss_dice_1: 3.055  loss_ce_2: 0.3337  loss_mask_2: 0.3993  loss_dice_2: 3.038  loss_ce_3: 0.319  loss_mask_3: 0.3965  loss_dice_3: 3.031  loss_ce_4: 0.3136  loss_mask_4: 0.3965  loss_dice_4: 3.026  loss_ce_5: 0.3119  loss_mask_5: 0.3961  loss_dice_5: 3.028  loss_ce_6: 0.3141  loss_mask_6: 0.3955  loss_dice_6: 3.029  loss_ce_7: 0.327  loss_mask_7: 0.3971  loss_dice_7: 3.02  loss_ce_8: 0.3259  loss_mask_8: 0.3977  loss_dice_8: 3.02  time: 1.5346  data_time: 0.1125  lr: 8.3522e-06  max_mem: 21410M
[01/17 11:05:24] d2.utils.events INFO:  eta: 1 day, 7:27:43  iter: 16339  total_loss: 37.95  loss_ce: 0.3035  loss_mask: 0.4093  loss_dice: 3.007  loss_ce_0: 0.5769  loss_mask_0: 0.4059  loss_dice_0: 3.128  loss_ce_1: 0.3312  loss_mask_1: 0.4237  loss_dice_1: 3.033  loss_ce_2: 0.3281  loss_mask_2: 0.4139  loss_dice_2: 3.012  loss_ce_3: 0.2957  loss_mask_3: 0.4105  loss_dice_3: 3.012  loss_ce_4: 0.304  loss_mask_4: 0.4091  loss_dice_4: 3.026  loss_ce_5: 0.2977  loss_mask_5: 0.4095  loss_dice_5: 3.018  loss_ce_6: 0.3071  loss_mask_6: 0.41  loss_dice_6: 3.003  loss_ce_7: 0.2935  loss_mask_7: 0.4084  loss_dice_7: 3.005  loss_ce_8: 0.3147  loss_mask_8: 0.4076  loss_dice_8: 3.005  time: 1.5346  data_time: 0.0969  lr: 8.3502e-06  max_mem: 21410M
[01/17 11:05:55] d2.utils.events INFO:  eta: 1 day, 7:27:35  iter: 16359  total_loss: 37.9  loss_ce: 0.2993  loss_mask: 0.4029  loss_dice: 3.032  loss_ce_0: 0.5328  loss_mask_0: 0.4046  loss_dice_0: 3.162  loss_ce_1: 0.3271  loss_mask_1: 0.4158  loss_dice_1: 3.073  loss_ce_2: 0.3154  loss_mask_2: 0.4115  loss_dice_2: 3.041  loss_ce_3: 0.3124  loss_mask_3: 0.4081  loss_dice_3: 3.034  loss_ce_4: 0.2859  loss_mask_4: 0.4065  loss_dice_4: 3.035  loss_ce_5: 0.2867  loss_mask_5: 0.4083  loss_dice_5: 3.033  loss_ce_6: 0.2912  loss_mask_6: 0.4056  loss_dice_6: 3.027  loss_ce_7: 0.2908  loss_mask_7: 0.4036  loss_dice_7: 3.016  loss_ce_8: 0.2893  loss_mask_8: 0.4023  loss_dice_8: 3.024  time: 1.5347  data_time: 0.0920  lr: 8.3481e-06  max_mem: 21410M
[01/17 11:06:26] d2.utils.events INFO:  eta: 1 day, 7:27:51  iter: 16379  total_loss: 38.13  loss_ce: 0.3003  loss_mask: 0.3966  loss_dice: 3.041  loss_ce_0: 0.5782  loss_mask_0: 0.3903  loss_dice_0: 3.188  loss_ce_1: 0.3253  loss_mask_1: 0.4024  loss_dice_1: 3.099  loss_ce_2: 0.3405  loss_mask_2: 0.3962  loss_dice_2: 3.073  loss_ce_3: 0.3137  loss_mask_3: 0.397  loss_dice_3: 3.054  loss_ce_4: 0.3127  loss_mask_4: 0.3973  loss_dice_4: 3.056  loss_ce_5: 0.3101  loss_mask_5: 0.3975  loss_dice_5: 3.057  loss_ce_6: 0.3117  loss_mask_6: 0.3963  loss_dice_6: 3.05  loss_ce_7: 0.3061  loss_mask_7: 0.3997  loss_dice_7: 3.05  loss_ce_8: 0.3056  loss_mask_8: 0.3984  loss_dice_8: 3.049  time: 1.5347  data_time: 0.0868  lr: 8.3461e-06  max_mem: 21410M
[01/17 11:06:57] d2.utils.events INFO:  eta: 1 day, 7:27:27  iter: 16399  total_loss: 37.8  loss_ce: 0.2974  loss_mask: 0.4041  loss_dice: 3.03  loss_ce_0: 0.5543  loss_mask_0: 0.3922  loss_dice_0: 3.147  loss_ce_1: 0.322  loss_mask_1: 0.4059  loss_dice_1: 3.068  loss_ce_2: 0.3011  loss_mask_2: 0.4039  loss_dice_2: 3.038  loss_ce_3: 0.2999  loss_mask_3: 0.4019  loss_dice_3: 3.031  loss_ce_4: 0.2952  loss_mask_4: 0.401  loss_dice_4: 3.036  loss_ce_5: 0.2728  loss_mask_5: 0.4023  loss_dice_5: 3.032  loss_ce_6: 0.2713  loss_mask_6: 0.4035  loss_dice_6: 3.03  loss_ce_7: 0.2906  loss_mask_7: 0.4031  loss_dice_7: 3.032  loss_ce_8: 0.2895  loss_mask_8: 0.4045  loss_dice_8: 3.036  time: 1.5347  data_time: 0.1062  lr: 8.3441e-06  max_mem: 21410M
[01/17 11:07:28] d2.utils.events INFO:  eta: 1 day, 7:27:00  iter: 16419  total_loss: 37.34  loss_ce: 0.305  loss_mask: 0.3981  loss_dice: 2.952  loss_ce_0: 0.5545  loss_mask_0: 0.391  loss_dice_0: 3.089  loss_ce_1: 0.3104  loss_mask_1: 0.4046  loss_dice_1: 3.001  loss_ce_2: 0.3245  loss_mask_2: 0.4011  loss_dice_2: 2.972  loss_ce_3: 0.312  loss_mask_3: 0.3978  loss_dice_3: 2.965  loss_ce_4: 0.3064  loss_mask_4: 0.3971  loss_dice_4: 2.964  loss_ce_5: 0.3076  loss_mask_5: 0.3985  loss_dice_5: 2.964  loss_ce_6: 0.3057  loss_mask_6: 0.3974  loss_dice_6: 2.957  loss_ce_7: 0.3063  loss_mask_7: 0.3974  loss_dice_7: 2.958  loss_ce_8: 0.3022  loss_mask_8: 0.3978  loss_dice_8: 2.958  time: 1.5347  data_time: 0.1030  lr: 8.342e-06  max_mem: 21410M
[01/17 11:07:59] d2.utils.events INFO:  eta: 1 day, 7:27:31  iter: 16439  total_loss: 39.31  loss_ce: 0.3321  loss_mask: 0.409  loss_dice: 3.13  loss_ce_0: 0.5965  loss_mask_0: 0.4157  loss_dice_0: 3.247  loss_ce_1: 0.3605  loss_mask_1: 0.4208  loss_dice_1: 3.164  loss_ce_2: 0.3594  loss_mask_2: 0.4134  loss_dice_2: 3.146  loss_ce_3: 0.3483  loss_mask_3: 0.4127  loss_dice_3: 3.143  loss_ce_4: 0.3518  loss_mask_4: 0.4108  loss_dice_4: 3.133  loss_ce_5: 0.3461  loss_mask_5: 0.4103  loss_dice_5: 3.133  loss_ce_6: 0.3322  loss_mask_6: 0.4101  loss_dice_6: 3.128  loss_ce_7: 0.3331  loss_mask_7: 0.4104  loss_dice_7: 3.124  loss_ce_8: 0.3284  loss_mask_8: 0.4114  loss_dice_8: 3.123  time: 1.5347  data_time: 0.0975  lr: 8.34e-06  max_mem: 21410M
[01/17 11:08:30] d2.utils.events INFO:  eta: 1 day, 7:27:51  iter: 16459  total_loss: 37.85  loss_ce: 0.3109  loss_mask: 0.3974  loss_dice: 3.013  loss_ce_0: 0.6021  loss_mask_0: 0.3999  loss_dice_0: 3.147  loss_ce_1: 0.3443  loss_mask_1: 0.4012  loss_dice_1: 3.056  loss_ce_2: 0.3609  loss_mask_2: 0.3989  loss_dice_2: 3.022  loss_ce_3: 0.3248  loss_mask_3: 0.3996  loss_dice_3: 3.011  loss_ce_4: 0.3226  loss_mask_4: 0.3981  loss_dice_4: 3.013  loss_ce_5: 0.3338  loss_mask_5: 0.399  loss_dice_5: 3.019  loss_ce_6: 0.3178  loss_mask_6: 0.3977  loss_dice_6: 3.016  loss_ce_7: 0.309  loss_mask_7: 0.3969  loss_dice_7: 3.015  loss_ce_8: 0.3059  loss_mask_8: 0.3958  loss_dice_8: 3.013  time: 1.5347  data_time: 0.1036  lr: 8.3379e-06  max_mem: 21410M
[01/17 11:09:01] d2.utils.events INFO:  eta: 1 day, 7:27:20  iter: 16479  total_loss: 37.66  loss_ce: 0.2883  loss_mask: 0.4009  loss_dice: 3.032  loss_ce_0: 0.5749  loss_mask_0: 0.3875  loss_dice_0: 3.144  loss_ce_1: 0.3143  loss_mask_1: 0.4041  loss_dice_1: 3.067  loss_ce_2: 0.3213  loss_mask_2: 0.3999  loss_dice_2: 3.04  loss_ce_3: 0.3053  loss_mask_3: 0.397  loss_dice_3: 3.037  loss_ce_4: 0.3016  loss_mask_4: 0.3987  loss_dice_4: 3.029  loss_ce_5: 0.2863  loss_mask_5: 0.4009  loss_dice_5: 3.033  loss_ce_6: 0.2997  loss_mask_6: 0.3982  loss_dice_6: 3.029  loss_ce_7: 0.2817  loss_mask_7: 0.3998  loss_dice_7: 3.029  loss_ce_8: 0.2825  loss_mask_8: 0.4019  loss_dice_8: 3.023  time: 1.5347  data_time: 0.1064  lr: 8.3359e-06  max_mem: 21410M
[01/17 11:09:32] d2.utils.events INFO:  eta: 1 day, 7:26:50  iter: 16499  total_loss: 38.07  loss_ce: 0.2837  loss_mask: 0.4058  loss_dice: 3.059  loss_ce_0: 0.5605  loss_mask_0: 0.4089  loss_dice_0: 3.172  loss_ce_1: 0.3195  loss_mask_1: 0.4134  loss_dice_1: 3.084  loss_ce_2: 0.3202  loss_mask_2: 0.4081  loss_dice_2: 3.058  loss_ce_3: 0.3025  loss_mask_3: 0.4089  loss_dice_3: 3.051  loss_ce_4: 0.2955  loss_mask_4: 0.4066  loss_dice_4: 3.063  loss_ce_5: 0.2959  loss_mask_5: 0.4097  loss_dice_5: 3.057  loss_ce_6: 0.2895  loss_mask_6: 0.4096  loss_dice_6: 3.054  loss_ce_7: 0.2826  loss_mask_7: 0.407  loss_dice_7: 3.05  loss_ce_8: 0.2798  loss_mask_8: 0.4059  loss_dice_8: 3.058  time: 1.5348  data_time: 0.1027  lr: 8.3338e-06  max_mem: 21410M
[01/17 11:10:03] d2.utils.events INFO:  eta: 1 day, 7:26:39  iter: 16519  total_loss: 37.68  loss_ce: 0.3094  loss_mask: 0.4027  loss_dice: 3.001  loss_ce_0: 0.5489  loss_mask_0: 0.3998  loss_dice_0: 3.132  loss_ce_1: 0.3186  loss_mask_1: 0.409  loss_dice_1: 3.049  loss_ce_2: 0.3286  loss_mask_2: 0.4036  loss_dice_2: 3.024  loss_ce_3: 0.3154  loss_mask_3: 0.4014  loss_dice_3: 3.011  loss_ce_4: 0.3171  loss_mask_4: 0.4005  loss_dice_4: 3.011  loss_ce_5: 0.317  loss_mask_5: 0.4008  loss_dice_5: 3.002  loss_ce_6: 0.2987  loss_mask_6: 0.4003  loss_dice_6: 3.003  loss_ce_7: 0.2961  loss_mask_7: 0.4006  loss_dice_7: 3.003  loss_ce_8: 0.2975  loss_mask_8: 0.4025  loss_dice_8: 3.008  time: 1.5348  data_time: 0.0958  lr: 8.3318e-06  max_mem: 21410M
[01/17 11:10:34] d2.utils.events INFO:  eta: 1 day, 7:26:19  iter: 16539  total_loss: 37.79  loss_ce: 0.2867  loss_mask: 0.401  loss_dice: 3.019  loss_ce_0: 0.545  loss_mask_0: 0.3922  loss_dice_0: 3.142  loss_ce_1: 0.331  loss_mask_1: 0.397  loss_dice_1: 3.064  loss_ce_2: 0.3136  loss_mask_2: 0.3974  loss_dice_2: 3.036  loss_ce_3: 0.3018  loss_mask_3: 0.3964  loss_dice_3: 3.031  loss_ce_4: 0.3037  loss_mask_4: 0.3954  loss_dice_4: 3.028  loss_ce_5: 0.2947  loss_mask_5: 0.3994  loss_dice_5: 3.025  loss_ce_6: 0.3038  loss_mask_6: 0.3982  loss_dice_6: 3.018  loss_ce_7: 0.2927  loss_mask_7: 0.3986  loss_dice_7: 3.02  loss_ce_8: 0.2952  loss_mask_8: 0.3989  loss_dice_8: 3.024  time: 1.5348  data_time: 0.1067  lr: 8.3298e-06  max_mem: 21410M
[01/17 11:11:05] d2.utils.events INFO:  eta: 1 day, 7:24:41  iter: 16559  total_loss: 37.84  loss_ce: 0.295  loss_mask: 0.4  loss_dice: 3.017  loss_ce_0: 0.5508  loss_mask_0: 0.3971  loss_dice_0: 3.156  loss_ce_1: 0.3065  loss_mask_1: 0.4055  loss_dice_1: 3.054  loss_ce_2: 0.3335  loss_mask_2: 0.3996  loss_dice_2: 3.035  loss_ce_3: 0.3086  loss_mask_3: 0.3967  loss_dice_3: 3.031  loss_ce_4: 0.3009  loss_mask_4: 0.3995  loss_dice_4: 3.029  loss_ce_5: 0.2863  loss_mask_5: 0.4007  loss_dice_5: 3.036  loss_ce_6: 0.279  loss_mask_6: 0.4023  loss_dice_6: 3.03  loss_ce_7: 0.2858  loss_mask_7: 0.3983  loss_dice_7: 3.025  loss_ce_8: 0.2862  loss_mask_8: 0.396  loss_dice_8: 3.024  time: 1.5348  data_time: 0.0969  lr: 8.3277e-06  max_mem: 21410M
[01/17 11:11:36] d2.utils.events INFO:  eta: 1 day, 7:24:33  iter: 16579  total_loss: 37.82  loss_ce: 0.2994  loss_mask: 0.3939  loss_dice: 3.018  loss_ce_0: 0.5814  loss_mask_0: 0.3923  loss_dice_0: 3.146  loss_ce_1: 0.2942  loss_mask_1: 0.4011  loss_dice_1: 3.059  loss_ce_2: 0.307  loss_mask_2: 0.3992  loss_dice_2: 3.036  loss_ce_3: 0.3002  loss_mask_3: 0.3948  loss_dice_3: 3.03  loss_ce_4: 0.2959  loss_mask_4: 0.3971  loss_dice_4: 3.02  loss_ce_5: 0.2863  loss_mask_5: 0.3961  loss_dice_5: 3.026  loss_ce_6: 0.3034  loss_mask_6: 0.3955  loss_dice_6: 3.026  loss_ce_7: 0.2864  loss_mask_7: 0.396  loss_dice_7: 3.015  loss_ce_8: 0.2835  loss_mask_8: 0.3947  loss_dice_8: 3.018  time: 1.5348  data_time: 0.1028  lr: 8.3257e-06  max_mem: 21410M
[01/17 11:12:07] d2.utils.events INFO:  eta: 1 day, 7:23:52  iter: 16599  total_loss: 37.95  loss_ce: 0.2975  loss_mask: 0.4156  loss_dice: 3.026  loss_ce_0: 0.5724  loss_mask_0: 0.4054  loss_dice_0: 3.146  loss_ce_1: 0.3234  loss_mask_1: 0.4192  loss_dice_1: 3.068  loss_ce_2: 0.3243  loss_mask_2: 0.4154  loss_dice_2: 3.05  loss_ce_3: 0.3187  loss_mask_3: 0.4156  loss_dice_3: 3.026  loss_ce_4: 0.3025  loss_mask_4: 0.4167  loss_dice_4: 3.015  loss_ce_5: 0.2951  loss_mask_5: 0.4171  loss_dice_5: 3.024  loss_ce_6: 0.3025  loss_mask_6: 0.4141  loss_dice_6: 3.021  loss_ce_7: 0.3018  loss_mask_7: 0.4143  loss_dice_7: 3.029  loss_ce_8: 0.3021  loss_mask_8: 0.4163  loss_dice_8: 3.029  time: 1.5348  data_time: 0.1101  lr: 8.3236e-06  max_mem: 21410M
[01/17 11:12:38] d2.utils.events INFO:  eta: 1 day, 7:24:05  iter: 16619  total_loss: 38.11  loss_ce: 0.3079  loss_mask: 0.3968  loss_dice: 3.051  loss_ce_0: 0.5901  loss_mask_0: 0.3878  loss_dice_0: 3.17  loss_ce_1: 0.3284  loss_mask_1: 0.3989  loss_dice_1: 3.098  loss_ce_2: 0.3387  loss_mask_2: 0.4016  loss_dice_2: 3.067  loss_ce_3: 0.3223  loss_mask_3: 0.3995  loss_dice_3: 3.06  loss_ce_4: 0.3163  loss_mask_4: 0.3995  loss_dice_4: 3.064  loss_ce_5: 0.3046  loss_mask_5: 0.3998  loss_dice_5: 3.059  loss_ce_6: 0.3123  loss_mask_6: 0.3982  loss_dice_6: 3.054  loss_ce_7: 0.3105  loss_mask_7: 0.3963  loss_dice_7: 3.049  loss_ce_8: 0.3065  loss_mask_8: 0.3975  loss_dice_8: 3.052  time: 1.5348  data_time: 0.1056  lr: 8.3216e-06  max_mem: 21410M
[01/17 11:13:09] d2.utils.events INFO:  eta: 1 day, 7:24:46  iter: 16639  total_loss: 37.71  loss_ce: 0.268  loss_mask: 0.3969  loss_dice: 3.044  loss_ce_0: 0.5398  loss_mask_0: 0.387  loss_dice_0: 3.142  loss_ce_1: 0.2966  loss_mask_1: 0.397  loss_dice_1: 3.065  loss_ce_2: 0.2797  loss_mask_2: 0.3962  loss_dice_2: 3.046  loss_ce_3: 0.2769  loss_mask_3: 0.394  loss_dice_3: 3.042  loss_ce_4: 0.2701  loss_mask_4: 0.3929  loss_dice_4: 3.047  loss_ce_5: 0.2673  loss_mask_5: 0.3939  loss_dice_5: 3.049  loss_ce_6: 0.2637  loss_mask_6: 0.3935  loss_dice_6: 3.049  loss_ce_7: 0.2726  loss_mask_7: 0.3963  loss_dice_7: 3.043  loss_ce_8: 0.2729  loss_mask_8: 0.3963  loss_dice_8: 3.042  time: 1.5349  data_time: 0.0983  lr: 8.3196e-06  max_mem: 21410M
[01/17 11:13:40] d2.utils.events INFO:  eta: 1 day, 7:25:19  iter: 16659  total_loss: 38.12  loss_ce: 0.2755  loss_mask: 0.3983  loss_dice: 3.06  loss_ce_0: 0.5666  loss_mask_0: 0.3972  loss_dice_0: 3.173  loss_ce_1: 0.3324  loss_mask_1: 0.4096  loss_dice_1: 3.098  loss_ce_2: 0.319  loss_mask_2: 0.4034  loss_dice_2: 3.071  loss_ce_3: 0.2899  loss_mask_3: 0.4005  loss_dice_3: 3.067  loss_ce_4: 0.2875  loss_mask_4: 0.402  loss_dice_4: 3.069  loss_ce_5: 0.271  loss_mask_5: 0.402  loss_dice_5: 3.07  loss_ce_6: 0.2671  loss_mask_6: 0.4038  loss_dice_6: 3.063  loss_ce_7: 0.2687  loss_mask_7: 0.4004  loss_dice_7: 3.068  loss_ce_8: 0.2818  loss_mask_8: 0.4004  loss_dice_8: 3.06  time: 1.5349  data_time: 0.0970  lr: 8.3175e-06  max_mem: 21410M
[01/17 11:14:11] d2.utils.events INFO:  eta: 1 day, 7:25:56  iter: 16679  total_loss: 38.34  loss_ce: 0.3276  loss_mask: 0.415  loss_dice: 3.062  loss_ce_0: 0.616  loss_mask_0: 0.4059  loss_dice_0: 3.146  loss_ce_1: 0.3441  loss_mask_1: 0.412  loss_dice_1: 3.088  loss_ce_2: 0.3299  loss_mask_2: 0.41  loss_dice_2: 3.07  loss_ce_3: 0.3296  loss_mask_3: 0.409  loss_dice_3: 3.061  loss_ce_4: 0.3394  loss_mask_4: 0.41  loss_dice_4: 3.058  loss_ce_5: 0.3214  loss_mask_5: 0.4127  loss_dice_5: 3.065  loss_ce_6: 0.3191  loss_mask_6: 0.4164  loss_dice_6: 3.056  loss_ce_7: 0.3137  loss_mask_7: 0.4103  loss_dice_7: 3.059  loss_ce_8: 0.3225  loss_mask_8: 0.4108  loss_dice_8: 3.055  time: 1.5349  data_time: 0.1135  lr: 8.3155e-06  max_mem: 21410M
[01/17 11:14:42] d2.utils.events INFO:  eta: 1 day, 7:26:21  iter: 16699  total_loss: 38.01  loss_ce: 0.3077  loss_mask: 0.4092  loss_dice: 3.027  loss_ce_0: 0.5796  loss_mask_0: 0.4061  loss_dice_0: 3.161  loss_ce_1: 0.3168  loss_mask_1: 0.4161  loss_dice_1: 3.071  loss_ce_2: 0.3196  loss_mask_2: 0.4127  loss_dice_2: 3.038  loss_ce_3: 0.3176  loss_mask_3: 0.4105  loss_dice_3: 3.034  loss_ce_4: 0.3078  loss_mask_4: 0.4075  loss_dice_4: 3.035  loss_ce_5: 0.3087  loss_mask_5: 0.4106  loss_dice_5: 3.03  loss_ce_6: 0.3061  loss_mask_6: 0.4095  loss_dice_6: 3.032  loss_ce_7: 0.297  loss_mask_7: 0.4083  loss_dice_7: 3.035  loss_ce_8: 0.3014  loss_mask_8: 0.4093  loss_dice_8: 3.021  time: 1.5349  data_time: 0.0908  lr: 8.3134e-06  max_mem: 21410M
[01/17 11:15:14] d2.utils.events INFO:  eta: 1 day, 7:25:29  iter: 16719  total_loss: 37.5  loss_ce: 0.3089  loss_mask: 0.3957  loss_dice: 2.998  loss_ce_0: 0.5673  loss_mask_0: 0.3879  loss_dice_0: 3.105  loss_ce_1: 0.3063  loss_mask_1: 0.4009  loss_dice_1: 3.033  loss_ce_2: 0.3317  loss_mask_2: 0.3968  loss_dice_2: 3.009  loss_ce_3: 0.3215  loss_mask_3: 0.3924  loss_dice_3: 3.001  loss_ce_4: 0.3179  loss_mask_4: 0.393  loss_dice_4: 3.004  loss_ce_5: 0.3027  loss_mask_5: 0.3937  loss_dice_5: 2.997  loss_ce_6: 0.3052  loss_mask_6: 0.3946  loss_dice_6: 3.003  loss_ce_7: 0.3103  loss_mask_7: 0.3965  loss_dice_7: 2.997  loss_ce_8: 0.3093  loss_mask_8: 0.3947  loss_dice_8: 2.996  time: 1.5349  data_time: 0.1019  lr: 8.3114e-06  max_mem: 21410M
[01/17 11:15:44] d2.utils.events INFO:  eta: 1 day, 7:24:23  iter: 16739  total_loss: 37.1  loss_ce: 0.2895  loss_mask: 0.4146  loss_dice: 2.961  loss_ce_0: 0.5466  loss_mask_0: 0.3987  loss_dice_0: 3.087  loss_ce_1: 0.3072  loss_mask_1: 0.41  loss_dice_1: 3.004  loss_ce_2: 0.3239  loss_mask_2: 0.4131  loss_dice_2: 2.98  loss_ce_3: 0.3021  loss_mask_3: 0.4142  loss_dice_3: 2.967  loss_ce_4: 0.301  loss_mask_4: 0.4141  loss_dice_4: 2.965  loss_ce_5: 0.2918  loss_mask_5: 0.4132  loss_dice_5: 2.969  loss_ce_6: 0.2933  loss_mask_6: 0.4163  loss_dice_6: 2.961  loss_ce_7: 0.2988  loss_mask_7: 0.4137  loss_dice_7: 2.963  loss_ce_8: 0.281  loss_mask_8: 0.4158  loss_dice_8: 2.962  time: 1.5349  data_time: 0.0927  lr: 8.3094e-06  max_mem: 21410M
[01/17 11:16:15] d2.utils.events INFO:  eta: 1 day, 7:21:49  iter: 16759  total_loss: 37.46  loss_ce: 0.289  loss_mask: 0.3898  loss_dice: 3.017  loss_ce_0: 0.5511  loss_mask_0: 0.3888  loss_dice_0: 3.138  loss_ce_1: 0.3234  loss_mask_1: 0.4005  loss_dice_1: 3.057  loss_ce_2: 0.3055  loss_mask_2: 0.3952  loss_dice_2: 3.032  loss_ce_3: 0.3058  loss_mask_3: 0.3918  loss_dice_3: 3.031  loss_ce_4: 0.3  loss_mask_4: 0.3925  loss_dice_4: 3.023  loss_ce_5: 0.2985  loss_mask_5: 0.3913  loss_dice_5: 3.026  loss_ce_6: 0.2908  loss_mask_6: 0.3885  loss_dice_6: 3.024  loss_ce_7: 0.2868  loss_mask_7: 0.3882  loss_dice_7: 3.021  loss_ce_8: 0.2948  loss_mask_8: 0.3894  loss_dice_8: 3.021  time: 1.5349  data_time: 0.1011  lr: 8.3073e-06  max_mem: 21410M
[01/17 11:16:46] d2.utils.events INFO:  eta: 1 day, 7:21:28  iter: 16779  total_loss: 37.15  loss_ce: 0.2889  loss_mask: 0.3947  loss_dice: 2.975  loss_ce_0: 0.5806  loss_mask_0: 0.3893  loss_dice_0: 3.1  loss_ce_1: 0.3039  loss_mask_1: 0.3988  loss_dice_1: 3.019  loss_ce_2: 0.3024  loss_mask_2: 0.398  loss_dice_2: 2.979  loss_ce_3: 0.2941  loss_mask_3: 0.3942  loss_dice_3: 2.981  loss_ce_4: 0.2903  loss_mask_4: 0.3939  loss_dice_4: 2.978  loss_ce_5: 0.2755  loss_mask_5: 0.3923  loss_dice_5: 2.987  loss_ce_6: 0.2668  loss_mask_6: 0.3927  loss_dice_6: 2.98  loss_ce_7: 0.2815  loss_mask_7: 0.3934  loss_dice_7: 2.973  loss_ce_8: 0.2861  loss_mask_8: 0.395  loss_dice_8: 2.977  time: 1.5349  data_time: 0.1036  lr: 8.3053e-06  max_mem: 21410M
[01/17 11:17:17] d2.utils.events INFO:  eta: 1 day, 7:21:43  iter: 16799  total_loss: 37.31  loss_ce: 0.2905  loss_mask: 0.4016  loss_dice: 3.023  loss_ce_0: 0.5678  loss_mask_0: 0.3899  loss_dice_0: 3.129  loss_ce_1: 0.307  loss_mask_1: 0.4051  loss_dice_1: 3.06  loss_ce_2: 0.2978  loss_mask_2: 0.4005  loss_dice_2: 3.033  loss_ce_3: 0.2825  loss_mask_3: 0.3999  loss_dice_3: 3.022  loss_ce_4: 0.2961  loss_mask_4: 0.4014  loss_dice_4: 3.029  loss_ce_5: 0.2782  loss_mask_5: 0.3995  loss_dice_5: 3.028  loss_ce_6: 0.2953  loss_mask_6: 0.4  loss_dice_6: 3.018  loss_ce_7: 0.27  loss_mask_7: 0.4022  loss_dice_7: 3.024  loss_ce_8: 0.2766  loss_mask_8: 0.4016  loss_dice_8: 3.015  time: 1.5350  data_time: 0.0984  lr: 8.3032e-06  max_mem: 21410M
[01/17 11:17:48] d2.utils.events INFO:  eta: 1 day, 7:23:05  iter: 16819  total_loss: 37.32  loss_ce: 0.2807  loss_mask: 0.3996  loss_dice: 2.993  loss_ce_0: 0.5754  loss_mask_0: 0.3863  loss_dice_0: 3.116  loss_ce_1: 0.2931  loss_mask_1: 0.3973  loss_dice_1: 3.028  loss_ce_2: 0.2994  loss_mask_2: 0.3983  loss_dice_2: 3.004  loss_ce_3: 0.2835  loss_mask_3: 0.4016  loss_dice_3: 2.995  loss_ce_4: 0.2776  loss_mask_4: 0.4005  loss_dice_4: 2.987  loss_ce_5: 0.2775  loss_mask_5: 0.3975  loss_dice_5: 2.996  loss_ce_6: 0.2687  loss_mask_6: 0.3986  loss_dice_6: 2.982  loss_ce_7: 0.2688  loss_mask_7: 0.3978  loss_dice_7: 2.985  loss_ce_8: 0.2625  loss_mask_8: 0.3971  loss_dice_8: 2.982  time: 1.5350  data_time: 0.0939  lr: 8.3012e-06  max_mem: 21410M
[01/17 11:18:19] d2.utils.events INFO:  eta: 1 day, 7:25:01  iter: 16839  total_loss: 36.92  loss_ce: 0.2963  loss_mask: 0.3981  loss_dice: 2.954  loss_ce_0: 0.5883  loss_mask_0: 0.3865  loss_dice_0: 3.111  loss_ce_1: 0.3401  loss_mask_1: 0.4006  loss_dice_1: 3.013  loss_ce_2: 0.338  loss_mask_2: 0.3973  loss_dice_2: 2.974  loss_ce_3: 0.3145  loss_mask_3: 0.396  loss_dice_3: 2.96  loss_ce_4: 0.3325  loss_mask_4: 0.3986  loss_dice_4: 2.961  loss_ce_5: 0.3257  loss_mask_5: 0.4003  loss_dice_5: 2.962  loss_ce_6: 0.3091  loss_mask_6: 0.3988  loss_dice_6: 2.958  loss_ce_7: 0.317  loss_mask_7: 0.3984  loss_dice_7: 2.963  loss_ce_8: 0.3057  loss_mask_8: 0.3995  loss_dice_8: 2.957  time: 1.5350  data_time: 0.0975  lr: 8.2991e-06  max_mem: 21410M
[01/17 11:18:50] d2.utils.events INFO:  eta: 1 day, 7:24:30  iter: 16859  total_loss: 37.39  loss_ce: 0.2648  loss_mask: 0.3896  loss_dice: 3.011  loss_ce_0: 0.5673  loss_mask_0: 0.3824  loss_dice_0: 3.149  loss_ce_1: 0.2908  loss_mask_1: 0.3993  loss_dice_1: 3.058  loss_ce_2: 0.2876  loss_mask_2: 0.3938  loss_dice_2: 3.032  loss_ce_3: 0.2804  loss_mask_3: 0.3897  loss_dice_3: 3.01  loss_ce_4: 0.2772  loss_mask_4: 0.3918  loss_dice_4: 3.026  loss_ce_5: 0.2637  loss_mask_5: 0.3918  loss_dice_5: 3.017  loss_ce_6: 0.2641  loss_mask_6: 0.3891  loss_dice_6: 3.016  loss_ce_7: 0.2664  loss_mask_7: 0.3882  loss_dice_7: 3.023  loss_ce_8: 0.2768  loss_mask_8: 0.3902  loss_dice_8: 3.021  time: 1.5350  data_time: 0.1026  lr: 8.2971e-06  max_mem: 21410M
[01/17 11:19:21] d2.utils.events INFO:  eta: 1 day, 7:24:21  iter: 16879  total_loss: 37.92  loss_ce: 0.2854  loss_mask: 0.3988  loss_dice: 3.008  loss_ce_0: 0.5792  loss_mask_0: 0.3892  loss_dice_0: 3.13  loss_ce_1: 0.3309  loss_mask_1: 0.4042  loss_dice_1: 3.045  loss_ce_2: 0.3058  loss_mask_2: 0.4012  loss_dice_2: 3.024  loss_ce_3: 0.2916  loss_mask_3: 0.4003  loss_dice_3: 3.007  loss_ce_4: 0.2977  loss_mask_4: 0.4002  loss_dice_4: 3.008  loss_ce_5: 0.2948  loss_mask_5: 0.3985  loss_dice_5: 3.017  loss_ce_6: 0.2869  loss_mask_6: 0.4015  loss_dice_6: 3.017  loss_ce_7: 0.287  loss_mask_7: 0.3991  loss_dice_7: 3.003  loss_ce_8: 0.2902  loss_mask_8: 0.3988  loss_dice_8: 3.016  time: 1.5350  data_time: 0.1003  lr: 8.2951e-06  max_mem: 21410M
[01/17 11:19:52] d2.utils.events INFO:  eta: 1 day, 7:24:18  iter: 16899  total_loss: 37.16  loss_ce: 0.302  loss_mask: 0.3996  loss_dice: 2.944  loss_ce_0: 0.5616  loss_mask_0: 0.3933  loss_dice_0: 3.082  loss_ce_1: 0.3286  loss_mask_1: 0.4082  loss_dice_1: 2.993  loss_ce_2: 0.322  loss_mask_2: 0.4074  loss_dice_2: 2.962  loss_ce_3: 0.292  loss_mask_3: 0.4019  loss_dice_3: 2.946  loss_ce_4: 0.2943  loss_mask_4: 0.4004  loss_dice_4: 2.959  loss_ce_5: 0.2867  loss_mask_5: 0.3983  loss_dice_5: 2.96  loss_ce_6: 0.2932  loss_mask_6: 0.3998  loss_dice_6: 2.95  loss_ce_7: 0.2939  loss_mask_7: 0.4006  loss_dice_7: 2.954  loss_ce_8: 0.2947  loss_mask_8: 0.3998  loss_dice_8: 2.943  time: 1.5350  data_time: 0.0981  lr: 8.293e-06  max_mem: 21410M
[01/17 11:20:23] d2.utils.events INFO:  eta: 1 day, 7:24:33  iter: 16919  total_loss: 36.88  loss_ce: 0.2656  loss_mask: 0.3974  loss_dice: 2.936  loss_ce_0: 0.5561  loss_mask_0: 0.3825  loss_dice_0: 3.078  loss_ce_1: 0.3036  loss_mask_1: 0.3961  loss_dice_1: 2.996  loss_ce_2: 0.3044  loss_mask_2: 0.3959  loss_dice_2: 2.965  loss_ce_3: 0.3016  loss_mask_3: 0.3975  loss_dice_3: 2.954  loss_ce_4: 0.2912  loss_mask_4: 0.3985  loss_dice_4: 2.941  loss_ce_5: 0.2762  loss_mask_5: 0.3987  loss_dice_5: 2.95  loss_ce_6: 0.2724  loss_mask_6: 0.3963  loss_dice_6: 2.942  loss_ce_7: 0.2691  loss_mask_7: 0.3969  loss_dice_7: 2.941  loss_ce_8: 0.2782  loss_mask_8: 0.3977  loss_dice_8: 2.947  time: 1.5351  data_time: 0.0909  lr: 8.291e-06  max_mem: 21410M
[01/17 11:20:54] d2.utils.events INFO:  eta: 1 day, 7:24:18  iter: 16939  total_loss: 39.49  loss_ce: 0.3217  loss_mask: 0.4479  loss_dice: 3.143  loss_ce_0: 0.5974  loss_mask_0: 0.4516  loss_dice_0: 3.238  loss_ce_1: 0.3513  loss_mask_1: 0.4656  loss_dice_1: 3.179  loss_ce_2: 0.3537  loss_mask_2: 0.4526  loss_dice_2: 3.148  loss_ce_3: 0.3204  loss_mask_3: 0.449  loss_dice_3: 3.136  loss_ce_4: 0.3261  loss_mask_4: 0.4496  loss_dice_4: 3.146  loss_ce_5: 0.3328  loss_mask_5: 0.4463  loss_dice_5: 3.143  loss_ce_6: 0.317  loss_mask_6: 0.4474  loss_dice_6: 3.148  loss_ce_7: 0.3105  loss_mask_7: 0.4452  loss_dice_7: 3.15  loss_ce_8: 0.324  loss_mask_8: 0.4469  loss_dice_8: 3.139  time: 1.5351  data_time: 0.0942  lr: 8.2889e-06  max_mem: 21410M
[01/17 11:21:25] d2.utils.events INFO:  eta: 1 day, 7:23:47  iter: 16959  total_loss: 37.89  loss_ce: 0.3089  loss_mask: 0.4097  loss_dice: 3.051  loss_ce_0: 0.556  loss_mask_0: 0.4095  loss_dice_0: 3.158  loss_ce_1: 0.3357  loss_mask_1: 0.4163  loss_dice_1: 3.085  loss_ce_2: 0.3322  loss_mask_2: 0.4118  loss_dice_2: 3.066  loss_ce_3: 0.3225  loss_mask_3: 0.4084  loss_dice_3: 3.055  loss_ce_4: 0.304  loss_mask_4: 0.4105  loss_dice_4: 3.053  loss_ce_5: 0.303  loss_mask_5: 0.4111  loss_dice_5: 3.059  loss_ce_6: 0.3129  loss_mask_6: 0.4108  loss_dice_6: 3.053  loss_ce_7: 0.2854  loss_mask_7: 0.4127  loss_dice_7: 3.052  loss_ce_8: 0.2972  loss_mask_8: 0.4099  loss_dice_8: 3.051  time: 1.5351  data_time: 0.1107  lr: 8.2869e-06  max_mem: 21410M
[01/17 11:21:56] d2.utils.events INFO:  eta: 1 day, 7:23:16  iter: 16979  total_loss: 37.98  loss_ce: 0.2887  loss_mask: 0.3895  loss_dice: 3.013  loss_ce_0: 0.5774  loss_mask_0: 0.3897  loss_dice_0: 3.148  loss_ce_1: 0.3211  loss_mask_1: 0.3944  loss_dice_1: 3.053  loss_ce_2: 0.3145  loss_mask_2: 0.39  loss_dice_2: 3.027  loss_ce_3: 0.3131  loss_mask_3: 0.3895  loss_dice_3: 3.025  loss_ce_4: 0.3083  loss_mask_4: 0.3889  loss_dice_4: 3.017  loss_ce_5: 0.2994  loss_mask_5: 0.3879  loss_dice_5: 3.018  loss_ce_6: 0.2945  loss_mask_6: 0.389  loss_dice_6: 3.011  loss_ce_7: 0.2928  loss_mask_7: 0.3887  loss_dice_7: 3.016  loss_ce_8: 0.2806  loss_mask_8: 0.3874  loss_dice_8: 3.005  time: 1.5351  data_time: 0.0989  lr: 8.2849e-06  max_mem: 21410M
[01/17 11:22:26] d2.utils.events INFO:  eta: 1 day, 7:22:45  iter: 16999  total_loss: 36.75  loss_ce: 0.2826  loss_mask: 0.3887  loss_dice: 2.961  loss_ce_0: 0.5577  loss_mask_0: 0.3794  loss_dice_0: 3.089  loss_ce_1: 0.3157  loss_mask_1: 0.3904  loss_dice_1: 3.006  loss_ce_2: 0.3101  loss_mask_2: 0.387  loss_dice_2: 2.985  loss_ce_3: 0.2906  loss_mask_3: 0.3868  loss_dice_3: 2.98  loss_ce_4: 0.2897  loss_mask_4: 0.3863  loss_dice_4: 2.97  loss_ce_5: 0.2692  loss_mask_5: 0.3873  loss_dice_5: 2.976  loss_ce_6: 0.2719  loss_mask_6: 0.387  loss_dice_6: 2.977  loss_ce_7: 0.2653  loss_mask_7: 0.3879  loss_dice_7: 2.966  loss_ce_8: 0.2734  loss_mask_8: 0.3883  loss_dice_8: 2.963  time: 1.5351  data_time: 0.1061  lr: 8.2828e-06  max_mem: 21410M
[01/17 11:22:57] d2.utils.events INFO:  eta: 1 day, 7:21:27  iter: 17019  total_loss: 37.3  loss_ce: 0.2867  loss_mask: 0.4132  loss_dice: 2.974  loss_ce_0: 0.5648  loss_mask_0: 0.4135  loss_dice_0: 3.074  loss_ce_1: 0.3086  loss_mask_1: 0.4252  loss_dice_1: 3.005  loss_ce_2: 0.2949  loss_mask_2: 0.4191  loss_dice_2: 2.99  loss_ce_3: 0.2853  loss_mask_3: 0.4165  loss_dice_3: 2.986  loss_ce_4: 0.2986  loss_mask_4: 0.4172  loss_dice_4: 2.976  loss_ce_5: 0.2815  loss_mask_5: 0.4172  loss_dice_5: 2.973  loss_ce_6: 0.2807  loss_mask_6: 0.4149  loss_dice_6: 2.981  loss_ce_7: 0.2849  loss_mask_7: 0.4159  loss_dice_7: 2.981  loss_ce_8: 0.2811  loss_mask_8: 0.4145  loss_dice_8: 2.972  time: 1.5351  data_time: 0.0900  lr: 8.2808e-06  max_mem: 21410M
[01/17 11:23:28] d2.utils.events INFO:  eta: 1 day, 7:21:13  iter: 17039  total_loss: 37.34  loss_ce: 0.3121  loss_mask: 0.3822  loss_dice: 2.954  loss_ce_0: 0.6144  loss_mask_0: 0.3802  loss_dice_0: 3.084  loss_ce_1: 0.3252  loss_mask_1: 0.3888  loss_dice_1: 2.998  loss_ce_2: 0.3315  loss_mask_2: 0.3852  loss_dice_2: 2.972  loss_ce_3: 0.3161  loss_mask_3: 0.385  loss_dice_3: 2.961  loss_ce_4: 0.2987  loss_mask_4: 0.384  loss_dice_4: 2.964  loss_ce_5: 0.3136  loss_mask_5: 0.3845  loss_dice_5: 2.966  loss_ce_6: 0.3272  loss_mask_6: 0.3844  loss_dice_6: 2.96  loss_ce_7: 0.3105  loss_mask_7: 0.3837  loss_dice_7: 2.964  loss_ce_8: 0.3228  loss_mask_8: 0.384  loss_dice_8: 2.963  time: 1.5351  data_time: 0.1057  lr: 8.2787e-06  max_mem: 21410M
[01/17 11:24:00] d2.utils.events INFO:  eta: 1 day, 7:21:23  iter: 17059  total_loss: 37.55  loss_ce: 0.299  loss_mask: 0.395  loss_dice: 3.008  loss_ce_0: 0.5888  loss_mask_0: 0.3839  loss_dice_0: 3.118  loss_ce_1: 0.3148  loss_mask_1: 0.4001  loss_dice_1: 3.035  loss_ce_2: 0.3215  loss_mask_2: 0.3986  loss_dice_2: 3.019  loss_ce_3: 0.3189  loss_mask_3: 0.3969  loss_dice_3: 3.014  loss_ce_4: 0.2971  loss_mask_4: 0.3942  loss_dice_4: 3.004  loss_ce_5: 0.309  loss_mask_5: 0.393  loss_dice_5: 3.007  loss_ce_6: 0.3026  loss_mask_6: 0.3924  loss_dice_6: 3.007  loss_ce_7: 0.2968  loss_mask_7: 0.3932  loss_dice_7: 3  loss_ce_8: 0.2916  loss_mask_8: 0.3945  loss_dice_8: 2.998  time: 1.5351  data_time: 0.1133  lr: 8.2767e-06  max_mem: 21410M
[01/17 11:24:30] d2.utils.events INFO:  eta: 1 day, 7:20:41  iter: 17079  total_loss: 37.69  loss_ce: 0.3038  loss_mask: 0.3987  loss_dice: 2.984  loss_ce_0: 0.5945  loss_mask_0: 0.394  loss_dice_0: 3.083  loss_ce_1: 0.334  loss_mask_1: 0.4028  loss_dice_1: 3.009  loss_ce_2: 0.3339  loss_mask_2: 0.3986  loss_dice_2: 3.003  loss_ce_3: 0.3297  loss_mask_3: 0.4009  loss_dice_3: 2.994  loss_ce_4: 0.3227  loss_mask_4: 0.3992  loss_dice_4: 2.998  loss_ce_5: 0.3096  loss_mask_5: 0.4008  loss_dice_5: 3.001  loss_ce_6: 0.305  loss_mask_6: 0.3991  loss_dice_6: 2.99  loss_ce_7: 0.3179  loss_mask_7: 0.4012  loss_dice_7: 2.99  loss_ce_8: 0.3138  loss_mask_8: 0.3997  loss_dice_8: 2.986  time: 1.5351  data_time: 0.1004  lr: 8.2746e-06  max_mem: 21410M
[01/17 11:25:02] d2.utils.events INFO:  eta: 1 day, 7:21:47  iter: 17099  total_loss: 37.15  loss_ce: 0.2605  loss_mask: 0.3984  loss_dice: 2.958  loss_ce_0: 0.5605  loss_mask_0: 0.3887  loss_dice_0: 3.101  loss_ce_1: 0.2886  loss_mask_1: 0.4013  loss_dice_1: 2.999  loss_ce_2: 0.291  loss_mask_2: 0.3968  loss_dice_2: 2.974  loss_ce_3: 0.2769  loss_mask_3: 0.3955  loss_dice_3: 2.963  loss_ce_4: 0.2716  loss_mask_4: 0.3957  loss_dice_4: 2.958  loss_ce_5: 0.2514  loss_mask_5: 0.3979  loss_dice_5: 2.963  loss_ce_6: 0.2611  loss_mask_6: 0.3977  loss_dice_6: 2.958  loss_ce_7: 0.2609  loss_mask_7: 0.3966  loss_dice_7: 2.958  loss_ce_8: 0.2521  loss_mask_8: 0.3979  loss_dice_8: 2.956  time: 1.5352  data_time: 0.0987  lr: 8.2726e-06  max_mem: 21410M
[01/17 11:25:33] d2.utils.events INFO:  eta: 1 day, 7:21:02  iter: 17119  total_loss: 37.96  loss_ce: 0.2866  loss_mask: 0.3987  loss_dice: 3.062  loss_ce_0: 0.5609  loss_mask_0: 0.3932  loss_dice_0: 3.168  loss_ce_1: 0.308  loss_mask_1: 0.4028  loss_dice_1: 3.101  loss_ce_2: 0.3137  loss_mask_2: 0.4005  loss_dice_2: 3.078  loss_ce_3: 0.2932  loss_mask_3: 0.3998  loss_dice_3: 3.058  loss_ce_4: 0.2992  loss_mask_4: 0.3987  loss_dice_4: 3.066  loss_ce_5: 0.2903  loss_mask_5: 0.3978  loss_dice_5: 3.073  loss_ce_6: 0.2863  loss_mask_6: 0.3985  loss_dice_6: 3.064  loss_ce_7: 0.2862  loss_mask_7: 0.398  loss_dice_7: 3.059  loss_ce_8: 0.2833  loss_mask_8: 0.397  loss_dice_8: 3.058  time: 1.5352  data_time: 0.1056  lr: 8.2706e-06  max_mem: 21410M
[01/17 11:26:04] d2.utils.events INFO:  eta: 1 day, 7:20:58  iter: 17139  total_loss: 37.9  loss_ce: 0.2798  loss_mask: 0.398  loss_dice: 3.047  loss_ce_0: 0.5599  loss_mask_0: 0.396  loss_dice_0: 3.151  loss_ce_1: 0.2979  loss_mask_1: 0.4077  loss_dice_1: 3.069  loss_ce_2: 0.3178  loss_mask_2: 0.4001  loss_dice_2: 3.058  loss_ce_3: 0.2962  loss_mask_3: 0.4008  loss_dice_3: 3.05  loss_ce_4: 0.2813  loss_mask_4: 0.3974  loss_dice_4: 3.045  loss_ce_5: 0.2951  loss_mask_5: 0.3987  loss_dice_5: 3.047  loss_ce_6: 0.2762  loss_mask_6: 0.3985  loss_dice_6: 3.047  loss_ce_7: 0.272  loss_mask_7: 0.3974  loss_dice_7: 3.047  loss_ce_8: 0.282  loss_mask_8: 0.3974  loss_dice_8: 3.049  time: 1.5352  data_time: 0.1032  lr: 8.2685e-06  max_mem: 21410M
[01/17 11:26:35] d2.utils.events INFO:  eta: 1 day, 7:20:21  iter: 17159  total_loss: 37.6  loss_ce: 0.2734  loss_mask: 0.399  loss_dice: 2.986  loss_ce_0: 0.5577  loss_mask_0: 0.4055  loss_dice_0: 3.107  loss_ce_1: 0.2895  loss_mask_1: 0.4105  loss_dice_1: 3.025  loss_ce_2: 0.2931  loss_mask_2: 0.4042  loss_dice_2: 3.004  loss_ce_3: 0.2806  loss_mask_3: 0.4017  loss_dice_3: 2.984  loss_ce_4: 0.2845  loss_mask_4: 0.4  loss_dice_4: 2.987  loss_ce_5: 0.2715  loss_mask_5: 0.4004  loss_dice_5: 2.99  loss_ce_6: 0.2609  loss_mask_6: 0.4008  loss_dice_6: 2.983  loss_ce_7: 0.2711  loss_mask_7: 0.3988  loss_dice_7: 2.983  loss_ce_8: 0.2732  loss_mask_8: 0.4014  loss_dice_8: 2.979  time: 1.5352  data_time: 0.0896  lr: 8.2665e-06  max_mem: 21410M
[01/17 11:27:07] d2.utils.events INFO:  eta: 1 day, 7:19:56  iter: 17179  total_loss: 36.81  loss_ce: 0.2915  loss_mask: 0.3921  loss_dice: 2.955  loss_ce_0: 0.5617  loss_mask_0: 0.3791  loss_dice_0: 3.085  loss_ce_1: 0.3162  loss_mask_1: 0.3935  loss_dice_1: 2.998  loss_ce_2: 0.3149  loss_mask_2: 0.3905  loss_dice_2: 2.979  loss_ce_3: 0.3118  loss_mask_3: 0.392  loss_dice_3: 2.966  loss_ce_4: 0.3034  loss_mask_4: 0.3916  loss_dice_4: 2.962  loss_ce_5: 0.2893  loss_mask_5: 0.3925  loss_dice_5: 2.966  loss_ce_6: 0.2957  loss_mask_6: 0.3918  loss_dice_6: 2.965  loss_ce_7: 0.2942  loss_mask_7: 0.3921  loss_dice_7: 2.955  loss_ce_8: 0.2935  loss_mask_8: 0.3916  loss_dice_8: 2.956  time: 1.5353  data_time: 0.1008  lr: 8.2644e-06  max_mem: 21410M
[01/17 11:27:38] d2.utils.events INFO:  eta: 1 day, 7:19:33  iter: 17199  total_loss: 37.13  loss_ce: 0.2741  loss_mask: 0.3889  loss_dice: 2.984  loss_ce_0: 0.5473  loss_mask_0: 0.3818  loss_dice_0: 3.132  loss_ce_1: 0.305  loss_mask_1: 0.3942  loss_dice_1: 3.031  loss_ce_2: 0.2915  loss_mask_2: 0.3932  loss_dice_2: 3.002  loss_ce_3: 0.2844  loss_mask_3: 0.3878  loss_dice_3: 2.986  loss_ce_4: 0.2838  loss_mask_4: 0.387  loss_dice_4: 2.989  loss_ce_5: 0.2657  loss_mask_5: 0.3864  loss_dice_5: 2.984  loss_ce_6: 0.2858  loss_mask_6: 0.3898  loss_dice_6: 2.982  loss_ce_7: 0.2771  loss_mask_7: 0.3901  loss_dice_7: 2.978  loss_ce_8: 0.2774  loss_mask_8: 0.3902  loss_dice_8: 2.982  time: 1.5353  data_time: 0.1027  lr: 8.2624e-06  max_mem: 21410M
[01/17 11:28:09] d2.utils.events INFO:  eta: 1 day, 7:19:24  iter: 17219  total_loss: 37.5  loss_ce: 0.315  loss_mask: 0.3914  loss_dice: 3.007  loss_ce_0: 0.5741  loss_mask_0: 0.3858  loss_dice_0: 3.136  loss_ce_1: 0.3266  loss_mask_1: 0.4  loss_dice_1: 3.043  loss_ce_2: 0.3244  loss_mask_2: 0.3976  loss_dice_2: 3.012  loss_ce_3: 0.3177  loss_mask_3: 0.3947  loss_dice_3: 3.008  loss_ce_4: 0.2923  loss_mask_4: 0.3947  loss_dice_4: 3.013  loss_ce_5: 0.2924  loss_mask_5: 0.3936  loss_dice_5: 3.014  loss_ce_6: 0.2845  loss_mask_6: 0.3933  loss_dice_6: 3.009  loss_ce_7: 0.3113  loss_mask_7: 0.3931  loss_dice_7: 3.001  loss_ce_8: 0.2939  loss_mask_8: 0.3922  loss_dice_8: 3.008  time: 1.5353  data_time: 0.0974  lr: 8.2603e-06  max_mem: 21410M
[01/17 11:28:41] d2.utils.events INFO:  eta: 1 day, 7:19:26  iter: 17239  total_loss: 37.66  loss_ce: 0.3161  loss_mask: 0.4004  loss_dice: 2.995  loss_ce_0: 0.547  loss_mask_0: 0.3914  loss_dice_0: 3.128  loss_ce_1: 0.3246  loss_mask_1: 0.4011  loss_dice_1: 3.039  loss_ce_2: 0.3359  loss_mask_2: 0.4022  loss_dice_2: 3.026  loss_ce_3: 0.3235  loss_mask_3: 0.3984  loss_dice_3: 3.01  loss_ce_4: 0.3212  loss_mask_4: 0.3971  loss_dice_4: 3.009  loss_ce_5: 0.311  loss_mask_5: 0.4014  loss_dice_5: 3.014  loss_ce_6: 0.3229  loss_mask_6: 0.3985  loss_dice_6: 3.002  loss_ce_7: 0.3128  loss_mask_7: 0.4004  loss_dice_7: 3.002  loss_ce_8: 0.3069  loss_mask_8: 0.3994  loss_dice_8: 3  time: 1.5354  data_time: 0.0966  lr: 8.2583e-06  max_mem: 21410M
[01/17 11:29:12] d2.utils.events INFO:  eta: 1 day, 7:19:05  iter: 17259  total_loss: 37.5  loss_ce: 0.2932  loss_mask: 0.3944  loss_dice: 2.95  loss_ce_0: 0.6055  loss_mask_0: 0.3961  loss_dice_0: 3.055  loss_ce_1: 0.3263  loss_mask_1: 0.4087  loss_dice_1: 2.971  loss_ce_2: 0.3097  loss_mask_2: 0.4051  loss_dice_2: 2.962  loss_ce_3: 0.3053  loss_mask_3: 0.4009  loss_dice_3: 2.953  loss_ce_4: 0.2851  loss_mask_4: 0.3971  loss_dice_4: 2.951  loss_ce_5: 0.3069  loss_mask_5: 0.3988  loss_dice_5: 2.953  loss_ce_6: 0.3069  loss_mask_6: 0.3972  loss_dice_6: 2.943  loss_ce_7: 0.2965  loss_mask_7: 0.3959  loss_dice_7: 2.945  loss_ce_8: 0.2978  loss_mask_8: 0.3952  loss_dice_8: 2.944  time: 1.5354  data_time: 0.0831  lr: 8.2563e-06  max_mem: 21410M
[01/17 11:29:43] d2.utils.events INFO:  eta: 1 day, 7:18:34  iter: 17279  total_loss: 37.43  loss_ce: 0.2825  loss_mask: 0.3872  loss_dice: 3.018  loss_ce_0: 0.5866  loss_mask_0: 0.3817  loss_dice_0: 3.138  loss_ce_1: 0.3298  loss_mask_1: 0.3895  loss_dice_1: 3.058  loss_ce_2: 0.3358  loss_mask_2: 0.3865  loss_dice_2: 3.039  loss_ce_3: 0.3138  loss_mask_3: 0.3877  loss_dice_3: 3.029  loss_ce_4: 0.3111  loss_mask_4: 0.3885  loss_dice_4: 3.021  loss_ce_5: 0.3105  loss_mask_5: 0.386  loss_dice_5: 3.02  loss_ce_6: 0.3163  loss_mask_6: 0.3844  loss_dice_6: 3.012  loss_ce_7: 0.2981  loss_mask_7: 0.385  loss_dice_7: 3.012  loss_ce_8: 0.3063  loss_mask_8: 0.3856  loss_dice_8: 3.015  time: 1.5354  data_time: 0.0995  lr: 8.2542e-06  max_mem: 21410M
[01/17 11:30:15] d2.utils.events INFO:  eta: 1 day, 7:18:27  iter: 17299  total_loss: 37.04  loss_ce: 0.281  loss_mask: 0.3971  loss_dice: 2.971  loss_ce_0: 0.563  loss_mask_0: 0.3956  loss_dice_0: 3.102  loss_ce_1: 0.3039  loss_mask_1: 0.4003  loss_dice_1: 3.012  loss_ce_2: 0.2981  loss_mask_2: 0.3991  loss_dice_2: 2.987  loss_ce_3: 0.294  loss_mask_3: 0.3962  loss_dice_3: 2.987  loss_ce_4: 0.289  loss_mask_4: 0.3953  loss_dice_4: 2.985  loss_ce_5: 0.2828  loss_mask_5: 0.3953  loss_dice_5: 2.981  loss_ce_6: 0.282  loss_mask_6: 0.3967  loss_dice_6: 2.975  loss_ce_7: 0.2789  loss_mask_7: 0.3969  loss_dice_7: 2.981  loss_ce_8: 0.2724  loss_mask_8: 0.3975  loss_dice_8: 2.983  time: 1.5355  data_time: 0.0911  lr: 8.2522e-06  max_mem: 21410M
[01/17 11:30:46] d2.utils.events INFO:  eta: 1 day, 7:18:59  iter: 17319  total_loss: 37.05  loss_ce: 0.2853  loss_mask: 0.3856  loss_dice: 2.988  loss_ce_0: 0.5866  loss_mask_0: 0.3776  loss_dice_0: 3.139  loss_ce_1: 0.3373  loss_mask_1: 0.3888  loss_dice_1: 3.022  loss_ce_2: 0.311  loss_mask_2: 0.3862  loss_dice_2: 3.002  loss_ce_3: 0.3013  loss_mask_3: 0.3849  loss_dice_3: 3  loss_ce_4: 0.3016  loss_mask_4: 0.3837  loss_dice_4: 2.997  loss_ce_5: 0.3026  loss_mask_5: 0.3841  loss_dice_5: 3.001  loss_ce_6: 0.2909  loss_mask_6: 0.3846  loss_dice_6: 2.996  loss_ce_7: 0.288  loss_mask_7: 0.3851  loss_dice_7: 2.987  loss_ce_8: 0.2891  loss_mask_8: 0.3866  loss_dice_8: 2.992  time: 1.5355  data_time: 0.0967  lr: 8.2501e-06  max_mem: 21410M
[01/17 11:31:17] d2.utils.events INFO:  eta: 1 day, 7:18:10  iter: 17339  total_loss: 38.01  loss_ce: 0.3117  loss_mask: 0.4002  loss_dice: 3.027  loss_ce_0: 0.5908  loss_mask_0: 0.3934  loss_dice_0: 3.148  loss_ce_1: 0.3449  loss_mask_1: 0.4075  loss_dice_1: 3.069  loss_ce_2: 0.3456  loss_mask_2: 0.4046  loss_dice_2: 3.052  loss_ce_3: 0.3233  loss_mask_3: 0.4008  loss_dice_3: 3.047  loss_ce_4: 0.3185  loss_mask_4: 0.3989  loss_dice_4: 3.038  loss_ce_5: 0.3064  loss_mask_5: 0.3981  loss_dice_5: 3.042  loss_ce_6: 0.2995  loss_mask_6: 0.3994  loss_dice_6: 3.037  loss_ce_7: 0.2993  loss_mask_7: 0.4  loss_dice_7: 3.031  loss_ce_8: 0.3095  loss_mask_8: 0.3999  loss_dice_8: 3.038  time: 1.5355  data_time: 0.1085  lr: 8.2481e-06  max_mem: 21410M
[01/17 11:31:49] d2.utils.events INFO:  eta: 1 day, 7:18:51  iter: 17359  total_loss: 37.37  loss_ce: 0.2811  loss_mask: 0.3966  loss_dice: 2.958  loss_ce_0: 0.5364  loss_mask_0: 0.3928  loss_dice_0: 3.085  loss_ce_1: 0.2957  loss_mask_1: 0.4028  loss_dice_1: 2.991  loss_ce_2: 0.2945  loss_mask_2: 0.3967  loss_dice_2: 2.971  loss_ce_3: 0.2857  loss_mask_3: 0.3974  loss_dice_3: 2.962  loss_ce_4: 0.2845  loss_mask_4: 0.3999  loss_dice_4: 2.955  loss_ce_5: 0.2823  loss_mask_5: 0.3996  loss_dice_5: 2.956  loss_ce_6: 0.2824  loss_mask_6: 0.3982  loss_dice_6: 2.948  loss_ce_7: 0.2866  loss_mask_7: 0.3968  loss_dice_7: 2.956  loss_ce_8: 0.2776  loss_mask_8: 0.3973  loss_dice_8: 2.955  time: 1.5356  data_time: 0.1153  lr: 8.246e-06  max_mem: 21410M
[01/17 11:32:20] d2.utils.events INFO:  eta: 1 day, 7:18:41  iter: 17379  total_loss: 37.02  loss_ce: 0.2653  loss_mask: 0.394  loss_dice: 2.961  loss_ce_0: 0.5536  loss_mask_0: 0.3874  loss_dice_0: 3.1  loss_ce_1: 0.2999  loss_mask_1: 0.3957  loss_dice_1: 3.007  loss_ce_2: 0.297  loss_mask_2: 0.3947  loss_dice_2: 2.973  loss_ce_3: 0.2783  loss_mask_3: 0.3914  loss_dice_3: 2.972  loss_ce_4: 0.281  loss_mask_4: 0.392  loss_dice_4: 2.967  loss_ce_5: 0.2683  loss_mask_5: 0.3914  loss_dice_5: 2.97  loss_ce_6: 0.2714  loss_mask_6: 0.3919  loss_dice_6: 2.958  loss_ce_7: 0.27  loss_mask_7: 0.3925  loss_dice_7: 2.963  loss_ce_8: 0.2733  loss_mask_8: 0.3934  loss_dice_8: 2.96  time: 1.5356  data_time: 0.1002  lr: 8.244e-06  max_mem: 21410M
[01/17 11:32:53] d2.utils.events INFO:  eta: 1 day, 7:19:00  iter: 17399  total_loss: 37.01  loss_ce: 0.275  loss_mask: 0.3833  loss_dice: 2.956  loss_ce_0: 0.5815  loss_mask_0: 0.3816  loss_dice_0: 3.092  loss_ce_1: 0.3132  loss_mask_1: 0.3919  loss_dice_1: 3.009  loss_ce_2: 0.3185  loss_mask_2: 0.3845  loss_dice_2: 2.975  loss_ce_3: 0.3132  loss_mask_3: 0.3837  loss_dice_3: 2.96  loss_ce_4: 0.3044  loss_mask_4: 0.3829  loss_dice_4: 2.963  loss_ce_5: 0.2882  loss_mask_5: 0.3838  loss_dice_5: 2.961  loss_ce_6: 0.2937  loss_mask_6: 0.3843  loss_dice_6: 2.968  loss_ce_7: 0.2956  loss_mask_7: 0.3834  loss_dice_7: 2.961  loss_ce_8: 0.2995  loss_mask_8: 0.3809  loss_dice_8: 2.966  time: 1.5357  data_time: 0.1078  lr: 8.242e-06  max_mem: 21410M
[01/17 11:33:24] d2.utils.events INFO:  eta: 1 day, 7:18:49  iter: 17419  total_loss: 36.88  loss_ce: 0.264  loss_mask: 0.4089  loss_dice: 2.945  loss_ce_0: 0.532  loss_mask_0: 0.4048  loss_dice_0: 3.057  loss_ce_1: 0.3039  loss_mask_1: 0.4175  loss_dice_1: 2.985  loss_ce_2: 0.2987  loss_mask_2: 0.4124  loss_dice_2: 2.955  loss_ce_3: 0.2776  loss_mask_3: 0.4139  loss_dice_3: 2.95  loss_ce_4: 0.2758  loss_mask_4: 0.4156  loss_dice_4: 2.948  loss_ce_5: 0.2683  loss_mask_5: 0.414  loss_dice_5: 2.947  loss_ce_6: 0.2823  loss_mask_6: 0.4137  loss_dice_6: 2.939  loss_ce_7: 0.2634  loss_mask_7: 0.4124  loss_dice_7: 2.939  loss_ce_8: 0.2712  loss_mask_8: 0.4084  loss_dice_8: 2.941  time: 1.5357  data_time: 0.1045  lr: 8.2399e-06  max_mem: 21410M
[01/17 11:33:55] d2.utils.events INFO:  eta: 1 day, 7:18:04  iter: 17439  total_loss: 37.63  loss_ce: 0.3248  loss_mask: 0.3947  loss_dice: 2.995  loss_ce_0: 0.5819  loss_mask_0: 0.3873  loss_dice_0: 3.122  loss_ce_1: 0.3432  loss_mask_1: 0.397  loss_dice_1: 3.036  loss_ce_2: 0.3628  loss_mask_2: 0.3925  loss_dice_2: 3.019  loss_ce_3: 0.3361  loss_mask_3: 0.3907  loss_dice_3: 3.014  loss_ce_4: 0.3304  loss_mask_4: 0.3928  loss_dice_4: 3.012  loss_ce_5: 0.3136  loss_mask_5: 0.3954  loss_dice_5: 3.011  loss_ce_6: 0.3079  loss_mask_6: 0.395  loss_dice_6: 2.994  loss_ce_7: 0.3143  loss_mask_7: 0.396  loss_dice_7: 2.997  loss_ce_8: 0.3206  loss_mask_8: 0.3961  loss_dice_8: 2.991  time: 1.5358  data_time: 0.1060  lr: 8.2379e-06  max_mem: 21410M
[01/17 11:34:26] d2.utils.events INFO:  eta: 1 day, 7:17:03  iter: 17459  total_loss: 37.06  loss_ce: 0.293  loss_mask: 0.3914  loss_dice: 2.999  loss_ce_0: 0.5792  loss_mask_0: 0.3926  loss_dice_0: 3.124  loss_ce_1: 0.3265  loss_mask_1: 0.4052  loss_dice_1: 3.051  loss_ce_2: 0.3201  loss_mask_2: 0.3982  loss_dice_2: 3.019  loss_ce_3: 0.3043  loss_mask_3: 0.3961  loss_dice_3: 2.997  loss_ce_4: 0.3103  loss_mask_4: 0.3958  loss_dice_4: 3.001  loss_ce_5: 0.2987  loss_mask_5: 0.3958  loss_dice_5: 2.997  loss_ce_6: 0.2824  loss_mask_6: 0.3933  loss_dice_6: 2.992  loss_ce_7: 0.2838  loss_mask_7: 0.3921  loss_dice_7: 2.991  loss_ce_8: 0.2901  loss_mask_8: 0.3907  loss_dice_8: 2.99  time: 1.5358  data_time: 0.0975  lr: 8.2358e-06  max_mem: 21410M
[01/17 11:34:57] d2.utils.events INFO:  eta: 1 day, 7:16:58  iter: 17479  total_loss: 37.88  loss_ce: 0.2956  loss_mask: 0.3901  loss_dice: 3.016  loss_ce_0: 0.5774  loss_mask_0: 0.3889  loss_dice_0: 3.141  loss_ce_1: 0.3172  loss_mask_1: 0.4009  loss_dice_1: 3.057  loss_ce_2: 0.3052  loss_mask_2: 0.3925  loss_dice_2: 3.036  loss_ce_3: 0.2999  loss_mask_3: 0.393  loss_dice_3: 3.016  loss_ce_4: 0.2999  loss_mask_4: 0.3908  loss_dice_4: 3.017  loss_ce_5: 0.2893  loss_mask_5: 0.389  loss_dice_5: 3.021  loss_ce_6: 0.2746  loss_mask_6: 0.3905  loss_dice_6: 3.012  loss_ce_7: 0.2838  loss_mask_7: 0.3892  loss_dice_7: 3.017  loss_ce_8: 0.2888  loss_mask_8: 0.3899  loss_dice_8: 3.021  time: 1.5358  data_time: 0.0991  lr: 8.2338e-06  max_mem: 21410M
[01/17 11:35:28] d2.utils.events INFO:  eta: 1 day, 7:16:17  iter: 17499  total_loss: 37.17  loss_ce: 0.2966  loss_mask: 0.3894  loss_dice: 2.962  loss_ce_0: 0.5727  loss_mask_0: 0.3925  loss_dice_0: 3.083  loss_ce_1: 0.3417  loss_mask_1: 0.3996  loss_dice_1: 3.005  loss_ce_2: 0.3513  loss_mask_2: 0.3947  loss_dice_2: 2.975  loss_ce_3: 0.3233  loss_mask_3: 0.3928  loss_dice_3: 2.966  loss_ce_4: 0.3161  loss_mask_4: 0.3892  loss_dice_4: 2.971  loss_ce_5: 0.3053  loss_mask_5: 0.3884  loss_dice_5: 2.969  loss_ce_6: 0.3076  loss_mask_6: 0.3892  loss_dice_6: 2.962  loss_ce_7: 0.3154  loss_mask_7: 0.3888  loss_dice_7: 2.971  loss_ce_8: 0.2918  loss_mask_8: 0.3872  loss_dice_8: 2.964  time: 1.5358  data_time: 0.0990  lr: 8.2317e-06  max_mem: 21410M
[01/17 11:35:58] d2.utils.events INFO:  eta: 1 day, 7:14:58  iter: 17519  total_loss: 37.74  loss_ce: 0.2927  loss_mask: 0.3952  loss_dice: 2.986  loss_ce_0: 0.5465  loss_mask_0: 0.392  loss_dice_0: 3.121  loss_ce_1: 0.3212  loss_mask_1: 0.3991  loss_dice_1: 3.035  loss_ce_2: 0.3127  loss_mask_2: 0.3975  loss_dice_2: 3.004  loss_ce_3: 0.3065  loss_mask_3: 0.3972  loss_dice_3: 2.998  loss_ce_4: 0.2954  loss_mask_4: 0.3944  loss_dice_4: 3.001  loss_ce_5: 0.2912  loss_mask_5: 0.3945  loss_dice_5: 2.995  loss_ce_6: 0.2859  loss_mask_6: 0.3969  loss_dice_6: 2.999  loss_ce_7: 0.294  loss_mask_7: 0.3943  loss_dice_7: 2.999  loss_ce_8: 0.2878  loss_mask_8: 0.3953  loss_dice_8: 2.995  time: 1.5358  data_time: 0.0978  lr: 8.2297e-06  max_mem: 21410M
[01/17 11:36:29] d2.utils.events INFO:  eta: 1 day, 7:13:50  iter: 17539  total_loss: 36.95  loss_ce: 0.2729  loss_mask: 0.3952  loss_dice: 2.931  loss_ce_0: 0.5674  loss_mask_0: 0.3953  loss_dice_0: 3.066  loss_ce_1: 0.2866  loss_mask_1: 0.4058  loss_dice_1: 2.99  loss_ce_2: 0.2969  loss_mask_2: 0.4001  loss_dice_2: 2.966  loss_ce_3: 0.2873  loss_mask_3: 0.3976  loss_dice_3: 2.945  loss_ce_4: 0.2894  loss_mask_4: 0.3983  loss_dice_4: 2.953  loss_ce_5: 0.2734  loss_mask_5: 0.3982  loss_dice_5: 2.953  loss_ce_6: 0.2847  loss_mask_6: 0.3971  loss_dice_6: 2.939  loss_ce_7: 0.2763  loss_mask_7: 0.3966  loss_dice_7: 2.936  loss_ce_8: 0.2778  loss_mask_8: 0.3957  loss_dice_8: 2.928  time: 1.5358  data_time: 0.0973  lr: 8.2276e-06  max_mem: 21410M
[01/17 11:37:00] d2.utils.events INFO:  eta: 1 day, 7:13:34  iter: 17559  total_loss: 37.05  loss_ce: 0.282  loss_mask: 0.3941  loss_dice: 2.932  loss_ce_0: 0.5711  loss_mask_0: 0.382  loss_dice_0: 3.065  loss_ce_1: 0.3134  loss_mask_1: 0.3915  loss_dice_1: 2.972  loss_ce_2: 0.3185  loss_mask_2: 0.393  loss_dice_2: 2.939  loss_ce_3: 0.3142  loss_mask_3: 0.391  loss_dice_3: 2.933  loss_ce_4: 0.316  loss_mask_4: 0.3923  loss_dice_4: 2.93  loss_ce_5: 0.2832  loss_mask_5: 0.3924  loss_dice_5: 2.937  loss_ce_6: 0.2945  loss_mask_6: 0.3922  loss_dice_6: 2.936  loss_ce_7: 0.2886  loss_mask_7: 0.3927  loss_dice_7: 2.932  loss_ce_8: 0.2821  loss_mask_8: 0.3926  loss_dice_8: 2.936  time: 1.5358  data_time: 0.1155  lr: 8.2256e-06  max_mem: 21410M
[01/17 11:37:33] d2.utils.events INFO:  eta: 1 day, 7:14:27  iter: 17579  total_loss: 37.99  loss_ce: 0.3182  loss_mask: 0.3848  loss_dice: 3.064  loss_ce_0: 0.5863  loss_mask_0: 0.3809  loss_dice_0: 3.196  loss_ce_1: 0.3453  loss_mask_1: 0.3903  loss_dice_1: 3.102  loss_ce_2: 0.3534  loss_mask_2: 0.3861  loss_dice_2: 3.084  loss_ce_3: 0.3242  loss_mask_3: 0.3863  loss_dice_3: 3.067  loss_ce_4: 0.329  loss_mask_4: 0.3838  loss_dice_4: 3.073  loss_ce_5: 0.3227  loss_mask_5: 0.3829  loss_dice_5: 3.07  loss_ce_6: 0.3037  loss_mask_6: 0.3841  loss_dice_6: 3.065  loss_ce_7: 0.3223  loss_mask_7: 0.3854  loss_dice_7: 3.059  loss_ce_8: 0.313  loss_mask_8: 0.3857  loss_dice_8: 3.053  time: 1.5359  data_time: 0.0980  lr: 8.2236e-06  max_mem: 21410M
[01/17 11:38:03] d2.utils.events INFO:  eta: 1 day, 7:13:46  iter: 17599  total_loss: 37.53  loss_ce: 0.2701  loss_mask: 0.3943  loss_dice: 2.994  loss_ce_0: 0.5661  loss_mask_0: 0.3869  loss_dice_0: 3.119  loss_ce_1: 0.2997  loss_mask_1: 0.3964  loss_dice_1: 3.032  loss_ce_2: 0.3089  loss_mask_2: 0.3964  loss_dice_2: 3.006  loss_ce_3: 0.285  loss_mask_3: 0.3955  loss_dice_3: 2.997  loss_ce_4: 0.267  loss_mask_4: 0.3916  loss_dice_4: 2.99  loss_ce_5: 0.2772  loss_mask_5: 0.3925  loss_dice_5: 3.001  loss_ce_6: 0.2623  loss_mask_6: 0.3937  loss_dice_6: 2.988  loss_ce_7: 0.2697  loss_mask_7: 0.3932  loss_dice_7: 2.993  loss_ce_8: 0.2712  loss_mask_8: 0.3951  loss_dice_8: 2.991  time: 1.5359  data_time: 0.1058  lr: 8.2215e-06  max_mem: 21410M
[01/17 11:38:34] d2.utils.events INFO:  eta: 1 day, 7:13:15  iter: 17619  total_loss: 37.8  loss_ce: 0.3173  loss_mask: 0.4036  loss_dice: 3.016  loss_ce_0: 0.5512  loss_mask_0: 0.3948  loss_dice_0: 3.122  loss_ce_1: 0.316  loss_mask_1: 0.4039  loss_dice_1: 3.052  loss_ce_2: 0.3256  loss_mask_2: 0.4044  loss_dice_2: 3.033  loss_ce_3: 0.304  loss_mask_3: 0.4047  loss_dice_3: 3.018  loss_ce_4: 0.3263  loss_mask_4: 0.4048  loss_dice_4: 3.028  loss_ce_5: 0.3015  loss_mask_5: 0.4044  loss_dice_5: 3.02  loss_ce_6: 0.2986  loss_mask_6: 0.4027  loss_dice_6: 3.01  loss_ce_7: 0.2973  loss_mask_7: 0.4053  loss_dice_7: 3.015  loss_ce_8: 0.2902  loss_mask_8: 0.4044  loss_dice_8: 3.013  time: 1.5359  data_time: 0.0953  lr: 8.2195e-06  max_mem: 21410M
[01/17 11:39:05] d2.utils.events INFO:  eta: 1 day, 7:11:15  iter: 17639  total_loss: 37.46  loss_ce: 0.2794  loss_mask: 0.41  loss_dice: 3.011  loss_ce_0: 0.5374  loss_mask_0: 0.4153  loss_dice_0: 3.122  loss_ce_1: 0.3085  loss_mask_1: 0.4188  loss_dice_1: 3.042  loss_ce_2: 0.3109  loss_mask_2: 0.4107  loss_dice_2: 3.023  loss_ce_3: 0.2918  loss_mask_3: 0.4107  loss_dice_3: 3.01  loss_ce_4: 0.2968  loss_mask_4: 0.4085  loss_dice_4: 3.004  loss_ce_5: 0.2822  loss_mask_5: 0.4113  loss_dice_5: 3.021  loss_ce_6: 0.277  loss_mask_6: 0.4085  loss_dice_6: 3.011  loss_ce_7: 0.2726  loss_mask_7: 0.4062  loss_dice_7: 3.017  loss_ce_8: 0.278  loss_mask_8: 0.4064  loss_dice_8: 3.01  time: 1.5359  data_time: 0.1060  lr: 8.2174e-06  max_mem: 21410M
[01/17 11:39:36] d2.utils.events INFO:  eta: 1 day, 7:10:58  iter: 17659  total_loss: 37.05  loss_ce: 0.2817  loss_mask: 0.3958  loss_dice: 2.938  loss_ce_0: 0.5852  loss_mask_0: 0.3899  loss_dice_0: 3.061  loss_ce_1: 0.3189  loss_mask_1: 0.3986  loss_dice_1: 2.972  loss_ce_2: 0.3101  loss_mask_2: 0.3961  loss_dice_2: 2.947  loss_ce_3: 0.2926  loss_mask_3: 0.3948  loss_dice_3: 2.943  loss_ce_4: 0.2921  loss_mask_4: 0.3967  loss_dice_4: 2.938  loss_ce_5: 0.2883  loss_mask_5: 0.3954  loss_dice_5: 2.954  loss_ce_6: 0.2811  loss_mask_6: 0.3945  loss_dice_6: 2.938  loss_ce_7: 0.2862  loss_mask_7: 0.3966  loss_dice_7: 2.936  loss_ce_8: 0.2984  loss_mask_8: 0.3953  loss_dice_8: 2.937  time: 1.5359  data_time: 0.0975  lr: 8.2154e-06  max_mem: 21410M
[01/17 11:40:07] d2.utils.events INFO:  eta: 1 day, 7:10:59  iter: 17679  total_loss: 37.43  loss_ce: 0.303  loss_mask: 0.3805  loss_dice: 2.989  loss_ce_0: 0.6064  loss_mask_0: 0.3754  loss_dice_0: 3.105  loss_ce_1: 0.3296  loss_mask_1: 0.3814  loss_dice_1: 3.036  loss_ce_2: 0.3326  loss_mask_2: 0.38  loss_dice_2: 3.011  loss_ce_3: 0.3249  loss_mask_3: 0.3815  loss_dice_3: 2.993  loss_ce_4: 0.3102  loss_mask_4: 0.3803  loss_dice_4: 2.985  loss_ce_5: 0.3047  loss_mask_5: 0.3846  loss_dice_5: 2.983  loss_ce_6: 0.308  loss_mask_6: 0.3836  loss_dice_6: 2.999  loss_ce_7: 0.304  loss_mask_7: 0.3817  loss_dice_7: 2.996  loss_ce_8: 0.2992  loss_mask_8: 0.3814  loss_dice_8: 2.997  time: 1.5359  data_time: 0.1059  lr: 8.2133e-06  max_mem: 21410M
[01/17 11:40:38] d2.utils.events INFO:  eta: 1 day, 7:10:50  iter: 17699  total_loss: 37.59  loss_ce: 0.3003  loss_mask: 0.3837  loss_dice: 3.019  loss_ce_0: 0.5573  loss_mask_0: 0.3804  loss_dice_0: 3.13  loss_ce_1: 0.3309  loss_mask_1: 0.3876  loss_dice_1: 3.053  loss_ce_2: 0.3347  loss_mask_2: 0.3834  loss_dice_2: 3.035  loss_ce_3: 0.3206  loss_mask_3: 0.383  loss_dice_3: 3.029  loss_ce_4: 0.3042  loss_mask_4: 0.3839  loss_dice_4: 3.021  loss_ce_5: 0.3207  loss_mask_5: 0.384  loss_dice_5: 3.018  loss_ce_6: 0.2998  loss_mask_6: 0.3852  loss_dice_6: 3.018  loss_ce_7: 0.2981  loss_mask_7: 0.3841  loss_dice_7: 3.016  loss_ce_8: 0.3031  loss_mask_8: 0.3843  loss_dice_8: 3.016  time: 1.5360  data_time: 0.0874  lr: 8.2113e-06  max_mem: 21410M
[01/17 11:41:10] d2.utils.events INFO:  eta: 1 day, 7:10:40  iter: 17719  total_loss: 37.67  loss_ce: 0.2955  loss_mask: 0.3929  loss_dice: 2.98  loss_ce_0: 0.5801  loss_mask_0: 0.3939  loss_dice_0: 3.092  loss_ce_1: 0.331  loss_mask_1: 0.4031  loss_dice_1: 3.02  loss_ce_2: 0.3251  loss_mask_2: 0.3972  loss_dice_2: 2.996  loss_ce_3: 0.3179  loss_mask_3: 0.397  loss_dice_3: 2.981  loss_ce_4: 0.305  loss_mask_4: 0.3956  loss_dice_4: 2.978  loss_ce_5: 0.3015  loss_mask_5: 0.3954  loss_dice_5: 2.975  loss_ce_6: 0.3125  loss_mask_6: 0.3932  loss_dice_6: 2.973  loss_ce_7: 0.3083  loss_mask_7: 0.3926  loss_dice_7: 2.989  loss_ce_8: 0.2917  loss_mask_8: 0.3933  loss_dice_8: 2.971  time: 1.5360  data_time: 0.1098  lr: 8.2092e-06  max_mem: 21410M
[01/17 11:41:41] d2.utils.events INFO:  eta: 1 day, 7:10:18  iter: 17739  total_loss: 36.88  loss_ce: 0.2823  loss_mask: 0.3886  loss_dice: 2.962  loss_ce_0: 0.5318  loss_mask_0: 0.3799  loss_dice_0: 3.094  loss_ce_1: 0.3027  loss_mask_1: 0.3904  loss_dice_1: 3.005  loss_ce_2: 0.3056  loss_mask_2: 0.3876  loss_dice_2: 2.981  loss_ce_3: 0.2953  loss_mask_3: 0.3884  loss_dice_3: 2.971  loss_ce_4: 0.2866  loss_mask_4: 0.3861  loss_dice_4: 2.969  loss_ce_5: 0.2806  loss_mask_5: 0.3843  loss_dice_5: 2.977  loss_ce_6: 0.2703  loss_mask_6: 0.3863  loss_dice_6: 2.969  loss_ce_7: 0.2749  loss_mask_7: 0.3884  loss_dice_7: 2.97  loss_ce_8: 0.2743  loss_mask_8: 0.3868  loss_dice_8: 2.972  time: 1.5360  data_time: 0.0927  lr: 8.2072e-06  max_mem: 21410M
[01/17 11:42:12] d2.utils.events INFO:  eta: 1 day, 7:10:01  iter: 17759  total_loss: 37.15  loss_ce: 0.2923  loss_mask: 0.386  loss_dice: 2.963  loss_ce_0: 0.5956  loss_mask_0: 0.3842  loss_dice_0: 3.093  loss_ce_1: 0.3119  loss_mask_1: 0.3937  loss_dice_1: 2.994  loss_ce_2: 0.3184  loss_mask_2: 0.3894  loss_dice_2: 2.974  loss_ce_3: 0.3016  loss_mask_3: 0.3916  loss_dice_3: 2.966  loss_ce_4: 0.2929  loss_mask_4: 0.3893  loss_dice_4: 2.966  loss_ce_5: 0.2924  loss_mask_5: 0.391  loss_dice_5: 2.971  loss_ce_6: 0.2944  loss_mask_6: 0.3902  loss_dice_6: 2.967  loss_ce_7: 0.282  loss_mask_7: 0.3883  loss_dice_7: 2.966  loss_ce_8: 0.2937  loss_mask_8: 0.3875  loss_dice_8: 2.963  time: 1.5360  data_time: 0.0988  lr: 8.2052e-06  max_mem: 21410M
[01/17 11:42:42] d2.utils.events INFO:  eta: 1 day, 7:09:30  iter: 17779  total_loss: 37.5  loss_ce: 0.3119  loss_mask: 0.3946  loss_dice: 2.987  loss_ce_0: 0.5629  loss_mask_0: 0.4095  loss_dice_0: 3.102  loss_ce_1: 0.3163  loss_mask_1: 0.4088  loss_dice_1: 3.028  loss_ce_2: 0.3408  loss_mask_2: 0.3999  loss_dice_2: 3.013  loss_ce_3: 0.3175  loss_mask_3: 0.3966  loss_dice_3: 2.983  loss_ce_4: 0.3217  loss_mask_4: 0.3957  loss_dice_4: 2.999  loss_ce_5: 0.3108  loss_mask_5: 0.3972  loss_dice_5: 2.995  loss_ce_6: 0.3126  loss_mask_6: 0.3957  loss_dice_6: 2.992  loss_ce_7: 0.3225  loss_mask_7: 0.3946  loss_dice_7: 2.99  loss_ce_8: 0.3118  loss_mask_8: 0.3951  loss_dice_8: 2.988  time: 1.5360  data_time: 0.0965  lr: 8.2031e-06  max_mem: 21410M
[01/17 11:43:13] d2.utils.events INFO:  eta: 1 day, 7:09:08  iter: 17799  total_loss: 36.86  loss_ce: 0.2834  loss_mask: 0.3938  loss_dice: 2.977  loss_ce_0: 0.5457  loss_mask_0: 0.3831  loss_dice_0: 3.093  loss_ce_1: 0.3241  loss_mask_1: 0.3921  loss_dice_1: 3.027  loss_ce_2: 0.3024  loss_mask_2: 0.389  loss_dice_2: 2.994  loss_ce_3: 0.2979  loss_mask_3: 0.3905  loss_dice_3: 2.988  loss_ce_4: 0.2892  loss_mask_4: 0.3918  loss_dice_4: 2.979  loss_ce_5: 0.276  loss_mask_5: 0.3939  loss_dice_5: 2.986  loss_ce_6: 0.2875  loss_mask_6: 0.3932  loss_dice_6: 2.977  loss_ce_7: 0.2747  loss_mask_7: 0.3936  loss_dice_7: 2.972  loss_ce_8: 0.2859  loss_mask_8: 0.3942  loss_dice_8: 2.972  time: 1.5360  data_time: 0.1011  lr: 8.2011e-06  max_mem: 21410M
[01/17 11:43:44] d2.utils.events INFO:  eta: 1 day, 7:09:01  iter: 17819  total_loss: 36.78  loss_ce: 0.2898  loss_mask: 0.3902  loss_dice: 2.958  loss_ce_0: 0.5446  loss_mask_0: 0.3771  loss_dice_0: 3.084  loss_ce_1: 0.3054  loss_mask_1: 0.389  loss_dice_1: 2.991  loss_ce_2: 0.3066  loss_mask_2: 0.3894  loss_dice_2: 2.967  loss_ce_3: 0.3132  loss_mask_3: 0.3905  loss_dice_3: 2.95  loss_ce_4: 0.2885  loss_mask_4: 0.3897  loss_dice_4: 2.953  loss_ce_5: 0.2892  loss_mask_5: 0.3905  loss_dice_5: 2.948  loss_ce_6: 0.2896  loss_mask_6: 0.3895  loss_dice_6: 2.959  loss_ce_7: 0.2832  loss_mask_7: 0.3892  loss_dice_7: 2.951  loss_ce_8: 0.2834  loss_mask_8: 0.3917  loss_dice_8: 2.955  time: 1.5360  data_time: 0.0862  lr: 8.199e-06  max_mem: 21410M
[01/17 11:44:15] d2.utils.events INFO:  eta: 1 day, 7:08:03  iter: 17839  total_loss: 36.69  loss_ce: 0.2718  loss_mask: 0.3958  loss_dice: 2.937  loss_ce_0: 0.571  loss_mask_0: 0.3945  loss_dice_0: 3.055  loss_ce_1: 0.2987  loss_mask_1: 0.4046  loss_dice_1: 2.979  loss_ce_2: 0.2989  loss_mask_2: 0.4016  loss_dice_2: 2.951  loss_ce_3: 0.2913  loss_mask_3: 0.3979  loss_dice_3: 2.94  loss_ce_4: 0.274  loss_mask_4: 0.3966  loss_dice_4: 2.952  loss_ce_5: 0.2672  loss_mask_5: 0.3961  loss_dice_5: 2.945  loss_ce_6: 0.2798  loss_mask_6: 0.3956  loss_dice_6: 2.932  loss_ce_7: 0.276  loss_mask_7: 0.3961  loss_dice_7: 2.935  loss_ce_8: 0.2779  loss_mask_8: 0.3972  loss_dice_8: 2.937  time: 1.5360  data_time: 0.0952  lr: 8.197e-06  max_mem: 21410M
[01/17 11:44:46] d2.utils.events INFO:  eta: 1 day, 7:07:19  iter: 17859  total_loss: 36.72  loss_ce: 0.3037  loss_mask: 0.3962  loss_dice: 2.927  loss_ce_0: 0.5722  loss_mask_0: 0.3861  loss_dice_0: 3.064  loss_ce_1: 0.3247  loss_mask_1: 0.3964  loss_dice_1: 2.983  loss_ce_2: 0.3294  loss_mask_2: 0.3968  loss_dice_2: 2.952  loss_ce_3: 0.3133  loss_mask_3: 0.3944  loss_dice_3: 2.938  loss_ce_4: 0.3054  loss_mask_4: 0.3949  loss_dice_4: 2.928  loss_ce_5: 0.3121  loss_mask_5: 0.3928  loss_dice_5: 2.938  loss_ce_6: 0.3115  loss_mask_6: 0.3951  loss_dice_6: 2.926  loss_ce_7: 0.3067  loss_mask_7: 0.3949  loss_dice_7: 2.932  loss_ce_8: 0.3092  loss_mask_8: 0.394  loss_dice_8: 2.944  time: 1.5360  data_time: 0.0940  lr: 8.1949e-06  max_mem: 21410M
[01/17 11:45:16] d2.utils.events INFO:  eta: 1 day, 7:06:41  iter: 17879  total_loss: 36.28  loss_ce: 0.2751  loss_mask: 0.395  loss_dice: 2.912  loss_ce_0: 0.5732  loss_mask_0: 0.3882  loss_dice_0: 3.025  loss_ce_1: 0.3321  loss_mask_1: 0.4011  loss_dice_1: 2.945  loss_ce_2: 0.3198  loss_mask_2: 0.3946  loss_dice_2: 2.931  loss_ce_3: 0.3031  loss_mask_3: 0.3928  loss_dice_3: 2.918  loss_ce_4: 0.2916  loss_mask_4: 0.3954  loss_dice_4: 2.907  loss_ce_5: 0.2854  loss_mask_5: 0.3956  loss_dice_5: 2.92  loss_ce_6: 0.2762  loss_mask_6: 0.3948  loss_dice_6: 2.921  loss_ce_7: 0.2802  loss_mask_7: 0.3949  loss_dice_7: 2.914  loss_ce_8: 0.2715  loss_mask_8: 0.3967  loss_dice_8: 2.916  time: 1.5360  data_time: 0.1041  lr: 8.1929e-06  max_mem: 21410M
[01/17 11:45:47] d2.utils.events INFO:  eta: 1 day, 7:06:01  iter: 17899  total_loss: 37.4  loss_ce: 0.3007  loss_mask: 0.373  loss_dice: 3.025  loss_ce_0: 0.5839  loss_mask_0: 0.3792  loss_dice_0: 3.134  loss_ce_1: 0.3106  loss_mask_1: 0.3893  loss_dice_1: 3.064  loss_ce_2: 0.3306  loss_mask_2: 0.3829  loss_dice_2: 3.041  loss_ce_3: 0.3059  loss_mask_3: 0.3767  loss_dice_3: 3.035  loss_ce_4: 0.3135  loss_mask_4: 0.3747  loss_dice_4: 3.035  loss_ce_5: 0.2962  loss_mask_5: 0.3726  loss_dice_5: 3.031  loss_ce_6: 0.3146  loss_mask_6: 0.3731  loss_dice_6: 3.027  loss_ce_7: 0.3082  loss_mask_7: 0.3725  loss_dice_7: 3.03  loss_ce_8: 0.2869  loss_mask_8: 0.3739  loss_dice_8: 3.026  time: 1.5360  data_time: 0.0996  lr: 8.1908e-06  max_mem: 21410M
[01/17 11:46:18] d2.utils.events INFO:  eta: 1 day, 7:05:34  iter: 17919  total_loss: 37.38  loss_ce: 0.2816  loss_mask: 0.3886  loss_dice: 3.008  loss_ce_0: 0.5528  loss_mask_0: 0.388  loss_dice_0: 3.126  loss_ce_1: 0.303  loss_mask_1: 0.3909  loss_dice_1: 3.041  loss_ce_2: 0.3013  loss_mask_2: 0.3868  loss_dice_2: 3.028  loss_ce_3: 0.2829  loss_mask_3: 0.3862  loss_dice_3: 3.011  loss_ce_4: 0.2836  loss_mask_4: 0.3849  loss_dice_4: 3.018  loss_ce_5: 0.269  loss_mask_5: 0.3884  loss_dice_5: 3.017  loss_ce_6: 0.2763  loss_mask_6: 0.3895  loss_dice_6: 2.999  loss_ce_7: 0.2599  loss_mask_7: 0.3919  loss_dice_7: 3.013  loss_ce_8: 0.2739  loss_mask_8: 0.3902  loss_dice_8: 3.013  time: 1.5360  data_time: 0.1100  lr: 8.1888e-06  max_mem: 21410M
[01/17 11:46:49] d2.utils.events INFO:  eta: 1 day, 7:05:08  iter: 17939  total_loss: 36.99  loss_ce: 0.3094  loss_mask: 0.3899  loss_dice: 2.958  loss_ce_0: 0.5808  loss_mask_0: 0.3867  loss_dice_0: 3.087  loss_ce_1: 0.3213  loss_mask_1: 0.3947  loss_dice_1: 2.999  loss_ce_2: 0.3238  loss_mask_2: 0.3939  loss_dice_2: 2.968  loss_ce_3: 0.2945  loss_mask_3: 0.392  loss_dice_3: 2.963  loss_ce_4: 0.3088  loss_mask_4: 0.392  loss_dice_4: 2.964  loss_ce_5: 0.2977  loss_mask_5: 0.3923  loss_dice_5: 2.965  loss_ce_6: 0.303  loss_mask_6: 0.39  loss_dice_6: 2.961  loss_ce_7: 0.3005  loss_mask_7: 0.3893  loss_dice_7: 2.964  loss_ce_8: 0.2899  loss_mask_8: 0.3893  loss_dice_8: 2.961  time: 1.5360  data_time: 0.1017  lr: 8.1868e-06  max_mem: 21410M
[01/17 11:47:19] d2.utils.events INFO:  eta: 1 day, 7:04:36  iter: 17959  total_loss: 36.76  loss_ce: 0.2874  loss_mask: 0.3981  loss_dice: 2.931  loss_ce_0: 0.5758  loss_mask_0: 0.392  loss_dice_0: 3.053  loss_ce_1: 0.3246  loss_mask_1: 0.4014  loss_dice_1: 2.969  loss_ce_2: 0.3098  loss_mask_2: 0.3974  loss_dice_2: 2.941  loss_ce_3: 0.3072  loss_mask_3: 0.3966  loss_dice_3: 2.929  loss_ce_4: 0.2907  loss_mask_4: 0.3977  loss_dice_4: 2.929  loss_ce_5: 0.2891  loss_mask_5: 0.399  loss_dice_5: 2.93  loss_ce_6: 0.2932  loss_mask_6: 0.3989  loss_dice_6: 2.92  loss_ce_7: 0.273  loss_mask_7: 0.3987  loss_dice_7: 2.914  loss_ce_8: 0.283  loss_mask_8: 0.3989  loss_dice_8: 2.929  time: 1.5360  data_time: 0.0899  lr: 8.1847e-06  max_mem: 21410M
[01/17 11:47:50] d2.utils.events INFO:  eta: 1 day, 7:03:40  iter: 17979  total_loss: 37.25  loss_ce: 0.2904  loss_mask: 0.3868  loss_dice: 2.99  loss_ce_0: 0.5812  loss_mask_0: 0.3836  loss_dice_0: 3.113  loss_ce_1: 0.3259  loss_mask_1: 0.3907  loss_dice_1: 3.031  loss_ce_2: 0.3067  loss_mask_2: 0.3896  loss_dice_2: 3.004  loss_ce_3: 0.3017  loss_mask_3: 0.3879  loss_dice_3: 2.981  loss_ce_4: 0.294  loss_mask_4: 0.3857  loss_dice_4: 2.989  loss_ce_5: 0.2915  loss_mask_5: 0.3861  loss_dice_5: 2.988  loss_ce_6: 0.2908  loss_mask_6: 0.3858  loss_dice_6: 2.99  loss_ce_7: 0.2922  loss_mask_7: 0.3857  loss_dice_7: 2.992  loss_ce_8: 0.2857  loss_mask_8: 0.3852  loss_dice_8: 2.986  time: 1.5360  data_time: 0.1006  lr: 8.1827e-06  max_mem: 21410M
[01/17 11:48:21] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 11:48:22] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 11:48:22] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 11:48:22] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 11:48:37] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0093 s/iter. Inference: 0.1780 s/iter. Eval: 0.2100 s/iter. Total: 0.3973 s/iter. ETA=0:07:09
[01/17 11:48:42] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0111 s/iter. Inference: 0.1815 s/iter. Eval: 0.2429 s/iter. Total: 0.4356 s/iter. ETA=0:07:46
[01/17 11:48:47] d2.evaluation.evaluator INFO: Inference done 36/1093. Dataloading: 0.0106 s/iter. Inference: 0.1805 s/iter. Eval: 0.2259 s/iter. Total: 0.4171 s/iter. ETA=0:07:20
[01/17 11:48:53] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0124 s/iter. Inference: 0.1761 s/iter. Eval: 0.2187 s/iter. Total: 0.4074 s/iter. ETA=0:07:04
[01/17 11:48:58] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0127 s/iter. Inference: 0.1754 s/iter. Eval: 0.2261 s/iter. Total: 0.4142 s/iter. ETA=0:07:07
[01/17 11:49:03] d2.evaluation.evaluator INFO: Inference done 75/1093. Dataloading: 0.0125 s/iter. Inference: 0.1772 s/iter. Eval: 0.2233 s/iter. Total: 0.4131 s/iter. ETA=0:07:00
[01/17 11:49:08] d2.evaluation.evaluator INFO: Inference done 87/1093. Dataloading: 0.0126 s/iter. Inference: 0.1739 s/iter. Eval: 0.2271 s/iter. Total: 0.4137 s/iter. ETA=0:06:56
[01/17 11:49:13] d2.evaluation.evaluator INFO: Inference done 99/1093. Dataloading: 0.0126 s/iter. Inference: 0.1732 s/iter. Eval: 0.2304 s/iter. Total: 0.4162 s/iter. ETA=0:06:53
[01/17 11:49:19] d2.evaluation.evaluator INFO: Inference done 113/1093. Dataloading: 0.0123 s/iter. Inference: 0.1719 s/iter. Eval: 0.2260 s/iter. Total: 0.4103 s/iter. ETA=0:06:42
[01/17 11:49:24] d2.evaluation.evaluator INFO: Inference done 126/1093. Dataloading: 0.0122 s/iter. Inference: 0.1709 s/iter. Eval: 0.2269 s/iter. Total: 0.4102 s/iter. ETA=0:06:36
[01/17 11:49:29] d2.evaluation.evaluator INFO: Inference done 139/1093. Dataloading: 0.0122 s/iter. Inference: 0.1709 s/iter. Eval: 0.2250 s/iter. Total: 0.4082 s/iter. ETA=0:06:29
[01/17 11:49:34] d2.evaluation.evaluator INFO: Inference done 153/1093. Dataloading: 0.0121 s/iter. Inference: 0.1719 s/iter. Eval: 0.2201 s/iter. Total: 0.4042 s/iter. ETA=0:06:19
[01/17 11:49:39] d2.evaluation.evaluator INFO: Inference done 166/1093. Dataloading: 0.0120 s/iter. Inference: 0.1714 s/iter. Eval: 0.2204 s/iter. Total: 0.4039 s/iter. ETA=0:06:14
[01/17 11:49:45] d2.evaluation.evaluator INFO: Inference done 180/1093. Dataloading: 0.0120 s/iter. Inference: 0.1708 s/iter. Eval: 0.2190 s/iter. Total: 0.4020 s/iter. ETA=0:06:06
[01/17 11:49:50] d2.evaluation.evaluator INFO: Inference done 194/1093. Dataloading: 0.0119 s/iter. Inference: 0.1702 s/iter. Eval: 0.2176 s/iter. Total: 0.3999 s/iter. ETA=0:05:59
[01/17 11:49:55] d2.evaluation.evaluator INFO: Inference done 207/1093. Dataloading: 0.0120 s/iter. Inference: 0.1702 s/iter. Eval: 0.2170 s/iter. Total: 0.3993 s/iter. ETA=0:05:53
[01/17 11:50:00] d2.evaluation.evaluator INFO: Inference done 221/1093. Dataloading: 0.0119 s/iter. Inference: 0.1701 s/iter. Eval: 0.2157 s/iter. Total: 0.3979 s/iter. ETA=0:05:46
[01/17 11:50:05] d2.evaluation.evaluator INFO: Inference done 235/1093. Dataloading: 0.0119 s/iter. Inference: 0.1696 s/iter. Eval: 0.2142 s/iter. Total: 0.3957 s/iter. ETA=0:05:39
[01/17 11:50:10] d2.evaluation.evaluator INFO: Inference done 246/1093. Dataloading: 0.0120 s/iter. Inference: 0.1710 s/iter. Eval: 0.2164 s/iter. Total: 0.3995 s/iter. ETA=0:05:38
[01/17 11:50:16] d2.evaluation.evaluator INFO: Inference done 259/1093. Dataloading: 0.0120 s/iter. Inference: 0.1708 s/iter. Eval: 0.2168 s/iter. Total: 0.3998 s/iter. ETA=0:05:33
[01/17 11:50:21] d2.evaluation.evaluator INFO: Inference done 273/1093. Dataloading: 0.0119 s/iter. Inference: 0.1708 s/iter. Eval: 0.2160 s/iter. Total: 0.3988 s/iter. ETA=0:05:27
[01/17 11:50:26] d2.evaluation.evaluator INFO: Inference done 287/1093. Dataloading: 0.0119 s/iter. Inference: 0.1706 s/iter. Eval: 0.2149 s/iter. Total: 0.3976 s/iter. ETA=0:05:20
[01/17 11:50:31] d2.evaluation.evaluator INFO: Inference done 300/1093. Dataloading: 0.0119 s/iter. Inference: 0.1706 s/iter. Eval: 0.2146 s/iter. Total: 0.3973 s/iter. ETA=0:05:15
[01/17 11:50:37] d2.evaluation.evaluator INFO: Inference done 312/1093. Dataloading: 0.0120 s/iter. Inference: 0.1710 s/iter. Eval: 0.2163 s/iter. Total: 0.3994 s/iter. ETA=0:05:11
[01/17 11:50:42] d2.evaluation.evaluator INFO: Inference done 324/1093. Dataloading: 0.0121 s/iter. Inference: 0.1720 s/iter. Eval: 0.2166 s/iter. Total: 0.4008 s/iter. ETA=0:05:08
[01/17 11:50:47] d2.evaluation.evaluator INFO: Inference done 339/1093. Dataloading: 0.0119 s/iter. Inference: 0.1719 s/iter. Eval: 0.2141 s/iter. Total: 0.3981 s/iter. ETA=0:05:00
[01/17 11:50:52] d2.evaluation.evaluator INFO: Inference done 353/1093. Dataloading: 0.0119 s/iter. Inference: 0.1724 s/iter. Eval: 0.2122 s/iter. Total: 0.3965 s/iter. ETA=0:04:53
[01/17 11:50:57] d2.evaluation.evaluator INFO: Inference done 365/1093. Dataloading: 0.0120 s/iter. Inference: 0.1729 s/iter. Eval: 0.2124 s/iter. Total: 0.3974 s/iter. ETA=0:04:49
[01/17 11:51:02] d2.evaluation.evaluator INFO: Inference done 379/1093. Dataloading: 0.0119 s/iter. Inference: 0.1730 s/iter. Eval: 0.2110 s/iter. Total: 0.3960 s/iter. ETA=0:04:42
[01/17 11:51:07] d2.evaluation.evaluator INFO: Inference done 392/1093. Dataloading: 0.0119 s/iter. Inference: 0.1726 s/iter. Eval: 0.2114 s/iter. Total: 0.3960 s/iter. ETA=0:04:37
[01/17 11:51:13] d2.evaluation.evaluator INFO: Inference done 406/1093. Dataloading: 0.0119 s/iter. Inference: 0.1728 s/iter. Eval: 0.2105 s/iter. Total: 0.3954 s/iter. ETA=0:04:31
[01/17 11:51:18] d2.evaluation.evaluator INFO: Inference done 419/1093. Dataloading: 0.0119 s/iter. Inference: 0.1726 s/iter. Eval: 0.2106 s/iter. Total: 0.3952 s/iter. ETA=0:04:26
[01/17 11:51:23] d2.evaluation.evaluator INFO: Inference done 432/1093. Dataloading: 0.0119 s/iter. Inference: 0.1725 s/iter. Eval: 0.2109 s/iter. Total: 0.3955 s/iter. ETA=0:04:21
[01/17 11:51:28] d2.evaluation.evaluator INFO: Inference done 447/1093. Dataloading: 0.0118 s/iter. Inference: 0.1727 s/iter. Eval: 0.2096 s/iter. Total: 0.3943 s/iter. ETA=0:04:14
[01/17 11:51:34] d2.evaluation.evaluator INFO: Inference done 460/1093. Dataloading: 0.0118 s/iter. Inference: 0.1725 s/iter. Eval: 0.2099 s/iter. Total: 0.3944 s/iter. ETA=0:04:09
[01/17 11:51:39] d2.evaluation.evaluator INFO: Inference done 475/1093. Dataloading: 0.0117 s/iter. Inference: 0.1730 s/iter. Eval: 0.2083 s/iter. Total: 0.3932 s/iter. ETA=0:04:02
[01/17 11:51:44] d2.evaluation.evaluator INFO: Inference done 488/1093. Dataloading: 0.0117 s/iter. Inference: 0.1739 s/iter. Eval: 0.2077 s/iter. Total: 0.3934 s/iter. ETA=0:03:58
[01/17 11:51:50] d2.evaluation.evaluator INFO: Inference done 504/1093. Dataloading: 0.0117 s/iter. Inference: 0.1735 s/iter. Eval: 0.2061 s/iter. Total: 0.3914 s/iter. ETA=0:03:50
[01/17 11:51:55] d2.evaluation.evaluator INFO: Inference done 518/1093. Dataloading: 0.0117 s/iter. Inference: 0.1730 s/iter. Eval: 0.2057 s/iter. Total: 0.3905 s/iter. ETA=0:03:44
[01/17 11:52:00] d2.evaluation.evaluator INFO: Inference done 530/1093. Dataloading: 0.0117 s/iter. Inference: 0.1730 s/iter. Eval: 0.2070 s/iter. Total: 0.3918 s/iter. ETA=0:03:40
[01/17 11:52:05] d2.evaluation.evaluator INFO: Inference done 544/1093. Dataloading: 0.0117 s/iter. Inference: 0.1735 s/iter. Eval: 0.2060 s/iter. Total: 0.3913 s/iter. ETA=0:03:34
[01/17 11:52:10] d2.evaluation.evaluator INFO: Inference done 558/1093. Dataloading: 0.0117 s/iter. Inference: 0.1730 s/iter. Eval: 0.2059 s/iter. Total: 0.3906 s/iter. ETA=0:03:28
[01/17 11:52:15] d2.evaluation.evaluator INFO: Inference done 572/1093. Dataloading: 0.0116 s/iter. Inference: 0.1731 s/iter. Eval: 0.2054 s/iter. Total: 0.3903 s/iter. ETA=0:03:23
[01/17 11:52:21] d2.evaluation.evaluator INFO: Inference done 588/1093. Dataloading: 0.0116 s/iter. Inference: 0.1734 s/iter. Eval: 0.2035 s/iter. Total: 0.3887 s/iter. ETA=0:03:16
[01/17 11:52:26] d2.evaluation.evaluator INFO: Inference done 602/1093. Dataloading: 0.0116 s/iter. Inference: 0.1730 s/iter. Eval: 0.2038 s/iter. Total: 0.3886 s/iter. ETA=0:03:10
[01/17 11:52:31] d2.evaluation.evaluator INFO: Inference done 615/1093. Dataloading: 0.0116 s/iter. Inference: 0.1731 s/iter. Eval: 0.2042 s/iter. Total: 0.3890 s/iter. ETA=0:03:05
[01/17 11:52:37] d2.evaluation.evaluator INFO: Inference done 629/1093. Dataloading: 0.0116 s/iter. Inference: 0.1727 s/iter. Eval: 0.2041 s/iter. Total: 0.3886 s/iter. ETA=0:03:00
[01/17 11:52:42] d2.evaluation.evaluator INFO: Inference done 644/1093. Dataloading: 0.0116 s/iter. Inference: 0.1729 s/iter. Eval: 0.2032 s/iter. Total: 0.3878 s/iter. ETA=0:02:54
[01/17 11:52:47] d2.evaluation.evaluator INFO: Inference done 658/1093. Dataloading: 0.0116 s/iter. Inference: 0.1730 s/iter. Eval: 0.2029 s/iter. Total: 0.3875 s/iter. ETA=0:02:48
[01/17 11:52:52] d2.evaluation.evaluator INFO: Inference done 671/1093. Dataloading: 0.0116 s/iter. Inference: 0.1733 s/iter. Eval: 0.2027 s/iter. Total: 0.3877 s/iter. ETA=0:02:43
[01/17 11:52:58] d2.evaluation.evaluator INFO: Inference done 686/1093. Dataloading: 0.0116 s/iter. Inference: 0.1736 s/iter. Eval: 0.2014 s/iter. Total: 0.3867 s/iter. ETA=0:02:37
[01/17 11:53:03] d2.evaluation.evaluator INFO: Inference done 701/1093. Dataloading: 0.0116 s/iter. Inference: 0.1734 s/iter. Eval: 0.2008 s/iter. Total: 0.3859 s/iter. ETA=0:02:31
[01/17 11:53:08] d2.evaluation.evaluator INFO: Inference done 713/1093. Dataloading: 0.0116 s/iter. Inference: 0.1733 s/iter. Eval: 0.2015 s/iter. Total: 0.3865 s/iter. ETA=0:02:26
[01/17 11:53:13] d2.evaluation.evaluator INFO: Inference done 728/1093. Dataloading: 0.0116 s/iter. Inference: 0.1732 s/iter. Eval: 0.2010 s/iter. Total: 0.3859 s/iter. ETA=0:02:20
[01/17 11:53:18] d2.evaluation.evaluator INFO: Inference done 744/1093. Dataloading: 0.0115 s/iter. Inference: 0.1728 s/iter. Eval: 0.2002 s/iter. Total: 0.3846 s/iter. ETA=0:02:14
[01/17 11:53:24] d2.evaluation.evaluator INFO: Inference done 760/1093. Dataloading: 0.0115 s/iter. Inference: 0.1724 s/iter. Eval: 0.1994 s/iter. Total: 0.3834 s/iter. ETA=0:02:07
[01/17 11:53:29] d2.evaluation.evaluator INFO: Inference done 771/1093. Dataloading: 0.0115 s/iter. Inference: 0.1728 s/iter. Eval: 0.2001 s/iter. Total: 0.3844 s/iter. ETA=0:02:03
[01/17 11:53:34] d2.evaluation.evaluator INFO: Inference done 785/1093. Dataloading: 0.0114 s/iter. Inference: 0.1730 s/iter. Eval: 0.1994 s/iter. Total: 0.3840 s/iter. ETA=0:01:58
[01/17 11:53:39] d2.evaluation.evaluator INFO: Inference done 800/1093. Dataloading: 0.0114 s/iter. Inference: 0.1730 s/iter. Eval: 0.1987 s/iter. Total: 0.3832 s/iter. ETA=0:01:52
[01/17 11:53:44] d2.evaluation.evaluator INFO: Inference done 815/1093. Dataloading: 0.0114 s/iter. Inference: 0.1729 s/iter. Eval: 0.1981 s/iter. Total: 0.3825 s/iter. ETA=0:01:46
[01/17 11:53:49] d2.evaluation.evaluator INFO: Inference done 830/1093. Dataloading: 0.0114 s/iter. Inference: 0.1728 s/iter. Eval: 0.1973 s/iter. Total: 0.3816 s/iter. ETA=0:01:40
[01/17 11:53:54] d2.evaluation.evaluator INFO: Inference done 845/1093. Dataloading: 0.0113 s/iter. Inference: 0.1728 s/iter. Eval: 0.1970 s/iter. Total: 0.3813 s/iter. ETA=0:01:34
[01/17 11:53:59] d2.evaluation.evaluator INFO: Inference done 856/1093. Dataloading: 0.0113 s/iter. Inference: 0.1729 s/iter. Eval: 0.1978 s/iter. Total: 0.3822 s/iter. ETA=0:01:30
[01/17 11:54:05] d2.evaluation.evaluator INFO: Inference done 869/1093. Dataloading: 0.0113 s/iter. Inference: 0.1728 s/iter. Eval: 0.1982 s/iter. Total: 0.3825 s/iter. ETA=0:01:25
[01/17 11:54:10] d2.evaluation.evaluator INFO: Inference done 883/1093. Dataloading: 0.0113 s/iter. Inference: 0.1726 s/iter. Eval: 0.1983 s/iter. Total: 0.3823 s/iter. ETA=0:01:20
[01/17 11:54:15] d2.evaluation.evaluator INFO: Inference done 894/1093. Dataloading: 0.0114 s/iter. Inference: 0.1729 s/iter. Eval: 0.1992 s/iter. Total: 0.3835 s/iter. ETA=0:01:16
[01/17 11:54:20] d2.evaluation.evaluator INFO: Inference done 909/1093. Dataloading: 0.0113 s/iter. Inference: 0.1729 s/iter. Eval: 0.1985 s/iter. Total: 0.3828 s/iter. ETA=0:01:10
[01/17 11:54:25] d2.evaluation.evaluator INFO: Inference done 922/1093. Dataloading: 0.0113 s/iter. Inference: 0.1730 s/iter. Eval: 0.1985 s/iter. Total: 0.3829 s/iter. ETA=0:01:05
[01/17 11:54:30] d2.evaluation.evaluator INFO: Inference done 935/1093. Dataloading: 0.0113 s/iter. Inference: 0.1731 s/iter. Eval: 0.1984 s/iter. Total: 0.3830 s/iter. ETA=0:01:00
[01/17 11:54:35] d2.evaluation.evaluator INFO: Inference done 947/1093. Dataloading: 0.0113 s/iter. Inference: 0.1735 s/iter. Eval: 0.1985 s/iter. Total: 0.3834 s/iter. ETA=0:00:55
[01/17 11:54:41] d2.evaluation.evaluator INFO: Inference done 961/1093. Dataloading: 0.0113 s/iter. Inference: 0.1734 s/iter. Eval: 0.1984 s/iter. Total: 0.3832 s/iter. ETA=0:00:50
[01/17 11:54:46] d2.evaluation.evaluator INFO: Inference done 975/1093. Dataloading: 0.0113 s/iter. Inference: 0.1734 s/iter. Eval: 0.1983 s/iter. Total: 0.3832 s/iter. ETA=0:00:45
[01/17 11:54:51] d2.evaluation.evaluator INFO: Inference done 989/1093. Dataloading: 0.0113 s/iter. Inference: 0.1736 s/iter. Eval: 0.1978 s/iter. Total: 0.3828 s/iter. ETA=0:00:39
[01/17 11:54:56] d2.evaluation.evaluator INFO: Inference done 1003/1093. Dataloading: 0.0113 s/iter. Inference: 0.1738 s/iter. Eval: 0.1975 s/iter. Total: 0.3828 s/iter. ETA=0:00:34
[01/17 11:55:02] d2.evaluation.evaluator INFO: Inference done 1016/1093. Dataloading: 0.0113 s/iter. Inference: 0.1740 s/iter. Eval: 0.1978 s/iter. Total: 0.3833 s/iter. ETA=0:00:29
[01/17 11:55:07] d2.evaluation.evaluator INFO: Inference done 1029/1093. Dataloading: 0.0113 s/iter. Inference: 0.1741 s/iter. Eval: 0.1980 s/iter. Total: 0.3835 s/iter. ETA=0:00:24
[01/17 11:55:12] d2.evaluation.evaluator INFO: Inference done 1042/1093. Dataloading: 0.0113 s/iter. Inference: 0.1741 s/iter. Eval: 0.1981 s/iter. Total: 0.3836 s/iter. ETA=0:00:19
[01/17 11:55:17] d2.evaluation.evaluator INFO: Inference done 1053/1093. Dataloading: 0.0113 s/iter. Inference: 0.1745 s/iter. Eval: 0.1984 s/iter. Total: 0.3843 s/iter. ETA=0:00:15
[01/17 11:55:22] d2.evaluation.evaluator INFO: Inference done 1069/1093. Dataloading: 0.0112 s/iter. Inference: 0.1743 s/iter. Eval: 0.1977 s/iter. Total: 0.3834 s/iter. ETA=0:00:09
[01/17 11:55:27] d2.evaluation.evaluator INFO: Inference done 1084/1093. Dataloading: 0.0112 s/iter. Inference: 0.1741 s/iter. Eval: 0.1976 s/iter. Total: 0.3830 s/iter. ETA=0:00:03
[01/17 11:55:31] d2.evaluation.evaluator INFO: Total inference time: 0:06:56.508434 (0.382820 s / iter per device, on 4 devices)
[01/17 11:55:31] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:09 (0.173821 s / iter per device, on 4 devices)
[01/17 11:55:54] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 13.608299397292544, 'fwIoU': 40.23027868930241, 'IoU-1': nan, 'IoU-2': 94.93069258118315, 'IoU-3': 45.31455702411273, 'IoU-4': 56.119298865292066, 'IoU-5': 48.356241083312746, 'IoU-6': 41.92573783514044, 'IoU-7': 39.078174726853476, 'IoU-8': 34.423335799805656, 'IoU-9': 24.70170586554778, 'IoU-10': 32.10487208241464, 'IoU-11': 39.50565432015162, 'IoU-12': 47.56467233906245, 'IoU-13': 47.870115297982004, 'IoU-14': 46.451864269247785, 'IoU-15': 47.905573656807135, 'IoU-16': 45.89845538966478, 'IoU-17': 45.24823082750709, 'IoU-18': 42.184161543886894, 'IoU-19': 43.08100160982909, 'IoU-20': 42.884440579443286, 'IoU-21': 43.901266121223436, 'IoU-22': 43.848474949554785, 'IoU-23': 44.85815444875905, 'IoU-24': 43.16106468440697, 'IoU-25': 42.39770548648002, 'IoU-26': 42.16917840182509, 'IoU-27': 39.91898396031403, 'IoU-28': 42.30366889403393, 'IoU-29': 42.274380092520836, 'IoU-30': 41.91504345500947, 'IoU-31': 38.32490143850908, 'IoU-32': 40.8422518935788, 'IoU-33': 39.70204963765872, 'IoU-34': 40.16184666028232, 'IoU-35': 40.88338878609156, 'IoU-36': 40.03102680282824, 'IoU-37': 41.4828545139828, 'IoU-38': 39.940642945719524, 'IoU-39': 40.51599261911022, 'IoU-40': 39.23011100647486, 'IoU-41': 39.725524191831354, 'IoU-42': 37.65911212126864, 'IoU-43': 37.650371262300816, 'IoU-44': 36.66673532702675, 'IoU-45': 35.78003342290829, 'IoU-46': 36.02746550636666, 'IoU-47': 34.31459771640508, 'IoU-48': 34.071574517977346, 'IoU-49': 34.10868658960881, 'IoU-50': 33.50155732736964, 'IoU-51': 33.514679843232535, 'IoU-52': 31.448163455111928, 'IoU-53': 29.42636814852048, 'IoU-54': 30.482481888454714, 'IoU-55': 30.80783888020254, 'IoU-56': 29.099163046106565, 'IoU-57': 28.53172697123105, 'IoU-58': 26.906175442151504, 'IoU-59': 26.39593233109793, 'IoU-60': 26.170487467395553, 'IoU-61': 24.92225756326083, 'IoU-62': 25.19496906134744, 'IoU-63': 24.71830118521127, 'IoU-64': 25.11051813578163, 'IoU-65': 24.037116397812426, 'IoU-66': 23.072420955303826, 'IoU-67': 23.03846927323521, 'IoU-68': 22.219290017477046, 'IoU-69': 19.840843099681322, 'IoU-70': 20.43894634433033, 'IoU-71': 19.851939324246114, 'IoU-72': 18.766593422582257, 'IoU-73': 17.92965124361801, 'IoU-74': 17.764459393691077, 'IoU-75': 19.57930262451841, 'IoU-76': 18.86676847709079, 'IoU-77': 19.509438521586336, 'IoU-78': 18.41501574284963, 'IoU-79': 16.94639444836963, 'IoU-80': 17.67587461366985, 'IoU-81': 16.559552329733613, 'IoU-82': 18.524090360571712, 'IoU-83': 15.077213442270368, 'IoU-84': 17.725397194699756, 'IoU-85': 17.59582411784997, 'IoU-86': 15.920883359807236, 'IoU-87': 16.969627021221136, 'IoU-88': 16.15452647425421, 'IoU-89': 16.362565961347048, 'IoU-90': 16.58226870582211, 'IoU-91': 16.507361253736207, 'IoU-92': 14.363368563363682, 'IoU-93': 17.554331790299667, 'IoU-94': 16.121488215981348, 'IoU-95': 16.104953131155558, 'IoU-96': 15.822602364969901, 'IoU-97': 16.072434694154744, 'IoU-98': 16.28400766218255, 'IoU-99': 16.504540680897495, 'IoU-100': 17.163206890373413, 'IoU-101': 14.116796160126244, 'IoU-102': 15.304991781935032, 'IoU-103': 12.41866005322134, 'IoU-104': 14.999479971196115, 'IoU-105': 12.90923410481906, 'IoU-106': 10.042639325128148, 'IoU-107': 10.205226349840792, 'IoU-108': 10.814972497591597, 'IoU-109': 11.86198972454754, 'IoU-110': 10.733338341180492, 'IoU-111': 11.330214217698174, 'IoU-112': 11.443363823362374, 'IoU-113': 10.53439888625145, 'IoU-114': 9.880612596744852, 'IoU-115': 9.047558912301925, 'IoU-116': 8.33484328244489, 'IoU-117': 6.486436399993639, 'IoU-118': 8.723409802148801, 'IoU-119': 7.640632981937114, 'IoU-120': 6.608442913271145, 'IoU-121': 6.978240608556853, 'IoU-122': 5.848136628418716, 'IoU-123': 6.693906856701759, 'IoU-124': 4.79684821594142, 'IoU-125': 5.560004000837745, 'IoU-126': 5.209102561322332, 'IoU-127': 5.888932018191776, 'IoU-128': 6.090370214745562, 'IoU-129': 4.291360991070521, 'IoU-130': 3.72215386418234, 'IoU-131': 5.011220656254683, 'IoU-132': 4.419020530549164, 'IoU-133': 4.3914210459689915, 'IoU-134': 3.2993572454836793, 'IoU-135': 3.6426217502746243, 'IoU-136': 1.1872460025252072, 'IoU-137': 2.7004326402135557, 'IoU-138': 1.964292150367465, 'IoU-139': 2.22006485269566, 'IoU-140': 1.7271029132626519, 'IoU-141': 3.7198707224511547, 'IoU-142': 1.446551591913892, 'IoU-143': 1.4442565085065506, 'IoU-144': 2.019326225806196, 'IoU-145': 1.616825708599855, 'IoU-146': 1.9886552968145048, 'IoU-147': 1.902425609362497, 'IoU-148': 1.614834899205754, 'IoU-149': 1.8012440683760387, 'IoU-150': 0.6938649348578584, 'IoU-151': 1.4339070626981412, 'IoU-152': 1.3269575262181124, 'IoU-153': 0.8052531857190622, 'IoU-154': 1.5972803410260887, 'IoU-155': 0.9294404810740551, 'IoU-156': 1.774401741685652, 'IoU-157': 2.1375769577992285, 'IoU-158': 0.86593617971353, 'IoU-159': 1.3639814837492958, 'IoU-160': 0.8999687412527243, 'IoU-161': 1.1003379400294682, 'IoU-162': 0.8441606426401103, 'IoU-163': 1.802913531685229, 'IoU-164': 0.8025905397620041, 'IoU-165': 0.6966420034149118, 'IoU-166': 0.37991700274709217, 'IoU-167': 1.1194237876430864, 'IoU-168': 0.9888405320159885, 'IoU-169': 0.1848954170158687, 'IoU-170': 1.116053394615654, 'IoU-171': 0.7816720460036142, 'IoU-172': 2.1452620011777106, 'IoU-173': 1.155052363576391, 'IoU-174': 0.5202738946310934, 'IoU-175': 1.6568419621586845, 'IoU-176': 1.0997166919903714, 'IoU-177': 0.8144499536563821, 'IoU-178': 0.9454820226113689, 'IoU-179': 0.47896679791303903, 'IoU-180': 2.0164294076750795, 'IoU-181': 0.5786894235382822, 'IoU-182': 1.0200235508031315, 'IoU-183': 1.6314729777092019, 'IoU-184': 0.5261129555512736, 'IoU-185': 0.4927774114204023, 'IoU-186': 0.433441244016491, 'IoU-187': 0.6942475391333673, 'IoU-188': 1.1988776198015456, 'IoU-189': 0.9991929760296709, 'IoU-190': 1.2944311426243682, 'IoU-191': 1.2394677447227445, 'IoU-192': 1.3890557685630733, 'IoU-193': 0.3294354727615509, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 20.738887682643462, 'pACC': 54.51548338770452, 'ACC-1': nan, 'ACC-2': 98.5435429756291, 'ACC-3': 62.60490994828831, 'ACC-4': 73.42825319872168, 'ACC-5': 65.08581443592044, 'ACC-6': 58.47330436854822, 'ACC-7': 54.275770604497474, 'ACC-8': 49.27803362613081, 'ACC-9': 31.875529527508778, 'ACC-10': 40.883894027478426, 'ACC-11': 61.5359880104385, 'ACC-12': 64.9366794872284, 'ACC-13': 66.84981957733937, 'ACC-14': 64.2709447399445, 'ACC-15': 67.20515407788879, 'ACC-16': 65.64909506137401, 'ACC-17': 62.557288648935284, 'ACC-18': 61.07284417389592, 'ACC-19': 60.95364534512624, 'ACC-20': 58.30667008449697, 'ACC-21': 58.94156376492289, 'ACC-22': 58.62972488997072, 'ACC-23': 59.44852290788247, 'ACC-24': 59.7471257613299, 'ACC-25': 58.11895522262872, 'ACC-26': 60.729803891286835, 'ACC-27': 55.795762874865154, 'ACC-28': 60.237733337055055, 'ACC-29': 60.770569111037844, 'ACC-30': 57.80425389103233, 'ACC-31': 55.50982834955482, 'ACC-32': 57.77134336239516, 'ACC-33': 55.38774919781433, 'ACC-34': 60.47289853967533, 'ACC-35': 59.47422540999846, 'ACC-36': 59.421552842746316, 'ACC-37': 59.63244170963584, 'ACC-38': 57.14069140351238, 'ACC-39': 57.54707753068937, 'ACC-40': 54.33726647231825, 'ACC-41': 57.17119554666941, 'ACC-42': 57.227862533216936, 'ACC-43': 53.30991386714289, 'ACC-44': 52.49488779433524, 'ACC-45': 52.49156352551994, 'ACC-46': 51.28672379788265, 'ACC-47': 51.61226417229594, 'ACC-48': 49.89685414624606, 'ACC-49': 51.797787077624825, 'ACC-50': 51.916616387008254, 'ACC-51': 50.70031753668466, 'ACC-52': 49.15185195464102, 'ACC-53': 45.332495761622354, 'ACC-54': 44.22027245100608, 'ACC-55': 47.26002964843127, 'ACC-56': 42.82651936552481, 'ACC-57': 43.70756825602361, 'ACC-58': 42.84242625366534, 'ACC-59': 40.80470494424491, 'ACC-60': 41.3347164828829, 'ACC-61': 38.01809371257381, 'ACC-62': 41.51160639877772, 'ACC-63': 42.23941710017995, 'ACC-64': 39.60058922745242, 'ACC-65': 38.52416326228646, 'ACC-66': 35.504684485334735, 'ACC-67': 36.21517879372747, 'ACC-68': 36.826780466055816, 'ACC-69': 39.971636242741724, 'ACC-70': 33.24062107994681, 'ACC-71': 30.57925774116465, 'ACC-72': 30.945721032004396, 'ACC-73': 33.191689674434514, 'ACC-74': 26.703296120471908, 'ACC-75': 34.821155373464904, 'ACC-76': 31.49730436292437, 'ACC-77': 30.987172953524993, 'ACC-78': 28.89652927436157, 'ACC-79': 27.000749594090028, 'ACC-80': 31.675962029076647, 'ACC-81': 28.47055438482109, 'ACC-82': 34.37425555418901, 'ACC-83': 23.508845318924056, 'ACC-84': 29.2444841052594, 'ACC-85': 31.472790161475324, 'ACC-86': 26.163232609952537, 'ACC-87': 29.48322262652327, 'ACC-88': 26.74100190958305, 'ACC-89': 26.20244013470336, 'ACC-90': 28.861238339769475, 'ACC-91': 27.89478428699761, 'ACC-92': 21.86473387090848, 'ACC-93': 35.74369144867918, 'ACC-94': 25.751413063606755, 'ACC-95': 27.181648384263752, 'ACC-96': 25.512152586497265, 'ACC-97': 27.391371901701767, 'ACC-98': 28.181684483486848, 'ACC-99': 28.246737479552547, 'ACC-100': 29.574720747419597, 'ACC-101': 24.810302421365822, 'ACC-102': 29.212899198894032, 'ACC-103': 19.427367467149978, 'ACC-104': 28.538419760083123, 'ACC-105': 26.952313027225106, 'ACC-106': 17.709129570780714, 'ACC-107': 19.02635270091781, 'ACC-108': 18.929578476038795, 'ACC-109': 23.024673428536765, 'ACC-110': 17.661290208268664, 'ACC-111': 19.61155846028913, 'ACC-112': 20.876178083732988, 'ACC-113': 23.570858015707298, 'ACC-114': 18.293930897924717, 'ACC-115': 17.858911653329066, 'ACC-116': 16.52894922797047, 'ACC-117': 12.986035017002283, 'ACC-118': 16.80695669225212, 'ACC-119': 15.467458706963985, 'ACC-120': 11.481825079336078, 'ACC-121': 13.16177168218092, 'ACC-122': 11.331983119756435, 'ACC-123': 13.291488578787963, 'ACC-124': 8.372872624928702, 'ACC-125': 10.720254264421573, 'ACC-126': 9.928504845202603, 'ACC-127': 10.8465810253281, 'ACC-128': 11.851923669971685, 'ACC-129': 9.963763475876672, 'ACC-130': 5.979634265188801, 'ACC-131': 10.09246910172504, 'ACC-132': 7.6199913206039955, 'ACC-133': 9.62545197427726, 'ACC-134': 6.067212367402206, 'ACC-135': 9.772270573028868, 'ACC-136': 1.7402086540106037, 'ACC-137': 6.527572330413715, 'ACC-138': 3.2029045570313928, 'ACC-139': 4.549283574891489, 'ACC-140': 3.189744153119525, 'ACC-141': 10.59290658901053, 'ACC-142': 2.444545804937902, 'ACC-143': 2.8321635266592784, 'ACC-144': 4.344688831319141, 'ACC-145': 2.820126353790614, 'ACC-146': 4.243833717211923, 'ACC-147': 3.6413150259304103, 'ACC-148': 3.0735861197437657, 'ACC-149': 3.946481681971691, 'ACC-150': 1.0768009135263354, 'ACC-151': 2.572625439202676, 'ACC-152': 1.9294818301088263, 'ACC-153': 1.0962472182884886, 'ACC-154': 2.4303074442551162, 'ACC-155': 1.3886883018832479, 'ACC-156': 3.6766971599578957, 'ACC-157': 4.615951443696795, 'ACC-158': 1.372471360344506, 'ACC-159': 2.357910722022832, 'ACC-160': 1.1714352953185119, 'ACC-161': 1.91320182780618, 'ACC-162': 1.0907753098346706, 'ACC-163': 2.932005557853145, 'ACC-164': 1.2351586062886084, 'ACC-165': 0.9703227115763305, 'ACC-166': 0.412925193836615, 'ACC-167': 2.0899681862823276, 'ACC-168': 1.8581923229922854, 'ACC-169': 0.20131514171179407, 'ACC-170': 1.745239895910285, 'ACC-171': 1.011719692775104, 'ACC-172': 4.998815228991439, 'ACC-173': 3.3205541356447017, 'ACC-174': 0.7463697493537232, 'ACC-175': 3.9073145181283215, 'ACC-176': 2.262798148939042, 'ACC-177': 1.0854423032273044, 'ACC-178': 1.2792759873696409, 'ACC-179': 0.5796670343106481, 'ACC-180': 8.397158746228806, 'ACC-181': 0.714763452876833, 'ACC-182': 6.30108886946594, 'ACC-183': 3.6724482277434376, 'ACC-184': 0.7842241689837104, 'ACC-185': 0.965763165156114, 'ACC-186': 0.7235933465441663, 'ACC-187': 2.075007140951765, 'ACC-188': 2.7717174645743485, 'ACC-189': 1.7611779185666439, 'ACC-190': 4.670351534320358, 'ACC-191': 6.201463051657462, 'ACC-192': 4.375334420880914, 'ACC-193': 0.47372308423724635, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 11:55:54] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 11:55:54] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 11:55:54] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 11:55:54] d2.evaluation.testing INFO: copypaste: 13.6083,40.2303,20.7389,54.5155
[01/17 11:55:55] d2.utils.events INFO:  eta: 1 day, 7:03:20  iter: 17999  total_loss: 37.57  loss_ce: 0.3163  loss_mask: 0.4107  loss_dice: 2.984  loss_ce_0: 0.5723  loss_mask_0: 0.411  loss_dice_0: 3.088  loss_ce_1: 0.3343  loss_mask_1: 0.418  loss_dice_1: 3.024  loss_ce_2: 0.3259  loss_mask_2: 0.4141  loss_dice_2: 3.01  loss_ce_3: 0.3177  loss_mask_3: 0.4156  loss_dice_3: 2.996  loss_ce_4: 0.3055  loss_mask_4: 0.4142  loss_dice_4: 2.982  loss_ce_5: 0.3096  loss_mask_5: 0.4127  loss_dice_5: 2.995  loss_ce_6: 0.3076  loss_mask_6: 0.413  loss_dice_6: 2.987  loss_ce_7: 0.3069  loss_mask_7: 0.4108  loss_dice_7: 2.991  loss_ce_8: 0.3039  loss_mask_8: 0.4106  loss_dice_8: 2.991  time: 1.5360  data_time: 0.0974  lr: 8.1806e-06  max_mem: 21410M
[01/17 11:56:25] d2.utils.events INFO:  eta: 1 day, 7:02:54  iter: 18019  total_loss: 36.67  loss_ce: 0.2876  loss_mask: 0.4019  loss_dice: 2.951  loss_ce_0: 0.5538  loss_mask_0: 0.3886  loss_dice_0: 3.062  loss_ce_1: 0.3249  loss_mask_1: 0.3974  loss_dice_1: 2.975  loss_ce_2: 0.3081  loss_mask_2: 0.3966  loss_dice_2: 2.951  loss_ce_3: 0.3058  loss_mask_3: 0.4005  loss_dice_3: 2.949  loss_ce_4: 0.2933  loss_mask_4: 0.3991  loss_dice_4: 2.958  loss_ce_5: 0.2868  loss_mask_5: 0.3971  loss_dice_5: 2.949  loss_ce_6: 0.2982  loss_mask_6: 0.3982  loss_dice_6: 2.943  loss_ce_7: 0.2839  loss_mask_7: 0.3994  loss_dice_7: 2.95  loss_ce_8: 0.2899  loss_mask_8: 0.402  loss_dice_8: 2.947  time: 1.5360  data_time: 0.0936  lr: 8.1786e-06  max_mem: 21410M
[01/17 11:56:56] d2.utils.events INFO:  eta: 1 day, 7:01:36  iter: 18039  total_loss: 37.2  loss_ce: 0.2657  loss_mask: 0.3973  loss_dice: 2.996  loss_ce_0: 0.5712  loss_mask_0: 0.396  loss_dice_0: 3.119  loss_ce_1: 0.3335  loss_mask_1: 0.4021  loss_dice_1: 3.033  loss_ce_2: 0.3155  loss_mask_2: 0.3962  loss_dice_2: 3.017  loss_ce_3: 0.2936  loss_mask_3: 0.3943  loss_dice_3: 3.004  loss_ce_4: 0.2927  loss_mask_4: 0.3945  loss_dice_4: 2.997  loss_ce_5: 0.2794  loss_mask_5: 0.3974  loss_dice_5: 3.004  loss_ce_6: 0.283  loss_mask_6: 0.3966  loss_dice_6: 3.003  loss_ce_7: 0.285  loss_mask_7: 0.3973  loss_dice_7: 2.998  loss_ce_8: 0.2801  loss_mask_8: 0.3963  loss_dice_8: 2.995  time: 1.5360  data_time: 0.1026  lr: 8.1765e-06  max_mem: 21410M
[01/17 11:57:26] d2.utils.events INFO:  eta: 1 day, 6:59:54  iter: 18059  total_loss: 37.5  loss_ce: 0.2916  loss_mask: 0.391  loss_dice: 3.001  loss_ce_0: 0.5814  loss_mask_0: 0.3812  loss_dice_0: 3.129  loss_ce_1: 0.3114  loss_mask_1: 0.3921  loss_dice_1: 3.045  loss_ce_2: 0.3122  loss_mask_2: 0.3908  loss_dice_2: 3.019  loss_ce_3: 0.3098  loss_mask_3: 0.3915  loss_dice_3: 3.009  loss_ce_4: 0.3045  loss_mask_4: 0.3926  loss_dice_4: 3.007  loss_ce_5: 0.2895  loss_mask_5: 0.3929  loss_dice_5: 3.004  loss_ce_6: 0.2952  loss_mask_6: 0.3932  loss_dice_6: 3.005  loss_ce_7: 0.2823  loss_mask_7: 0.3927  loss_dice_7: 3.005  loss_ce_8: 0.2865  loss_mask_8: 0.3927  loss_dice_8: 3  time: 1.5360  data_time: 0.0985  lr: 8.1745e-06  max_mem: 21410M
[01/17 11:57:57] d2.utils.events INFO:  eta: 1 day, 6:59:49  iter: 18079  total_loss: 37.93  loss_ce: 0.3184  loss_mask: 0.3925  loss_dice: 3.02  loss_ce_0: 0.5819  loss_mask_0: 0.3987  loss_dice_0: 3.128  loss_ce_1: 0.3276  loss_mask_1: 0.3985  loss_dice_1: 3.064  loss_ce_2: 0.3272  loss_mask_2: 0.3936  loss_dice_2: 3.036  loss_ce_3: 0.3294  loss_mask_3: 0.3898  loss_dice_3: 3.034  loss_ce_4: 0.3229  loss_mask_4: 0.3893  loss_dice_4: 3.032  loss_ce_5: 0.3114  loss_mask_5: 0.3904  loss_dice_5: 3.031  loss_ce_6: 0.3287  loss_mask_6: 0.3933  loss_dice_6: 3.039  loss_ce_7: 0.3134  loss_mask_7: 0.3937  loss_dice_7: 3.028  loss_ce_8: 0.3051  loss_mask_8: 0.3938  loss_dice_8: 3.03  time: 1.5360  data_time: 0.0914  lr: 8.1724e-06  max_mem: 21410M
[01/17 11:58:27] d2.utils.events INFO:  eta: 1 day, 6:58:27  iter: 18099  total_loss: 37.35  loss_ce: 0.2995  loss_mask: 0.3904  loss_dice: 2.98  loss_ce_0: 0.5733  loss_mask_0: 0.3913  loss_dice_0: 3.097  loss_ce_1: 0.3273  loss_mask_1: 0.3928  loss_dice_1: 3.021  loss_ce_2: 0.3248  loss_mask_2: 0.3907  loss_dice_2: 2.993  loss_ce_3: 0.3229  loss_mask_3: 0.3897  loss_dice_3: 2.988  loss_ce_4: 0.3029  loss_mask_4: 0.3887  loss_dice_4: 2.982  loss_ce_5: 0.2972  loss_mask_5: 0.3875  loss_dice_5: 2.987  loss_ce_6: 0.2987  loss_mask_6: 0.3892  loss_dice_6: 2.986  loss_ce_7: 0.2907  loss_mask_7: 0.3891  loss_dice_7: 2.989  loss_ce_8: 0.3003  loss_mask_8: 0.3877  loss_dice_8: 2.985  time: 1.5360  data_time: 0.0946  lr: 8.1704e-06  max_mem: 21410M
[01/17 11:58:58] d2.utils.events INFO:  eta: 1 day, 6:55:49  iter: 18119  total_loss: 37.26  loss_ce: 0.3094  loss_mask: 0.3914  loss_dice: 2.969  loss_ce_0: 0.5928  loss_mask_0: 0.3866  loss_dice_0: 3.08  loss_ce_1: 0.3491  loss_mask_1: 0.3949  loss_dice_1: 3.014  loss_ce_2: 0.3416  loss_mask_2: 0.3929  loss_dice_2: 2.987  loss_ce_3: 0.3174  loss_mask_3: 0.3918  loss_dice_3: 2.982  loss_ce_4: 0.3228  loss_mask_4: 0.39  loss_dice_4: 2.974  loss_ce_5: 0.3103  loss_mask_5: 0.3906  loss_dice_5: 2.979  loss_ce_6: 0.3175  loss_mask_6: 0.3917  loss_dice_6: 2.966  loss_ce_7: 0.3019  loss_mask_7: 0.3937  loss_dice_7: 2.962  loss_ce_8: 0.3271  loss_mask_8: 0.3942  loss_dice_8: 2.964  time: 1.5359  data_time: 0.1051  lr: 8.1684e-06  max_mem: 21410M
[01/17 11:59:28] d2.utils.events INFO:  eta: 1 day, 6:54:22  iter: 18139  total_loss: 37.13  loss_ce: 0.2587  loss_mask: 0.4124  loss_dice: 2.966  loss_ce_0: 0.5433  loss_mask_0: 0.4119  loss_dice_0: 3.078  loss_ce_1: 0.3137  loss_mask_1: 0.4213  loss_dice_1: 2.99  loss_ce_2: 0.306  loss_mask_2: 0.4163  loss_dice_2: 2.974  loss_ce_3: 0.2811  loss_mask_3: 0.412  loss_dice_3: 2.969  loss_ce_4: 0.2885  loss_mask_4: 0.4107  loss_dice_4: 2.969  loss_ce_5: 0.2783  loss_mask_5: 0.4115  loss_dice_5: 2.971  loss_ce_6: 0.2788  loss_mask_6: 0.4113  loss_dice_6: 2.971  loss_ce_7: 0.2732  loss_mask_7: 0.412  loss_dice_7: 2.959  loss_ce_8: 0.2796  loss_mask_8: 0.4106  loss_dice_8: 2.97  time: 1.5359  data_time: 0.0946  lr: 8.1663e-06  max_mem: 21410M
[01/17 11:59:59] d2.utils.events INFO:  eta: 1 day, 6:53:22  iter: 18159  total_loss: 36.82  loss_ce: 0.2735  loss_mask: 0.3924  loss_dice: 2.935  loss_ce_0: 0.5459  loss_mask_0: 0.3927  loss_dice_0: 3.049  loss_ce_1: 0.2928  loss_mask_1: 0.3991  loss_dice_1: 2.968  loss_ce_2: 0.2864  loss_mask_2: 0.3948  loss_dice_2: 2.947  loss_ce_3: 0.2834  loss_mask_3: 0.3943  loss_dice_3: 2.939  loss_ce_4: 0.2575  loss_mask_4: 0.394  loss_dice_4: 2.948  loss_ce_5: 0.2606  loss_mask_5: 0.3945  loss_dice_5: 2.942  loss_ce_6: 0.2615  loss_mask_6: 0.3924  loss_dice_6: 2.939  loss_ce_7: 0.2672  loss_mask_7: 0.3926  loss_dice_7: 2.936  loss_ce_8: 0.261  loss_mask_8: 0.3934  loss_dice_8: 2.941  time: 1.5359  data_time: 0.0942  lr: 8.1643e-06  max_mem: 21410M
[01/17 12:00:30] d2.utils.events INFO:  eta: 1 day, 6:52:05  iter: 18179  total_loss: 36.89  loss_ce: 0.2625  loss_mask: 0.3876  loss_dice: 2.971  loss_ce_0: 0.5463  loss_mask_0: 0.3791  loss_dice_0: 3.094  loss_ce_1: 0.299  loss_mask_1: 0.3873  loss_dice_1: 3.006  loss_ce_2: 0.2981  loss_mask_2: 0.3846  loss_dice_2: 2.99  loss_ce_3: 0.2714  loss_mask_3: 0.3872  loss_dice_3: 2.987  loss_ce_4: 0.2718  loss_mask_4: 0.3862  loss_dice_4: 2.976  loss_ce_5: 0.2676  loss_mask_5: 0.3842  loss_dice_5: 2.981  loss_ce_6: 0.2584  loss_mask_6: 0.3859  loss_dice_6: 2.971  loss_ce_7: 0.257  loss_mask_7: 0.388  loss_dice_7: 2.978  loss_ce_8: 0.27  loss_mask_8: 0.3887  loss_dice_8: 2.974  time: 1.5359  data_time: 0.1054  lr: 8.1622e-06  max_mem: 21410M
[01/17 12:01:00] d2.utils.events INFO:  eta: 1 day, 6:49:39  iter: 18199  total_loss: 36.61  loss_ce: 0.2926  loss_mask: 0.3816  loss_dice: 2.932  loss_ce_0: 0.5872  loss_mask_0: 0.3676  loss_dice_0: 3.046  loss_ce_1: 0.3401  loss_mask_1: 0.3815  loss_dice_1: 2.968  loss_ce_2: 0.3137  loss_mask_2: 0.3804  loss_dice_2: 2.942  loss_ce_3: 0.299  loss_mask_3: 0.3783  loss_dice_3: 2.928  loss_ce_4: 0.3125  loss_mask_4: 0.3777  loss_dice_4: 2.926  loss_ce_5: 0.2871  loss_mask_5: 0.3786  loss_dice_5: 2.938  loss_ce_6: 0.2931  loss_mask_6: 0.3771  loss_dice_6: 2.932  loss_ce_7: 0.2982  loss_mask_7: 0.3795  loss_dice_7: 2.925  loss_ce_8: 0.3029  loss_mask_8: 0.381  loss_dice_8: 2.922  time: 1.5359  data_time: 0.0935  lr: 8.1602e-06  max_mem: 21410M
[01/17 12:01:32] d2.utils.events INFO:  eta: 1 day, 6:49:44  iter: 18219  total_loss: 37.3  loss_ce: 0.277  loss_mask: 0.3683  loss_dice: 2.982  loss_ce_0: 0.5524  loss_mask_0: 0.3774  loss_dice_0: 3.099  loss_ce_1: 0.3037  loss_mask_1: 0.3763  loss_dice_1: 3.012  loss_ce_2: 0.3049  loss_mask_2: 0.3705  loss_dice_2: 2.992  loss_ce_3: 0.3112  loss_mask_3: 0.368  loss_dice_3: 2.99  loss_ce_4: 0.2922  loss_mask_4: 0.3657  loss_dice_4: 2.981  loss_ce_5: 0.2936  loss_mask_5: 0.3693  loss_dice_5: 2.988  loss_ce_6: 0.29  loss_mask_6: 0.3709  loss_dice_6: 2.982  loss_ce_7: 0.3005  loss_mask_7: 0.3698  loss_dice_7: 2.971  loss_ce_8: 0.2904  loss_mask_8: 0.3692  loss_dice_8: 2.983  time: 1.5359  data_time: 0.1107  lr: 8.1581e-06  max_mem: 21410M
[01/17 12:02:03] d2.utils.events INFO:  eta: 1 day, 6:49:13  iter: 18239  total_loss: 37.11  loss_ce: 0.2807  loss_mask: 0.3844  loss_dice: 2.973  loss_ce_0: 0.5899  loss_mask_0: 0.3677  loss_dice_0: 3.094  loss_ce_1: 0.3237  loss_mask_1: 0.3798  loss_dice_1: 3.017  loss_ce_2: 0.3248  loss_mask_2: 0.3805  loss_dice_2: 2.999  loss_ce_3: 0.2953  loss_mask_3: 0.379  loss_dice_3: 2.978  loss_ce_4: 0.2874  loss_mask_4: 0.3803  loss_dice_4: 2.985  loss_ce_5: 0.282  loss_mask_5: 0.3797  loss_dice_5: 2.986  loss_ce_6: 0.2876  loss_mask_6: 0.3819  loss_dice_6: 2.974  loss_ce_7: 0.2789  loss_mask_7: 0.383  loss_dice_7: 2.983  loss_ce_8: 0.2767  loss_mask_8: 0.381  loss_dice_8: 2.98  time: 1.5360  data_time: 0.1010  lr: 8.1561e-06  max_mem: 21410M
[01/17 12:02:35] d2.utils.events INFO:  eta: 1 day, 6:49:13  iter: 18259  total_loss: 36.83  loss_ce: 0.2604  loss_mask: 0.388  loss_dice: 2.942  loss_ce_0: 0.5709  loss_mask_0: 0.378  loss_dice_0: 3.063  loss_ce_1: 0.3067  loss_mask_1: 0.3895  loss_dice_1: 2.985  loss_ce_2: 0.2973  loss_mask_2: 0.3857  loss_dice_2: 2.969  loss_ce_3: 0.2787  loss_mask_3: 0.3864  loss_dice_3: 2.952  loss_ce_4: 0.272  loss_mask_4: 0.3886  loss_dice_4: 2.953  loss_ce_5: 0.2633  loss_mask_5: 0.3841  loss_dice_5: 2.958  loss_ce_6: 0.2734  loss_mask_6: 0.3861  loss_dice_6: 2.954  loss_ce_7: 0.2575  loss_mask_7: 0.3858  loss_dice_7: 2.952  loss_ce_8: 0.2643  loss_mask_8: 0.388  loss_dice_8: 2.94  time: 1.5360  data_time: 0.0996  lr: 8.154e-06  max_mem: 21410M
[01/17 12:03:06] d2.utils.events INFO:  eta: 1 day, 6:48:42  iter: 18279  total_loss: 36.64  loss_ce: 0.2936  loss_mask: 0.374  loss_dice: 2.923  loss_ce_0: 0.5594  loss_mask_0: 0.3603  loss_dice_0: 3.063  loss_ce_1: 0.3382  loss_mask_1: 0.3757  loss_dice_1: 2.969  loss_ce_2: 0.321  loss_mask_2: 0.3722  loss_dice_2: 2.95  loss_ce_3: 0.3124  loss_mask_3: 0.3711  loss_dice_3: 2.924  loss_ce_4: 0.2993  loss_mask_4: 0.3733  loss_dice_4: 2.917  loss_ce_5: 0.2897  loss_mask_5: 0.3734  loss_dice_5: 2.92  loss_ce_6: 0.3028  loss_mask_6: 0.3758  loss_dice_6: 2.916  loss_ce_7: 0.2985  loss_mask_7: 0.3736  loss_dice_7: 2.92  loss_ce_8: 0.2839  loss_mask_8: 0.3733  loss_dice_8: 2.922  time: 1.5360  data_time: 0.1131  lr: 8.152e-06  max_mem: 21410M
[01/17 12:03:37] d2.utils.events INFO:  eta: 1 day, 6:47:44  iter: 18299  total_loss: 36.43  loss_ce: 0.2578  loss_mask: 0.3838  loss_dice: 2.93  loss_ce_0: 0.5495  loss_mask_0: 0.3728  loss_dice_0: 3.075  loss_ce_1: 0.2876  loss_mask_1: 0.3892  loss_dice_1: 2.977  loss_ce_2: 0.2711  loss_mask_2: 0.3859  loss_dice_2: 2.954  loss_ce_3: 0.254  loss_mask_3: 0.3855  loss_dice_3: 2.947  loss_ce_4: 0.2602  loss_mask_4: 0.3852  loss_dice_4: 2.954  loss_ce_5: 0.2523  loss_mask_5: 0.3863  loss_dice_5: 2.944  loss_ce_6: 0.2515  loss_mask_6: 0.3864  loss_dice_6: 2.939  loss_ce_7: 0.264  loss_mask_7: 0.3863  loss_dice_7: 2.928  loss_ce_8: 0.2478  loss_mask_8: 0.3842  loss_dice_8: 2.938  time: 1.5360  data_time: 0.0948  lr: 8.1499e-06  max_mem: 21410M
[01/17 12:04:08] d2.utils.events INFO:  eta: 1 day, 6:46:31  iter: 18319  total_loss: 36.16  loss_ce: 0.2656  loss_mask: 0.3911  loss_dice: 2.934  loss_ce_0: 0.58  loss_mask_0: 0.385  loss_dice_0: 3.042  loss_ce_1: 0.3053  loss_mask_1: 0.3988  loss_dice_1: 2.959  loss_ce_2: 0.2867  loss_mask_2: 0.394  loss_dice_2: 2.945  loss_ce_3: 0.2668  loss_mask_3: 0.3927  loss_dice_3: 2.937  loss_ce_4: 0.2862  loss_mask_4: 0.3906  loss_dice_4: 2.934  loss_ce_5: 0.276  loss_mask_5: 0.3924  loss_dice_5: 2.935  loss_ce_6: 0.2806  loss_mask_6: 0.3898  loss_dice_6: 2.937  loss_ce_7: 0.2663  loss_mask_7: 0.3907  loss_dice_7: 2.945  loss_ce_8: 0.2739  loss_mask_8: 0.3911  loss_dice_8: 2.937  time: 1.5360  data_time: 0.1048  lr: 8.1479e-06  max_mem: 21410M
[01/17 12:04:38] d2.utils.events INFO:  eta: 1 day, 6:45:46  iter: 18339  total_loss: 36.35  loss_ce: 0.2722  loss_mask: 0.3824  loss_dice: 2.951  loss_ce_0: 0.5568  loss_mask_0: 0.3822  loss_dice_0: 3.069  loss_ce_1: 0.3081  loss_mask_1: 0.3851  loss_dice_1: 2.974  loss_ce_2: 0.2882  loss_mask_2: 0.3832  loss_dice_2: 2.959  loss_ce_3: 0.2902  loss_mask_3: 0.3843  loss_dice_3: 2.958  loss_ce_4: 0.289  loss_mask_4: 0.3843  loss_dice_4: 2.95  loss_ce_5: 0.2761  loss_mask_5: 0.3826  loss_dice_5: 2.966  loss_ce_6: 0.2876  loss_mask_6: 0.3802  loss_dice_6: 2.957  loss_ce_7: 0.2663  loss_mask_7: 0.3805  loss_dice_7: 2.959  loss_ce_8: 0.2771  loss_mask_8: 0.3811  loss_dice_8: 2.954  time: 1.5361  data_time: 0.0960  lr: 8.1458e-06  max_mem: 21410M
[01/17 12:05:09] d2.utils.events INFO:  eta: 1 day, 6:43:10  iter: 18359  total_loss: 36.54  loss_ce: 0.2619  loss_mask: 0.3779  loss_dice: 2.893  loss_ce_0: 0.5708  loss_mask_0: 0.3734  loss_dice_0: 3.033  loss_ce_1: 0.3114  loss_mask_1: 0.3841  loss_dice_1: 2.945  loss_ce_2: 0.2927  loss_mask_2: 0.3811  loss_dice_2: 2.912  loss_ce_3: 0.2821  loss_mask_3: 0.3773  loss_dice_3: 2.896  loss_ce_4: 0.2757  loss_mask_4: 0.3753  loss_dice_4: 2.902  loss_ce_5: 0.2673  loss_mask_5: 0.3774  loss_dice_5: 2.91  loss_ce_6: 0.2688  loss_mask_6: 0.3789  loss_dice_6: 2.897  loss_ce_7: 0.2711  loss_mask_7: 0.3797  loss_dice_7: 2.903  loss_ce_8: 0.2675  loss_mask_8: 0.379  loss_dice_8: 2.901  time: 1.5360  data_time: 0.1009  lr: 8.1438e-06  max_mem: 21410M
[01/17 12:05:40] d2.utils.events INFO:  eta: 1 day, 6:42:12  iter: 18379  total_loss: 37.1  loss_ce: 0.2718  loss_mask: 0.3935  loss_dice: 2.979  loss_ce_0: 0.5823  loss_mask_0: 0.3812  loss_dice_0: 3.096  loss_ce_1: 0.3057  loss_mask_1: 0.3979  loss_dice_1: 3.017  loss_ce_2: 0.3013  loss_mask_2: 0.394  loss_dice_2: 2.99  loss_ce_3: 0.3051  loss_mask_3: 0.3945  loss_dice_3: 2.987  loss_ce_4: 0.2921  loss_mask_4: 0.3939  loss_dice_4: 2.983  loss_ce_5: 0.2696  loss_mask_5: 0.393  loss_dice_5: 2.988  loss_ce_6: 0.2582  loss_mask_6: 0.3953  loss_dice_6: 2.992  loss_ce_7: 0.2662  loss_mask_7: 0.3939  loss_dice_7: 2.984  loss_ce_8: 0.2751  loss_mask_8: 0.3909  loss_dice_8: 2.981  time: 1.5361  data_time: 0.0976  lr: 8.1418e-06  max_mem: 21410M
[01/17 12:06:11] d2.utils.events INFO:  eta: 1 day, 6:39:53  iter: 18399  total_loss: 36.76  loss_ce: 0.2585  loss_mask: 0.3936  loss_dice: 2.98  loss_ce_0: 0.5545  loss_mask_0: 0.3881  loss_dice_0: 3.077  loss_ce_1: 0.282  loss_mask_1: 0.3973  loss_dice_1: 3.002  loss_ce_2: 0.3023  loss_mask_2: 0.3943  loss_dice_2: 2.989  loss_ce_3: 0.2899  loss_mask_3: 0.3928  loss_dice_3: 2.979  loss_ce_4: 0.2747  loss_mask_4: 0.3913  loss_dice_4: 2.973  loss_ce_5: 0.2672  loss_mask_5: 0.3925  loss_dice_5: 2.975  loss_ce_6: 0.2594  loss_mask_6: 0.3916  loss_dice_6: 2.971  loss_ce_7: 0.2731  loss_mask_7: 0.3912  loss_dice_7: 2.979  loss_ce_8: 0.2723  loss_mask_8: 0.3926  loss_dice_8: 2.978  time: 1.5360  data_time: 0.0954  lr: 8.1397e-06  max_mem: 21410M
[01/17 12:06:41] d2.utils.events INFO:  eta: 1 day, 6:36:42  iter: 18419  total_loss: 36.41  loss_ce: 0.2718  loss_mask: 0.3779  loss_dice: 2.936  loss_ce_0: 0.616  loss_mask_0: 0.3715  loss_dice_0: 3.063  loss_ce_1: 0.3128  loss_mask_1: 0.3819  loss_dice_1: 2.973  loss_ce_2: 0.3243  loss_mask_2: 0.3786  loss_dice_2: 2.944  loss_ce_3: 0.2894  loss_mask_3: 0.3784  loss_dice_3: 2.94  loss_ce_4: 0.3073  loss_mask_4: 0.3777  loss_dice_4: 2.938  loss_ce_5: 0.2855  loss_mask_5: 0.3759  loss_dice_5: 2.939  loss_ce_6: 0.2584  loss_mask_6: 0.3757  loss_dice_6: 2.932  loss_ce_7: 0.2782  loss_mask_7: 0.3778  loss_dice_7: 2.93  loss_ce_8: 0.2873  loss_mask_8: 0.3782  loss_dice_8: 2.928  time: 1.5360  data_time: 0.0913  lr: 8.1377e-06  max_mem: 21410M
[01/17 12:07:12] d2.utils.events INFO:  eta: 1 day, 6:34:30  iter: 18439  total_loss: 36.74  loss_ce: 0.2733  loss_mask: 0.3918  loss_dice: 2.893  loss_ce_0: 0.5843  loss_mask_0: 0.3904  loss_dice_0: 3.029  loss_ce_1: 0.3232  loss_mask_1: 0.3972  loss_dice_1: 2.935  loss_ce_2: 0.3109  loss_mask_2: 0.3903  loss_dice_2: 2.92  loss_ce_3: 0.2922  loss_mask_3: 0.3908  loss_dice_3: 2.905  loss_ce_4: 0.2829  loss_mask_4: 0.3905  loss_dice_4: 2.904  loss_ce_5: 0.2845  loss_mask_5: 0.3921  loss_dice_5: 2.903  loss_ce_6: 0.281  loss_mask_6: 0.3933  loss_dice_6: 2.91  loss_ce_7: 0.292  loss_mask_7: 0.3941  loss_dice_7: 2.906  loss_ce_8: 0.2866  loss_mask_8: 0.3934  loss_dice_8: 2.9  time: 1.5360  data_time: 0.0948  lr: 8.1356e-06  max_mem: 21410M
[01/17 12:07:42] d2.utils.events INFO:  eta: 1 day, 6:34:54  iter: 18459  total_loss: 36.58  loss_ce: 0.2735  loss_mask: 0.3929  loss_dice: 2.936  loss_ce_0: 0.5587  loss_mask_0: 0.3864  loss_dice_0: 3.062  loss_ce_1: 0.3282  loss_mask_1: 0.3973  loss_dice_1: 2.984  loss_ce_2: 0.3173  loss_mask_2: 0.3896  loss_dice_2: 2.951  loss_ce_3: 0.3042  loss_mask_3: 0.3889  loss_dice_3: 2.945  loss_ce_4: 0.2837  loss_mask_4: 0.3938  loss_dice_4: 2.944  loss_ce_5: 0.2916  loss_mask_5: 0.3923  loss_dice_5: 2.946  loss_ce_6: 0.2811  loss_mask_6: 0.3902  loss_dice_6: 2.938  loss_ce_7: 0.2787  loss_mask_7: 0.3915  loss_dice_7: 2.945  loss_ce_8: 0.2797  loss_mask_8: 0.3932  loss_dice_8: 2.936  time: 1.5360  data_time: 0.0949  lr: 8.1336e-06  max_mem: 21410M
[01/17 12:08:13] d2.utils.events INFO:  eta: 1 day, 6:35:10  iter: 18479  total_loss: 36.61  loss_ce: 0.2914  loss_mask: 0.3874  loss_dice: 2.923  loss_ce_0: 0.5642  loss_mask_0: 0.3916  loss_dice_0: 3.032  loss_ce_1: 0.3024  loss_mask_1: 0.399  loss_dice_1: 2.949  loss_ce_2: 0.3116  loss_mask_2: 0.3917  loss_dice_2: 2.925  loss_ce_3: 0.2839  loss_mask_3: 0.3895  loss_dice_3: 2.921  loss_ce_4: 0.3055  loss_mask_4: 0.3899  loss_dice_4: 2.926  loss_ce_5: 0.2907  loss_mask_5: 0.3898  loss_dice_5: 2.921  loss_ce_6: 0.2956  loss_mask_6: 0.3886  loss_dice_6: 2.916  loss_ce_7: 0.2975  loss_mask_7: 0.3901  loss_dice_7: 2.922  loss_ce_8: 0.2952  loss_mask_8: 0.3884  loss_dice_8: 2.919  time: 1.5360  data_time: 0.0967  lr: 8.1315e-06  max_mem: 21410M
[01/17 12:08:44] d2.utils.events INFO:  eta: 1 day, 6:34:35  iter: 18499  total_loss: 36.95  loss_ce: 0.3093  loss_mask: 0.3838  loss_dice: 2.912  loss_ce_0: 0.5849  loss_mask_0: 0.3809  loss_dice_0: 3.033  loss_ce_1: 0.325  loss_mask_1: 0.3906  loss_dice_1: 2.959  loss_ce_2: 0.3417  loss_mask_2: 0.3858  loss_dice_2: 2.932  loss_ce_3: 0.3327  loss_mask_3: 0.3833  loss_dice_3: 2.912  loss_ce_4: 0.3111  loss_mask_4: 0.3822  loss_dice_4: 2.915  loss_ce_5: 0.3067  loss_mask_5: 0.3831  loss_dice_5: 2.909  loss_ce_6: 0.2972  loss_mask_6: 0.3818  loss_dice_6: 2.912  loss_ce_7: 0.3055  loss_mask_7: 0.3823  loss_dice_7: 2.907  loss_ce_8: 0.2955  loss_mask_8: 0.3817  loss_dice_8: 2.916  time: 1.5360  data_time: 0.1010  lr: 8.1295e-06  max_mem: 21410M
[01/17 12:09:15] d2.utils.events INFO:  eta: 1 day, 6:34:04  iter: 18519  total_loss: 36.35  loss_ce: 0.2748  loss_mask: 0.3855  loss_dice: 2.912  loss_ce_0: 0.5935  loss_mask_0: 0.3828  loss_dice_0: 3.044  loss_ce_1: 0.3158  loss_mask_1: 0.3933  loss_dice_1: 2.955  loss_ce_2: 0.3138  loss_mask_2: 0.3886  loss_dice_2: 2.93  loss_ce_3: 0.2714  loss_mask_3: 0.3878  loss_dice_3: 2.912  loss_ce_4: 0.2746  loss_mask_4: 0.3864  loss_dice_4: 2.912  loss_ce_5: 0.2661  loss_mask_5: 0.3853  loss_dice_5: 2.92  loss_ce_6: 0.2681  loss_mask_6: 0.3861  loss_dice_6: 2.915  loss_ce_7: 0.2718  loss_mask_7: 0.3844  loss_dice_7: 2.907  loss_ce_8: 0.2734  loss_mask_8: 0.3857  loss_dice_8: 2.915  time: 1.5360  data_time: 0.1052  lr: 8.1274e-06  max_mem: 21410M
[01/17 12:09:45] d2.utils.events INFO:  eta: 1 day, 6:34:46  iter: 18539  total_loss: 36.86  loss_ce: 0.3058  loss_mask: 0.3875  loss_dice: 2.943  loss_ce_0: 0.5838  loss_mask_0: 0.3881  loss_dice_0: 3.076  loss_ce_1: 0.3278  loss_mask_1: 0.3979  loss_dice_1: 2.992  loss_ce_2: 0.3398  loss_mask_2: 0.3913  loss_dice_2: 2.965  loss_ce_3: 0.3221  loss_mask_3: 0.3881  loss_dice_3: 2.958  loss_ce_4: 0.2909  loss_mask_4: 0.3894  loss_dice_4: 2.96  loss_ce_5: 0.2991  loss_mask_5: 0.391  loss_dice_5: 2.955  loss_ce_6: 0.2922  loss_mask_6: 0.3896  loss_dice_6: 2.952  loss_ce_7: 0.2955  loss_mask_7: 0.3871  loss_dice_7: 2.949  loss_ce_8: 0.3067  loss_mask_8: 0.3876  loss_dice_8: 2.952  time: 1.5360  data_time: 0.0923  lr: 8.1254e-06  max_mem: 21410M
[01/17 12:10:16] d2.utils.events INFO:  eta: 1 day, 6:34:06  iter: 18559  total_loss: 35.45  loss_ce: 0.2603  loss_mask: 0.3838  loss_dice: 2.806  loss_ce_0: 0.5535  loss_mask_0: 0.376  loss_dice_0: 2.962  loss_ce_1: 0.3049  loss_mask_1: 0.3864  loss_dice_1: 2.851  loss_ce_2: 0.2881  loss_mask_2: 0.3844  loss_dice_2: 2.827  loss_ce_3: 0.2832  loss_mask_3: 0.3836  loss_dice_3: 2.814  loss_ce_4: 0.268  loss_mask_4: 0.3824  loss_dice_4: 2.813  loss_ce_5: 0.2695  loss_mask_5: 0.383  loss_dice_5: 2.816  loss_ce_6: 0.2623  loss_mask_6: 0.3852  loss_dice_6: 2.808  loss_ce_7: 0.276  loss_mask_7: 0.3846  loss_dice_7: 2.814  loss_ce_8: 0.2603  loss_mask_8: 0.3851  loss_dice_8: 2.805  time: 1.5360  data_time: 0.0970  lr: 8.1233e-06  max_mem: 21410M
[01/17 12:10:46] d2.utils.events INFO:  eta: 1 day, 6:30:50  iter: 18579  total_loss: 36.7  loss_ce: 0.28  loss_mask: 0.3874  loss_dice: 2.911  loss_ce_0: 0.5736  loss_mask_0: 0.3804  loss_dice_0: 3.023  loss_ce_1: 0.3265  loss_mask_1: 0.3951  loss_dice_1: 2.939  loss_ce_2: 0.2978  loss_mask_2: 0.3887  loss_dice_2: 2.914  loss_ce_3: 0.2924  loss_mask_3: 0.3893  loss_dice_3: 2.911  loss_ce_4: 0.2791  loss_mask_4: 0.3873  loss_dice_4: 2.91  loss_ce_5: 0.2865  loss_mask_5: 0.3889  loss_dice_5: 2.919  loss_ce_6: 0.279  loss_mask_6: 0.3877  loss_dice_6: 2.914  loss_ce_7: 0.2733  loss_mask_7: 0.3888  loss_dice_7: 2.914  loss_ce_8: 0.2797  loss_mask_8: 0.387  loss_dice_8: 2.917  time: 1.5360  data_time: 0.0910  lr: 8.1213e-06  max_mem: 21410M
[01/17 12:11:17] d2.utils.events INFO:  eta: 1 day, 6:32:05  iter: 18599  total_loss: 35.95  loss_ce: 0.2662  loss_mask: 0.3787  loss_dice: 2.89  loss_ce_0: 0.5748  loss_mask_0: 0.3752  loss_dice_0: 3.018  loss_ce_1: 0.2914  loss_mask_1: 0.3873  loss_dice_1: 2.921  loss_ce_2: 0.2954  loss_mask_2: 0.3801  loss_dice_2: 2.908  loss_ce_3: 0.2692  loss_mask_3: 0.3794  loss_dice_3: 2.896  loss_ce_4: 0.2689  loss_mask_4: 0.3793  loss_dice_4: 2.893  loss_ce_5: 0.2629  loss_mask_5: 0.3781  loss_dice_5: 2.904  loss_ce_6: 0.2699  loss_mask_6: 0.3788  loss_dice_6: 2.895  loss_ce_7: 0.2738  loss_mask_7: 0.3782  loss_dice_7: 2.894  loss_ce_8: 0.2499  loss_mask_8: 0.3787  loss_dice_8: 2.894  time: 1.5360  data_time: 0.0933  lr: 8.1192e-06  max_mem: 21410M
[01/17 12:11:48] d2.utils.events INFO:  eta: 1 day, 6:31:42  iter: 18619  total_loss: 37.06  loss_ce: 0.2752  loss_mask: 0.3995  loss_dice: 2.928  loss_ce_0: 0.5994  loss_mask_0: 0.3903  loss_dice_0: 3.078  loss_ce_1: 0.3276  loss_mask_1: 0.4017  loss_dice_1: 2.977  loss_ce_2: 0.3223  loss_mask_2: 0.3987  loss_dice_2: 2.952  loss_ce_3: 0.2976  loss_mask_3: 0.3988  loss_dice_3: 2.956  loss_ce_4: 0.2967  loss_mask_4: 0.3988  loss_dice_4: 2.95  loss_ce_5: 0.2858  loss_mask_5: 0.398  loss_dice_5: 2.949  loss_ce_6: 0.2834  loss_mask_6: 0.3983  loss_dice_6: 2.946  loss_ce_7: 0.2752  loss_mask_7: 0.3995  loss_dice_7: 2.939  loss_ce_8: 0.281  loss_mask_8: 0.4  loss_dice_8: 2.94  time: 1.5360  data_time: 0.1000  lr: 8.1172e-06  max_mem: 21410M
[01/17 12:12:19] d2.utils.events INFO:  eta: 1 day, 6:31:38  iter: 18639  total_loss: 36.49  loss_ce: 0.2754  loss_mask: 0.3824  loss_dice: 2.93  loss_ce_0: 0.5765  loss_mask_0: 0.3797  loss_dice_0: 3.062  loss_ce_1: 0.3073  loss_mask_1: 0.3837  loss_dice_1: 2.965  loss_ce_2: 0.3132  loss_mask_2: 0.3838  loss_dice_2: 2.943  loss_ce_3: 0.2848  loss_mask_3: 0.3834  loss_dice_3: 2.934  loss_ce_4: 0.2683  loss_mask_4: 0.3835  loss_dice_4: 2.929  loss_ce_5: 0.26  loss_mask_5: 0.384  loss_dice_5: 2.935  loss_ce_6: 0.2679  loss_mask_6: 0.385  loss_dice_6: 2.93  loss_ce_7: 0.2601  loss_mask_7: 0.3816  loss_dice_7: 2.943  loss_ce_8: 0.2703  loss_mask_8: 0.3821  loss_dice_8: 2.93  time: 1.5360  data_time: 0.0864  lr: 8.1152e-06  max_mem: 21410M
[01/17 12:12:50] d2.utils.events INFO:  eta: 1 day, 6:29:46  iter: 18659  total_loss: 36  loss_ce: 0.2595  loss_mask: 0.3859  loss_dice: 2.874  loss_ce_0: 0.5458  loss_mask_0: 0.3752  loss_dice_0: 2.997  loss_ce_1: 0.3176  loss_mask_1: 0.387  loss_dice_1: 2.915  loss_ce_2: 0.3082  loss_mask_2: 0.3847  loss_dice_2: 2.897  loss_ce_3: 0.273  loss_mask_3: 0.3842  loss_dice_3: 2.887  loss_ce_4: 0.2865  loss_mask_4: 0.3822  loss_dice_4: 2.884  loss_ce_5: 0.2671  loss_mask_5: 0.3818  loss_dice_5: 2.889  loss_ce_6: 0.2618  loss_mask_6: 0.3836  loss_dice_6: 2.888  loss_ce_7: 0.2688  loss_mask_7: 0.3837  loss_dice_7: 2.886  loss_ce_8: 0.2677  loss_mask_8: 0.3844  loss_dice_8: 2.873  time: 1.5360  data_time: 0.0904  lr: 8.1131e-06  max_mem: 21410M
[01/17 12:13:21] d2.utils.events INFO:  eta: 1 day, 6:28:16  iter: 18679  total_loss: 36.54  loss_ce: 0.2834  loss_mask: 0.3823  loss_dice: 2.949  loss_ce_0: 0.6081  loss_mask_0: 0.375  loss_dice_0: 3.073  loss_ce_1: 0.288  loss_mask_1: 0.3912  loss_dice_1: 2.982  loss_ce_2: 0.3109  loss_mask_2: 0.3872  loss_dice_2: 2.967  loss_ce_3: 0.2997  loss_mask_3: 0.3845  loss_dice_3: 2.962  loss_ce_4: 0.2922  loss_mask_4: 0.3872  loss_dice_4: 2.953  loss_ce_5: 0.283  loss_mask_5: 0.3871  loss_dice_5: 2.964  loss_ce_6: 0.2925  loss_mask_6: 0.3871  loss_dice_6: 2.96  loss_ce_7: 0.2932  loss_mask_7: 0.3842  loss_dice_7: 2.951  loss_ce_8: 0.296  loss_mask_8: 0.3831  loss_dice_8: 2.949  time: 1.5360  data_time: 0.1014  lr: 8.1111e-06  max_mem: 21410M
[01/17 12:13:52] d2.utils.events INFO:  eta: 1 day, 6:29:38  iter: 18699  total_loss: 37.04  loss_ce: 0.2784  loss_mask: 0.3831  loss_dice: 3.004  loss_ce_0: 0.5654  loss_mask_0: 0.3816  loss_dice_0: 3.119  loss_ce_1: 0.3029  loss_mask_1: 0.3875  loss_dice_1: 3.043  loss_ce_2: 0.3108  loss_mask_2: 0.3848  loss_dice_2: 3.013  loss_ce_3: 0.2941  loss_mask_3: 0.3841  loss_dice_3: 3.005  loss_ce_4: 0.2887  loss_mask_4: 0.3797  loss_dice_4: 2.999  loss_ce_5: 0.2885  loss_mask_5: 0.3838  loss_dice_5: 3  loss_ce_6: 0.2918  loss_mask_6: 0.3817  loss_dice_6: 2.999  loss_ce_7: 0.2711  loss_mask_7: 0.3811  loss_dice_7: 2.999  loss_ce_8: 0.2598  loss_mask_8: 0.3807  loss_dice_8: 2.995  time: 1.5360  data_time: 0.0985  lr: 8.109e-06  max_mem: 21410M
[01/17 12:14:22] d2.utils.events INFO:  eta: 1 day, 6:27:25  iter: 18719  total_loss: 36.15  loss_ce: 0.2619  loss_mask: 0.3914  loss_dice: 2.883  loss_ce_0: 0.5599  loss_mask_0: 0.3826  loss_dice_0: 3.011  loss_ce_1: 0.315  loss_mask_1: 0.3947  loss_dice_1: 2.927  loss_ce_2: 0.3001  loss_mask_2: 0.3909  loss_dice_2: 2.9  loss_ce_3: 0.2929  loss_mask_3: 0.3924  loss_dice_3: 2.885  loss_ce_4: 0.2787  loss_mask_4: 0.394  loss_dice_4: 2.89  loss_ce_5: 0.2639  loss_mask_5: 0.3927  loss_dice_5: 2.893  loss_ce_6: 0.2749  loss_mask_6: 0.3911  loss_dice_6: 2.887  loss_ce_7: 0.2687  loss_mask_7: 0.3889  loss_dice_7: 2.893  loss_ce_8: 0.2794  loss_mask_8: 0.3911  loss_dice_8: 2.886  time: 1.5360  data_time: 0.1039  lr: 8.107e-06  max_mem: 21410M
[01/17 12:14:54] d2.utils.events INFO:  eta: 1 day, 6:27:28  iter: 18739  total_loss: 36.89  loss_ce: 0.2535  loss_mask: 0.3927  loss_dice: 2.954  loss_ce_0: 0.5598  loss_mask_0: 0.3921  loss_dice_0: 3.059  loss_ce_1: 0.2937  loss_mask_1: 0.3961  loss_dice_1: 2.992  loss_ce_2: 0.2902  loss_mask_2: 0.394  loss_dice_2: 2.975  loss_ce_3: 0.2579  loss_mask_3: 0.3894  loss_dice_3: 2.973  loss_ce_4: 0.257  loss_mask_4: 0.3878  loss_dice_4: 2.977  loss_ce_5: 0.261  loss_mask_5: 0.3898  loss_dice_5: 2.973  loss_ce_6: 0.265  loss_mask_6: 0.3917  loss_dice_6: 2.962  loss_ce_7: 0.2734  loss_mask_7: 0.3915  loss_dice_7: 2.958  loss_ce_8: 0.2628  loss_mask_8: 0.3929  loss_dice_8: 2.963  time: 1.5360  data_time: 0.0980  lr: 8.1049e-06  max_mem: 21410M
[01/17 12:15:24] d2.utils.events INFO:  eta: 1 day, 6:26:13  iter: 18759  total_loss: 36.05  loss_ce: 0.2822  loss_mask: 0.3911  loss_dice: 2.855  loss_ce_0: 0.5606  loss_mask_0: 0.3866  loss_dice_0: 2.995  loss_ce_1: 0.3168  loss_mask_1: 0.3982  loss_dice_1: 2.903  loss_ce_2: 0.3062  loss_mask_2: 0.3984  loss_dice_2: 2.875  loss_ce_3: 0.2854  loss_mask_3: 0.3949  loss_dice_3: 2.871  loss_ce_4: 0.2926  loss_mask_4: 0.3929  loss_dice_4: 2.858  loss_ce_5: 0.2844  loss_mask_5: 0.394  loss_dice_5: 2.863  loss_ce_6: 0.2906  loss_mask_6: 0.3901  loss_dice_6: 2.853  loss_ce_7: 0.285  loss_mask_7: 0.3901  loss_dice_7: 2.86  loss_ce_8: 0.2927  loss_mask_8: 0.3911  loss_dice_8: 2.856  time: 1.5360  data_time: 0.0977  lr: 8.1029e-06  max_mem: 21410M
[01/17 12:15:56] d2.utils.events INFO:  eta: 1 day, 6:25:53  iter: 18779  total_loss: 36.91  loss_ce: 0.2697  loss_mask: 0.4037  loss_dice: 2.95  loss_ce_0: 0.5447  loss_mask_0: 0.4122  loss_dice_0: 3.058  loss_ce_1: 0.3121  loss_mask_1: 0.4163  loss_dice_1: 2.981  loss_ce_2: 0.3113  loss_mask_2: 0.4053  loss_dice_2: 2.969  loss_ce_3: 0.2781  loss_mask_3: 0.4037  loss_dice_3: 2.954  loss_ce_4: 0.2756  loss_mask_4: 0.404  loss_dice_4: 2.954  loss_ce_5: 0.2759  loss_mask_5: 0.4036  loss_dice_5: 2.957  loss_ce_6: 0.261  loss_mask_6: 0.4023  loss_dice_6: 2.958  loss_ce_7: 0.2638  loss_mask_7: 0.4009  loss_dice_7: 2.952  loss_ce_8: 0.2623  loss_mask_8: 0.4028  loss_dice_8: 2.953  time: 1.5361  data_time: 0.1080  lr: 8.1008e-06  max_mem: 21410M
[01/17 12:16:27] d2.utils.events INFO:  eta: 1 day, 6:25:11  iter: 18799  total_loss: 36.71  loss_ce: 0.2855  loss_mask: 0.3882  loss_dice: 2.917  loss_ce_0: 0.5878  loss_mask_0: 0.379  loss_dice_0: 3.045  loss_ce_1: 0.3298  loss_mask_1: 0.3916  loss_dice_1: 2.976  loss_ce_2: 0.3324  loss_mask_2: 0.386  loss_dice_2: 2.95  loss_ce_3: 0.3174  loss_mask_3: 0.3884  loss_dice_3: 2.933  loss_ce_4: 0.2964  loss_mask_4: 0.3898  loss_dice_4: 2.935  loss_ce_5: 0.2818  loss_mask_5: 0.3896  loss_dice_5: 2.937  loss_ce_6: 0.2934  loss_mask_6: 0.3898  loss_dice_6: 2.923  loss_ce_7: 0.2777  loss_mask_7: 0.3898  loss_dice_7: 2.926  loss_ce_8: 0.2938  loss_mask_8: 0.3871  loss_dice_8: 2.93  time: 1.5361  data_time: 0.1083  lr: 8.0988e-06  max_mem: 21410M
[01/17 12:16:57] d2.utils.events INFO:  eta: 1 day, 6:24:10  iter: 18819  total_loss: 36.23  loss_ce: 0.2602  loss_mask: 0.3963  loss_dice: 2.886  loss_ce_0: 0.5668  loss_mask_0: 0.3933  loss_dice_0: 2.993  loss_ce_1: 0.3048  loss_mask_1: 0.3997  loss_dice_1: 2.914  loss_ce_2: 0.2875  loss_mask_2: 0.3951  loss_dice_2: 2.899  loss_ce_3: 0.2781  loss_mask_3: 0.3968  loss_dice_3: 2.883  loss_ce_4: 0.2753  loss_mask_4: 0.3965  loss_dice_4: 2.885  loss_ce_5: 0.2597  loss_mask_5: 0.3962  loss_dice_5: 2.889  loss_ce_6: 0.2701  loss_mask_6: 0.3974  loss_dice_6: 2.885  loss_ce_7: 0.269  loss_mask_7: 0.3962  loss_dice_7: 2.888  loss_ce_8: 0.2545  loss_mask_8: 0.395  loss_dice_8: 2.89  time: 1.5361  data_time: 0.0956  lr: 8.0967e-06  max_mem: 21410M
[01/17 12:17:28] d2.utils.events INFO:  eta: 1 day, 6:22:57  iter: 18839  total_loss: 35.59  loss_ce: 0.2556  loss_mask: 0.3818  loss_dice: 2.827  loss_ce_0: 0.5326  loss_mask_0: 0.3728  loss_dice_0: 2.971  loss_ce_1: 0.29  loss_mask_1: 0.384  loss_dice_1: 2.876  loss_ce_2: 0.2828  loss_mask_2: 0.3817  loss_dice_2: 2.86  loss_ce_3: 0.2756  loss_mask_3: 0.3827  loss_dice_3: 2.844  loss_ce_4: 0.2634  loss_mask_4: 0.3812  loss_dice_4: 2.839  loss_ce_5: 0.2612  loss_mask_5: 0.3799  loss_dice_5: 2.848  loss_ce_6: 0.2518  loss_mask_6: 0.3815  loss_dice_6: 2.84  loss_ce_7: 0.2557  loss_mask_7: 0.381  loss_dice_7: 2.832  loss_ce_8: 0.2686  loss_mask_8: 0.3817  loss_dice_8: 2.835  time: 1.5361  data_time: 0.0949  lr: 8.0947e-06  max_mem: 21410M
[01/17 12:17:58] d2.utils.events INFO:  eta: 1 day, 6:21:49  iter: 18859  total_loss: 35.98  loss_ce: 0.2903  loss_mask: 0.3928  loss_dice: 2.844  loss_ce_0: 0.5544  loss_mask_0: 0.3765  loss_dice_0: 2.956  loss_ce_1: 0.3098  loss_mask_1: 0.3938  loss_dice_1: 2.875  loss_ce_2: 0.3213  loss_mask_2: 0.393  loss_dice_2: 2.859  loss_ce_3: 0.2973  loss_mask_3: 0.3947  loss_dice_3: 2.843  loss_ce_4: 0.2934  loss_mask_4: 0.3916  loss_dice_4: 2.849  loss_ce_5: 0.283  loss_mask_5: 0.3918  loss_dice_5: 2.851  loss_ce_6: 0.2763  loss_mask_6: 0.3896  loss_dice_6: 2.837  loss_ce_7: 0.2956  loss_mask_7: 0.3912  loss_dice_7: 2.835  loss_ce_8: 0.275  loss_mask_8: 0.392  loss_dice_8: 2.84  time: 1.5360  data_time: 0.0969  lr: 8.0926e-06  max_mem: 21410M
[01/17 12:18:30] d2.utils.events INFO:  eta: 1 day, 6:22:19  iter: 18879  total_loss: 36.3  loss_ce: 0.2797  loss_mask: 0.3736  loss_dice: 2.908  loss_ce_0: 0.5678  loss_mask_0: 0.365  loss_dice_0: 3.024  loss_ce_1: 0.2962  loss_mask_1: 0.3765  loss_dice_1: 2.935  loss_ce_2: 0.298  loss_mask_2: 0.377  loss_dice_2: 2.915  loss_ce_3: 0.3004  loss_mask_3: 0.3733  loss_dice_3: 2.908  loss_ce_4: 0.2856  loss_mask_4: 0.373  loss_dice_4: 2.908  loss_ce_5: 0.286  loss_mask_5: 0.3741  loss_dice_5: 2.911  loss_ce_6: 0.2847  loss_mask_6: 0.3735  loss_dice_6: 2.91  loss_ce_7: 0.2763  loss_mask_7: 0.3749  loss_dice_7: 2.91  loss_ce_8: 0.272  loss_mask_8: 0.3735  loss_dice_8: 2.898  time: 1.5361  data_time: 0.0944  lr: 8.0906e-06  max_mem: 21410M
[01/17 12:19:00] d2.utils.events INFO:  eta: 1 day, 6:21:51  iter: 18899  total_loss: 36.99  loss_ce: 0.284  loss_mask: 0.3738  loss_dice: 2.934  loss_ce_0: 0.5881  loss_mask_0: 0.3695  loss_dice_0: 3.046  loss_ce_1: 0.3009  loss_mask_1: 0.3785  loss_dice_1: 2.982  loss_ce_2: 0.2979  loss_mask_2: 0.3762  loss_dice_2: 2.95  loss_ce_3: 0.2775  loss_mask_3: 0.3751  loss_dice_3: 2.952  loss_ce_4: 0.2937  loss_mask_4: 0.3737  loss_dice_4: 2.945  loss_ce_5: 0.2747  loss_mask_5: 0.3734  loss_dice_5: 2.957  loss_ce_6: 0.2907  loss_mask_6: 0.3737  loss_dice_6: 2.95  loss_ce_7: 0.2795  loss_mask_7: 0.3734  loss_dice_7: 2.93  loss_ce_8: 0.2762  loss_mask_8: 0.3737  loss_dice_8: 2.944  time: 1.5361  data_time: 0.0973  lr: 8.0885e-06  max_mem: 21410M
[01/17 12:19:31] d2.utils.events INFO:  eta: 1 day, 6:20:29  iter: 18919  total_loss: 36.14  loss_ce: 0.2865  loss_mask: 0.3798  loss_dice: 2.866  loss_ce_0: 0.5821  loss_mask_0: 0.374  loss_dice_0: 2.986  loss_ce_1: 0.3186  loss_mask_1: 0.3863  loss_dice_1: 2.911  loss_ce_2: 0.3086  loss_mask_2: 0.3838  loss_dice_2: 2.895  loss_ce_3: 0.3041  loss_mask_3: 0.3826  loss_dice_3: 2.877  loss_ce_4: 0.2949  loss_mask_4: 0.3815  loss_dice_4: 2.886  loss_ce_5: 0.2849  loss_mask_5: 0.3817  loss_dice_5: 2.877  loss_ce_6: 0.2848  loss_mask_6: 0.3817  loss_dice_6: 2.874  loss_ce_7: 0.2845  loss_mask_7: 0.3805  loss_dice_7: 2.868  loss_ce_8: 0.2832  loss_mask_8: 0.3802  loss_dice_8: 2.866  time: 1.5361  data_time: 0.0947  lr: 8.0865e-06  max_mem: 21410M
[01/17 12:20:02] d2.utils.events INFO:  eta: 1 day, 6:20:24  iter: 18939  total_loss: 36.61  loss_ce: 0.2715  loss_mask: 0.3912  loss_dice: 2.929  loss_ce_0: 0.5615  loss_mask_0: 0.3895  loss_dice_0: 3.045  loss_ce_1: 0.3131  loss_mask_1: 0.3987  loss_dice_1: 2.972  loss_ce_2: 0.2922  loss_mask_2: 0.3911  loss_dice_2: 2.955  loss_ce_3: 0.2818  loss_mask_3: 0.3909  loss_dice_3: 2.944  loss_ce_4: 0.2803  loss_mask_4: 0.3904  loss_dice_4: 2.941  loss_ce_5: 0.2694  loss_mask_5: 0.3922  loss_dice_5: 2.941  loss_ce_6: 0.271  loss_mask_6: 0.3916  loss_dice_6: 2.925  loss_ce_7: 0.2726  loss_mask_7: 0.3917  loss_dice_7: 2.936  loss_ce_8: 0.254  loss_mask_8: 0.3904  loss_dice_8: 2.941  time: 1.5361  data_time: 0.0971  lr: 8.0844e-06  max_mem: 21410M
[01/17 12:20:34] d2.utils.events INFO:  eta: 1 day, 6:20:15  iter: 18959  total_loss: 37.59  loss_ce: 0.3271  loss_mask: 0.3933  loss_dice: 2.968  loss_ce_0: 0.6026  loss_mask_0: 0.3969  loss_dice_0: 3.074  loss_ce_1: 0.3467  loss_mask_1: 0.3978  loss_dice_1: 2.993  loss_ce_2: 0.3384  loss_mask_2: 0.394  loss_dice_2: 2.979  loss_ce_3: 0.3294  loss_mask_3: 0.3959  loss_dice_3: 2.968  loss_ce_4: 0.3225  loss_mask_4: 0.3939  loss_dice_4: 2.968  loss_ce_5: 0.3311  loss_mask_5: 0.3958  loss_dice_5: 2.966  loss_ce_6: 0.3186  loss_mask_6: 0.3937  loss_dice_6: 2.964  loss_ce_7: 0.3025  loss_mask_7: 0.3944  loss_dice_7: 2.962  loss_ce_8: 0.3275  loss_mask_8: 0.3945  loss_dice_8: 2.975  time: 1.5361  data_time: 0.1083  lr: 8.0824e-06  max_mem: 21410M
[01/17 12:21:05] d2.utils.events INFO:  eta: 1 day, 6:19:48  iter: 18979  total_loss: 36.57  loss_ce: 0.2928  loss_mask: 0.3809  loss_dice: 2.904  loss_ce_0: 0.5722  loss_mask_0: 0.3797  loss_dice_0: 3.032  loss_ce_1: 0.3208  loss_mask_1: 0.3921  loss_dice_1: 2.946  loss_ce_2: 0.3182  loss_mask_2: 0.387  loss_dice_2: 2.924  loss_ce_3: 0.3102  loss_mask_3: 0.3831  loss_dice_3: 2.907  loss_ce_4: 0.3045  loss_mask_4: 0.3826  loss_dice_4: 2.902  loss_ce_5: 0.3019  loss_mask_5: 0.382  loss_dice_5: 2.909  loss_ce_6: 0.3134  loss_mask_6: 0.3821  loss_dice_6: 2.9  loss_ce_7: 0.293  loss_mask_7: 0.3791  loss_dice_7: 2.902  loss_ce_8: 0.2946  loss_mask_8: 0.3788  loss_dice_8: 2.907  time: 1.5361  data_time: 0.1035  lr: 8.0803e-06  max_mem: 21410M
[01/17 12:21:36] d2.utils.events INFO:  eta: 1 day, 6:20:04  iter: 18999  total_loss: 36.28  loss_ce: 0.2869  loss_mask: 0.3851  loss_dice: 2.892  loss_ce_0: 0.5364  loss_mask_0: 0.3818  loss_dice_0: 2.999  loss_ce_1: 0.3017  loss_mask_1: 0.3888  loss_dice_1: 2.934  loss_ce_2: 0.3109  loss_mask_2: 0.3853  loss_dice_2: 2.912  loss_ce_3: 0.2922  loss_mask_3: 0.3825  loss_dice_3: 2.911  loss_ce_4: 0.2898  loss_mask_4: 0.3846  loss_dice_4: 2.895  loss_ce_5: 0.2844  loss_mask_5: 0.3815  loss_dice_5: 2.905  loss_ce_6: 0.279  loss_mask_6: 0.3835  loss_dice_6: 2.902  loss_ce_7: 0.2877  loss_mask_7: 0.3874  loss_dice_7: 2.894  loss_ce_8: 0.2876  loss_mask_8: 0.3866  loss_dice_8: 2.894  time: 1.5362  data_time: 0.1105  lr: 8.0783e-06  max_mem: 21410M
[01/17 12:22:07] d2.utils.events INFO:  eta: 1 day, 6:19:43  iter: 19019  total_loss: 36.72  loss_ce: 0.3313  loss_mask: 0.3842  loss_dice: 2.943  loss_ce_0: 0.5767  loss_mask_0: 0.3797  loss_dice_0: 3.067  loss_ce_1: 0.3261  loss_mask_1: 0.3901  loss_dice_1: 2.992  loss_ce_2: 0.3103  loss_mask_2: 0.3855  loss_dice_2: 2.963  loss_ce_3: 0.3201  loss_mask_3: 0.3873  loss_dice_3: 2.942  loss_ce_4: 0.3002  loss_mask_4: 0.3877  loss_dice_4: 2.944  loss_ce_5: 0.315  loss_mask_5: 0.3862  loss_dice_5: 2.953  loss_ce_6: 0.304  loss_mask_6: 0.3859  loss_dice_6: 2.955  loss_ce_7: 0.2993  loss_mask_7: 0.3831  loss_dice_7: 2.944  loss_ce_8: 0.2839  loss_mask_8: 0.3836  loss_dice_8: 2.946  time: 1.5362  data_time: 0.0915  lr: 8.0762e-06  max_mem: 21410M
[01/17 12:22:38] d2.utils.events INFO:  eta: 1 day, 6:19:46  iter: 19039  total_loss: 36.09  loss_ce: 0.2845  loss_mask: 0.3821  loss_dice: 2.908  loss_ce_0: 0.5914  loss_mask_0: 0.382  loss_dice_0: 3.028  loss_ce_1: 0.3002  loss_mask_1: 0.391  loss_dice_1: 2.943  loss_ce_2: 0.3097  loss_mask_2: 0.3877  loss_dice_2: 2.922  loss_ce_3: 0.2895  loss_mask_3: 0.3826  loss_dice_3: 2.915  loss_ce_4: 0.2935  loss_mask_4: 0.3816  loss_dice_4: 2.905  loss_ce_5: 0.2908  loss_mask_5: 0.3838  loss_dice_5: 2.916  loss_ce_6: 0.2868  loss_mask_6: 0.3838  loss_dice_6: 2.908  loss_ce_7: 0.2912  loss_mask_7: 0.3826  loss_dice_7: 2.906  loss_ce_8: 0.2789  loss_mask_8: 0.3835  loss_dice_8: 2.899  time: 1.5362  data_time: 0.1012  lr: 8.0742e-06  max_mem: 21410M
[01/17 12:23:09] d2.utils.events INFO:  eta: 1 day, 6:20:01  iter: 19059  total_loss: 35.82  loss_ce: 0.2847  loss_mask: 0.3994  loss_dice: 2.849  loss_ce_0: 0.5706  loss_mask_0: 0.3983  loss_dice_0: 2.968  loss_ce_1: 0.3247  loss_mask_1: 0.4039  loss_dice_1: 2.885  loss_ce_2: 0.3032  loss_mask_2: 0.402  loss_dice_2: 2.86  loss_ce_3: 0.2877  loss_mask_3: 0.403  loss_dice_3: 2.854  loss_ce_4: 0.2915  loss_mask_4: 0.4009  loss_dice_4: 2.854  loss_ce_5: 0.2796  loss_mask_5: 0.3984  loss_dice_5: 2.85  loss_ce_6: 0.2797  loss_mask_6: 0.4004  loss_dice_6: 2.857  loss_ce_7: 0.2792  loss_mask_7: 0.3989  loss_dice_7: 2.848  loss_ce_8: 0.282  loss_mask_8: 0.3995  loss_dice_8: 2.855  time: 1.5362  data_time: 0.1084  lr: 8.0722e-06  max_mem: 21410M
[01/17 12:23:39] d2.utils.events INFO:  eta: 1 day, 6:18:01  iter: 19079  total_loss: 35.84  loss_ce: 0.2688  loss_mask: 0.3802  loss_dice: 2.879  loss_ce_0: 0.5606  loss_mask_0: 0.3675  loss_dice_0: 2.988  loss_ce_1: 0.3219  loss_mask_1: 0.3795  loss_dice_1: 2.915  loss_ce_2: 0.301  loss_mask_2: 0.3812  loss_dice_2: 2.894  loss_ce_3: 0.2963  loss_mask_3: 0.3792  loss_dice_3: 2.873  loss_ce_4: 0.2799  loss_mask_4: 0.377  loss_dice_4: 2.872  loss_ce_5: 0.2765  loss_mask_5: 0.3798  loss_dice_5: 2.884  loss_ce_6: 0.2759  loss_mask_6: 0.3775  loss_dice_6: 2.877  loss_ce_7: 0.2677  loss_mask_7: 0.3776  loss_dice_7: 2.883  loss_ce_8: 0.2694  loss_mask_8: 0.3815  loss_dice_8: 2.873  time: 1.5362  data_time: 0.0976  lr: 8.0701e-06  max_mem: 21410M
[01/17 12:24:10] d2.utils.events INFO:  eta: 1 day, 6:17:30  iter: 19099  total_loss: 36.72  loss_ce: 0.268  loss_mask: 0.3845  loss_dice: 2.947  loss_ce_0: 0.5915  loss_mask_0: 0.3815  loss_dice_0: 3.058  loss_ce_1: 0.2944  loss_mask_1: 0.3942  loss_dice_1: 2.984  loss_ce_2: 0.3078  loss_mask_2: 0.3869  loss_dice_2: 2.976  loss_ce_3: 0.2799  loss_mask_3: 0.3836  loss_dice_3: 2.966  loss_ce_4: 0.2835  loss_mask_4: 0.3863  loss_dice_4: 2.954  loss_ce_5: 0.2853  loss_mask_5: 0.388  loss_dice_5: 2.956  loss_ce_6: 0.2706  loss_mask_6: 0.3878  loss_dice_6: 2.95  loss_ce_7: 0.2678  loss_mask_7: 0.3871  loss_dice_7: 2.954  loss_ce_8: 0.267  loss_mask_8: 0.3858  loss_dice_8: 2.956  time: 1.5362  data_time: 0.0883  lr: 8.0681e-06  max_mem: 21410M
[01/17 12:24:41] d2.utils.events INFO:  eta: 1 day, 6:17:10  iter: 19119  total_loss: 36.25  loss_ce: 0.272  loss_mask: 0.3724  loss_dice: 2.931  loss_ce_0: 0.5586  loss_mask_0: 0.3733  loss_dice_0: 3.051  loss_ce_1: 0.311  loss_mask_1: 0.3813  loss_dice_1: 2.97  loss_ce_2: 0.2951  loss_mask_2: 0.3747  loss_dice_2: 2.944  loss_ce_3: 0.2808  loss_mask_3: 0.371  loss_dice_3: 2.929  loss_ce_4: 0.2848  loss_mask_4: 0.3714  loss_dice_4: 2.94  loss_ce_5: 0.2735  loss_mask_5: 0.3709  loss_dice_5: 2.935  loss_ce_6: 0.2845  loss_mask_6: 0.3715  loss_dice_6: 2.933  loss_ce_7: 0.2713  loss_mask_7: 0.3719  loss_dice_7: 2.926  loss_ce_8: 0.2777  loss_mask_8: 0.3715  loss_dice_8: 2.937  time: 1.5362  data_time: 0.0942  lr: 8.066e-06  max_mem: 21410M
[01/17 12:25:12] d2.utils.events INFO:  eta: 1 day, 6:17:12  iter: 19139  total_loss: 36.05  loss_ce: 0.2686  loss_mask: 0.3773  loss_dice: 2.9  loss_ce_0: 0.5839  loss_mask_0: 0.3697  loss_dice_0: 3.025  loss_ce_1: 0.2992  loss_mask_1: 0.3842  loss_dice_1: 2.93  loss_ce_2: 0.2921  loss_mask_2: 0.3785  loss_dice_2: 2.917  loss_ce_3: 0.2774  loss_mask_3: 0.3778  loss_dice_3: 2.899  loss_ce_4: 0.2952  loss_mask_4: 0.3775  loss_dice_4: 2.894  loss_ce_5: 0.2747  loss_mask_5: 0.3758  loss_dice_5: 2.9  loss_ce_6: 0.2727  loss_mask_6: 0.3768  loss_dice_6: 2.892  loss_ce_7: 0.2651  loss_mask_7: 0.3776  loss_dice_7: 2.896  loss_ce_8: 0.2627  loss_mask_8: 0.3774  loss_dice_8: 2.899  time: 1.5362  data_time: 0.1076  lr: 8.064e-06  max_mem: 21410M
[01/17 12:25:42] d2.utils.events INFO:  eta: 1 day, 6:17:27  iter: 19159  total_loss: 35.6  loss_ce: 0.264  loss_mask: 0.3762  loss_dice: 2.838  loss_ce_0: 0.5906  loss_mask_0: 0.3701  loss_dice_0: 2.966  loss_ce_1: 0.3115  loss_mask_1: 0.3798  loss_dice_1: 2.873  loss_ce_2: 0.2878  loss_mask_2: 0.3792  loss_dice_2: 2.865  loss_ce_3: 0.2854  loss_mask_3: 0.3775  loss_dice_3: 2.854  loss_ce_4: 0.2772  loss_mask_4: 0.3776  loss_dice_4: 2.849  loss_ce_5: 0.2699  loss_mask_5: 0.3781  loss_dice_5: 2.858  loss_ce_6: 0.2792  loss_mask_6: 0.377  loss_dice_6: 2.841  loss_ce_7: 0.2627  loss_mask_7: 0.3766  loss_dice_7: 2.837  loss_ce_8: 0.2704  loss_mask_8: 0.3767  loss_dice_8: 2.836  time: 1.5362  data_time: 0.1046  lr: 8.0619e-06  max_mem: 21410M
[01/17 12:26:13] d2.utils.events INFO:  eta: 1 day, 6:15:41  iter: 19179  total_loss: 36.63  loss_ce: 0.277  loss_mask: 0.3784  loss_dice: 2.921  loss_ce_0: 0.566  loss_mask_0: 0.3749  loss_dice_0: 3.035  loss_ce_1: 0.3167  loss_mask_1: 0.3874  loss_dice_1: 2.96  loss_ce_2: 0.3105  loss_mask_2: 0.3836  loss_dice_2: 2.942  loss_ce_3: 0.306  loss_mask_3: 0.3772  loss_dice_3: 2.936  loss_ce_4: 0.3105  loss_mask_4: 0.3743  loss_dice_4: 2.927  loss_ce_5: 0.2873  loss_mask_5: 0.3743  loss_dice_5: 2.943  loss_ce_6: 0.293  loss_mask_6: 0.375  loss_dice_6: 2.934  loss_ce_7: 0.2823  loss_mask_7: 0.3767  loss_dice_7: 2.928  loss_ce_8: 0.2853  loss_mask_8: 0.3768  loss_dice_8: 2.933  time: 1.5362  data_time: 0.1050  lr: 8.0599e-06  max_mem: 21410M
[01/17 12:26:43] d2.utils.events INFO:  eta: 1 day, 6:15:11  iter: 19199  total_loss: 36.01  loss_ce: 0.282  loss_mask: 0.3764  loss_dice: 2.858  loss_ce_0: 0.5704  loss_mask_0: 0.3674  loss_dice_0: 2.987  loss_ce_1: 0.3144  loss_mask_1: 0.3811  loss_dice_1: 2.902  loss_ce_2: 0.3104  loss_mask_2: 0.3808  loss_dice_2: 2.884  loss_ce_3: 0.2922  loss_mask_3: 0.3797  loss_dice_3: 2.872  loss_ce_4: 0.2931  loss_mask_4: 0.3786  loss_dice_4: 2.864  loss_ce_5: 0.2909  loss_mask_5: 0.3798  loss_dice_5: 2.866  loss_ce_6: 0.281  loss_mask_6: 0.3782  loss_dice_6: 2.859  loss_ce_7: 0.276  loss_mask_7: 0.3781  loss_dice_7: 2.859  loss_ce_8: 0.2809  loss_mask_8: 0.3778  loss_dice_8: 2.864  time: 1.5361  data_time: 0.0861  lr: 8.0578e-06  max_mem: 21410M
[01/17 12:27:14] d2.utils.events INFO:  eta: 1 day, 6:13:53  iter: 19219  total_loss: 35.8  loss_ce: 0.2699  loss_mask: 0.3786  loss_dice: 2.862  loss_ce_0: 0.5947  loss_mask_0: 0.3753  loss_dice_0: 2.983  loss_ce_1: 0.2838  loss_mask_1: 0.3791  loss_dice_1: 2.907  loss_ce_2: 0.2984  loss_mask_2: 0.3768  loss_dice_2: 2.889  loss_ce_3: 0.2737  loss_mask_3: 0.376  loss_dice_3: 2.876  loss_ce_4: 0.2738  loss_mask_4: 0.3765  loss_dice_4: 2.867  loss_ce_5: 0.2684  loss_mask_5: 0.3753  loss_dice_5: 2.872  loss_ce_6: 0.2711  loss_mask_6: 0.3766  loss_dice_6: 2.871  loss_ce_7: 0.2709  loss_mask_7: 0.3773  loss_dice_7: 2.864  loss_ce_8: 0.2427  loss_mask_8: 0.3779  loss_dice_8: 2.864  time: 1.5361  data_time: 0.0885  lr: 8.0558e-06  max_mem: 21410M
[01/17 12:27:45] d2.utils.events INFO:  eta: 1 day, 6:13:09  iter: 19239  total_loss: 35.45  loss_ce: 0.2469  loss_mask: 0.3831  loss_dice: 2.846  loss_ce_0: 0.5398  loss_mask_0: 0.3735  loss_dice_0: 2.97  loss_ce_1: 0.2933  loss_mask_1: 0.3843  loss_dice_1: 2.888  loss_ce_2: 0.2813  loss_mask_2: 0.3823  loss_dice_2: 2.852  loss_ce_3: 0.2685  loss_mask_3: 0.3845  loss_dice_3: 2.855  loss_ce_4: 0.2607  loss_mask_4: 0.3831  loss_dice_4: 2.847  loss_ce_5: 0.2596  loss_mask_5: 0.3837  loss_dice_5: 2.85  loss_ce_6: 0.2627  loss_mask_6: 0.3828  loss_dice_6: 2.843  loss_ce_7: 0.248  loss_mask_7: 0.3821  loss_dice_7: 2.847  loss_ce_8: 0.2685  loss_mask_8: 0.3807  loss_dice_8: 2.84  time: 1.5361  data_time: 0.0937  lr: 8.0537e-06  max_mem: 21410M
[01/17 12:28:16] d2.utils.events INFO:  eta: 1 day, 6:12:18  iter: 19259  total_loss: 37.02  loss_ce: 0.2676  loss_mask: 0.3773  loss_dice: 2.961  loss_ce_0: 0.5734  loss_mask_0: 0.3875  loss_dice_0: 3.102  loss_ce_1: 0.2883  loss_mask_1: 0.3919  loss_dice_1: 3.017  loss_ce_2: 0.2831  loss_mask_2: 0.3849  loss_dice_2: 2.984  loss_ce_3: 0.2776  loss_mask_3: 0.3803  loss_dice_3: 2.973  loss_ce_4: 0.28  loss_mask_4: 0.3819  loss_dice_4: 2.973  loss_ce_5: 0.2852  loss_mask_5: 0.3824  loss_dice_5: 2.965  loss_ce_6: 0.272  loss_mask_6: 0.3809  loss_dice_6: 2.957  loss_ce_7: 0.2643  loss_mask_7: 0.3792  loss_dice_7: 2.956  loss_ce_8: 0.2628  loss_mask_8: 0.3783  loss_dice_8: 2.955  time: 1.5361  data_time: 0.0837  lr: 8.0517e-06  max_mem: 21410M
[01/17 12:28:47] d2.utils.events INFO:  eta: 1 day, 6:11:00  iter: 19279  total_loss: 36.71  loss_ce: 0.2947  loss_mask: 0.3867  loss_dice: 2.887  loss_ce_0: 0.5778  loss_mask_0: 0.3922  loss_dice_0: 3.005  loss_ce_1: 0.3162  loss_mask_1: 0.3964  loss_dice_1: 2.936  loss_ce_2: 0.3214  loss_mask_2: 0.3899  loss_dice_2: 2.911  loss_ce_3: 0.3139  loss_mask_3: 0.3907  loss_dice_3: 2.904  loss_ce_4: 0.3118  loss_mask_4: 0.3865  loss_dice_4: 2.891  loss_ce_5: 0.3047  loss_mask_5: 0.3872  loss_dice_5: 2.906  loss_ce_6: 0.2995  loss_mask_6: 0.385  loss_dice_6: 2.905  loss_ce_7: 0.2839  loss_mask_7: 0.3846  loss_dice_7: 2.9  loss_ce_8: 0.3092  loss_mask_8: 0.3855  loss_dice_8: 2.889  time: 1.5362  data_time: 0.0981  lr: 8.0496e-06  max_mem: 21410M
[01/17 12:29:18] d2.utils.events INFO:  eta: 1 day, 6:10:29  iter: 19299  total_loss: 36.15  loss_ce: 0.311  loss_mask: 0.3832  loss_dice: 2.914  loss_ce_0: 0.5792  loss_mask_0: 0.3758  loss_dice_0: 3.036  loss_ce_1: 0.3341  loss_mask_1: 0.3832  loss_dice_1: 2.963  loss_ce_2: 0.3205  loss_mask_2: 0.3793  loss_dice_2: 2.948  loss_ce_3: 0.3005  loss_mask_3: 0.3815  loss_dice_3: 2.922  loss_ce_4: 0.3251  loss_mask_4: 0.3844  loss_dice_4: 2.922  loss_ce_5: 0.3095  loss_mask_5: 0.3851  loss_dice_5: 2.909  loss_ce_6: 0.3177  loss_mask_6: 0.3812  loss_dice_6: 2.912  loss_ce_7: 0.3146  loss_mask_7: 0.3822  loss_dice_7: 2.916  loss_ce_8: 0.3029  loss_mask_8: 0.3829  loss_dice_8: 2.922  time: 1.5362  data_time: 0.1092  lr: 8.0476e-06  max_mem: 21410M
[01/17 12:29:49] d2.utils.events INFO:  eta: 1 day, 6:10:45  iter: 19319  total_loss: 36.35  loss_ce: 0.2692  loss_mask: 0.3856  loss_dice: 2.914  loss_ce_0: 0.5704  loss_mask_0: 0.3871  loss_dice_0: 3.032  loss_ce_1: 0.3108  loss_mask_1: 0.3907  loss_dice_1: 2.948  loss_ce_2: 0.3081  loss_mask_2: 0.3895  loss_dice_2: 2.935  loss_ce_3: 0.2831  loss_mask_3: 0.3882  loss_dice_3: 2.919  loss_ce_4: 0.284  loss_mask_4: 0.388  loss_dice_4: 2.924  loss_ce_5: 0.2627  loss_mask_5: 0.3869  loss_dice_5: 2.922  loss_ce_6: 0.2705  loss_mask_6: 0.387  loss_dice_6: 2.921  loss_ce_7: 0.2701  loss_mask_7: 0.3871  loss_dice_7: 2.919  loss_ce_8: 0.276  loss_mask_8: 0.3847  loss_dice_8: 2.915  time: 1.5362  data_time: 0.0995  lr: 8.0455e-06  max_mem: 21410M
[01/17 12:30:20] d2.utils.events INFO:  eta: 1 day, 6:09:56  iter: 19339  total_loss: 36.34  loss_ce: 0.2814  loss_mask: 0.3764  loss_dice: 2.904  loss_ce_0: 0.5494  loss_mask_0: 0.3714  loss_dice_0: 3.034  loss_ce_1: 0.3027  loss_mask_1: 0.38  loss_dice_1: 2.948  loss_ce_2: 0.2959  loss_mask_2: 0.379  loss_dice_2: 2.92  loss_ce_3: 0.2826  loss_mask_3: 0.3784  loss_dice_3: 2.918  loss_ce_4: 0.2835  loss_mask_4: 0.3788  loss_dice_4: 2.913  loss_ce_5: 0.2766  loss_mask_5: 0.3783  loss_dice_5: 2.91  loss_ce_6: 0.2817  loss_mask_6: 0.3798  loss_dice_6: 2.905  loss_ce_7: 0.2803  loss_mask_7: 0.379  loss_dice_7: 2.908  loss_ce_8: 0.2805  loss_mask_8: 0.3774  loss_dice_8: 2.916  time: 1.5362  data_time: 0.0986  lr: 8.0435e-06  max_mem: 21410M
[01/17 12:30:50] d2.utils.events INFO:  eta: 1 day, 6:09:26  iter: 19359  total_loss: 36.34  loss_ce: 0.3023  loss_mask: 0.3774  loss_dice: 2.907  loss_ce_0: 0.5538  loss_mask_0: 0.3716  loss_dice_0: 3.024  loss_ce_1: 0.3034  loss_mask_1: 0.3758  loss_dice_1: 2.951  loss_ce_2: 0.3169  loss_mask_2: 0.3753  loss_dice_2: 2.919  loss_ce_3: 0.3074  loss_mask_3: 0.3762  loss_dice_3: 2.907  loss_ce_4: 0.3089  loss_mask_4: 0.375  loss_dice_4: 2.911  loss_ce_5: 0.3082  loss_mask_5: 0.3756  loss_dice_5: 2.91  loss_ce_6: 0.3045  loss_mask_6: 0.3784  loss_dice_6: 2.906  loss_ce_7: 0.3028  loss_mask_7: 0.3742  loss_dice_7: 2.913  loss_ce_8: 0.3087  loss_mask_8: 0.3762  loss_dice_8: 2.9  time: 1.5362  data_time: 0.0928  lr: 8.0414e-06  max_mem: 21410M
[01/17 12:31:21] d2.utils.events INFO:  eta: 1 day, 6:07:46  iter: 19379  total_loss: 35.88  loss_ce: 0.274  loss_mask: 0.3822  loss_dice: 2.911  loss_ce_0: 0.5784  loss_mask_0: 0.382  loss_dice_0: 3.022  loss_ce_1: 0.305  loss_mask_1: 0.3906  loss_dice_1: 2.949  loss_ce_2: 0.2986  loss_mask_2: 0.3858  loss_dice_2: 2.921  loss_ce_3: 0.2658  loss_mask_3: 0.383  loss_dice_3: 2.918  loss_ce_4: 0.2872  loss_mask_4: 0.3839  loss_dice_4: 2.912  loss_ce_5: 0.2674  loss_mask_5: 0.3846  loss_dice_5: 2.922  loss_ce_6: 0.2751  loss_mask_6: 0.3824  loss_dice_6: 2.916  loss_ce_7: 0.2889  loss_mask_7: 0.3822  loss_dice_7: 2.914  loss_ce_8: 0.2803  loss_mask_8: 0.3836  loss_dice_8: 2.903  time: 1.5362  data_time: 0.0976  lr: 8.0394e-06  max_mem: 21410M
[01/17 12:31:51] d2.utils.events INFO:  eta: 1 day, 6:06:08  iter: 19399  total_loss: 36.21  loss_ce: 0.294  loss_mask: 0.3856  loss_dice: 2.861  loss_ce_0: 0.6087  loss_mask_0: 0.3815  loss_dice_0: 2.979  loss_ce_1: 0.3279  loss_mask_1: 0.3948  loss_dice_1: 2.898  loss_ce_2: 0.3316  loss_mask_2: 0.3901  loss_dice_2: 2.883  loss_ce_3: 0.3198  loss_mask_3: 0.3866  loss_dice_3: 2.874  loss_ce_4: 0.3165  loss_mask_4: 0.387  loss_dice_4: 2.873  loss_ce_5: 0.3038  loss_mask_5: 0.3886  loss_dice_5: 2.87  loss_ce_6: 0.3076  loss_mask_6: 0.3833  loss_dice_6: 2.868  loss_ce_7: 0.3019  loss_mask_7: 0.3826  loss_dice_7: 2.866  loss_ce_8: 0.2972  loss_mask_8: 0.3862  loss_dice_8: 2.861  time: 1.5361  data_time: 0.1072  lr: 8.0373e-06  max_mem: 21410M
[01/17 12:32:22] d2.utils.events INFO:  eta: 1 day, 6:05:54  iter: 19419  total_loss: 36.15  loss_ce: 0.2811  loss_mask: 0.3779  loss_dice: 2.875  loss_ce_0: 0.5714  loss_mask_0: 0.3707  loss_dice_0: 3.003  loss_ce_1: 0.3194  loss_mask_1: 0.3764  loss_dice_1: 2.919  loss_ce_2: 0.3171  loss_mask_2: 0.3747  loss_dice_2: 2.896  loss_ce_3: 0.2983  loss_mask_3: 0.3809  loss_dice_3: 2.879  loss_ce_4: 0.2921  loss_mask_4: 0.3774  loss_dice_4: 2.882  loss_ce_5: 0.2863  loss_mask_5: 0.3773  loss_dice_5: 2.877  loss_ce_6: 0.2754  loss_mask_6: 0.3773  loss_dice_6: 2.883  loss_ce_7: 0.2853  loss_mask_7: 0.3777  loss_dice_7: 2.882  loss_ce_8: 0.2782  loss_mask_8: 0.3741  loss_dice_8: 2.88  time: 1.5362  data_time: 0.0954  lr: 8.0353e-06  max_mem: 21410M
[01/17 12:32:53] d2.utils.events INFO:  eta: 1 day, 6:07:00  iter: 19439  total_loss: 37.61  loss_ce: 0.3115  loss_mask: 0.3943  loss_dice: 2.951  loss_ce_0: 0.5807  loss_mask_0: 0.3977  loss_dice_0: 3.064  loss_ce_1: 0.3443  loss_mask_1: 0.3969  loss_dice_1: 2.996  loss_ce_2: 0.3477  loss_mask_2: 0.3976  loss_dice_2: 2.966  loss_ce_3: 0.3243  loss_mask_3: 0.3946  loss_dice_3: 2.951  loss_ce_4: 0.3351  loss_mask_4: 0.3953  loss_dice_4: 2.947  loss_ce_5: 0.3222  loss_mask_5: 0.3953  loss_dice_5: 2.945  loss_ce_6: 0.3171  loss_mask_6: 0.3939  loss_dice_6: 2.957  loss_ce_7: 0.3034  loss_mask_7: 0.3924  loss_dice_7: 2.946  loss_ce_8: 0.3141  loss_mask_8: 0.3934  loss_dice_8: 2.946  time: 1.5362  data_time: 0.1058  lr: 8.0332e-06  max_mem: 21410M
[01/17 12:33:24] d2.utils.events INFO:  eta: 1 day, 6:06:40  iter: 19459  total_loss: 36.22  loss_ce: 0.28  loss_mask: 0.3917  loss_dice: 2.904  loss_ce_0: 0.5558  loss_mask_0: 0.3952  loss_dice_0: 3.003  loss_ce_1: 0.3122  loss_mask_1: 0.4011  loss_dice_1: 2.925  loss_ce_2: 0.3039  loss_mask_2: 0.39  loss_dice_2: 2.911  loss_ce_3: 0.3  loss_mask_3: 0.3901  loss_dice_3: 2.904  loss_ce_4: 0.2804  loss_mask_4: 0.3927  loss_dice_4: 2.902  loss_ce_5: 0.2832  loss_mask_5: 0.393  loss_dice_5: 2.897  loss_ce_6: 0.2772  loss_mask_6: 0.3932  loss_dice_6: 2.899  loss_ce_7: 0.2745  loss_mask_7: 0.3935  loss_dice_7: 2.903  loss_ce_8: 0.2843  loss_mask_8: 0.3938  loss_dice_8: 2.892  time: 1.5362  data_time: 0.0931  lr: 8.0312e-06  max_mem: 21410M
[01/17 12:33:56] d2.utils.events INFO:  eta: 1 day, 6:05:55  iter: 19479  total_loss: 36.76  loss_ce: 0.2991  loss_mask: 0.383  loss_dice: 2.941  loss_ce_0: 0.5674  loss_mask_0: 0.382  loss_dice_0: 3.063  loss_ce_1: 0.3143  loss_mask_1: 0.3861  loss_dice_1: 2.977  loss_ce_2: 0.3148  loss_mask_2: 0.3831  loss_dice_2: 2.951  loss_ce_3: 0.3165  loss_mask_3: 0.3845  loss_dice_3: 2.941  loss_ce_4: 0.3056  loss_mask_4: 0.3828  loss_dice_4: 2.934  loss_ce_5: 0.2968  loss_mask_5: 0.3833  loss_dice_5: 2.932  loss_ce_6: 0.291  loss_mask_6: 0.383  loss_dice_6: 2.932  loss_ce_7: 0.2851  loss_mask_7: 0.3828  loss_dice_7: 2.929  loss_ce_8: 0.2989  loss_mask_8: 0.3838  loss_dice_8: 2.923  time: 1.5362  data_time: 0.1103  lr: 8.0291e-06  max_mem: 21410M
[01/17 12:34:26] d2.utils.events INFO:  eta: 1 day, 6:04:41  iter: 19499  total_loss: 36.61  loss_ce: 0.2771  loss_mask: 0.3879  loss_dice: 2.927  loss_ce_0: 0.5823  loss_mask_0: 0.3839  loss_dice_0: 3.046  loss_ce_1: 0.2838  loss_mask_1: 0.3898  loss_dice_1: 2.971  loss_ce_2: 0.2985  loss_mask_2: 0.3857  loss_dice_2: 2.951  loss_ce_3: 0.2747  loss_mask_3: 0.3878  loss_dice_3: 2.943  loss_ce_4: 0.2731  loss_mask_4: 0.3851  loss_dice_4: 2.944  loss_ce_5: 0.2694  loss_mask_5: 0.3855  loss_dice_5: 2.937  loss_ce_6: 0.271  loss_mask_6: 0.3857  loss_dice_6: 2.93  loss_ce_7: 0.269  loss_mask_7: 0.3867  loss_dice_7: 2.93  loss_ce_8: 0.2698  loss_mask_8: 0.3881  loss_dice_8: 2.928  time: 1.5362  data_time: 0.0927  lr: 8.0271e-06  max_mem: 21410M
[01/17 12:34:57] d2.utils.events INFO:  eta: 1 day, 6:04:47  iter: 19519  total_loss: 36.78  loss_ce: 0.2709  loss_mask: 0.3852  loss_dice: 2.967  loss_ce_0: 0.5573  loss_mask_0: 0.3864  loss_dice_0: 3.06  loss_ce_1: 0.3059  loss_mask_1: 0.3904  loss_dice_1: 2.994  loss_ce_2: 0.3033  loss_mask_2: 0.3876  loss_dice_2: 2.974  loss_ce_3: 0.296  loss_mask_3: 0.3879  loss_dice_3: 2.97  loss_ce_4: 0.2982  loss_mask_4: 0.3894  loss_dice_4: 2.957  loss_ce_5: 0.2872  loss_mask_5: 0.3903  loss_dice_5: 2.969  loss_ce_6: 0.2742  loss_mask_6: 0.3859  loss_dice_6: 2.961  loss_ce_7: 0.283  loss_mask_7: 0.3853  loss_dice_7: 2.97  loss_ce_8: 0.2791  loss_mask_8: 0.3881  loss_dice_8: 2.972  time: 1.5362  data_time: 0.0959  lr: 8.025e-06  max_mem: 21410M
[01/17 12:35:27] d2.utils.events INFO:  eta: 1 day, 6:03:03  iter: 19539  total_loss: 36.38  loss_ce: 0.2731  loss_mask: 0.3941  loss_dice: 2.912  loss_ce_0: 0.565  loss_mask_0: 0.3997  loss_dice_0: 3.033  loss_ce_1: 0.2979  loss_mask_1: 0.4023  loss_dice_1: 2.937  loss_ce_2: 0.2891  loss_mask_2: 0.396  loss_dice_2: 2.928  loss_ce_3: 0.2696  loss_mask_3: 0.3935  loss_dice_3: 2.913  loss_ce_4: 0.2613  loss_mask_4: 0.3942  loss_dice_4: 2.915  loss_ce_5: 0.2668  loss_mask_5: 0.3944  loss_dice_5: 2.916  loss_ce_6: 0.2665  loss_mask_6: 0.3972  loss_dice_6: 2.898  loss_ce_7: 0.2613  loss_mask_7: 0.3945  loss_dice_7: 2.917  loss_ce_8: 0.2786  loss_mask_8: 0.3958  loss_dice_8: 2.906  time: 1.5362  data_time: 0.0951  lr: 8.023e-06  max_mem: 21410M
[01/17 12:35:58] d2.utils.events INFO:  eta: 1 day, 6:03:09  iter: 19559  total_loss: 36.66  loss_ce: 0.2878  loss_mask: 0.3812  loss_dice: 2.939  loss_ce_0: 0.5926  loss_mask_0: 0.3786  loss_dice_0: 3.052  loss_ce_1: 0.3255  loss_mask_1: 0.3868  loss_dice_1: 2.979  loss_ce_2: 0.3229  loss_mask_2: 0.3835  loss_dice_2: 2.955  loss_ce_3: 0.2919  loss_mask_3: 0.3839  loss_dice_3: 2.945  loss_ce_4: 0.2958  loss_mask_4: 0.3813  loss_dice_4: 2.937  loss_ce_5: 0.3047  loss_mask_5: 0.3807  loss_dice_5: 2.946  loss_ce_6: 0.2942  loss_mask_6: 0.3811  loss_dice_6: 2.936  loss_ce_7: 0.2999  loss_mask_7: 0.3794  loss_dice_7: 2.944  loss_ce_8: 0.2917  loss_mask_8: 0.3791  loss_dice_8: 2.938  time: 1.5362  data_time: 0.1002  lr: 8.0209e-06  max_mem: 21410M
[01/17 12:36:30] d2.utils.events INFO:  eta: 1 day, 6:04:53  iter: 19579  total_loss: 36.49  loss_ce: 0.3098  loss_mask: 0.3735  loss_dice: 2.916  loss_ce_0: 0.5766  loss_mask_0: 0.3676  loss_dice_0: 3.038  loss_ce_1: 0.3102  loss_mask_1: 0.3784  loss_dice_1: 2.94  loss_ce_2: 0.3185  loss_mask_2: 0.3812  loss_dice_2: 2.928  loss_ce_3: 0.319  loss_mask_3: 0.3753  loss_dice_3: 2.915  loss_ce_4: 0.3123  loss_mask_4: 0.3745  loss_dice_4: 2.917  loss_ce_5: 0.2963  loss_mask_5: 0.3762  loss_dice_5: 2.915  loss_ce_6: 0.3037  loss_mask_6: 0.3758  loss_dice_6: 2.914  loss_ce_7: 0.3004  loss_mask_7: 0.3749  loss_dice_7: 2.907  loss_ce_8: 0.3058  loss_mask_8: 0.3741  loss_dice_8: 2.905  time: 1.5362  data_time: 0.0994  lr: 8.0189e-06  max_mem: 21410M
[01/17 12:37:01] d2.utils.events INFO:  eta: 1 day, 6:04:25  iter: 19599  total_loss: 36.31  loss_ce: 0.2841  loss_mask: 0.3806  loss_dice: 2.904  loss_ce_0: 0.5843  loss_mask_0: 0.3733  loss_dice_0: 3.024  loss_ce_1: 0.3251  loss_mask_1: 0.3798  loss_dice_1: 2.943  loss_ce_2: 0.304  loss_mask_2: 0.3794  loss_dice_2: 2.919  loss_ce_3: 0.2775  loss_mask_3: 0.3785  loss_dice_3: 2.915  loss_ce_4: 0.3055  loss_mask_4: 0.3786  loss_dice_4: 2.917  loss_ce_5: 0.288  loss_mask_5: 0.3777  loss_dice_5: 2.913  loss_ce_6: 0.2816  loss_mask_6: 0.3765  loss_dice_6: 2.908  loss_ce_7: 0.2774  loss_mask_7: 0.3768  loss_dice_7: 2.901  loss_ce_8: 0.2837  loss_mask_8: 0.3787  loss_dice_8: 2.912  time: 1.5362  data_time: 0.0984  lr: 8.0168e-06  max_mem: 21410M
[01/17 12:37:31] d2.utils.events INFO:  eta: 1 day, 6:03:51  iter: 19619  total_loss: 36.37  loss_ce: 0.2552  loss_mask: 0.3855  loss_dice: 2.902  loss_ce_0: 0.5629  loss_mask_0: 0.3782  loss_dice_0: 3.011  loss_ce_1: 0.2936  loss_mask_1: 0.3867  loss_dice_1: 2.942  loss_ce_2: 0.2915  loss_mask_2: 0.3829  loss_dice_2: 2.911  loss_ce_3: 0.2662  loss_mask_3: 0.3797  loss_dice_3: 2.901  loss_ce_4: 0.2671  loss_mask_4: 0.3821  loss_dice_4: 2.902  loss_ce_5: 0.2743  loss_mask_5: 0.3832  loss_dice_5: 2.906  loss_ce_6: 0.2626  loss_mask_6: 0.3836  loss_dice_6: 2.898  loss_ce_7: 0.2484  loss_mask_7: 0.3836  loss_dice_7: 2.901  loss_ce_8: 0.2618  loss_mask_8: 0.3849  loss_dice_8: 2.898  time: 1.5362  data_time: 0.0943  lr: 8.0148e-06  max_mem: 21410M
[01/17 12:38:02] d2.utils.events INFO:  eta: 1 day, 6:03:20  iter: 19639  total_loss: 35.79  loss_ce: 0.2922  loss_mask: 0.3751  loss_dice: 2.874  loss_ce_0: 0.5774  loss_mask_0: 0.369  loss_dice_0: 2.987  loss_ce_1: 0.3087  loss_mask_1: 0.3814  loss_dice_1: 2.912  loss_ce_2: 0.3232  loss_mask_2: 0.3793  loss_dice_2: 2.875  loss_ce_3: 0.2898  loss_mask_3: 0.3758  loss_dice_3: 2.873  loss_ce_4: 0.2976  loss_mask_4: 0.3751  loss_dice_4: 2.87  loss_ce_5: 0.2662  loss_mask_5: 0.3762  loss_dice_5: 2.874  loss_ce_6: 0.2946  loss_mask_6: 0.3791  loss_dice_6: 2.864  loss_ce_7: 0.2642  loss_mask_7: 0.379  loss_dice_7: 2.868  loss_ce_8: 0.2713  loss_mask_8: 0.3778  loss_dice_8: 2.864  time: 1.5362  data_time: 0.0935  lr: 8.0127e-06  max_mem: 21410M
[01/17 12:38:33] d2.utils.events INFO:  eta: 1 day, 6:03:23  iter: 19659  total_loss: 36.3  loss_ce: 0.277  loss_mask: 0.3707  loss_dice: 2.923  loss_ce_0: 0.6072  loss_mask_0: 0.3618  loss_dice_0: 3.05  loss_ce_1: 0.3139  loss_mask_1: 0.3735  loss_dice_1: 2.977  loss_ce_2: 0.3035  loss_mask_2: 0.3732  loss_dice_2: 2.939  loss_ce_3: 0.3002  loss_mask_3: 0.3725  loss_dice_3: 2.925  loss_ce_4: 0.3051  loss_mask_4: 0.3723  loss_dice_4: 2.918  loss_ce_5: 0.2776  loss_mask_5: 0.3718  loss_dice_5: 2.93  loss_ce_6: 0.2781  loss_mask_6: 0.3723  loss_dice_6: 2.925  loss_ce_7: 0.295  loss_mask_7: 0.3709  loss_dice_7: 2.923  loss_ce_8: 0.2908  loss_mask_8: 0.3702  loss_dice_8: 2.919  time: 1.5362  data_time: 0.0887  lr: 8.0107e-06  max_mem: 21410M
[01/17 12:39:04] d2.utils.events INFO:  eta: 1 day, 6:02:52  iter: 19679  total_loss: 35.14  loss_ce: 0.2804  loss_mask: 0.3865  loss_dice: 2.802  loss_ce_0: 0.5442  loss_mask_0: 0.3827  loss_dice_0: 2.912  loss_ce_1: 0.3104  loss_mask_1: 0.3957  loss_dice_1: 2.831  loss_ce_2: 0.3109  loss_mask_2: 0.3888  loss_dice_2: 2.819  loss_ce_3: 0.2774  loss_mask_3: 0.3861  loss_dice_3: 2.812  loss_ce_4: 0.2883  loss_mask_4: 0.387  loss_dice_4: 2.817  loss_ce_5: 0.2777  loss_mask_5: 0.3855  loss_dice_5: 2.809  loss_ce_6: 0.2903  loss_mask_6: 0.3866  loss_dice_6: 2.811  loss_ce_7: 0.2808  loss_mask_7: 0.3865  loss_dice_7: 2.793  loss_ce_8: 0.2729  loss_mask_8: 0.3847  loss_dice_8: 2.801  time: 1.5362  data_time: 0.0911  lr: 8.0086e-06  max_mem: 21410M
[01/17 12:39:34] d2.utils.events INFO:  eta: 1 day, 6:01:49  iter: 19699  total_loss: 35.63  loss_ce: 0.2749  loss_mask: 0.3808  loss_dice: 2.858  loss_ce_0: 0.562  loss_mask_0: 0.3806  loss_dice_0: 2.985  loss_ce_1: 0.2929  loss_mask_1: 0.3864  loss_dice_1: 2.896  loss_ce_2: 0.3067  loss_mask_2: 0.3826  loss_dice_2: 2.877  loss_ce_3: 0.283  loss_mask_3: 0.3815  loss_dice_3: 2.862  loss_ce_4: 0.2746  loss_mask_4: 0.3818  loss_dice_4: 2.863  loss_ce_5: 0.2695  loss_mask_5: 0.3827  loss_dice_5: 2.871  loss_ce_6: 0.2699  loss_mask_6: 0.3836  loss_dice_6: 2.864  loss_ce_7: 0.2725  loss_mask_7: 0.3811  loss_dice_7: 2.855  loss_ce_8: 0.2578  loss_mask_8: 0.381  loss_dice_8: 2.86  time: 1.5362  data_time: 0.1117  lr: 8.0066e-06  max_mem: 21410M
[01/17 12:40:05] d2.utils.events INFO:  eta: 1 day, 6:00:31  iter: 19719  total_loss: 36.44  loss_ce: 0.2735  loss_mask: 0.3794  loss_dice: 2.911  loss_ce_0: 0.6003  loss_mask_0: 0.3791  loss_dice_0: 3.027  loss_ce_1: 0.3029  loss_mask_1: 0.383  loss_dice_1: 2.95  loss_ce_2: 0.2872  loss_mask_2: 0.3833  loss_dice_2: 2.917  loss_ce_3: 0.2862  loss_mask_3: 0.3782  loss_dice_3: 2.904  loss_ce_4: 0.2887  loss_mask_4: 0.3773  loss_dice_4: 2.911  loss_ce_5: 0.2716  loss_mask_5: 0.3762  loss_dice_5: 2.909  loss_ce_6: 0.2746  loss_mask_6: 0.3771  loss_dice_6: 2.914  loss_ce_7: 0.2744  loss_mask_7: 0.3779  loss_dice_7: 2.904  loss_ce_8: 0.2658  loss_mask_8: 0.3789  loss_dice_8: 2.912  time: 1.5362  data_time: 0.0857  lr: 8.0045e-06  max_mem: 21410M
[01/17 12:40:36] d2.utils.events INFO:  eta: 1 day, 6:00:00  iter: 19739  total_loss: 36.13  loss_ce: 0.2586  loss_mask: 0.3663  loss_dice: 2.917  loss_ce_0: 0.5563  loss_mask_0: 0.3731  loss_dice_0: 3.023  loss_ce_1: 0.2785  loss_mask_1: 0.3763  loss_dice_1: 2.957  loss_ce_2: 0.2804  loss_mask_2: 0.3655  loss_dice_2: 2.944  loss_ce_3: 0.2618  loss_mask_3: 0.363  loss_dice_3: 2.926  loss_ce_4: 0.2681  loss_mask_4: 0.3637  loss_dice_4: 2.931  loss_ce_5: 0.2577  loss_mask_5: 0.3659  loss_dice_5: 2.928  loss_ce_6: 0.2667  loss_mask_6: 0.3629  loss_dice_6: 2.922  loss_ce_7: 0.2634  loss_mask_7: 0.3632  loss_dice_7: 2.923  loss_ce_8: 0.2547  loss_mask_8: 0.3654  loss_dice_8: 2.916  time: 1.5362  data_time: 0.1086  lr: 8.0025e-06  max_mem: 21410M
[01/17 12:41:07] d2.utils.events INFO:  eta: 1 day, 5:58:59  iter: 19759  total_loss: 35.77  loss_ce: 0.2662  loss_mask: 0.3848  loss_dice: 2.862  loss_ce_0: 0.5593  loss_mask_0: 0.3761  loss_dice_0: 2.987  loss_ce_1: 0.3064  loss_mask_1: 0.3845  loss_dice_1: 2.901  loss_ce_2: 0.3121  loss_mask_2: 0.3862  loss_dice_2: 2.878  loss_ce_3: 0.2757  loss_mask_3: 0.3837  loss_dice_3: 2.866  loss_ce_4: 0.2775  loss_mask_4: 0.3844  loss_dice_4: 2.876  loss_ce_5: 0.2817  loss_mask_5: 0.3853  loss_dice_5: 2.884  loss_ce_6: 0.2582  loss_mask_6: 0.3856  loss_dice_6: 2.871  loss_ce_7: 0.2685  loss_mask_7: 0.3858  loss_dice_7: 2.865  loss_ce_8: 0.2709  loss_mask_8: 0.3855  loss_dice_8: 2.866  time: 1.5362  data_time: 0.0999  lr: 8.0004e-06  max_mem: 21410M
[01/17 12:41:38] d2.utils.events INFO:  eta: 1 day, 5:59:18  iter: 19779  total_loss: 35.86  loss_ce: 0.2687  loss_mask: 0.3901  loss_dice: 2.875  loss_ce_0: 0.5521  loss_mask_0: 0.3807  loss_dice_0: 3.008  loss_ce_1: 0.2982  loss_mask_1: 0.3951  loss_dice_1: 2.936  loss_ce_2: 0.2996  loss_mask_2: 0.3901  loss_dice_2: 2.908  loss_ce_3: 0.2798  loss_mask_3: 0.3923  loss_dice_3: 2.888  loss_ce_4: 0.2693  loss_mask_4: 0.3889  loss_dice_4: 2.894  loss_ce_5: 0.2755  loss_mask_5: 0.388  loss_dice_5: 2.894  loss_ce_6: 0.2666  loss_mask_6: 0.3906  loss_dice_6: 2.886  loss_ce_7: 0.2654  loss_mask_7: 0.3914  loss_dice_7: 2.878  loss_ce_8: 0.2662  loss_mask_8: 0.3907  loss_dice_8: 2.877  time: 1.5362  data_time: 0.0988  lr: 7.9984e-06  max_mem: 21410M
[01/17 12:42:08] d2.utils.events INFO:  eta: 1 day, 5:57:30  iter: 19799  total_loss: 36.31  loss_ce: 0.2625  loss_mask: 0.399  loss_dice: 2.882  loss_ce_0: 0.5922  loss_mask_0: 0.4094  loss_dice_0: 2.989  loss_ce_1: 0.3033  loss_mask_1: 0.4106  loss_dice_1: 2.912  loss_ce_2: 0.3103  loss_mask_2: 0.4057  loss_dice_2: 2.9  loss_ce_3: 0.2874  loss_mask_3: 0.4003  loss_dice_3: 2.879  loss_ce_4: 0.2884  loss_mask_4: 0.3976  loss_dice_4: 2.878  loss_ce_5: 0.2711  loss_mask_5: 0.3973  loss_dice_5: 2.888  loss_ce_6: 0.2648  loss_mask_6: 0.3988  loss_dice_6: 2.881  loss_ce_7: 0.2795  loss_mask_7: 0.3962  loss_dice_7: 2.889  loss_ce_8: 0.2691  loss_mask_8: 0.3958  loss_dice_8: 2.881  time: 1.5362  data_time: 0.0920  lr: 7.9963e-06  max_mem: 21410M
[01/17 12:42:39] d2.utils.events INFO:  eta: 1 day, 5:57:12  iter: 19819  total_loss: 35.82  loss_ce: 0.2887  loss_mask: 0.3646  loss_dice: 2.868  loss_ce_0: 0.5546  loss_mask_0: 0.3658  loss_dice_0: 2.979  loss_ce_1: 0.3049  loss_mask_1: 0.3667  loss_dice_1: 2.909  loss_ce_2: 0.3182  loss_mask_2: 0.3666  loss_dice_2: 2.883  loss_ce_3: 0.316  loss_mask_3: 0.3659  loss_dice_3: 2.87  loss_ce_4: 0.2883  loss_mask_4: 0.3658  loss_dice_4: 2.874  loss_ce_5: 0.3009  loss_mask_5: 0.3665  loss_dice_5: 2.876  loss_ce_6: 0.2891  loss_mask_6: 0.3666  loss_dice_6: 2.871  loss_ce_7: 0.2781  loss_mask_7: 0.3645  loss_dice_7: 2.872  loss_ce_8: 0.2803  loss_mask_8: 0.364  loss_dice_8: 2.873  time: 1.5362  data_time: 0.1005  lr: 7.9943e-06  max_mem: 21410M
[01/17 12:43:09] d2.utils.events INFO:  eta: 1 day, 5:57:26  iter: 19839  total_loss: 35.6  loss_ce: 0.2513  loss_mask: 0.3646  loss_dice: 2.877  loss_ce_0: 0.5616  loss_mask_0: 0.3602  loss_dice_0: 2.989  loss_ce_1: 0.3097  loss_mask_1: 0.3646  loss_dice_1: 2.914  loss_ce_2: 0.298  loss_mask_2: 0.3629  loss_dice_2: 2.886  loss_ce_3: 0.2829  loss_mask_3: 0.365  loss_dice_3: 2.874  loss_ce_4: 0.2597  loss_mask_4: 0.3646  loss_dice_4: 2.87  loss_ce_5: 0.2633  loss_mask_5: 0.3639  loss_dice_5: 2.876  loss_ce_6: 0.2598  loss_mask_6: 0.3641  loss_dice_6: 2.871  loss_ce_7: 0.2672  loss_mask_7: 0.3639  loss_dice_7: 2.87  loss_ce_8: 0.2559  loss_mask_8: 0.3646  loss_dice_8: 2.868  time: 1.5362  data_time: 0.0998  lr: 7.9922e-06  max_mem: 21410M
[01/17 12:43:40] d2.utils.events INFO:  eta: 1 day, 5:57:43  iter: 19859  total_loss: 35.57  loss_ce: 0.2784  loss_mask: 0.3786  loss_dice: 2.826  loss_ce_0: 0.5824  loss_mask_0: 0.372  loss_dice_0: 2.95  loss_ce_1: 0.3191  loss_mask_1: 0.379  loss_dice_1: 2.873  loss_ce_2: 0.3187  loss_mask_2: 0.3804  loss_dice_2: 2.839  loss_ce_3: 0.2832  loss_mask_3: 0.3778  loss_dice_3: 2.831  loss_ce_4: 0.2792  loss_mask_4: 0.3805  loss_dice_4: 2.827  loss_ce_5: 0.2877  loss_mask_5: 0.3785  loss_dice_5: 2.84  loss_ce_6: 0.2844  loss_mask_6: 0.3786  loss_dice_6: 2.826  loss_ce_7: 0.2933  loss_mask_7: 0.3796  loss_dice_7: 2.829  loss_ce_8: 0.2803  loss_mask_8: 0.3784  loss_dice_8: 2.832  time: 1.5362  data_time: 0.0969  lr: 7.9902e-06  max_mem: 21410M
[01/17 12:44:11] d2.utils.events INFO:  eta: 1 day, 5:56:52  iter: 19879  total_loss: 36.09  loss_ce: 0.2714  loss_mask: 0.3709  loss_dice: 2.896  loss_ce_0: 0.5798  loss_mask_0: 0.3706  loss_dice_0: 3.013  loss_ce_1: 0.3098  loss_mask_1: 0.3769  loss_dice_1: 2.927  loss_ce_2: 0.2987  loss_mask_2: 0.3735  loss_dice_2: 2.902  loss_ce_3: 0.2893  loss_mask_3: 0.3747  loss_dice_3: 2.9  loss_ce_4: 0.2752  loss_mask_4: 0.3743  loss_dice_4: 2.893  loss_ce_5: 0.274  loss_mask_5: 0.3738  loss_dice_5: 2.904  loss_ce_6: 0.2669  loss_mask_6: 0.3717  loss_dice_6: 2.901  loss_ce_7: 0.2664  loss_mask_7: 0.3729  loss_dice_7: 2.905  loss_ce_8: 0.2798  loss_mask_8: 0.3718  loss_dice_8: 2.895  time: 1.5362  data_time: 0.0971  lr: 7.9881e-06  max_mem: 21410M
[01/17 12:44:41] d2.utils.events INFO:  eta: 1 day, 5:56:13  iter: 19899  total_loss: 35.68  loss_ce: 0.2762  loss_mask: 0.3783  loss_dice: 2.894  loss_ce_0: 0.5671  loss_mask_0: 0.372  loss_dice_0: 3.004  loss_ce_1: 0.3024  loss_mask_1: 0.3838  loss_dice_1: 2.926  loss_ce_2: 0.2772  loss_mask_2: 0.3816  loss_dice_2: 2.899  loss_ce_3: 0.2805  loss_mask_3: 0.3758  loss_dice_3: 2.901  loss_ce_4: 0.26  loss_mask_4: 0.3721  loss_dice_4: 2.898  loss_ce_5: 0.2574  loss_mask_5: 0.3735  loss_dice_5: 2.902  loss_ce_6: 0.2597  loss_mask_6: 0.3756  loss_dice_6: 2.898  loss_ce_7: 0.2612  loss_mask_7: 0.3766  loss_dice_7: 2.892  loss_ce_8: 0.268  loss_mask_8: 0.3766  loss_dice_8: 2.892  time: 1.5362  data_time: 0.0962  lr: 7.9861e-06  max_mem: 21410M
[01/17 12:45:12] d2.utils.events INFO:  eta: 1 day, 5:55:51  iter: 19919  total_loss: 36.23  loss_ce: 0.252  loss_mask: 0.3749  loss_dice: 2.893  loss_ce_0: 0.5701  loss_mask_0: 0.3804  loss_dice_0: 3.004  loss_ce_1: 0.2724  loss_mask_1: 0.382  loss_dice_1: 2.925  loss_ce_2: 0.2738  loss_mask_2: 0.3797  loss_dice_2: 2.907  loss_ce_3: 0.2621  loss_mask_3: 0.3791  loss_dice_3: 2.89  loss_ce_4: 0.2532  loss_mask_4: 0.3799  loss_dice_4: 2.898  loss_ce_5: 0.2594  loss_mask_5: 0.3823  loss_dice_5: 2.898  loss_ce_6: 0.2456  loss_mask_6: 0.3796  loss_dice_6: 2.895  loss_ce_7: 0.2497  loss_mask_7: 0.3772  loss_dice_7: 2.9  loss_ce_8: 0.2579  loss_mask_8: 0.3742  loss_dice_8: 2.888  time: 1.5361  data_time: 0.0939  lr: 7.984e-06  max_mem: 21410M
[01/17 12:45:42] d2.utils.events INFO:  eta: 1 day, 5:54:11  iter: 19939  total_loss: 36.62  loss_ce: 0.2789  loss_mask: 0.3846  loss_dice: 2.96  loss_ce_0: 0.5639  loss_mask_0: 0.3828  loss_dice_0: 3.057  loss_ce_1: 0.3154  loss_mask_1: 0.3883  loss_dice_1: 2.992  loss_ce_2: 0.3043  loss_mask_2: 0.3822  loss_dice_2: 2.963  loss_ce_3: 0.2831  loss_mask_3: 0.3814  loss_dice_3: 2.965  loss_ce_4: 0.2637  loss_mask_4: 0.3798  loss_dice_4: 2.96  loss_ce_5: 0.2701  loss_mask_5: 0.3815  loss_dice_5: 2.957  loss_ce_6: 0.2701  loss_mask_6: 0.3813  loss_dice_6: 2.958  loss_ce_7: 0.2728  loss_mask_7: 0.3829  loss_dice_7: 2.957  loss_ce_8: 0.2737  loss_mask_8: 0.3855  loss_dice_8: 2.949  time: 1.5361  data_time: 0.0931  lr: 7.982e-06  max_mem: 21410M
[01/17 12:46:13] d2.utils.events INFO:  eta: 1 day, 5:53:54  iter: 19959  total_loss: 36.34  loss_ce: 0.2783  loss_mask: 0.3827  loss_dice: 2.915  loss_ce_0: 0.5579  loss_mask_0: 0.3805  loss_dice_0: 3.04  loss_ce_1: 0.2989  loss_mask_1: 0.3857  loss_dice_1: 2.958  loss_ce_2: 0.3145  loss_mask_2: 0.3823  loss_dice_2: 2.933  loss_ce_3: 0.2969  loss_mask_3: 0.3845  loss_dice_3: 2.919  loss_ce_4: 0.2836  loss_mask_4: 0.383  loss_dice_4: 2.921  loss_ce_5: 0.2834  loss_mask_5: 0.383  loss_dice_5: 2.919  loss_ce_6: 0.274  loss_mask_6: 0.3847  loss_dice_6: 2.921  loss_ce_7: 0.2781  loss_mask_7: 0.3841  loss_dice_7: 2.913  loss_ce_8: 0.2769  loss_mask_8: 0.383  loss_dice_8: 2.914  time: 1.5362  data_time: 0.0955  lr: 7.9799e-06  max_mem: 21410M
[01/17 12:46:44] d2.utils.events INFO:  eta: 1 day, 5:52:37  iter: 19979  total_loss: 35.76  loss_ce: 0.2795  loss_mask: 0.3724  loss_dice: 2.863  loss_ce_0: 0.5831  loss_mask_0: 0.3655  loss_dice_0: 2.973  loss_ce_1: 0.2972  loss_mask_1: 0.3762  loss_dice_1: 2.888  loss_ce_2: 0.2988  loss_mask_2: 0.3747  loss_dice_2: 2.868  loss_ce_3: 0.3001  loss_mask_3: 0.3721  loss_dice_3: 2.855  loss_ce_4: 0.2866  loss_mask_4: 0.3707  loss_dice_4: 2.861  loss_ce_5: 0.2743  loss_mask_5: 0.3707  loss_dice_5: 2.854  loss_ce_6: 0.2785  loss_mask_6: 0.3713  loss_dice_6: 2.844  loss_ce_7: 0.2655  loss_mask_7: 0.3702  loss_dice_7: 2.852  loss_ce_8: 0.2657  loss_mask_8: 0.372  loss_dice_8: 2.849  time: 1.5361  data_time: 0.1012  lr: 7.9779e-06  max_mem: 21410M
[01/17 12:47:15] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla/model_0019999.pth
[01/17 12:47:16] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 12:47:17] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 12:47:17] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 12:47:17] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 12:47:31] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0075 s/iter. Inference: 0.1472 s/iter. Eval: 0.1903 s/iter. Total: 0.3450 s/iter. ETA=0:06:13
[01/17 12:47:36] d2.evaluation.evaluator INFO: Inference done 22/1093. Dataloading: 0.0122 s/iter. Inference: 0.1697 s/iter. Eval: 0.2339 s/iter. Total: 0.4159 s/iter. ETA=0:07:25
[01/17 12:47:41] d2.evaluation.evaluator INFO: Inference done 35/1093. Dataloading: 0.0123 s/iter. Inference: 0.1734 s/iter. Eval: 0.2236 s/iter. Total: 0.4094 s/iter. ETA=0:07:13
[01/17 12:47:46] d2.evaluation.evaluator INFO: Inference done 47/1093. Dataloading: 0.0155 s/iter. Inference: 0.1762 s/iter. Eval: 0.2235 s/iter. Total: 0.4153 s/iter. ETA=0:07:14
[01/17 12:47:52] d2.evaluation.evaluator INFO: Inference done 59/1093. Dataloading: 0.0157 s/iter. Inference: 0.1740 s/iter. Eval: 0.2287 s/iter. Total: 0.4185 s/iter. ETA=0:07:12
[01/17 12:47:57] d2.evaluation.evaluator INFO: Inference done 70/1093. Dataloading: 0.0153 s/iter. Inference: 0.1772 s/iter. Eval: 0.2346 s/iter. Total: 0.4272 s/iter. ETA=0:07:16
[01/17 12:48:02] d2.evaluation.evaluator INFO: Inference done 83/1093. Dataloading: 0.0149 s/iter. Inference: 0.1754 s/iter. Eval: 0.2343 s/iter. Total: 0.4248 s/iter. ETA=0:07:09
[01/17 12:48:07] d2.evaluation.evaluator INFO: Inference done 94/1093. Dataloading: 0.0150 s/iter. Inference: 0.1756 s/iter. Eval: 0.2379 s/iter. Total: 0.4285 s/iter. ETA=0:07:08
[01/17 12:48:12] d2.evaluation.evaluator INFO: Inference done 108/1093. Dataloading: 0.0146 s/iter. Inference: 0.1725 s/iter. Eval: 0.2333 s/iter. Total: 0.4205 s/iter. ETA=0:06:54
[01/17 12:48:18] d2.evaluation.evaluator INFO: Inference done 122/1093. Dataloading: 0.0142 s/iter. Inference: 0.1721 s/iter. Eval: 0.2290 s/iter. Total: 0.4154 s/iter. ETA=0:06:43
[01/17 12:48:23] d2.evaluation.evaluator INFO: Inference done 135/1093. Dataloading: 0.0140 s/iter. Inference: 0.1729 s/iter. Eval: 0.2269 s/iter. Total: 0.4138 s/iter. ETA=0:06:36
[01/17 12:48:28] d2.evaluation.evaluator INFO: Inference done 149/1093. Dataloading: 0.0136 s/iter. Inference: 0.1737 s/iter. Eval: 0.2210 s/iter. Total: 0.4084 s/iter. ETA=0:06:25
[01/17 12:48:33] d2.evaluation.evaluator INFO: Inference done 163/1093. Dataloading: 0.0134 s/iter. Inference: 0.1736 s/iter. Eval: 0.2170 s/iter. Total: 0.4042 s/iter. ETA=0:06:15
[01/17 12:48:38] d2.evaluation.evaluator INFO: Inference done 175/1093. Dataloading: 0.0135 s/iter. Inference: 0.1738 s/iter. Eval: 0.2181 s/iter. Total: 0.4054 s/iter. ETA=0:06:12
[01/17 12:48:43] d2.evaluation.evaluator INFO: Inference done 188/1093. Dataloading: 0.0134 s/iter. Inference: 0.1739 s/iter. Eval: 0.2168 s/iter. Total: 0.4041 s/iter. ETA=0:06:05
[01/17 12:48:48] d2.evaluation.evaluator INFO: Inference done 203/1093. Dataloading: 0.0132 s/iter. Inference: 0.1725 s/iter. Eval: 0.2144 s/iter. Total: 0.4003 s/iter. ETA=0:05:56
[01/17 12:48:53] d2.evaluation.evaluator INFO: Inference done 216/1093. Dataloading: 0.0135 s/iter. Inference: 0.1733 s/iter. Eval: 0.2131 s/iter. Total: 0.4000 s/iter. ETA=0:05:50
[01/17 12:48:59] d2.evaluation.evaluator INFO: Inference done 229/1093. Dataloading: 0.0134 s/iter. Inference: 0.1735 s/iter. Eval: 0.2134 s/iter. Total: 0.4005 s/iter. ETA=0:05:46
[01/17 12:49:04] d2.evaluation.evaluator INFO: Inference done 241/1093. Dataloading: 0.0134 s/iter. Inference: 0.1736 s/iter. Eval: 0.2143 s/iter. Total: 0.4014 s/iter. ETA=0:05:41
[01/17 12:49:09] d2.evaluation.evaluator INFO: Inference done 254/1093. Dataloading: 0.0134 s/iter. Inference: 0.1732 s/iter. Eval: 0.2142 s/iter. Total: 0.4008 s/iter. ETA=0:05:36
[01/17 12:49:14] d2.evaluation.evaluator INFO: Inference done 266/1093. Dataloading: 0.0134 s/iter. Inference: 0.1739 s/iter. Eval: 0.2150 s/iter. Total: 0.4025 s/iter. ETA=0:05:32
[01/17 12:49:19] d2.evaluation.evaluator INFO: Inference done 278/1093. Dataloading: 0.0136 s/iter. Inference: 0.1739 s/iter. Eval: 0.2157 s/iter. Total: 0.4032 s/iter. ETA=0:05:28
[01/17 12:49:24] d2.evaluation.evaluator INFO: Inference done 293/1093. Dataloading: 0.0134 s/iter. Inference: 0.1734 s/iter. Eval: 0.2139 s/iter. Total: 0.4009 s/iter. ETA=0:05:20
[01/17 12:49:30] d2.evaluation.evaluator INFO: Inference done 305/1093. Dataloading: 0.0135 s/iter. Inference: 0.1740 s/iter. Eval: 0.2141 s/iter. Total: 0.4017 s/iter. ETA=0:05:16
[01/17 12:49:35] d2.evaluation.evaluator INFO: Inference done 318/1093. Dataloading: 0.0134 s/iter. Inference: 0.1736 s/iter. Eval: 0.2143 s/iter. Total: 0.4014 s/iter. ETA=0:05:11
[01/17 12:49:40] d2.evaluation.evaluator INFO: Inference done 332/1093. Dataloading: 0.0134 s/iter. Inference: 0.1734 s/iter. Eval: 0.2134 s/iter. Total: 0.4003 s/iter. ETA=0:05:04
[01/17 12:49:45] d2.evaluation.evaluator INFO: Inference done 346/1093. Dataloading: 0.0133 s/iter. Inference: 0.1742 s/iter. Eval: 0.2116 s/iter. Total: 0.3992 s/iter. ETA=0:04:58
[01/17 12:49:50] d2.evaluation.evaluator INFO: Inference done 360/1093. Dataloading: 0.0133 s/iter. Inference: 0.1738 s/iter. Eval: 0.2107 s/iter. Total: 0.3979 s/iter. ETA=0:04:51
[01/17 12:49:55] d2.evaluation.evaluator INFO: Inference done 372/1093. Dataloading: 0.0133 s/iter. Inference: 0.1739 s/iter. Eval: 0.2113 s/iter. Total: 0.3987 s/iter. ETA=0:04:47
[01/17 12:50:01] d2.evaluation.evaluator INFO: Inference done 386/1093. Dataloading: 0.0133 s/iter. Inference: 0.1736 s/iter. Eval: 0.2109 s/iter. Total: 0.3979 s/iter. ETA=0:04:41
[01/17 12:50:06] d2.evaluation.evaluator INFO: Inference done 400/1093. Dataloading: 0.0132 s/iter. Inference: 0.1731 s/iter. Eval: 0.2108 s/iter. Total: 0.3972 s/iter. ETA=0:04:35
[01/17 12:50:11] d2.evaluation.evaluator INFO: Inference done 414/1093. Dataloading: 0.0132 s/iter. Inference: 0.1731 s/iter. Eval: 0.2108 s/iter. Total: 0.3972 s/iter. ETA=0:04:29
[01/17 12:50:17] d2.evaluation.evaluator INFO: Inference done 425/1093. Dataloading: 0.0132 s/iter. Inference: 0.1731 s/iter. Eval: 0.2126 s/iter. Total: 0.3990 s/iter. ETA=0:04:26
[01/17 12:50:22] d2.evaluation.evaluator INFO: Inference done 439/1093. Dataloading: 0.0132 s/iter. Inference: 0.1728 s/iter. Eval: 0.2118 s/iter. Total: 0.3978 s/iter. ETA=0:04:20
[01/17 12:50:27] d2.evaluation.evaluator INFO: Inference done 453/1093. Dataloading: 0.0132 s/iter. Inference: 0.1726 s/iter. Eval: 0.2111 s/iter. Total: 0.3970 s/iter. ETA=0:04:14
[01/17 12:50:32] d2.evaluation.evaluator INFO: Inference done 468/1093. Dataloading: 0.0131 s/iter. Inference: 0.1720 s/iter. Eval: 0.2100 s/iter. Total: 0.3953 s/iter. ETA=0:04:07
[01/17 12:50:37] d2.evaluation.evaluator INFO: Inference done 482/1093. Dataloading: 0.0131 s/iter. Inference: 0.1720 s/iter. Eval: 0.2093 s/iter. Total: 0.3945 s/iter. ETA=0:04:01
[01/17 12:50:42] d2.evaluation.evaluator INFO: Inference done 496/1093. Dataloading: 0.0130 s/iter. Inference: 0.1719 s/iter. Eval: 0.2089 s/iter. Total: 0.3939 s/iter. ETA=0:03:55
[01/17 12:50:48] d2.evaluation.evaluator INFO: Inference done 512/1093. Dataloading: 0.0129 s/iter. Inference: 0.1720 s/iter. Eval: 0.2070 s/iter. Total: 0.3920 s/iter. ETA=0:03:47
[01/17 12:50:53] d2.evaluation.evaluator INFO: Inference done 525/1093. Dataloading: 0.0130 s/iter. Inference: 0.1719 s/iter. Eval: 0.2074 s/iter. Total: 0.3924 s/iter. ETA=0:03:42
[01/17 12:50:58] d2.evaluation.evaluator INFO: Inference done 540/1093. Dataloading: 0.0129 s/iter. Inference: 0.1715 s/iter. Eval: 0.2067 s/iter. Total: 0.3912 s/iter. ETA=0:03:36
[01/17 12:51:03] d2.evaluation.evaluator INFO: Inference done 552/1093. Dataloading: 0.0130 s/iter. Inference: 0.1717 s/iter. Eval: 0.2073 s/iter. Total: 0.3920 s/iter. ETA=0:03:32
[01/17 12:51:09] d2.evaluation.evaluator INFO: Inference done 566/1093. Dataloading: 0.0129 s/iter. Inference: 0.1717 s/iter. Eval: 0.2072 s/iter. Total: 0.3919 s/iter. ETA=0:03:26
[01/17 12:51:14] d2.evaluation.evaluator INFO: Inference done 582/1093. Dataloading: 0.0130 s/iter. Inference: 0.1713 s/iter. Eval: 0.2056 s/iter. Total: 0.3900 s/iter. ETA=0:03:19
[01/17 12:51:19] d2.evaluation.evaluator INFO: Inference done 595/1093. Dataloading: 0.0129 s/iter. Inference: 0.1716 s/iter. Eval: 0.2056 s/iter. Total: 0.3902 s/iter. ETA=0:03:14
[01/17 12:51:24] d2.evaluation.evaluator INFO: Inference done 606/1093. Dataloading: 0.0130 s/iter. Inference: 0.1717 s/iter. Eval: 0.2067 s/iter. Total: 0.3915 s/iter. ETA=0:03:10
[01/17 12:51:29] d2.evaluation.evaluator INFO: Inference done 619/1093. Dataloading: 0.0130 s/iter. Inference: 0.1718 s/iter. Eval: 0.2067 s/iter. Total: 0.3915 s/iter. ETA=0:03:05
[01/17 12:51:35] d2.evaluation.evaluator INFO: Inference done 632/1093. Dataloading: 0.0129 s/iter. Inference: 0.1722 s/iter. Eval: 0.2064 s/iter. Total: 0.3916 s/iter. ETA=0:03:00
[01/17 12:51:40] d2.evaluation.evaluator INFO: Inference done 647/1093. Dataloading: 0.0129 s/iter. Inference: 0.1720 s/iter. Eval: 0.2055 s/iter. Total: 0.3905 s/iter. ETA=0:02:54
[01/17 12:51:45] d2.evaluation.evaluator INFO: Inference done 660/1093. Dataloading: 0.0129 s/iter. Inference: 0.1722 s/iter. Eval: 0.2053 s/iter. Total: 0.3906 s/iter. ETA=0:02:49
[01/17 12:51:50] d2.evaluation.evaluator INFO: Inference done 672/1093. Dataloading: 0.0129 s/iter. Inference: 0.1726 s/iter. Eval: 0.2055 s/iter. Total: 0.3911 s/iter. ETA=0:02:44
[01/17 12:51:55] d2.evaluation.evaluator INFO: Inference done 687/1093. Dataloading: 0.0128 s/iter. Inference: 0.1726 s/iter. Eval: 0.2046 s/iter. Total: 0.3901 s/iter. ETA=0:02:38
[01/17 12:52:00] d2.evaluation.evaluator INFO: Inference done 701/1093. Dataloading: 0.0128 s/iter. Inference: 0.1725 s/iter. Eval: 0.2043 s/iter. Total: 0.3897 s/iter. ETA=0:02:32
[01/17 12:52:05] d2.evaluation.evaluator INFO: Inference done 712/1093. Dataloading: 0.0128 s/iter. Inference: 0.1725 s/iter. Eval: 0.2054 s/iter. Total: 0.3908 s/iter. ETA=0:02:28
[01/17 12:52:11] d2.evaluation.evaluator INFO: Inference done 727/1093. Dataloading: 0.0128 s/iter. Inference: 0.1722 s/iter. Eval: 0.2049 s/iter. Total: 0.3900 s/iter. ETA=0:02:22
[01/17 12:52:16] d2.evaluation.evaluator INFO: Inference done 742/1093. Dataloading: 0.0127 s/iter. Inference: 0.1721 s/iter. Eval: 0.2043 s/iter. Total: 0.3893 s/iter. ETA=0:02:16
[01/17 12:52:21] d2.evaluation.evaluator INFO: Inference done 756/1093. Dataloading: 0.0127 s/iter. Inference: 0.1722 s/iter. Eval: 0.2041 s/iter. Total: 0.3891 s/iter. ETA=0:02:11
[01/17 12:52:26] d2.evaluation.evaluator INFO: Inference done 768/1093. Dataloading: 0.0127 s/iter. Inference: 0.1723 s/iter. Eval: 0.2046 s/iter. Total: 0.3897 s/iter. ETA=0:02:06
[01/17 12:52:31] d2.evaluation.evaluator INFO: Inference done 782/1093. Dataloading: 0.0127 s/iter. Inference: 0.1722 s/iter. Eval: 0.2043 s/iter. Total: 0.3893 s/iter. ETA=0:02:01
[01/17 12:52:37] d2.evaluation.evaluator INFO: Inference done 797/1093. Dataloading: 0.0127 s/iter. Inference: 0.1721 s/iter. Eval: 0.2037 s/iter. Total: 0.3887 s/iter. ETA=0:01:55
[01/17 12:52:42] d2.evaluation.evaluator INFO: Inference done 810/1093. Dataloading: 0.0127 s/iter. Inference: 0.1724 s/iter. Eval: 0.2037 s/iter. Total: 0.3889 s/iter. ETA=0:01:50
[01/17 12:52:47] d2.evaluation.evaluator INFO: Inference done 825/1093. Dataloading: 0.0127 s/iter. Inference: 0.1726 s/iter. Eval: 0.2029 s/iter. Total: 0.3882 s/iter. ETA=0:01:44
[01/17 12:52:52] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0126 s/iter. Inference: 0.1727 s/iter. Eval: 0.2024 s/iter. Total: 0.3878 s/iter. ETA=0:01:38
[01/17 12:52:58] d2.evaluation.evaluator INFO: Inference done 853/1093. Dataloading: 0.0126 s/iter. Inference: 0.1726 s/iter. Eval: 0.2025 s/iter. Total: 0.3878 s/iter. ETA=0:01:33
[01/17 12:53:03] d2.evaluation.evaluator INFO: Inference done 866/1093. Dataloading: 0.0126 s/iter. Inference: 0.1725 s/iter. Eval: 0.2027 s/iter. Total: 0.3880 s/iter. ETA=0:01:28
[01/17 12:53:08] d2.evaluation.evaluator INFO: Inference done 879/1093. Dataloading: 0.0126 s/iter. Inference: 0.1727 s/iter. Eval: 0.2025 s/iter. Total: 0.3879 s/iter. ETA=0:01:23
[01/17 12:53:13] d2.evaluation.evaluator INFO: Inference done 890/1093. Dataloading: 0.0126 s/iter. Inference: 0.1729 s/iter. Eval: 0.2033 s/iter. Total: 0.3889 s/iter. ETA=0:01:18
[01/17 12:53:18] d2.evaluation.evaluator INFO: Inference done 902/1093. Dataloading: 0.0126 s/iter. Inference: 0.1733 s/iter. Eval: 0.2034 s/iter. Total: 0.3895 s/iter. ETA=0:01:14
[01/17 12:53:23] d2.evaluation.evaluator INFO: Inference done 915/1093. Dataloading: 0.0126 s/iter. Inference: 0.1737 s/iter. Eval: 0.2032 s/iter. Total: 0.3896 s/iter. ETA=0:01:09
[01/17 12:53:29] d2.evaluation.evaluator INFO: Inference done 928/1093. Dataloading: 0.0126 s/iter. Inference: 0.1737 s/iter. Eval: 0.2033 s/iter. Total: 0.3897 s/iter. ETA=0:01:04
[01/17 12:53:34] d2.evaluation.evaluator INFO: Inference done 941/1093. Dataloading: 0.0126 s/iter. Inference: 0.1737 s/iter. Eval: 0.2035 s/iter. Total: 0.3900 s/iter. ETA=0:00:59
[01/17 12:53:39] d2.evaluation.evaluator INFO: Inference done 953/1093. Dataloading: 0.0127 s/iter. Inference: 0.1740 s/iter. Eval: 0.2039 s/iter. Total: 0.3907 s/iter. ETA=0:00:54
[01/17 12:53:44] d2.evaluation.evaluator INFO: Inference done 965/1093. Dataloading: 0.0127 s/iter. Inference: 0.1742 s/iter. Eval: 0.2041 s/iter. Total: 0.3910 s/iter. ETA=0:00:50
[01/17 12:53:50] d2.evaluation.evaluator INFO: Inference done 978/1093. Dataloading: 0.0126 s/iter. Inference: 0.1747 s/iter. Eval: 0.2038 s/iter. Total: 0.3912 s/iter. ETA=0:00:44
[01/17 12:53:55] d2.evaluation.evaluator INFO: Inference done 994/1093. Dataloading: 0.0126 s/iter. Inference: 0.1746 s/iter. Eval: 0.2029 s/iter. Total: 0.3903 s/iter. ETA=0:00:38
[01/17 12:54:00] d2.evaluation.evaluator INFO: Inference done 1008/1093. Dataloading: 0.0126 s/iter. Inference: 0.1744 s/iter. Eval: 0.2028 s/iter. Total: 0.3898 s/iter. ETA=0:00:33
[01/17 12:54:05] d2.evaluation.evaluator INFO: Inference done 1022/1093. Dataloading: 0.0126 s/iter. Inference: 0.1743 s/iter. Eval: 0.2027 s/iter. Total: 0.3897 s/iter. ETA=0:00:27
[01/17 12:54:10] d2.evaluation.evaluator INFO: Inference done 1035/1093. Dataloading: 0.0126 s/iter. Inference: 0.1741 s/iter. Eval: 0.2029 s/iter. Total: 0.3897 s/iter. ETA=0:00:22
[01/17 12:54:16] d2.evaluation.evaluator INFO: Inference done 1049/1093. Dataloading: 0.0126 s/iter. Inference: 0.1741 s/iter. Eval: 0.2028 s/iter. Total: 0.3896 s/iter. ETA=0:00:17
[01/17 12:54:21] d2.evaluation.evaluator INFO: Inference done 1065/1093. Dataloading: 0.0125 s/iter. Inference: 0.1738 s/iter. Eval: 0.2023 s/iter. Total: 0.3887 s/iter. ETA=0:00:10
[01/17 12:54:26] d2.evaluation.evaluator INFO: Inference done 1082/1093. Dataloading: 0.0125 s/iter. Inference: 0.1734 s/iter. Eval: 0.2014 s/iter. Total: 0.3873 s/iter. ETA=0:00:04
[01/17 12:54:30] d2.evaluation.evaluator INFO: Total inference time: 0:07:00.959261 (0.386911 s / iter per device, on 4 devices)
[01/17 12:54:30] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:08 (0.172976 s / iter per device, on 4 devices)
[01/17 12:54:54] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 14.345126049493901, 'fwIoU': 41.2597816038213, 'IoU-1': nan, 'IoU-2': 95.13331051115287, 'IoU-3': 45.24121094462209, 'IoU-4': 56.75242156977248, 'IoU-5': 48.83567960847696, 'IoU-6': 43.53108060296064, 'IoU-7': 40.517182012258104, 'IoU-8': 34.0563192571291, 'IoU-9': 20.97402956924102, 'IoU-10': 31.267896557803176, 'IoU-11': 38.436669783730395, 'IoU-12': 51.61653816827973, 'IoU-13': 50.67017988329323, 'IoU-14': 50.300829330932174, 'IoU-15': 49.82994548626141, 'IoU-16': 48.68742426796428, 'IoU-17': 48.41735341338554, 'IoU-18': 43.86454325760715, 'IoU-19': 41.55850236223306, 'IoU-20': 42.44317247271506, 'IoU-21': 43.25271745430997, 'IoU-22': 43.48648533124333, 'IoU-23': 45.911585751172765, 'IoU-24': 44.16724837910723, 'IoU-25': 44.2449292035478, 'IoU-26': 43.41355736063992, 'IoU-27': 43.17402913611838, 'IoU-28': 44.51256605143502, 'IoU-29': 44.40066758182188, 'IoU-30': 46.01732250075934, 'IoU-31': 44.02139633250703, 'IoU-32': 45.65733486906976, 'IoU-33': 43.70744284557778, 'IoU-34': 42.28931184318698, 'IoU-35': 40.82388851693452, 'IoU-36': 42.96463293532475, 'IoU-37': 41.54526679979902, 'IoU-38': 40.78519675008346, 'IoU-39': 40.89678450730317, 'IoU-40': 40.515001858864416, 'IoU-41': 40.349993885282935, 'IoU-42': 38.919948162387435, 'IoU-43': 39.33441099039233, 'IoU-44': 38.7935404017179, 'IoU-45': 37.99719170962623, 'IoU-46': 36.96392293790262, 'IoU-47': 35.80556220556331, 'IoU-48': 35.43410474136578, 'IoU-49': 34.91491461356455, 'IoU-50': 32.71164714847145, 'IoU-51': 32.82100393405876, 'IoU-52': 30.55844303970015, 'IoU-53': 29.55194319519123, 'IoU-54': 29.548518783923228, 'IoU-55': 29.84625841367956, 'IoU-56': 28.527682908723463, 'IoU-57': 27.36289469247492, 'IoU-58': 26.83520553923685, 'IoU-59': 24.33376152554411, 'IoU-60': 24.08974400581737, 'IoU-61': 24.841475510810554, 'IoU-62': 22.858079313290503, 'IoU-63': 22.861059658688117, 'IoU-64': 23.005508586830757, 'IoU-65': 23.452709979596012, 'IoU-66': 22.867778695281103, 'IoU-67': 21.64822445715183, 'IoU-68': 21.131031019865876, 'IoU-69': 20.1391001022631, 'IoU-70': 18.2272748741733, 'IoU-71': 19.391027070462965, 'IoU-72': 17.925595914799, 'IoU-73': 18.569930332610447, 'IoU-74': 19.491142173580627, 'IoU-75': 18.141534997068238, 'IoU-76': 19.730116389116002, 'IoU-77': 18.729977321421515, 'IoU-78': 17.811587722898956, 'IoU-79': 17.927402090785442, 'IoU-80': 18.323178858759285, 'IoU-81': 17.020135468067394, 'IoU-82': 16.940167890894678, 'IoU-83': 16.611080309126752, 'IoU-84': 17.360768910234405, 'IoU-85': 14.666991211950151, 'IoU-86': 15.569947017711112, 'IoU-87': 15.271037601709434, 'IoU-88': 15.431090247198377, 'IoU-89': 14.391964208457514, 'IoU-90': 15.009959351131249, 'IoU-91': 14.555965089853522, 'IoU-92': 15.406103305884455, 'IoU-93': 14.228080160854228, 'IoU-94': 15.772889941849774, 'IoU-95': 14.628477102875575, 'IoU-96': 15.051603735057093, 'IoU-97': 15.524477584099511, 'IoU-98': 14.537539359734353, 'IoU-99': 14.929227439919796, 'IoU-100': 15.240820774632116, 'IoU-101': 12.89075394787865, 'IoU-102': 13.344918711190365, 'IoU-103': 13.682456158404271, 'IoU-104': 13.853535336883233, 'IoU-105': 12.969299191097589, 'IoU-106': 13.819501062209167, 'IoU-107': 12.850155961817103, 'IoU-108': 13.081772246430074, 'IoU-109': 13.52102567429122, 'IoU-110': 12.636984694571494, 'IoU-111': 13.078843673245807, 'IoU-112': 12.547858251190405, 'IoU-113': 12.162254067947357, 'IoU-114': 11.87038023613537, 'IoU-115': 11.548817982935585, 'IoU-116': 12.342270557081738, 'IoU-117': 10.655738599682952, 'IoU-118': 11.217803374983536, 'IoU-119': 10.476737050041441, 'IoU-120': 10.11799878409331, 'IoU-121': 11.341231426309514, 'IoU-122': 8.537154680040516, 'IoU-123': 9.721729858769015, 'IoU-124': 9.985599974234209, 'IoU-125': 8.215568206306521, 'IoU-126': 7.6396048071052105, 'IoU-127': 8.01585438828411, 'IoU-128': 8.721956893797001, 'IoU-129': 7.90767522839253, 'IoU-130': 7.3692192291340985, 'IoU-131': 8.073120682557732, 'IoU-132': 7.594943960510349, 'IoU-133': 7.606860140209677, 'IoU-134': 7.048639658284429, 'IoU-135': 7.167071575593588, 'IoU-136': 5.091909423443697, 'IoU-137': 6.397110999659954, 'IoU-138': 7.124845190175535, 'IoU-139': 5.552952327965464, 'IoU-140': 4.699894156045381, 'IoU-141': 4.831186957985766, 'IoU-142': 5.913888940560522, 'IoU-143': 4.6833080712391055, 'IoU-144': 4.499029570581349, 'IoU-145': 6.11545565769506, 'IoU-146': 4.387917465005765, 'IoU-147': 4.807904559428526, 'IoU-148': 4.720223741534725, 'IoU-149': 2.5615677236536643, 'IoU-150': 4.120824442144976, 'IoU-151': 3.754888523519346, 'IoU-152': 2.5255369855245715, 'IoU-153': 4.29597090359454, 'IoU-154': 2.1439901254977745, 'IoU-155': 3.268258011344623, 'IoU-156': 2.139653870022942, 'IoU-157': 2.8615254374264354, 'IoU-158': 2.480282182259193, 'IoU-159': 2.3351921073595197, 'IoU-160': 1.8652491108148408, 'IoU-161': 2.093341397718644, 'IoU-162': 2.3851624917100347, 'IoU-163': 1.6393154716356273, 'IoU-164': 1.9254367645576185, 'IoU-165': 2.3148948626785226, 'IoU-166': 1.806712659835101, 'IoU-167': 1.3570697424100122, 'IoU-168': 1.6319868014946188, 'IoU-169': 1.5484420150168148, 'IoU-170': 1.235320195810339, 'IoU-171': 1.4185646664261, 'IoU-172': 1.3481341054784752, 'IoU-173': 1.7841256416181641, 'IoU-174': 1.612457379213102, 'IoU-175': 0.5435969441621984, 'IoU-176': 1.7736682193496778, 'IoU-177': 0.9097730989281733, 'IoU-178': 1.2389376315247282, 'IoU-179': 3.0771453215851188, 'IoU-180': 0.7467202316947128, 'IoU-181': 1.2903405120386253, 'IoU-182': 0.9422978829447978, 'IoU-183': 2.116455632186814, 'IoU-184': 0.3095011987579282, 'IoU-185': 1.619515703668185, 'IoU-186': 1.1136581265651428, 'IoU-187': 1.3036642618065486, 'IoU-188': 1.3434139553730824, 'IoU-189': 1.2392127704068592, 'IoU-190': 1.4486190207778578, 'IoU-191': 1.017864289421761, 'IoU-192': 1.8515888951051864, 'IoU-193': 1.0367662394846007, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 21.823428154339485, 'pACC': 55.50663588823377, 'ACC-1': nan, 'ACC-2': 98.67334808013625, 'ACC-3': 58.49574591453962, 'ACC-4': 69.61797893014689, 'ACC-5': 65.11129258706487, 'ACC-6': 62.91160795589946, 'ACC-7': 58.851539311910074, 'ACC-8': 48.730907088761754, 'ACC-9': 25.65961222207787, 'ACC-10': 39.06005509029236, 'ACC-11': 53.060929066332854, 'ACC-12': 67.89273247816206, 'ACC-13': 71.43943035221037, 'ACC-14': 69.2965781296617, 'ACC-15': 67.86990286647625, 'ACC-16': 66.77652495294879, 'ACC-17': 64.5886838997575, 'ACC-18': 63.43609541288991, 'ACC-19': 58.736858002931314, 'ACC-20': 59.69400562031111, 'ACC-21': 59.85322583664734, 'ACC-22': 58.92249479769589, 'ACC-23': 62.08940630086332, 'ACC-24': 61.25846855116377, 'ACC-25': 59.72238249940125, 'ACC-26': 61.5816783522524, 'ACC-27': 60.77473994068603, 'ACC-28': 60.25034771298883, 'ACC-29': 60.73512436827938, 'ACC-30': 62.718378021575546, 'ACC-31': 62.006814007408586, 'ACC-32': 62.05704642260882, 'ACC-33': 62.19460276603355, 'ACC-34': 58.70654745920212, 'ACC-35': 58.35880417384285, 'ACC-36': 62.022665650807184, 'ACC-37': 59.61202125656266, 'ACC-38': 57.41584915591345, 'ACC-39': 57.14651154570854, 'ACC-40': 58.00503296215478, 'ACC-41': 56.60110086805178, 'ACC-42': 55.96168511099322, 'ACC-43': 54.330071866524875, 'ACC-44': 55.76270555177143, 'ACC-45': 56.70271659354004, 'ACC-46': 54.182575568172986, 'ACC-47': 52.26713113759025, 'ACC-48': 51.875192686252866, 'ACC-49': 54.7974788130007, 'ACC-50': 49.96017505993037, 'ACC-51': 50.98767632149108, 'ACC-52': 48.04904430878956, 'ACC-53': 45.20672412603712, 'ACC-54': 46.257644607805126, 'ACC-55': 44.94998619752595, 'ACC-56': 44.49149779238828, 'ACC-57': 42.479335553934554, 'ACC-58': 42.09305125115472, 'ACC-59': 40.38149574414972, 'ACC-60': 38.07783261368955, 'ACC-61': 39.307922756291546, 'ACC-62': 37.77997657485363, 'ACC-63': 35.59048820103146, 'ACC-64': 35.656192495724305, 'ACC-65': 36.05611162835988, 'ACC-66': 37.219498419051114, 'ACC-67': 34.500942547334844, 'ACC-68': 35.318012426828965, 'ACC-69': 34.699304152700435, 'ACC-70': 30.166414946490526, 'ACC-71': 31.080759652283536, 'ACC-72': 30.45024910317894, 'ACC-73': 30.968010070854124, 'ACC-74': 32.216542346332865, 'ACC-75': 30.433133517503318, 'ACC-76': 31.11867027613763, 'ACC-77': 30.266369110028602, 'ACC-78': 31.840558089695982, 'ACC-79': 29.550545247065457, 'ACC-80': 32.025489040984354, 'ACC-81': 30.603674726433503, 'ACC-82': 29.86221866180914, 'ACC-83': 27.70528909950047, 'ACC-84': 30.47839646151136, 'ACC-85': 27.3228359763108, 'ACC-86': 25.71996687455706, 'ACC-87': 27.35653719612448, 'ACC-88': 25.598922470536532, 'ACC-89': 25.190304816999955, 'ACC-90': 25.92837556490528, 'ACC-91': 24.82232779122141, 'ACC-92': 28.51819929289155, 'ACC-93': 23.130371300035733, 'ACC-94': 26.234433539272985, 'ACC-95': 25.81191263371212, 'ACC-96': 25.640115969767272, 'ACC-97': 27.91364807458747, 'ACC-98': 23.7577576744671, 'ACC-99': 24.75933794759856, 'ACC-100': 26.704788696759586, 'ACC-101': 21.209089427294554, 'ACC-102': 22.681495466589634, 'ACC-103': 23.194698685999093, 'ACC-104': 24.80464248606782, 'ACC-105': 21.410073534836823, 'ACC-106': 23.07772265001784, 'ACC-107': 22.223306080418258, 'ACC-108': 23.848853830789917, 'ACC-109': 25.115775446070902, 'ACC-110': 21.150587611607936, 'ACC-111': 22.286459978399172, 'ACC-112': 22.423619056931297, 'ACC-113': 21.54014676962028, 'ACC-114': 20.429170112754253, 'ACC-115': 20.3867205394175, 'ACC-116': 24.34749597822982, 'ACC-117': 19.18251476596902, 'ACC-118': 22.41512882063959, 'ACC-119': 21.22385845050819, 'ACC-120': 17.409803061242435, 'ACC-121': 22.130873190735723, 'ACC-122': 14.344845879887918, 'ACC-123': 18.4356100276449, 'ACC-124': 17.439788106238986, 'ACC-125': 15.821418240147972, 'ACC-126': 13.759146278455844, 'ACC-127': 14.327543966702795, 'ACC-128': 16.483835471782747, 'ACC-129': 14.630237434000373, 'ACC-130': 12.465556656515385, 'ACC-131': 15.013104841310174, 'ACC-132': 14.978072230385461, 'ACC-133': 15.605553377366268, 'ACC-134': 12.687882313716683, 'ACC-135': 14.802998707453686, 'ACC-136': 8.945755658172281, 'ACC-137': 11.996613515654118, 'ACC-138': 13.620262903015302, 'ACC-139': 10.687758585679113, 'ACC-140': 7.659419571011508, 'ACC-141': 9.827443691846934, 'ACC-142': 11.772076904562736, 'ACC-143': 8.634823821612997, 'ACC-144': 9.733810305583935, 'ACC-145': 11.824007220216606, 'ACC-146': 7.34578143518415, 'ACC-147': 10.091430399122707, 'ACC-148': 9.485665047637706, 'ACC-149': 3.613132823408751, 'ACC-150': 8.440788626353733, 'ACC-151': 7.010253187118319, 'ACC-152': 4.215039598926976, 'ACC-153': 6.324916548654663, 'ACC-154': 3.516691401454772, 'ACC-155': 6.937805503794324, 'ACC-156': 3.7565664110357613, 'ACC-157': 4.623190935122522, 'ACC-158': 4.164097706241588, 'ACC-159': 4.033725165866677, 'ACC-160': 2.949357362193673, 'ACC-161': 3.8314934624259154, 'ACC-162': 3.994641608416157, 'ACC-163': 2.9160877236306946, 'ACC-164': 4.087182630734965, 'ACC-165': 4.5638462831249464, 'ACC-166': 3.462060115555527, 'ACC-167': 2.1589678589116184, 'ACC-168': 2.433177514083663, 'ACC-169': 2.852872916520149, 'ACC-170': 3.720873918437321, 'ACC-171': 2.371977795760751, 'ACC-172': 2.91031191622475, 'ACC-173': 5.047597881246064, 'ACC-174': 4.785082388533714, 'ACC-175': 1.1228729442422005, 'ACC-176': 4.288020056241784, 'ACC-177': 1.90224489203622, 'ACC-178': 1.9198367133015544, 'ACC-179': 7.076921293926534, 'ACC-180': 1.0891067012118543, 'ACC-181': 3.439203833519591, 'ACC-182': 2.2928554340436924, 'ACC-183': 4.759185858906534, 'ACC-184': 0.3583275848673694, 'ACC-185': 3.1861930064124784, 'ACC-186': 3.25282115950464, 'ACC-187': 2.8089317372224176, 'ACC-188': 3.3962720013529997, 'ACC-189': 3.3627073160889394, 'ACC-190': 3.8267513176439785, 'ACC-191': 6.103996143244914, 'ACC-192': 10.210505709624798, 'ACC-193': 5.064082791174148, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 12:54:54] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 12:54:54] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 12:54:54] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 12:54:54] d2.evaluation.testing INFO: copypaste: 14.3451,41.2598,21.8234,55.5066
[01/17 12:54:54] d2.utils.events INFO:  eta: 1 day, 5:51:02  iter: 19999  total_loss: 34.98  loss_ce: 0.2641  loss_mask: 0.3699  loss_dice: 2.812  loss_ce_0: 0.5633  loss_mask_0: 0.3675  loss_dice_0: 2.935  loss_ce_1: 0.2977  loss_mask_1: 0.3727  loss_dice_1: 2.853  loss_ce_2: 0.2871  loss_mask_2: 0.3688  loss_dice_2: 2.834  loss_ce_3: 0.2563  loss_mask_3: 0.3713  loss_dice_3: 2.825  loss_ce_4: 0.2677  loss_mask_4: 0.3731  loss_dice_4: 2.819  loss_ce_5: 0.2519  loss_mask_5: 0.3717  loss_dice_5: 2.827  loss_ce_6: 0.2651  loss_mask_6: 0.3708  loss_dice_6: 2.817  loss_ce_7: 0.2569  loss_mask_7: 0.3693  loss_dice_7: 2.819  loss_ce_8: 0.262  loss_mask_8: 0.3698  loss_dice_8: 2.814  time: 1.5362  data_time: 0.0970  lr: 7.9758e-06  max_mem: 21410M
[01/17 12:55:25] d2.utils.events INFO:  eta: 1 day, 5:50:18  iter: 20019  total_loss: 36.52  loss_ce: 0.2653  loss_mask: 0.3785  loss_dice: 2.911  loss_ce_0: 0.6041  loss_mask_0: 0.3798  loss_dice_0: 3.018  loss_ce_1: 0.3167  loss_mask_1: 0.3868  loss_dice_1: 2.956  loss_ce_2: 0.2997  loss_mask_2: 0.3826  loss_dice_2: 2.941  loss_ce_3: 0.2767  loss_mask_3: 0.3777  loss_dice_3: 2.928  loss_ce_4: 0.2768  loss_mask_4: 0.3778  loss_dice_4: 2.929  loss_ce_5: 0.2683  loss_mask_5: 0.3779  loss_dice_5: 2.922  loss_ce_6: 0.2694  loss_mask_6: 0.3781  loss_dice_6: 2.926  loss_ce_7: 0.2741  loss_mask_7: 0.3777  loss_dice_7: 2.914  loss_ce_8: 0.252  loss_mask_8: 0.3768  loss_dice_8: 2.919  time: 1.5361  data_time: 0.0934  lr: 7.9738e-06  max_mem: 21410M
[01/17 12:55:56] d2.utils.events INFO:  eta: 1 day, 5:49:18  iter: 20039  total_loss: 35.32  loss_ce: 0.254  loss_mask: 0.3832  loss_dice: 2.837  loss_ce_0: 0.5951  loss_mask_0: 0.3829  loss_dice_0: 2.955  loss_ce_1: 0.3121  loss_mask_1: 0.3864  loss_dice_1: 2.874  loss_ce_2: 0.2888  loss_mask_2: 0.3848  loss_dice_2: 2.846  loss_ce_3: 0.2791  loss_mask_3: 0.3841  loss_dice_3: 2.841  loss_ce_4: 0.2607  loss_mask_4: 0.3836  loss_dice_4: 2.842  loss_ce_5: 0.2455  loss_mask_5: 0.3829  loss_dice_5: 2.849  loss_ce_6: 0.2456  loss_mask_6: 0.3846  loss_dice_6: 2.837  loss_ce_7: 0.2589  loss_mask_7: 0.3825  loss_dice_7: 2.835  loss_ce_8: 0.2434  loss_mask_8: 0.3824  loss_dice_8: 2.835  time: 1.5361  data_time: 0.0989  lr: 7.9717e-06  max_mem: 21410M
[01/17 12:56:27] d2.utils.events INFO:  eta: 1 day, 5:48:47  iter: 20059  total_loss: 35.56  loss_ce: 0.2624  loss_mask: 0.3711  loss_dice: 2.858  loss_ce_0: 0.5929  loss_mask_0: 0.3625  loss_dice_0: 2.986  loss_ce_1: 0.3118  loss_mask_1: 0.377  loss_dice_1: 2.882  loss_ce_2: 0.2891  loss_mask_2: 0.3749  loss_dice_2: 2.864  loss_ce_3: 0.2848  loss_mask_3: 0.3736  loss_dice_3: 2.853  loss_ce_4: 0.2801  loss_mask_4: 0.3717  loss_dice_4: 2.862  loss_ce_5: 0.2629  loss_mask_5: 0.3731  loss_dice_5: 2.863  loss_ce_6: 0.2734  loss_mask_6: 0.3717  loss_dice_6: 2.862  loss_ce_7: 0.2576  loss_mask_7: 0.3717  loss_dice_7: 2.864  loss_ce_8: 0.2572  loss_mask_8: 0.3694  loss_dice_8: 2.855  time: 1.5362  data_time: 0.0883  lr: 7.9697e-06  max_mem: 21410M
[01/17 12:56:58] d2.utils.events INFO:  eta: 1 day, 5:48:32  iter: 20079  total_loss: 35.77  loss_ce: 0.2704  loss_mask: 0.3809  loss_dice: 2.858  loss_ce_0: 0.5802  loss_mask_0: 0.3739  loss_dice_0: 2.978  loss_ce_1: 0.2958  loss_mask_1: 0.3831  loss_dice_1: 2.896  loss_ce_2: 0.3009  loss_mask_2: 0.3804  loss_dice_2: 2.875  loss_ce_3: 0.2784  loss_mask_3: 0.3796  loss_dice_3: 2.862  loss_ce_4: 0.2721  loss_mask_4: 0.3791  loss_dice_4: 2.856  loss_ce_5: 0.2705  loss_mask_5: 0.3819  loss_dice_5: 2.861  loss_ce_6: 0.2535  loss_mask_6: 0.382  loss_dice_6: 2.866  loss_ce_7: 0.2711  loss_mask_7: 0.3825  loss_dice_7: 2.849  loss_ce_8: 0.248  loss_mask_8: 0.3822  loss_dice_8: 2.854  time: 1.5362  data_time: 0.0953  lr: 7.9676e-06  max_mem: 21410M
[01/17 12:57:28] d2.utils.events INFO:  eta: 1 day, 5:48:42  iter: 20099  total_loss: 35.25  loss_ce: 0.2821  loss_mask: 0.3735  loss_dice: 2.802  loss_ce_0: 0.5872  loss_mask_0: 0.3727  loss_dice_0: 2.929  loss_ce_1: 0.3079  loss_mask_1: 0.3759  loss_dice_1: 2.845  loss_ce_2: 0.3067  loss_mask_2: 0.3742  loss_dice_2: 2.818  loss_ce_3: 0.2979  loss_mask_3: 0.374  loss_dice_3: 2.806  loss_ce_4: 0.2865  loss_mask_4: 0.3739  loss_dice_4: 2.81  loss_ce_5: 0.279  loss_mask_5: 0.3733  loss_dice_5: 2.805  loss_ce_6: 0.2917  loss_mask_6: 0.3729  loss_dice_6: 2.803  loss_ce_7: 0.2835  loss_mask_7: 0.3733  loss_dice_7: 2.8  loss_ce_8: 0.2832  loss_mask_8: 0.372  loss_dice_8: 2.804  time: 1.5362  data_time: 0.1052  lr: 7.9656e-06  max_mem: 21410M
[01/17 12:58:00] d2.utils.events INFO:  eta: 1 day, 5:49:02  iter: 20119  total_loss: 35.27  loss_ce: 0.2645  loss_mask: 0.3797  loss_dice: 2.852  loss_ce_0: 0.5451  loss_mask_0: 0.3754  loss_dice_0: 2.974  loss_ce_1: 0.2965  loss_mask_1: 0.3851  loss_dice_1: 2.878  loss_ce_2: 0.2837  loss_mask_2: 0.3781  loss_dice_2: 2.855  loss_ce_3: 0.2772  loss_mask_3: 0.3819  loss_dice_3: 2.848  loss_ce_4: 0.2759  loss_mask_4: 0.3802  loss_dice_4: 2.856  loss_ce_5: 0.2542  loss_mask_5: 0.3807  loss_dice_5: 2.85  loss_ce_6: 0.255  loss_mask_6: 0.383  loss_dice_6: 2.848  loss_ce_7: 0.2585  loss_mask_7: 0.3803  loss_dice_7: 2.855  loss_ce_8: 0.2673  loss_mask_8: 0.3805  loss_dice_8: 2.845  time: 1.5362  data_time: 0.1021  lr: 7.9635e-06  max_mem: 21478M
[01/17 12:58:31] d2.utils.events INFO:  eta: 1 day, 5:48:31  iter: 20139  total_loss: 36.07  loss_ce: 0.275  loss_mask: 0.3773  loss_dice: 2.879  loss_ce_0: 0.5714  loss_mask_0: 0.375  loss_dice_0: 2.993  loss_ce_1: 0.3049  loss_mask_1: 0.3859  loss_dice_1: 2.921  loss_ce_2: 0.2692  loss_mask_2: 0.3786  loss_dice_2: 2.905  loss_ce_3: 0.2718  loss_mask_3: 0.3777  loss_dice_3: 2.897  loss_ce_4: 0.2681  loss_mask_4: 0.378  loss_dice_4: 2.887  loss_ce_5: 0.2566  loss_mask_5: 0.3778  loss_dice_5: 2.887  loss_ce_6: 0.2556  loss_mask_6: 0.3775  loss_dice_6: 2.883  loss_ce_7: 0.2594  loss_mask_7: 0.3783  loss_dice_7: 2.884  loss_ce_8: 0.2618  loss_mask_8: 0.3771  loss_dice_8: 2.881  time: 1.5362  data_time: 0.0992  lr: 7.9615e-06  max_mem: 21478M
[01/17 12:59:02] d2.utils.events INFO:  eta: 1 day, 5:48:01  iter: 20159  total_loss: 35.98  loss_ce: 0.2619  loss_mask: 0.3846  loss_dice: 2.877  loss_ce_0: 0.5872  loss_mask_0: 0.3834  loss_dice_0: 2.981  loss_ce_1: 0.2852  loss_mask_1: 0.3932  loss_dice_1: 2.912  loss_ce_2: 0.2965  loss_mask_2: 0.3919  loss_dice_2: 2.883  loss_ce_3: 0.271  loss_mask_3: 0.3886  loss_dice_3: 2.877  loss_ce_4: 0.2693  loss_mask_4: 0.387  loss_dice_4: 2.888  loss_ce_5: 0.2714  loss_mask_5: 0.3847  loss_dice_5: 2.879  loss_ce_6: 0.2516  loss_mask_6: 0.3851  loss_dice_6: 2.879  loss_ce_7: 0.2693  loss_mask_7: 0.3862  loss_dice_7: 2.865  loss_ce_8: 0.2524  loss_mask_8: 0.3847  loss_dice_8: 2.87  time: 1.5362  data_time: 0.0972  lr: 7.9594e-06  max_mem: 21478M
[01/17 12:59:33] d2.utils.events INFO:  eta: 1 day, 5:47:47  iter: 20179  total_loss: 35.63  loss_ce: 0.2656  loss_mask: 0.3733  loss_dice: 2.853  loss_ce_0: 0.5647  loss_mask_0: 0.3645  loss_dice_0: 2.981  loss_ce_1: 0.2936  loss_mask_1: 0.3785  loss_dice_1: 2.898  loss_ce_2: 0.306  loss_mask_2: 0.3727  loss_dice_2: 2.871  loss_ce_3: 0.2765  loss_mask_3: 0.3722  loss_dice_3: 2.864  loss_ce_4: 0.281  loss_mask_4: 0.3705  loss_dice_4: 2.862  loss_ce_5: 0.2711  loss_mask_5: 0.3715  loss_dice_5: 2.864  loss_ce_6: 0.2786  loss_mask_6: 0.3734  loss_dice_6: 2.862  loss_ce_7: 0.2747  loss_mask_7: 0.3731  loss_dice_7: 2.856  loss_ce_8: 0.2685  loss_mask_8: 0.3727  loss_dice_8: 2.856  time: 1.5363  data_time: 0.1110  lr: 7.9574e-06  max_mem: 21478M
[01/17 13:00:05] d2.utils.events INFO:  eta: 1 day, 5:48:00  iter: 20199  total_loss: 35.97  loss_ce: 0.2715  loss_mask: 0.3742  loss_dice: 2.88  loss_ce_0: 0.5748  loss_mask_0: 0.3683  loss_dice_0: 2.997  loss_ce_1: 0.3118  loss_mask_1: 0.3765  loss_dice_1: 2.91  loss_ce_2: 0.2978  loss_mask_2: 0.3751  loss_dice_2: 2.886  loss_ce_3: 0.2867  loss_mask_3: 0.3736  loss_dice_3: 2.879  loss_ce_4: 0.273  loss_mask_4: 0.3744  loss_dice_4: 2.878  loss_ce_5: 0.2788  loss_mask_5: 0.3748  loss_dice_5: 2.877  loss_ce_6: 0.2763  loss_mask_6: 0.374  loss_dice_6: 2.877  loss_ce_7: 0.2634  loss_mask_7: 0.3748  loss_dice_7: 2.877  loss_ce_8: 0.2527  loss_mask_8: 0.3724  loss_dice_8: 2.88  time: 1.5363  data_time: 0.1019  lr: 7.9553e-06  max_mem: 21478M
[01/17 13:00:36] d2.utils.events INFO:  eta: 1 day, 5:47:17  iter: 20219  total_loss: 35.62  loss_ce: 0.3009  loss_mask: 0.3758  loss_dice: 2.826  loss_ce_0: 0.6122  loss_mask_0: 0.3645  loss_dice_0: 2.952  loss_ce_1: 0.3222  loss_mask_1: 0.3752  loss_dice_1: 2.864  loss_ce_2: 0.3377  loss_mask_2: 0.3751  loss_dice_2: 2.843  loss_ce_3: 0.3009  loss_mask_3: 0.376  loss_dice_3: 2.832  loss_ce_4: 0.3017  loss_mask_4: 0.3754  loss_dice_4: 2.834  loss_ce_5: 0.3067  loss_mask_5: 0.3749  loss_dice_5: 2.835  loss_ce_6: 0.2986  loss_mask_6: 0.3726  loss_dice_6: 2.83  loss_ce_7: 0.2982  loss_mask_7: 0.3746  loss_dice_7: 2.835  loss_ce_8: 0.2865  loss_mask_8: 0.3744  loss_dice_8: 2.831  time: 1.5363  data_time: 0.1031  lr: 7.9533e-06  max_mem: 21478M
[01/17 13:01:07] d2.utils.events INFO:  eta: 1 day, 5:46:46  iter: 20239  total_loss: 35.6  loss_ce: 0.2623  loss_mask: 0.3701  loss_dice: 2.859  loss_ce_0: 0.5686  loss_mask_0: 0.3688  loss_dice_0: 2.985  loss_ce_1: 0.2857  loss_mask_1: 0.3786  loss_dice_1: 2.901  loss_ce_2: 0.2872  loss_mask_2: 0.3728  loss_dice_2: 2.878  loss_ce_3: 0.2735  loss_mask_3: 0.3701  loss_dice_3: 2.852  loss_ce_4: 0.2752  loss_mask_4: 0.3704  loss_dice_4: 2.865  loss_ce_5: 0.2615  loss_mask_5: 0.3697  loss_dice_5: 2.868  loss_ce_6: 0.2585  loss_mask_6: 0.3716  loss_dice_6: 2.862  loss_ce_7: 0.2533  loss_mask_7: 0.3715  loss_dice_7: 2.857  loss_ce_8: 0.2533  loss_mask_8: 0.3724  loss_dice_8: 2.858  time: 1.5363  data_time: 0.0938  lr: 7.9512e-06  max_mem: 21478M
[01/17 13:01:38] d2.utils.events INFO:  eta: 1 day, 5:46:15  iter: 20259  total_loss: 35.8  loss_ce: 0.2685  loss_mask: 0.3782  loss_dice: 2.892  loss_ce_0: 0.5945  loss_mask_0: 0.3736  loss_dice_0: 2.991  loss_ce_1: 0.2869  loss_mask_1: 0.3825  loss_dice_1: 2.921  loss_ce_2: 0.2937  loss_mask_2: 0.3778  loss_dice_2: 2.902  loss_ce_3: 0.2855  loss_mask_3: 0.3777  loss_dice_3: 2.902  loss_ce_4: 0.2938  loss_mask_4: 0.3776  loss_dice_4: 2.884  loss_ce_5: 0.2724  loss_mask_5: 0.3793  loss_dice_5: 2.887  loss_ce_6: 0.2841  loss_mask_6: 0.3785  loss_dice_6: 2.884  loss_ce_7: 0.2636  loss_mask_7: 0.3781  loss_dice_7: 2.891  loss_ce_8: 0.2744  loss_mask_8: 0.378  loss_dice_8: 2.893  time: 1.5364  data_time: 0.1082  lr: 7.9492e-06  max_mem: 21478M
[01/17 13:02:10] d2.utils.events INFO:  eta: 1 day, 5:46:46  iter: 20279  total_loss: 35.86  loss_ce: 0.2837  loss_mask: 0.3796  loss_dice: 2.863  loss_ce_0: 0.5921  loss_mask_0: 0.3819  loss_dice_0: 2.972  loss_ce_1: 0.3231  loss_mask_1: 0.3828  loss_dice_1: 2.899  loss_ce_2: 0.3089  loss_mask_2: 0.3782  loss_dice_2: 2.879  loss_ce_3: 0.2876  loss_mask_3: 0.3787  loss_dice_3: 2.873  loss_ce_4: 0.2994  loss_mask_4: 0.379  loss_dice_4: 2.869  loss_ce_5: 0.2872  loss_mask_5: 0.378  loss_dice_5: 2.872  loss_ce_6: 0.2796  loss_mask_6: 0.3774  loss_dice_6: 2.873  loss_ce_7: 0.2821  loss_mask_7: 0.3785  loss_dice_7: 2.864  loss_ce_8: 0.2812  loss_mask_8: 0.3793  loss_dice_8: 2.866  time: 1.5364  data_time: 0.1002  lr: 7.9471e-06  max_mem: 21478M
[01/17 13:02:41] d2.utils.events INFO:  eta: 1 day, 5:45:00  iter: 20299  total_loss: 35.47  loss_ce: 0.2604  loss_mask: 0.3785  loss_dice: 2.843  loss_ce_0: 0.5794  loss_mask_0: 0.3816  loss_dice_0: 2.944  loss_ce_1: 0.2919  loss_mask_1: 0.3868  loss_dice_1: 2.877  loss_ce_2: 0.2874  loss_mask_2: 0.38  loss_dice_2: 2.86  loss_ce_3: 0.275  loss_mask_3: 0.3816  loss_dice_3: 2.839  loss_ce_4: 0.2792  loss_mask_4: 0.3793  loss_dice_4: 2.838  loss_ce_5: 0.2681  loss_mask_5: 0.3798  loss_dice_5: 2.838  loss_ce_6: 0.2632  loss_mask_6: 0.3801  loss_dice_6: 2.838  loss_ce_7: 0.2666  loss_mask_7: 0.3786  loss_dice_7: 2.842  loss_ce_8: 0.2561  loss_mask_8: 0.3785  loss_dice_8: 2.842  time: 1.5364  data_time: 0.0925  lr: 7.9451e-06  max_mem: 21478M
[01/17 13:03:12] d2.utils.events INFO:  eta: 1 day, 5:44:12  iter: 20319  total_loss: 35.91  loss_ce: 0.2669  loss_mask: 0.3732  loss_dice: 2.891  loss_ce_0: 0.5776  loss_mask_0: 0.3745  loss_dice_0: 3.013  loss_ce_1: 0.2871  loss_mask_1: 0.3708  loss_dice_1: 2.946  loss_ce_2: 0.2981  loss_mask_2: 0.3727  loss_dice_2: 2.915  loss_ce_3: 0.2903  loss_mask_3: 0.3719  loss_dice_3: 2.904  loss_ce_4: 0.2873  loss_mask_4: 0.373  loss_dice_4: 2.907  loss_ce_5: 0.283  loss_mask_5: 0.3747  loss_dice_5: 2.902  loss_ce_6: 0.2729  loss_mask_6: 0.3733  loss_dice_6: 2.901  loss_ce_7: 0.2604  loss_mask_7: 0.3726  loss_dice_7: 2.9  loss_ce_8: 0.2787  loss_mask_8: 0.3719  loss_dice_8: 2.898  time: 1.5364  data_time: 0.1042  lr: 7.943e-06  max_mem: 21478M
[01/17 13:03:43] d2.utils.events INFO:  eta: 1 day, 5:43:44  iter: 20339  total_loss: 35.67  loss_ce: 0.261  loss_mask: 0.3743  loss_dice: 2.867  loss_ce_0: 0.5476  loss_mask_0: 0.3726  loss_dice_0: 2.981  loss_ce_1: 0.2851  loss_mask_1: 0.3758  loss_dice_1: 2.893  loss_ce_2: 0.2816  loss_mask_2: 0.3736  loss_dice_2: 2.892  loss_ce_3: 0.266  loss_mask_3: 0.3741  loss_dice_3: 2.884  loss_ce_4: 0.2783  loss_mask_4: 0.373  loss_dice_4: 2.872  loss_ce_5: 0.2613  loss_mask_5: 0.3758  loss_dice_5: 2.87  loss_ce_6: 0.2574  loss_mask_6: 0.3738  loss_dice_6: 2.863  loss_ce_7: 0.2547  loss_mask_7: 0.3748  loss_dice_7: 2.861  loss_ce_8: 0.2728  loss_mask_8: 0.3752  loss_dice_8: 2.87  time: 1.5364  data_time: 0.1021  lr: 7.941e-06  max_mem: 21478M
[01/17 13:04:14] d2.utils.events INFO:  eta: 1 day, 5:43:26  iter: 20359  total_loss: 35.62  loss_ce: 0.2683  loss_mask: 0.3632  loss_dice: 2.86  loss_ce_0: 0.572  loss_mask_0: 0.3624  loss_dice_0: 3.006  loss_ce_1: 0.3045  loss_mask_1: 0.3665  loss_dice_1: 2.919  loss_ce_2: 0.3033  loss_mask_2: 0.3633  loss_dice_2: 2.887  loss_ce_3: 0.2854  loss_mask_3: 0.3645  loss_dice_3: 2.883  loss_ce_4: 0.2783  loss_mask_4: 0.3635  loss_dice_4: 2.879  loss_ce_5: 0.2718  loss_mask_5: 0.364  loss_dice_5: 2.874  loss_ce_6: 0.2786  loss_mask_6: 0.3638  loss_dice_6: 2.88  loss_ce_7: 0.2745  loss_mask_7: 0.364  loss_dice_7: 2.878  loss_ce_8: 0.2723  loss_mask_8: 0.3644  loss_dice_8: 2.871  time: 1.5364  data_time: 0.0940  lr: 7.9389e-06  max_mem: 21478M
[01/17 13:04:45] d2.utils.events INFO:  eta: 1 day, 5:44:38  iter: 20379  total_loss: 36.9  loss_ce: 0.2848  loss_mask: 0.3744  loss_dice: 2.966  loss_ce_0: 0.5684  loss_mask_0: 0.3719  loss_dice_0: 3.074  loss_ce_1: 0.3019  loss_mask_1: 0.378  loss_dice_1: 3.002  loss_ce_2: 0.31  loss_mask_2: 0.3717  loss_dice_2: 2.978  loss_ce_3: 0.2966  loss_mask_3: 0.3734  loss_dice_3: 2.964  loss_ce_4: 0.2975  loss_mask_4: 0.3736  loss_dice_4: 2.971  loss_ce_5: 0.2925  loss_mask_5: 0.3724  loss_dice_5: 2.972  loss_ce_6: 0.2818  loss_mask_6: 0.372  loss_dice_6: 2.979  loss_ce_7: 0.2743  loss_mask_7: 0.3724  loss_dice_7: 2.974  loss_ce_8: 0.2818  loss_mask_8: 0.3726  loss_dice_8: 2.971  time: 1.5364  data_time: 0.0968  lr: 7.9368e-06  max_mem: 21478M
[01/17 13:05:16] d2.utils.events INFO:  eta: 1 day, 5:44:13  iter: 20399  total_loss: 36.07  loss_ce: 0.2854  loss_mask: 0.3645  loss_dice: 2.912  loss_ce_0: 0.531  loss_mask_0: 0.3706  loss_dice_0: 3.019  loss_ce_1: 0.3126  loss_mask_1: 0.3762  loss_dice_1: 2.957  loss_ce_2: 0.3117  loss_mask_2: 0.3693  loss_dice_2: 2.927  loss_ce_3: 0.3014  loss_mask_3: 0.367  loss_dice_3: 2.918  loss_ce_4: 0.2934  loss_mask_4: 0.3686  loss_dice_4: 2.915  loss_ce_5: 0.2898  loss_mask_5: 0.3696  loss_dice_5: 2.918  loss_ce_6: 0.2856  loss_mask_6: 0.3682  loss_dice_6: 2.912  loss_ce_7: 0.2787  loss_mask_7: 0.3655  loss_dice_7: 2.914  loss_ce_8: 0.2888  loss_mask_8: 0.3666  loss_dice_8: 2.914  time: 1.5365  data_time: 0.1013  lr: 7.9348e-06  max_mem: 21478M
[01/17 13:05:47] d2.utils.events INFO:  eta: 1 day, 5:43:50  iter: 20419  total_loss: 36.1  loss_ce: 0.2693  loss_mask: 0.372  loss_dice: 2.844  loss_ce_0: 0.5589  loss_mask_0: 0.3791  loss_dice_0: 2.938  loss_ce_1: 0.2984  loss_mask_1: 0.3754  loss_dice_1: 2.893  loss_ce_2: 0.3101  loss_mask_2: 0.3718  loss_dice_2: 2.864  loss_ce_3: 0.2915  loss_mask_3: 0.3738  loss_dice_3: 2.852  loss_ce_4: 0.2744  loss_mask_4: 0.3714  loss_dice_4: 2.853  loss_ce_5: 0.2765  loss_mask_5: 0.3719  loss_dice_5: 2.857  loss_ce_6: 0.2781  loss_mask_6: 0.3735  loss_dice_6: 2.855  loss_ce_7: 0.2603  loss_mask_7: 0.3731  loss_dice_7: 2.844  loss_ce_8: 0.2726  loss_mask_8: 0.3707  loss_dice_8: 2.852  time: 1.5365  data_time: 0.0993  lr: 7.9327e-06  max_mem: 21478M
[01/17 13:06:17] d2.utils.events INFO:  eta: 1 day, 5:43:03  iter: 20439  total_loss: 35.53  loss_ce: 0.2668  loss_mask: 0.3627  loss_dice: 2.862  loss_ce_0: 0.5407  loss_mask_0: 0.36  loss_dice_0: 2.986  loss_ce_1: 0.2962  loss_mask_1: 0.3702  loss_dice_1: 2.903  loss_ce_2: 0.2864  loss_mask_2: 0.3644  loss_dice_2: 2.882  loss_ce_3: 0.2854  loss_mask_3: 0.362  loss_dice_3: 2.88  loss_ce_4: 0.277  loss_mask_4: 0.3632  loss_dice_4: 2.87  loss_ce_5: 0.2709  loss_mask_5: 0.3631  loss_dice_5: 2.869  loss_ce_6: 0.265  loss_mask_6: 0.3618  loss_dice_6: 2.875  loss_ce_7: 0.2798  loss_mask_7: 0.3634  loss_dice_7: 2.866  loss_ce_8: 0.2725  loss_mask_8: 0.3636  loss_dice_8: 2.867  time: 1.5365  data_time: 0.0888  lr: 7.9307e-06  max_mem: 21478M
[01/17 13:06:48] d2.utils.events INFO:  eta: 1 day, 5:42:09  iter: 20459  total_loss: 35.68  loss_ce: 0.2514  loss_mask: 0.3811  loss_dice: 2.824  loss_ce_0: 0.5453  loss_mask_0: 0.383  loss_dice_0: 2.938  loss_ce_1: 0.2835  loss_mask_1: 0.397  loss_dice_1: 2.859  loss_ce_2: 0.2806  loss_mask_2: 0.3929  loss_dice_2: 2.844  loss_ce_3: 0.2711  loss_mask_3: 0.3887  loss_dice_3: 2.829  loss_ce_4: 0.2703  loss_mask_4: 0.3849  loss_dice_4: 2.819  loss_ce_5: 0.2531  loss_mask_5: 0.3847  loss_dice_5: 2.83  loss_ce_6: 0.2511  loss_mask_6: 0.3822  loss_dice_6: 2.817  loss_ce_7: 0.2562  loss_mask_7: 0.3839  loss_dice_7: 2.822  loss_ce_8: 0.2423  loss_mask_8: 0.3822  loss_dice_8: 2.82  time: 1.5364  data_time: 0.1018  lr: 7.9286e-06  max_mem: 21478M
[01/17 13:07:19] d2.utils.events INFO:  eta: 1 day, 5:42:01  iter: 20479  total_loss: 36.39  loss_ce: 0.2812  loss_mask: 0.3654  loss_dice: 2.885  loss_ce_0: 0.5893  loss_mask_0: 0.3723  loss_dice_0: 3.002  loss_ce_1: 0.314  loss_mask_1: 0.3688  loss_dice_1: 2.934  loss_ce_2: 0.3306  loss_mask_2: 0.3661  loss_dice_2: 2.913  loss_ce_3: 0.2937  loss_mask_3: 0.3643  loss_dice_3: 2.9  loss_ce_4: 0.2941  loss_mask_4: 0.3644  loss_dice_4: 2.895  loss_ce_5: 0.3051  loss_mask_5: 0.3652  loss_dice_5: 2.896  loss_ce_6: 0.2975  loss_mask_6: 0.366  loss_dice_6: 2.895  loss_ce_7: 0.3005  loss_mask_7: 0.3652  loss_dice_7: 2.896  loss_ce_8: 0.2948  loss_mask_8: 0.3654  loss_dice_8: 2.893  time: 1.5365  data_time: 0.0991  lr: 7.9266e-06  max_mem: 21478M
[01/17 13:07:50] d2.utils.events INFO:  eta: 1 day, 5:41:47  iter: 20499  total_loss: 35.46  loss_ce: 0.2765  loss_mask: 0.3611  loss_dice: 2.845  loss_ce_0: 0.5847  loss_mask_0: 0.3584  loss_dice_0: 2.961  loss_ce_1: 0.3295  loss_mask_1: 0.3644  loss_dice_1: 2.874  loss_ce_2: 0.3341  loss_mask_2: 0.3627  loss_dice_2: 2.86  loss_ce_3: 0.302  loss_mask_3: 0.3619  loss_dice_3: 2.841  loss_ce_4: 0.3029  loss_mask_4: 0.3619  loss_dice_4: 2.843  loss_ce_5: 0.2846  loss_mask_5: 0.3611  loss_dice_5: 2.843  loss_ce_6: 0.2969  loss_mask_6: 0.3611  loss_dice_6: 2.843  loss_ce_7: 0.2792  loss_mask_7: 0.3607  loss_dice_7: 2.845  loss_ce_8: 0.2955  loss_mask_8: 0.3605  loss_dice_8: 2.841  time: 1.5365  data_time: 0.1006  lr: 7.9245e-06  max_mem: 21478M
[01/17 13:08:20] d2.utils.events INFO:  eta: 1 day, 5:41:07  iter: 20519  total_loss: 34.54  loss_ce: 0.2653  loss_mask: 0.3851  loss_dice: 2.785  loss_ce_0: 0.5443  loss_mask_0: 0.3781  loss_dice_0: 2.889  loss_ce_1: 0.304  loss_mask_1: 0.3907  loss_dice_1: 2.813  loss_ce_2: 0.2879  loss_mask_2: 0.3895  loss_dice_2: 2.79  loss_ce_3: 0.2815  loss_mask_3: 0.3878  loss_dice_3: 2.786  loss_ce_4: 0.2875  loss_mask_4: 0.3873  loss_dice_4: 2.777  loss_ce_5: 0.2634  loss_mask_5: 0.3861  loss_dice_5: 2.786  loss_ce_6: 0.2658  loss_mask_6: 0.3864  loss_dice_6: 2.78  loss_ce_7: 0.2838  loss_mask_7: 0.385  loss_dice_7: 2.783  loss_ce_8: 0.2755  loss_mask_8: 0.3859  loss_dice_8: 2.784  time: 1.5364  data_time: 0.0965  lr: 7.9225e-06  max_mem: 21478M
[01/17 13:08:51] d2.utils.events INFO:  eta: 1 day, 5:40:45  iter: 20539  total_loss: 35.95  loss_ce: 0.2906  loss_mask: 0.3817  loss_dice: 2.844  loss_ce_0: 0.5738  loss_mask_0: 0.3816  loss_dice_0: 2.969  loss_ce_1: 0.3058  loss_mask_1: 0.3904  loss_dice_1: 2.885  loss_ce_2: 0.3084  loss_mask_2: 0.3835  loss_dice_2: 2.861  loss_ce_3: 0.2999  loss_mask_3: 0.3822  loss_dice_3: 2.854  loss_ce_4: 0.2847  loss_mask_4: 0.3825  loss_dice_4: 2.844  loss_ce_5: 0.2941  loss_mask_5: 0.3821  loss_dice_5: 2.856  loss_ce_6: 0.2985  loss_mask_6: 0.3819  loss_dice_6: 2.851  loss_ce_7: 0.2939  loss_mask_7: 0.3822  loss_dice_7: 2.856  loss_ce_8: 0.2926  loss_mask_8: 0.3817  loss_dice_8: 2.847  time: 1.5364  data_time: 0.0892  lr: 7.9204e-06  max_mem: 21478M
[01/17 13:09:21] d2.utils.events INFO:  eta: 1 day, 5:40:17  iter: 20559  total_loss: 35.42  loss_ce: 0.2703  loss_mask: 0.3755  loss_dice: 2.831  loss_ce_0: 0.5776  loss_mask_0: 0.375  loss_dice_0: 2.949  loss_ce_1: 0.3075  loss_mask_1: 0.3833  loss_dice_1: 2.866  loss_ce_2: 0.3017  loss_mask_2: 0.3798  loss_dice_2: 2.845  loss_ce_3: 0.287  loss_mask_3: 0.3778  loss_dice_3: 2.837  loss_ce_4: 0.2743  loss_mask_4: 0.3774  loss_dice_4: 2.835  loss_ce_5: 0.2803  loss_mask_5: 0.3748  loss_dice_5: 2.833  loss_ce_6: 0.2721  loss_mask_6: 0.3755  loss_dice_6: 2.819  loss_ce_7: 0.2791  loss_mask_7: 0.3758  loss_dice_7: 2.825  loss_ce_8: 0.277  loss_mask_8: 0.3746  loss_dice_8: 2.83  time: 1.5364  data_time: 0.1059  lr: 7.9184e-06  max_mem: 21478M
[01/17 13:09:52] d2.utils.events INFO:  eta: 1 day, 5:38:58  iter: 20579  total_loss: 35.3  loss_ce: 0.26  loss_mask: 0.3798  loss_dice: 2.81  loss_ce_0: 0.5757  loss_mask_0: 0.3723  loss_dice_0: 2.934  loss_ce_1: 0.3149  loss_mask_1: 0.385  loss_dice_1: 2.865  loss_ce_2: 0.3011  loss_mask_2: 0.3828  loss_dice_2: 2.836  loss_ce_3: 0.2857  loss_mask_3: 0.3808  loss_dice_3: 2.815  loss_ce_4: 0.2743  loss_mask_4: 0.3815  loss_dice_4: 2.826  loss_ce_5: 0.2683  loss_mask_5: 0.3807  loss_dice_5: 2.821  loss_ce_6: 0.2778  loss_mask_6: 0.3799  loss_dice_6: 2.824  loss_ce_7: 0.2744  loss_mask_7: 0.3799  loss_dice_7: 2.815  loss_ce_8: 0.2715  loss_mask_8: 0.379  loss_dice_8: 2.812  time: 1.5364  data_time: 0.1062  lr: 7.9163e-06  max_mem: 21478M
[01/17 13:10:22] d2.utils.events INFO:  eta: 1 day, 5:38:04  iter: 20599  total_loss: 36.31  loss_ce: 0.296  loss_mask: 0.3727  loss_dice: 2.897  loss_ce_0: 0.5777  loss_mask_0: 0.3825  loss_dice_0: 3.016  loss_ce_1: 0.3426  loss_mask_1: 0.3837  loss_dice_1: 2.938  loss_ce_2: 0.3253  loss_mask_2: 0.3789  loss_dice_2: 2.903  loss_ce_3: 0.3062  loss_mask_3: 0.3772  loss_dice_3: 2.885  loss_ce_4: 0.3086  loss_mask_4: 0.377  loss_dice_4: 2.901  loss_ce_5: 0.2873  loss_mask_5: 0.3751  loss_dice_5: 2.899  loss_ce_6: 0.2905  loss_mask_6: 0.3735  loss_dice_6: 2.895  loss_ce_7: 0.2937  loss_mask_7: 0.3721  loss_dice_7: 2.881  loss_ce_8: 0.3108  loss_mask_8: 0.3726  loss_dice_8: 2.887  time: 1.5364  data_time: 0.0960  lr: 7.9143e-06  max_mem: 21478M
[01/17 13:10:53] d2.utils.events INFO:  eta: 1 day, 5:37:33  iter: 20619  total_loss: 36.04  loss_ce: 0.2819  loss_mask: 0.3836  loss_dice: 2.872  loss_ce_0: 0.595  loss_mask_0: 0.3798  loss_dice_0: 2.981  loss_ce_1: 0.3152  loss_mask_1: 0.3886  loss_dice_1: 2.903  loss_ce_2: 0.2937  loss_mask_2: 0.3831  loss_dice_2: 2.884  loss_ce_3: 0.2753  loss_mask_3: 0.3815  loss_dice_3: 2.88  loss_ce_4: 0.272  loss_mask_4: 0.38  loss_dice_4: 2.88  loss_ce_5: 0.2867  loss_mask_5: 0.38  loss_dice_5: 2.876  loss_ce_6: 0.2774  loss_mask_6: 0.3821  loss_dice_6: 2.874  loss_ce_7: 0.2778  loss_mask_7: 0.3824  loss_dice_7: 2.873  loss_ce_8: 0.2678  loss_mask_8: 0.3817  loss_dice_8: 2.873  time: 1.5364  data_time: 0.1110  lr: 7.9122e-06  max_mem: 21478M
[01/17 13:11:24] d2.utils.events INFO:  eta: 1 day, 5:37:46  iter: 20639  total_loss: 36.11  loss_ce: 0.2775  loss_mask: 0.369  loss_dice: 2.884  loss_ce_0: 0.5722  loss_mask_0: 0.3656  loss_dice_0: 2.99  loss_ce_1: 0.3073  loss_mask_1: 0.3698  loss_dice_1: 2.921  loss_ce_2: 0.3149  loss_mask_2: 0.3659  loss_dice_2: 2.898  loss_ce_3: 0.2987  loss_mask_3: 0.3673  loss_dice_3: 2.882  loss_ce_4: 0.2901  loss_mask_4: 0.3685  loss_dice_4: 2.88  loss_ce_5: 0.2779  loss_mask_5: 0.3668  loss_dice_5: 2.891  loss_ce_6: 0.29  loss_mask_6: 0.3698  loss_dice_6: 2.883  loss_ce_7: 0.2849  loss_mask_7: 0.3687  loss_dice_7: 2.886  loss_ce_8: 0.2807  loss_mask_8: 0.3673  loss_dice_8: 2.885  time: 1.5364  data_time: 0.1032  lr: 7.9102e-06  max_mem: 21478M
[01/17 13:11:54] d2.utils.events INFO:  eta: 1 day, 5:36:32  iter: 20659  total_loss: 35.28  loss_ce: 0.2631  loss_mask: 0.3765  loss_dice: 2.858  loss_ce_0: 0.5668  loss_mask_0: 0.3712  loss_dice_0: 2.963  loss_ce_1: 0.3086  loss_mask_1: 0.3807  loss_dice_1: 2.888  loss_ce_2: 0.2971  loss_mask_2: 0.3771  loss_dice_2: 2.862  loss_ce_3: 0.2812  loss_mask_3: 0.3772  loss_dice_3: 2.858  loss_ce_4: 0.2846  loss_mask_4: 0.3759  loss_dice_4: 2.848  loss_ce_5: 0.2791  loss_mask_5: 0.3791  loss_dice_5: 2.852  loss_ce_6: 0.2632  loss_mask_6: 0.3755  loss_dice_6: 2.866  loss_ce_7: 0.2662  loss_mask_7: 0.3751  loss_dice_7: 2.851  loss_ce_8: 0.2527  loss_mask_8: 0.3763  loss_dice_8: 2.853  time: 1.5364  data_time: 0.0983  lr: 7.9081e-06  max_mem: 21478M
[01/17 13:12:25] d2.utils.events INFO:  eta: 1 day, 5:36:01  iter: 20679  total_loss: 36.5  loss_ce: 0.286  loss_mask: 0.3666  loss_dice: 2.928  loss_ce_0: 0.5797  loss_mask_0: 0.3707  loss_dice_0: 3.038  loss_ce_1: 0.3208  loss_mask_1: 0.3694  loss_dice_1: 2.966  loss_ce_2: 0.3293  loss_mask_2: 0.3675  loss_dice_2: 2.943  loss_ce_3: 0.3266  loss_mask_3: 0.3693  loss_dice_3: 2.936  loss_ce_4: 0.284  loss_mask_4: 0.3691  loss_dice_4: 2.933  loss_ce_5: 0.3167  loss_mask_5: 0.3681  loss_dice_5: 2.945  loss_ce_6: 0.2914  loss_mask_6: 0.3679  loss_dice_6: 2.929  loss_ce_7: 0.2956  loss_mask_7: 0.3687  loss_dice_7: 2.926  loss_ce_8: 0.2861  loss_mask_8: 0.3687  loss_dice_8: 2.927  time: 1.5364  data_time: 0.1001  lr: 7.9061e-06  max_mem: 21478M
[01/17 13:12:55] d2.utils.events INFO:  eta: 1 day, 5:35:11  iter: 20699  total_loss: 35.45  loss_ce: 0.2685  loss_mask: 0.3716  loss_dice: 2.832  loss_ce_0: 0.5564  loss_mask_0: 0.3659  loss_dice_0: 2.955  loss_ce_1: 0.3027  loss_mask_1: 0.3793  loss_dice_1: 2.879  loss_ce_2: 0.2932  loss_mask_2: 0.3716  loss_dice_2: 2.857  loss_ce_3: 0.2794  loss_mask_3: 0.3709  loss_dice_3: 2.846  loss_ce_4: 0.2841  loss_mask_4: 0.3707  loss_dice_4: 2.849  loss_ce_5: 0.2917  loss_mask_5: 0.3705  loss_dice_5: 2.836  loss_ce_6: 0.2655  loss_mask_6: 0.3701  loss_dice_6: 2.846  loss_ce_7: 0.2617  loss_mask_7: 0.3718  loss_dice_7: 2.844  loss_ce_8: 0.267  loss_mask_8: 0.3711  loss_dice_8: 2.844  time: 1.5363  data_time: 0.0908  lr: 7.904e-06  max_mem: 21478M
[01/17 13:13:26] d2.utils.events INFO:  eta: 1 day, 5:34:40  iter: 20719  total_loss: 35.67  loss_ce: 0.2958  loss_mask: 0.3817  loss_dice: 2.832  loss_ce_0: 0.5981  loss_mask_0: 0.3745  loss_dice_0: 2.957  loss_ce_1: 0.3394  loss_mask_1: 0.3864  loss_dice_1: 2.869  loss_ce_2: 0.3291  loss_mask_2: 0.3865  loss_dice_2: 2.841  loss_ce_3: 0.3066  loss_mask_3: 0.3846  loss_dice_3: 2.837  loss_ce_4: 0.2914  loss_mask_4: 0.385  loss_dice_4: 2.835  loss_ce_5: 0.284  loss_mask_5: 0.3823  loss_dice_5: 2.837  loss_ce_6: 0.2808  loss_mask_6: 0.3819  loss_dice_6: 2.834  loss_ce_7: 0.2846  loss_mask_7: 0.3798  loss_dice_7: 2.822  loss_ce_8: 0.2801  loss_mask_8: 0.3798  loss_dice_8: 2.825  time: 1.5363  data_time: 0.0943  lr: 7.902e-06  max_mem: 21478M
[01/17 13:13:57] d2.utils.events INFO:  eta: 1 day, 5:33:42  iter: 20739  total_loss: 36.33  loss_ce: 0.2871  loss_mask: 0.3897  loss_dice: 2.924  loss_ce_0: 0.5735  loss_mask_0: 0.394  loss_dice_0: 3.012  loss_ce_1: 0.3062  loss_mask_1: 0.3957  loss_dice_1: 2.946  loss_ce_2: 0.2993  loss_mask_2: 0.3887  loss_dice_2: 2.941  loss_ce_3: 0.2873  loss_mask_3: 0.3894  loss_dice_3: 2.931  loss_ce_4: 0.2986  loss_mask_4: 0.3892  loss_dice_4: 2.931  loss_ce_5: 0.287  loss_mask_5: 0.3906  loss_dice_5: 2.927  loss_ce_6: 0.2768  loss_mask_6: 0.388  loss_dice_6: 2.926  loss_ce_7: 0.2806  loss_mask_7: 0.388  loss_dice_7: 2.928  loss_ce_8: 0.286  loss_mask_8: 0.3894  loss_dice_8: 2.92  time: 1.5363  data_time: 0.0880  lr: 7.8999e-06  max_mem: 21478M
[01/17 13:14:27] d2.utils.events INFO:  eta: 1 day, 5:33:19  iter: 20759  total_loss: 35.57  loss_ce: 0.2804  loss_mask: 0.3778  loss_dice: 2.847  loss_ce_0: 0.5585  loss_mask_0: 0.3751  loss_dice_0: 2.985  loss_ce_1: 0.3017  loss_mask_1: 0.3817  loss_dice_1: 2.887  loss_ce_2: 0.3098  loss_mask_2: 0.3812  loss_dice_2: 2.865  loss_ce_3: 0.3038  loss_mask_3: 0.378  loss_dice_3: 2.862  loss_ce_4: 0.2793  loss_mask_4: 0.3769  loss_dice_4: 2.86  loss_ce_5: 0.2621  loss_mask_5: 0.3751  loss_dice_5: 2.86  loss_ce_6: 0.2747  loss_mask_6: 0.3751  loss_dice_6: 2.851  loss_ce_7: 0.2637  loss_mask_7: 0.3749  loss_dice_7: 2.851  loss_ce_8: 0.2784  loss_mask_8: 0.3774  loss_dice_8: 2.852  time: 1.5363  data_time: 0.1018  lr: 7.8978e-06  max_mem: 21478M
[01/17 13:14:58] d2.utils.events INFO:  eta: 1 day, 5:31:24  iter: 20779  total_loss: 35.39  loss_ce: 0.2555  loss_mask: 0.3854  loss_dice: 2.833  loss_ce_0: 0.5168  loss_mask_0: 0.3828  loss_dice_0: 2.937  loss_ce_1: 0.2903  loss_mask_1: 0.3891  loss_dice_1: 2.87  loss_ce_2: 0.3008  loss_mask_2: 0.3852  loss_dice_2: 2.841  loss_ce_3: 0.2693  loss_mask_3: 0.3843  loss_dice_3: 2.848  loss_ce_4: 0.274  loss_mask_4: 0.3875  loss_dice_4: 2.842  loss_ce_5: 0.2624  loss_mask_5: 0.3864  loss_dice_5: 2.841  loss_ce_6: 0.269  loss_mask_6: 0.3878  loss_dice_6: 2.835  loss_ce_7: 0.27  loss_mask_7: 0.3878  loss_dice_7: 2.84  loss_ce_8: 0.2744  loss_mask_8: 0.3865  loss_dice_8: 2.829  time: 1.5363  data_time: 0.1054  lr: 7.8958e-06  max_mem: 21478M
[01/17 13:15:29] d2.utils.events INFO:  eta: 1 day, 5:31:32  iter: 20799  total_loss: 35.68  loss_ce: 0.264  loss_mask: 0.376  loss_dice: 2.856  loss_ce_0: 0.5769  loss_mask_0: 0.3752  loss_dice_0: 2.956  loss_ce_1: 0.2948  loss_mask_1: 0.3818  loss_dice_1: 2.891  loss_ce_2: 0.2993  loss_mask_2: 0.3771  loss_dice_2: 2.872  loss_ce_3: 0.2507  loss_mask_3: 0.3747  loss_dice_3: 2.868  loss_ce_4: 0.2749  loss_mask_4: 0.3779  loss_dice_4: 2.864  loss_ce_5: 0.2797  loss_mask_5: 0.3772  loss_dice_5: 2.87  loss_ce_6: 0.2685  loss_mask_6: 0.3756  loss_dice_6: 2.866  loss_ce_7: 0.2486  loss_mask_7: 0.3752  loss_dice_7: 2.868  loss_ce_8: 0.2559  loss_mask_8: 0.376  loss_dice_8: 2.866  time: 1.5363  data_time: 0.0932  lr: 7.8937e-06  max_mem: 21478M
[01/17 13:15:59] d2.utils.events INFO:  eta: 1 day, 5:30:19  iter: 20819  total_loss: 35.3  loss_ce: 0.2489  loss_mask: 0.3761  loss_dice: 2.783  loss_ce_0: 0.5673  loss_mask_0: 0.3661  loss_dice_0: 2.895  loss_ce_1: 0.2848  loss_mask_1: 0.3775  loss_dice_1: 2.818  loss_ce_2: 0.2721  loss_mask_2: 0.3736  loss_dice_2: 2.804  loss_ce_3: 0.2608  loss_mask_3: 0.3756  loss_dice_3: 2.793  loss_ce_4: 0.251  loss_mask_4: 0.375  loss_dice_4: 2.794  loss_ce_5: 0.2477  loss_mask_5: 0.3762  loss_dice_5: 2.789  loss_ce_6: 0.2429  loss_mask_6: 0.3763  loss_dice_6: 2.787  loss_ce_7: 0.2317  loss_mask_7: 0.3744  loss_dice_7: 2.787  loss_ce_8: 0.2464  loss_mask_8: 0.3749  loss_dice_8: 2.79  time: 1.5363  data_time: 0.0933  lr: 7.8917e-06  max_mem: 21478M
[01/17 13:16:29] d2.utils.events INFO:  eta: 1 day, 5:29:52  iter: 20839  total_loss: 35.37  loss_ce: 0.2776  loss_mask: 0.3788  loss_dice: 2.839  loss_ce_0: 0.5735  loss_mask_0: 0.3745  loss_dice_0: 2.929  loss_ce_1: 0.3062  loss_mask_1: 0.3829  loss_dice_1: 2.862  loss_ce_2: 0.304  loss_mask_2: 0.3792  loss_dice_2: 2.847  loss_ce_3: 0.2746  loss_mask_3: 0.3774  loss_dice_3: 2.837  loss_ce_4: 0.2968  loss_mask_4: 0.3759  loss_dice_4: 2.841  loss_ce_5: 0.2719  loss_mask_5: 0.3761  loss_dice_5: 2.837  loss_ce_6: 0.2725  loss_mask_6: 0.3769  loss_dice_6: 2.83  loss_ce_7: 0.2853  loss_mask_7: 0.3776  loss_dice_7: 2.841  loss_ce_8: 0.2773  loss_mask_8: 0.3772  loss_dice_8: 2.838  time: 1.5363  data_time: 0.0965  lr: 7.8896e-06  max_mem: 21478M
[01/17 13:17:00] d2.utils.events INFO:  eta: 1 day, 5:30:19  iter: 20859  total_loss: 35.98  loss_ce: 0.287  loss_mask: 0.364  loss_dice: 2.904  loss_ce_0: 0.5933  loss_mask_0: 0.3682  loss_dice_0: 2.996  loss_ce_1: 0.301  loss_mask_1: 0.3684  loss_dice_1: 2.936  loss_ce_2: 0.3035  loss_mask_2: 0.3692  loss_dice_2: 2.902  loss_ce_3: 0.2897  loss_mask_3: 0.367  loss_dice_3: 2.901  loss_ce_4: 0.3146  loss_mask_4: 0.3648  loss_dice_4: 2.897  loss_ce_5: 0.2835  loss_mask_5: 0.3641  loss_dice_5: 2.901  loss_ce_6: 0.2971  loss_mask_6: 0.3642  loss_dice_6: 2.891  loss_ce_7: 0.2851  loss_mask_7: 0.3647  loss_dice_7: 2.895  loss_ce_8: 0.2853  loss_mask_8: 0.3633  loss_dice_8: 2.888  time: 1.5363  data_time: 0.1050  lr: 7.8876e-06  max_mem: 21478M
[01/17 13:17:31] d2.utils.events INFO:  eta: 1 day, 5:29:21  iter: 20879  total_loss: 35.41  loss_ce: 0.2904  loss_mask: 0.3811  loss_dice: 2.837  loss_ce_0: 0.5928  loss_mask_0: 0.3794  loss_dice_0: 2.914  loss_ce_1: 0.3506  loss_mask_1: 0.3878  loss_dice_1: 2.853  loss_ce_2: 0.3402  loss_mask_2: 0.3819  loss_dice_2: 2.846  loss_ce_3: 0.3179  loss_mask_3: 0.3807  loss_dice_3: 2.837  loss_ce_4: 0.3122  loss_mask_4: 0.3816  loss_dice_4: 2.835  loss_ce_5: 0.3067  loss_mask_5: 0.3812  loss_dice_5: 2.85  loss_ce_6: 0.2862  loss_mask_6: 0.3792  loss_dice_6: 2.834  loss_ce_7: 0.3001  loss_mask_7: 0.3812  loss_dice_7: 2.828  loss_ce_8: 0.2959  loss_mask_8: 0.3806  loss_dice_8: 2.83  time: 1.5363  data_time: 0.1047  lr: 7.8855e-06  max_mem: 21478M
[01/17 13:18:02] d2.utils.events INFO:  eta: 1 day, 5:28:50  iter: 20899  total_loss: 36.61  loss_ce: 0.2671  loss_mask: 0.3604  loss_dice: 2.959  loss_ce_0: 0.6021  loss_mask_0: 0.3642  loss_dice_0: 3.072  loss_ce_1: 0.324  loss_mask_1: 0.3636  loss_dice_1: 3.001  loss_ce_2: 0.3033  loss_mask_2: 0.3619  loss_dice_2: 2.973  loss_ce_3: 0.2894  loss_mask_3: 0.3612  loss_dice_3: 2.965  loss_ce_4: 0.2922  loss_mask_4: 0.3613  loss_dice_4: 2.959  loss_ce_5: 0.2839  loss_mask_5: 0.3619  loss_dice_5: 2.953  loss_ce_6: 0.2719  loss_mask_6: 0.3611  loss_dice_6: 2.966  loss_ce_7: 0.2698  loss_mask_7: 0.3603  loss_dice_7: 2.959  loss_ce_8: 0.2662  loss_mask_8: 0.3613  loss_dice_8: 2.959  time: 1.5363  data_time: 0.0996  lr: 7.8835e-06  max_mem: 21478M
[01/17 13:18:32] d2.utils.events INFO:  eta: 1 day, 5:28:28  iter: 20919  total_loss: 36.17  loss_ce: 0.319  loss_mask: 0.3898  loss_dice: 2.904  loss_ce_0: 0.5826  loss_mask_0: 0.3877  loss_dice_0: 2.989  loss_ce_1: 0.3229  loss_mask_1: 0.3972  loss_dice_1: 2.934  loss_ce_2: 0.3245  loss_mask_2: 0.3883  loss_dice_2: 2.915  loss_ce_3: 0.3076  loss_mask_3: 0.386  loss_dice_3: 2.91  loss_ce_4: 0.3118  loss_mask_4: 0.3853  loss_dice_4: 2.906  loss_ce_5: 0.2959  loss_mask_5: 0.3843  loss_dice_5: 2.912  loss_ce_6: 0.2932  loss_mask_6: 0.3881  loss_dice_6: 2.901  loss_ce_7: 0.2977  loss_mask_7: 0.3895  loss_dice_7: 2.904  loss_ce_8: 0.3085  loss_mask_8: 0.3896  loss_dice_8: 2.907  time: 1.5363  data_time: 0.0947  lr: 7.8814e-06  max_mem: 21478M
[01/17 13:19:03] d2.utils.events INFO:  eta: 1 day, 5:28:19  iter: 20939  total_loss: 36.36  loss_ce: 0.2734  loss_mask: 0.3834  loss_dice: 2.868  loss_ce_0: 0.5723  loss_mask_0: 0.3916  loss_dice_0: 2.962  loss_ce_1: 0.3122  loss_mask_1: 0.3928  loss_dice_1: 2.884  loss_ce_2: 0.307  loss_mask_2: 0.387  loss_dice_2: 2.875  loss_ce_3: 0.2845  loss_mask_3: 0.3864  loss_dice_3: 2.868  loss_ce_4: 0.2946  loss_mask_4: 0.3865  loss_dice_4: 2.866  loss_ce_5: 0.2743  loss_mask_5: 0.3857  loss_dice_5: 2.867  loss_ce_6: 0.2727  loss_mask_6: 0.3862  loss_dice_6: 2.869  loss_ce_7: 0.2698  loss_mask_7: 0.3866  loss_dice_7: 2.865  loss_ce_8: 0.271  loss_mask_8: 0.3841  loss_dice_8: 2.866  time: 1.5363  data_time: 0.0972  lr: 7.8794e-06  max_mem: 21478M
[01/17 13:19:34] d2.utils.events INFO:  eta: 1 day, 5:27:18  iter: 20959  total_loss: 35.07  loss_ce: 0.2553  loss_mask: 0.3706  loss_dice: 2.82  loss_ce_0: 0.5289  loss_mask_0: 0.3747  loss_dice_0: 2.935  loss_ce_1: 0.2973  loss_mask_1: 0.38  loss_dice_1: 2.855  loss_ce_2: 0.2851  loss_mask_2: 0.3813  loss_dice_2: 2.837  loss_ce_3: 0.2805  loss_mask_3: 0.3785  loss_dice_3: 2.827  loss_ce_4: 0.2706  loss_mask_4: 0.3771  loss_dice_4: 2.825  loss_ce_5: 0.2574  loss_mask_5: 0.3763  loss_dice_5: 2.832  loss_ce_6: 0.2533  loss_mask_6: 0.3716  loss_dice_6: 2.821  loss_ce_7: 0.251  loss_mask_7: 0.3705  loss_dice_7: 2.828  loss_ce_8: 0.2581  loss_mask_8: 0.3692  loss_dice_8: 2.831  time: 1.5363  data_time: 0.1059  lr: 7.8773e-06  max_mem: 21478M
[01/17 13:20:05] d2.utils.events INFO:  eta: 1 day, 5:28:35  iter: 20979  total_loss: 35.5  loss_ce: 0.2768  loss_mask: 0.3758  loss_dice: 2.864  loss_ce_0: 0.5626  loss_mask_0: 0.3681  loss_dice_0: 2.968  loss_ce_1: 0.2738  loss_mask_1: 0.3706  loss_dice_1: 2.907  loss_ce_2: 0.2898  loss_mask_2: 0.3657  loss_dice_2: 2.888  loss_ce_3: 0.2826  loss_mask_3: 0.3659  loss_dice_3: 2.879  loss_ce_4: 0.2597  loss_mask_4: 0.3686  loss_dice_4: 2.872  loss_ce_5: 0.2568  loss_mask_5: 0.3714  loss_dice_5: 2.881  loss_ce_6: 0.2572  loss_mask_6: 0.3757  loss_dice_6: 2.863  loss_ce_7: 0.2541  loss_mask_7: 0.3732  loss_dice_7: 2.871  loss_ce_8: 0.2635  loss_mask_8: 0.3749  loss_dice_8: 2.86  time: 1.5363  data_time: 0.1106  lr: 7.8753e-06  max_mem: 21478M
[01/17 13:20:36] d2.utils.events INFO:  eta: 1 day, 5:28:16  iter: 20999  total_loss: 35.75  loss_ce: 0.2683  loss_mask: 0.3721  loss_dice: 2.874  loss_ce_0: 0.5612  loss_mask_0: 0.3737  loss_dice_0: 2.993  loss_ce_1: 0.3053  loss_mask_1: 0.377  loss_dice_1: 2.917  loss_ce_2: 0.3016  loss_mask_2: 0.3741  loss_dice_2: 2.892  loss_ce_3: 0.2696  loss_mask_3: 0.3728  loss_dice_3: 2.882  loss_ce_4: 0.28  loss_mask_4: 0.373  loss_dice_4: 2.879  loss_ce_5: 0.2731  loss_mask_5: 0.3721  loss_dice_5: 2.882  loss_ce_6: 0.2787  loss_mask_6: 0.3732  loss_dice_6: 2.876  loss_ce_7: 0.2648  loss_mask_7: 0.3731  loss_dice_7: 2.871  loss_ce_8: 0.2582  loss_mask_8: 0.3728  loss_dice_8: 2.879  time: 1.5363  data_time: 0.1026  lr: 7.8732e-06  max_mem: 21478M
[01/17 13:21:07] d2.utils.events INFO:  eta: 1 day, 5:28:32  iter: 21019  total_loss: 35.33  loss_ce: 0.2718  loss_mask: 0.3619  loss_dice: 2.862  loss_ce_0: 0.5948  loss_mask_0: 0.36  loss_dice_0: 2.988  loss_ce_1: 0.3124  loss_mask_1: 0.3638  loss_dice_1: 2.91  loss_ce_2: 0.317  loss_mask_2: 0.362  loss_dice_2: 2.867  loss_ce_3: 0.296  loss_mask_3: 0.3601  loss_dice_3: 2.864  loss_ce_4: 0.2893  loss_mask_4: 0.3583  loss_dice_4: 2.866  loss_ce_5: 0.2849  loss_mask_5: 0.3584  loss_dice_5: 2.869  loss_ce_6: 0.2664  loss_mask_6: 0.3601  loss_dice_6: 2.862  loss_ce_7: 0.2781  loss_mask_7: 0.3603  loss_dice_7: 2.859  loss_ce_8: 0.2767  loss_mask_8: 0.3602  loss_dice_8: 2.861  time: 1.5363  data_time: 0.0990  lr: 7.8712e-06  max_mem: 21478M
[01/17 13:21:37] d2.utils.events INFO:  eta: 1 day, 5:27:47  iter: 21039  total_loss: 35.1  loss_ce: 0.2781  loss_mask: 0.3761  loss_dice: 2.812  loss_ce_0: 0.5585  loss_mask_0: 0.3685  loss_dice_0: 2.918  loss_ce_1: 0.3037  loss_mask_1: 0.3779  loss_dice_1: 2.841  loss_ce_2: 0.3054  loss_mask_2: 0.3744  loss_dice_2: 2.83  loss_ce_3: 0.3077  loss_mask_3: 0.3746  loss_dice_3: 2.812  loss_ce_4: 0.2966  loss_mask_4: 0.3744  loss_dice_4: 2.811  loss_ce_5: 0.2812  loss_mask_5: 0.374  loss_dice_5: 2.815  loss_ce_6: 0.2961  loss_mask_6: 0.3743  loss_dice_6: 2.81  loss_ce_7: 0.2906  loss_mask_7: 0.3761  loss_dice_7: 2.814  loss_ce_8: 0.2899  loss_mask_8: 0.376  loss_dice_8: 2.804  time: 1.5363  data_time: 0.0978  lr: 7.8691e-06  max_mem: 21478M
[01/17 13:22:08] d2.utils.events INFO:  eta: 1 day, 5:27:30  iter: 21059  total_loss: 35.43  loss_ce: 0.2973  loss_mask: 0.376  loss_dice: 2.82  loss_ce_0: 0.575  loss_mask_0: 0.3688  loss_dice_0: 2.948  loss_ce_1: 0.3094  loss_mask_1: 0.3765  loss_dice_1: 2.859  loss_ce_2: 0.319  loss_mask_2: 0.3745  loss_dice_2: 2.84  loss_ce_3: 0.301  loss_mask_3: 0.3732  loss_dice_3: 2.834  loss_ce_4: 0.2972  loss_mask_4: 0.3751  loss_dice_4: 2.831  loss_ce_5: 0.2943  loss_mask_5: 0.3746  loss_dice_5: 2.832  loss_ce_6: 0.2798  loss_mask_6: 0.3748  loss_dice_6: 2.825  loss_ce_7: 0.2949  loss_mask_7: 0.3756  loss_dice_7: 2.823  loss_ce_8: 0.2883  loss_mask_8: 0.3745  loss_dice_8: 2.821  time: 1.5363  data_time: 0.1025  lr: 7.867e-06  max_mem: 21478M
[01/17 13:22:39] d2.utils.events INFO:  eta: 1 day, 5:28:10  iter: 21079  total_loss: 35.95  loss_ce: 0.2835  loss_mask: 0.3756  loss_dice: 2.852  loss_ce_0: 0.586  loss_mask_0: 0.3822  loss_dice_0: 2.968  loss_ce_1: 0.3221  loss_mask_1: 0.3806  loss_dice_1: 2.883  loss_ce_2: 0.3209  loss_mask_2: 0.3752  loss_dice_2: 2.871  loss_ce_3: 0.3027  loss_mask_3: 0.3752  loss_dice_3: 2.854  loss_ce_4: 0.3036  loss_mask_4: 0.3742  loss_dice_4: 2.865  loss_ce_5: 0.2925  loss_mask_5: 0.3732  loss_dice_5: 2.864  loss_ce_6: 0.2837  loss_mask_6: 0.3725  loss_dice_6: 2.862  loss_ce_7: 0.2831  loss_mask_7: 0.3747  loss_dice_7: 2.858  loss_ce_8: 0.2879  loss_mask_8: 0.375  loss_dice_8: 2.851  time: 1.5363  data_time: 0.0916  lr: 7.865e-06  max_mem: 21478M
[01/17 13:23:10] d2.utils.events INFO:  eta: 1 day, 5:27:58  iter: 21099  total_loss: 34.59  loss_ce: 0.2872  loss_mask: 0.3688  loss_dice: 2.78  loss_ce_0: 0.5654  loss_mask_0: 0.3613  loss_dice_0: 2.891  loss_ce_1: 0.3133  loss_mask_1: 0.3741  loss_dice_1: 2.82  loss_ce_2: 0.3029  loss_mask_2: 0.3667  loss_dice_2: 2.792  loss_ce_3: 0.294  loss_mask_3: 0.3668  loss_dice_3: 2.777  loss_ce_4: 0.2912  loss_mask_4: 0.3678  loss_dice_4: 2.776  loss_ce_5: 0.292  loss_mask_5: 0.3679  loss_dice_5: 2.778  loss_ce_6: 0.2788  loss_mask_6: 0.3673  loss_dice_6: 2.777  loss_ce_7: 0.2885  loss_mask_7: 0.3673  loss_dice_7: 2.775  loss_ce_8: 0.279  loss_mask_8: 0.3678  loss_dice_8: 2.772  time: 1.5363  data_time: 0.0997  lr: 7.8629e-06  max_mem: 21478M
[01/17 13:23:41] d2.utils.events INFO:  eta: 1 day, 5:27:53  iter: 21119  total_loss: 35.6  loss_ce: 0.278  loss_mask: 0.3624  loss_dice: 2.84  loss_ce_0: 0.5698  loss_mask_0: 0.3714  loss_dice_0: 2.953  loss_ce_1: 0.2981  loss_mask_1: 0.3696  loss_dice_1: 2.879  loss_ce_2: 0.3009  loss_mask_2: 0.367  loss_dice_2: 2.859  loss_ce_3: 0.2808  loss_mask_3: 0.3652  loss_dice_3: 2.852  loss_ce_4: 0.2847  loss_mask_4: 0.3662  loss_dice_4: 2.849  loss_ce_5: 0.2765  loss_mask_5: 0.3669  loss_dice_5: 2.845  loss_ce_6: 0.279  loss_mask_6: 0.3652  loss_dice_6: 2.837  loss_ce_7: 0.2786  loss_mask_7: 0.3638  loss_dice_7: 2.848  loss_ce_8: 0.2694  loss_mask_8: 0.364  loss_dice_8: 2.838  time: 1.5363  data_time: 0.1098  lr: 7.8609e-06  max_mem: 21478M
[01/17 13:24:11] d2.utils.events INFO:  eta: 1 day, 5:26:23  iter: 21139  total_loss: 34.65  loss_ce: 0.2691  loss_mask: 0.3711  loss_dice: 2.769  loss_ce_0: 0.5437  loss_mask_0: 0.3632  loss_dice_0: 2.891  loss_ce_1: 0.2828  loss_mask_1: 0.3715  loss_dice_1: 2.814  loss_ce_2: 0.285  loss_mask_2: 0.3708  loss_dice_2: 2.807  loss_ce_3: 0.2705  loss_mask_3: 0.3702  loss_dice_3: 2.779  loss_ce_4: 0.273  loss_mask_4: 0.3695  loss_dice_4: 2.791  loss_ce_5: 0.2776  loss_mask_5: 0.3667  loss_dice_5: 2.78  loss_ce_6: 0.26  loss_mask_6: 0.3682  loss_dice_6: 2.771  loss_ce_7: 0.2653  loss_mask_7: 0.3692  loss_dice_7: 2.771  loss_ce_8: 0.2623  loss_mask_8: 0.3694  loss_dice_8: 2.772  time: 1.5363  data_time: 0.0890  lr: 7.8588e-06  max_mem: 21478M
[01/17 13:24:43] d2.utils.events INFO:  eta: 1 day, 5:27:03  iter: 21159  total_loss: 35.25  loss_ce: 0.2564  loss_mask: 0.3603  loss_dice: 2.859  loss_ce_0: 0.5573  loss_mask_0: 0.3571  loss_dice_0: 2.981  loss_ce_1: 0.3015  loss_mask_1: 0.3602  loss_dice_1: 2.891  loss_ce_2: 0.2908  loss_mask_2: 0.3611  loss_dice_2: 2.863  loss_ce_3: 0.2568  loss_mask_3: 0.3593  loss_dice_3: 2.866  loss_ce_4: 0.2572  loss_mask_4: 0.3591  loss_dice_4: 2.859  loss_ce_5: 0.2535  loss_mask_5: 0.3576  loss_dice_5: 2.866  loss_ce_6: 0.2433  loss_mask_6: 0.3613  loss_dice_6: 2.863  loss_ce_7: 0.2591  loss_mask_7: 0.3628  loss_dice_7: 2.865  loss_ce_8: 0.2553  loss_mask_8: 0.3612  loss_dice_8: 2.861  time: 1.5363  data_time: 0.1059  lr: 7.8568e-06  max_mem: 21478M
[01/17 13:25:13] d2.utils.events INFO:  eta: 1 day, 5:26:21  iter: 21179  total_loss: 34.96  loss_ce: 0.2595  loss_mask: 0.3581  loss_dice: 2.817  loss_ce_0: 0.6219  loss_mask_0: 0.3524  loss_dice_0: 2.941  loss_ce_1: 0.3146  loss_mask_1: 0.3606  loss_dice_1: 2.853  loss_ce_2: 0.2946  loss_mask_2: 0.359  loss_dice_2: 2.821  loss_ce_3: 0.2876  loss_mask_3: 0.359  loss_dice_3: 2.828  loss_ce_4: 0.2704  loss_mask_4: 0.3561  loss_dice_4: 2.812  loss_ce_5: 0.2645  loss_mask_5: 0.3584  loss_dice_5: 2.82  loss_ce_6: 0.2633  loss_mask_6: 0.3573  loss_dice_6: 2.818  loss_ce_7: 0.2699  loss_mask_7: 0.3561  loss_dice_7: 2.809  loss_ce_8: 0.2713  loss_mask_8: 0.3556  loss_dice_8: 2.815  time: 1.5363  data_time: 0.0950  lr: 7.8547e-06  max_mem: 21478M
[01/17 13:25:44] d2.utils.events INFO:  eta: 1 day, 5:24:51  iter: 21199  total_loss: 36.19  loss_ce: 0.268  loss_mask: 0.371  loss_dice: 2.889  loss_ce_0: 0.5651  loss_mask_0: 0.3663  loss_dice_0: 3.016  loss_ce_1: 0.294  loss_mask_1: 0.3805  loss_dice_1: 2.937  loss_ce_2: 0.2998  loss_mask_2: 0.3756  loss_dice_2: 2.909  loss_ce_3: 0.2754  loss_mask_3: 0.3741  loss_dice_3: 2.902  loss_ce_4: 0.262  loss_mask_4: 0.3718  loss_dice_4: 2.902  loss_ce_5: 0.2652  loss_mask_5: 0.3723  loss_dice_5: 2.906  loss_ce_6: 0.2603  loss_mask_6: 0.3738  loss_dice_6: 2.89  loss_ce_7: 0.2677  loss_mask_7: 0.37  loss_dice_7: 2.885  loss_ce_8: 0.2621  loss_mask_8: 0.3713  loss_dice_8: 2.892  time: 1.5363  data_time: 0.1018  lr: 7.8527e-06  max_mem: 21478M
[01/17 13:26:15] d2.utils.events INFO:  eta: 1 day, 5:23:59  iter: 21219  total_loss: 35.76  loss_ce: 0.263  loss_mask: 0.3754  loss_dice: 2.905  loss_ce_0: 0.5779  loss_mask_0: 0.3798  loss_dice_0: 3.001  loss_ce_1: 0.2977  loss_mask_1: 0.3838  loss_dice_1: 2.939  loss_ce_2: 0.2856  loss_mask_2: 0.3765  loss_dice_2: 2.918  loss_ce_3: 0.2698  loss_mask_3: 0.3749  loss_dice_3: 2.909  loss_ce_4: 0.276  loss_mask_4: 0.3763  loss_dice_4: 2.912  loss_ce_5: 0.2646  loss_mask_5: 0.3758  loss_dice_5: 2.908  loss_ce_6: 0.2636  loss_mask_6: 0.3768  loss_dice_6: 2.911  loss_ce_7: 0.2591  loss_mask_7: 0.3755  loss_dice_7: 2.898  loss_ce_8: 0.2487  loss_mask_8: 0.3768  loss_dice_8: 2.907  time: 1.5363  data_time: 0.0933  lr: 7.8506e-06  max_mem: 21478M
[01/17 13:26:45] d2.utils.events INFO:  eta: 1 day, 5:20:08  iter: 21239  total_loss: 34.66  loss_ce: 0.2566  loss_mask: 0.361  loss_dice: 2.802  loss_ce_0: 0.563  loss_mask_0: 0.3695  loss_dice_0: 2.92  loss_ce_1: 0.2872  loss_mask_1: 0.3748  loss_dice_1: 2.834  loss_ce_2: 0.2739  loss_mask_2: 0.366  loss_dice_2: 2.808  loss_ce_3: 0.2787  loss_mask_3: 0.3655  loss_dice_3: 2.806  loss_ce_4: 0.2726  loss_mask_4: 0.3619  loss_dice_4: 2.804  loss_ce_5: 0.2597  loss_mask_5: 0.3627  loss_dice_5: 2.797  loss_ce_6: 0.2592  loss_mask_6: 0.3624  loss_dice_6: 2.802  loss_ce_7: 0.2537  loss_mask_7: 0.3613  loss_dice_7: 2.795  loss_ce_8: 0.2531  loss_mask_8: 0.3621  loss_dice_8: 2.804  time: 1.5363  data_time: 0.0813  lr: 7.8486e-06  max_mem: 21478M
[01/17 13:27:16] d2.utils.events INFO:  eta: 1 day, 5:19:09  iter: 21259  total_loss: 35.5  loss_ce: 0.2605  loss_mask: 0.38  loss_dice: 2.825  loss_ce_0: 0.5606  loss_mask_0: 0.3791  loss_dice_0: 2.956  loss_ce_1: 0.2973  loss_mask_1: 0.3832  loss_dice_1: 2.878  loss_ce_2: 0.3011  loss_mask_2: 0.3806  loss_dice_2: 2.846  loss_ce_3: 0.2681  loss_mask_3: 0.3785  loss_dice_3: 2.839  loss_ce_4: 0.2821  loss_mask_4: 0.3775  loss_dice_4: 2.831  loss_ce_5: 0.2623  loss_mask_5: 0.375  loss_dice_5: 2.845  loss_ce_6: 0.2741  loss_mask_6: 0.3774  loss_dice_6: 2.831  loss_ce_7: 0.2514  loss_mask_7: 0.3785  loss_dice_7: 2.829  loss_ce_8: 0.2598  loss_mask_8: 0.3789  loss_dice_8: 2.833  time: 1.5363  data_time: 0.0959  lr: 7.8465e-06  max_mem: 21478M
[01/17 13:27:46] d2.utils.events INFO:  eta: 1 day, 5:17:13  iter: 21279  total_loss: 33.76  loss_ce: 0.2341  loss_mask: 0.3627  loss_dice: 2.701  loss_ce_0: 0.5401  loss_mask_0: 0.3584  loss_dice_0: 2.826  loss_ce_1: 0.2766  loss_mask_1: 0.3674  loss_dice_1: 2.738  loss_ce_2: 0.2645  loss_mask_2: 0.3677  loss_dice_2: 2.719  loss_ce_3: 0.2494  loss_mask_3: 0.3618  loss_dice_3: 2.703  loss_ce_4: 0.2464  loss_mask_4: 0.3621  loss_dice_4: 2.706  loss_ce_5: 0.2344  loss_mask_5: 0.3623  loss_dice_5: 2.704  loss_ce_6: 0.2347  loss_mask_6: 0.3623  loss_dice_6: 2.708  loss_ce_7: 0.2278  loss_mask_7: 0.3613  loss_dice_7: 2.7  loss_ce_8: 0.231  loss_mask_8: 0.3639  loss_dice_8: 2.706  time: 1.5362  data_time: 0.1000  lr: 7.8444e-06  max_mem: 21478M
[01/17 13:28:17] d2.utils.events INFO:  eta: 1 day, 5:16:43  iter: 21299  total_loss: 35.22  loss_ce: 0.2574  loss_mask: 0.364  loss_dice: 2.815  loss_ce_0: 0.5853  loss_mask_0: 0.3595  loss_dice_0: 2.935  loss_ce_1: 0.3164  loss_mask_1: 0.3665  loss_dice_1: 2.856  loss_ce_2: 0.2964  loss_mask_2: 0.3661  loss_dice_2: 2.839  loss_ce_3: 0.2873  loss_mask_3: 0.3656  loss_dice_3: 2.827  loss_ce_4: 0.2755  loss_mask_4: 0.3661  loss_dice_4: 2.824  loss_ce_5: 0.2695  loss_mask_5: 0.3648  loss_dice_5: 2.829  loss_ce_6: 0.2598  loss_mask_6: 0.3661  loss_dice_6: 2.816  loss_ce_7: 0.272  loss_mask_7: 0.3659  loss_dice_7: 2.823  loss_ce_8: 0.28  loss_mask_8: 0.364  loss_dice_8: 2.811  time: 1.5362  data_time: 0.1011  lr: 7.8424e-06  max_mem: 21478M
[01/17 13:28:47] d2.utils.events INFO:  eta: 1 day, 5:16:17  iter: 21319  total_loss: 35.05  loss_ce: 0.2507  loss_mask: 0.3728  loss_dice: 2.815  loss_ce_0: 0.5812  loss_mask_0: 0.3682  loss_dice_0: 2.949  loss_ce_1: 0.2929  loss_mask_1: 0.3767  loss_dice_1: 2.855  loss_ce_2: 0.2804  loss_mask_2: 0.3738  loss_dice_2: 2.841  loss_ce_3: 0.2783  loss_mask_3: 0.3732  loss_dice_3: 2.83  loss_ce_4: 0.276  loss_mask_4: 0.3721  loss_dice_4: 2.824  loss_ce_5: 0.2697  loss_mask_5: 0.3706  loss_dice_5: 2.824  loss_ce_6: 0.26  loss_mask_6: 0.3721  loss_dice_6: 2.82  loss_ce_7: 0.2605  loss_mask_7: 0.3717  loss_dice_7: 2.821  loss_ce_8: 0.259  loss_mask_8: 0.3722  loss_dice_8: 2.819  time: 1.5362  data_time: 0.0869  lr: 7.8403e-06  max_mem: 21478M
[01/17 13:29:18] d2.utils.events INFO:  eta: 1 day, 5:16:02  iter: 21339  total_loss: 35.66  loss_ce: 0.278  loss_mask: 0.3733  loss_dice: 2.86  loss_ce_0: 0.5736  loss_mask_0: 0.3699  loss_dice_0: 2.96  loss_ce_1: 0.3155  loss_mask_1: 0.378  loss_dice_1: 2.888  loss_ce_2: 0.3046  loss_mask_2: 0.3738  loss_dice_2: 2.867  loss_ce_3: 0.2947  loss_mask_3: 0.372  loss_dice_3: 2.865  loss_ce_4: 0.3043  loss_mask_4: 0.3727  loss_dice_4: 2.858  loss_ce_5: 0.2909  loss_mask_5: 0.3747  loss_dice_5: 2.858  loss_ce_6: 0.2762  loss_mask_6: 0.3744  loss_dice_6: 2.853  loss_ce_7: 0.2819  loss_mask_7: 0.3741  loss_dice_7: 2.854  loss_ce_8: 0.2871  loss_mask_8: 0.3733  loss_dice_8: 2.851  time: 1.5363  data_time: 0.0901  lr: 7.8383e-06  max_mem: 21478M
[01/17 13:29:49] d2.utils.events INFO:  eta: 1 day, 5:15:11  iter: 21359  total_loss: 34.69  loss_ce: 0.2658  loss_mask: 0.3742  loss_dice: 2.8  loss_ce_0: 0.5445  loss_mask_0: 0.37  loss_dice_0: 2.903  loss_ce_1: 0.3087  loss_mask_1: 0.3772  loss_dice_1: 2.83  loss_ce_2: 0.2773  loss_mask_2: 0.3764  loss_dice_2: 2.809  loss_ce_3: 0.2874  loss_mask_3: 0.3756  loss_dice_3: 2.797  loss_ce_4: 0.2636  loss_mask_4: 0.3736  loss_dice_4: 2.803  loss_ce_5: 0.2655  loss_mask_5: 0.374  loss_dice_5: 2.798  loss_ce_6: 0.2732  loss_mask_6: 0.3728  loss_dice_6: 2.798  loss_ce_7: 0.2613  loss_mask_7: 0.3739  loss_dice_7: 2.8  loss_ce_8: 0.2711  loss_mask_8: 0.3738  loss_dice_8: 2.798  time: 1.5363  data_time: 0.0963  lr: 7.8362e-06  max_mem: 21478M
[01/17 13:30:20] d2.utils.events INFO:  eta: 1 day, 5:14:15  iter: 21379  total_loss: 34.85  loss_ce: 0.2795  loss_mask: 0.3668  loss_dice: 2.785  loss_ce_0: 0.5718  loss_mask_0: 0.3636  loss_dice_0: 2.906  loss_ce_1: 0.2983  loss_mask_1: 0.3698  loss_dice_1: 2.826  loss_ce_2: 0.3155  loss_mask_2: 0.3652  loss_dice_2: 2.803  loss_ce_3: 0.305  loss_mask_3: 0.368  loss_dice_3: 2.79  loss_ce_4: 0.2912  loss_mask_4: 0.3686  loss_dice_4: 2.787  loss_ce_5: 0.2816  loss_mask_5: 0.368  loss_dice_5: 2.79  loss_ce_6: 0.2702  loss_mask_6: 0.3682  loss_dice_6: 2.791  loss_ce_7: 0.2786  loss_mask_7: 0.3673  loss_dice_7: 2.785  loss_ce_8: 0.2738  loss_mask_8: 0.3678  loss_dice_8: 2.781  time: 1.5362  data_time: 0.0958  lr: 7.8342e-06  max_mem: 21478M
[01/17 13:30:51] d2.utils.events INFO:  eta: 1 day, 5:14:07  iter: 21399  total_loss: 35.82  loss_ce: 0.2771  loss_mask: 0.3725  loss_dice: 2.847  loss_ce_0: 0.5582  loss_mask_0: 0.3739  loss_dice_0: 2.971  loss_ce_1: 0.2915  loss_mask_1: 0.3759  loss_dice_1: 2.898  loss_ce_2: 0.2862  loss_mask_2: 0.3731  loss_dice_2: 2.872  loss_ce_3: 0.2702  loss_mask_3: 0.373  loss_dice_3: 2.867  loss_ce_4: 0.2706  loss_mask_4: 0.3716  loss_dice_4: 2.856  loss_ce_5: 0.2624  loss_mask_5: 0.3708  loss_dice_5: 2.856  loss_ce_6: 0.2639  loss_mask_6: 0.3716  loss_dice_6: 2.846  loss_ce_7: 0.2628  loss_mask_7: 0.3704  loss_dice_7: 2.857  loss_ce_8: 0.2581  loss_mask_8: 0.3718  loss_dice_8: 2.851  time: 1.5362  data_time: 0.0923  lr: 7.8321e-06  max_mem: 21478M
[01/17 13:31:21] d2.utils.events INFO:  eta: 1 day, 5:13:37  iter: 21419  total_loss: 35.54  loss_ce: 0.2699  loss_mask: 0.3615  loss_dice: 2.864  loss_ce_0: 0.5751  loss_mask_0: 0.3662  loss_dice_0: 2.97  loss_ce_1: 0.2962  loss_mask_1: 0.372  loss_dice_1: 2.903  loss_ce_2: 0.2879  loss_mask_2: 0.3638  loss_dice_2: 2.877  loss_ce_3: 0.2741  loss_mask_3: 0.3616  loss_dice_3: 2.863  loss_ce_4: 0.2838  loss_mask_4: 0.3613  loss_dice_4: 2.866  loss_ce_5: 0.2767  loss_mask_5: 0.3611  loss_dice_5: 2.862  loss_ce_6: 0.267  loss_mask_6: 0.3607  loss_dice_6: 2.865  loss_ce_7: 0.257  loss_mask_7: 0.3612  loss_dice_7: 2.864  loss_ce_8: 0.2762  loss_mask_8: 0.3612  loss_dice_8: 2.863  time: 1.5362  data_time: 0.0945  lr: 7.8301e-06  max_mem: 21478M
[01/17 13:31:52] d2.utils.events INFO:  eta: 1 day, 5:13:12  iter: 21439  total_loss: 34.9  loss_ce: 0.2788  loss_mask: 0.364  loss_dice: 2.784  loss_ce_0: 0.5396  loss_mask_0: 0.3658  loss_dice_0: 2.91  loss_ce_1: 0.314  loss_mask_1: 0.3718  loss_dice_1: 2.828  loss_ce_2: 0.2993  loss_mask_2: 0.3686  loss_dice_2: 2.798  loss_ce_3: 0.3077  loss_mask_3: 0.3668  loss_dice_3: 2.79  loss_ce_4: 0.2913  loss_mask_4: 0.3653  loss_dice_4: 2.786  loss_ce_5: 0.2866  loss_mask_5: 0.3645  loss_dice_5: 2.791  loss_ce_6: 0.2876  loss_mask_6: 0.3653  loss_dice_6: 2.794  loss_ce_7: 0.2946  loss_mask_7: 0.3635  loss_dice_7: 2.793  loss_ce_8: 0.2946  loss_mask_8: 0.3628  loss_dice_8: 2.788  time: 1.5362  data_time: 0.0853  lr: 7.828e-06  max_mem: 21478M
[01/17 13:32:23] d2.utils.events INFO:  eta: 1 day, 5:12:56  iter: 21459  total_loss: 35.68  loss_ce: 0.2674  loss_mask: 0.3819  loss_dice: 2.861  loss_ce_0: 0.5597  loss_mask_0: 0.3858  loss_dice_0: 2.982  loss_ce_1: 0.294  loss_mask_1: 0.3932  loss_dice_1: 2.908  loss_ce_2: 0.2908  loss_mask_2: 0.3853  loss_dice_2: 2.881  loss_ce_3: 0.2838  loss_mask_3: 0.3829  loss_dice_3: 2.868  loss_ce_4: 0.2767  loss_mask_4: 0.3813  loss_dice_4: 2.875  loss_ce_5: 0.2748  loss_mask_5: 0.381  loss_dice_5: 2.872  loss_ce_6: 0.2675  loss_mask_6: 0.3835  loss_dice_6: 2.875  loss_ce_7: 0.2773  loss_mask_7: 0.383  loss_dice_7: 2.863  loss_ce_8: 0.2726  loss_mask_8: 0.3832  loss_dice_8: 2.865  time: 1.5362  data_time: 0.0912  lr: 7.826e-06  max_mem: 21478M
[01/17 13:32:54] d2.utils.events INFO:  eta: 1 day, 5:12:17  iter: 21479  total_loss: 35.86  loss_ce: 0.2859  loss_mask: 0.3618  loss_dice: 2.861  loss_ce_0: 0.5827  loss_mask_0: 0.3729  loss_dice_0: 2.967  loss_ce_1: 0.3308  loss_mask_1: 0.3689  loss_dice_1: 2.896  loss_ce_2: 0.3086  loss_mask_2: 0.365  loss_dice_2: 2.872  loss_ce_3: 0.2925  loss_mask_3: 0.3614  loss_dice_3: 2.867  loss_ce_4: 0.3038  loss_mask_4: 0.361  loss_dice_4: 2.861  loss_ce_5: 0.3073  loss_mask_5: 0.3621  loss_dice_5: 2.869  loss_ce_6: 0.2883  loss_mask_6: 0.3646  loss_dice_6: 2.856  loss_ce_7: 0.2905  loss_mask_7: 0.3655  loss_dice_7: 2.86  loss_ce_8: 0.2825  loss_mask_8: 0.3628  loss_dice_8: 2.864  time: 1.5362  data_time: 0.0847  lr: 7.8239e-06  max_mem: 21478M
[01/17 13:33:24] d2.utils.events INFO:  eta: 1 day, 5:11:36  iter: 21499  total_loss: 34.87  loss_ce: 0.2777  loss_mask: 0.3634  loss_dice: 2.786  loss_ce_0: 0.5685  loss_mask_0: 0.3705  loss_dice_0: 2.881  loss_ce_1: 0.3015  loss_mask_1: 0.3689  loss_dice_1: 2.807  loss_ce_2: 0.3003  loss_mask_2: 0.3664  loss_dice_2: 2.799  loss_ce_3: 0.2813  loss_mask_3: 0.3657  loss_dice_3: 2.784  loss_ce_4: 0.2861  loss_mask_4: 0.3614  loss_dice_4: 2.788  loss_ce_5: 0.2765  loss_mask_5: 0.363  loss_dice_5: 2.789  loss_ce_6: 0.2787  loss_mask_6: 0.3605  loss_dice_6: 2.792  loss_ce_7: 0.2668  loss_mask_7: 0.3619  loss_dice_7: 2.783  loss_ce_8: 0.2639  loss_mask_8: 0.3626  loss_dice_8: 2.784  time: 1.5362  data_time: 0.0990  lr: 7.8218e-06  max_mem: 21478M
[01/17 13:33:55] d2.utils.events INFO:  eta: 1 day, 5:11:03  iter: 21519  total_loss: 34.36  loss_ce: 0.2569  loss_mask: 0.3735  loss_dice: 2.756  loss_ce_0: 0.5416  loss_mask_0: 0.3696  loss_dice_0: 2.886  loss_ce_1: 0.2922  loss_mask_1: 0.3766  loss_dice_1: 2.797  loss_ce_2: 0.2766  loss_mask_2: 0.3756  loss_dice_2: 2.782  loss_ce_3: 0.2628  loss_mask_3: 0.3726  loss_dice_3: 2.765  loss_ce_4: 0.2498  loss_mask_4: 0.3705  loss_dice_4: 2.765  loss_ce_5: 0.2628  loss_mask_5: 0.3717  loss_dice_5: 2.767  loss_ce_6: 0.2369  loss_mask_6: 0.3719  loss_dice_6: 2.763  loss_ce_7: 0.2605  loss_mask_7: 0.3727  loss_dice_7: 2.76  loss_ce_8: 0.2591  loss_mask_8: 0.3723  loss_dice_8: 2.756  time: 1.5362  data_time: 0.1021  lr: 7.8198e-06  max_mem: 21478M
[01/17 13:34:26] d2.utils.events INFO:  eta: 1 day, 5:10:17  iter: 21539  total_loss: 34.97  loss_ce: 0.2589  loss_mask: 0.3644  loss_dice: 2.811  loss_ce_0: 0.5363  loss_mask_0: 0.3649  loss_dice_0: 2.923  loss_ce_1: 0.3001  loss_mask_1: 0.373  loss_dice_1: 2.84  loss_ce_2: 0.2899  loss_mask_2: 0.3689  loss_dice_2: 2.831  loss_ce_3: 0.2677  loss_mask_3: 0.366  loss_dice_3: 2.813  loss_ce_4: 0.2649  loss_mask_4: 0.3667  loss_dice_4: 2.805  loss_ce_5: 0.2569  loss_mask_5: 0.3658  loss_dice_5: 2.812  loss_ce_6: 0.2506  loss_mask_6: 0.3646  loss_dice_6: 2.809  loss_ce_7: 0.2458  loss_mask_7: 0.3642  loss_dice_7: 2.806  loss_ce_8: 0.2581  loss_mask_8: 0.3639  loss_dice_8: 2.808  time: 1.5362  data_time: 0.0949  lr: 7.8177e-06  max_mem: 21478M
[01/17 13:34:56] d2.utils.events INFO:  eta: 1 day, 5:09:25  iter: 21559  total_loss: 35.34  loss_ce: 0.2625  loss_mask: 0.3642  loss_dice: 2.826  loss_ce_0: 0.5678  loss_mask_0: 0.3659  loss_dice_0: 2.937  loss_ce_1: 0.3066  loss_mask_1: 0.3672  loss_dice_1: 2.866  loss_ce_2: 0.2917  loss_mask_2: 0.3661  loss_dice_2: 2.844  loss_ce_3: 0.2912  loss_mask_3: 0.3652  loss_dice_3: 2.831  loss_ce_4: 0.2905  loss_mask_4: 0.3643  loss_dice_4: 2.827  loss_ce_5: 0.2838  loss_mask_5: 0.3651  loss_dice_5: 2.831  loss_ce_6: 0.2675  loss_mask_6: 0.3653  loss_dice_6: 2.832  loss_ce_7: 0.276  loss_mask_7: 0.3657  loss_dice_7: 2.833  loss_ce_8: 0.2596  loss_mask_8: 0.3659  loss_dice_8: 2.819  time: 1.5362  data_time: 0.0938  lr: 7.8157e-06  max_mem: 21478M
[01/17 13:35:27] d2.utils.events INFO:  eta: 1 day, 5:08:42  iter: 21579  total_loss: 34.88  loss_ce: 0.2658  loss_mask: 0.3755  loss_dice: 2.789  loss_ce_0: 0.5574  loss_mask_0: 0.3748  loss_dice_0: 2.899  loss_ce_1: 0.3037  loss_mask_1: 0.3855  loss_dice_1: 2.828  loss_ce_2: 0.3052  loss_mask_2: 0.3778  loss_dice_2: 2.811  loss_ce_3: 0.3011  loss_mask_3: 0.3772  loss_dice_3: 2.799  loss_ce_4: 0.2811  loss_mask_4: 0.3781  loss_dice_4: 2.797  loss_ce_5: 0.2664  loss_mask_5: 0.3735  loss_dice_5: 2.802  loss_ce_6: 0.2687  loss_mask_6: 0.3731  loss_dice_6: 2.802  loss_ce_7: 0.2617  loss_mask_7: 0.3746  loss_dice_7: 2.788  loss_ce_8: 0.278  loss_mask_8: 0.376  loss_dice_8: 2.787  time: 1.5362  data_time: 0.0948  lr: 7.8136e-06  max_mem: 21478M
[01/17 13:35:58] d2.utils.events INFO:  eta: 1 day, 5:08:01  iter: 21599  total_loss: 35.39  loss_ce: 0.2454  loss_mask: 0.3561  loss_dice: 2.847  loss_ce_0: 0.585  loss_mask_0: 0.3576  loss_dice_0: 2.953  loss_ce_1: 0.3079  loss_mask_1: 0.3591  loss_dice_1: 2.896  loss_ce_2: 0.2903  loss_mask_2: 0.358  loss_dice_2: 2.866  loss_ce_3: 0.2775  loss_mask_3: 0.3556  loss_dice_3: 2.848  loss_ce_4: 0.2695  loss_mask_4: 0.3555  loss_dice_4: 2.846  loss_ce_5: 0.2612  loss_mask_5: 0.3556  loss_dice_5: 2.856  loss_ce_6: 0.2645  loss_mask_6: 0.3553  loss_dice_6: 2.843  loss_ce_7: 0.2535  loss_mask_7: 0.3549  loss_dice_7: 2.844  loss_ce_8: 0.2676  loss_mask_8: 0.3546  loss_dice_8: 2.841  time: 1.5362  data_time: 0.1079  lr: 7.8116e-06  max_mem: 21478M
[01/17 13:36:29] d2.utils.events INFO:  eta: 1 day, 5:07:53  iter: 21619  total_loss: 35.75  loss_ce: 0.2824  loss_mask: 0.3708  loss_dice: 2.825  loss_ce_0: 0.5623  loss_mask_0: 0.3785  loss_dice_0: 2.936  loss_ce_1: 0.3134  loss_mask_1: 0.3808  loss_dice_1: 2.87  loss_ce_2: 0.3191  loss_mask_2: 0.374  loss_dice_2: 2.848  loss_ce_3: 0.3149  loss_mask_3: 0.3718  loss_dice_3: 2.836  loss_ce_4: 0.2914  loss_mask_4: 0.3691  loss_dice_4: 2.844  loss_ce_5: 0.2784  loss_mask_5: 0.3688  loss_dice_5: 2.833  loss_ce_6: 0.2909  loss_mask_6: 0.3692  loss_dice_6: 2.832  loss_ce_7: 0.2826  loss_mask_7: 0.37  loss_dice_7: 2.829  loss_ce_8: 0.2807  loss_mask_8: 0.3688  loss_dice_8: 2.837  time: 1.5362  data_time: 0.0946  lr: 7.8095e-06  max_mem: 21478M
[01/17 13:36:59] d2.utils.events INFO:  eta: 1 day, 5:06:54  iter: 21639  total_loss: 34.37  loss_ce: 0.2739  loss_mask: 0.3706  loss_dice: 2.76  loss_ce_0: 0.5828  loss_mask_0: 0.3668  loss_dice_0: 2.885  loss_ce_1: 0.2896  loss_mask_1: 0.3755  loss_dice_1: 2.803  loss_ce_2: 0.2947  loss_mask_2: 0.3704  loss_dice_2: 2.779  loss_ce_3: 0.2754  loss_mask_3: 0.3701  loss_dice_3: 2.758  loss_ce_4: 0.264  loss_mask_4: 0.3704  loss_dice_4: 2.757  loss_ce_5: 0.2688  loss_mask_5: 0.3733  loss_dice_5: 2.765  loss_ce_6: 0.2508  loss_mask_6: 0.3707  loss_dice_6: 2.756  loss_ce_7: 0.2497  loss_mask_7: 0.3711  loss_dice_7: 2.762  loss_ce_8: 0.2534  loss_mask_8: 0.3705  loss_dice_8: 2.751  time: 1.5362  data_time: 0.0936  lr: 7.8075e-06  max_mem: 21478M
[01/17 13:37:30] d2.utils.events INFO:  eta: 1 day, 5:06:29  iter: 21659  total_loss: 34.63  loss_ce: 0.2572  loss_mask: 0.3606  loss_dice: 2.829  loss_ce_0: 0.55  loss_mask_0: 0.3567  loss_dice_0: 2.942  loss_ce_1: 0.2912  loss_mask_1: 0.3622  loss_dice_1: 2.863  loss_ce_2: 0.2896  loss_mask_2: 0.3594  loss_dice_2: 2.836  loss_ce_3: 0.266  loss_mask_3: 0.3578  loss_dice_3: 2.831  loss_ce_4: 0.2708  loss_mask_4: 0.3586  loss_dice_4: 2.836  loss_ce_5: 0.2645  loss_mask_5: 0.3594  loss_dice_5: 2.837  loss_ce_6: 0.2528  loss_mask_6: 0.3616  loss_dice_6: 2.83  loss_ce_7: 0.2478  loss_mask_7: 0.3617  loss_dice_7: 2.838  loss_ce_8: 0.2618  loss_mask_8: 0.3615  loss_dice_8: 2.825  time: 1.5362  data_time: 0.0870  lr: 7.8054e-06  max_mem: 21478M
[01/17 13:38:01] d2.utils.events INFO:  eta: 1 day, 5:06:08  iter: 21679  total_loss: 35.17  loss_ce: 0.2512  loss_mask: 0.3649  loss_dice: 2.836  loss_ce_0: 0.5587  loss_mask_0: 0.3666  loss_dice_0: 2.951  loss_ce_1: 0.2975  loss_mask_1: 0.3717  loss_dice_1: 2.871  loss_ce_2: 0.2978  loss_mask_2: 0.3681  loss_dice_2: 2.851  loss_ce_3: 0.2799  loss_mask_3: 0.3676  loss_dice_3: 2.837  loss_ce_4: 0.2757  loss_mask_4: 0.3668  loss_dice_4: 2.829  loss_ce_5: 0.2647  loss_mask_5: 0.3665  loss_dice_5: 2.841  loss_ce_6: 0.2608  loss_mask_6: 0.3676  loss_dice_6: 2.832  loss_ce_7: 0.2543  loss_mask_7: 0.3673  loss_dice_7: 2.834  loss_ce_8: 0.2602  loss_mask_8: 0.3646  loss_dice_8: 2.832  time: 1.5362  data_time: 0.1080  lr: 7.8033e-06  max_mem: 21478M
[01/17 13:38:32] d2.utils.events INFO:  eta: 1 day, 5:05:58  iter: 21699  total_loss: 34.12  loss_ce: 0.2498  loss_mask: 0.3671  loss_dice: 2.736  loss_ce_0: 0.5477  loss_mask_0: 0.3671  loss_dice_0: 2.86  loss_ce_1: 0.2831  loss_mask_1: 0.3727  loss_dice_1: 2.773  loss_ce_2: 0.2641  loss_mask_2: 0.3678  loss_dice_2: 2.748  loss_ce_3: 0.2585  loss_mask_3: 0.3662  loss_dice_3: 2.745  loss_ce_4: 0.2653  loss_mask_4: 0.367  loss_dice_4: 2.745  loss_ce_5: 0.2592  loss_mask_5: 0.3684  loss_dice_5: 2.744  loss_ce_6: 0.2601  loss_mask_6: 0.3676  loss_dice_6: 2.731  loss_ce_7: 0.2662  loss_mask_7: 0.3669  loss_dice_7: 2.738  loss_ce_8: 0.2547  loss_mask_8: 0.3669  loss_dice_8: 2.738  time: 1.5362  data_time: 0.0836  lr: 7.8013e-06  max_mem: 21478M
[01/17 13:39:03] d2.utils.events INFO:  eta: 1 day, 5:06:09  iter: 21719  total_loss: 35.03  loss_ce: 0.2614  loss_mask: 0.3719  loss_dice: 2.809  loss_ce_0: 0.5929  loss_mask_0: 0.3734  loss_dice_0: 2.917  loss_ce_1: 0.2947  loss_mask_1: 0.3828  loss_dice_1: 2.841  loss_ce_2: 0.2814  loss_mask_2: 0.3785  loss_dice_2: 2.823  loss_ce_3: 0.2672  loss_mask_3: 0.3727  loss_dice_3: 2.823  loss_ce_4: 0.264  loss_mask_4: 0.3734  loss_dice_4: 2.815  loss_ce_5: 0.2541  loss_mask_5: 0.3729  loss_dice_5: 2.807  loss_ce_6: 0.2585  loss_mask_6: 0.3726  loss_dice_6: 2.803  loss_ce_7: 0.2495  loss_mask_7: 0.3727  loss_dice_7: 2.811  loss_ce_8: 0.2565  loss_mask_8: 0.3735  loss_dice_8: 2.814  time: 1.5362  data_time: 0.0983  lr: 7.7992e-06  max_mem: 21478M
[01/17 13:39:33] d2.utils.events INFO:  eta: 1 day, 5:05:26  iter: 21739  total_loss: 33.98  loss_ce: 0.2582  loss_mask: 0.3673  loss_dice: 2.741  loss_ce_0: 0.6002  loss_mask_0: 0.3628  loss_dice_0: 2.85  loss_ce_1: 0.3042  loss_mask_1: 0.3652  loss_dice_1: 2.783  loss_ce_2: 0.288  loss_mask_2: 0.3656  loss_dice_2: 2.759  loss_ce_3: 0.2656  loss_mask_3: 0.3686  loss_dice_3: 2.742  loss_ce_4: 0.257  loss_mask_4: 0.37  loss_dice_4: 2.751  loss_ce_5: 0.2752  loss_mask_5: 0.367  loss_dice_5: 2.75  loss_ce_6: 0.252  loss_mask_6: 0.3675  loss_dice_6: 2.742  loss_ce_7: 0.2535  loss_mask_7: 0.366  loss_dice_7: 2.747  loss_ce_8: 0.2481  loss_mask_8: 0.3678  loss_dice_8: 2.74  time: 1.5362  data_time: 0.0938  lr: 7.7972e-06  max_mem: 21478M
[01/17 13:40:04] d2.utils.events INFO:  eta: 1 day, 5:05:15  iter: 21759  total_loss: 34.55  loss_ce: 0.263  loss_mask: 0.3718  loss_dice: 2.77  loss_ce_0: 0.5538  loss_mask_0: 0.3666  loss_dice_0: 2.886  loss_ce_1: 0.2964  loss_mask_1: 0.3761  loss_dice_1: 2.809  loss_ce_2: 0.2913  loss_mask_2: 0.3736  loss_dice_2: 2.789  loss_ce_3: 0.2788  loss_mask_3: 0.3725  loss_dice_3: 2.773  loss_ce_4: 0.2917  loss_mask_4: 0.3722  loss_dice_4: 2.768  loss_ce_5: 0.2531  loss_mask_5: 0.3725  loss_dice_5: 2.777  loss_ce_6: 0.2642  loss_mask_6: 0.372  loss_dice_6: 2.773  loss_ce_7: 0.262  loss_mask_7: 0.3704  loss_dice_7: 2.769  loss_ce_8: 0.2607  loss_mask_8: 0.3713  loss_dice_8: 2.768  time: 1.5362  data_time: 0.0974  lr: 7.7951e-06  max_mem: 21478M
[01/17 13:40:36] d2.utils.events INFO:  eta: 1 day, 5:05:28  iter: 21779  total_loss: 34.91  loss_ce: 0.266  loss_mask: 0.3585  loss_dice: 2.794  loss_ce_0: 0.5689  loss_mask_0: 0.3516  loss_dice_0: 2.935  loss_ce_1: 0.3078  loss_mask_1: 0.3598  loss_dice_1: 2.852  loss_ce_2: 0.2937  loss_mask_2: 0.3565  loss_dice_2: 2.817  loss_ce_3: 0.2701  loss_mask_3: 0.3559  loss_dice_3: 2.809  loss_ce_4: 0.27  loss_mask_4: 0.3541  loss_dice_4: 2.808  loss_ce_5: 0.2701  loss_mask_5: 0.3566  loss_dice_5: 2.817  loss_ce_6: 0.2635  loss_mask_6: 0.3555  loss_dice_6: 2.81  loss_ce_7: 0.2574  loss_mask_7: 0.3568  loss_dice_7: 2.794  loss_ce_8: 0.2665  loss_mask_8: 0.3567  loss_dice_8: 2.799  time: 1.5363  data_time: 0.1111  lr: 7.7931e-06  max_mem: 21478M
[01/17 13:41:07] d2.utils.events INFO:  eta: 1 day, 5:04:40  iter: 21799  total_loss: 35.68  loss_ce: 0.2796  loss_mask: 0.3794  loss_dice: 2.839  loss_ce_0: 0.6093  loss_mask_0: 0.3856  loss_dice_0: 2.939  loss_ce_1: 0.3139  loss_mask_1: 0.3915  loss_dice_1: 2.859  loss_ce_2: 0.3186  loss_mask_2: 0.3892  loss_dice_2: 2.844  loss_ce_3: 0.301  loss_mask_3: 0.3845  loss_dice_3: 2.828  loss_ce_4: 0.2831  loss_mask_4: 0.3832  loss_dice_4: 2.829  loss_ce_5: 0.2904  loss_mask_5: 0.3817  loss_dice_5: 2.84  loss_ce_6: 0.2763  loss_mask_6: 0.3794  loss_dice_6: 2.834  loss_ce_7: 0.2706  loss_mask_7: 0.3803  loss_dice_7: 2.842  loss_ce_8: 0.2657  loss_mask_8: 0.3788  loss_dice_8: 2.831  time: 1.5363  data_time: 0.1049  lr: 7.791e-06  max_mem: 21478M
[01/17 13:41:37] d2.utils.events INFO:  eta: 1 day, 5:04:44  iter: 21819  total_loss: 35.02  loss_ce: 0.2674  loss_mask: 0.3665  loss_dice: 2.81  loss_ce_0: 0.6014  loss_mask_0: 0.363  loss_dice_0: 2.909  loss_ce_1: 0.3004  loss_mask_1: 0.3701  loss_dice_1: 2.846  loss_ce_2: 0.3053  loss_mask_2: 0.3646  loss_dice_2: 2.819  loss_ce_3: 0.2734  loss_mask_3: 0.3617  loss_dice_3: 2.818  loss_ce_4: 0.2847  loss_mask_4: 0.3598  loss_dice_4: 2.815  loss_ce_5: 0.2749  loss_mask_5: 0.3627  loss_dice_5: 2.812  loss_ce_6: 0.2769  loss_mask_6: 0.3648  loss_dice_6: 2.811  loss_ce_7: 0.2724  loss_mask_7: 0.3668  loss_dice_7: 2.805  loss_ce_8: 0.2815  loss_mask_8: 0.3645  loss_dice_8: 2.809  time: 1.5363  data_time: 0.1071  lr: 7.7889e-06  max_mem: 21478M
[01/17 13:42:09] d2.utils.events INFO:  eta: 1 day, 5:04:41  iter: 21839  total_loss: 34.74  loss_ce: 0.2428  loss_mask: 0.3642  loss_dice: 2.794  loss_ce_0: 0.5589  loss_mask_0: 0.3678  loss_dice_0: 2.91  loss_ce_1: 0.2901  loss_mask_1: 0.3706  loss_dice_1: 2.844  loss_ce_2: 0.2667  loss_mask_2: 0.3671  loss_dice_2: 2.819  loss_ce_3: 0.2632  loss_mask_3: 0.3688  loss_dice_3: 2.807  loss_ce_4: 0.2616  loss_mask_4: 0.3673  loss_dice_4: 2.802  loss_ce_5: 0.261  loss_mask_5: 0.3675  loss_dice_5: 2.804  loss_ce_6: 0.253  loss_mask_6: 0.3681  loss_dice_6: 2.798  loss_ce_7: 0.267  loss_mask_7: 0.3661  loss_dice_7: 2.805  loss_ce_8: 0.2466  loss_mask_8: 0.3663  loss_dice_8: 2.797  time: 1.5363  data_time: 0.1049  lr: 7.7869e-06  max_mem: 21478M
[01/17 13:42:41] d2.utils.events INFO:  eta: 1 day, 5:04:10  iter: 21859  total_loss: 34.91  loss_ce: 0.2473  loss_mask: 0.3644  loss_dice: 2.824  loss_ce_0: 0.5635  loss_mask_0: 0.363  loss_dice_0: 2.945  loss_ce_1: 0.2857  loss_mask_1: 0.3691  loss_dice_1: 2.863  loss_ce_2: 0.2753  loss_mask_2: 0.3658  loss_dice_2: 2.838  loss_ce_3: 0.2501  loss_mask_3: 0.3638  loss_dice_3: 2.829  loss_ce_4: 0.2503  loss_mask_4: 0.3625  loss_dice_4: 2.827  loss_ce_5: 0.2595  loss_mask_5: 0.362  loss_dice_5: 2.828  loss_ce_6: 0.2427  loss_mask_6: 0.3642  loss_dice_6: 2.835  loss_ce_7: 0.2552  loss_mask_7: 0.3637  loss_dice_7: 2.822  loss_ce_8: 0.2518  loss_mask_8: 0.3641  loss_dice_8: 2.825  time: 1.5363  data_time: 0.1132  lr: 7.7848e-06  max_mem: 21478M
[01/17 13:43:11] d2.utils.events INFO:  eta: 1 day, 5:03:23  iter: 21879  total_loss: 35.22  loss_ce: 0.2513  loss_mask: 0.3653  loss_dice: 2.814  loss_ce_0: 0.5674  loss_mask_0: 0.371  loss_dice_0: 2.937  loss_ce_1: 0.2911  loss_mask_1: 0.3717  loss_dice_1: 2.862  loss_ce_2: 0.3027  loss_mask_2: 0.3685  loss_dice_2: 2.83  loss_ce_3: 0.2505  loss_mask_3: 0.3704  loss_dice_3: 2.822  loss_ce_4: 0.2612  loss_mask_4: 0.368  loss_dice_4: 2.813  loss_ce_5: 0.2559  loss_mask_5: 0.3669  loss_dice_5: 2.82  loss_ce_6: 0.2366  loss_mask_6: 0.3672  loss_dice_6: 2.815  loss_ce_7: 0.2445  loss_mask_7: 0.3659  loss_dice_7: 2.814  loss_ce_8: 0.2615  loss_mask_8: 0.3656  loss_dice_8: 2.82  time: 1.5363  data_time: 0.1048  lr: 7.7828e-06  max_mem: 21478M
[01/17 13:43:43] d2.utils.events INFO:  eta: 1 day, 5:04:12  iter: 21899  total_loss: 35.23  loss_ce: 0.2664  loss_mask: 0.3613  loss_dice: 2.807  loss_ce_0: 0.5486  loss_mask_0: 0.3578  loss_dice_0: 2.928  loss_ce_1: 0.2868  loss_mask_1: 0.3602  loss_dice_1: 2.854  loss_ce_2: 0.2916  loss_mask_2: 0.3597  loss_dice_2: 2.831  loss_ce_3: 0.2645  loss_mask_3: 0.3606  loss_dice_3: 2.818  loss_ce_4: 0.2648  loss_mask_4: 0.3586  loss_dice_4: 2.815  loss_ce_5: 0.2651  loss_mask_5: 0.3568  loss_dice_5: 2.819  loss_ce_6: 0.2688  loss_mask_6: 0.3594  loss_dice_6: 2.818  loss_ce_7: 0.2796  loss_mask_7: 0.36  loss_dice_7: 2.812  loss_ce_8: 0.2618  loss_mask_8: 0.3598  loss_dice_8: 2.81  time: 1.5364  data_time: 0.0992  lr: 7.7807e-06  max_mem: 21478M
[01/17 13:44:13] d2.utils.events INFO:  eta: 1 day, 5:04:17  iter: 21919  total_loss: 35.49  loss_ce: 0.2603  loss_mask: 0.3672  loss_dice: 2.853  loss_ce_0: 0.5632  loss_mask_0: 0.3679  loss_dice_0: 2.984  loss_ce_1: 0.2932  loss_mask_1: 0.3719  loss_dice_1: 2.9  loss_ce_2: 0.2942  loss_mask_2: 0.3661  loss_dice_2: 2.875  loss_ce_3: 0.285  loss_mask_3: 0.3646  loss_dice_3: 2.856  loss_ce_4: 0.2794  loss_mask_4: 0.3658  loss_dice_4: 2.859  loss_ce_5: 0.2657  loss_mask_5: 0.3655  loss_dice_5: 2.856  loss_ce_6: 0.2755  loss_mask_6: 0.3657  loss_dice_6: 2.864  loss_ce_7: 0.2676  loss_mask_7: 0.3676  loss_dice_7: 2.848  loss_ce_8: 0.2554  loss_mask_8: 0.3669  loss_dice_8: 2.853  time: 1.5364  data_time: 0.0964  lr: 7.7787e-06  max_mem: 21478M
[01/17 13:44:44] d2.utils.events INFO:  eta: 1 day, 5:03:11  iter: 21939  total_loss: 34.38  loss_ce: 0.2389  loss_mask: 0.3704  loss_dice: 2.749  loss_ce_0: 0.566  loss_mask_0: 0.3648  loss_dice_0: 2.863  loss_ce_1: 0.2783  loss_mask_1: 0.3746  loss_dice_1: 2.78  loss_ce_2: 0.2888  loss_mask_2: 0.3706  loss_dice_2: 2.759  loss_ce_3: 0.2608  loss_mask_3: 0.3685  loss_dice_3: 2.751  loss_ce_4: 0.2678  loss_mask_4: 0.3682  loss_dice_4: 2.752  loss_ce_5: 0.2579  loss_mask_5: 0.3684  loss_dice_5: 2.744  loss_ce_6: 0.2448  loss_mask_6: 0.3681  loss_dice_6: 2.753  loss_ce_7: 0.2535  loss_mask_7: 0.3671  loss_dice_7: 2.75  loss_ce_8: 0.2577  loss_mask_8: 0.3693  loss_dice_8: 2.745  time: 1.5364  data_time: 0.0982  lr: 7.7766e-06  max_mem: 21478M
[01/17 13:45:15] d2.utils.events INFO:  eta: 1 day, 5:02:19  iter: 21959  total_loss: 35.59  loss_ce: 0.2827  loss_mask: 0.3747  loss_dice: 2.843  loss_ce_0: 0.5613  loss_mask_0: 0.3719  loss_dice_0: 2.963  loss_ce_1: 0.2917  loss_mask_1: 0.3772  loss_dice_1: 2.894  loss_ce_2: 0.297  loss_mask_2: 0.3757  loss_dice_2: 2.87  loss_ce_3: 0.2828  loss_mask_3: 0.3745  loss_dice_3: 2.866  loss_ce_4: 0.2656  loss_mask_4: 0.3732  loss_dice_4: 2.857  loss_ce_5: 0.2711  loss_mask_5: 0.3724  loss_dice_5: 2.873  loss_ce_6: 0.2701  loss_mask_6: 0.3742  loss_dice_6: 2.861  loss_ce_7: 0.2897  loss_mask_7: 0.3727  loss_dice_7: 2.858  loss_ce_8: 0.2785  loss_mask_8: 0.3728  loss_dice_8: 2.854  time: 1.5363  data_time: 0.0985  lr: 7.7746e-06  max_mem: 21478M
[01/17 13:45:46] d2.utils.events INFO:  eta: 1 day, 5:01:28  iter: 21979  total_loss: 35.56  loss_ce: 0.257  loss_mask: 0.3654  loss_dice: 2.824  loss_ce_0: 0.5865  loss_mask_0: 0.3751  loss_dice_0: 2.938  loss_ce_1: 0.3183  loss_mask_1: 0.3753  loss_dice_1: 2.86  loss_ce_2: 0.2928  loss_mask_2: 0.3679  loss_dice_2: 2.835  loss_ce_3: 0.2798  loss_mask_3: 0.3633  loss_dice_3: 2.843  loss_ce_4: 0.2861  loss_mask_4: 0.3648  loss_dice_4: 2.83  loss_ce_5: 0.2793  loss_mask_5: 0.3655  loss_dice_5: 2.828  loss_ce_6: 0.2715  loss_mask_6: 0.3655  loss_dice_6: 2.823  loss_ce_7: 0.2728  loss_mask_7: 0.364  loss_dice_7: 2.819  loss_ce_8: 0.2738  loss_mask_8: 0.3627  loss_dice_8: 2.83  time: 1.5364  data_time: 0.1067  lr: 7.7725e-06  max_mem: 21478M
[01/17 13:46:17] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 13:46:18] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 13:46:18] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 13:46:18] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 13:46:33] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0092 s/iter. Inference: 0.1834 s/iter. Eval: 0.2346 s/iter. Total: 0.4272 s/iter. ETA=0:07:42
[01/17 13:46:38] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0122 s/iter. Inference: 0.1713 s/iter. Eval: 0.2356 s/iter. Total: 0.4192 s/iter. ETA=0:07:28
[01/17 13:46:43] d2.evaluation.evaluator INFO: Inference done 36/1093. Dataloading: 0.0119 s/iter. Inference: 0.1780 s/iter. Eval: 0.2293 s/iter. Total: 0.4193 s/iter. ETA=0:07:23
[01/17 13:46:49] d2.evaluation.evaluator INFO: Inference done 49/1093. Dataloading: 0.0130 s/iter. Inference: 0.1755 s/iter. Eval: 0.2223 s/iter. Total: 0.4109 s/iter. ETA=0:07:08
[01/17 13:46:54] d2.evaluation.evaluator INFO: Inference done 61/1093. Dataloading: 0.0134 s/iter. Inference: 0.1767 s/iter. Eval: 0.2277 s/iter. Total: 0.4179 s/iter. ETA=0:07:11
[01/17 13:46:59] d2.evaluation.evaluator INFO: Inference done 72/1093. Dataloading: 0.0145 s/iter. Inference: 0.1763 s/iter. Eval: 0.2354 s/iter. Total: 0.4263 s/iter. ETA=0:07:15
[01/17 13:47:04] d2.evaluation.evaluator INFO: Inference done 84/1093. Dataloading: 0.0143 s/iter. Inference: 0.1760 s/iter. Eval: 0.2395 s/iter. Total: 0.4300 s/iter. ETA=0:07:13
[01/17 13:47:10] d2.evaluation.evaluator INFO: Inference done 95/1093. Dataloading: 0.0144 s/iter. Inference: 0.1761 s/iter. Eval: 0.2441 s/iter. Total: 0.4348 s/iter. ETA=0:07:13
[01/17 13:47:15] d2.evaluation.evaluator INFO: Inference done 109/1093. Dataloading: 0.0139 s/iter. Inference: 0.1740 s/iter. Eval: 0.2370 s/iter. Total: 0.4251 s/iter. ETA=0:06:58
[01/17 13:47:20] d2.evaluation.evaluator INFO: Inference done 121/1093. Dataloading: 0.0143 s/iter. Inference: 0.1756 s/iter. Eval: 0.2371 s/iter. Total: 0.4270 s/iter. ETA=0:06:55
[01/17 13:47:25] d2.evaluation.evaluator INFO: Inference done 135/1093. Dataloading: 0.0140 s/iter. Inference: 0.1738 s/iter. Eval: 0.2347 s/iter. Total: 0.4226 s/iter. ETA=0:06:44
[01/17 13:47:30] d2.evaluation.evaluator INFO: Inference done 149/1093. Dataloading: 0.0138 s/iter. Inference: 0.1741 s/iter. Eval: 0.2289 s/iter. Total: 0.4169 s/iter. ETA=0:06:33
[01/17 13:47:36] d2.evaluation.evaluator INFO: Inference done 163/1093. Dataloading: 0.0139 s/iter. Inference: 0.1727 s/iter. Eval: 0.2257 s/iter. Total: 0.4124 s/iter. ETA=0:06:23
[01/17 13:47:41] d2.evaluation.evaluator INFO: Inference done 175/1093. Dataloading: 0.0140 s/iter. Inference: 0.1723 s/iter. Eval: 0.2283 s/iter. Total: 0.4148 s/iter. ETA=0:06:20
[01/17 13:47:46] d2.evaluation.evaluator INFO: Inference done 189/1093. Dataloading: 0.0138 s/iter. Inference: 0.1720 s/iter. Eval: 0.2257 s/iter. Total: 0.4116 s/iter. ETA=0:06:12
[01/17 13:47:51] d2.evaluation.evaluator INFO: Inference done 202/1093. Dataloading: 0.0137 s/iter. Inference: 0.1716 s/iter. Eval: 0.2254 s/iter. Total: 0.4107 s/iter. ETA=0:06:05
[01/17 13:47:56] d2.evaluation.evaluator INFO: Inference done 215/1093. Dataloading: 0.0137 s/iter. Inference: 0.1708 s/iter. Eval: 0.2249 s/iter. Total: 0.4095 s/iter. ETA=0:05:59
[01/17 13:48:02] d2.evaluation.evaluator INFO: Inference done 228/1093. Dataloading: 0.0137 s/iter. Inference: 0.1711 s/iter. Eval: 0.2242 s/iter. Total: 0.4091 s/iter. ETA=0:05:53
[01/17 13:48:07] d2.evaluation.evaluator INFO: Inference done 240/1093. Dataloading: 0.0137 s/iter. Inference: 0.1713 s/iter. Eval: 0.2254 s/iter. Total: 0.4105 s/iter. ETA=0:05:50
[01/17 13:48:12] d2.evaluation.evaluator INFO: Inference done 253/1093. Dataloading: 0.0137 s/iter. Inference: 0.1709 s/iter. Eval: 0.2245 s/iter. Total: 0.4091 s/iter. ETA=0:05:43
[01/17 13:48:17] d2.evaluation.evaluator INFO: Inference done 265/1093. Dataloading: 0.0137 s/iter. Inference: 0.1708 s/iter. Eval: 0.2251 s/iter. Total: 0.4097 s/iter. ETA=0:05:39
[01/17 13:48:22] d2.evaluation.evaluator INFO: Inference done 279/1093. Dataloading: 0.0138 s/iter. Inference: 0.1698 s/iter. Eval: 0.2248 s/iter. Total: 0.4085 s/iter. ETA=0:05:32
[01/17 13:48:28] d2.evaluation.evaluator INFO: Inference done 293/1093. Dataloading: 0.0136 s/iter. Inference: 0.1702 s/iter. Eval: 0.2228 s/iter. Total: 0.4067 s/iter. ETA=0:05:25
[01/17 13:48:33] d2.evaluation.evaluator INFO: Inference done 307/1093. Dataloading: 0.0137 s/iter. Inference: 0.1696 s/iter. Eval: 0.2229 s/iter. Total: 0.4063 s/iter. ETA=0:05:19
[01/17 13:48:38] d2.evaluation.evaluator INFO: Inference done 320/1093. Dataloading: 0.0137 s/iter. Inference: 0.1691 s/iter. Eval: 0.2234 s/iter. Total: 0.4063 s/iter. ETA=0:05:14
[01/17 13:48:43] d2.evaluation.evaluator INFO: Inference done 331/1093. Dataloading: 0.0137 s/iter. Inference: 0.1697 s/iter. Eval: 0.2244 s/iter. Total: 0.4080 s/iter. ETA=0:05:10
[01/17 13:48:49] d2.evaluation.evaluator INFO: Inference done 347/1093. Dataloading: 0.0135 s/iter. Inference: 0.1697 s/iter. Eval: 0.2204 s/iter. Total: 0.4037 s/iter. ETA=0:05:01
[01/17 13:48:54] d2.evaluation.evaluator INFO: Inference done 360/1093. Dataloading: 0.0135 s/iter. Inference: 0.1699 s/iter. Eval: 0.2203 s/iter. Total: 0.4038 s/iter. ETA=0:04:55
[01/17 13:48:59] d2.evaluation.evaluator INFO: Inference done 373/1093. Dataloading: 0.0134 s/iter. Inference: 0.1694 s/iter. Eval: 0.2204 s/iter. Total: 0.4034 s/iter. ETA=0:04:50
[01/17 13:49:04] d2.evaluation.evaluator INFO: Inference done 386/1093. Dataloading: 0.0134 s/iter. Inference: 0.1693 s/iter. Eval: 0.2203 s/iter. Total: 0.4031 s/iter. ETA=0:04:45
[01/17 13:49:09] d2.evaluation.evaluator INFO: Inference done 398/1093. Dataloading: 0.0134 s/iter. Inference: 0.1698 s/iter. Eval: 0.2207 s/iter. Total: 0.4040 s/iter. ETA=0:04:40
[01/17 13:49:14] d2.evaluation.evaluator INFO: Inference done 411/1093. Dataloading: 0.0134 s/iter. Inference: 0.1694 s/iter. Eval: 0.2207 s/iter. Total: 0.4036 s/iter. ETA=0:04:35
[01/17 13:49:19] d2.evaluation.evaluator INFO: Inference done 423/1093. Dataloading: 0.0134 s/iter. Inference: 0.1693 s/iter. Eval: 0.2214 s/iter. Total: 0.4042 s/iter. ETA=0:04:30
[01/17 13:49:25] d2.evaluation.evaluator INFO: Inference done 436/1093. Dataloading: 0.0134 s/iter. Inference: 0.1694 s/iter. Eval: 0.2210 s/iter. Total: 0.4039 s/iter. ETA=0:04:25
[01/17 13:49:30] d2.evaluation.evaluator INFO: Inference done 451/1093. Dataloading: 0.0134 s/iter. Inference: 0.1693 s/iter. Eval: 0.2194 s/iter. Total: 0.4022 s/iter. ETA=0:04:18
[01/17 13:49:35] d2.evaluation.evaluator INFO: Inference done 464/1093. Dataloading: 0.0134 s/iter. Inference: 0.1698 s/iter. Eval: 0.2193 s/iter. Total: 0.4026 s/iter. ETA=0:04:13
[01/17 13:49:40] d2.evaluation.evaluator INFO: Inference done 479/1093. Dataloading: 0.0133 s/iter. Inference: 0.1698 s/iter. Eval: 0.2175 s/iter. Total: 0.4007 s/iter. ETA=0:04:06
[01/17 13:49:46] d2.evaluation.evaluator INFO: Inference done 494/1093. Dataloading: 0.0132 s/iter. Inference: 0.1696 s/iter. Eval: 0.2165 s/iter. Total: 0.3994 s/iter. ETA=0:03:59
[01/17 13:49:51] d2.evaluation.evaluator INFO: Inference done 508/1093. Dataloading: 0.0132 s/iter. Inference: 0.1697 s/iter. Eval: 0.2154 s/iter. Total: 0.3984 s/iter. ETA=0:03:53
[01/17 13:49:56] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0132 s/iter. Inference: 0.1700 s/iter. Eval: 0.2151 s/iter. Total: 0.3984 s/iter. ETA=0:03:47
[01/17 13:50:01] d2.evaluation.evaluator INFO: Inference done 533/1093. Dataloading: 0.0132 s/iter. Inference: 0.1699 s/iter. Eval: 0.2158 s/iter. Total: 0.3990 s/iter. ETA=0:03:43
[01/17 13:50:06] d2.evaluation.evaluator INFO: Inference done 547/1093. Dataloading: 0.0131 s/iter. Inference: 0.1700 s/iter. Eval: 0.2152 s/iter. Total: 0.3984 s/iter. ETA=0:03:37
[01/17 13:50:11] d2.evaluation.evaluator INFO: Inference done 559/1093. Dataloading: 0.0131 s/iter. Inference: 0.1702 s/iter. Eval: 0.2155 s/iter. Total: 0.3989 s/iter. ETA=0:03:33
[01/17 13:50:17] d2.evaluation.evaluator INFO: Inference done 572/1093. Dataloading: 0.0131 s/iter. Inference: 0.1703 s/iter. Eval: 0.2153 s/iter. Total: 0.3988 s/iter. ETA=0:03:27
[01/17 13:50:22] d2.evaluation.evaluator INFO: Inference done 588/1093. Dataloading: 0.0132 s/iter. Inference: 0.1702 s/iter. Eval: 0.2134 s/iter. Total: 0.3970 s/iter. ETA=0:03:20
[01/17 13:50:27] d2.evaluation.evaluator INFO: Inference done 600/1093. Dataloading: 0.0132 s/iter. Inference: 0.1704 s/iter. Eval: 0.2141 s/iter. Total: 0.3978 s/iter. ETA=0:03:16
[01/17 13:50:33] d2.evaluation.evaluator INFO: Inference done 614/1093. Dataloading: 0.0132 s/iter. Inference: 0.1704 s/iter. Eval: 0.2137 s/iter. Total: 0.3974 s/iter. ETA=0:03:10
[01/17 13:50:38] d2.evaluation.evaluator INFO: Inference done 628/1093. Dataloading: 0.0132 s/iter. Inference: 0.1704 s/iter. Eval: 0.2133 s/iter. Total: 0.3970 s/iter. ETA=0:03:04
[01/17 13:50:43] d2.evaluation.evaluator INFO: Inference done 640/1093. Dataloading: 0.0132 s/iter. Inference: 0.1712 s/iter. Eval: 0.2133 s/iter. Total: 0.3978 s/iter. ETA=0:03:00
[01/17 13:50:48] d2.evaluation.evaluator INFO: Inference done 654/1093. Dataloading: 0.0131 s/iter. Inference: 0.1716 s/iter. Eval: 0.2125 s/iter. Total: 0.3973 s/iter. ETA=0:02:54
[01/17 13:50:53] d2.evaluation.evaluator INFO: Inference done 666/1093. Dataloading: 0.0131 s/iter. Inference: 0.1718 s/iter. Eval: 0.2127 s/iter. Total: 0.3977 s/iter. ETA=0:02:49
[01/17 13:50:58] d2.evaluation.evaluator INFO: Inference done 680/1093. Dataloading: 0.0131 s/iter. Inference: 0.1720 s/iter. Eval: 0.2118 s/iter. Total: 0.3970 s/iter. ETA=0:02:43
[01/17 13:51:04] d2.evaluation.evaluator INFO: Inference done 693/1093. Dataloading: 0.0131 s/iter. Inference: 0.1728 s/iter. Eval: 0.2109 s/iter. Total: 0.3969 s/iter. ETA=0:02:38
[01/17 13:51:09] d2.evaluation.evaluator INFO: Inference done 705/1093. Dataloading: 0.0131 s/iter. Inference: 0.1729 s/iter. Eval: 0.2114 s/iter. Total: 0.3976 s/iter. ETA=0:02:34
[01/17 13:51:14] d2.evaluation.evaluator INFO: Inference done 718/1093. Dataloading: 0.0131 s/iter. Inference: 0.1727 s/iter. Eval: 0.2120 s/iter. Total: 0.3979 s/iter. ETA=0:02:29
[01/17 13:51:19] d2.evaluation.evaluator INFO: Inference done 731/1093. Dataloading: 0.0135 s/iter. Inference: 0.1730 s/iter. Eval: 0.2112 s/iter. Total: 0.3978 s/iter. ETA=0:02:24
[01/17 13:51:24] d2.evaluation.evaluator INFO: Inference done 744/1093. Dataloading: 0.0134 s/iter. Inference: 0.1736 s/iter. Eval: 0.2106 s/iter. Total: 0.3977 s/iter. ETA=0:02:18
[01/17 13:51:30] d2.evaluation.evaluator INFO: Inference done 757/1093. Dataloading: 0.0134 s/iter. Inference: 0.1739 s/iter. Eval: 0.2106 s/iter. Total: 0.3980 s/iter. ETA=0:02:13
[01/17 13:51:35] d2.evaluation.evaluator INFO: Inference done 769/1093. Dataloading: 0.0134 s/iter. Inference: 0.1740 s/iter. Eval: 0.2109 s/iter. Total: 0.3984 s/iter. ETA=0:02:09
[01/17 13:51:40] d2.evaluation.evaluator INFO: Inference done 782/1093. Dataloading: 0.0134 s/iter. Inference: 0.1741 s/iter. Eval: 0.2107 s/iter. Total: 0.3983 s/iter. ETA=0:02:03
[01/17 13:51:45] d2.evaluation.evaluator INFO: Inference done 796/1093. Dataloading: 0.0133 s/iter. Inference: 0.1743 s/iter. Eval: 0.2102 s/iter. Total: 0.3979 s/iter. ETA=0:01:58
[01/17 13:51:50] d2.evaluation.evaluator INFO: Inference done 809/1093. Dataloading: 0.0133 s/iter. Inference: 0.1742 s/iter. Eval: 0.2103 s/iter. Total: 0.3979 s/iter. ETA=0:01:53
[01/17 13:51:56] d2.evaluation.evaluator INFO: Inference done 824/1093. Dataloading: 0.0133 s/iter. Inference: 0.1741 s/iter. Eval: 0.2095 s/iter. Total: 0.3970 s/iter. ETA=0:01:46
[01/17 13:52:01] d2.evaluation.evaluator INFO: Inference done 838/1093. Dataloading: 0.0132 s/iter. Inference: 0.1741 s/iter. Eval: 0.2091 s/iter. Total: 0.3965 s/iter. ETA=0:01:41
[01/17 13:52:06] d2.evaluation.evaluator INFO: Inference done 851/1093. Dataloading: 0.0133 s/iter. Inference: 0.1740 s/iter. Eval: 0.2093 s/iter. Total: 0.3967 s/iter. ETA=0:01:35
[01/17 13:52:11] d2.evaluation.evaluator INFO: Inference done 862/1093. Dataloading: 0.0133 s/iter. Inference: 0.1739 s/iter. Eval: 0.2103 s/iter. Total: 0.3977 s/iter. ETA=0:01:31
[01/17 13:52:16] d2.evaluation.evaluator INFO: Inference done 875/1093. Dataloading: 0.0133 s/iter. Inference: 0.1740 s/iter. Eval: 0.2102 s/iter. Total: 0.3976 s/iter. ETA=0:01:26
[01/17 13:52:22] d2.evaluation.evaluator INFO: Inference done 886/1093. Dataloading: 0.0133 s/iter. Inference: 0.1740 s/iter. Eval: 0.2111 s/iter. Total: 0.3985 s/iter. ETA=0:01:22
[01/17 13:52:27] d2.evaluation.evaluator INFO: Inference done 898/1093. Dataloading: 0.0133 s/iter. Inference: 0.1739 s/iter. Eval: 0.2116 s/iter. Total: 0.3989 s/iter. ETA=0:01:17
[01/17 13:52:32] d2.evaluation.evaluator INFO: Inference done 913/1093. Dataloading: 0.0133 s/iter. Inference: 0.1738 s/iter. Eval: 0.2111 s/iter. Total: 0.3983 s/iter. ETA=0:01:11
[01/17 13:52:37] d2.evaluation.evaluator INFO: Inference done 927/1093. Dataloading: 0.0133 s/iter. Inference: 0.1736 s/iter. Eval: 0.2109 s/iter. Total: 0.3979 s/iter. ETA=0:01:06
[01/17 13:52:42] d2.evaluation.evaluator INFO: Inference done 941/1093. Dataloading: 0.0133 s/iter. Inference: 0.1734 s/iter. Eval: 0.2107 s/iter. Total: 0.3974 s/iter. ETA=0:01:00
[01/17 13:52:48] d2.evaluation.evaluator INFO: Inference done 952/1093. Dataloading: 0.0133 s/iter. Inference: 0.1736 s/iter. Eval: 0.2113 s/iter. Total: 0.3982 s/iter. ETA=0:00:56
[01/17 13:52:53] d2.evaluation.evaluator INFO: Inference done 964/1093. Dataloading: 0.0133 s/iter. Inference: 0.1736 s/iter. Eval: 0.2116 s/iter. Total: 0.3985 s/iter. ETA=0:00:51
[01/17 13:52:58] d2.evaluation.evaluator INFO: Inference done 977/1093. Dataloading: 0.0132 s/iter. Inference: 0.1736 s/iter. Eval: 0.2116 s/iter. Total: 0.3986 s/iter. ETA=0:00:46
[01/17 13:53:03] d2.evaluation.evaluator INFO: Inference done 992/1093. Dataloading: 0.0132 s/iter. Inference: 0.1734 s/iter. Eval: 0.2111 s/iter. Total: 0.3979 s/iter. ETA=0:00:40
[01/17 13:53:09] d2.evaluation.evaluator INFO: Inference done 1005/1093. Dataloading: 0.0132 s/iter. Inference: 0.1735 s/iter. Eval: 0.2114 s/iter. Total: 0.3982 s/iter. ETA=0:00:35
[01/17 13:53:14] d2.evaluation.evaluator INFO: Inference done 1018/1093. Dataloading: 0.0132 s/iter. Inference: 0.1734 s/iter. Eval: 0.2113 s/iter. Total: 0.3981 s/iter. ETA=0:00:29
[01/17 13:53:19] d2.evaluation.evaluator INFO: Inference done 1031/1093. Dataloading: 0.0132 s/iter. Inference: 0.1735 s/iter. Eval: 0.2114 s/iter. Total: 0.3981 s/iter. ETA=0:00:24
[01/17 13:53:24] d2.evaluation.evaluator INFO: Inference done 1043/1093. Dataloading: 0.0132 s/iter. Inference: 0.1734 s/iter. Eval: 0.2117 s/iter. Total: 0.3984 s/iter. ETA=0:00:19
[01/17 13:53:29] d2.evaluation.evaluator INFO: Inference done 1058/1093. Dataloading: 0.0132 s/iter. Inference: 0.1731 s/iter. Eval: 0.2113 s/iter. Total: 0.3977 s/iter. ETA=0:00:13
[01/17 13:53:34] d2.evaluation.evaluator INFO: Inference done 1076/1093. Dataloading: 0.0131 s/iter. Inference: 0.1725 s/iter. Eval: 0.2102 s/iter. Total: 0.3959 s/iter. ETA=0:00:06
[01/17 13:53:40] d2.evaluation.evaluator INFO: Inference done 1091/1093. Dataloading: 0.0130 s/iter. Inference: 0.1722 s/iter. Eval: 0.2097 s/iter. Total: 0.3951 s/iter. ETA=0:00:00
[01/17 13:53:41] d2.evaluation.evaluator INFO: Total inference time: 0:07:10.123225 (0.395334 s / iter per device, on 4 devices)
[01/17 13:53:41] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:07 (0.172156 s / iter per device, on 4 devices)
[01/17 13:54:05] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 15.295098047569006, 'fwIoU': 43.24092318672838, 'IoU-1': nan, 'IoU-2': 95.26819583711246, 'IoU-3': 45.008070840637174, 'IoU-4': 58.74706486381687, 'IoU-5': 51.61459640567793, 'IoU-6': 44.542248978231825, 'IoU-7': 40.66390705252044, 'IoU-8': 34.161991072918404, 'IoU-9': 17.159691426416384, 'IoU-10': 24.506710181949757, 'IoU-11': 33.6131061560064, 'IoU-12': 48.33959831842209, 'IoU-13': 48.43570053497048, 'IoU-14': 50.749642088462046, 'IoU-15': 52.80652473861744, 'IoU-16': 52.62150333676915, 'IoU-17': 51.1756367936977, 'IoU-18': 48.886348993021386, 'IoU-19': 49.1046457102443, 'IoU-20': 48.89655844612797, 'IoU-21': 50.0797398719927, 'IoU-22': 49.39790376624211, 'IoU-23': 50.135633576631186, 'IoU-24': 46.756808817780836, 'IoU-25': 48.52830572622871, 'IoU-26': 45.87191753100146, 'IoU-27': 45.3377427276852, 'IoU-28': 48.00500755981578, 'IoU-29': 45.53468377743898, 'IoU-30': 46.23059524526821, 'IoU-31': 45.302942296006954, 'IoU-32': 47.58074683492781, 'IoU-33': 46.30712468478683, 'IoU-34': 44.78181758867902, 'IoU-35': 44.196568007315115, 'IoU-36': 45.1937541160067, 'IoU-37': 44.880953480076045, 'IoU-38': 44.10038316826835, 'IoU-39': 43.09362965129463, 'IoU-40': 41.52760490353174, 'IoU-41': 41.90573573615385, 'IoU-42': 39.99982665301353, 'IoU-43': 40.313620408524706, 'IoU-44': 39.96544648220288, 'IoU-45': 38.86910443959812, 'IoU-46': 38.81978631312214, 'IoU-47': 37.35114088542827, 'IoU-48': 35.807702170407154, 'IoU-49': 35.953389747650164, 'IoU-50': 35.88069706869083, 'IoU-51': 35.3222715064059, 'IoU-52': 33.83370444786969, 'IoU-53': 32.72719486640793, 'IoU-54': 32.3029423910229, 'IoU-55': 33.15539905574328, 'IoU-56': 31.998860586119697, 'IoU-57': 30.311819982491617, 'IoU-58': 30.337120584401163, 'IoU-59': 28.570954293367993, 'IoU-60': 28.528072946864956, 'IoU-61': 27.222149932745182, 'IoU-62': 26.542592136504755, 'IoU-63': 27.131462002013645, 'IoU-64': 27.0465571647812, 'IoU-65': 26.04125134098421, 'IoU-66': 23.77885810259951, 'IoU-67': 23.99708794540123, 'IoU-68': 22.063603191288692, 'IoU-69': 22.697712758826206, 'IoU-70': 22.226649358742982, 'IoU-71': 22.330044272291826, 'IoU-72': 19.573571773058834, 'IoU-73': 20.783928114930468, 'IoU-74': 18.925737749865725, 'IoU-75': 20.400986866615163, 'IoU-76': 19.081639117725494, 'IoU-77': 19.606347950482565, 'IoU-78': 19.15861900775954, 'IoU-79': 19.086975076531214, 'IoU-80': 17.725868317690725, 'IoU-81': 17.96235342169334, 'IoU-82': 19.361896045577044, 'IoU-83': 17.14988544884775, 'IoU-84': 17.78184859543155, 'IoU-85': 19.11711802610717, 'IoU-86': 17.502154392225286, 'IoU-87': 18.476587512943617, 'IoU-88': 16.87579846101282, 'IoU-89': 17.516283365189384, 'IoU-90': 17.254900995949267, 'IoU-91': 18.337943524262386, 'IoU-92': 17.439091851360796, 'IoU-93': 17.40175982942258, 'IoU-94': 17.82436200936127, 'IoU-95': 16.474564690570595, 'IoU-96': 17.300709351030235, 'IoU-97': 16.850451846114122, 'IoU-98': 17.677428108567405, 'IoU-99': 15.512685030631726, 'IoU-100': 16.244669873821106, 'IoU-101': 16.57356491340856, 'IoU-102': 16.532560327760862, 'IoU-103': 15.548029132763345, 'IoU-104': 14.858332898991799, 'IoU-105': 14.422816417422476, 'IoU-106': 15.04007565572599, 'IoU-107': 14.093437144864856, 'IoU-108': 13.427930985599392, 'IoU-109': 14.980922113046724, 'IoU-110': 13.755581774025602, 'IoU-111': 13.997836385467558, 'IoU-112': 13.64893247224688, 'IoU-113': 13.162008387861665, 'IoU-114': 12.872702667552929, 'IoU-115': 14.375878069926642, 'IoU-116': 10.917085418859429, 'IoU-117': 12.571644074369212, 'IoU-118': 11.721436836052957, 'IoU-119': 11.624602330979467, 'IoU-120': 11.353105606477607, 'IoU-121': 11.860878557482948, 'IoU-122': 10.871518462573587, 'IoU-123': 11.631759505456253, 'IoU-124': 10.840063327661177, 'IoU-125': 7.549047234631129, 'IoU-126': 10.931058493202874, 'IoU-127': 9.574013733278855, 'IoU-128': 8.754477404244092, 'IoU-129': 6.321970563562876, 'IoU-130': 9.351942525225079, 'IoU-131': 7.559473184240124, 'IoU-132': 8.40226803217441, 'IoU-133': 7.660713305001231, 'IoU-134': 6.204680093478174, 'IoU-135': 8.240305729753938, 'IoU-136': 6.684718522416612, 'IoU-137': 6.43901521025843, 'IoU-138': 5.442389052492933, 'IoU-139': 4.378866439743289, 'IoU-140': 6.327809871660641, 'IoU-141': 7.073469823307821, 'IoU-142': 3.864510332654962, 'IoU-143': 3.730083905740136, 'IoU-144': 5.482477804846679, 'IoU-145': 5.7005362179854915, 'IoU-146': 4.806044601436318, 'IoU-147': 5.409285920582003, 'IoU-148': 5.044690710566378, 'IoU-149': 4.154030575190615, 'IoU-150': 6.026709716030104, 'IoU-151': 5.660642249833852, 'IoU-152': 2.477572147429354, 'IoU-153': 4.330913299720448, 'IoU-154': 3.63635422389521, 'IoU-155': 3.8579248340224948, 'IoU-156': 1.8273271069193928, 'IoU-157': 2.5168932483921496, 'IoU-158': 3.151260857642283, 'IoU-159': 1.852862571458588, 'IoU-160': 1.806749713790206, 'IoU-161': 2.02637283132163, 'IoU-162': 1.2929341693630734, 'IoU-163': 2.332053030840435, 'IoU-164': 1.587242770547296, 'IoU-165': 1.1133762638303142, 'IoU-166': 1.2812150952859152, 'IoU-167': 2.5397227125698842, 'IoU-168': 1.6657498786604108, 'IoU-169': 1.273750570942694, 'IoU-170': 1.4478604816330969, 'IoU-171': 1.1068257905458745, 'IoU-172': 0.8590464405168625, 'IoU-173': 1.3050142359519377, 'IoU-174': 1.1485651799076668, 'IoU-175': 0.3433787352887447, 'IoU-176': 1.7309092028560056, 'IoU-177': 1.894666770775296, 'IoU-178': 1.5573879991865471, 'IoU-179': 2.8116423574633735, 'IoU-180': 1.4104246952258268, 'IoU-181': 1.612961246174054, 'IoU-182': 0.982037508957001, 'IoU-183': 2.242788275630945, 'IoU-184': 0.021230344623876754, 'IoU-185': 0.7359699181906183, 'IoU-186': 0.6043440473168894, 'IoU-187': 1.3795128398482674, 'IoU-188': 0.08566215977627573, 'IoU-189': 1.1784146013484709, 'IoU-190': 1.6347363995102147, 'IoU-191': 0.6856856998046943, 'IoU-192': 2.233940701791712, 'IoU-193': 1.3291581704105755, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 23.123806708743803, 'pACC': 57.587924086444964, 'ACC-1': nan, 'ACC-2': 98.31097882176478, 'ACC-3': 64.2939712094856, 'ACC-4': 75.62016032314195, 'ACC-5': 69.49165724645552, 'ACC-6': 62.6576794819796, 'ACC-7': 56.99274106343679, 'ACC-8': 47.777036364498215, 'ACC-9': 20.31070425631852, 'ACC-10': 30.68916300190821, 'ACC-11': 45.764415589921086, 'ACC-12': 63.59354119866557, 'ACC-13': 67.74323860937406, 'ACC-14': 68.84754950805413, 'ACC-15': 71.10945757106886, 'ACC-16': 70.61585423899508, 'ACC-17': 70.24951200556741, 'ACC-18': 66.08142552089316, 'ACC-19': 66.66247111704344, 'ACC-20': 66.45138326884991, 'ACC-21': 65.90424065706031, 'ACC-22': 66.47879288107139, 'ACC-23': 66.90553516811141, 'ACC-24': 62.790098435077454, 'ACC-25': 67.10208047556222, 'ACC-26': 64.89322942440171, 'ACC-27': 60.88850234876343, 'ACC-28': 64.10633180207103, 'ACC-29': 65.94969474839982, 'ACC-30': 62.044934615967975, 'ACC-31': 62.22338821554931, 'ACC-32': 62.85973370268722, 'ACC-33': 62.2692438890778, 'ACC-34': 60.26871525609906, 'ACC-35': 59.86766566499598, 'ACC-36': 61.99963220792822, 'ACC-37': 61.80829140545549, 'ACC-38': 60.86311439191107, 'ACC-39': 60.25783121658896, 'ACC-40': 59.205931217610306, 'ACC-41': 59.11815700441655, 'ACC-42': 56.88127441167092, 'ACC-43': 56.487750330277464, 'ACC-44': 58.73677559125539, 'ACC-45': 55.796297501954605, 'ACC-46': 55.923749583140456, 'ACC-47': 54.43550527673812, 'ACC-48': 53.21666601843287, 'ACC-49': 53.684788627265064, 'ACC-50': 52.89474397510154, 'ACC-51': 51.012593810271625, 'ACC-52': 49.47387309443969, 'ACC-53': 50.80339990007672, 'ACC-54': 48.39136256248804, 'ACC-55': 53.24414716843141, 'ACC-56': 47.222428345670565, 'ACC-57': 47.35843857921022, 'ACC-58': 46.87376858357615, 'ACC-59': 44.707041623687026, 'ACC-60': 44.260207329481496, 'ACC-61': 41.962587150824405, 'ACC-62': 42.35164463492029, 'ACC-63': 41.7407745966942, 'ACC-64': 42.536369263960026, 'ACC-65': 40.95394028342612, 'ACC-66': 36.71871870188591, 'ACC-67': 38.08619964342234, 'ACC-68': 35.13420006123905, 'ACC-69': 36.89076954625216, 'ACC-70': 36.35596675323553, 'ACC-71': 35.47245705542393, 'ACC-72': 30.581490671792988, 'ACC-73': 35.50108602009716, 'ACC-74': 31.591036560652956, 'ACC-75': 36.26714502498669, 'ACC-76': 32.19990630518084, 'ACC-77': 32.39617183895848, 'ACC-78': 32.93068482485685, 'ACC-79': 31.501763731226042, 'ACC-80': 29.768739224513602, 'ACC-81': 33.01063524751275, 'ACC-82': 33.871738704090895, 'ACC-83': 28.277747762201933, 'ACC-84': 30.7205757456334, 'ACC-85': 34.167716436394045, 'ACC-86': 27.365476493017315, 'ACC-87': 34.98658533476186, 'ACC-88': 26.685884843626585, 'ACC-89': 33.87924709097504, 'ACC-90': 27.856641963799166, 'ACC-91': 31.6746077898041, 'ACC-92': 29.754251101063822, 'ACC-93': 28.38203577470505, 'ACC-94': 30.409723993140158, 'ACC-95': 28.434944812200918, 'ACC-96': 29.269475460306165, 'ACC-97': 27.17036791310518, 'ACC-98': 31.451323091294057, 'ACC-99': 26.943624881503624, 'ACC-100': 24.642454105847932, 'ACC-101': 28.36950906907287, 'ACC-102': 27.413441351099006, 'ACC-103': 27.631264159492524, 'ACC-104': 26.320936053650705, 'ACC-105': 24.82331964290106, 'ACC-106': 28.862272512859622, 'ACC-107': 21.84667409808794, 'ACC-108': 23.854701077851605, 'ACC-109': 27.733349325549472, 'ACC-110': 22.34670431463267, 'ACC-111': 22.826721554970554, 'ACC-112': 23.66337071172507, 'ACC-113': 23.888300538057404, 'ACC-114': 19.80479972812753, 'ACC-115': 25.36147087624367, 'ACC-116': 18.68593357689474, 'ACC-117': 23.189685123669413, 'ACC-118': 21.200014144192, 'ACC-119': 25.874156116416152, 'ACC-120': 18.977095366704454, 'ACC-121': 21.72395344669457, 'ACC-122': 20.219092516579785, 'ACC-123': 20.934642927906253, 'ACC-124': 22.12529284404611, 'ACC-125': 13.087095473285668, 'ACC-126': 20.55053335842317, 'ACC-127': 20.033424712429074, 'ACC-128': 16.975733349531346, 'ACC-129': 9.908878131361364, 'ACC-130': 18.811415203327897, 'ACC-131': 12.943352517651551, 'ACC-132': 20.326534030077497, 'ACC-133': 16.640695948232818, 'ACC-134': 9.556635162995775, 'ACC-135': 16.53935372684188, 'ACC-136': 12.25664158257796, 'ACC-137': 13.45013743943347, 'ACC-138': 9.561927528035433, 'ACC-139': 6.903127141660533, 'ACC-140': 9.880033088666313, 'ACC-141': 19.6547217349388, 'ACC-142': 5.310289343066185, 'ACC-143': 6.8931698835258555, 'ACC-144': 10.572106109252525, 'ACC-145': 9.506498194945848, 'ACC-146': 10.145039021445024, 'ACC-147': 8.951497566882182, 'ACC-148': 11.91652978737826, 'ACC-149': 8.323645906899952, 'ACC-150': 12.152902078860608, 'ACC-151': 11.683234157694997, 'ACC-152': 3.9576545669057204, 'ACC-153': 8.40192781037157, 'ACC-154': 6.434938346250389, 'ACC-155': 6.888490454602563, 'ACC-156': 3.9278908847984657, 'ACC-157': 4.173308253666608, 'ACC-158': 5.098722937448025, 'ACC-159': 3.0616003646879335, 'ACC-160': 2.8593341971776676, 'ACC-161': 2.96198479844365, 'ACC-162': 1.895376561811069, 'ACC-163': 6.406705127327797, 'ACC-164': 2.2263428146023876, 'ACC-165': 1.6252905418903536, 'ACC-166': 1.768908003443161, 'ACC-167': 4.683415735618772, 'ACC-168': 2.2356674952707034, 'ACC-169': 1.7357057458330403, 'ACC-170': 2.196377214670122, 'ACC-171': 1.435680111305138, 'ACC-172': 1.055823838327434, 'ACC-173': 3.100159276956699, 'ACC-174': 2.3160604019521887, 'ACC-175': 0.4134349585539842, 'ACC-176': 2.9106325074554564, 'ACC-177': 5.193761609008591, 'ACC-178': 3.3777536637083867, 'ACC-179': 12.003801348628869, 'ACC-180': 4.216230692601257, 'ACC-181': 6.643257475701852, 'ACC-182': 1.7269843976872479, 'ACC-183': 5.359059345303952, 'ACC-184': 0.02165974888817213, 'ACC-185': 0.9933563984462888, 'ACC-186': 0.7259029327125166, 'ACC-187': 3.98170027832007, 'ACC-188': 0.0949516181640271, 'ACC-189': 2.530668657439167, 'ACC-190': 11.817848074943328, 'ACC-191': 1.9105610111405724, 'ACC-192': 4.987928221859706, 'ACC-193': 11.508584462222423, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 13:54:05] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 13:54:05] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 13:54:05] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 13:54:05] d2.evaluation.testing INFO: copypaste: 15.2951,43.2409,23.1238,57.5879
[01/17 13:54:05] d2.utils.events INFO:  eta: 1 day, 5:00:45  iter: 21999  total_loss: 34.51  loss_ce: 0.253  loss_mask: 0.3556  loss_dice: 2.787  loss_ce_0: 0.5444  loss_mask_0: 0.3553  loss_dice_0: 2.929  loss_ce_1: 0.2728  loss_mask_1: 0.3565  loss_dice_1: 2.837  loss_ce_2: 0.2804  loss_mask_2: 0.3535  loss_dice_2: 2.804  loss_ce_3: 0.262  loss_mask_3: 0.3546  loss_dice_3: 2.793  loss_ce_4: 0.2608  loss_mask_4: 0.3542  loss_dice_4: 2.789  loss_ce_5: 0.2486  loss_mask_5: 0.3552  loss_dice_5: 2.796  loss_ce_6: 0.2592  loss_mask_6: 0.3546  loss_dice_6: 2.787  loss_ce_7: 0.2515  loss_mask_7: 0.3553  loss_dice_7: 2.781  loss_ce_8: 0.2432  loss_mask_8: 0.3553  loss_dice_8: 2.79  time: 1.5364  data_time: 0.1104  lr: 7.7704e-06  max_mem: 21478M
[01/17 13:54:36] d2.utils.events INFO:  eta: 1 day, 4:59:57  iter: 22019  total_loss: 35.51  loss_ce: 0.2726  loss_mask: 0.3608  loss_dice: 2.885  loss_ce_0: 0.5582  loss_mask_0: 0.3573  loss_dice_0: 2.992  loss_ce_1: 0.2957  loss_mask_1: 0.3694  loss_dice_1: 2.929  loss_ce_2: 0.3023  loss_mask_2: 0.362  loss_dice_2: 2.907  loss_ce_3: 0.2938  loss_mask_3: 0.3629  loss_dice_3: 2.895  loss_ce_4: 0.2845  loss_mask_4: 0.3615  loss_dice_4: 2.891  loss_ce_5: 0.2713  loss_mask_5: 0.3618  loss_dice_5: 2.894  loss_ce_6: 0.2782  loss_mask_6: 0.3619  loss_dice_6: 2.884  loss_ce_7: 0.2825  loss_mask_7: 0.3617  loss_dice_7: 2.875  loss_ce_8: 0.2721  loss_mask_8: 0.3617  loss_dice_8: 2.887  time: 1.5364  data_time: 0.0994  lr: 7.7684e-06  max_mem: 21478M
[01/17 13:55:07] d2.utils.events INFO:  eta: 1 day, 4:59:27  iter: 22039  total_loss: 35.28  loss_ce: 0.2393  loss_mask: 0.3704  loss_dice: 2.868  loss_ce_0: 0.5534  loss_mask_0: 0.3728  loss_dice_0: 2.972  loss_ce_1: 0.2673  loss_mask_1: 0.3753  loss_dice_1: 2.892  loss_ce_2: 0.2776  loss_mask_2: 0.3727  loss_dice_2: 2.877  loss_ce_3: 0.2635  loss_mask_3: 0.3716  loss_dice_3: 2.867  loss_ce_4: 0.2658  loss_mask_4: 0.3702  loss_dice_4: 2.863  loss_ce_5: 0.2555  loss_mask_5: 0.3717  loss_dice_5: 2.878  loss_ce_6: 0.2374  loss_mask_6: 0.3712  loss_dice_6: 2.873  loss_ce_7: 0.2418  loss_mask_7: 0.3693  loss_dice_7: 2.874  loss_ce_8: 0.2388  loss_mask_8: 0.371  loss_dice_8: 2.864  time: 1.5364  data_time: 0.1004  lr: 7.7663e-06  max_mem: 21478M
[01/17 13:55:38] d2.utils.events INFO:  eta: 1 day, 4:58:40  iter: 22059  total_loss: 35.05  loss_ce: 0.2751  loss_mask: 0.3639  loss_dice: 2.829  loss_ce_0: 0.5506  loss_mask_0: 0.3632  loss_dice_0: 2.933  loss_ce_1: 0.3126  loss_mask_1: 0.3726  loss_dice_1: 2.856  loss_ce_2: 0.3296  loss_mask_2: 0.3707  loss_dice_2: 2.842  loss_ce_3: 0.3011  loss_mask_3: 0.3659  loss_dice_3: 2.84  loss_ce_4: 0.2914  loss_mask_4: 0.3621  loss_dice_4: 2.839  loss_ce_5: 0.2787  loss_mask_5: 0.364  loss_dice_5: 2.838  loss_ce_6: 0.2729  loss_mask_6: 0.3626  loss_dice_6: 2.823  loss_ce_7: 0.2676  loss_mask_7: 0.3638  loss_dice_7: 2.834  loss_ce_8: 0.2737  loss_mask_8: 0.3651  loss_dice_8: 2.834  time: 1.5364  data_time: 0.1120  lr: 7.7643e-06  max_mem: 21478M
[01/17 13:56:09] d2.utils.events INFO:  eta: 1 day, 4:58:04  iter: 22079  total_loss: 36.25  loss_ce: 0.2975  loss_mask: 0.3795  loss_dice: 2.863  loss_ce_0: 0.5739  loss_mask_0: 0.387  loss_dice_0: 2.988  loss_ce_1: 0.3414  loss_mask_1: 0.3864  loss_dice_1: 2.904  loss_ce_2: 0.3199  loss_mask_2: 0.383  loss_dice_2: 2.889  loss_ce_3: 0.3045  loss_mask_3: 0.3815  loss_dice_3: 2.865  loss_ce_4: 0.2884  loss_mask_4: 0.3782  loss_dice_4: 2.875  loss_ce_5: 0.2959  loss_mask_5: 0.3795  loss_dice_5: 2.876  loss_ce_6: 0.286  loss_mask_6: 0.3799  loss_dice_6: 2.866  loss_ce_7: 0.3042  loss_mask_7: 0.3792  loss_dice_7: 2.858  loss_ce_8: 0.3018  loss_mask_8: 0.3804  loss_dice_8: 2.866  time: 1.5364  data_time: 0.0929  lr: 7.7622e-06  max_mem: 21478M
[01/17 13:56:40] d2.utils.events INFO:  eta: 1 day, 4:57:34  iter: 22099  total_loss: 35.45  loss_ce: 0.277  loss_mask: 0.3667  loss_dice: 2.839  loss_ce_0: 0.6118  loss_mask_0: 0.3655  loss_dice_0: 2.954  loss_ce_1: 0.3213  loss_mask_1: 0.3686  loss_dice_1: 2.879  loss_ce_2: 0.2988  loss_mask_2: 0.3696  loss_dice_2: 2.863  loss_ce_3: 0.2859  loss_mask_3: 0.3655  loss_dice_3: 2.843  loss_ce_4: 0.2911  loss_mask_4: 0.3651  loss_dice_4: 2.838  loss_ce_5: 0.2738  loss_mask_5: 0.3663  loss_dice_5: 2.847  loss_ce_6: 0.2863  loss_mask_6: 0.3651  loss_dice_6: 2.831  loss_ce_7: 0.2752  loss_mask_7: 0.3666  loss_dice_7: 2.838  loss_ce_8: 0.2846  loss_mask_8: 0.3676  loss_dice_8: 2.837  time: 1.5364  data_time: 0.1031  lr: 7.7602e-06  max_mem: 21478M
[01/17 13:57:11] d2.utils.events INFO:  eta: 1 day, 4:56:15  iter: 22119  total_loss: 35.31  loss_ce: 0.2664  loss_mask: 0.3663  loss_dice: 2.846  loss_ce_0: 0.5693  loss_mask_0: 0.3645  loss_dice_0: 2.945  loss_ce_1: 0.3029  loss_mask_1: 0.3708  loss_dice_1: 2.879  loss_ce_2: 0.2984  loss_mask_2: 0.3637  loss_dice_2: 2.863  loss_ce_3: 0.2824  loss_mask_3: 0.3627  loss_dice_3: 2.848  loss_ce_4: 0.2802  loss_mask_4: 0.3643  loss_dice_4: 2.848  loss_ce_5: 0.269  loss_mask_5: 0.3636  loss_dice_5: 2.856  loss_ce_6: 0.2722  loss_mask_6: 0.3653  loss_dice_6: 2.841  loss_ce_7: 0.265  loss_mask_7: 0.3637  loss_dice_7: 2.85  loss_ce_8: 0.2694  loss_mask_8: 0.3663  loss_dice_8: 2.846  time: 1.5364  data_time: 0.1066  lr: 7.7581e-06  max_mem: 21478M
[01/17 13:57:41] d2.utils.events INFO:  eta: 1 day, 4:56:32  iter: 22139  total_loss: 34.92  loss_ce: 0.2699  loss_mask: 0.3713  loss_dice: 2.797  loss_ce_0: 0.5585  loss_mask_0: 0.3746  loss_dice_0: 2.909  loss_ce_1: 0.305  loss_mask_1: 0.3752  loss_dice_1: 2.838  loss_ce_2: 0.3005  loss_mask_2: 0.3741  loss_dice_2: 2.825  loss_ce_3: 0.2831  loss_mask_3: 0.3732  loss_dice_3: 2.808  loss_ce_4: 0.2987  loss_mask_4: 0.3711  loss_dice_4: 2.801  loss_ce_5: 0.2829  loss_mask_5: 0.3698  loss_dice_5: 2.815  loss_ce_6: 0.2636  loss_mask_6: 0.3704  loss_dice_6: 2.805  loss_ce_7: 0.2741  loss_mask_7: 0.3715  loss_dice_7: 2.789  loss_ce_8: 0.2649  loss_mask_8: 0.372  loss_dice_8: 2.8  time: 1.5364  data_time: 0.0936  lr: 7.756e-06  max_mem: 21478M
[01/17 13:58:12] d2.utils.events INFO:  eta: 1 day, 4:56:18  iter: 22159  total_loss: 35.05  loss_ce: 0.2769  loss_mask: 0.3636  loss_dice: 2.834  loss_ce_0: 0.5642  loss_mask_0: 0.3713  loss_dice_0: 2.926  loss_ce_1: 0.3163  loss_mask_1: 0.3721  loss_dice_1: 2.866  loss_ce_2: 0.2992  loss_mask_2: 0.3619  loss_dice_2: 2.843  loss_ce_3: 0.2794  loss_mask_3: 0.366  loss_dice_3: 2.829  loss_ce_4: 0.2919  loss_mask_4: 0.3649  loss_dice_4: 2.831  loss_ce_5: 0.2777  loss_mask_5: 0.3663  loss_dice_5: 2.834  loss_ce_6: 0.2611  loss_mask_6: 0.3654  loss_dice_6: 2.827  loss_ce_7: 0.2672  loss_mask_7: 0.3646  loss_dice_7: 2.825  loss_ce_8: 0.2761  loss_mask_8: 0.3633  loss_dice_8: 2.822  time: 1.5364  data_time: 0.0953  lr: 7.754e-06  max_mem: 21478M
[01/17 13:58:43] d2.utils.events INFO:  eta: 1 day, 4:55:35  iter: 22179  total_loss: 34.68  loss_ce: 0.2644  loss_mask: 0.3659  loss_dice: 2.787  loss_ce_0: 0.5811  loss_mask_0: 0.3633  loss_dice_0: 2.887  loss_ce_1: 0.292  loss_mask_1: 0.3705  loss_dice_1: 2.825  loss_ce_2: 0.285  loss_mask_2: 0.369  loss_dice_2: 2.802  loss_ce_3: 0.27  loss_mask_3: 0.3683  loss_dice_3: 2.785  loss_ce_4: 0.262  loss_mask_4: 0.3672  loss_dice_4: 2.796  loss_ce_5: 0.2614  loss_mask_5: 0.3667  loss_dice_5: 2.794  loss_ce_6: 0.2617  loss_mask_6: 0.3677  loss_dice_6: 2.785  loss_ce_7: 0.2646  loss_mask_7: 0.3668  loss_dice_7: 2.785  loss_ce_8: 0.2697  loss_mask_8: 0.3652  loss_dice_8: 2.781  time: 1.5364  data_time: 0.0926  lr: 7.7519e-06  max_mem: 21478M
[01/17 13:59:14] d2.utils.events INFO:  eta: 1 day, 4:55:04  iter: 22199  total_loss: 35.4  loss_ce: 0.2522  loss_mask: 0.3718  loss_dice: 2.834  loss_ce_0: 0.5533  loss_mask_0: 0.3671  loss_dice_0: 2.945  loss_ce_1: 0.2869  loss_mask_1: 0.3723  loss_dice_1: 2.864  loss_ce_2: 0.2808  loss_mask_2: 0.3688  loss_dice_2: 2.846  loss_ce_3: 0.261  loss_mask_3: 0.3692  loss_dice_3: 2.831  loss_ce_4: 0.2524  loss_mask_4: 0.3678  loss_dice_4: 2.837  loss_ce_5: 0.2409  loss_mask_5: 0.3705  loss_dice_5: 2.835  loss_ce_6: 0.254  loss_mask_6: 0.3707  loss_dice_6: 2.825  loss_ce_7: 0.2512  loss_mask_7: 0.3713  loss_dice_7: 2.825  loss_ce_8: 0.2496  loss_mask_8: 0.3715  loss_dice_8: 2.826  time: 1.5364  data_time: 0.1058  lr: 7.7499e-06  max_mem: 21478M
[01/17 13:59:44] d2.utils.events INFO:  eta: 1 day, 4:54:33  iter: 22219  total_loss: 34.73  loss_ce: 0.2735  loss_mask: 0.3746  loss_dice: 2.757  loss_ce_0: 0.5328  loss_mask_0: 0.3764  loss_dice_0: 2.884  loss_ce_1: 0.3026  loss_mask_1: 0.3834  loss_dice_1: 2.799  loss_ce_2: 0.2943  loss_mask_2: 0.3791  loss_dice_2: 2.783  loss_ce_3: 0.2946  loss_mask_3: 0.3771  loss_dice_3: 2.762  loss_ce_4: 0.2846  loss_mask_4: 0.3753  loss_dice_4: 2.761  loss_ce_5: 0.2581  loss_mask_5: 0.3753  loss_dice_5: 2.766  loss_ce_6: 0.2455  loss_mask_6: 0.3747  loss_dice_6: 2.759  loss_ce_7: 0.2648  loss_mask_7: 0.3733  loss_dice_7: 2.762  loss_ce_8: 0.2627  loss_mask_8: 0.3742  loss_dice_8: 2.763  time: 1.5364  data_time: 0.0863  lr: 7.7478e-06  max_mem: 21478M
[01/17 14:00:15] d2.utils.events INFO:  eta: 1 day, 4:54:03  iter: 22239  total_loss: 34.54  loss_ce: 0.2611  loss_mask: 0.3591  loss_dice: 2.783  loss_ce_0: 0.5828  loss_mask_0: 0.3573  loss_dice_0: 2.882  loss_ce_1: 0.3038  loss_mask_1: 0.3614  loss_dice_1: 2.823  loss_ce_2: 0.292  loss_mask_2: 0.3588  loss_dice_2: 2.791  loss_ce_3: 0.2713  loss_mask_3: 0.3598  loss_dice_3: 2.781  loss_ce_4: 0.2744  loss_mask_4: 0.3573  loss_dice_4: 2.782  loss_ce_5: 0.2605  loss_mask_5: 0.3578  loss_dice_5: 2.79  loss_ce_6: 0.2563  loss_mask_6: 0.3587  loss_dice_6: 2.781  loss_ce_7: 0.257  loss_mask_7: 0.3573  loss_dice_7: 2.794  loss_ce_8: 0.2593  loss_mask_8: 0.3579  loss_dice_8: 2.783  time: 1.5364  data_time: 0.0880  lr: 7.7458e-06  max_mem: 21478M
[01/17 14:00:45] d2.utils.events INFO:  eta: 1 day, 4:54:17  iter: 22259  total_loss: 34.66  loss_ce: 0.2599  loss_mask: 0.3674  loss_dice: 2.789  loss_ce_0: 0.5468  loss_mask_0: 0.3686  loss_dice_0: 2.876  loss_ce_1: 0.2798  loss_mask_1: 0.3728  loss_dice_1: 2.808  loss_ce_2: 0.2909  loss_mask_2: 0.3674  loss_dice_2: 2.798  loss_ce_3: 0.2558  loss_mask_3: 0.3653  loss_dice_3: 2.79  loss_ce_4: 0.2673  loss_mask_4: 0.366  loss_dice_4: 2.787  loss_ce_5: 0.2538  loss_mask_5: 0.3659  loss_dice_5: 2.785  loss_ce_6: 0.2453  loss_mask_6: 0.3679  loss_dice_6: 2.783  loss_ce_7: 0.2416  loss_mask_7: 0.3671  loss_dice_7: 2.78  loss_ce_8: 0.2473  loss_mask_8: 0.3663  loss_dice_8: 2.78  time: 1.5364  data_time: 0.1043  lr: 7.7437e-06  max_mem: 21478M
[01/17 14:01:16] d2.utils.events INFO:  eta: 1 day, 4:55:05  iter: 22279  total_loss: 34.91  loss_ce: 0.264  loss_mask: 0.3645  loss_dice: 2.806  loss_ce_0: 0.5516  loss_mask_0: 0.3678  loss_dice_0: 2.924  loss_ce_1: 0.3039  loss_mask_1: 0.3704  loss_dice_1: 2.848  loss_ce_2: 0.2943  loss_mask_2: 0.3663  loss_dice_2: 2.829  loss_ce_3: 0.2806  loss_mask_3: 0.3641  loss_dice_3: 2.803  loss_ce_4: 0.2688  loss_mask_4: 0.3641  loss_dice_4: 2.813  loss_ce_5: 0.2671  loss_mask_5: 0.3645  loss_dice_5: 2.807  loss_ce_6: 0.2651  loss_mask_6: 0.366  loss_dice_6: 2.803  loss_ce_7: 0.2646  loss_mask_7: 0.3641  loss_dice_7: 2.809  loss_ce_8: 0.2705  loss_mask_8: 0.3648  loss_dice_8: 2.799  time: 1.5364  data_time: 0.0901  lr: 7.7416e-06  max_mem: 21478M
[01/17 14:01:47] d2.utils.events INFO:  eta: 1 day, 4:55:04  iter: 22299  total_loss: 34.62  loss_ce: 0.2618  loss_mask: 0.3679  loss_dice: 2.76  loss_ce_0: 0.5855  loss_mask_0: 0.367  loss_dice_0: 2.877  loss_ce_1: 0.3115  loss_mask_1: 0.3724  loss_dice_1: 2.798  loss_ce_2: 0.3036  loss_mask_2: 0.369  loss_dice_2: 2.762  loss_ce_3: 0.2873  loss_mask_3: 0.3693  loss_dice_3: 2.759  loss_ce_4: 0.2866  loss_mask_4: 0.3699  loss_dice_4: 2.761  loss_ce_5: 0.2755  loss_mask_5: 0.3695  loss_dice_5: 2.747  loss_ce_6: 0.2696  loss_mask_6: 0.3678  loss_dice_6: 2.76  loss_ce_7: 0.2703  loss_mask_7: 0.3699  loss_dice_7: 2.757  loss_ce_8: 0.2713  loss_mask_8: 0.3683  loss_dice_8: 2.751  time: 1.5364  data_time: 0.0978  lr: 7.7396e-06  max_mem: 21478M
[01/17 14:02:17] d2.utils.events INFO:  eta: 1 day, 4:55:02  iter: 22319  total_loss: 35.45  loss_ce: 0.2638  loss_mask: 0.37  loss_dice: 2.831  loss_ce_0: 0.5841  loss_mask_0: 0.3658  loss_dice_0: 2.953  loss_ce_1: 0.3054  loss_mask_1: 0.3685  loss_dice_1: 2.875  loss_ce_2: 0.3007  loss_mask_2: 0.3675  loss_dice_2: 2.846  loss_ce_3: 0.2791  loss_mask_3: 0.3666  loss_dice_3: 2.839  loss_ce_4: 0.2758  loss_mask_4: 0.3681  loss_dice_4: 2.832  loss_ce_5: 0.2874  loss_mask_5: 0.3689  loss_dice_5: 2.835  loss_ce_6: 0.2808  loss_mask_6: 0.368  loss_dice_6: 2.836  loss_ce_7: 0.2632  loss_mask_7: 0.3686  loss_dice_7: 2.827  loss_ce_8: 0.271  loss_mask_8: 0.3691  loss_dice_8: 2.83  time: 1.5364  data_time: 0.0912  lr: 7.7375e-06  max_mem: 21478M
[01/17 14:02:48] d2.utils.events INFO:  eta: 1 day, 4:54:03  iter: 22339  total_loss: 35.3  loss_ce: 0.2696  loss_mask: 0.3672  loss_dice: 2.847  loss_ce_0: 0.6043  loss_mask_0: 0.367  loss_dice_0: 2.966  loss_ce_1: 0.3055  loss_mask_1: 0.3741  loss_dice_1: 2.894  loss_ce_2: 0.2912  loss_mask_2: 0.3685  loss_dice_2: 2.871  loss_ce_3: 0.2875  loss_mask_3: 0.3704  loss_dice_3: 2.85  loss_ce_4: 0.2863  loss_mask_4: 0.3695  loss_dice_4: 2.853  loss_ce_5: 0.2796  loss_mask_5: 0.3682  loss_dice_5: 2.857  loss_ce_6: 0.2738  loss_mask_6: 0.3703  loss_dice_6: 2.849  loss_ce_7: 0.2619  loss_mask_7: 0.3693  loss_dice_7: 2.843  loss_ce_8: 0.2613  loss_mask_8: 0.3681  loss_dice_8: 2.846  time: 1.5364  data_time: 0.0916  lr: 7.7355e-06  max_mem: 21478M
[01/17 14:03:20] d2.utils.events INFO:  eta: 1 day, 4:54:18  iter: 22359  total_loss: 35.36  loss_ce: 0.2734  loss_mask: 0.3677  loss_dice: 2.816  loss_ce_0: 0.5672  loss_mask_0: 0.3795  loss_dice_0: 2.927  loss_ce_1: 0.3046  loss_mask_1: 0.3801  loss_dice_1: 2.853  loss_ce_2: 0.2988  loss_mask_2: 0.3727  loss_dice_2: 2.83  loss_ce_3: 0.2697  loss_mask_3: 0.3712  loss_dice_3: 2.82  loss_ce_4: 0.2661  loss_mask_4: 0.3702  loss_dice_4: 2.824  loss_ce_5: 0.2638  loss_mask_5: 0.3692  loss_dice_5: 2.817  loss_ce_6: 0.2657  loss_mask_6: 0.3685  loss_dice_6: 2.817  loss_ce_7: 0.2626  loss_mask_7: 0.3672  loss_dice_7: 2.816  loss_ce_8: 0.2504  loss_mask_8: 0.3677  loss_dice_8: 2.818  time: 1.5364  data_time: 0.1152  lr: 7.7334e-06  max_mem: 21478M
[01/17 14:03:50] d2.utils.events INFO:  eta: 1 day, 4:52:28  iter: 22379  total_loss: 35.19  loss_ce: 0.2693  loss_mask: 0.3729  loss_dice: 2.794  loss_ce_0: 0.5923  loss_mask_0: 0.3731  loss_dice_0: 2.908  loss_ce_1: 0.3121  loss_mask_1: 0.3748  loss_dice_1: 2.837  loss_ce_2: 0.3109  loss_mask_2: 0.3735  loss_dice_2: 2.811  loss_ce_3: 0.2993  loss_mask_3: 0.3725  loss_dice_3: 2.796  loss_ce_4: 0.2831  loss_mask_4: 0.3714  loss_dice_4: 2.799  loss_ce_5: 0.2668  loss_mask_5: 0.3712  loss_dice_5: 2.801  loss_ce_6: 0.2714  loss_mask_6: 0.3721  loss_dice_6: 2.795  loss_ce_7: 0.2724  loss_mask_7: 0.3697  loss_dice_7: 2.792  loss_ce_8: 0.2675  loss_mask_8: 0.3716  loss_dice_8: 2.789  time: 1.5364  data_time: 0.0999  lr: 7.7313e-06  max_mem: 21478M
[01/17 14:04:20] d2.utils.events INFO:  eta: 1 day, 4:50:42  iter: 22399  total_loss: 34.18  loss_ce: 0.2393  loss_mask: 0.3589  loss_dice: 2.718  loss_ce_0: 0.5529  loss_mask_0: 0.3573  loss_dice_0: 2.832  loss_ce_1: 0.2801  loss_mask_1: 0.3639  loss_dice_1: 2.759  loss_ce_2: 0.2829  loss_mask_2: 0.3633  loss_dice_2: 2.732  loss_ce_3: 0.2599  loss_mask_3: 0.3601  loss_dice_3: 2.731  loss_ce_4: 0.2689  loss_mask_4: 0.3599  loss_dice_4: 2.731  loss_ce_5: 0.2547  loss_mask_5: 0.3615  loss_dice_5: 2.734  loss_ce_6: 0.2508  loss_mask_6: 0.3612  loss_dice_6: 2.723  loss_ce_7: 0.2327  loss_mask_7: 0.3592  loss_dice_7: 2.721  loss_ce_8: 0.2453  loss_mask_8: 0.359  loss_dice_8: 2.724  time: 1.5364  data_time: 0.0896  lr: 7.7293e-06  max_mem: 21478M
[01/17 14:04:51] d2.utils.events INFO:  eta: 1 day, 4:50:18  iter: 22419  total_loss: 34.61  loss_ce: 0.2559  loss_mask: 0.3657  loss_dice: 2.762  loss_ce_0: 0.5615  loss_mask_0: 0.3584  loss_dice_0: 2.879  loss_ce_1: 0.2862  loss_mask_1: 0.3659  loss_dice_1: 2.808  loss_ce_2: 0.2792  loss_mask_2: 0.3654  loss_dice_2: 2.781  loss_ce_3: 0.2732  loss_mask_3: 0.3653  loss_dice_3: 2.766  loss_ce_4: 0.267  loss_mask_4: 0.3654  loss_dice_4: 2.767  loss_ce_5: 0.2634  loss_mask_5: 0.3665  loss_dice_5: 2.766  loss_ce_6: 0.2534  loss_mask_6: 0.3652  loss_dice_6: 2.77  loss_ce_7: 0.2442  loss_mask_7: 0.3665  loss_dice_7: 2.76  loss_ce_8: 0.2412  loss_mask_8: 0.3652  loss_dice_8: 2.763  time: 1.5364  data_time: 0.0980  lr: 7.7272e-06  max_mem: 21478M
[01/17 14:05:23] d2.utils.events INFO:  eta: 1 day, 4:50:22  iter: 22439  total_loss: 34.43  loss_ce: 0.2383  loss_mask: 0.3549  loss_dice: 2.751  loss_ce_0: 0.5681  loss_mask_0: 0.3528  loss_dice_0: 2.867  loss_ce_1: 0.2875  loss_mask_1: 0.3574  loss_dice_1: 2.79  loss_ce_2: 0.2805  loss_mask_2: 0.3567  loss_dice_2: 2.777  loss_ce_3: 0.2508  loss_mask_3: 0.3585  loss_dice_3: 2.764  loss_ce_4: 0.2533  loss_mask_4: 0.3576  loss_dice_4: 2.761  loss_ce_5: 0.2469  loss_mask_5: 0.3576  loss_dice_5: 2.762  loss_ce_6: 0.2443  loss_mask_6: 0.3573  loss_dice_6: 2.757  loss_ce_7: 0.2455  loss_mask_7: 0.3548  loss_dice_7: 2.755  loss_ce_8: 0.2466  loss_mask_8: 0.3564  loss_dice_8: 2.747  time: 1.5364  data_time: 0.0955  lr: 7.7252e-06  max_mem: 21478M
[01/17 14:05:54] d2.utils.events INFO:  eta: 1 day, 4:50:09  iter: 22459  total_loss: 34.33  loss_ce: 0.2514  loss_mask: 0.3545  loss_dice: 2.78  loss_ce_0: 0.5555  loss_mask_0: 0.3553  loss_dice_0: 2.891  loss_ce_1: 0.2863  loss_mask_1: 0.3609  loss_dice_1: 2.804  loss_ce_2: 0.3026  loss_mask_2: 0.3582  loss_dice_2: 2.782  loss_ce_3: 0.2716  loss_mask_3: 0.3566  loss_dice_3: 2.781  loss_ce_4: 0.2583  loss_mask_4: 0.3514  loss_dice_4: 2.784  loss_ce_5: 0.2515  loss_mask_5: 0.3529  loss_dice_5: 2.78  loss_ce_6: 0.2467  loss_mask_6: 0.3534  loss_dice_6: 2.784  loss_ce_7: 0.2439  loss_mask_7: 0.3548  loss_dice_7: 2.781  loss_ce_8: 0.2419  loss_mask_8: 0.3555  loss_dice_8: 2.774  time: 1.5364  data_time: 0.1050  lr: 7.7231e-06  max_mem: 21478M
[01/17 14:06:24] d2.utils.events INFO:  eta: 1 day, 4:49:21  iter: 22479  total_loss: 34.13  loss_ce: 0.2647  loss_mask: 0.3522  loss_dice: 2.762  loss_ce_0: 0.5494  loss_mask_0: 0.3493  loss_dice_0: 2.876  loss_ce_1: 0.2957  loss_mask_1: 0.3553  loss_dice_1: 2.788  loss_ce_2: 0.2836  loss_mask_2: 0.354  loss_dice_2: 2.774  loss_ce_3: 0.2737  loss_mask_3: 0.3532  loss_dice_3: 2.764  loss_ce_4: 0.25  loss_mask_4: 0.3526  loss_dice_4: 2.765  loss_ce_5: 0.2633  loss_mask_5: 0.3515  loss_dice_5: 2.767  loss_ce_6: 0.2481  loss_mask_6: 0.3505  loss_dice_6: 2.759  loss_ce_7: 0.2537  loss_mask_7: 0.3516  loss_dice_7: 2.769  loss_ce_8: 0.2453  loss_mask_8: 0.3508  loss_dice_8: 2.763  time: 1.5364  data_time: 0.0886  lr: 7.7211e-06  max_mem: 21478M
[01/17 14:06:55] d2.utils.events INFO:  eta: 1 day, 4:49:24  iter: 22499  total_loss: 34.79  loss_ce: 0.2493  loss_mask: 0.3721  loss_dice: 2.755  loss_ce_0: 0.5491  loss_mask_0: 0.3663  loss_dice_0: 2.872  loss_ce_1: 0.2847  loss_mask_1: 0.3729  loss_dice_1: 2.801  loss_ce_2: 0.2719  loss_mask_2: 0.3714  loss_dice_2: 2.782  loss_ce_3: 0.2625  loss_mask_3: 0.3696  loss_dice_3: 2.764  loss_ce_4: 0.2409  loss_mask_4: 0.3704  loss_dice_4: 2.763  loss_ce_5: 0.2369  loss_mask_5: 0.3698  loss_dice_5: 2.767  loss_ce_6: 0.2412  loss_mask_6: 0.3721  loss_dice_6: 2.756  loss_ce_7: 0.239  loss_mask_7: 0.3715  loss_dice_7: 2.762  loss_ce_8: 0.2464  loss_mask_8: 0.3707  loss_dice_8: 2.755  time: 1.5364  data_time: 0.0916  lr: 7.719e-06  max_mem: 21478M
[01/17 14:07:26] d2.utils.events INFO:  eta: 1 day, 4:49:17  iter: 22519  total_loss: 34.94  loss_ce: 0.2714  loss_mask: 0.3568  loss_dice: 2.809  loss_ce_0: 0.5815  loss_mask_0: 0.3559  loss_dice_0: 2.913  loss_ce_1: 0.2924  loss_mask_1: 0.3593  loss_dice_1: 2.843  loss_ce_2: 0.2892  loss_mask_2: 0.3558  loss_dice_2: 2.828  loss_ce_3: 0.2918  loss_mask_3: 0.3577  loss_dice_3: 2.813  loss_ce_4: 0.2927  loss_mask_4: 0.3578  loss_dice_4: 2.808  loss_ce_5: 0.2865  loss_mask_5: 0.3564  loss_dice_5: 2.816  loss_ce_6: 0.2636  loss_mask_6: 0.3556  loss_dice_6: 2.804  loss_ce_7: 0.2778  loss_mask_7: 0.3543  loss_dice_7: 2.817  loss_ce_8: 0.2813  loss_mask_8: 0.3547  loss_dice_8: 2.81  time: 1.5364  data_time: 0.0958  lr: 7.7169e-06  max_mem: 21478M
[01/17 14:07:57] d2.utils.events INFO:  eta: 1 day, 4:49:39  iter: 22539  total_loss: 35.24  loss_ce: 0.2632  loss_mask: 0.3553  loss_dice: 2.83  loss_ce_0: 0.5941  loss_mask_0: 0.3577  loss_dice_0: 2.948  loss_ce_1: 0.2991  loss_mask_1: 0.3602  loss_dice_1: 2.874  loss_ce_2: 0.3035  loss_mask_2: 0.3593  loss_dice_2: 2.851  loss_ce_3: 0.2855  loss_mask_3: 0.3581  loss_dice_3: 2.843  loss_ce_4: 0.2795  loss_mask_4: 0.3571  loss_dice_4: 2.836  loss_ce_5: 0.2668  loss_mask_5: 0.3548  loss_dice_5: 2.838  loss_ce_6: 0.2725  loss_mask_6: 0.3545  loss_dice_6: 2.823  loss_ce_7: 0.2653  loss_mask_7: 0.356  loss_dice_7: 2.839  loss_ce_8: 0.2638  loss_mask_8: 0.3553  loss_dice_8: 2.827  time: 1.5364  data_time: 0.1019  lr: 7.7149e-06  max_mem: 21478M
[01/17 14:08:27] d2.utils.events INFO:  eta: 1 day, 4:49:08  iter: 22559  total_loss: 35.47  loss_ce: 0.2837  loss_mask: 0.3608  loss_dice: 2.823  loss_ce_0: 0.5823  loss_mask_0: 0.3679  loss_dice_0: 2.942  loss_ce_1: 0.3261  loss_mask_1: 0.3727  loss_dice_1: 2.856  loss_ce_2: 0.3049  loss_mask_2: 0.3691  loss_dice_2: 2.831  loss_ce_3: 0.2955  loss_mask_3: 0.3645  loss_dice_3: 2.821  loss_ce_4: 0.2988  loss_mask_4: 0.3627  loss_dice_4: 2.828  loss_ce_5: 0.2949  loss_mask_5: 0.3633  loss_dice_5: 2.83  loss_ce_6: 0.2894  loss_mask_6: 0.3586  loss_dice_6: 2.822  loss_ce_7: 0.2768  loss_mask_7: 0.358  loss_dice_7: 2.83  loss_ce_8: 0.2895  loss_mask_8: 0.3593  loss_dice_8: 2.827  time: 1.5364  data_time: 0.0913  lr: 7.7128e-06  max_mem: 21478M
[01/17 14:08:58] d2.utils.events INFO:  eta: 1 day, 4:48:52  iter: 22579  total_loss: 34.43  loss_ce: 0.2643  loss_mask: 0.366  loss_dice: 2.765  loss_ce_0: 0.5637  loss_mask_0: 0.3546  loss_dice_0: 2.899  loss_ce_1: 0.2824  loss_mask_1: 0.3654  loss_dice_1: 2.816  loss_ce_2: 0.2784  loss_mask_2: 0.3624  loss_dice_2: 2.794  loss_ce_3: 0.2631  loss_mask_3: 0.3661  loss_dice_3: 2.764  loss_ce_4: 0.2724  loss_mask_4: 0.3644  loss_dice_4: 2.767  loss_ce_5: 0.2627  loss_mask_5: 0.3668  loss_dice_5: 2.775  loss_ce_6: 0.2591  loss_mask_6: 0.3678  loss_dice_6: 2.767  loss_ce_7: 0.2478  loss_mask_7: 0.3675  loss_dice_7: 2.771  loss_ce_8: 0.2468  loss_mask_8: 0.3663  loss_dice_8: 2.77  time: 1.5364  data_time: 0.0989  lr: 7.7108e-06  max_mem: 21478M
[01/17 14:09:29] d2.utils.events INFO:  eta: 1 day, 4:48:53  iter: 22599  total_loss: 34.45  loss_ce: 0.2415  loss_mask: 0.3685  loss_dice: 2.757  loss_ce_0: 0.5727  loss_mask_0: 0.3629  loss_dice_0: 2.87  loss_ce_1: 0.2831  loss_mask_1: 0.3694  loss_dice_1: 2.786  loss_ce_2: 0.2801  loss_mask_2: 0.368  loss_dice_2: 2.768  loss_ce_3: 0.2792  loss_mask_3: 0.3676  loss_dice_3: 2.759  loss_ce_4: 0.2603  loss_mask_4: 0.3679  loss_dice_4: 2.766  loss_ce_5: 0.2463  loss_mask_5: 0.3688  loss_dice_5: 2.759  loss_ce_6: 0.2405  loss_mask_6: 0.371  loss_dice_6: 2.751  loss_ce_7: 0.244  loss_mask_7: 0.368  loss_dice_7: 2.752  loss_ce_8: 0.2367  loss_mask_8: 0.3679  loss_dice_8: 2.758  time: 1.5364  data_time: 0.0969  lr: 7.7087e-06  max_mem: 21478M
[01/17 14:10:00] d2.utils.events INFO:  eta: 1 day, 4:48:28  iter: 22619  total_loss: 35.02  loss_ce: 0.2561  loss_mask: 0.3736  loss_dice: 2.812  loss_ce_0: 0.5654  loss_mask_0: 0.3711  loss_dice_0: 2.939  loss_ce_1: 0.2897  loss_mask_1: 0.3844  loss_dice_1: 2.84  loss_ce_2: 0.2769  loss_mask_2: 0.3781  loss_dice_2: 2.812  loss_ce_3: 0.2615  loss_mask_3: 0.3779  loss_dice_3: 2.8  loss_ce_4: 0.267  loss_mask_4: 0.3745  loss_dice_4: 2.806  loss_ce_5: 0.2551  loss_mask_5: 0.376  loss_dice_5: 2.808  loss_ce_6: 0.2519  loss_mask_6: 0.3744  loss_dice_6: 2.811  loss_ce_7: 0.2495  loss_mask_7: 0.3739  loss_dice_7: 2.807  loss_ce_8: 0.2455  loss_mask_8: 0.3751  loss_dice_8: 2.8  time: 1.5364  data_time: 0.0973  lr: 7.7066e-06  max_mem: 21478M
[01/17 14:10:31] d2.utils.events INFO:  eta: 1 day, 4:48:20  iter: 22639  total_loss: 34.03  loss_ce: 0.2758  loss_mask: 0.3588  loss_dice: 2.713  loss_ce_0: 0.5773  loss_mask_0: 0.3543  loss_dice_0: 2.838  loss_ce_1: 0.3127  loss_mask_1: 0.3618  loss_dice_1: 2.758  loss_ce_2: 0.2982  loss_mask_2: 0.3592  loss_dice_2: 2.735  loss_ce_3: 0.2915  loss_mask_3: 0.3566  loss_dice_3: 2.726  loss_ce_4: 0.2742  loss_mask_4: 0.358  loss_dice_4: 2.728  loss_ce_5: 0.273  loss_mask_5: 0.3605  loss_dice_5: 2.727  loss_ce_6: 0.267  loss_mask_6: 0.3597  loss_dice_6: 2.721  loss_ce_7: 0.2575  loss_mask_7: 0.3594  loss_dice_7: 2.712  loss_ce_8: 0.2649  loss_mask_8: 0.3597  loss_dice_8: 2.722  time: 1.5364  data_time: 0.0994  lr: 7.7046e-06  max_mem: 21478M
[01/17 14:11:02] d2.utils.events INFO:  eta: 1 day, 4:49:19  iter: 22659  total_loss: 34.78  loss_ce: 0.2649  loss_mask: 0.3655  loss_dice: 2.812  loss_ce_0: 0.5703  loss_mask_0: 0.3678  loss_dice_0: 2.934  loss_ce_1: 0.2916  loss_mask_1: 0.3726  loss_dice_1: 2.86  loss_ce_2: 0.3009  loss_mask_2: 0.3676  loss_dice_2: 2.836  loss_ce_3: 0.2719  loss_mask_3: 0.3684  loss_dice_3: 2.822  loss_ce_4: 0.268  loss_mask_4: 0.3668  loss_dice_4: 2.825  loss_ce_5: 0.2722  loss_mask_5: 0.3653  loss_dice_5: 2.822  loss_ce_6: 0.2554  loss_mask_6: 0.3667  loss_dice_6: 2.82  loss_ce_7: 0.2574  loss_mask_7: 0.3671  loss_dice_7: 2.819  loss_ce_8: 0.2569  loss_mask_8: 0.3661  loss_dice_8: 2.816  time: 1.5364  data_time: 0.1082  lr: 7.7025e-06  max_mem: 21478M
[01/17 14:11:33] d2.utils.events INFO:  eta: 1 day, 4:48:05  iter: 22679  total_loss: 35.08  loss_ce: 0.259  loss_mask: 0.3581  loss_dice: 2.792  loss_ce_0: 0.5775  loss_mask_0: 0.3611  loss_dice_0: 2.901  loss_ce_1: 0.2997  loss_mask_1: 0.3635  loss_dice_1: 2.839  loss_ce_2: 0.2837  loss_mask_2: 0.3595  loss_dice_2: 2.821  loss_ce_3: 0.2985  loss_mask_3: 0.3584  loss_dice_3: 2.801  loss_ce_4: 0.2846  loss_mask_4: 0.3591  loss_dice_4: 2.803  loss_ce_5: 0.2857  loss_mask_5: 0.3589  loss_dice_5: 2.806  loss_ce_6: 0.2675  loss_mask_6: 0.3597  loss_dice_6: 2.796  loss_ce_7: 0.2616  loss_mask_7: 0.3581  loss_dice_7: 2.808  loss_ce_8: 0.2596  loss_mask_8: 0.3576  loss_dice_8: 2.791  time: 1.5364  data_time: 0.0993  lr: 7.7005e-06  max_mem: 21478M
[01/17 14:12:04] d2.utils.events INFO:  eta: 1 day, 4:46:48  iter: 22699  total_loss: 34.5  loss_ce: 0.2532  loss_mask: 0.3584  loss_dice: 2.758  loss_ce_0: 0.5782  loss_mask_0: 0.3625  loss_dice_0: 2.864  loss_ce_1: 0.2893  loss_mask_1: 0.3614  loss_dice_1: 2.798  loss_ce_2: 0.2873  loss_mask_2: 0.3587  loss_dice_2: 2.771  loss_ce_3: 0.2785  loss_mask_3: 0.3565  loss_dice_3: 2.756  loss_ce_4: 0.2727  loss_mask_4: 0.3567  loss_dice_4: 2.758  loss_ce_5: 0.2545  loss_mask_5: 0.3577  loss_dice_5: 2.765  loss_ce_6: 0.2556  loss_mask_6: 0.3543  loss_dice_6: 2.756  loss_ce_7: 0.2519  loss_mask_7: 0.356  loss_dice_7: 2.752  loss_ce_8: 0.2519  loss_mask_8: 0.3572  loss_dice_8: 2.758  time: 1.5364  data_time: 0.0990  lr: 7.6984e-06  max_mem: 21478M
[01/17 14:12:34] d2.utils.events INFO:  eta: 1 day, 4:44:40  iter: 22719  total_loss: 34.41  loss_ce: 0.2625  loss_mask: 0.3621  loss_dice: 2.745  loss_ce_0: 0.5787  loss_mask_0: 0.3586  loss_dice_0: 2.872  loss_ce_1: 0.3168  loss_mask_1: 0.3649  loss_dice_1: 2.783  loss_ce_2: 0.2923  loss_mask_2: 0.3637  loss_dice_2: 2.752  loss_ce_3: 0.274  loss_mask_3: 0.3617  loss_dice_3: 2.745  loss_ce_4: 0.2692  loss_mask_4: 0.3615  loss_dice_4: 2.75  loss_ce_5: 0.2692  loss_mask_5: 0.3624  loss_dice_5: 2.744  loss_ce_6: 0.2555  loss_mask_6: 0.3619  loss_dice_6: 2.748  loss_ce_7: 0.2598  loss_mask_7: 0.3617  loss_dice_7: 2.739  loss_ce_8: 0.2493  loss_mask_8: 0.3612  loss_dice_8: 2.745  time: 1.5364  data_time: 0.1049  lr: 7.6964e-06  max_mem: 21478M
[01/17 14:13:06] d2.utils.events INFO:  eta: 1 day, 4:45:36  iter: 22739  total_loss: 34.76  loss_ce: 0.2682  loss_mask: 0.3606  loss_dice: 2.807  loss_ce_0: 0.5895  loss_mask_0: 0.36  loss_dice_0: 2.912  loss_ce_1: 0.3104  loss_mask_1: 0.3631  loss_dice_1: 2.85  loss_ce_2: 0.3195  loss_mask_2: 0.3611  loss_dice_2: 2.822  loss_ce_3: 0.2879  loss_mask_3: 0.3598  loss_dice_3: 2.801  loss_ce_4: 0.276  loss_mask_4: 0.361  loss_dice_4: 2.816  loss_ce_5: 0.2701  loss_mask_5: 0.3596  loss_dice_5: 2.815  loss_ce_6: 0.2699  loss_mask_6: 0.3593  loss_dice_6: 2.818  loss_ce_7: 0.2631  loss_mask_7: 0.3599  loss_dice_7: 2.807  loss_ce_8: 0.2594  loss_mask_8: 0.3609  loss_dice_8: 2.812  time: 1.5364  data_time: 0.1064  lr: 7.6943e-06  max_mem: 21478M
[01/17 14:13:36] d2.utils.events INFO:  eta: 1 day, 4:44:47  iter: 22759  total_loss: 34.78  loss_ce: 0.2421  loss_mask: 0.3655  loss_dice: 2.781  loss_ce_0: 0.5673  loss_mask_0: 0.3626  loss_dice_0: 2.895  loss_ce_1: 0.2726  loss_mask_1: 0.3683  loss_dice_1: 2.817  loss_ce_2: 0.2783  loss_mask_2: 0.3652  loss_dice_2: 2.794  loss_ce_3: 0.2674  loss_mask_3: 0.3675  loss_dice_3: 2.79  loss_ce_4: 0.2593  loss_mask_4: 0.3666  loss_dice_4: 2.783  loss_ce_5: 0.2408  loss_mask_5: 0.3638  loss_dice_5: 2.789  loss_ce_6: 0.2509  loss_mask_6: 0.3678  loss_dice_6: 2.783  loss_ce_7: 0.2425  loss_mask_7: 0.3664  loss_dice_7: 2.782  loss_ce_8: 0.2493  loss_mask_8: 0.3656  loss_dice_8: 2.78  time: 1.5364  data_time: 0.0914  lr: 7.6922e-06  max_mem: 21478M
[01/17 14:14:06] d2.utils.events INFO:  eta: 1 day, 4:41:59  iter: 22779  total_loss: 34.01  loss_ce: 0.2472  loss_mask: 0.3619  loss_dice: 2.725  loss_ce_0: 0.5586  loss_mask_0: 0.3587  loss_dice_0: 2.841  loss_ce_1: 0.2936  loss_mask_1: 0.3669  loss_dice_1: 2.766  loss_ce_2: 0.2778  loss_mask_2: 0.3623  loss_dice_2: 2.738  loss_ce_3: 0.2575  loss_mask_3: 0.3583  loss_dice_3: 2.728  loss_ce_4: 0.2596  loss_mask_4: 0.36  loss_dice_4: 2.718  loss_ce_5: 0.2517  loss_mask_5: 0.3602  loss_dice_5: 2.722  loss_ce_6: 0.2549  loss_mask_6: 0.361  loss_dice_6: 2.727  loss_ce_7: 0.2458  loss_mask_7: 0.3604  loss_dice_7: 2.725  loss_ce_8: 0.251  loss_mask_8: 0.3612  loss_dice_8: 2.72  time: 1.5364  data_time: 0.0875  lr: 7.6902e-06  max_mem: 21478M
[01/17 14:14:37] d2.utils.events INFO:  eta: 1 day, 4:41:25  iter: 22799  total_loss: 34.47  loss_ce: 0.2699  loss_mask: 0.3663  loss_dice: 2.795  loss_ce_0: 0.5478  loss_mask_0: 0.3691  loss_dice_0: 2.892  loss_ce_1: 0.3017  loss_mask_1: 0.3666  loss_dice_1: 2.826  loss_ce_2: 0.2934  loss_mask_2: 0.365  loss_dice_2: 2.794  loss_ce_3: 0.2873  loss_mask_3: 0.3648  loss_dice_3: 2.799  loss_ce_4: 0.2771  loss_mask_4: 0.3649  loss_dice_4: 2.78  loss_ce_5: 0.2704  loss_mask_5: 0.3649  loss_dice_5: 2.79  loss_ce_6: 0.2576  loss_mask_6: 0.3663  loss_dice_6: 2.781  loss_ce_7: 0.2566  loss_mask_7: 0.3669  loss_dice_7: 2.789  loss_ce_8: 0.2719  loss_mask_8: 0.3655  loss_dice_8: 2.789  time: 1.5364  data_time: 0.1004  lr: 7.6881e-06  max_mem: 21478M
[01/17 14:15:08] d2.utils.events INFO:  eta: 1 day, 4:40:21  iter: 22819  total_loss: 34.73  loss_ce: 0.2755  loss_mask: 0.3628  loss_dice: 2.795  loss_ce_0: 0.5638  loss_mask_0: 0.3635  loss_dice_0: 2.908  loss_ce_1: 0.3098  loss_mask_1: 0.3706  loss_dice_1: 2.834  loss_ce_2: 0.3165  loss_mask_2: 0.369  loss_dice_2: 2.808  loss_ce_3: 0.3025  loss_mask_3: 0.3658  loss_dice_3: 2.799  loss_ce_4: 0.2839  loss_mask_4: 0.3645  loss_dice_4: 2.804  loss_ce_5: 0.27  loss_mask_5: 0.3637  loss_dice_5: 2.809  loss_ce_6: 0.2793  loss_mask_6: 0.3618  loss_dice_6: 2.797  loss_ce_7: 0.2838  loss_mask_7: 0.3626  loss_dice_7: 2.799  loss_ce_8: 0.2813  loss_mask_8: 0.3643  loss_dice_8: 2.801  time: 1.5364  data_time: 0.0885  lr: 7.6861e-06  max_mem: 21478M
[01/17 14:15:38] d2.utils.events INFO:  eta: 1 day, 4:38:42  iter: 22839  total_loss: 33.97  loss_ce: 0.2663  loss_mask: 0.3595  loss_dice: 2.741  loss_ce_0: 0.5467  loss_mask_0: 0.3561  loss_dice_0: 2.856  loss_ce_1: 0.2927  loss_mask_1: 0.3622  loss_dice_1: 2.776  loss_ce_2: 0.2838  loss_mask_2: 0.3601  loss_dice_2: 2.751  loss_ce_3: 0.2849  loss_mask_3: 0.3597  loss_dice_3: 2.739  loss_ce_4: 0.2668  loss_mask_4: 0.3597  loss_dice_4: 2.74  loss_ce_5: 0.2626  loss_mask_5: 0.3601  loss_dice_5: 2.752  loss_ce_6: 0.258  loss_mask_6: 0.3608  loss_dice_6: 2.742  loss_ce_7: 0.2463  loss_mask_7: 0.3577  loss_dice_7: 2.739  loss_ce_8: 0.2515  loss_mask_8: 0.3587  loss_dice_8: 2.736  time: 1.5364  data_time: 0.0958  lr: 7.684e-06  max_mem: 21478M
[01/17 14:16:09] d2.utils.events INFO:  eta: 1 day, 4:37:26  iter: 22859  total_loss: 34.79  loss_ce: 0.2449  loss_mask: 0.3729  loss_dice: 2.79  loss_ce_0: 0.5213  loss_mask_0: 0.374  loss_dice_0: 2.917  loss_ce_1: 0.2988  loss_mask_1: 0.3782  loss_dice_1: 2.833  loss_ce_2: 0.3014  loss_mask_2: 0.3734  loss_dice_2: 2.806  loss_ce_3: 0.2627  loss_mask_3: 0.3728  loss_dice_3: 2.809  loss_ce_4: 0.2672  loss_mask_4: 0.372  loss_dice_4: 2.804  loss_ce_5: 0.2606  loss_mask_5: 0.3706  loss_dice_5: 2.798  loss_ce_6: 0.2528  loss_mask_6: 0.3725  loss_dice_6: 2.794  loss_ce_7: 0.2382  loss_mask_7: 0.3702  loss_dice_7: 2.799  loss_ce_8: 0.2571  loss_mask_8: 0.3731  loss_dice_8: 2.802  time: 1.5364  data_time: 0.0984  lr: 7.6819e-06  max_mem: 21478M
[01/17 14:16:40] d2.utils.events INFO:  eta: 1 day, 4:38:20  iter: 22879  total_loss: 35.86  loss_ce: 0.2848  loss_mask: 0.3611  loss_dice: 2.876  loss_ce_0: 0.5932  loss_mask_0: 0.3673  loss_dice_0: 2.975  loss_ce_1: 0.3206  loss_mask_1: 0.375  loss_dice_1: 2.917  loss_ce_2: 0.3071  loss_mask_2: 0.3668  loss_dice_2: 2.887  loss_ce_3: 0.2834  loss_mask_3: 0.3635  loss_dice_3: 2.881  loss_ce_4: 0.283  loss_mask_4: 0.3641  loss_dice_4: 2.878  loss_ce_5: 0.2978  loss_mask_5: 0.36  loss_dice_5: 2.875  loss_ce_6: 0.2848  loss_mask_6: 0.3596  loss_dice_6: 2.874  loss_ce_7: 0.2859  loss_mask_7: 0.3592  loss_dice_7: 2.877  loss_ce_8: 0.2863  loss_mask_8: 0.3603  loss_dice_8: 2.878  time: 1.5364  data_time: 0.0946  lr: 7.6799e-06  max_mem: 21478M
[01/17 14:17:12] d2.utils.events INFO:  eta: 1 day, 4:37:10  iter: 22899  total_loss: 35.18  loss_ce: 0.2463  loss_mask: 0.3647  loss_dice: 2.828  loss_ce_0: 0.5895  loss_mask_0: 0.3828  loss_dice_0: 2.928  loss_ce_1: 0.3093  loss_mask_1: 0.3747  loss_dice_1: 2.865  loss_ce_2: 0.2965  loss_mask_2: 0.3695  loss_dice_2: 2.848  loss_ce_3: 0.2731  loss_mask_3: 0.3696  loss_dice_3: 2.824  loss_ce_4: 0.2602  loss_mask_4: 0.368  loss_dice_4: 2.831  loss_ce_5: 0.2594  loss_mask_5: 0.3655  loss_dice_5: 2.832  loss_ce_6: 0.2625  loss_mask_6: 0.3677  loss_dice_6: 2.827  loss_ce_7: 0.2598  loss_mask_7: 0.3658  loss_dice_7: 2.828  loss_ce_8: 0.2617  loss_mask_8: 0.3647  loss_dice_8: 2.827  time: 1.5364  data_time: 0.1030  lr: 7.6778e-06  max_mem: 21478M
[01/17 14:17:42] d2.utils.events INFO:  eta: 1 day, 4:35:57  iter: 22919  total_loss: 34.23  loss_ce: 0.2578  loss_mask: 0.3687  loss_dice: 2.764  loss_ce_0: 0.5555  loss_mask_0: 0.3634  loss_dice_0: 2.869  loss_ce_1: 0.3121  loss_mask_1: 0.3715  loss_dice_1: 2.798  loss_ce_2: 0.2884  loss_mask_2: 0.3699  loss_dice_2: 2.782  loss_ce_3: 0.2795  loss_mask_3: 0.3697  loss_dice_3: 2.77  loss_ce_4: 0.2706  loss_mask_4: 0.3681  loss_dice_4: 2.769  loss_ce_5: 0.2579  loss_mask_5: 0.3673  loss_dice_5: 2.777  loss_ce_6: 0.2509  loss_mask_6: 0.3677  loss_dice_6: 2.766  loss_ce_7: 0.2482  loss_mask_7: 0.3679  loss_dice_7: 2.766  loss_ce_8: 0.2533  loss_mask_8: 0.3681  loss_dice_8: 2.762  time: 1.5364  data_time: 0.0922  lr: 7.6758e-06  max_mem: 21478M
[01/17 14:18:13] d2.utils.events INFO:  eta: 1 day, 4:35:32  iter: 22939  total_loss: 34.65  loss_ce: 0.2585  loss_mask: 0.3608  loss_dice: 2.739  loss_ce_0: 0.5602  loss_mask_0: 0.3721  loss_dice_0: 2.831  loss_ce_1: 0.2974  loss_mask_1: 0.376  loss_dice_1: 2.772  loss_ce_2: 0.2985  loss_mask_2: 0.3688  loss_dice_2: 2.752  loss_ce_3: 0.2817  loss_mask_3: 0.364  loss_dice_3: 2.746  loss_ce_4: 0.2749  loss_mask_4: 0.3649  loss_dice_4: 2.748  loss_ce_5: 0.2704  loss_mask_5: 0.3637  loss_dice_5: 2.752  loss_ce_6: 0.268  loss_mask_6: 0.3598  loss_dice_6: 2.739  loss_ce_7: 0.2559  loss_mask_7: 0.3608  loss_dice_7: 2.737  loss_ce_8: 0.2732  loss_mask_8: 0.3605  loss_dice_8: 2.745  time: 1.5364  data_time: 0.0962  lr: 7.6737e-06  max_mem: 21478M
[01/17 14:18:44] d2.utils.events INFO:  eta: 1 day, 4:35:01  iter: 22959  total_loss: 34.69  loss_ce: 0.2752  loss_mask: 0.3704  loss_dice: 2.752  loss_ce_0: 0.5772  loss_mask_0: 0.3684  loss_dice_0: 2.857  loss_ce_1: 0.3116  loss_mask_1: 0.3702  loss_dice_1: 2.789  loss_ce_2: 0.3065  loss_mask_2: 0.3692  loss_dice_2: 2.767  loss_ce_3: 0.3028  loss_mask_3: 0.3693  loss_dice_3: 2.759  loss_ce_4: 0.2948  loss_mask_4: 0.3675  loss_dice_4: 2.751  loss_ce_5: 0.2697  loss_mask_5: 0.3669  loss_dice_5: 2.761  loss_ce_6: 0.2746  loss_mask_6: 0.3673  loss_dice_6: 2.756  loss_ce_7: 0.2871  loss_mask_7: 0.3678  loss_dice_7: 2.754  loss_ce_8: 0.275  loss_mask_8: 0.3691  loss_dice_8: 2.753  time: 1.5364  data_time: 0.1048  lr: 7.6716e-06  max_mem: 21478M
[01/17 14:19:14] d2.utils.events INFO:  eta: 1 day, 4:34:15  iter: 22979  total_loss: 35.05  loss_ce: 0.2723  loss_mask: 0.3726  loss_dice: 2.794  loss_ce_0: 0.5538  loss_mask_0: 0.377  loss_dice_0: 2.886  loss_ce_1: 0.2976  loss_mask_1: 0.3773  loss_dice_1: 2.825  loss_ce_2: 0.2887  loss_mask_2: 0.3727  loss_dice_2: 2.798  loss_ce_3: 0.2723  loss_mask_3: 0.3686  loss_dice_3: 2.789  loss_ce_4: 0.2887  loss_mask_4: 0.3701  loss_dice_4: 2.786  loss_ce_5: 0.2611  loss_mask_5: 0.3702  loss_dice_5: 2.787  loss_ce_6: 0.2595  loss_mask_6: 0.37  loss_dice_6: 2.784  loss_ce_7: 0.2719  loss_mask_7: 0.3699  loss_dice_7: 2.782  loss_ce_8: 0.2549  loss_mask_8: 0.3733  loss_dice_8: 2.791  time: 1.5364  data_time: 0.0944  lr: 7.6696e-06  max_mem: 21478M
[01/17 14:19:45] d2.utils.events INFO:  eta: 1 day, 4:33:44  iter: 22999  total_loss: 35.04  loss_ce: 0.2802  loss_mask: 0.3632  loss_dice: 2.751  loss_ce_0: 0.6037  loss_mask_0: 0.3607  loss_dice_0: 2.864  loss_ce_1: 0.3331  loss_mask_1: 0.3699  loss_dice_1: 2.788  loss_ce_2: 0.3268  loss_mask_2: 0.3672  loss_dice_2: 2.765  loss_ce_3: 0.3053  loss_mask_3: 0.3646  loss_dice_3: 2.752  loss_ce_4: 0.3077  loss_mask_4: 0.3626  loss_dice_4: 2.753  loss_ce_5: 0.2911  loss_mask_5: 0.3632  loss_dice_5: 2.758  loss_ce_6: 0.2848  loss_mask_6: 0.3624  loss_dice_6: 2.757  loss_ce_7: 0.3089  loss_mask_7: 0.3614  loss_dice_7: 2.745  loss_ce_8: 0.2755  loss_mask_8: 0.3625  loss_dice_8: 2.752  time: 1.5364  data_time: 0.0901  lr: 7.6675e-06  max_mem: 21478M
[01/17 14:20:16] d2.utils.events INFO:  eta: 1 day, 4:33:49  iter: 23019  total_loss: 34.75  loss_ce: 0.2605  loss_mask: 0.3643  loss_dice: 2.781  loss_ce_0: 0.5556  loss_mask_0: 0.368  loss_dice_0: 2.898  loss_ce_1: 0.3132  loss_mask_1: 0.3689  loss_dice_1: 2.829  loss_ce_2: 0.2934  loss_mask_2: 0.3645  loss_dice_2: 2.805  loss_ce_3: 0.2867  loss_mask_3: 0.3639  loss_dice_3: 2.789  loss_ce_4: 0.2818  loss_mask_4: 0.3636  loss_dice_4: 2.787  loss_ce_5: 0.2642  loss_mask_5: 0.3644  loss_dice_5: 2.787  loss_ce_6: 0.2581  loss_mask_6: 0.364  loss_dice_6: 2.789  loss_ce_7: 0.2487  loss_mask_7: 0.3637  loss_dice_7: 2.784  loss_ce_8: 0.2576  loss_mask_8: 0.3647  loss_dice_8: 2.786  time: 1.5364  data_time: 0.0995  lr: 7.6655e-06  max_mem: 21478M
[01/17 14:20:48] d2.utils.events INFO:  eta: 1 day, 4:34:19  iter: 23039  total_loss: 35.04  loss_ce: 0.2737  loss_mask: 0.3445  loss_dice: 2.837  loss_ce_0: 0.5908  loss_mask_0: 0.3459  loss_dice_0: 2.954  loss_ce_1: 0.3383  loss_mask_1: 0.3463  loss_dice_1: 2.875  loss_ce_2: 0.2987  loss_mask_2: 0.3438  loss_dice_2: 2.852  loss_ce_3: 0.3106  loss_mask_3: 0.3442  loss_dice_3: 2.851  loss_ce_4: 0.2904  loss_mask_4: 0.3434  loss_dice_4: 2.837  loss_ce_5: 0.2901  loss_mask_5: 0.3419  loss_dice_5: 2.842  loss_ce_6: 0.2939  loss_mask_6: 0.3438  loss_dice_6: 2.832  loss_ce_7: 0.2837  loss_mask_7: 0.3428  loss_dice_7: 2.83  loss_ce_8: 0.2762  loss_mask_8: 0.3426  loss_dice_8: 2.831  time: 1.5365  data_time: 0.1136  lr: 7.6634e-06  max_mem: 21478M
[01/17 14:21:19] d2.utils.events INFO:  eta: 1 day, 4:34:06  iter: 23059  total_loss: 35.32  loss_ce: 0.2547  loss_mask: 0.3604  loss_dice: 2.838  loss_ce_0: 0.5823  loss_mask_0: 0.3575  loss_dice_0: 2.944  loss_ce_1: 0.3001  loss_mask_1: 0.3599  loss_dice_1: 2.874  loss_ce_2: 0.2931  loss_mask_2: 0.3595  loss_dice_2: 2.857  loss_ce_3: 0.2881  loss_mask_3: 0.3584  loss_dice_3: 2.843  loss_ce_4: 0.2809  loss_mask_4: 0.356  loss_dice_4: 2.841  loss_ce_5: 0.2587  loss_mask_5: 0.3596  loss_dice_5: 2.842  loss_ce_6: 0.266  loss_mask_6: 0.3602  loss_dice_6: 2.839  loss_ce_7: 0.2596  loss_mask_7: 0.3605  loss_dice_7: 2.83  loss_ce_8: 0.2613  loss_mask_8: 0.3606  loss_dice_8: 2.839  time: 1.5365  data_time: 0.0941  lr: 7.6613e-06  max_mem: 21478M
[01/17 14:21:50] d2.utils.events INFO:  eta: 1 day, 4:33:21  iter: 23079  total_loss: 34.02  loss_ce: 0.2521  loss_mask: 0.3528  loss_dice: 2.74  loss_ce_0: 0.5529  loss_mask_0: 0.3588  loss_dice_0: 2.862  loss_ce_1: 0.278  loss_mask_1: 0.362  loss_dice_1: 2.78  loss_ce_2: 0.2853  loss_mask_2: 0.355  loss_dice_2: 2.767  loss_ce_3: 0.2569  loss_mask_3: 0.3525  loss_dice_3: 2.752  loss_ce_4: 0.2734  loss_mask_4: 0.3524  loss_dice_4: 2.749  loss_ce_5: 0.2688  loss_mask_5: 0.3518  loss_dice_5: 2.754  loss_ce_6: 0.2634  loss_mask_6: 0.3523  loss_dice_6: 2.748  loss_ce_7: 0.2548  loss_mask_7: 0.3537  loss_dice_7: 2.745  loss_ce_8: 0.253  loss_mask_8: 0.3516  loss_dice_8: 2.737  time: 1.5365  data_time: 0.0987  lr: 7.6593e-06  max_mem: 21478M
[01/17 14:22:21] d2.utils.events INFO:  eta: 1 day, 4:33:00  iter: 23099  total_loss: 34.87  loss_ce: 0.2752  loss_mask: 0.3576  loss_dice: 2.795  loss_ce_0: 0.5904  loss_mask_0: 0.3588  loss_dice_0: 2.907  loss_ce_1: 0.3094  loss_mask_1: 0.3608  loss_dice_1: 2.832  loss_ce_2: 0.2948  loss_mask_2: 0.3585  loss_dice_2: 2.814  loss_ce_3: 0.2727  loss_mask_3: 0.3573  loss_dice_3: 2.797  loss_ce_4: 0.2757  loss_mask_4: 0.3589  loss_dice_4: 2.803  loss_ce_5: 0.2691  loss_mask_5: 0.3606  loss_dice_5: 2.8  loss_ce_6: 0.275  loss_mask_6: 0.3595  loss_dice_6: 2.794  loss_ce_7: 0.2876  loss_mask_7: 0.3562  loss_dice_7: 2.795  loss_ce_8: 0.275  loss_mask_8: 0.357  loss_dice_8: 2.787  time: 1.5365  data_time: 0.1037  lr: 7.6572e-06  max_mem: 21478M
[01/17 14:22:53] d2.utils.events INFO:  eta: 1 day, 4:33:07  iter: 23119  total_loss: 35.21  loss_ce: 0.2773  loss_mask: 0.359  loss_dice: 2.788  loss_ce_0: 0.59  loss_mask_0: 0.3553  loss_dice_0: 2.921  loss_ce_1: 0.3096  loss_mask_1: 0.3692  loss_dice_1: 2.83  loss_ce_2: 0.2926  loss_mask_2: 0.3643  loss_dice_2: 2.813  loss_ce_3: 0.2819  loss_mask_3: 0.3622  loss_dice_3: 2.796  loss_ce_4: 0.2843  loss_mask_4: 0.3629  loss_dice_4: 2.791  loss_ce_5: 0.2683  loss_mask_5: 0.3642  loss_dice_5: 2.791  loss_ce_6: 0.2645  loss_mask_6: 0.3626  loss_dice_6: 2.779  loss_ce_7: 0.2607  loss_mask_7: 0.3618  loss_dice_7: 2.787  loss_ce_8: 0.2797  loss_mask_8: 0.362  loss_dice_8: 2.795  time: 1.5365  data_time: 0.1080  lr: 7.6552e-06  max_mem: 21478M
[01/17 14:23:25] d2.utils.events INFO:  eta: 1 day, 4:33:52  iter: 23139  total_loss: 34.82  loss_ce: 0.2697  loss_mask: 0.3483  loss_dice: 2.794  loss_ce_0: 0.5627  loss_mask_0: 0.3484  loss_dice_0: 2.892  loss_ce_1: 0.3043  loss_mask_1: 0.3512  loss_dice_1: 2.819  loss_ce_2: 0.3005  loss_mask_2: 0.3493  loss_dice_2: 2.8  loss_ce_3: 0.2908  loss_mask_3: 0.3505  loss_dice_3: 2.795  loss_ce_4: 0.2906  loss_mask_4: 0.3472  loss_dice_4: 2.796  loss_ce_5: 0.2763  loss_mask_5: 0.3477  loss_dice_5: 2.79  loss_ce_6: 0.2762  loss_mask_6: 0.3476  loss_dice_6: 2.794  loss_ce_7: 0.2871  loss_mask_7: 0.3478  loss_dice_7: 2.792  loss_ce_8: 0.2798  loss_mask_8: 0.3482  loss_dice_8: 2.785  time: 1.5366  data_time: 0.1136  lr: 7.6531e-06  max_mem: 21478M
[01/17 14:23:55] d2.utils.events INFO:  eta: 1 day, 4:32:22  iter: 23159  total_loss: 35.03  loss_ce: 0.2686  loss_mask: 0.3631  loss_dice: 2.792  loss_ce_0: 0.5618  loss_mask_0: 0.3655  loss_dice_0: 2.91  loss_ce_1: 0.3116  loss_mask_1: 0.3681  loss_dice_1: 2.834  loss_ce_2: 0.292  loss_mask_2: 0.3657  loss_dice_2: 2.805  loss_ce_3: 0.2807  loss_mask_3: 0.3646  loss_dice_3: 2.8  loss_ce_4: 0.2687  loss_mask_4: 0.3633  loss_dice_4: 2.797  loss_ce_5: 0.2707  loss_mask_5: 0.3636  loss_dice_5: 2.788  loss_ce_6: 0.258  loss_mask_6: 0.362  loss_dice_6: 2.795  loss_ce_7: 0.2701  loss_mask_7: 0.3639  loss_dice_7: 2.793  loss_ce_8: 0.2747  loss_mask_8: 0.3627  loss_dice_8: 2.795  time: 1.5366  data_time: 0.1100  lr: 7.651e-06  max_mem: 21478M
[01/17 14:24:27] d2.utils.events INFO:  eta: 1 day, 4:32:59  iter: 23179  total_loss: 34.44  loss_ce: 0.2516  loss_mask: 0.3522  loss_dice: 2.769  loss_ce_0: 0.5653  loss_mask_0: 0.3525  loss_dice_0: 2.88  loss_ce_1: 0.3066  loss_mask_1: 0.3558  loss_dice_1: 2.817  loss_ce_2: 0.2813  loss_mask_2: 0.3523  loss_dice_2: 2.79  loss_ce_3: 0.2621  loss_mask_3: 0.3531  loss_dice_3: 2.779  loss_ce_4: 0.2753  loss_mask_4: 0.3515  loss_dice_4: 2.768  loss_ce_5: 0.258  loss_mask_5: 0.3526  loss_dice_5: 2.772  loss_ce_6: 0.2522  loss_mask_6: 0.3514  loss_dice_6: 2.768  loss_ce_7: 0.2628  loss_mask_7: 0.3511  loss_dice_7: 2.766  loss_ce_8: 0.256  loss_mask_8: 0.3508  loss_dice_8: 2.766  time: 1.5366  data_time: 0.1017  lr: 7.649e-06  max_mem: 21478M
[01/17 14:24:58] d2.utils.events INFO:  eta: 1 day, 4:33:58  iter: 23199  total_loss: 34.48  loss_ce: 0.2555  loss_mask: 0.3619  loss_dice: 2.767  loss_ce_0: 0.559  loss_mask_0: 0.3722  loss_dice_0: 2.871  loss_ce_1: 0.29  loss_mask_1: 0.3729  loss_dice_1: 2.793  loss_ce_2: 0.2933  loss_mask_2: 0.3715  loss_dice_2: 2.779  loss_ce_3: 0.2685  loss_mask_3: 0.3669  loss_dice_3: 2.767  loss_ce_4: 0.2698  loss_mask_4: 0.3654  loss_dice_4: 2.772  loss_ce_5: 0.2491  loss_mask_5: 0.3663  loss_dice_5: 2.771  loss_ce_6: 0.2527  loss_mask_6: 0.3659  loss_dice_6: 2.764  loss_ce_7: 0.2554  loss_mask_7: 0.3636  loss_dice_7: 2.762  loss_ce_8: 0.2486  loss_mask_8: 0.3645  loss_dice_8: 2.769  time: 1.5366  data_time: 0.1087  lr: 7.6469e-06  max_mem: 21478M
[01/17 14:25:29] d2.utils.events INFO:  eta: 1 day, 4:33:27  iter: 23219  total_loss: 34.7  loss_ce: 0.2591  loss_mask: 0.351  loss_dice: 2.793  loss_ce_0: 0.5535  loss_mask_0: 0.3561  loss_dice_0: 2.886  loss_ce_1: 0.3085  loss_mask_1: 0.3585  loss_dice_1: 2.81  loss_ce_2: 0.3006  loss_mask_2: 0.3517  loss_dice_2: 2.792  loss_ce_3: 0.2598  loss_mask_3: 0.3513  loss_dice_3: 2.781  loss_ce_4: 0.2604  loss_mask_4: 0.3517  loss_dice_4: 2.789  loss_ce_5: 0.263  loss_mask_5: 0.3525  loss_dice_5: 2.785  loss_ce_6: 0.263  loss_mask_6: 0.3536  loss_dice_6: 2.796  loss_ce_7: 0.2596  loss_mask_7: 0.3527  loss_dice_7: 2.791  loss_ce_8: 0.2573  loss_mask_8: 0.3516  loss_dice_8: 2.788  time: 1.5366  data_time: 0.0937  lr: 7.6449e-06  max_mem: 21478M
[01/17 14:26:00] d2.utils.events INFO:  eta: 1 day, 4:33:24  iter: 23239  total_loss: 35.41  loss_ce: 0.2894  loss_mask: 0.3709  loss_dice: 2.833  loss_ce_0: 0.558  loss_mask_0: 0.3701  loss_dice_0: 2.941  loss_ce_1: 0.3082  loss_mask_1: 0.3745  loss_dice_1: 2.872  loss_ce_2: 0.3245  loss_mask_2: 0.3704  loss_dice_2: 2.841  loss_ce_3: 0.3012  loss_mask_3: 0.3702  loss_dice_3: 2.832  loss_ce_4: 0.2952  loss_mask_4: 0.3706  loss_dice_4: 2.825  loss_ce_5: 0.277  loss_mask_5: 0.3697  loss_dice_5: 2.829  loss_ce_6: 0.2771  loss_mask_6: 0.3691  loss_dice_6: 2.827  loss_ce_7: 0.2751  loss_mask_7: 0.3702  loss_dice_7: 2.824  loss_ce_8: 0.2798  loss_mask_8: 0.3704  loss_dice_8: 2.827  time: 1.5367  data_time: 0.0916  lr: 7.6428e-06  max_mem: 21478M
[01/17 14:26:31] d2.utils.events INFO:  eta: 1 day, 4:32:22  iter: 23259  total_loss: 34.42  loss_ce: 0.2697  loss_mask: 0.3641  loss_dice: 2.789  loss_ce_0: 0.5504  loss_mask_0: 0.3748  loss_dice_0: 2.886  loss_ce_1: 0.321  loss_mask_1: 0.3711  loss_dice_1: 2.816  loss_ce_2: 0.3162  loss_mask_2: 0.3676  loss_dice_2: 2.791  loss_ce_3: 0.2859  loss_mask_3: 0.364  loss_dice_3: 2.783  loss_ce_4: 0.2903  loss_mask_4: 0.3633  loss_dice_4: 2.787  loss_ce_5: 0.2824  loss_mask_5: 0.3645  loss_dice_5: 2.787  loss_ce_6: 0.2743  loss_mask_6: 0.3647  loss_dice_6: 2.783  loss_ce_7: 0.2757  loss_mask_7: 0.3644  loss_dice_7: 2.785  loss_ce_8: 0.2716  loss_mask_8: 0.3638  loss_dice_8: 2.79  time: 1.5366  data_time: 0.1001  lr: 7.6407e-06  max_mem: 21478M
[01/17 14:27:01] d2.utils.events INFO:  eta: 1 day, 4:31:09  iter: 23279  total_loss: 35.07  loss_ce: 0.2578  loss_mask: 0.3574  loss_dice: 2.803  loss_ce_0: 0.5528  loss_mask_0: 0.3599  loss_dice_0: 2.915  loss_ce_1: 0.3093  loss_mask_1: 0.3555  loss_dice_1: 2.855  loss_ce_2: 0.3097  loss_mask_2: 0.3551  loss_dice_2: 2.828  loss_ce_3: 0.276  loss_mask_3: 0.3577  loss_dice_3: 2.818  loss_ce_4: 0.275  loss_mask_4: 0.3586  loss_dice_4: 2.809  loss_ce_5: 0.2731  loss_mask_5: 0.3587  loss_dice_5: 2.804  loss_ce_6: 0.2659  loss_mask_6: 0.3588  loss_dice_6: 2.806  loss_ce_7: 0.2771  loss_mask_7: 0.3582  loss_dice_7: 2.805  loss_ce_8: 0.2743  loss_mask_8: 0.3577  loss_dice_8: 2.799  time: 1.5366  data_time: 0.0973  lr: 7.6387e-06  max_mem: 21478M
[01/17 14:27:32] d2.utils.events INFO:  eta: 1 day, 4:29:46  iter: 23299  total_loss: 34.87  loss_ce: 0.2525  loss_mask: 0.3614  loss_dice: 2.792  loss_ce_0: 0.5534  loss_mask_0: 0.3709  loss_dice_0: 2.892  loss_ce_1: 0.2762  loss_mask_1: 0.3765  loss_dice_1: 2.834  loss_ce_2: 0.2751  loss_mask_2: 0.3719  loss_dice_2: 2.814  loss_ce_3: 0.2599  loss_mask_3: 0.3639  loss_dice_3: 2.811  loss_ce_4: 0.2635  loss_mask_4: 0.3616  loss_dice_4: 2.805  loss_ce_5: 0.2655  loss_mask_5: 0.3621  loss_dice_5: 2.8  loss_ce_6: 0.2446  loss_mask_6: 0.3629  loss_dice_6: 2.789  loss_ce_7: 0.2342  loss_mask_7: 0.3631  loss_dice_7: 2.786  loss_ce_8: 0.2424  loss_mask_8: 0.3642  loss_dice_8: 2.789  time: 1.5366  data_time: 0.0994  lr: 7.6366e-06  max_mem: 21478M
[01/17 14:28:04] d2.utils.events INFO:  eta: 1 day, 4:29:39  iter: 23319  total_loss: 34.98  loss_ce: 0.2517  loss_mask: 0.3646  loss_dice: 2.822  loss_ce_0: 0.5582  loss_mask_0: 0.3724  loss_dice_0: 2.937  loss_ce_1: 0.3014  loss_mask_1: 0.3686  loss_dice_1: 2.86  loss_ce_2: 0.2806  loss_mask_2: 0.3695  loss_dice_2: 2.836  loss_ce_3: 0.2604  loss_mask_3: 0.3682  loss_dice_3: 2.826  loss_ce_4: 0.2642  loss_mask_4: 0.3725  loss_dice_4: 2.828  loss_ce_5: 0.2573  loss_mask_5: 0.3689  loss_dice_5: 2.824  loss_ce_6: 0.2456  loss_mask_6: 0.3656  loss_dice_6: 2.824  loss_ce_7: 0.2528  loss_mask_7: 0.3656  loss_dice_7: 2.829  loss_ce_8: 0.2506  loss_mask_8: 0.3648  loss_dice_8: 2.818  time: 1.5367  data_time: 0.0957  lr: 7.6346e-06  max_mem: 21478M
[01/17 14:28:35] d2.utils.events INFO:  eta: 1 day, 4:29:36  iter: 23339  total_loss: 34.34  loss_ce: 0.2528  loss_mask: 0.3577  loss_dice: 2.781  loss_ce_0: 0.5456  loss_mask_0: 0.3539  loss_dice_0: 2.883  loss_ce_1: 0.3052  loss_mask_1: 0.3603  loss_dice_1: 2.821  loss_ce_2: 0.2775  loss_mask_2: 0.3595  loss_dice_2: 2.798  loss_ce_3: 0.2767  loss_mask_3: 0.3562  loss_dice_3: 2.785  loss_ce_4: 0.2689  loss_mask_4: 0.355  loss_dice_4: 2.781  loss_ce_5: 0.2519  loss_mask_5: 0.3579  loss_dice_5: 2.777  loss_ce_6: 0.2565  loss_mask_6: 0.3563  loss_dice_6: 2.781  loss_ce_7: 0.2482  loss_mask_7: 0.3563  loss_dice_7: 2.771  loss_ce_8: 0.2493  loss_mask_8: 0.3579  loss_dice_8: 2.775  time: 1.5367  data_time: 0.0913  lr: 7.6325e-06  max_mem: 21478M
[01/17 14:29:05] d2.utils.events INFO:  eta: 1 day, 4:28:14  iter: 23359  total_loss: 34.19  loss_ce: 0.2398  loss_mask: 0.355  loss_dice: 2.752  loss_ce_0: 0.5443  loss_mask_0: 0.3553  loss_dice_0: 2.864  loss_ce_1: 0.2836  loss_mask_1: 0.3593  loss_dice_1: 2.779  loss_ce_2: 0.2705  loss_mask_2: 0.3554  loss_dice_2: 2.759  loss_ce_3: 0.2694  loss_mask_3: 0.3566  loss_dice_3: 2.749  loss_ce_4: 0.2503  loss_mask_4: 0.3555  loss_dice_4: 2.737  loss_ce_5: 0.249  loss_mask_5: 0.355  loss_dice_5: 2.751  loss_ce_6: 0.2398  loss_mask_6: 0.3555  loss_dice_6: 2.739  loss_ce_7: 0.2586  loss_mask_7: 0.3556  loss_dice_7: 2.743  loss_ce_8: 0.2491  loss_mask_8: 0.3554  loss_dice_8: 2.749  time: 1.5367  data_time: 0.0963  lr: 7.6304e-06  max_mem: 21478M
[01/17 14:29:35] d2.utils.events INFO:  eta: 1 day, 4:27:52  iter: 23379  total_loss: 33.51  loss_ce: 0.2256  loss_mask: 0.3566  loss_dice: 2.689  loss_ce_0: 0.5299  loss_mask_0: 0.3577  loss_dice_0: 2.809  loss_ce_1: 0.2688  loss_mask_1: 0.3595  loss_dice_1: 2.73  loss_ce_2: 0.2613  loss_mask_2: 0.355  loss_dice_2: 2.708  loss_ce_3: 0.2376  loss_mask_3: 0.3536  loss_dice_3: 2.698  loss_ce_4: 0.2389  loss_mask_4: 0.3532  loss_dice_4: 2.698  loss_ce_5: 0.2305  loss_mask_5: 0.3549  loss_dice_5: 2.695  loss_ce_6: 0.2363  loss_mask_6: 0.3542  loss_dice_6: 2.693  loss_ce_7: 0.2151  loss_mask_7: 0.3549  loss_dice_7: 2.692  loss_ce_8: 0.2283  loss_mask_8: 0.3544  loss_dice_8: 2.686  time: 1.5366  data_time: 0.0889  lr: 7.6284e-06  max_mem: 21478M
[01/17 14:30:06] d2.utils.events INFO:  eta: 1 day, 4:28:20  iter: 23399  total_loss: 34.66  loss_ce: 0.2389  loss_mask: 0.3619  loss_dice: 2.797  loss_ce_0: 0.557  loss_mask_0: 0.3644  loss_dice_0: 2.899  loss_ce_1: 0.2945  loss_mask_1: 0.37  loss_dice_1: 2.83  loss_ce_2: 0.2699  loss_mask_2: 0.365  loss_dice_2: 2.807  loss_ce_3: 0.2534  loss_mask_3: 0.3631  loss_dice_3: 2.799  loss_ce_4: 0.246  loss_mask_4: 0.3612  loss_dice_4: 2.8  loss_ce_5: 0.2332  loss_mask_5: 0.3613  loss_dice_5: 2.798  loss_ce_6: 0.2375  loss_mask_6: 0.3609  loss_dice_6: 2.797  loss_ce_7: 0.2439  loss_mask_7: 0.3596  loss_dice_7: 2.802  loss_ce_8: 0.2405  loss_mask_8: 0.3621  loss_dice_8: 2.798  time: 1.5366  data_time: 0.0960  lr: 7.6263e-06  max_mem: 21478M
[01/17 14:30:37] d2.utils.events INFO:  eta: 1 day, 4:27:43  iter: 23419  total_loss: 34.46  loss_ce: 0.2531  loss_mask: 0.3633  loss_dice: 2.765  loss_ce_0: 0.5531  loss_mask_0: 0.37  loss_dice_0: 2.884  loss_ce_1: 0.2878  loss_mask_1: 0.3697  loss_dice_1: 2.808  loss_ce_2: 0.2992  loss_mask_2: 0.3657  loss_dice_2: 2.78  loss_ce_3: 0.2674  loss_mask_3: 0.3631  loss_dice_3: 2.772  loss_ce_4: 0.2761  loss_mask_4: 0.3616  loss_dice_4: 2.767  loss_ce_5: 0.2552  loss_mask_5: 0.3626  loss_dice_5: 2.783  loss_ce_6: 0.2575  loss_mask_6: 0.3607  loss_dice_6: 2.777  loss_ce_7: 0.2694  loss_mask_7: 0.3631  loss_dice_7: 2.766  loss_ce_8: 0.269  loss_mask_8: 0.3627  loss_dice_8: 2.765  time: 1.5366  data_time: 0.0974  lr: 7.6242e-06  max_mem: 21478M
[01/17 14:31:07] d2.utils.events INFO:  eta: 1 day, 4:24:55  iter: 23439  total_loss: 34.28  loss_ce: 0.2644  loss_mask: 0.3676  loss_dice: 2.731  loss_ce_0: 0.5585  loss_mask_0: 0.3705  loss_dice_0: 2.824  loss_ce_1: 0.3033  loss_mask_1: 0.3727  loss_dice_1: 2.757  loss_ce_2: 0.2827  loss_mask_2: 0.3703  loss_dice_2: 2.748  loss_ce_3: 0.2602  loss_mask_3: 0.3692  loss_dice_3: 2.734  loss_ce_4: 0.2611  loss_mask_4: 0.3697  loss_dice_4: 2.735  loss_ce_5: 0.2537  loss_mask_5: 0.371  loss_dice_5: 2.748  loss_ce_6: 0.2587  loss_mask_6: 0.3686  loss_dice_6: 2.732  loss_ce_7: 0.2631  loss_mask_7: 0.3678  loss_dice_7: 2.732  loss_ce_8: 0.2621  loss_mask_8: 0.3672  loss_dice_8: 2.731  time: 1.5366  data_time: 0.0917  lr: 7.6222e-06  max_mem: 21478M
[01/17 14:31:38] d2.utils.events INFO:  eta: 1 day, 4:24:48  iter: 23459  total_loss: 34.27  loss_ce: 0.2331  loss_mask: 0.3648  loss_dice: 2.773  loss_ce_0: 0.5507  loss_mask_0: 0.3615  loss_dice_0: 2.902  loss_ce_1: 0.2622  loss_mask_1: 0.3636  loss_dice_1: 2.814  loss_ce_2: 0.271  loss_mask_2: 0.3647  loss_dice_2: 2.8  loss_ce_3: 0.2622  loss_mask_3: 0.3653  loss_dice_3: 2.776  loss_ce_4: 0.2487  loss_mask_4: 0.3642  loss_dice_4: 2.785  loss_ce_5: 0.2304  loss_mask_5: 0.3648  loss_dice_5: 2.793  loss_ce_6: 0.2259  loss_mask_6: 0.3649  loss_dice_6: 2.775  loss_ce_7: 0.2279  loss_mask_7: 0.3652  loss_dice_7: 2.78  loss_ce_8: 0.238  loss_mask_8: 0.3655  loss_dice_8: 2.773  time: 1.5366  data_time: 0.0989  lr: 7.6201e-06  max_mem: 21478M
[01/17 14:32:09] d2.utils.events INFO:  eta: 1 day, 4:24:57  iter: 23479  total_loss: 35.45  loss_ce: 0.2566  loss_mask: 0.3631  loss_dice: 2.821  loss_ce_0: 0.5566  loss_mask_0: 0.369  loss_dice_0: 2.927  loss_ce_1: 0.3001  loss_mask_1: 0.3712  loss_dice_1: 2.858  loss_ce_2: 0.3029  loss_mask_2: 0.369  loss_dice_2: 2.838  loss_ce_3: 0.2846  loss_mask_3: 0.3662  loss_dice_3: 2.833  loss_ce_4: 0.2811  loss_mask_4: 0.3678  loss_dice_4: 2.817  loss_ce_5: 0.2942  loss_mask_5: 0.3672  loss_dice_5: 2.826  loss_ce_6: 0.273  loss_mask_6: 0.3654  loss_dice_6: 2.816  loss_ce_7: 0.2766  loss_mask_7: 0.3648  loss_dice_7: 2.829  loss_ce_8: 0.2719  loss_mask_8: 0.3634  loss_dice_8: 2.822  time: 1.5366  data_time: 0.0951  lr: 7.6181e-06  max_mem: 21478M
[01/17 14:32:40] d2.utils.events INFO:  eta: 1 day, 4:24:47  iter: 23499  total_loss: 34.01  loss_ce: 0.2614  loss_mask: 0.3565  loss_dice: 2.724  loss_ce_0: 0.5697  loss_mask_0: 0.3662  loss_dice_0: 2.829  loss_ce_1: 0.2875  loss_mask_1: 0.3667  loss_dice_1: 2.748  loss_ce_2: 0.2911  loss_mask_2: 0.363  loss_dice_2: 2.735  loss_ce_3: 0.2712  loss_mask_3: 0.361  loss_dice_3: 2.719  loss_ce_4: 0.2806  loss_mask_4: 0.36  loss_dice_4: 2.724  loss_ce_5: 0.2566  loss_mask_5: 0.3586  loss_dice_5: 2.721  loss_ce_6: 0.2577  loss_mask_6: 0.3581  loss_dice_6: 2.714  loss_ce_7: 0.2597  loss_mask_7: 0.3576  loss_dice_7: 2.727  loss_ce_8: 0.2538  loss_mask_8: 0.3564  loss_dice_8: 2.716  time: 1.5366  data_time: 0.0999  lr: 7.616e-06  max_mem: 21478M
[01/17 14:33:11] d2.utils.events INFO:  eta: 1 day, 4:24:16  iter: 23519  total_loss: 33.46  loss_ce: 0.2383  loss_mask: 0.3601  loss_dice: 2.699  loss_ce_0: 0.5569  loss_mask_0: 0.3546  loss_dice_0: 2.817  loss_ce_1: 0.2859  loss_mask_1: 0.3589  loss_dice_1: 2.733  loss_ce_2: 0.261  loss_mask_2: 0.3568  loss_dice_2: 2.71  loss_ce_3: 0.2635  loss_mask_3: 0.3574  loss_dice_3: 2.705  loss_ce_4: 0.2501  loss_mask_4: 0.3574  loss_dice_4: 2.7  loss_ce_5: 0.235  loss_mask_5: 0.3582  loss_dice_5: 2.7  loss_ce_6: 0.246  loss_mask_6: 0.3596  loss_dice_6: 2.703  loss_ce_7: 0.2308  loss_mask_7: 0.3578  loss_dice_7: 2.701  loss_ce_8: 0.2437  loss_mask_8: 0.3594  loss_dice_8: 2.694  time: 1.5366  data_time: 0.0936  lr: 7.6139e-06  max_mem: 21478M
[01/17 14:33:41] d2.utils.events INFO:  eta: 1 day, 4:23:46  iter: 23539  total_loss: 35.21  loss_ce: 0.2637  loss_mask: 0.3792  loss_dice: 2.817  loss_ce_0: 0.569  loss_mask_0: 0.3895  loss_dice_0: 2.912  loss_ce_1: 0.2979  loss_mask_1: 0.3899  loss_dice_1: 2.851  loss_ce_2: 0.303  loss_mask_2: 0.3853  loss_dice_2: 2.824  loss_ce_3: 0.2763  loss_mask_3: 0.3862  loss_dice_3: 2.811  loss_ce_4: 0.2765  loss_mask_4: 0.3842  loss_dice_4: 2.818  loss_ce_5: 0.275  loss_mask_5: 0.3819  loss_dice_5: 2.814  loss_ce_6: 0.2585  loss_mask_6: 0.3811  loss_dice_6: 2.817  loss_ce_7: 0.2653  loss_mask_7: 0.3795  loss_dice_7: 2.817  loss_ce_8: 0.2624  loss_mask_8: 0.3787  loss_dice_8: 2.811  time: 1.5366  data_time: 0.1126  lr: 7.6119e-06  max_mem: 21478M
[01/17 14:34:12] d2.utils.events INFO:  eta: 1 day, 4:22:54  iter: 23559  total_loss: 34.18  loss_ce: 0.2509  loss_mask: 0.37  loss_dice: 2.724  loss_ce_0: 0.5471  loss_mask_0: 0.372  loss_dice_0: 2.839  loss_ce_1: 0.291  loss_mask_1: 0.3767  loss_dice_1: 2.768  loss_ce_2: 0.2755  loss_mask_2: 0.3707  loss_dice_2: 2.735  loss_ce_3: 0.2635  loss_mask_3: 0.368  loss_dice_3: 2.723  loss_ce_4: 0.2554  loss_mask_4: 0.3678  loss_dice_4: 2.726  loss_ce_5: 0.2579  loss_mask_5: 0.3692  loss_dice_5: 2.732  loss_ce_6: 0.246  loss_mask_6: 0.3701  loss_dice_6: 2.728  loss_ce_7: 0.237  loss_mask_7: 0.3709  loss_dice_7: 2.725  loss_ce_8: 0.2522  loss_mask_8: 0.3699  loss_dice_8: 2.714  time: 1.5366  data_time: 0.0934  lr: 7.6098e-06  max_mem: 21478M
[01/17 14:34:43] d2.utils.events INFO:  eta: 1 day, 4:21:44  iter: 23579  total_loss: 34.62  loss_ce: 0.2674  loss_mask: 0.3648  loss_dice: 2.789  loss_ce_0: 0.5702  loss_mask_0: 0.3673  loss_dice_0: 2.895  loss_ce_1: 0.2762  loss_mask_1: 0.364  loss_dice_1: 2.838  loss_ce_2: 0.287  loss_mask_2: 0.3629  loss_dice_2: 2.818  loss_ce_3: 0.2761  loss_mask_3: 0.3658  loss_dice_3: 2.799  loss_ce_4: 0.2609  loss_mask_4: 0.3624  loss_dice_4: 2.798  loss_ce_5: 0.2723  loss_mask_5: 0.364  loss_dice_5: 2.8  loss_ce_6: 0.2727  loss_mask_6: 0.3643  loss_dice_6: 2.791  loss_ce_7: 0.2713  loss_mask_7: 0.3619  loss_dice_7: 2.793  loss_ce_8: 0.2622  loss_mask_8: 0.3626  loss_dice_8: 2.79  time: 1.5366  data_time: 0.0854  lr: 7.6078e-06  max_mem: 21478M
[01/17 14:35:13] d2.utils.events INFO:  eta: 1 day, 4:21:13  iter: 23599  total_loss: 34.12  loss_ce: 0.2537  loss_mask: 0.3608  loss_dice: 2.753  loss_ce_0: 0.5694  loss_mask_0: 0.3565  loss_dice_0: 2.854  loss_ce_1: 0.2861  loss_mask_1: 0.3631  loss_dice_1: 2.781  loss_ce_2: 0.2818  loss_mask_2: 0.3619  loss_dice_2: 2.764  loss_ce_3: 0.2721  loss_mask_3: 0.3624  loss_dice_3: 2.751  loss_ce_4: 0.269  loss_mask_4: 0.3639  loss_dice_4: 2.758  loss_ce_5: 0.2451  loss_mask_5: 0.362  loss_dice_5: 2.761  loss_ce_6: 0.2488  loss_mask_6: 0.362  loss_dice_6: 2.75  loss_ce_7: 0.2544  loss_mask_7: 0.3613  loss_dice_7: 2.752  loss_ce_8: 0.2611  loss_mask_8: 0.3623  loss_dice_8: 2.747  time: 1.5366  data_time: 0.0909  lr: 7.6057e-06  max_mem: 21478M
[01/17 14:35:44] d2.utils.events INFO:  eta: 1 day, 4:19:41  iter: 23619  total_loss: 34.02  loss_ce: 0.2483  loss_mask: 0.3545  loss_dice: 2.717  loss_ce_0: 0.5657  loss_mask_0: 0.3614  loss_dice_0: 2.823  loss_ce_1: 0.3035  loss_mask_1: 0.3647  loss_dice_1: 2.756  loss_ce_2: 0.278  loss_mask_2: 0.3607  loss_dice_2: 2.736  loss_ce_3: 0.2508  loss_mask_3: 0.3591  loss_dice_3: 2.723  loss_ce_4: 0.2603  loss_mask_4: 0.3572  loss_dice_4: 2.72  loss_ce_5: 0.2558  loss_mask_5: 0.3583  loss_dice_5: 2.722  loss_ce_6: 0.2574  loss_mask_6: 0.3589  loss_dice_6: 2.721  loss_ce_7: 0.2458  loss_mask_7: 0.3565  loss_dice_7: 2.714  loss_ce_8: 0.26  loss_mask_8: 0.3559  loss_dice_8: 2.717  time: 1.5366  data_time: 0.0886  lr: 7.6036e-06  max_mem: 21478M
[01/17 14:36:15] d2.utils.events INFO:  eta: 1 day, 4:19:20  iter: 23639  total_loss: 35.07  loss_ce: 0.2876  loss_mask: 0.354  loss_dice: 2.839  loss_ce_0: 0.5967  loss_mask_0: 0.3503  loss_dice_0: 2.948  loss_ce_1: 0.3119  loss_mask_1: 0.3583  loss_dice_1: 2.875  loss_ce_2: 0.3233  loss_mask_2: 0.3577  loss_dice_2: 2.856  loss_ce_3: 0.2993  loss_mask_3: 0.3565  loss_dice_3: 2.842  loss_ce_4: 0.3018  loss_mask_4: 0.3544  loss_dice_4: 2.849  loss_ce_5: 0.2781  loss_mask_5: 0.3568  loss_dice_5: 2.848  loss_ce_6: 0.2669  loss_mask_6: 0.3556  loss_dice_6: 2.848  loss_ce_7: 0.2707  loss_mask_7: 0.3543  loss_dice_7: 2.851  loss_ce_8: 0.2776  loss_mask_8: 0.3537  loss_dice_8: 2.849  time: 1.5366  data_time: 0.0965  lr: 7.6016e-06  max_mem: 21478M
[01/17 14:36:46] d2.utils.events INFO:  eta: 1 day, 4:18:24  iter: 23659  total_loss: 34.93  loss_ce: 0.2746  loss_mask: 0.3578  loss_dice: 2.781  loss_ce_0: 0.6041  loss_mask_0: 0.3586  loss_dice_0: 2.895  loss_ce_1: 0.3132  loss_mask_1: 0.3636  loss_dice_1: 2.823  loss_ce_2: 0.3073  loss_mask_2: 0.3613  loss_dice_2: 2.81  loss_ce_3: 0.2858  loss_mask_3: 0.3586  loss_dice_3: 2.8  loss_ce_4: 0.2817  loss_mask_4: 0.3563  loss_dice_4: 2.803  loss_ce_5: 0.2778  loss_mask_5: 0.355  loss_dice_5: 2.789  loss_ce_6: 0.274  loss_mask_6: 0.3567  loss_dice_6: 2.789  loss_ce_7: 0.2766  loss_mask_7: 0.357  loss_dice_7: 2.79  loss_ce_8: 0.2821  loss_mask_8: 0.3584  loss_dice_8: 2.789  time: 1.5366  data_time: 0.0946  lr: 7.5995e-06  max_mem: 21478M
[01/17 14:37:16] d2.utils.events INFO:  eta: 1 day, 4:17:55  iter: 23679  total_loss: 34.13  loss_ce: 0.2582  loss_mask: 0.3587  loss_dice: 2.73  loss_ce_0: 0.5907  loss_mask_0: 0.3629  loss_dice_0: 2.845  loss_ce_1: 0.3063  loss_mask_1: 0.3618  loss_dice_1: 2.769  loss_ce_2: 0.2945  loss_mask_2: 0.3604  loss_dice_2: 2.756  loss_ce_3: 0.2797  loss_mask_3: 0.3593  loss_dice_3: 2.734  loss_ce_4: 0.2604  loss_mask_4: 0.3596  loss_dice_4: 2.734  loss_ce_5: 0.2759  loss_mask_5: 0.3591  loss_dice_5: 2.733  loss_ce_6: 0.2671  loss_mask_6: 0.3579  loss_dice_6: 2.736  loss_ce_7: 0.2587  loss_mask_7: 0.3582  loss_dice_7: 2.735  loss_ce_8: 0.2635  loss_mask_8: 0.3591  loss_dice_8: 2.73  time: 1.5366  data_time: 0.0855  lr: 7.5974e-06  max_mem: 21478M
[01/17 14:37:48] d2.utils.events INFO:  eta: 1 day, 4:17:29  iter: 23699  total_loss: 34.78  loss_ce: 0.2642  loss_mask: 0.3636  loss_dice: 2.794  loss_ce_0: 0.5719  loss_mask_0: 0.3671  loss_dice_0: 2.899  loss_ce_1: 0.2885  loss_mask_1: 0.3712  loss_dice_1: 2.822  loss_ce_2: 0.2806  loss_mask_2: 0.3697  loss_dice_2: 2.801  loss_ce_3: 0.2826  loss_mask_3: 0.3707  loss_dice_3: 2.791  loss_ce_4: 0.2806  loss_mask_4: 0.3698  loss_dice_4: 2.792  loss_ce_5: 0.259  loss_mask_5: 0.3687  loss_dice_5: 2.789  loss_ce_6: 0.2621  loss_mask_6: 0.3678  loss_dice_6: 2.795  loss_ce_7: 0.24  loss_mask_7: 0.3662  loss_dice_7: 2.78  loss_ce_8: 0.253  loss_mask_8: 0.3645  loss_dice_8: 2.793  time: 1.5366  data_time: 0.0977  lr: 7.5954e-06  max_mem: 21478M
[01/17 14:38:19] d2.utils.events INFO:  eta: 1 day, 4:18:48  iter: 23719  total_loss: 34.86  loss_ce: 0.2514  loss_mask: 0.3665  loss_dice: 2.803  loss_ce_0: 0.5894  loss_mask_0: 0.3612  loss_dice_0: 2.92  loss_ce_1: 0.2925  loss_mask_1: 0.368  loss_dice_1: 2.844  loss_ce_2: 0.2818  loss_mask_2: 0.3671  loss_dice_2: 2.819  loss_ce_3: 0.2645  loss_mask_3: 0.3658  loss_dice_3: 2.803  loss_ce_4: 0.2666  loss_mask_4: 0.3641  loss_dice_4: 2.809  loss_ce_5: 0.2502  loss_mask_5: 0.3661  loss_dice_5: 2.813  loss_ce_6: 0.2599  loss_mask_6: 0.3667  loss_dice_6: 2.807  loss_ce_7: 0.244  loss_mask_7: 0.3669  loss_dice_7: 2.806  loss_ce_8: 0.2511  loss_mask_8: 0.3677  loss_dice_8: 2.805  time: 1.5366  data_time: 0.0890  lr: 7.5933e-06  max_mem: 21478M
[01/17 14:38:50] d2.utils.events INFO:  eta: 1 day, 4:17:38  iter: 23739  total_loss: 35.15  loss_ce: 0.2607  loss_mask: 0.3598  loss_dice: 2.832  loss_ce_0: 0.6068  loss_mask_0: 0.3663  loss_dice_0: 2.934  loss_ce_1: 0.3057  loss_mask_1: 0.3655  loss_dice_1: 2.861  loss_ce_2: 0.2963  loss_mask_2: 0.3613  loss_dice_2: 2.843  loss_ce_3: 0.2783  loss_mask_3: 0.3596  loss_dice_3: 2.827  loss_ce_4: 0.2866  loss_mask_4: 0.3607  loss_dice_4: 2.823  loss_ce_5: 0.2649  loss_mask_5: 0.3606  loss_dice_5: 2.82  loss_ce_6: 0.276  loss_mask_6: 0.36  loss_dice_6: 2.818  loss_ce_7: 0.2577  loss_mask_7: 0.3603  loss_dice_7: 2.823  loss_ce_8: 0.2772  loss_mask_8: 0.3598  loss_dice_8: 2.825  time: 1.5366  data_time: 0.0914  lr: 7.5913e-06  max_mem: 21478M
[01/17 14:39:20] d2.utils.events INFO:  eta: 1 day, 4:17:07  iter: 23759  total_loss: 35.07  loss_ce: 0.2717  loss_mask: 0.3615  loss_dice: 2.79  loss_ce_0: 0.5809  loss_mask_0: 0.3701  loss_dice_0: 2.896  loss_ce_1: 0.3086  loss_mask_1: 0.3712  loss_dice_1: 2.827  loss_ce_2: 0.2999  loss_mask_2: 0.3657  loss_dice_2: 2.802  loss_ce_3: 0.2815  loss_mask_3: 0.3608  loss_dice_3: 2.796  loss_ce_4: 0.2757  loss_mask_4: 0.361  loss_dice_4: 2.8  loss_ce_5: 0.2745  loss_mask_5: 0.3624  loss_dice_5: 2.796  loss_ce_6: 0.2802  loss_mask_6: 0.361  loss_dice_6: 2.792  loss_ce_7: 0.2755  loss_mask_7: 0.3614  loss_dice_7: 2.782  loss_ce_8: 0.2781  loss_mask_8: 0.3629  loss_dice_8: 2.788  time: 1.5366  data_time: 0.0890  lr: 7.5892e-06  max_mem: 21478M
[01/17 14:39:51] d2.utils.events INFO:  eta: 1 day, 4:17:16  iter: 23779  total_loss: 34  loss_ce: 0.2545  loss_mask: 0.3584  loss_dice: 2.754  loss_ce_0: 0.5663  loss_mask_0: 0.3664  loss_dice_0: 2.862  loss_ce_1: 0.2966  loss_mask_1: 0.3666  loss_dice_1: 2.789  loss_ce_2: 0.2911  loss_mask_2: 0.3628  loss_dice_2: 2.777  loss_ce_3: 0.2612  loss_mask_3: 0.362  loss_dice_3: 2.757  loss_ce_4: 0.2747  loss_mask_4: 0.3622  loss_dice_4: 2.764  loss_ce_5: 0.2711  loss_mask_5: 0.3617  loss_dice_5: 2.771  loss_ce_6: 0.2485  loss_mask_6: 0.3621  loss_dice_6: 2.762  loss_ce_7: 0.2536  loss_mask_7: 0.3611  loss_dice_7: 2.763  loss_ce_8: 0.2659  loss_mask_8: 0.3601  loss_dice_8: 2.747  time: 1.5366  data_time: 0.1020  lr: 7.5871e-06  max_mem: 21478M
[01/17 14:40:22] d2.utils.events INFO:  eta: 1 day, 4:17:13  iter: 23799  total_loss: 34.61  loss_ce: 0.2536  loss_mask: 0.3507  loss_dice: 2.738  loss_ce_0: 0.5615  loss_mask_0: 0.3489  loss_dice_0: 2.845  loss_ce_1: 0.3038  loss_mask_1: 0.3511  loss_dice_1: 2.775  loss_ce_2: 0.3079  loss_mask_2: 0.3503  loss_dice_2: 2.76  loss_ce_3: 0.277  loss_mask_3: 0.3509  loss_dice_3: 2.744  loss_ce_4: 0.2706  loss_mask_4: 0.3503  loss_dice_4: 2.745  loss_ce_5: 0.2719  loss_mask_5: 0.3507  loss_dice_5: 2.747  loss_ce_6: 0.2661  loss_mask_6: 0.3512  loss_dice_6: 2.736  loss_ce_7: 0.2567  loss_mask_7: 0.3501  loss_dice_7: 2.746  loss_ce_8: 0.2637  loss_mask_8: 0.3504  loss_dice_8: 2.731  time: 1.5366  data_time: 0.0983  lr: 7.5851e-06  max_mem: 21478M
[01/17 14:40:52] d2.utils.events INFO:  eta: 1 day, 4:16:35  iter: 23819  total_loss: 33.77  loss_ce: 0.2577  loss_mask: 0.3546  loss_dice: 2.703  loss_ce_0: 0.5895  loss_mask_0: 0.3542  loss_dice_0: 2.802  loss_ce_1: 0.299  loss_mask_1: 0.3589  loss_dice_1: 2.733  loss_ce_2: 0.286  loss_mask_2: 0.3542  loss_dice_2: 2.713  loss_ce_3: 0.2573  loss_mask_3: 0.3527  loss_dice_3: 2.711  loss_ce_4: 0.2622  loss_mask_4: 0.3531  loss_dice_4: 2.71  loss_ce_5: 0.2619  loss_mask_5: 0.3532  loss_dice_5: 2.706  loss_ce_6: 0.2646  loss_mask_6: 0.3526  loss_dice_6: 2.708  loss_ce_7: 0.2578  loss_mask_7: 0.3552  loss_dice_7: 2.701  loss_ce_8: 0.2549  loss_mask_8: 0.3547  loss_dice_8: 2.701  time: 1.5366  data_time: 0.0905  lr: 7.583e-06  max_mem: 21478M
[01/17 14:41:23] d2.utils.events INFO:  eta: 1 day, 4:16:07  iter: 23839  total_loss: 34.44  loss_ce: 0.2597  loss_mask: 0.3479  loss_dice: 2.798  loss_ce_0: 0.5767  loss_mask_0: 0.3475  loss_dice_0: 2.914  loss_ce_1: 0.301  loss_mask_1: 0.3486  loss_dice_1: 2.842  loss_ce_2: 0.3011  loss_mask_2: 0.3449  loss_dice_2: 2.819  loss_ce_3: 0.2731  loss_mask_3: 0.346  loss_dice_3: 2.809  loss_ce_4: 0.273  loss_mask_4: 0.3452  loss_dice_4: 2.805  loss_ce_5: 0.2555  loss_mask_5: 0.345  loss_dice_5: 2.801  loss_ce_6: 0.2534  loss_mask_6: 0.3478  loss_dice_6: 2.8  loss_ce_7: 0.2718  loss_mask_7: 0.3491  loss_dice_7: 2.791  loss_ce_8: 0.2625  loss_mask_8: 0.3477  loss_dice_8: 2.804  time: 1.5366  data_time: 0.1002  lr: 7.5809e-06  max_mem: 21478M
[01/17 14:41:54] d2.utils.events INFO:  eta: 1 day, 4:16:00  iter: 23859  total_loss: 34.86  loss_ce: 0.2643  loss_mask: 0.3551  loss_dice: 2.804  loss_ce_0: 0.5827  loss_mask_0: 0.3638  loss_dice_0: 2.92  loss_ce_1: 0.2979  loss_mask_1: 0.3644  loss_dice_1: 2.846  loss_ce_2: 0.2989  loss_mask_2: 0.3611  loss_dice_2: 2.817  loss_ce_3: 0.2914  loss_mask_3: 0.3611  loss_dice_3: 2.814  loss_ce_4: 0.264  loss_mask_4: 0.3598  loss_dice_4: 2.809  loss_ce_5: 0.2673  loss_mask_5: 0.3582  loss_dice_5: 2.814  loss_ce_6: 0.2548  loss_mask_6: 0.3567  loss_dice_6: 2.813  loss_ce_7: 0.2588  loss_mask_7: 0.3548  loss_dice_7: 2.812  loss_ce_8: 0.2734  loss_mask_8: 0.3548  loss_dice_8: 2.807  time: 1.5366  data_time: 0.0912  lr: 7.5789e-06  max_mem: 21478M
[01/17 14:42:25] d2.utils.events INFO:  eta: 1 day, 4:15:30  iter: 23879  total_loss: 34.06  loss_ce: 0.2451  loss_mask: 0.3698  loss_dice: 2.733  loss_ce_0: 0.5348  loss_mask_0: 0.3706  loss_dice_0: 2.838  loss_ce_1: 0.272  loss_mask_1: 0.3737  loss_dice_1: 2.776  loss_ce_2: 0.2654  loss_mask_2: 0.3676  loss_dice_2: 2.749  loss_ce_3: 0.2575  loss_mask_3: 0.3679  loss_dice_3: 2.736  loss_ce_4: 0.2391  loss_mask_4: 0.3695  loss_dice_4: 2.736  loss_ce_5: 0.2402  loss_mask_5: 0.3707  loss_dice_5: 2.739  loss_ce_6: 0.2396  loss_mask_6: 0.3713  loss_dice_6: 2.741  loss_ce_7: 0.2375  loss_mask_7: 0.3705  loss_dice_7: 2.725  loss_ce_8: 0.2419  loss_mask_8: 0.3708  loss_dice_8: 2.724  time: 1.5366  data_time: 0.0949  lr: 7.5768e-06  max_mem: 21478M
[01/17 14:42:56] d2.utils.events INFO:  eta: 1 day, 4:15:16  iter: 23899  total_loss: 33.72  loss_ce: 0.246  loss_mask: 0.3672  loss_dice: 2.701  loss_ce_0: 0.5727  loss_mask_0: 0.3689  loss_dice_0: 2.81  loss_ce_1: 0.2876  loss_mask_1: 0.3722  loss_dice_1: 2.733  loss_ce_2: 0.2688  loss_mask_2: 0.3694  loss_dice_2: 2.714  loss_ce_3: 0.2746  loss_mask_3: 0.3712  loss_dice_3: 2.712  loss_ce_4: 0.2465  loss_mask_4: 0.3706  loss_dice_4: 2.702  loss_ce_5: 0.2487  loss_mask_5: 0.3689  loss_dice_5: 2.699  loss_ce_6: 0.2416  loss_mask_6: 0.3692  loss_dice_6: 2.709  loss_ce_7: 0.2447  loss_mask_7: 0.3678  loss_dice_7: 2.7  loss_ce_8: 0.2494  loss_mask_8: 0.3686  loss_dice_8: 2.7  time: 1.5366  data_time: 0.0898  lr: 7.5748e-06  max_mem: 21478M
[01/17 14:43:26] d2.utils.events INFO:  eta: 1 day, 4:14:28  iter: 23919  total_loss: 33.44  loss_ce: 0.237  loss_mask: 0.3534  loss_dice: 2.665  loss_ce_0: 0.5681  loss_mask_0: 0.3552  loss_dice_0: 2.78  loss_ce_1: 0.2892  loss_mask_1: 0.358  loss_dice_1: 2.716  loss_ce_2: 0.2784  loss_mask_2: 0.3534  loss_dice_2: 2.69  loss_ce_3: 0.2485  loss_mask_3: 0.353  loss_dice_3: 2.683  loss_ce_4: 0.2532  loss_mask_4: 0.3521  loss_dice_4: 2.669  loss_ce_5: 0.2433  loss_mask_5: 0.3515  loss_dice_5: 2.678  loss_ce_6: 0.2389  loss_mask_6: 0.3514  loss_dice_6: 2.676  loss_ce_7: 0.2484  loss_mask_7: 0.3541  loss_dice_7: 2.67  loss_ce_8: 0.2514  loss_mask_8: 0.3535  loss_dice_8: 2.678  time: 1.5366  data_time: 0.1017  lr: 7.5727e-06  max_mem: 21478M
[01/17 14:43:57] d2.utils.events INFO:  eta: 1 day, 4:14:14  iter: 23939  total_loss: 34.14  loss_ce: 0.2688  loss_mask: 0.3534  loss_dice: 2.727  loss_ce_0: 0.5757  loss_mask_0: 0.3525  loss_dice_0: 2.859  loss_ce_1: 0.2866  loss_mask_1: 0.3584  loss_dice_1: 2.767  loss_ce_2: 0.2701  loss_mask_2: 0.3566  loss_dice_2: 2.745  loss_ce_3: 0.2733  loss_mask_3: 0.3523  loss_dice_3: 2.728  loss_ce_4: 0.2812  loss_mask_4: 0.3529  loss_dice_4: 2.726  loss_ce_5: 0.2659  loss_mask_5: 0.3543  loss_dice_5: 2.727  loss_ce_6: 0.2637  loss_mask_6: 0.3548  loss_dice_6: 2.723  loss_ce_7: 0.2695  loss_mask_7: 0.3532  loss_dice_7: 2.729  loss_ce_8: 0.2577  loss_mask_8: 0.3538  loss_dice_8: 2.726  time: 1.5366  data_time: 0.1020  lr: 7.5706e-06  max_mem: 21478M
[01/17 14:44:28] d2.utils.events INFO:  eta: 1 day, 4:14:13  iter: 23959  total_loss: 34.72  loss_ce: 0.256  loss_mask: 0.3398  loss_dice: 2.784  loss_ce_0: 0.5708  loss_mask_0: 0.3495  loss_dice_0: 2.912  loss_ce_1: 0.3002  loss_mask_1: 0.3457  loss_dice_1: 2.836  loss_ce_2: 0.3086  loss_mask_2: 0.3416  loss_dice_2: 2.8  loss_ce_3: 0.3001  loss_mask_3: 0.3393  loss_dice_3: 2.793  loss_ce_4: 0.2877  loss_mask_4: 0.3402  loss_dice_4: 2.793  loss_ce_5: 0.2677  loss_mask_5: 0.3403  loss_dice_5: 2.787  loss_ce_6: 0.261  loss_mask_6: 0.3386  loss_dice_6: 2.797  loss_ce_7: 0.2619  loss_mask_7: 0.3406  loss_dice_7: 2.789  loss_ce_8: 0.2745  loss_mask_8: 0.3389  loss_dice_8: 2.785  time: 1.5366  data_time: 0.0930  lr: 7.5686e-06  max_mem: 21478M
[01/17 14:44:59] d2.utils.events INFO:  eta: 1 day, 4:14:40  iter: 23979  total_loss: 34.23  loss_ce: 0.247  loss_mask: 0.3452  loss_dice: 2.757  loss_ce_0: 0.5617  loss_mask_0: 0.3463  loss_dice_0: 2.865  loss_ce_1: 0.292  loss_mask_1: 0.3511  loss_dice_1: 2.791  loss_ce_2: 0.2925  loss_mask_2: 0.3477  loss_dice_2: 2.768  loss_ce_3: 0.2695  loss_mask_3: 0.3461  loss_dice_3: 2.759  loss_ce_4: 0.2745  loss_mask_4: 0.3451  loss_dice_4: 2.762  loss_ce_5: 0.2715  loss_mask_5: 0.3463  loss_dice_5: 2.761  loss_ce_6: 0.2578  loss_mask_6: 0.3453  loss_dice_6: 2.744  loss_ce_7: 0.2518  loss_mask_7: 0.3446  loss_dice_7: 2.757  loss_ce_8: 0.2427  loss_mask_8: 0.3466  loss_dice_8: 2.745  time: 1.5367  data_time: 0.0960  lr: 7.5665e-06  max_mem: 21478M
[01/17 14:45:30] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 14:45:31] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 14:45:31] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 14:45:31] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 14:45:46] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0070 s/iter. Inference: 0.1594 s/iter. Eval: 0.1939 s/iter. Total: 0.3603 s/iter. ETA=0:06:29
[01/17 14:45:51] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0110 s/iter. Inference: 0.1762 s/iter. Eval: 0.2134 s/iter. Total: 0.4008 s/iter. ETA=0:07:08
[01/17 14:45:57] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0111 s/iter. Inference: 0.1738 s/iter. Eval: 0.2064 s/iter. Total: 0.3914 s/iter. ETA=0:06:52
[01/17 14:46:02] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0125 s/iter. Inference: 0.1763 s/iter. Eval: 0.2129 s/iter. Total: 0.4018 s/iter. ETA=0:06:59
[01/17 14:46:07] d2.evaluation.evaluator INFO: Inference done 61/1093. Dataloading: 0.0134 s/iter. Inference: 0.1769 s/iter. Eval: 0.2283 s/iter. Total: 0.4187 s/iter. ETA=0:07:12
[01/17 14:46:13] d2.evaluation.evaluator INFO: Inference done 73/1093. Dataloading: 0.0135 s/iter. Inference: 0.1782 s/iter. Eval: 0.2314 s/iter. Total: 0.4232 s/iter. ETA=0:07:11
[01/17 14:46:18] d2.evaluation.evaluator INFO: Inference done 85/1093. Dataloading: 0.0136 s/iter. Inference: 0.1791 s/iter. Eval: 0.2354 s/iter. Total: 0.4282 s/iter. ETA=0:07:11
[01/17 14:46:23] d2.evaluation.evaluator INFO: Inference done 97/1093. Dataloading: 0.0139 s/iter. Inference: 0.1782 s/iter. Eval: 0.2375 s/iter. Total: 0.4297 s/iter. ETA=0:07:07
[01/17 14:46:29] d2.evaluation.evaluator INFO: Inference done 110/1093. Dataloading: 0.0140 s/iter. Inference: 0.1778 s/iter. Eval: 0.2343 s/iter. Total: 0.4262 s/iter. ETA=0:06:59
[01/17 14:46:34] d2.evaluation.evaluator INFO: Inference done 122/1093. Dataloading: 0.0140 s/iter. Inference: 0.1768 s/iter. Eval: 0.2343 s/iter. Total: 0.4253 s/iter. ETA=0:06:52
[01/17 14:46:39] d2.evaluation.evaluator INFO: Inference done 136/1093. Dataloading: 0.0138 s/iter. Inference: 0.1761 s/iter. Eval: 0.2307 s/iter. Total: 0.4207 s/iter. ETA=0:06:42
[01/17 14:46:44] d2.evaluation.evaluator INFO: Inference done 149/1093. Dataloading: 0.0139 s/iter. Inference: 0.1761 s/iter. Eval: 0.2284 s/iter. Total: 0.4185 s/iter. ETA=0:06:35
[01/17 14:46:50] d2.evaluation.evaluator INFO: Inference done 162/1093. Dataloading: 0.0139 s/iter. Inference: 0.1756 s/iter. Eval: 0.2291 s/iter. Total: 0.4188 s/iter. ETA=0:06:29
[01/17 14:46:55] d2.evaluation.evaluator INFO: Inference done 174/1093. Dataloading: 0.0139 s/iter. Inference: 0.1746 s/iter. Eval: 0.2300 s/iter. Total: 0.4187 s/iter. ETA=0:06:24
[01/17 14:47:00] d2.evaluation.evaluator INFO: Inference done 187/1093. Dataloading: 0.0138 s/iter. Inference: 0.1741 s/iter. Eval: 0.2295 s/iter. Total: 0.4175 s/iter. ETA=0:06:18
[01/17 14:47:05] d2.evaluation.evaluator INFO: Inference done 200/1093. Dataloading: 0.0136 s/iter. Inference: 0.1744 s/iter. Eval: 0.2276 s/iter. Total: 0.4157 s/iter. ETA=0:06:11
[01/17 14:47:10] d2.evaluation.evaluator INFO: Inference done 212/1093. Dataloading: 0.0136 s/iter. Inference: 0.1749 s/iter. Eval: 0.2290 s/iter. Total: 0.4177 s/iter. ETA=0:06:07
[01/17 14:47:15] d2.evaluation.evaluator INFO: Inference done 225/1093. Dataloading: 0.0135 s/iter. Inference: 0.1744 s/iter. Eval: 0.2284 s/iter. Total: 0.4164 s/iter. ETA=0:06:01
[01/17 14:47:21] d2.evaluation.evaluator INFO: Inference done 237/1093. Dataloading: 0.0135 s/iter. Inference: 0.1747 s/iter. Eval: 0.2291 s/iter. Total: 0.4174 s/iter. ETA=0:05:57
[01/17 14:47:26] d2.evaluation.evaluator INFO: Inference done 249/1093. Dataloading: 0.0135 s/iter. Inference: 0.1749 s/iter. Eval: 0.2296 s/iter. Total: 0.4182 s/iter. ETA=0:05:52
[01/17 14:47:31] d2.evaluation.evaluator INFO: Inference done 261/1093. Dataloading: 0.0135 s/iter. Inference: 0.1750 s/iter. Eval: 0.2297 s/iter. Total: 0.4184 s/iter. ETA=0:05:48
[01/17 14:47:36] d2.evaluation.evaluator INFO: Inference done 274/1093. Dataloading: 0.0135 s/iter. Inference: 0.1752 s/iter. Eval: 0.2292 s/iter. Total: 0.4181 s/iter. ETA=0:05:42
[01/17 14:47:41] d2.evaluation.evaluator INFO: Inference done 288/1093. Dataloading: 0.0134 s/iter. Inference: 0.1749 s/iter. Eval: 0.2274 s/iter. Total: 0.4158 s/iter. ETA=0:05:34
[01/17 14:47:47] d2.evaluation.evaluator INFO: Inference done 300/1093. Dataloading: 0.0134 s/iter. Inference: 0.1759 s/iter. Eval: 0.2269 s/iter. Total: 0.4163 s/iter. ETA=0:05:30
[01/17 14:47:52] d2.evaluation.evaluator INFO: Inference done 311/1093. Dataloading: 0.0134 s/iter. Inference: 0.1762 s/iter. Eval: 0.2283 s/iter. Total: 0.4180 s/iter. ETA=0:05:26
[01/17 14:47:57] d2.evaluation.evaluator INFO: Inference done 323/1093. Dataloading: 0.0134 s/iter. Inference: 0.1761 s/iter. Eval: 0.2284 s/iter. Total: 0.4181 s/iter. ETA=0:05:21
[01/17 14:48:02] d2.evaluation.evaluator INFO: Inference done 337/1093. Dataloading: 0.0134 s/iter. Inference: 0.1757 s/iter. Eval: 0.2264 s/iter. Total: 0.4156 s/iter. ETA=0:05:14
[01/17 14:48:07] d2.evaluation.evaluator INFO: Inference done 351/1093. Dataloading: 0.0134 s/iter. Inference: 0.1761 s/iter. Eval: 0.2245 s/iter. Total: 0.4141 s/iter. ETA=0:05:07
[01/17 14:48:12] d2.evaluation.evaluator INFO: Inference done 363/1093. Dataloading: 0.0135 s/iter. Inference: 0.1765 s/iter. Eval: 0.2242 s/iter. Total: 0.4143 s/iter. ETA=0:05:02
[01/17 14:48:17] d2.evaluation.evaluator INFO: Inference done 377/1093. Dataloading: 0.0135 s/iter. Inference: 0.1761 s/iter. Eval: 0.2234 s/iter. Total: 0.4131 s/iter. ETA=0:04:55
[01/17 14:48:23] d2.evaluation.evaluator INFO: Inference done 389/1093. Dataloading: 0.0140 s/iter. Inference: 0.1759 s/iter. Eval: 0.2236 s/iter. Total: 0.4136 s/iter. ETA=0:04:51
[01/17 14:48:28] d2.evaluation.evaluator INFO: Inference done 403/1093. Dataloading: 0.0140 s/iter. Inference: 0.1755 s/iter. Eval: 0.2228 s/iter. Total: 0.4124 s/iter. ETA=0:04:44
[01/17 14:48:33] d2.evaluation.evaluator INFO: Inference done 418/1093. Dataloading: 0.0139 s/iter. Inference: 0.1745 s/iter. Eval: 0.2217 s/iter. Total: 0.4102 s/iter. ETA=0:04:36
[01/17 14:48:39] d2.evaluation.evaluator INFO: Inference done 429/1093. Dataloading: 0.0139 s/iter. Inference: 0.1748 s/iter. Eval: 0.2233 s/iter. Total: 0.4122 s/iter. ETA=0:04:33
[01/17 14:48:44] d2.evaluation.evaluator INFO: Inference done 443/1093. Dataloading: 0.0139 s/iter. Inference: 0.1746 s/iter. Eval: 0.2224 s/iter. Total: 0.4110 s/iter. ETA=0:04:27
[01/17 14:48:49] d2.evaluation.evaluator INFO: Inference done 457/1093. Dataloading: 0.0139 s/iter. Inference: 0.1744 s/iter. Eval: 0.2215 s/iter. Total: 0.4099 s/iter. ETA=0:04:20
[01/17 14:48:54] d2.evaluation.evaluator INFO: Inference done 471/1093. Dataloading: 0.0138 s/iter. Inference: 0.1742 s/iter. Eval: 0.2208 s/iter. Total: 0.4089 s/iter. ETA=0:04:14
[01/17 14:49:00] d2.evaluation.evaluator INFO: Inference done 486/1093. Dataloading: 0.0137 s/iter. Inference: 0.1737 s/iter. Eval: 0.2200 s/iter. Total: 0.4075 s/iter. ETA=0:04:07
[01/17 14:49:05] d2.evaluation.evaluator INFO: Inference done 501/1093. Dataloading: 0.0136 s/iter. Inference: 0.1735 s/iter. Eval: 0.2184 s/iter. Total: 0.4056 s/iter. ETA=0:04:00
[01/17 14:49:10] d2.evaluation.evaluator INFO: Inference done 516/1093. Dataloading: 0.0135 s/iter. Inference: 0.1733 s/iter. Eval: 0.2169 s/iter. Total: 0.4038 s/iter. ETA=0:03:52
[01/17 14:49:15] d2.evaluation.evaluator INFO: Inference done 528/1093. Dataloading: 0.0135 s/iter. Inference: 0.1733 s/iter. Eval: 0.2179 s/iter. Total: 0.4048 s/iter. ETA=0:03:48
[01/17 14:49:21] d2.evaluation.evaluator INFO: Inference done 541/1093. Dataloading: 0.0135 s/iter. Inference: 0.1732 s/iter. Eval: 0.2178 s/iter. Total: 0.4046 s/iter. ETA=0:03:43
[01/17 14:49:26] d2.evaluation.evaluator INFO: Inference done 553/1093. Dataloading: 0.0136 s/iter. Inference: 0.1737 s/iter. Eval: 0.2182 s/iter. Total: 0.4055 s/iter. ETA=0:03:38
[01/17 14:49:31] d2.evaluation.evaluator INFO: Inference done 568/1093. Dataloading: 0.0135 s/iter. Inference: 0.1731 s/iter. Eval: 0.2175 s/iter. Total: 0.4042 s/iter. ETA=0:03:32
[01/17 14:49:37] d2.evaluation.evaluator INFO: Inference done 584/1093. Dataloading: 0.0136 s/iter. Inference: 0.1728 s/iter. Eval: 0.2154 s/iter. Total: 0.4019 s/iter. ETA=0:03:24
[01/17 14:49:42] d2.evaluation.evaluator INFO: Inference done 598/1093. Dataloading: 0.0135 s/iter. Inference: 0.1725 s/iter. Eval: 0.2152 s/iter. Total: 0.4014 s/iter. ETA=0:03:18
[01/17 14:49:47] d2.evaluation.evaluator INFO: Inference done 609/1093. Dataloading: 0.0136 s/iter. Inference: 0.1726 s/iter. Eval: 0.2164 s/iter. Total: 0.4027 s/iter. ETA=0:03:14
[01/17 14:49:52] d2.evaluation.evaluator INFO: Inference done 622/1093. Dataloading: 0.0136 s/iter. Inference: 0.1727 s/iter. Eval: 0.2160 s/iter. Total: 0.4024 s/iter. ETA=0:03:09
[01/17 14:49:57] d2.evaluation.evaluator INFO: Inference done 634/1093. Dataloading: 0.0135 s/iter. Inference: 0.1725 s/iter. Eval: 0.2165 s/iter. Total: 0.4027 s/iter. ETA=0:03:04
[01/17 14:50:02] d2.evaluation.evaluator INFO: Inference done 649/1093. Dataloading: 0.0135 s/iter. Inference: 0.1725 s/iter. Eval: 0.2154 s/iter. Total: 0.4015 s/iter. ETA=0:02:58
[01/17 14:50:08] d2.evaluation.evaluator INFO: Inference done 663/1093. Dataloading: 0.0134 s/iter. Inference: 0.1725 s/iter. Eval: 0.2152 s/iter. Total: 0.4012 s/iter. ETA=0:02:52
[01/17 14:50:13] d2.evaluation.evaluator INFO: Inference done 675/1093. Dataloading: 0.0134 s/iter. Inference: 0.1729 s/iter. Eval: 0.2153 s/iter. Total: 0.4018 s/iter. ETA=0:02:47
[01/17 14:50:18] d2.evaluation.evaluator INFO: Inference done 690/1093. Dataloading: 0.0134 s/iter. Inference: 0.1728 s/iter. Eval: 0.2140 s/iter. Total: 0.4004 s/iter. ETA=0:02:41
[01/17 14:50:23] d2.evaluation.evaluator INFO: Inference done 702/1093. Dataloading: 0.0134 s/iter. Inference: 0.1733 s/iter. Eval: 0.2139 s/iter. Total: 0.4007 s/iter. ETA=0:02:36
[01/17 14:50:28] d2.evaluation.evaluator INFO: Inference done 714/1093. Dataloading: 0.0134 s/iter. Inference: 0.1729 s/iter. Eval: 0.2148 s/iter. Total: 0.4013 s/iter. ETA=0:02:32
[01/17 14:50:34] d2.evaluation.evaluator INFO: Inference done 727/1093. Dataloading: 0.0134 s/iter. Inference: 0.1731 s/iter. Eval: 0.2147 s/iter. Total: 0.4014 s/iter. ETA=0:02:26
[01/17 14:50:39] d2.evaluation.evaluator INFO: Inference done 741/1093. Dataloading: 0.0134 s/iter. Inference: 0.1730 s/iter. Eval: 0.2141 s/iter. Total: 0.4006 s/iter. ETA=0:02:21
[01/17 14:50:44] d2.evaluation.evaluator INFO: Inference done 754/1093. Dataloading: 0.0134 s/iter. Inference: 0.1730 s/iter. Eval: 0.2139 s/iter. Total: 0.4004 s/iter. ETA=0:02:15
[01/17 14:50:49] d2.evaluation.evaluator INFO: Inference done 768/1093. Dataloading: 0.0133 s/iter. Inference: 0.1728 s/iter. Eval: 0.2137 s/iter. Total: 0.3999 s/iter. ETA=0:02:09
[01/17 14:50:54] d2.evaluation.evaluator INFO: Inference done 781/1093. Dataloading: 0.0133 s/iter. Inference: 0.1728 s/iter. Eval: 0.2134 s/iter. Total: 0.3997 s/iter. ETA=0:02:04
[01/17 14:50:59] d2.evaluation.evaluator INFO: Inference done 797/1093. Dataloading: 0.0132 s/iter. Inference: 0.1723 s/iter. Eval: 0.2125 s/iter. Total: 0.3982 s/iter. ETA=0:01:57
[01/17 14:51:04] d2.evaluation.evaluator INFO: Inference done 810/1093. Dataloading: 0.0132 s/iter. Inference: 0.1723 s/iter. Eval: 0.2127 s/iter. Total: 0.3984 s/iter. ETA=0:01:52
[01/17 14:51:10] d2.evaluation.evaluator INFO: Inference done 825/1093. Dataloading: 0.0132 s/iter. Inference: 0.1723 s/iter. Eval: 0.2120 s/iter. Total: 0.3976 s/iter. ETA=0:01:46
[01/17 14:51:15] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0131 s/iter. Inference: 0.1724 s/iter. Eval: 0.2113 s/iter. Total: 0.3970 s/iter. ETA=0:01:40
[01/17 14:51:20] d2.evaluation.evaluator INFO: Inference done 853/1093. Dataloading: 0.0131 s/iter. Inference: 0.1725 s/iter. Eval: 0.2111 s/iter. Total: 0.3968 s/iter. ETA=0:01:35
[01/17 14:51:25] d2.evaluation.evaluator INFO: Inference done 865/1093. Dataloading: 0.0131 s/iter. Inference: 0.1723 s/iter. Eval: 0.2117 s/iter. Total: 0.3973 s/iter. ETA=0:01:30
[01/17 14:51:30] d2.evaluation.evaluator INFO: Inference done 879/1093. Dataloading: 0.0131 s/iter. Inference: 0.1721 s/iter. Eval: 0.2114 s/iter. Total: 0.3967 s/iter. ETA=0:01:24
[01/17 14:51:36] d2.evaluation.evaluator INFO: Inference done 890/1093. Dataloading: 0.0131 s/iter. Inference: 0.1721 s/iter. Eval: 0.2122 s/iter. Total: 0.3975 s/iter. ETA=0:01:20
[01/17 14:51:41] d2.evaluation.evaluator INFO: Inference done 905/1093. Dataloading: 0.0130 s/iter. Inference: 0.1720 s/iter. Eval: 0.2114 s/iter. Total: 0.3966 s/iter. ETA=0:01:14
[01/17 14:51:46] d2.evaluation.evaluator INFO: Inference done 918/1093. Dataloading: 0.0130 s/iter. Inference: 0.1718 s/iter. Eval: 0.2116 s/iter. Total: 0.3966 s/iter. ETA=0:01:09
[01/17 14:51:51] d2.evaluation.evaluator INFO: Inference done 932/1093. Dataloading: 0.0130 s/iter. Inference: 0.1716 s/iter. Eval: 0.2116 s/iter. Total: 0.3963 s/iter. ETA=0:01:03
[01/17 14:51:56] d2.evaluation.evaluator INFO: Inference done 945/1093. Dataloading: 0.0130 s/iter. Inference: 0.1716 s/iter. Eval: 0.2116 s/iter. Total: 0.3963 s/iter. ETA=0:00:58
[01/17 14:52:02] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0130 s/iter. Inference: 0.1717 s/iter. Eval: 0.2117 s/iter. Total: 0.3965 s/iter. ETA=0:00:53
[01/17 14:52:07] d2.evaluation.evaluator INFO: Inference done 971/1093. Dataloading: 0.0130 s/iter. Inference: 0.1718 s/iter. Eval: 0.2115 s/iter. Total: 0.3964 s/iter. ETA=0:00:48
[01/17 14:52:12] d2.evaluation.evaluator INFO: Inference done 986/1093. Dataloading: 0.0130 s/iter. Inference: 0.1716 s/iter. Eval: 0.2109 s/iter. Total: 0.3956 s/iter. ETA=0:00:42
[01/17 14:52:17] d2.evaluation.evaluator INFO: Inference done 998/1093. Dataloading: 0.0130 s/iter. Inference: 0.1718 s/iter. Eval: 0.2111 s/iter. Total: 0.3960 s/iter. ETA=0:00:37
[01/17 14:52:22] d2.evaluation.evaluator INFO: Inference done 1011/1093. Dataloading: 0.0129 s/iter. Inference: 0.1719 s/iter. Eval: 0.2109 s/iter. Total: 0.3959 s/iter. ETA=0:00:32
[01/17 14:52:27] d2.evaluation.evaluator INFO: Inference done 1024/1093. Dataloading: 0.0129 s/iter. Inference: 0.1718 s/iter. Eval: 0.2111 s/iter. Total: 0.3960 s/iter. ETA=0:00:27
[01/17 14:52:32] d2.evaluation.evaluator INFO: Inference done 1036/1093. Dataloading: 0.0129 s/iter. Inference: 0.1718 s/iter. Eval: 0.2114 s/iter. Total: 0.3962 s/iter. ETA=0:00:22
[01/17 14:52:37] d2.evaluation.evaluator INFO: Inference done 1049/1093. Dataloading: 0.0129 s/iter. Inference: 0.1718 s/iter. Eval: 0.2113 s/iter. Total: 0.3962 s/iter. ETA=0:00:17
[01/17 14:52:43] d2.evaluation.evaluator INFO: Inference done 1063/1093. Dataloading: 0.0129 s/iter. Inference: 0.1719 s/iter. Eval: 0.2111 s/iter. Total: 0.3960 s/iter. ETA=0:00:11
[01/17 14:52:48] d2.evaluation.evaluator INFO: Inference done 1079/1093. Dataloading: 0.0128 s/iter. Inference: 0.1716 s/iter. Eval: 0.2103 s/iter. Total: 0.3949 s/iter. ETA=0:00:05
[01/17 14:52:53] d2.evaluation.evaluator INFO: Total inference time: 0:07:08.835016 (0.394150 s / iter per device, on 4 devices)
[01/17 14:52:53] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:06 (0.171197 s / iter per device, on 4 devices)
[01/17 14:53:17] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 14.454478626150921, 'fwIoU': 40.976606386168754, 'IoU-1': nan, 'IoU-2': 95.40774617205058, 'IoU-3': 43.81241230275153, 'IoU-4': 59.16145278765518, 'IoU-5': 52.28468106479156, 'IoU-6': 45.48673042881277, 'IoU-7': 41.27168506803344, 'IoU-8': 33.762141802768916, 'IoU-9': 21.55481279096108, 'IoU-10': 32.14654640877359, 'IoU-11': 39.93940264679623, 'IoU-12': 53.12685950871787, 'IoU-13': 52.673819407039026, 'IoU-14': 52.537175932236615, 'IoU-15': 52.87003402028214, 'IoU-16': 51.48048368150011, 'IoU-17': 53.222517870197116, 'IoU-18': 49.52326319055504, 'IoU-19': 48.02873900739872, 'IoU-20': 46.28842744641474, 'IoU-21': 47.079560766816314, 'IoU-22': 45.67325070160223, 'IoU-23': 46.1427733611338, 'IoU-24': 42.179715962170164, 'IoU-25': 43.80902908094253, 'IoU-26': 42.033580710508836, 'IoU-27': 40.76018160582494, 'IoU-28': 41.054304211087164, 'IoU-29': 39.859357382899304, 'IoU-30': 40.209069603093255, 'IoU-31': 38.55520218715567, 'IoU-32': 41.39667581613111, 'IoU-33': 39.213129310179276, 'IoU-34': 36.97293565346501, 'IoU-35': 39.706348532934776, 'IoU-36': 41.03486281095628, 'IoU-37': 40.45063374789909, 'IoU-38': 39.4644320071521, 'IoU-39': 38.320380342445894, 'IoU-40': 37.10086505216278, 'IoU-41': 36.92234246858811, 'IoU-42': 36.067193555470354, 'IoU-43': 35.169737669246885, 'IoU-44': 35.284081017089605, 'IoU-45': 35.41834355941618, 'IoU-46': 34.93956827501356, 'IoU-47': 34.01643479483759, 'IoU-48': 33.76380205350501, 'IoU-49': 32.22139454109274, 'IoU-50': 30.751516487431335, 'IoU-51': 29.606946298435627, 'IoU-52': 28.337451976640658, 'IoU-53': 27.97487629247427, 'IoU-54': 27.987960671487933, 'IoU-55': 28.861326752429832, 'IoU-56': 28.339476361543785, 'IoU-57': 28.229019369480113, 'IoU-58': 26.47338719751282, 'IoU-59': 27.247748092900213, 'IoU-60': 26.520397161683533, 'IoU-61': 26.168632321478203, 'IoU-62': 25.399200362854437, 'IoU-63': 25.82290072244261, 'IoU-64': 25.51771880076179, 'IoU-65': 25.39150015284189, 'IoU-66': 24.30038728463711, 'IoU-67': 22.801210845962398, 'IoU-68': 20.70312730553891, 'IoU-69': 19.634031617266505, 'IoU-70': 19.32524916566143, 'IoU-71': 18.550031631142502, 'IoU-72': 18.185581098078167, 'IoU-73': 17.291039640065332, 'IoU-74': 17.300180637239237, 'IoU-75': 18.302273366457598, 'IoU-76': 18.267943304566696, 'IoU-77': 18.812303265125898, 'IoU-78': 17.95454315410194, 'IoU-79': 17.96637252937882, 'IoU-80': 17.48259817700034, 'IoU-81': 18.18298080120025, 'IoU-82': 18.49018665034751, 'IoU-83': 18.060049412433763, 'IoU-84': 18.549887825829405, 'IoU-85': 18.99416033026928, 'IoU-86': 18.992803530809585, 'IoU-87': 18.076249766542485, 'IoU-88': 17.154824075324445, 'IoU-89': 17.29516287053182, 'IoU-90': 18.278372362652586, 'IoU-91': 17.655417213731585, 'IoU-92': 16.937214869469525, 'IoU-93': 15.788591603011648, 'IoU-94': 16.19890217693334, 'IoU-95': 15.86786841038334, 'IoU-96': 16.15188866033637, 'IoU-97': 16.745490563190163, 'IoU-98': 16.070533675499068, 'IoU-99': 17.79455849047222, 'IoU-100': 15.799068436300079, 'IoU-101': 16.88062838980737, 'IoU-102': 15.433009205925549, 'IoU-103': 15.955883161785506, 'IoU-104': 16.744380820047397, 'IoU-105': 13.889037816903443, 'IoU-106': 14.022828353734113, 'IoU-107': 13.92979007617263, 'IoU-108': 14.767833608140545, 'IoU-109': 13.681278692775479, 'IoU-110': 13.539857233157134, 'IoU-111': 13.441633800439746, 'IoU-112': 13.830914731048258, 'IoU-113': 11.538223839082367, 'IoU-114': 13.217710844853839, 'IoU-115': 11.251502101945347, 'IoU-116': 11.923127593639677, 'IoU-117': 9.479871612139688, 'IoU-118': 12.748531579699481, 'IoU-119': 9.976150592673736, 'IoU-120': 12.34218218487851, 'IoU-121': 10.089534665753188, 'IoU-122': 11.83953764646455, 'IoU-123': 10.596495427578153, 'IoU-124': 10.650110774998934, 'IoU-125': 8.472037398517406, 'IoU-126': 7.995791127218088, 'IoU-127': 8.765599251585595, 'IoU-128': 7.9136531792110105, 'IoU-129': 7.595019150083639, 'IoU-130': 6.837430355292998, 'IoU-131': 8.228722942010647, 'IoU-132': 7.915915767645099, 'IoU-133': 5.957580418552059, 'IoU-134': 8.602548806330255, 'IoU-135': 8.507323334874032, 'IoU-136': 6.214631669908585, 'IoU-137': 7.398535347593506, 'IoU-138': 4.518116213675502, 'IoU-139': 4.714228575934686, 'IoU-140': 6.863553210173734, 'IoU-141': 4.8444136768924055, 'IoU-142': 4.841304810844175, 'IoU-143': 5.664168440950205, 'IoU-144': 4.606933096623662, 'IoU-145': 4.640236841618928, 'IoU-146': 4.348035518944542, 'IoU-147': 2.1613579178531945, 'IoU-148': 3.9625224714077354, 'IoU-149': 4.9674224001292515, 'IoU-150': 2.3012972799136873, 'IoU-151': 2.791986234311019, 'IoU-152': 3.7560088928334867, 'IoU-153': 4.518300559012537, 'IoU-154': 3.296679017851173, 'IoU-155': 1.9402468581748284, 'IoU-156': 2.8354500920520156, 'IoU-157': 2.243430951594554, 'IoU-158': 1.99410735840411, 'IoU-159': 3.188351263324122, 'IoU-160': 1.9751241867821572, 'IoU-161': 1.7068469586204258, 'IoU-162': 1.3084523067995353, 'IoU-163': 1.5774272146643435, 'IoU-164': 1.9006391095352828, 'IoU-165': 1.6407138673623984, 'IoU-166': 1.9789782728225274, 'IoU-167': 1.2124983130321796, 'IoU-168': 1.4216897606079033, 'IoU-169': 1.4457329088979838, 'IoU-170': 1.2060004417822014, 'IoU-171': 0.7310535407814303, 'IoU-172': 0.9543916709099728, 'IoU-173': 1.548608860517927, 'IoU-174': 0.9264005029152662, 'IoU-175': 0.4470995793417937, 'IoU-176': 1.08809323680354, 'IoU-177': 1.9915763658948662, 'IoU-178': 1.6126105843718503, 'IoU-179': 2.565361864586525, 'IoU-180': 0.6678478279546187, 'IoU-181': 1.5375395268339753, 'IoU-182': 0.8385246916126903, 'IoU-183': 1.469342080189425, 'IoU-184': 0.6431988352745425, 'IoU-185': 1.7849698038141297, 'IoU-186': 0.2800999988532242, 'IoU-187': 1.271885624069867, 'IoU-188': 0.6167471757959295, 'IoU-189': 0.45907531354280884, 'IoU-190': 0.7124403998502709, 'IoU-191': 0.3415100817810688, 'IoU-192': 0.9631438354647734, 'IoU-193': 1.449689026832679, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 22.008255964723855, 'pACC': 55.245057257302356, 'ACC-1': nan, 'ACC-2': 98.78868837440362, 'ACC-3': 53.97966292733146, 'ACC-4': 75.62408867917117, 'ACC-5': 69.79592000933305, 'ACC-6': 63.99281614410638, 'ACC-7': 56.31289483078865, 'ACC-8': 49.28860788110162, 'ACC-9': 25.886469302117444, 'ACC-10': 40.774121152930235, 'ACC-11': 55.05858875779116, 'ACC-12': 67.61089493700226, 'ACC-13': 73.14389366456253, 'ACC-14': 70.90942467955959, 'ACC-15': 67.9002574940797, 'ACC-16': 72.22994817362371, 'ACC-17': 68.64684530987624, 'ACC-18': 68.87959627118087, 'ACC-19': 66.64253305338461, 'ACC-20': 64.38187855732971, 'ACC-21': 62.41751828287765, 'ACC-22': 65.750362553994, 'ACC-23': 63.04921592783032, 'ACC-24': 59.67906829082086, 'ACC-25': 60.80725381547425, 'ACC-26': 58.47169722496406, 'ACC-27': 61.16308605999971, 'ACC-28': 57.69022149028391, 'ACC-29': 57.08658105383252, 'ACC-30': 57.662989089856055, 'ACC-31': 57.510301265804166, 'ACC-32': 57.59036658967719, 'ACC-33': 58.20251940791007, 'ACC-34': 52.06554900408804, 'ACC-35': 55.43916912476836, 'ACC-36': 59.10721066808823, 'ACC-37': 57.848134745614644, 'ACC-38': 57.804597337410314, 'ACC-39': 54.926069361321474, 'ACC-40': 55.09466543514683, 'ACC-41': 52.89844250977013, 'ACC-42': 54.11613730912757, 'ACC-43': 52.25515410845163, 'ACC-44': 51.656994673665494, 'ACC-45': 51.248377380439536, 'ACC-46': 50.8119778707702, 'ACC-47': 50.02864603159253, 'ACC-48': 50.167242188935425, 'ACC-49': 51.376705918202994, 'ACC-50': 48.18980289935008, 'ACC-51': 45.01914422295252, 'ACC-52': 43.926375315691224, 'ACC-53': 43.14820002215872, 'ACC-54': 41.73078518968368, 'ACC-55': 45.48379015937376, 'ACC-56': 43.16422058293921, 'ACC-57': 42.65841745989141, 'ACC-58': 41.26147707642806, 'ACC-59': 42.31660472888262, 'ACC-60': 41.16267652392768, 'ACC-61': 40.23167965818216, 'ACC-62': 39.609448453993764, 'ACC-63': 40.18897697408314, 'ACC-64': 39.99361043108193, 'ACC-65': 39.93631104432257, 'ACC-66': 38.59346244374686, 'ACC-67': 37.17767681603381, 'ACC-68': 33.77262580720111, 'ACC-69': 31.76013758374746, 'ACC-70': 32.25629085951239, 'ACC-71': 29.435339877551144, 'ACC-72': 31.036993627567455, 'ACC-73': 27.556434735893987, 'ACC-74': 27.996626843203455, 'ACC-75': 31.04163301212696, 'ACC-76': 29.58143095357957, 'ACC-77': 31.490898977742138, 'ACC-78': 30.40059916962478, 'ACC-79': 30.110755355249697, 'ACC-80': 29.463497589086728, 'ACC-81': 30.327976803865788, 'ACC-82': 29.386814874401257, 'ACC-83': 28.74135151999701, 'ACC-84': 31.805919569544127, 'ACC-85': 30.411126761410014, 'ACC-86': 30.99152283067927, 'ACC-87': 30.563487089607595, 'ACC-88': 28.84647382912021, 'ACC-89': 32.200042581826104, 'ACC-90': 29.5161319048303, 'ACC-91': 29.63035478687151, 'ACC-92': 30.16083437817484, 'ACC-93': 29.94023803256897, 'ACC-94': 26.615419677718226, 'ACC-95': 29.96880253301902, 'ACC-96': 26.935615373234405, 'ACC-97': 26.422841197701395, 'ACC-98': 23.816875994828816, 'ACC-99': 33.85402835319692, 'ACC-100': 23.4627512810406, 'ACC-101': 29.203229623070083, 'ACC-102': 25.51726272458565, 'ACC-103': 26.61764839148165, 'ACC-104': 30.186337017096438, 'ACC-105': 23.76424583753474, 'ACC-106': 23.62145199998672, 'ACC-107': 24.339609134512134, 'ACC-108': 26.29012027379258, 'ACC-109': 27.3837499001638, 'ACC-110': 23.622255957974893, 'ACC-111': 22.82873891692955, 'ACC-112': 26.186686372823054, 'ACC-113': 19.310842093532436, 'ACC-114': 24.248111370615234, 'ACC-115': 19.528155460811362, 'ACC-116': 23.437636251559844, 'ACC-117': 16.26984189369131, 'ACC-118': 25.05740050645851, 'ACC-119': 17.053680631919175, 'ACC-120': 21.853266750645602, 'ACC-121': 17.329843943818368, 'ACC-122': 24.96476469447368, 'ACC-123': 17.959938483021503, 'ACC-124': 19.734284118428402, 'ACC-125': 17.65349866629446, 'ACC-126': 14.26210321711881, 'ACC-127': 16.83781273053489, 'ACC-128': 13.18744217776075, 'ACC-129': 12.160945784256688, 'ACC-130': 11.033981170449291, 'ACC-131': 13.618071266603552, 'ACC-132': 14.916798087077071, 'ACC-133': 10.046211123510036, 'ACC-134': 19.216830989269702, 'ACC-135': 17.14171477811288, 'ACC-136': 12.945670144233508, 'ACC-137': 14.832466571934402, 'ACC-138': 8.37090344576509, 'ACC-139': 10.527134045739524, 'ACC-140': 17.332771470945346, 'ACC-141': 8.972116345059607, 'ACC-142': 8.627105778277473, 'ACC-143': 11.472401906972642, 'ACC-144': 8.59978811217654, 'ACC-145': 9.291064981949457, 'ACC-146': 7.508844115476643, 'ACC-147': 3.4555299170683784, 'ACC-148': 7.159165929267222, 'ACC-149': 10.1701343375939, 'ACC-150': 3.599443852306482, 'ACC-151': 4.449614293799621, 'ACC-152': 7.324762358478198, 'ACC-153': 8.882721356800863, 'ACC-154': 6.492996390553143, 'ACC-155': 3.3706836122569035, 'ACC-156': 4.7912922795522395, 'ACC-157': 3.793364230448525, 'ACC-158': 3.5953196603391984, 'ACC-159': 7.2947466025734204, 'ACC-160': 3.422156189482459, 'ACC-161': 3.3415995566212735, 'ACC-162': 1.855357127812243, 'ACC-163': 4.394661128648829, 'ACC-164': 5.62302898189606, 'ACC-165': 2.6870962934306615, 'ACC-166': 3.6354886969669056, 'ACC-167': 2.5942869614122266, 'ACC-168': 2.8632872824045847, 'ACC-169': 4.024544623391237, 'ACC-170': 3.257278120629662, 'ACC-171': 0.9229879182815849, 'ACC-172': 1.7420725961547292, 'ACC-173': 2.539911842056525, 'ACC-174': 1.2310303176257653, 'ACC-175': 0.5893964557924698, 'ACC-176': 1.7447525232075938, 'ACC-177': 3.6178386928256328, 'ACC-178': 3.132860525329923, 'ACC-179': 5.256600690346545, 'ACC-180': 0.8359015665038824, 'ACC-181': 3.9694029598022285, 'ACC-182': 3.081142922111606, 'ACC-183': 6.01010687319499, 'ACC-184': 0.8736883491739866, 'ACC-185': 3.5340564175492966, 'ACC-186': 0.4512931372956594, 'ACC-187': 9.349099295819533, 'ACC-188': 1.16696263545102, 'ACC-189': 1.9947237803155642, 'ACC-190': 1.7883503071721423, 'ACC-191': 0.8782502069861765, 'ACC-192': 6.244176182707994, 'ACC-193': 5.1311511131360525, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 14:53:17] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 14:53:17] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 14:53:17] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 14:53:17] d2.evaluation.testing INFO: copypaste: 14.4545,40.9766,22.0083,55.2451
[01/17 14:53:17] d2.utils.events INFO:  eta: 1 day, 4:13:55  iter: 23999  total_loss: 34.21  loss_ce: 0.2623  loss_mask: 0.3633  loss_dice: 2.728  loss_ce_0: 0.5619  loss_mask_0: 0.3671  loss_dice_0: 2.863  loss_ce_1: 0.313  loss_mask_1: 0.3725  loss_dice_1: 2.774  loss_ce_2: 0.2984  loss_mask_2: 0.364  loss_dice_2: 2.754  loss_ce_3: 0.2841  loss_mask_3: 0.3611  loss_dice_3: 2.749  loss_ce_4: 0.2767  loss_mask_4: 0.3608  loss_dice_4: 2.752  loss_ce_5: 0.2709  loss_mask_5: 0.3636  loss_dice_5: 2.735  loss_ce_6: 0.2643  loss_mask_6: 0.3637  loss_dice_6: 2.738  loss_ce_7: 0.2702  loss_mask_7: 0.3618  loss_dice_7: 2.734  loss_ce_8: 0.2722  loss_mask_8: 0.3619  loss_dice_8: 2.732  time: 1.5366  data_time: 0.1077  lr: 7.5644e-06  max_mem: 21478M
[01/17 14:53:47] d2.utils.events INFO:  eta: 1 day, 4:11:35  iter: 24019  total_loss: 34.07  loss_ce: 0.2497  loss_mask: 0.3696  loss_dice: 2.709  loss_ce_0: 0.557  loss_mask_0: 0.3697  loss_dice_0: 2.818  loss_ce_1: 0.2927  loss_mask_1: 0.3739  loss_dice_1: 2.742  loss_ce_2: 0.2832  loss_mask_2: 0.372  loss_dice_2: 2.73  loss_ce_3: 0.2785  loss_mask_3: 0.3686  loss_dice_3: 2.71  loss_ce_4: 0.2772  loss_mask_4: 0.3668  loss_dice_4: 2.709  loss_ce_5: 0.2619  loss_mask_5: 0.3665  loss_dice_5: 2.711  loss_ce_6: 0.2636  loss_mask_6: 0.368  loss_dice_6: 2.717  loss_ce_7: 0.2614  loss_mask_7: 0.368  loss_dice_7: 2.704  loss_ce_8: 0.2556  loss_mask_8: 0.3675  loss_dice_8: 2.704  time: 1.5366  data_time: 0.0834  lr: 7.5624e-06  max_mem: 21478M
[01/17 14:54:17] d2.utils.events INFO:  eta: 1 day, 4:08:48  iter: 24039  total_loss: 34.64  loss_ce: 0.2493  loss_mask: 0.3573  loss_dice: 2.808  loss_ce_0: 0.5735  loss_mask_0: 0.3607  loss_dice_0: 2.919  loss_ce_1: 0.2762  loss_mask_1: 0.3614  loss_dice_1: 2.837  loss_ce_2: 0.2793  loss_mask_2: 0.3557  loss_dice_2: 2.831  loss_ce_3: 0.2736  loss_mask_3: 0.3581  loss_dice_3: 2.814  loss_ce_4: 0.2581  loss_mask_4: 0.3566  loss_dice_4: 2.815  loss_ce_5: 0.2458  loss_mask_5: 0.355  loss_dice_5: 2.82  loss_ce_6: 0.2478  loss_mask_6: 0.3564  loss_dice_6: 2.807  loss_ce_7: 0.2603  loss_mask_7: 0.3541  loss_dice_7: 2.806  loss_ce_8: 0.2465  loss_mask_8: 0.3548  loss_dice_8: 2.805  time: 1.5366  data_time: 0.0921  lr: 7.5603e-06  max_mem: 21478M
[01/17 14:54:47] d2.utils.events INFO:  eta: 1 day, 4:06:46  iter: 24059  total_loss: 33.81  loss_ce: 0.2524  loss_mask: 0.3606  loss_dice: 2.716  loss_ce_0: 0.5483  loss_mask_0: 0.356  loss_dice_0: 2.825  loss_ce_1: 0.3077  loss_mask_1: 0.3597  loss_dice_1: 2.749  loss_ce_2: 0.2866  loss_mask_2: 0.3601  loss_dice_2: 2.732  loss_ce_3: 0.2738  loss_mask_3: 0.3598  loss_dice_3: 2.724  loss_ce_4: 0.2822  loss_mask_4: 0.3605  loss_dice_4: 2.723  loss_ce_5: 0.2593  loss_mask_5: 0.361  loss_dice_5: 2.721  loss_ce_6: 0.2509  loss_mask_6: 0.3597  loss_dice_6: 2.719  loss_ce_7: 0.2504  loss_mask_7: 0.3598  loss_dice_7: 2.714  loss_ce_8: 0.254  loss_mask_8: 0.3605  loss_dice_8: 2.713  time: 1.5365  data_time: 0.0917  lr: 7.5583e-06  max_mem: 21478M
[01/17 14:55:17] d2.utils.events INFO:  eta: 1 day, 4:05:59  iter: 24079  total_loss: 33.82  loss_ce: 0.2347  loss_mask: 0.3502  loss_dice: 2.762  loss_ce_0: 0.541  loss_mask_0: 0.3535  loss_dice_0: 2.861  loss_ce_1: 0.2836  loss_mask_1: 0.3561  loss_dice_1: 2.796  loss_ce_2: 0.2683  loss_mask_2: 0.3525  loss_dice_2: 2.773  loss_ce_3: 0.2522  loss_mask_3: 0.3504  loss_dice_3: 2.769  loss_ce_4: 0.2487  loss_mask_4: 0.3517  loss_dice_4: 2.756  loss_ce_5: 0.2448  loss_mask_5: 0.3493  loss_dice_5: 2.754  loss_ce_6: 0.2442  loss_mask_6: 0.3491  loss_dice_6: 2.763  loss_ce_7: 0.2376  loss_mask_7: 0.3494  loss_dice_7: 2.753  loss_ce_8: 0.2501  loss_mask_8: 0.3505  loss_dice_8: 2.755  time: 1.5365  data_time: 0.0883  lr: 7.5562e-06  max_mem: 21478M
[01/17 14:55:47] d2.utils.events INFO:  eta: 1 day, 4:04:11  iter: 24099  total_loss: 34.43  loss_ce: 0.2907  loss_mask: 0.3606  loss_dice: 2.735  loss_ce_0: 0.5667  loss_mask_0: 0.36  loss_dice_0: 2.821  loss_ce_1: 0.3372  loss_mask_1: 0.3599  loss_dice_1: 2.765  loss_ce_2: 0.3236  loss_mask_2: 0.3612  loss_dice_2: 2.748  loss_ce_3: 0.2974  loss_mask_3: 0.3579  loss_dice_3: 2.74  loss_ce_4: 0.2851  loss_mask_4: 0.3588  loss_dice_4: 2.74  loss_ce_5: 0.3007  loss_mask_5: 0.3581  loss_dice_5: 2.75  loss_ce_6: 0.277  loss_mask_6: 0.3573  loss_dice_6: 2.729  loss_ce_7: 0.2902  loss_mask_7: 0.3592  loss_dice_7: 2.737  loss_ce_8: 0.2856  loss_mask_8: 0.3591  loss_dice_8: 2.739  time: 1.5364  data_time: 0.0921  lr: 7.5541e-06  max_mem: 21478M
[01/17 14:56:16] d2.utils.events INFO:  eta: 1 day, 4:02:54  iter: 24119  total_loss: 34.13  loss_ce: 0.2535  loss_mask: 0.3588  loss_dice: 2.737  loss_ce_0: 0.5769  loss_mask_0: 0.355  loss_dice_0: 2.844  loss_ce_1: 0.2897  loss_mask_1: 0.3631  loss_dice_1: 2.779  loss_ce_2: 0.2916  loss_mask_2: 0.3624  loss_dice_2: 2.756  loss_ce_3: 0.2514  loss_mask_3: 0.3594  loss_dice_3: 2.747  loss_ce_4: 0.2563  loss_mask_4: 0.3577  loss_dice_4: 2.747  loss_ce_5: 0.2591  loss_mask_5: 0.3572  loss_dice_5: 2.741  loss_ce_6: 0.2542  loss_mask_6: 0.3588  loss_dice_6: 2.741  loss_ce_7: 0.254  loss_mask_7: 0.3578  loss_dice_7: 2.737  loss_ce_8: 0.2481  loss_mask_8: 0.3587  loss_dice_8: 2.73  time: 1.5364  data_time: 0.0796  lr: 7.5521e-06  max_mem: 21478M
[01/17 14:56:46] d2.utils.events INFO:  eta: 1 day, 4:00:12  iter: 24139  total_loss: 34.19  loss_ce: 0.2362  loss_mask: 0.3567  loss_dice: 2.732  loss_ce_0: 0.5585  loss_mask_0: 0.3513  loss_dice_0: 2.841  loss_ce_1: 0.2825  loss_mask_1: 0.3589  loss_dice_1: 2.774  loss_ce_2: 0.2778  loss_mask_2: 0.3557  loss_dice_2: 2.761  loss_ce_3: 0.2634  loss_mask_3: 0.3532  loss_dice_3: 2.745  loss_ce_4: 0.2597  loss_mask_4: 0.3545  loss_dice_4: 2.729  loss_ce_5: 0.2553  loss_mask_5: 0.353  loss_dice_5: 2.742  loss_ce_6: 0.2572  loss_mask_6: 0.352  loss_dice_6: 2.734  loss_ce_7: 0.2517  loss_mask_7: 0.3523  loss_dice_7: 2.734  loss_ce_8: 0.2419  loss_mask_8: 0.3542  loss_dice_8: 2.715  time: 1.5364  data_time: 0.0908  lr: 7.55e-06  max_mem: 21478M
[01/17 14:57:16] d2.utils.events INFO:  eta: 1 day, 3:58:54  iter: 24159  total_loss: 34.34  loss_ce: 0.266  loss_mask: 0.3454  loss_dice: 2.742  loss_ce_0: 0.6094  loss_mask_0: 0.3572  loss_dice_0: 2.842  loss_ce_1: 0.3286  loss_mask_1: 0.3529  loss_dice_1: 2.774  loss_ce_2: 0.3194  loss_mask_2: 0.3483  loss_dice_2: 2.747  loss_ce_3: 0.2836  loss_mask_3: 0.3498  loss_dice_3: 2.743  loss_ce_4: 0.2811  loss_mask_4: 0.351  loss_dice_4: 2.736  loss_ce_5: 0.268  loss_mask_5: 0.3503  loss_dice_5: 2.734  loss_ce_6: 0.2643  loss_mask_6: 0.3503  loss_dice_6: 2.735  loss_ce_7: 0.2671  loss_mask_7: 0.3495  loss_dice_7: 2.741  loss_ce_8: 0.2697  loss_mask_8: 0.3461  loss_dice_8: 2.739  time: 1.5363  data_time: 0.0924  lr: 7.5479e-06  max_mem: 21478M
[01/17 14:57:47] d2.utils.events INFO:  eta: 1 day, 3:56:36  iter: 24179  total_loss: 33.8  loss_ce: 0.2706  loss_mask: 0.3529  loss_dice: 2.704  loss_ce_0: 0.5624  loss_mask_0: 0.3468  loss_dice_0: 2.805  loss_ce_1: 0.3072  loss_mask_1: 0.3549  loss_dice_1: 2.741  loss_ce_2: 0.2978  loss_mask_2: 0.3509  loss_dice_2: 2.717  loss_ce_3: 0.2794  loss_mask_3: 0.3482  loss_dice_3: 2.728  loss_ce_4: 0.268  loss_mask_4: 0.3456  loss_dice_4: 2.713  loss_ce_5: 0.2713  loss_mask_5: 0.3485  loss_dice_5: 2.715  loss_ce_6: 0.2658  loss_mask_6: 0.3477  loss_dice_6: 2.71  loss_ce_7: 0.2729  loss_mask_7: 0.3485  loss_dice_7: 2.704  loss_ce_8: 0.2687  loss_mask_8: 0.3489  loss_dice_8: 2.701  time: 1.5363  data_time: 0.0778  lr: 7.5459e-06  max_mem: 21478M
[01/17 14:58:17] d2.utils.events INFO:  eta: 1 day, 3:54:27  iter: 24199  total_loss: 34.16  loss_ce: 0.2667  loss_mask: 0.3559  loss_dice: 2.739  loss_ce_0: 0.5821  loss_mask_0: 0.366  loss_dice_0: 2.837  loss_ce_1: 0.3045  loss_mask_1: 0.3642  loss_dice_1: 2.773  loss_ce_2: 0.295  loss_mask_2: 0.3609  loss_dice_2: 2.745  loss_ce_3: 0.2815  loss_mask_3: 0.359  loss_dice_3: 2.735  loss_ce_4: 0.2814  loss_mask_4: 0.359  loss_dice_4: 2.744  loss_ce_5: 0.2728  loss_mask_5: 0.3577  loss_dice_5: 2.743  loss_ce_6: 0.2726  loss_mask_6: 0.3583  loss_dice_6: 2.735  loss_ce_7: 0.2685  loss_mask_7: 0.3568  loss_dice_7: 2.741  loss_ce_8: 0.2769  loss_mask_8: 0.3547  loss_dice_8: 2.747  time: 1.5363  data_time: 0.0877  lr: 7.5438e-06  max_mem: 21478M
[01/17 14:58:46] d2.utils.events INFO:  eta: 1 day, 3:53:15  iter: 24219  total_loss: 33.77  loss_ce: 0.2275  loss_mask: 0.3574  loss_dice: 2.728  loss_ce_0: 0.5638  loss_mask_0: 0.3559  loss_dice_0: 2.836  loss_ce_1: 0.277  loss_mask_1: 0.3592  loss_dice_1: 2.773  loss_ce_2: 0.2633  loss_mask_2: 0.358  loss_dice_2: 2.76  loss_ce_3: 0.2429  loss_mask_3: 0.3547  loss_dice_3: 2.74  loss_ce_4: 0.2448  loss_mask_4: 0.3529  loss_dice_4: 2.739  loss_ce_5: 0.2318  loss_mask_5: 0.3535  loss_dice_5: 2.746  loss_ce_6: 0.2326  loss_mask_6: 0.3552  loss_dice_6: 2.735  loss_ce_7: 0.2347  loss_mask_7: 0.3557  loss_dice_7: 2.73  loss_ce_8: 0.2337  loss_mask_8: 0.3559  loss_dice_8: 2.732  time: 1.5362  data_time: 0.0878  lr: 7.5418e-06  max_mem: 21478M
[01/17 14:59:16] d2.utils.events INFO:  eta: 1 day, 3:52:39  iter: 24239  total_loss: 34.62  loss_ce: 0.2719  loss_mask: 0.3552  loss_dice: 2.8  loss_ce_0: 0.58  loss_mask_0: 0.3554  loss_dice_0: 2.891  loss_ce_1: 0.2923  loss_mask_1: 0.3581  loss_dice_1: 2.832  loss_ce_2: 0.2821  loss_mask_2: 0.3542  loss_dice_2: 2.807  loss_ce_3: 0.2731  loss_mask_3: 0.3546  loss_dice_3: 2.801  loss_ce_4: 0.2581  loss_mask_4: 0.3566  loss_dice_4: 2.799  loss_ce_5: 0.2617  loss_mask_5: 0.3563  loss_dice_5: 2.79  loss_ce_6: 0.2511  loss_mask_6: 0.3567  loss_dice_6: 2.799  loss_ce_7: 0.2558  loss_mask_7: 0.3554  loss_dice_7: 2.794  loss_ce_8: 0.2552  loss_mask_8: 0.3554  loss_dice_8: 2.796  time: 1.5362  data_time: 0.0835  lr: 7.5397e-06  max_mem: 21478M
[01/17 14:59:46] d2.utils.events INFO:  eta: 1 day, 3:51:41  iter: 24259  total_loss: 34.49  loss_ce: 0.2578  loss_mask: 0.354  loss_dice: 2.766  loss_ce_0: 0.5857  loss_mask_0: 0.3613  loss_dice_0: 2.87  loss_ce_1: 0.2777  loss_mask_1: 0.3631  loss_dice_1: 2.802  loss_ce_2: 0.2727  loss_mask_2: 0.357  loss_dice_2: 2.788  loss_ce_3: 0.2649  loss_mask_3: 0.3542  loss_dice_3: 2.77  loss_ce_4: 0.2488  loss_mask_4: 0.3522  loss_dice_4: 2.776  loss_ce_5: 0.2592  loss_mask_5: 0.3527  loss_dice_5: 2.773  loss_ce_6: 0.2545  loss_mask_6: 0.3513  loss_dice_6: 2.77  loss_ce_7: 0.2368  loss_mask_7: 0.3523  loss_dice_7: 2.769  loss_ce_8: 0.2275  loss_mask_8: 0.3534  loss_dice_8: 2.772  time: 1.5362  data_time: 0.0888  lr: 7.5376e-06  max_mem: 21478M
[01/17 15:00:16] d2.utils.events INFO:  eta: 1 day, 3:51:03  iter: 24279  total_loss: 34.5  loss_ce: 0.2582  loss_mask: 0.3637  loss_dice: 2.778  loss_ce_0: 0.561  loss_mask_0: 0.3661  loss_dice_0: 2.879  loss_ce_1: 0.2795  loss_mask_1: 0.3662  loss_dice_1: 2.802  loss_ce_2: 0.2937  loss_mask_2: 0.365  loss_dice_2: 2.791  loss_ce_3: 0.272  loss_mask_3: 0.3644  loss_dice_3: 2.772  loss_ce_4: 0.2655  loss_mask_4: 0.3637  loss_dice_4: 2.779  loss_ce_5: 0.2597  loss_mask_5: 0.3657  loss_dice_5: 2.782  loss_ce_6: 0.2597  loss_mask_6: 0.3654  loss_dice_6: 2.772  loss_ce_7: 0.2634  loss_mask_7: 0.3664  loss_dice_7: 2.774  loss_ce_8: 0.2569  loss_mask_8: 0.3644  loss_dice_8: 2.78  time: 1.5362  data_time: 0.0885  lr: 7.5356e-06  max_mem: 21478M
[01/17 15:00:47] d2.utils.events INFO:  eta: 1 day, 3:50:22  iter: 24299  total_loss: 34.68  loss_ce: 0.2624  loss_mask: 0.3633  loss_dice: 2.815  loss_ce_0: 0.5435  loss_mask_0: 0.3749  loss_dice_0: 2.915  loss_ce_1: 0.2968  loss_mask_1: 0.371  loss_dice_1: 2.849  loss_ce_2: 0.2903  loss_mask_2: 0.3655  loss_dice_2: 2.827  loss_ce_3: 0.2816  loss_mask_3: 0.363  loss_dice_3: 2.812  loss_ce_4: 0.2747  loss_mask_4: 0.3619  loss_dice_4: 2.812  loss_ce_5: 0.2562  loss_mask_5: 0.3645  loss_dice_5: 2.811  loss_ce_6: 0.2594  loss_mask_6: 0.3614  loss_dice_6: 2.812  loss_ce_7: 0.2716  loss_mask_7: 0.3613  loss_dice_7: 2.82  loss_ce_8: 0.26  loss_mask_8: 0.3633  loss_dice_8: 2.817  time: 1.5361  data_time: 0.0933  lr: 7.5335e-06  max_mem: 21478M
[01/17 15:01:17] d2.utils.events INFO:  eta: 1 day, 3:48:49  iter: 24319  total_loss: 34.55  loss_ce: 0.2411  loss_mask: 0.3631  loss_dice: 2.803  loss_ce_0: 0.5756  loss_mask_0: 0.3651  loss_dice_0: 2.892  loss_ce_1: 0.2736  loss_mask_1: 0.3693  loss_dice_1: 2.837  loss_ce_2: 0.2696  loss_mask_2: 0.3677  loss_dice_2: 2.817  loss_ce_3: 0.2597  loss_mask_3: 0.366  loss_dice_3: 2.808  loss_ce_4: 0.2488  loss_mask_4: 0.3671  loss_dice_4: 2.799  loss_ce_5: 0.237  loss_mask_5: 0.3647  loss_dice_5: 2.807  loss_ce_6: 0.2417  loss_mask_6: 0.3648  loss_dice_6: 2.794  loss_ce_7: 0.2389  loss_mask_7: 0.3658  loss_dice_7: 2.795  loss_ce_8: 0.2478  loss_mask_8: 0.3634  loss_dice_8: 2.792  time: 1.5361  data_time: 0.0929  lr: 7.5314e-06  max_mem: 21478M
[01/17 15:01:47] d2.utils.events INFO:  eta: 1 day, 3:46:54  iter: 24339  total_loss: 33.19  loss_ce: 0.2428  loss_mask: 0.3478  loss_dice: 2.669  loss_ce_0: 0.5531  loss_mask_0: 0.3464  loss_dice_0: 2.795  loss_ce_1: 0.2787  loss_mask_1: 0.35  loss_dice_1: 2.72  loss_ce_2: 0.2631  loss_mask_2: 0.3491  loss_dice_2: 2.684  loss_ce_3: 0.2608  loss_mask_3: 0.35  loss_dice_3: 2.674  loss_ce_4: 0.2614  loss_mask_4: 0.3493  loss_dice_4: 2.679  loss_ce_5: 0.2504  loss_mask_5: 0.3466  loss_dice_5: 2.678  loss_ce_6: 0.2547  loss_mask_6: 0.3478  loss_dice_6: 2.667  loss_ce_7: 0.2405  loss_mask_7: 0.3467  loss_dice_7: 2.67  loss_ce_8: 0.2334  loss_mask_8: 0.347  loss_dice_8: 2.67  time: 1.5361  data_time: 0.0881  lr: 7.5294e-06  max_mem: 21478M
[01/17 15:02:16] d2.utils.events INFO:  eta: 1 day, 3:45:41  iter: 24359  total_loss: 33.64  loss_ce: 0.2536  loss_mask: 0.3655  loss_dice: 2.673  loss_ce_0: 0.5564  loss_mask_0: 0.3588  loss_dice_0: 2.796  loss_ce_1: 0.2924  loss_mask_1: 0.368  loss_dice_1: 2.714  loss_ce_2: 0.2891  loss_mask_2: 0.3639  loss_dice_2: 2.687  loss_ce_3: 0.2703  loss_mask_3: 0.3655  loss_dice_3: 2.676  loss_ce_4: 0.2567  loss_mask_4: 0.3643  loss_dice_4: 2.678  loss_ce_5: 0.2572  loss_mask_5: 0.3641  loss_dice_5: 2.669  loss_ce_6: 0.2568  loss_mask_6: 0.3651  loss_dice_6: 2.679  loss_ce_7: 0.2665  loss_mask_7: 0.3631  loss_dice_7: 2.664  loss_ce_8: 0.2618  loss_mask_8: 0.3653  loss_dice_8: 2.672  time: 1.5360  data_time: 0.0881  lr: 7.5273e-06  max_mem: 21478M
[01/17 15:02:46] d2.utils.events INFO:  eta: 1 day, 3:44:49  iter: 24379  total_loss: 34.61  loss_ce: 0.2419  loss_mask: 0.3617  loss_dice: 2.73  loss_ce_0: 0.5644  loss_mask_0: 0.3669  loss_dice_0: 2.831  loss_ce_1: 0.2797  loss_mask_1: 0.3682  loss_dice_1: 2.758  loss_ce_2: 0.2776  loss_mask_2: 0.3632  loss_dice_2: 2.738  loss_ce_3: 0.2728  loss_mask_3: 0.3665  loss_dice_3: 2.735  loss_ce_4: 0.268  loss_mask_4: 0.3635  loss_dice_4: 2.726  loss_ce_5: 0.2546  loss_mask_5: 0.3641  loss_dice_5: 2.73  loss_ce_6: 0.2479  loss_mask_6: 0.3642  loss_dice_6: 2.731  loss_ce_7: 0.2325  loss_mask_7: 0.3647  loss_dice_7: 2.722  loss_ce_8: 0.2451  loss_mask_8: 0.363  loss_dice_8: 2.735  time: 1.5360  data_time: 0.0854  lr: 7.5252e-06  max_mem: 21478M
[01/17 15:03:16] d2.utils.events INFO:  eta: 1 day, 3:43:13  iter: 24399  total_loss: 34.75  loss_ce: 0.2546  loss_mask: 0.3773  loss_dice: 2.773  loss_ce_0: 0.5804  loss_mask_0: 0.377  loss_dice_0: 2.88  loss_ce_1: 0.2889  loss_mask_1: 0.3834  loss_dice_1: 2.803  loss_ce_2: 0.2905  loss_mask_2: 0.3753  loss_dice_2: 2.794  loss_ce_3: 0.2576  loss_mask_3: 0.3736  loss_dice_3: 2.774  loss_ce_4: 0.2614  loss_mask_4: 0.3772  loss_dice_4: 2.776  loss_ce_5: 0.2469  loss_mask_5: 0.375  loss_dice_5: 2.777  loss_ce_6: 0.2548  loss_mask_6: 0.3752  loss_dice_6: 2.775  loss_ce_7: 0.2516  loss_mask_7: 0.3742  loss_dice_7: 2.777  loss_ce_8: 0.2443  loss_mask_8: 0.3748  loss_dice_8: 2.776  time: 1.5359  data_time: 0.0898  lr: 7.5232e-06  max_mem: 21478M
[01/17 15:03:46] d2.utils.events INFO:  eta: 1 day, 3:42:10  iter: 24419  total_loss: 34.06  loss_ce: 0.2229  loss_mask: 0.366  loss_dice: 2.735  loss_ce_0: 0.5513  loss_mask_0: 0.3676  loss_dice_0: 2.867  loss_ce_1: 0.2544  loss_mask_1: 0.3672  loss_dice_1: 2.784  loss_ce_2: 0.257  loss_mask_2: 0.3647  loss_dice_2: 2.768  loss_ce_3: 0.2499  loss_mask_3: 0.3642  loss_dice_3: 2.737  loss_ce_4: 0.2394  loss_mask_4: 0.3672  loss_dice_4: 2.749  loss_ce_5: 0.2285  loss_mask_5: 0.3663  loss_dice_5: 2.739  loss_ce_6: 0.2315  loss_mask_6: 0.3641  loss_dice_6: 2.725  loss_ce_7: 0.232  loss_mask_7: 0.3661  loss_dice_7: 2.737  loss_ce_8: 0.2235  loss_mask_8: 0.3656  loss_dice_8: 2.738  time: 1.5359  data_time: 0.0992  lr: 7.5211e-06  max_mem: 21478M
[01/17 15:04:17] d2.utils.events INFO:  eta: 1 day, 3:42:21  iter: 24439  total_loss: 34.35  loss_ce: 0.2637  loss_mask: 0.3623  loss_dice: 2.731  loss_ce_0: 0.5901  loss_mask_0: 0.3671  loss_dice_0: 2.84  loss_ce_1: 0.3075  loss_mask_1: 0.3728  loss_dice_1: 2.756  loss_ce_2: 0.2922  loss_mask_2: 0.3696  loss_dice_2: 2.736  loss_ce_3: 0.2832  loss_mask_3: 0.3648  loss_dice_3: 2.728  loss_ce_4: 0.2502  loss_mask_4: 0.3639  loss_dice_4: 2.728  loss_ce_5: 0.2672  loss_mask_5: 0.3642  loss_dice_5: 2.725  loss_ce_6: 0.2602  loss_mask_6: 0.3628  loss_dice_6: 2.722  loss_ce_7: 0.2594  loss_mask_7: 0.3626  loss_dice_7: 2.726  loss_ce_8: 0.2521  loss_mask_8: 0.3625  loss_dice_8: 2.734  time: 1.5359  data_time: 0.0965  lr: 7.519e-06  max_mem: 21478M
[01/17 15:04:48] d2.utils.events INFO:  eta: 1 day, 3:41:41  iter: 24459  total_loss: 33.88  loss_ce: 0.2311  loss_mask: 0.3474  loss_dice: 2.779  loss_ce_0: 0.5673  loss_mask_0: 0.3532  loss_dice_0: 2.885  loss_ce_1: 0.2705  loss_mask_1: 0.3519  loss_dice_1: 2.814  loss_ce_2: 0.2632  loss_mask_2: 0.3507  loss_dice_2: 2.801  loss_ce_3: 0.262  loss_mask_3: 0.3488  loss_dice_3: 2.784  loss_ce_4: 0.2433  loss_mask_4: 0.3488  loss_dice_4: 2.79  loss_ce_5: 0.2444  loss_mask_5: 0.3483  loss_dice_5: 2.789  loss_ce_6: 0.2385  loss_mask_6: 0.3478  loss_dice_6: 2.783  loss_ce_7: 0.244  loss_mask_7: 0.3476  loss_dice_7: 2.774  loss_ce_8: 0.224  loss_mask_8: 0.3472  loss_dice_8: 2.775  time: 1.5359  data_time: 0.0919  lr: 7.517e-06  max_mem: 21478M
[01/17 15:05:18] d2.utils.events INFO:  eta: 1 day, 3:39:45  iter: 24479  total_loss: 33.29  loss_ce: 0.2394  loss_mask: 0.3656  loss_dice: 2.684  loss_ce_0: 0.546  loss_mask_0: 0.3579  loss_dice_0: 2.801  loss_ce_1: 0.2896  loss_mask_1: 0.3685  loss_dice_1: 2.737  loss_ce_2: 0.2648  loss_mask_2: 0.3655  loss_dice_2: 2.712  loss_ce_3: 0.2535  loss_mask_3: 0.363  loss_dice_3: 2.701  loss_ce_4: 0.2581  loss_mask_4: 0.3613  loss_dice_4: 2.696  loss_ce_5: 0.2341  loss_mask_5: 0.3642  loss_dice_5: 2.707  loss_ce_6: 0.244  loss_mask_6: 0.3648  loss_dice_6: 2.692  loss_ce_7: 0.2357  loss_mask_7: 0.3641  loss_dice_7: 2.697  loss_ce_8: 0.2374  loss_mask_8: 0.3654  loss_dice_8: 2.693  time: 1.5359  data_time: 0.0894  lr: 7.5149e-06  max_mem: 21478M
[01/17 15:05:48] d2.utils.events INFO:  eta: 1 day, 3:37:46  iter: 24499  total_loss: 34.76  loss_ce: 0.2457  loss_mask: 0.3547  loss_dice: 2.808  loss_ce_0: 0.5772  loss_mask_0: 0.3634  loss_dice_0: 2.927  loss_ce_1: 0.2954  loss_mask_1: 0.3599  loss_dice_1: 2.848  loss_ce_2: 0.2913  loss_mask_2: 0.3566  loss_dice_2: 2.825  loss_ce_3: 0.2862  loss_mask_3: 0.3554  loss_dice_3: 2.824  loss_ce_4: 0.2732  loss_mask_4: 0.3561  loss_dice_4: 2.809  loss_ce_5: 0.254  loss_mask_5: 0.3583  loss_dice_5: 2.817  loss_ce_6: 0.2671  loss_mask_6: 0.3575  loss_dice_6: 2.809  loss_ce_7: 0.2573  loss_mask_7: 0.3572  loss_dice_7: 2.808  loss_ce_8: 0.2429  loss_mask_8: 0.3562  loss_dice_8: 2.804  time: 1.5359  data_time: 0.0924  lr: 7.5129e-06  max_mem: 21478M
[01/17 15:06:18] d2.utils.events INFO:  eta: 1 day, 3:36:23  iter: 24519  total_loss: 33.92  loss_ce: 0.2688  loss_mask: 0.3542  loss_dice: 2.732  loss_ce_0: 0.5531  loss_mask_0: 0.3694  loss_dice_0: 2.849  loss_ce_1: 0.2861  loss_mask_1: 0.3662  loss_dice_1: 2.773  loss_ce_2: 0.2876  loss_mask_2: 0.3575  loss_dice_2: 2.752  loss_ce_3: 0.272  loss_mask_3: 0.3562  loss_dice_3: 2.74  loss_ce_4: 0.2597  loss_mask_4: 0.3549  loss_dice_4: 2.743  loss_ce_5: 0.2672  loss_mask_5: 0.357  loss_dice_5: 2.738  loss_ce_6: 0.2535  loss_mask_6: 0.3529  loss_dice_6: 2.744  loss_ce_7: 0.25  loss_mask_7: 0.3533  loss_dice_7: 2.731  loss_ce_8: 0.2483  loss_mask_8: 0.3545  loss_dice_8: 2.735  time: 1.5359  data_time: 0.0912  lr: 7.5108e-06  max_mem: 21478M
[01/17 15:06:49] d2.utils.events INFO:  eta: 1 day, 3:35:57  iter: 24539  total_loss: 34.14  loss_ce: 0.2558  loss_mask: 0.3565  loss_dice: 2.726  loss_ce_0: 0.5668  loss_mask_0: 0.3523  loss_dice_0: 2.843  loss_ce_1: 0.2952  loss_mask_1: 0.3596  loss_dice_1: 2.759  loss_ce_2: 0.2832  loss_mask_2: 0.3586  loss_dice_2: 2.76  loss_ce_3: 0.2532  loss_mask_3: 0.3567  loss_dice_3: 2.744  loss_ce_4: 0.2653  loss_mask_4: 0.3558  loss_dice_4: 2.74  loss_ce_5: 0.2607  loss_mask_5: 0.3562  loss_dice_5: 2.734  loss_ce_6: 0.2592  loss_mask_6: 0.3571  loss_dice_6: 2.728  loss_ce_7: 0.2646  loss_mask_7: 0.3547  loss_dice_7: 2.734  loss_ce_8: 0.2647  loss_mask_8: 0.3562  loss_dice_8: 2.729  time: 1.5358  data_time: 0.0943  lr: 7.5087e-06  max_mem: 21478M
[01/17 15:07:19] d2.utils.events INFO:  eta: 1 day, 3:35:49  iter: 24559  total_loss: 33.93  loss_ce: 0.2464  loss_mask: 0.3635  loss_dice: 2.716  loss_ce_0: 0.5417  loss_mask_0: 0.3703  loss_dice_0: 2.807  loss_ce_1: 0.267  loss_mask_1: 0.3707  loss_dice_1: 2.74  loss_ce_2: 0.2614  loss_mask_2: 0.3627  loss_dice_2: 2.715  loss_ce_3: 0.2465  loss_mask_3: 0.3629  loss_dice_3: 2.709  loss_ce_4: 0.2455  loss_mask_4: 0.3629  loss_dice_4: 2.702  loss_ce_5: 0.2503  loss_mask_5: 0.3623  loss_dice_5: 2.716  loss_ce_6: 0.2307  loss_mask_6: 0.3642  loss_dice_6: 2.705  loss_ce_7: 0.2335  loss_mask_7: 0.3633  loss_dice_7: 2.71  loss_ce_8: 0.2462  loss_mask_8: 0.3626  loss_dice_8: 2.698  time: 1.5358  data_time: 0.0909  lr: 7.5067e-06  max_mem: 21478M
[01/17 15:07:49] d2.utils.events INFO:  eta: 1 day, 3:34:07  iter: 24579  total_loss: 34.42  loss_ce: 0.2493  loss_mask: 0.3537  loss_dice: 2.744  loss_ce_0: 0.5653  loss_mask_0: 0.3576  loss_dice_0: 2.826  loss_ce_1: 0.2863  loss_mask_1: 0.3561  loss_dice_1: 2.771  loss_ce_2: 0.2875  loss_mask_2: 0.353  loss_dice_2: 2.753  loss_ce_3: 0.2667  loss_mask_3: 0.354  loss_dice_3: 2.742  loss_ce_4: 0.2617  loss_mask_4: 0.3518  loss_dice_4: 2.748  loss_ce_5: 0.258  loss_mask_5: 0.3542  loss_dice_5: 2.742  loss_ce_6: 0.2533  loss_mask_6: 0.3531  loss_dice_6: 2.738  loss_ce_7: 0.253  loss_mask_7: 0.3536  loss_dice_7: 2.745  loss_ce_8: 0.245  loss_mask_8: 0.3544  loss_dice_8: 2.739  time: 1.5358  data_time: 0.0853  lr: 7.5046e-06  max_mem: 21478M
[01/17 15:08:18] d2.utils.events INFO:  eta: 1 day, 3:31:53  iter: 24599  total_loss: 33.69  loss_ce: 0.2608  loss_mask: 0.3499  loss_dice: 2.694  loss_ce_0: 0.5457  loss_mask_0: 0.351  loss_dice_0: 2.808  loss_ce_1: 0.2852  loss_mask_1: 0.354  loss_dice_1: 2.736  loss_ce_2: 0.2921  loss_mask_2: 0.3499  loss_dice_2: 2.71  loss_ce_3: 0.2713  loss_mask_3: 0.3494  loss_dice_3: 2.701  loss_ce_4: 0.2643  loss_mask_4: 0.3485  loss_dice_4: 2.697  loss_ce_5: 0.2505  loss_mask_5: 0.3482  loss_dice_5: 2.7  loss_ce_6: 0.2515  loss_mask_6: 0.3502  loss_dice_6: 2.694  loss_ce_7: 0.2467  loss_mask_7: 0.3495  loss_dice_7: 2.698  loss_ce_8: 0.2447  loss_mask_8: 0.3493  loss_dice_8: 2.698  time: 1.5357  data_time: 0.0871  lr: 7.5025e-06  max_mem: 21478M
[01/17 15:08:48] d2.utils.events INFO:  eta: 1 day, 3:29:29  iter: 24619  total_loss: 34.23  loss_ce: 0.2537  loss_mask: 0.3625  loss_dice: 2.754  loss_ce_0: 0.576  loss_mask_0: 0.3608  loss_dice_0: 2.858  loss_ce_1: 0.2757  loss_mask_1: 0.3607  loss_dice_1: 2.787  loss_ce_2: 0.2739  loss_mask_2: 0.3574  loss_dice_2: 2.753  loss_ce_3: 0.2753  loss_mask_3: 0.3573  loss_dice_3: 2.753  loss_ce_4: 0.2676  loss_mask_4: 0.3585  loss_dice_4: 2.756  loss_ce_5: 0.2603  loss_mask_5: 0.3589  loss_dice_5: 2.748  loss_ce_6: 0.254  loss_mask_6: 0.3599  loss_dice_6: 2.752  loss_ce_7: 0.2569  loss_mask_7: 0.36  loss_dice_7: 2.747  loss_ce_8: 0.2573  loss_mask_8: 0.3608  loss_dice_8: 2.751  time: 1.5357  data_time: 0.0870  lr: 7.5005e-06  max_mem: 21478M
[01/17 15:09:18] d2.utils.events INFO:  eta: 1 day, 3:26:41  iter: 24639  total_loss: 33.88  loss_ce: 0.2633  loss_mask: 0.3593  loss_dice: 2.714  loss_ce_0: 0.5802  loss_mask_0: 0.3593  loss_dice_0: 2.813  loss_ce_1: 0.3073  loss_mask_1: 0.3646  loss_dice_1: 2.743  loss_ce_2: 0.2924  loss_mask_2: 0.3621  loss_dice_2: 2.729  loss_ce_3: 0.2778  loss_mask_3: 0.3585  loss_dice_3: 2.715  loss_ce_4: 0.2688  loss_mask_4: 0.3567  loss_dice_4: 2.716  loss_ce_5: 0.2652  loss_mask_5: 0.3563  loss_dice_5: 2.717  loss_ce_6: 0.2549  loss_mask_6: 0.356  loss_dice_6: 2.725  loss_ce_7: 0.2703  loss_mask_7: 0.356  loss_dice_7: 2.718  loss_ce_8: 0.2674  loss_mask_8: 0.3576  loss_dice_8: 2.712  time: 1.5356  data_time: 0.0916  lr: 7.4984e-06  max_mem: 21478M
[01/17 15:09:48] d2.utils.events INFO:  eta: 1 day, 3:24:16  iter: 24659  total_loss: 34.14  loss_ce: 0.2811  loss_mask: 0.3535  loss_dice: 2.732  loss_ce_0: 0.5488  loss_mask_0: 0.3505  loss_dice_0: 2.852  loss_ce_1: 0.3132  loss_mask_1: 0.3566  loss_dice_1: 2.769  loss_ce_2: 0.3097  loss_mask_2: 0.3515  loss_dice_2: 2.753  loss_ce_3: 0.3004  loss_mask_3: 0.3524  loss_dice_3: 2.737  loss_ce_4: 0.2971  loss_mask_4: 0.3509  loss_dice_4: 2.73  loss_ce_5: 0.2833  loss_mask_5: 0.3539  loss_dice_5: 2.738  loss_ce_6: 0.2758  loss_mask_6: 0.3547  loss_dice_6: 2.734  loss_ce_7: 0.2758  loss_mask_7: 0.3542  loss_dice_7: 2.732  loss_ce_8: 0.2913  loss_mask_8: 0.3539  loss_dice_8: 2.738  time: 1.5356  data_time: 0.0863  lr: 7.4963e-06  max_mem: 21478M
[01/17 15:10:18] d2.utils.events INFO:  eta: 1 day, 3:22:54  iter: 24679  total_loss: 34.61  loss_ce: 0.2664  loss_mask: 0.3516  loss_dice: 2.758  loss_ce_0: 0.5662  loss_mask_0: 0.3618  loss_dice_0: 2.858  loss_ce_1: 0.3021  loss_mask_1: 0.3622  loss_dice_1: 2.8  loss_ce_2: 0.307  loss_mask_2: 0.356  loss_dice_2: 2.783  loss_ce_3: 0.3013  loss_mask_3: 0.3543  loss_dice_3: 2.769  loss_ce_4: 0.2641  loss_mask_4: 0.3525  loss_dice_4: 2.765  loss_ce_5: 0.2745  loss_mask_5: 0.3507  loss_dice_5: 2.766  loss_ce_6: 0.2601  loss_mask_6: 0.3533  loss_dice_6: 2.764  loss_ce_7: 0.2809  loss_mask_7: 0.3525  loss_dice_7: 2.76  loss_ce_8: 0.259  loss_mask_8: 0.3534  loss_dice_8: 2.762  time: 1.5356  data_time: 0.0872  lr: 7.4943e-06  max_mem: 21478M
[01/17 15:10:48] d2.utils.events INFO:  eta: 1 day, 3:21:01  iter: 24699  total_loss: 34.17  loss_ce: 0.2676  loss_mask: 0.3582  loss_dice: 2.69  loss_ce_0: 0.5834  loss_mask_0: 0.3566  loss_dice_0: 2.797  loss_ce_1: 0.2963  loss_mask_1: 0.3597  loss_dice_1: 2.713  loss_ce_2: 0.2989  loss_mask_2: 0.3587  loss_dice_2: 2.704  loss_ce_3: 0.2712  loss_mask_3: 0.3587  loss_dice_3: 2.69  loss_ce_4: 0.2749  loss_mask_4: 0.3582  loss_dice_4: 2.685  loss_ce_5: 0.2615  loss_mask_5: 0.359  loss_dice_5: 2.687  loss_ce_6: 0.2701  loss_mask_6: 0.3576  loss_dice_6: 2.687  loss_ce_7: 0.2623  loss_mask_7: 0.3577  loss_dice_7: 2.689  loss_ce_8: 0.2676  loss_mask_8: 0.357  loss_dice_8: 2.692  time: 1.5356  data_time: 0.0862  lr: 7.4922e-06  max_mem: 21478M
[01/17 15:11:18] d2.utils.events INFO:  eta: 1 day, 3:20:05  iter: 24719  total_loss: 33.4  loss_ce: 0.2468  loss_mask: 0.3627  loss_dice: 2.696  loss_ce_0: 0.5324  loss_mask_0: 0.3689  loss_dice_0: 2.797  loss_ce_1: 0.2717  loss_mask_1: 0.3705  loss_dice_1: 2.722  loss_ce_2: 0.2758  loss_mask_2: 0.3692  loss_dice_2: 2.707  loss_ce_3: 0.2508  loss_mask_3: 0.3637  loss_dice_3: 2.695  loss_ce_4: 0.2481  loss_mask_4: 0.3622  loss_dice_4: 2.701  loss_ce_5: 0.2471  loss_mask_5: 0.3614  loss_dice_5: 2.701  loss_ce_6: 0.2502  loss_mask_6: 0.3626  loss_dice_6: 2.694  loss_ce_7: 0.2565  loss_mask_7: 0.3637  loss_dice_7: 2.689  loss_ce_8: 0.2415  loss_mask_8: 0.3632  loss_dice_8: 2.694  time: 1.5355  data_time: 0.0931  lr: 7.4901e-06  max_mem: 21478M
[01/17 15:11:48] d2.utils.events INFO:  eta: 1 day, 3:18:25  iter: 24739  total_loss: 33.8  loss_ce: 0.2563  loss_mask: 0.353  loss_dice: 2.696  loss_ce_0: 0.5737  loss_mask_0: 0.3573  loss_dice_0: 2.805  loss_ce_1: 0.3191  loss_mask_1: 0.3556  loss_dice_1: 2.721  loss_ce_2: 0.3065  loss_mask_2: 0.355  loss_dice_2: 2.707  loss_ce_3: 0.2739  loss_mask_3: 0.3541  loss_dice_3: 2.705  loss_ce_4: 0.2708  loss_mask_4: 0.3535  loss_dice_4: 2.709  loss_ce_5: 0.2696  loss_mask_5: 0.3563  loss_dice_5: 2.702  loss_ce_6: 0.2529  loss_mask_6: 0.3544  loss_dice_6: 2.693  loss_ce_7: 0.2522  loss_mask_7: 0.3528  loss_dice_7: 2.701  loss_ce_8: 0.2348  loss_mask_8: 0.3533  loss_dice_8: 2.704  time: 1.5355  data_time: 0.0893  lr: 7.4881e-06  max_mem: 21478M
[01/17 15:12:17] d2.utils.events INFO:  eta: 1 day, 3:17:39  iter: 24759  total_loss: 33.91  loss_ce: 0.2426  loss_mask: 0.3627  loss_dice: 2.728  loss_ce_0: 0.5445  loss_mask_0: 0.3572  loss_dice_0: 2.854  loss_ce_1: 0.2594  loss_mask_1: 0.3687  loss_dice_1: 2.784  loss_ce_2: 0.2563  loss_mask_2: 0.3639  loss_dice_2: 2.762  loss_ce_3: 0.2491  loss_mask_3: 0.3621  loss_dice_3: 2.744  loss_ce_4: 0.2391  loss_mask_4: 0.3623  loss_dice_4: 2.741  loss_ce_5: 0.2433  loss_mask_5: 0.362  loss_dice_5: 2.737  loss_ce_6: 0.2419  loss_mask_6: 0.3635  loss_dice_6: 2.734  loss_ce_7: 0.2375  loss_mask_7: 0.3637  loss_dice_7: 2.735  loss_ce_8: 0.2439  loss_mask_8: 0.362  loss_dice_8: 2.737  time: 1.5354  data_time: 0.0851  lr: 7.486e-06  max_mem: 21478M
[01/17 15:12:47] d2.utils.events INFO:  eta: 1 day, 3:16:48  iter: 24779  total_loss: 34.01  loss_ce: 0.2614  loss_mask: 0.3553  loss_dice: 2.698  loss_ce_0: 0.564  loss_mask_0: 0.3578  loss_dice_0: 2.812  loss_ce_1: 0.3099  loss_mask_1: 0.3612  loss_dice_1: 2.74  loss_ce_2: 0.2919  loss_mask_2: 0.3546  loss_dice_2: 2.718  loss_ce_3: 0.2753  loss_mask_3: 0.356  loss_dice_3: 2.712  loss_ce_4: 0.2762  loss_mask_4: 0.3547  loss_dice_4: 2.705  loss_ce_5: 0.2711  loss_mask_5: 0.3554  loss_dice_5: 2.71  loss_ce_6: 0.265  loss_mask_6: 0.3542  loss_dice_6: 2.716  loss_ce_7: 0.2649  loss_mask_7: 0.3545  loss_dice_7: 2.71  loss_ce_8: 0.2599  loss_mask_8: 0.3543  loss_dice_8: 2.707  time: 1.5354  data_time: 0.0819  lr: 7.4839e-06  max_mem: 21478M
[01/17 15:13:17] d2.utils.events INFO:  eta: 1 day, 3:15:37  iter: 24799  total_loss: 34.69  loss_ce: 0.2475  loss_mask: 0.3566  loss_dice: 2.793  loss_ce_0: 0.5647  loss_mask_0: 0.3663  loss_dice_0: 2.905  loss_ce_1: 0.283  loss_mask_1: 0.3642  loss_dice_1: 2.827  loss_ce_2: 0.2699  loss_mask_2: 0.3608  loss_dice_2: 2.811  loss_ce_3: 0.2544  loss_mask_3: 0.3582  loss_dice_3: 2.794  loss_ce_4: 0.2423  loss_mask_4: 0.3582  loss_dice_4: 2.796  loss_ce_5: 0.2554  loss_mask_5: 0.3579  loss_dice_5: 2.792  loss_ce_6: 0.2533  loss_mask_6: 0.3573  loss_dice_6: 2.784  loss_ce_7: 0.2419  loss_mask_7: 0.3576  loss_dice_7: 2.789  loss_ce_8: 0.2407  loss_mask_8: 0.3573  loss_dice_8: 2.792  time: 1.5354  data_time: 0.0982  lr: 7.4819e-06  max_mem: 21478M
[01/17 15:13:47] d2.utils.events INFO:  eta: 1 day, 3:15:06  iter: 24819  total_loss: 33.59  loss_ce: 0.2429  loss_mask: 0.3529  loss_dice: 2.716  loss_ce_0: 0.5728  loss_mask_0: 0.3557  loss_dice_0: 2.835  loss_ce_1: 0.2884  loss_mask_1: 0.3531  loss_dice_1: 2.755  loss_ce_2: 0.2758  loss_mask_2: 0.3516  loss_dice_2: 2.74  loss_ce_3: 0.2705  loss_mask_3: 0.3544  loss_dice_3: 2.72  loss_ce_4: 0.2529  loss_mask_4: 0.354  loss_dice_4: 2.721  loss_ce_5: 0.2429  loss_mask_5: 0.3529  loss_dice_5: 2.73  loss_ce_6: 0.2362  loss_mask_6: 0.3537  loss_dice_6: 2.72  loss_ce_7: 0.2534  loss_mask_7: 0.3536  loss_dice_7: 2.713  loss_ce_8: 0.2487  loss_mask_8: 0.3539  loss_dice_8: 2.72  time: 1.5353  data_time: 0.0851  lr: 7.4798e-06  max_mem: 21478M
[01/17 15:14:17] d2.utils.events INFO:  eta: 1 day, 3:13:02  iter: 24839  total_loss: 33.26  loss_ce: 0.2393  loss_mask: 0.3524  loss_dice: 2.672  loss_ce_0: 0.5703  loss_mask_0: 0.3548  loss_dice_0: 2.79  loss_ce_1: 0.2943  loss_mask_1: 0.3584  loss_dice_1: 2.702  loss_ce_2: 0.2957  loss_mask_2: 0.3553  loss_dice_2: 2.692  loss_ce_3: 0.2506  loss_mask_3: 0.3547  loss_dice_3: 2.673  loss_ce_4: 0.2586  loss_mask_4: 0.3538  loss_dice_4: 2.676  loss_ce_5: 0.2414  loss_mask_5: 0.3552  loss_dice_5: 2.681  loss_ce_6: 0.2565  loss_mask_6: 0.3541  loss_dice_6: 2.679  loss_ce_7: 0.2474  loss_mask_7: 0.3537  loss_dice_7: 2.673  loss_ce_8: 0.2376  loss_mask_8: 0.3535  loss_dice_8: 2.678  time: 1.5353  data_time: 0.0953  lr: 7.4777e-06  max_mem: 21478M
[01/17 15:14:47] d2.utils.events INFO:  eta: 1 day, 3:09:40  iter: 24859  total_loss: 33.91  loss_ce: 0.2589  loss_mask: 0.3506  loss_dice: 2.709  loss_ce_0: 0.5957  loss_mask_0: 0.3514  loss_dice_0: 2.821  loss_ce_1: 0.3126  loss_mask_1: 0.3525  loss_dice_1: 2.746  loss_ce_2: 0.3029  loss_mask_2: 0.3528  loss_dice_2: 2.727  loss_ce_3: 0.276  loss_mask_3: 0.3502  loss_dice_3: 2.707  loss_ce_4: 0.2828  loss_mask_4: 0.35  loss_dice_4: 2.706  loss_ce_5: 0.2724  loss_mask_5: 0.3506  loss_dice_5: 2.724  loss_ce_6: 0.271  loss_mask_6: 0.3504  loss_dice_6: 2.708  loss_ce_7: 0.2636  loss_mask_7: 0.35  loss_dice_7: 2.712  loss_ce_8: 0.2664  loss_mask_8: 0.3499  loss_dice_8: 2.705  time: 1.5353  data_time: 0.0854  lr: 7.4757e-06  max_mem: 21478M
[01/17 15:15:17] d2.utils.events INFO:  eta: 1 day, 3:07:47  iter: 24879  total_loss: 34.21  loss_ce: 0.2554  loss_mask: 0.3612  loss_dice: 2.763  loss_ce_0: 0.5642  loss_mask_0: 0.3605  loss_dice_0: 2.881  loss_ce_1: 0.2928  loss_mask_1: 0.3625  loss_dice_1: 2.803  loss_ce_2: 0.2784  loss_mask_2: 0.3601  loss_dice_2: 2.773  loss_ce_3: 0.2577  loss_mask_3: 0.3576  loss_dice_3: 2.757  loss_ce_4: 0.2708  loss_mask_4: 0.3572  loss_dice_4: 2.762  loss_ce_5: 0.2426  loss_mask_5: 0.3591  loss_dice_5: 2.765  loss_ce_6: 0.2464  loss_mask_6: 0.3574  loss_dice_6: 2.771  loss_ce_7: 0.2562  loss_mask_7: 0.36  loss_dice_7: 2.762  loss_ce_8: 0.2444  loss_mask_8: 0.3598  loss_dice_8: 2.768  time: 1.5352  data_time: 0.0820  lr: 7.4736e-06  max_mem: 21478M
[01/17 15:15:47] d2.utils.events INFO:  eta: 1 day, 3:06:52  iter: 24899  total_loss: 33.86  loss_ce: 0.263  loss_mask: 0.3446  loss_dice: 2.707  loss_ce_0: 0.573  loss_mask_0: 0.3498  loss_dice_0: 2.81  loss_ce_1: 0.3052  loss_mask_1: 0.3525  loss_dice_1: 2.742  loss_ce_2: 0.2936  loss_mask_2: 0.3462  loss_dice_2: 2.724  loss_ce_3: 0.2693  loss_mask_3: 0.3457  loss_dice_3: 2.716  loss_ce_4: 0.2671  loss_mask_4: 0.3446  loss_dice_4: 2.708  loss_ce_5: 0.2513  loss_mask_5: 0.3453  loss_dice_5: 2.713  loss_ce_6: 0.254  loss_mask_6: 0.3452  loss_dice_6: 2.715  loss_ce_7: 0.2695  loss_mask_7: 0.3446  loss_dice_7: 2.705  loss_ce_8: 0.2663  loss_mask_8: 0.3445  loss_dice_8: 2.706  time: 1.5352  data_time: 0.0805  lr: 7.4715e-06  max_mem: 21478M
[01/17 15:16:16] d2.utils.events INFO:  eta: 1 day, 3:04:31  iter: 24919  total_loss: 34.34  loss_ce: 0.2757  loss_mask: 0.3626  loss_dice: 2.757  loss_ce_0: 0.5787  loss_mask_0: 0.3647  loss_dice_0: 2.855  loss_ce_1: 0.3293  loss_mask_1: 0.3681  loss_dice_1: 2.8  loss_ce_2: 0.3105  loss_mask_2: 0.3621  loss_dice_2: 2.776  loss_ce_3: 0.2978  loss_mask_3: 0.3604  loss_dice_3: 2.762  loss_ce_4: 0.3004  loss_mask_4: 0.3613  loss_dice_4: 2.769  loss_ce_5: 0.2753  loss_mask_5: 0.3621  loss_dice_5: 2.774  loss_ce_6: 0.292  loss_mask_6: 0.3618  loss_dice_6: 2.767  loss_ce_7: 0.2793  loss_mask_7: 0.3602  loss_dice_7: 2.754  loss_ce_8: 0.2791  loss_mask_8: 0.3623  loss_dice_8: 2.764  time: 1.5352  data_time: 0.0904  lr: 7.4695e-06  max_mem: 21478M
[01/17 15:16:46] d2.utils.events INFO:  eta: 1 day, 3:03:02  iter: 24939  total_loss: 33.61  loss_ce: 0.2603  loss_mask: 0.3491  loss_dice: 2.68  loss_ce_0: 0.5319  loss_mask_0: 0.3522  loss_dice_0: 2.793  loss_ce_1: 0.2893  loss_mask_1: 0.3588  loss_dice_1: 2.719  loss_ce_2: 0.2757  loss_mask_2: 0.3524  loss_dice_2: 2.714  loss_ce_3: 0.2783  loss_mask_3: 0.3504  loss_dice_3: 2.7  loss_ce_4: 0.2676  loss_mask_4: 0.3497  loss_dice_4: 2.699  loss_ce_5: 0.2607  loss_mask_5: 0.3488  loss_dice_5: 2.702  loss_ce_6: 0.2665  loss_mask_6: 0.3513  loss_dice_6: 2.69  loss_ce_7: 0.2538  loss_mask_7: 0.3495  loss_dice_7: 2.692  loss_ce_8: 0.2586  loss_mask_8: 0.3495  loss_dice_8: 2.69  time: 1.5351  data_time: 0.0854  lr: 7.4674e-06  max_mem: 21478M
[01/17 15:17:16] d2.utils.events INFO:  eta: 1 day, 3:01:56  iter: 24959  total_loss: 34.1  loss_ce: 0.2562  loss_mask: 0.3559  loss_dice: 2.758  loss_ce_0: 0.5495  loss_mask_0: 0.3722  loss_dice_0: 2.856  loss_ce_1: 0.2749  loss_mask_1: 0.3691  loss_dice_1: 2.819  loss_ce_2: 0.2734  loss_mask_2: 0.3602  loss_dice_2: 2.79  loss_ce_3: 0.2742  loss_mask_3: 0.3568  loss_dice_3: 2.769  loss_ce_4: 0.2567  loss_mask_4: 0.3543  loss_dice_4: 2.772  loss_ce_5: 0.2596  loss_mask_5: 0.355  loss_dice_5: 2.772  loss_ce_6: 0.2611  loss_mask_6: 0.3571  loss_dice_6: 2.77  loss_ce_7: 0.2559  loss_mask_7: 0.3561  loss_dice_7: 2.766  loss_ce_8: 0.244  loss_mask_8: 0.3554  loss_dice_8: 2.767  time: 1.5351  data_time: 0.0910  lr: 7.4654e-06  max_mem: 21478M
[01/17 15:17:46] d2.utils.events INFO:  eta: 1 day, 3:00:39  iter: 24979  total_loss: 33.75  loss_ce: 0.243  loss_mask: 0.3575  loss_dice: 2.692  loss_ce_0: 0.5824  loss_mask_0: 0.3559  loss_dice_0: 2.79  loss_ce_1: 0.2982  loss_mask_1: 0.3581  loss_dice_1: 2.722  loss_ce_2: 0.2808  loss_mask_2: 0.3535  loss_dice_2: 2.704  loss_ce_3: 0.2677  loss_mask_3: 0.3569  loss_dice_3: 2.69  loss_ce_4: 0.261  loss_mask_4: 0.3566  loss_dice_4: 2.693  loss_ce_5: 0.2669  loss_mask_5: 0.3563  loss_dice_5: 2.694  loss_ce_6: 0.2579  loss_mask_6: 0.3593  loss_dice_6: 2.687  loss_ce_7: 0.2594  loss_mask_7: 0.3572  loss_dice_7: 2.686  loss_ce_8: 0.2383  loss_mask_8: 0.3578  loss_dice_8: 2.693  time: 1.5350  data_time: 0.0838  lr: 7.4633e-06  max_mem: 21478M
[01/17 15:18:16] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla/model_0024999.pth
[01/17 15:18:17] d2.utils.events INFO:  eta: 1 day, 2:59:33  iter: 24999  total_loss: 34.43  loss_ce: 0.2536  loss_mask: 0.3578  loss_dice: 2.793  loss_ce_0: 0.569  loss_mask_0: 0.361  loss_dice_0: 2.882  loss_ce_1: 0.2818  loss_mask_1: 0.3689  loss_dice_1: 2.811  loss_ce_2: 0.2777  loss_mask_2: 0.3635  loss_dice_2: 2.802  loss_ce_3: 0.2695  loss_mask_3: 0.3603  loss_dice_3: 2.793  loss_ce_4: 0.2731  loss_mask_4: 0.3598  loss_dice_4: 2.791  loss_ce_5: 0.2439  loss_mask_5: 0.3603  loss_dice_5: 2.797  loss_ce_6: 0.2531  loss_mask_6: 0.3599  loss_dice_6: 2.794  loss_ce_7: 0.264  loss_mask_7: 0.3584  loss_dice_7: 2.795  loss_ce_8: 0.2467  loss_mask_8: 0.3595  loss_dice_8: 2.796  time: 1.5350  data_time: 0.0854  lr: 7.4612e-06  max_mem: 21478M
[01/17 15:18:47] d2.utils.events INFO:  eta: 1 day, 2:59:03  iter: 25019  total_loss: 34.16  loss_ce: 0.2581  loss_mask: 0.3523  loss_dice: 2.764  loss_ce_0: 0.5615  loss_mask_0: 0.3614  loss_dice_0: 2.87  loss_ce_1: 0.3048  loss_mask_1: 0.3628  loss_dice_1: 2.803  loss_ce_2: 0.2858  loss_mask_2: 0.3581  loss_dice_2: 2.775  loss_ce_3: 0.276  loss_mask_3: 0.3551  loss_dice_3: 2.769  loss_ce_4: 0.2592  loss_mask_4: 0.3541  loss_dice_4: 2.767  loss_ce_5: 0.2501  loss_mask_5: 0.3546  loss_dice_5: 2.78  loss_ce_6: 0.2629  loss_mask_6: 0.354  loss_dice_6: 2.772  loss_ce_7: 0.2587  loss_mask_7: 0.3536  loss_dice_7: 2.764  loss_ce_8: 0.2709  loss_mask_8: 0.3514  loss_dice_8: 2.761  time: 1.5350  data_time: 0.0893  lr: 7.4592e-06  max_mem: 21478M
[01/17 15:19:17] d2.utils.events INFO:  eta: 1 day, 2:57:12  iter: 25039  total_loss: 33.33  loss_ce: 0.2511  loss_mask: 0.3531  loss_dice: 2.661  loss_ce_0: 0.5558  loss_mask_0: 0.3537  loss_dice_0: 2.773  loss_ce_1: 0.2966  loss_mask_1: 0.3575  loss_dice_1: 2.704  loss_ce_2: 0.2942  loss_mask_2: 0.3526  loss_dice_2: 2.686  loss_ce_3: 0.2554  loss_mask_3: 0.3515  loss_dice_3: 2.682  loss_ce_4: 0.2564  loss_mask_4: 0.352  loss_dice_4: 2.675  loss_ce_5: 0.2559  loss_mask_5: 0.3497  loss_dice_5: 2.675  loss_ce_6: 0.2469  loss_mask_6: 0.3523  loss_dice_6: 2.667  loss_ce_7: 0.2578  loss_mask_7: 0.352  loss_dice_7: 2.673  loss_ce_8: 0.2524  loss_mask_8: 0.3515  loss_dice_8: 2.67  time: 1.5349  data_time: 0.0807  lr: 7.4571e-06  max_mem: 21478M
[01/17 15:19:47] d2.utils.events INFO:  eta: 1 day, 2:58:39  iter: 25059  total_loss: 34.78  loss_ce: 0.275  loss_mask: 0.3447  loss_dice: 2.783  loss_ce_0: 0.5738  loss_mask_0: 0.351  loss_dice_0: 2.903  loss_ce_1: 0.3069  loss_mask_1: 0.3476  loss_dice_1: 2.822  loss_ce_2: 0.2987  loss_mask_2: 0.3461  loss_dice_2: 2.798  loss_ce_3: 0.2855  loss_mask_3: 0.3476  loss_dice_3: 2.791  loss_ce_4: 0.2792  loss_mask_4: 0.3486  loss_dice_4: 2.785  loss_ce_5: 0.2732  loss_mask_5: 0.348  loss_dice_5: 2.794  loss_ce_6: 0.2783  loss_mask_6: 0.3474  loss_dice_6: 2.78  loss_ce_7: 0.2686  loss_mask_7: 0.3455  loss_dice_7: 2.784  loss_ce_8: 0.2681  loss_mask_8: 0.3463  loss_dice_8: 2.781  time: 1.5349  data_time: 0.0914  lr: 7.455e-06  max_mem: 21478M
[01/17 15:20:17] d2.utils.events INFO:  eta: 1 day, 2:58:20  iter: 25079  total_loss: 34.07  loss_ce: 0.2545  loss_mask: 0.3614  loss_dice: 2.71  loss_ce_0: 0.546  loss_mask_0: 0.3627  loss_dice_0: 2.819  loss_ce_1: 0.2969  loss_mask_1: 0.3666  loss_dice_1: 2.758  loss_ce_2: 0.2842  loss_mask_2: 0.3613  loss_dice_2: 2.74  loss_ce_3: 0.2498  loss_mask_3: 0.3593  loss_dice_3: 2.728  loss_ce_4: 0.2601  loss_mask_4: 0.3581  loss_dice_4: 2.719  loss_ce_5: 0.2491  loss_mask_5: 0.3596  loss_dice_5: 2.723  loss_ce_6: 0.2481  loss_mask_6: 0.359  loss_dice_6: 2.719  loss_ce_7: 0.2655  loss_mask_7: 0.3603  loss_dice_7: 2.722  loss_ce_8: 0.2525  loss_mask_8: 0.3605  loss_dice_8: 2.72  time: 1.5349  data_time: 0.0884  lr: 7.453e-06  max_mem: 21478M
[01/17 15:20:47] d2.utils.events INFO:  eta: 1 day, 2:58:17  iter: 25099  total_loss: 33.7  loss_ce: 0.2504  loss_mask: 0.3571  loss_dice: 2.691  loss_ce_0: 0.5698  loss_mask_0: 0.3586  loss_dice_0: 2.807  loss_ce_1: 0.2804  loss_mask_1: 0.3615  loss_dice_1: 2.738  loss_ce_2: 0.272  loss_mask_2: 0.3583  loss_dice_2: 2.711  loss_ce_3: 0.2538  loss_mask_3: 0.3591  loss_dice_3: 2.691  loss_ce_4: 0.2589  loss_mask_4: 0.3574  loss_dice_4: 2.687  loss_ce_5: 0.26  loss_mask_5: 0.3565  loss_dice_5: 2.695  loss_ce_6: 0.2477  loss_mask_6: 0.3573  loss_dice_6: 2.688  loss_ce_7: 0.257  loss_mask_7: 0.357  loss_dice_7: 2.696  loss_ce_8: 0.245  loss_mask_8: 0.3578  loss_dice_8: 2.694  time: 1.5349  data_time: 0.0856  lr: 7.4509e-06  max_mem: 21478M
[01/17 15:21:17] d2.utils.events INFO:  eta: 1 day, 2:57:52  iter: 25119  total_loss: 34.37  loss_ce: 0.255  loss_mask: 0.3558  loss_dice: 2.743  loss_ce_0: 0.5737  loss_mask_0: 0.3584  loss_dice_0: 2.851  loss_ce_1: 0.2913  loss_mask_1: 0.3566  loss_dice_1: 2.78  loss_ce_2: 0.2752  loss_mask_2: 0.356  loss_dice_2: 2.763  loss_ce_3: 0.2725  loss_mask_3: 0.3562  loss_dice_3: 2.748  loss_ce_4: 0.252  loss_mask_4: 0.3567  loss_dice_4: 2.749  loss_ce_5: 0.2611  loss_mask_5: 0.359  loss_dice_5: 2.749  loss_ce_6: 0.2665  loss_mask_6: 0.3582  loss_dice_6: 2.759  loss_ce_7: 0.2557  loss_mask_7: 0.3581  loss_dice_7: 2.743  loss_ce_8: 0.2609  loss_mask_8: 0.3565  loss_dice_8: 2.75  time: 1.5348  data_time: 0.0861  lr: 7.4488e-06  max_mem: 21478M
[01/17 15:21:47] d2.utils.events INFO:  eta: 1 day, 2:57:18  iter: 25139  total_loss: 33.76  loss_ce: 0.2486  loss_mask: 0.3567  loss_dice: 2.727  loss_ce_0: 0.5592  loss_mask_0: 0.3581  loss_dice_0: 2.843  loss_ce_1: 0.2839  loss_mask_1: 0.3596  loss_dice_1: 2.767  loss_ce_2: 0.2872  loss_mask_2: 0.3592  loss_dice_2: 2.736  loss_ce_3: 0.2613  loss_mask_3: 0.3605  loss_dice_3: 2.735  loss_ce_4: 0.2604  loss_mask_4: 0.3572  loss_dice_4: 2.731  loss_ce_5: 0.2497  loss_mask_5: 0.3576  loss_dice_5: 2.744  loss_ce_6: 0.2536  loss_mask_6: 0.356  loss_dice_6: 2.726  loss_ce_7: 0.241  loss_mask_7: 0.3571  loss_dice_7: 2.727  loss_ce_8: 0.2341  loss_mask_8: 0.3572  loss_dice_8: 2.729  time: 1.5348  data_time: 0.0896  lr: 7.4468e-06  max_mem: 21478M
[01/17 15:22:17] d2.utils.events INFO:  eta: 1 day, 2:56:27  iter: 25159  total_loss: 33.92  loss_ce: 0.2513  loss_mask: 0.3385  loss_dice: 2.717  loss_ce_0: 0.5711  loss_mask_0: 0.3378  loss_dice_0: 2.833  loss_ce_1: 0.2799  loss_mask_1: 0.3405  loss_dice_1: 2.763  loss_ce_2: 0.2901  loss_mask_2: 0.3363  loss_dice_2: 2.735  loss_ce_3: 0.2614  loss_mask_3: 0.3356  loss_dice_3: 2.725  loss_ce_4: 0.2848  loss_mask_4: 0.334  loss_dice_4: 2.721  loss_ce_5: 0.2676  loss_mask_5: 0.3357  loss_dice_5: 2.719  loss_ce_6: 0.2605  loss_mask_6: 0.3349  loss_dice_6: 2.724  loss_ce_7: 0.2628  loss_mask_7: 0.3368  loss_dice_7: 2.718  loss_ce_8: 0.2551  loss_mask_8: 0.3359  loss_dice_8: 2.728  time: 1.5348  data_time: 0.0891  lr: 7.4447e-06  max_mem: 21478M
[01/17 15:22:47] d2.utils.events INFO:  eta: 1 day, 2:55:39  iter: 25179  total_loss: 34.49  loss_ce: 0.2413  loss_mask: 0.3597  loss_dice: 2.778  loss_ce_0: 0.5602  loss_mask_0: 0.3636  loss_dice_0: 2.873  loss_ce_1: 0.2852  loss_mask_1: 0.3663  loss_dice_1: 2.81  loss_ce_2: 0.2675  loss_mask_2: 0.3587  loss_dice_2: 2.794  loss_ce_3: 0.2595  loss_mask_3: 0.3598  loss_dice_3: 2.779  loss_ce_4: 0.2534  loss_mask_4: 0.3588  loss_dice_4: 2.778  loss_ce_5: 0.2405  loss_mask_5: 0.3598  loss_dice_5: 2.788  loss_ce_6: 0.2394  loss_mask_6: 0.3585  loss_dice_6: 2.778  loss_ce_7: 0.2407  loss_mask_7: 0.3576  loss_dice_7: 2.778  loss_ce_8: 0.2331  loss_mask_8: 0.3587  loss_dice_8: 2.769  time: 1.5347  data_time: 0.0826  lr: 7.4426e-06  max_mem: 21478M
[01/17 15:23:17] d2.utils.events INFO:  eta: 1 day, 2:55:12  iter: 25199  total_loss: 33.82  loss_ce: 0.2556  loss_mask: 0.355  loss_dice: 2.708  loss_ce_0: 0.5557  loss_mask_0: 0.3687  loss_dice_0: 2.797  loss_ce_1: 0.286  loss_mask_1: 0.3664  loss_dice_1: 2.736  loss_ce_2: 0.2891  loss_mask_2: 0.3587  loss_dice_2: 2.728  loss_ce_3: 0.2546  loss_mask_3: 0.3567  loss_dice_3: 2.714  loss_ce_4: 0.2611  loss_mask_4: 0.3556  loss_dice_4: 2.701  loss_ce_5: 0.254  loss_mask_5: 0.355  loss_dice_5: 2.715  loss_ce_6: 0.2476  loss_mask_6: 0.3541  loss_dice_6: 2.709  loss_ce_7: 0.2638  loss_mask_7: 0.3524  loss_dice_7: 2.707  loss_ce_8: 0.2565  loss_mask_8: 0.3546  loss_dice_8: 2.697  time: 1.5347  data_time: 0.0881  lr: 7.4406e-06  max_mem: 21478M
[01/17 15:23:47] d2.utils.events INFO:  eta: 1 day, 2:55:06  iter: 25219  total_loss: 32.76  loss_ce: 0.256  loss_mask: 0.344  loss_dice: 2.641  loss_ce_0: 0.5302  loss_mask_0: 0.3428  loss_dice_0: 2.754  loss_ce_1: 0.2667  loss_mask_1: 0.3486  loss_dice_1: 2.686  loss_ce_2: 0.2648  loss_mask_2: 0.346  loss_dice_2: 2.659  loss_ce_3: 0.2612  loss_mask_3: 0.3456  loss_dice_3: 2.642  loss_ce_4: 0.2544  loss_mask_4: 0.3447  loss_dice_4: 2.646  loss_ce_5: 0.2392  loss_mask_5: 0.3446  loss_dice_5: 2.654  loss_ce_6: 0.2465  loss_mask_6: 0.3458  loss_dice_6: 2.64  loss_ce_7: 0.2486  loss_mask_7: 0.3458  loss_dice_7: 2.64  loss_ce_8: 0.2538  loss_mask_8: 0.3452  loss_dice_8: 2.637  time: 1.5347  data_time: 0.0878  lr: 7.4385e-06  max_mem: 21478M
[01/17 15:24:17] d2.utils.events INFO:  eta: 1 day, 2:54:44  iter: 25239  total_loss: 34.13  loss_ce: 0.256  loss_mask: 0.3527  loss_dice: 2.733  loss_ce_0: 0.5347  loss_mask_0: 0.3498  loss_dice_0: 2.848  loss_ce_1: 0.2886  loss_mask_1: 0.3541  loss_dice_1: 2.779  loss_ce_2: 0.2823  loss_mask_2: 0.3519  loss_dice_2: 2.754  loss_ce_3: 0.2607  loss_mask_3: 0.349  loss_dice_3: 2.755  loss_ce_4: 0.2621  loss_mask_4: 0.3493  loss_dice_4: 2.743  loss_ce_5: 0.2638  loss_mask_5: 0.3506  loss_dice_5: 2.737  loss_ce_6: 0.2502  loss_mask_6: 0.3513  loss_dice_6: 2.746  loss_ce_7: 0.2547  loss_mask_7: 0.3522  loss_dice_7: 2.74  loss_ce_8: 0.2537  loss_mask_8: 0.3525  loss_dice_8: 2.741  time: 1.5347  data_time: 0.0921  lr: 7.4364e-06  max_mem: 21478M
[01/17 15:24:47] d2.utils.events INFO:  eta: 1 day, 2:54:14  iter: 25259  total_loss: 33.3  loss_ce: 0.2304  loss_mask: 0.3442  loss_dice: 2.703  loss_ce_0: 0.5651  loss_mask_0: 0.3372  loss_dice_0: 2.82  loss_ce_1: 0.2733  loss_mask_1: 0.3438  loss_dice_1: 2.744  loss_ce_2: 0.2735  loss_mask_2: 0.3435  loss_dice_2: 2.715  loss_ce_3: 0.2581  loss_mask_3: 0.3444  loss_dice_3: 2.701  loss_ce_4: 0.2473  loss_mask_4: 0.3407  loss_dice_4: 2.7  loss_ce_5: 0.244  loss_mask_5: 0.342  loss_dice_5: 2.701  loss_ce_6: 0.2337  loss_mask_6: 0.3427  loss_dice_6: 2.71  loss_ce_7: 0.2418  loss_mask_7: 0.3434  loss_dice_7: 2.707  loss_ce_8: 0.2423  loss_mask_8: 0.3436  loss_dice_8: 2.699  time: 1.5346  data_time: 0.0822  lr: 7.4344e-06  max_mem: 21478M
[01/17 15:25:17] d2.utils.events INFO:  eta: 1 day, 2:53:36  iter: 25279  total_loss: 33.63  loss_ce: 0.2484  loss_mask: 0.3495  loss_dice: 2.697  loss_ce_0: 0.56  loss_mask_0: 0.3525  loss_dice_0: 2.799  loss_ce_1: 0.2871  loss_mask_1: 0.3526  loss_dice_1: 2.739  loss_ce_2: 0.2729  loss_mask_2: 0.3526  loss_dice_2: 2.707  loss_ce_3: 0.2565  loss_mask_3: 0.3518  loss_dice_3: 2.698  loss_ce_4: 0.2583  loss_mask_4: 0.3517  loss_dice_4: 2.702  loss_ce_5: 0.2489  loss_mask_5: 0.3515  loss_dice_5: 2.703  loss_ce_6: 0.2495  loss_mask_6: 0.3508  loss_dice_6: 2.69  loss_ce_7: 0.2409  loss_mask_7: 0.351  loss_dice_7: 2.694  loss_ce_8: 0.2436  loss_mask_8: 0.3507  loss_dice_8: 2.693  time: 1.5346  data_time: 0.0901  lr: 7.4323e-06  max_mem: 21478M
[01/17 15:25:47] d2.utils.events INFO:  eta: 1 day, 2:52:58  iter: 25299  total_loss: 34.25  loss_ce: 0.2518  loss_mask: 0.3489  loss_dice: 2.738  loss_ce_0: 0.5855  loss_mask_0: 0.3479  loss_dice_0: 2.847  loss_ce_1: 0.2902  loss_mask_1: 0.3489  loss_dice_1: 2.774  loss_ce_2: 0.283  loss_mask_2: 0.3486  loss_dice_2: 2.75  loss_ce_3: 0.2609  loss_mask_3: 0.3496  loss_dice_3: 2.736  loss_ce_4: 0.2674  loss_mask_4: 0.3498  loss_dice_4: 2.733  loss_ce_5: 0.2627  loss_mask_5: 0.3485  loss_dice_5: 2.746  loss_ce_6: 0.2601  loss_mask_6: 0.3488  loss_dice_6: 2.739  loss_ce_7: 0.2514  loss_mask_7: 0.3481  loss_dice_7: 2.73  loss_ce_8: 0.2433  loss_mask_8: 0.3491  loss_dice_8: 2.742  time: 1.5345  data_time: 0.0855  lr: 7.4302e-06  max_mem: 21478M
[01/17 15:26:17] d2.utils.events INFO:  eta: 1 day, 2:52:19  iter: 25319  total_loss: 34.35  loss_ce: 0.2356  loss_mask: 0.3529  loss_dice: 2.778  loss_ce_0: 0.5763  loss_mask_0: 0.3619  loss_dice_0: 2.876  loss_ce_1: 0.2791  loss_mask_1: 0.3622  loss_dice_1: 2.815  loss_ce_2: 0.2788  loss_mask_2: 0.3592  loss_dice_2: 2.793  loss_ce_3: 0.2479  loss_mask_3: 0.3532  loss_dice_3: 2.787  loss_ce_4: 0.2636  loss_mask_4: 0.3524  loss_dice_4: 2.782  loss_ce_5: 0.2408  loss_mask_5: 0.351  loss_dice_5: 2.795  loss_ce_6: 0.2418  loss_mask_6: 0.3527  loss_dice_6: 2.781  loss_ce_7: 0.235  loss_mask_7: 0.3513  loss_dice_7: 2.774  loss_ce_8: 0.2441  loss_mask_8: 0.3513  loss_dice_8: 2.783  time: 1.5345  data_time: 0.0922  lr: 7.4282e-06  max_mem: 21478M
[01/17 15:26:46] d2.utils.events INFO:  eta: 1 day, 2:51:28  iter: 25339  total_loss: 33.76  loss_ce: 0.2518  loss_mask: 0.3495  loss_dice: 2.709  loss_ce_0: 0.5513  loss_mask_0: 0.3595  loss_dice_0: 2.818  loss_ce_1: 0.2869  loss_mask_1: 0.3603  loss_dice_1: 2.754  loss_ce_2: 0.2648  loss_mask_2: 0.3571  loss_dice_2: 2.727  loss_ce_3: 0.2603  loss_mask_3: 0.3558  loss_dice_3: 2.718  loss_ce_4: 0.2566  loss_mask_4: 0.3531  loss_dice_4: 2.716  loss_ce_5: 0.2496  loss_mask_5: 0.3516  loss_dice_5: 2.713  loss_ce_6: 0.2431  loss_mask_6: 0.3519  loss_dice_6: 2.711  loss_ce_7: 0.2432  loss_mask_7: 0.3505  loss_dice_7: 2.708  loss_ce_8: 0.2348  loss_mask_8: 0.351  loss_dice_8: 2.715  time: 1.5345  data_time: 0.0828  lr: 7.4261e-06  max_mem: 21478M
[01/17 15:27:16] d2.utils.events INFO:  eta: 1 day, 2:51:20  iter: 25359  total_loss: 33.54  loss_ce: 0.244  loss_mask: 0.3483  loss_dice: 2.703  loss_ce_0: 0.5824  loss_mask_0: 0.3567  loss_dice_0: 2.814  loss_ce_1: 0.2957  loss_mask_1: 0.3597  loss_dice_1: 2.732  loss_ce_2: 0.2669  loss_mask_2: 0.3567  loss_dice_2: 2.721  loss_ce_3: 0.2651  loss_mask_3: 0.3523  loss_dice_3: 2.701  loss_ce_4: 0.2678  loss_mask_4: 0.347  loss_dice_4: 2.701  loss_ce_5: 0.2464  loss_mask_5: 0.3483  loss_dice_5: 2.704  loss_ce_6: 0.261  loss_mask_6: 0.3469  loss_dice_6: 2.701  loss_ce_7: 0.2426  loss_mask_7: 0.3469  loss_dice_7: 2.703  loss_ce_8: 0.2467  loss_mask_8: 0.3474  loss_dice_8: 2.699  time: 1.5344  data_time: 0.0747  lr: 7.424e-06  max_mem: 21478M
[01/17 15:27:46] d2.utils.events INFO:  eta: 1 day, 2:49:02  iter: 25379  total_loss: 33.63  loss_ce: 0.2699  loss_mask: 0.3487  loss_dice: 2.691  loss_ce_0: 0.5429  loss_mask_0: 0.3576  loss_dice_0: 2.799  loss_ce_1: 0.2892  loss_mask_1: 0.3584  loss_dice_1: 2.736  loss_ce_2: 0.2983  loss_mask_2: 0.3522  loss_dice_2: 2.713  loss_ce_3: 0.2769  loss_mask_3: 0.3475  loss_dice_3: 2.706  loss_ce_4: 0.273  loss_mask_4: 0.3472  loss_dice_4: 2.702  loss_ce_5: 0.2718  loss_mask_5: 0.349  loss_dice_5: 2.703  loss_ce_6: 0.2757  loss_mask_6: 0.3482  loss_dice_6: 2.694  loss_ce_7: 0.2704  loss_mask_7: 0.3472  loss_dice_7: 2.692  loss_ce_8: 0.2629  loss_mask_8: 0.3494  loss_dice_8: 2.685  time: 1.5344  data_time: 0.0850  lr: 7.422e-06  max_mem: 21478M
[01/17 15:28:15] d2.utils.events INFO:  eta: 1 day, 2:46:47  iter: 25399  total_loss: 33.48  loss_ce: 0.2796  loss_mask: 0.3472  loss_dice: 2.662  loss_ce_0: 0.602  loss_mask_0: 0.348  loss_dice_0: 2.765  loss_ce_1: 0.3217  loss_mask_1: 0.3468  loss_dice_1: 2.693  loss_ce_2: 0.3057  loss_mask_2: 0.347  loss_dice_2: 2.681  loss_ce_3: 0.2981  loss_mask_3: 0.3471  loss_dice_3: 2.671  loss_ce_4: 0.2787  loss_mask_4: 0.3475  loss_dice_4: 2.673  loss_ce_5: 0.2853  loss_mask_5: 0.3474  loss_dice_5: 2.672  loss_ce_6: 0.2731  loss_mask_6: 0.3463  loss_dice_6: 2.672  loss_ce_7: 0.2808  loss_mask_7: 0.3466  loss_dice_7: 2.668  loss_ce_8: 0.2736  loss_mask_8: 0.3467  loss_dice_8: 2.663  time: 1.5343  data_time: 0.0837  lr: 7.4199e-06  max_mem: 21478M
[01/17 15:28:45] d2.utils.events INFO:  eta: 1 day, 2:45:10  iter: 25419  total_loss: 33.46  loss_ce: 0.2707  loss_mask: 0.3596  loss_dice: 2.67  loss_ce_0: 0.6212  loss_mask_0: 0.3593  loss_dice_0: 2.782  loss_ce_1: 0.3145  loss_mask_1: 0.3592  loss_dice_1: 2.713  loss_ce_2: 0.2987  loss_mask_2: 0.3561  loss_dice_2: 2.688  loss_ce_3: 0.2811  loss_mask_3: 0.3575  loss_dice_3: 2.683  loss_ce_4: 0.2754  loss_mask_4: 0.3574  loss_dice_4: 2.68  loss_ce_5: 0.2801  loss_mask_5: 0.3574  loss_dice_5: 2.684  loss_ce_6: 0.2732  loss_mask_6: 0.3589  loss_dice_6: 2.678  loss_ce_7: 0.2687  loss_mask_7: 0.3589  loss_dice_7: 2.673  loss_ce_8: 0.2808  loss_mask_8: 0.3593  loss_dice_8: 2.678  time: 1.5343  data_time: 0.0916  lr: 7.4178e-06  max_mem: 21478M
[01/17 15:29:15] d2.utils.events INFO:  eta: 1 day, 2:44:06  iter: 25439  total_loss: 34.18  loss_ce: 0.2482  loss_mask: 0.3534  loss_dice: 2.729  loss_ce_0: 0.5447  loss_mask_0: 0.3645  loss_dice_0: 2.844  loss_ce_1: 0.2834  loss_mask_1: 0.3646  loss_dice_1: 2.774  loss_ce_2: 0.2849  loss_mask_2: 0.3579  loss_dice_2: 2.751  loss_ce_3: 0.2601  loss_mask_3: 0.3562  loss_dice_3: 2.74  loss_ce_4: 0.2627  loss_mask_4: 0.3569  loss_dice_4: 2.731  loss_ce_5: 0.2505  loss_mask_5: 0.3546  loss_dice_5: 2.725  loss_ce_6: 0.2538  loss_mask_6: 0.3542  loss_dice_6: 2.726  loss_ce_7: 0.2529  loss_mask_7: 0.3537  loss_dice_7: 2.735  loss_ce_8: 0.2431  loss_mask_8: 0.3541  loss_dice_8: 2.726  time: 1.5343  data_time: 0.0932  lr: 7.4157e-06  max_mem: 21478M
[01/17 15:29:45] d2.utils.events INFO:  eta: 1 day, 2:43:15  iter: 25459  total_loss: 33.24  loss_ce: 0.2243  loss_mask: 0.3497  loss_dice: 2.701  loss_ce_0: 0.5487  loss_mask_0: 0.3528  loss_dice_0: 2.803  loss_ce_1: 0.2811  loss_mask_1: 0.3533  loss_dice_1: 2.732  loss_ce_2: 0.2609  loss_mask_2: 0.3485  loss_dice_2: 2.709  loss_ce_3: 0.2484  loss_mask_3: 0.3485  loss_dice_3: 2.707  loss_ce_4: 0.2422  loss_mask_4: 0.3491  loss_dice_4: 2.697  loss_ce_5: 0.234  loss_mask_5: 0.3496  loss_dice_5: 2.701  loss_ce_6: 0.2329  loss_mask_6: 0.3497  loss_dice_6: 2.704  loss_ce_7: 0.2293  loss_mask_7: 0.3498  loss_dice_7: 2.705  loss_ce_8: 0.2402  loss_mask_8: 0.3497  loss_dice_8: 2.701  time: 1.5342  data_time: 0.0827  lr: 7.4137e-06  max_mem: 21478M
[01/17 15:30:14] d2.utils.events INFO:  eta: 1 day, 2:42:35  iter: 25479  total_loss: 34.04  loss_ce: 0.258  loss_mask: 0.3526  loss_dice: 2.743  loss_ce_0: 0.5963  loss_mask_0: 0.3527  loss_dice_0: 2.85  loss_ce_1: 0.2907  loss_mask_1: 0.3534  loss_dice_1: 2.787  loss_ce_2: 0.3004  loss_mask_2: 0.3507  loss_dice_2: 2.771  loss_ce_3: 0.2612  loss_mask_3: 0.3523  loss_dice_3: 2.753  loss_ce_4: 0.2797  loss_mask_4: 0.3504  loss_dice_4: 2.745  loss_ce_5: 0.2642  loss_mask_5: 0.3524  loss_dice_5: 2.751  loss_ce_6: 0.2667  loss_mask_6: 0.3508  loss_dice_6: 2.744  loss_ce_7: 0.2629  loss_mask_7: 0.3503  loss_dice_7: 2.741  loss_ce_8: 0.2629  loss_mask_8: 0.3516  loss_dice_8: 2.749  time: 1.5342  data_time: 0.0932  lr: 7.4116e-06  max_mem: 21478M
[01/17 15:30:44] d2.utils.events INFO:  eta: 1 day, 2:40:56  iter: 25499  total_loss: 33.07  loss_ce: 0.2505  loss_mask: 0.336  loss_dice: 2.667  loss_ce_0: 0.5489  loss_mask_0: 0.3375  loss_dice_0: 2.793  loss_ce_1: 0.2734  loss_mask_1: 0.3393  loss_dice_1: 2.717  loss_ce_2: 0.288  loss_mask_2: 0.3382  loss_dice_2: 2.677  loss_ce_3: 0.2656  loss_mask_3: 0.3363  loss_dice_3: 2.662  loss_ce_4: 0.2482  loss_mask_4: 0.3364  loss_dice_4: 2.682  loss_ce_5: 0.2487  loss_mask_5: 0.3368  loss_dice_5: 2.667  loss_ce_6: 0.2615  loss_mask_6: 0.3341  loss_dice_6: 2.657  loss_ce_7: 0.2549  loss_mask_7: 0.3344  loss_dice_7: 2.672  loss_ce_8: 0.2454  loss_mask_8: 0.3339  loss_dice_8: 2.66  time: 1.5342  data_time: 0.0871  lr: 7.4095e-06  max_mem: 21478M
[01/17 15:31:14] d2.utils.events INFO:  eta: 1 day, 2:41:02  iter: 25519  total_loss: 33.59  loss_ce: 0.243  loss_mask: 0.346  loss_dice: 2.713  loss_ce_0: 0.5369  loss_mask_0: 0.3486  loss_dice_0: 2.804  loss_ce_1: 0.2906  loss_mask_1: 0.3539  loss_dice_1: 2.746  loss_ce_2: 0.2704  loss_mask_2: 0.349  loss_dice_2: 2.722  loss_ce_3: 0.258  loss_mask_3: 0.3476  loss_dice_3: 2.714  loss_ce_4: 0.2634  loss_mask_4: 0.3472  loss_dice_4: 2.715  loss_ce_5: 0.2415  loss_mask_5: 0.348  loss_dice_5: 2.706  loss_ce_6: 0.2458  loss_mask_6: 0.3489  loss_dice_6: 2.714  loss_ce_7: 0.241  loss_mask_7: 0.3487  loss_dice_7: 2.703  loss_ce_8: 0.244  loss_mask_8: 0.3471  loss_dice_8: 2.713  time: 1.5341  data_time: 0.0931  lr: 7.4075e-06  max_mem: 21478M
[01/17 15:31:44] d2.utils.events INFO:  eta: 1 day, 2:39:20  iter: 25539  total_loss: 33.1  loss_ce: 0.2584  loss_mask: 0.3458  loss_dice: 2.622  loss_ce_0: 0.5785  loss_mask_0: 0.3499  loss_dice_0: 2.744  loss_ce_1: 0.2847  loss_mask_1: 0.3529  loss_dice_1: 2.662  loss_ce_2: 0.293  loss_mask_2: 0.3494  loss_dice_2: 2.632  loss_ce_3: 0.2845  loss_mask_3: 0.3465  loss_dice_3: 2.63  loss_ce_4: 0.2631  loss_mask_4: 0.3482  loss_dice_4: 2.632  loss_ce_5: 0.2682  loss_mask_5: 0.3489  loss_dice_5: 2.627  loss_ce_6: 0.258  loss_mask_6: 0.3481  loss_dice_6: 2.625  loss_ce_7: 0.2609  loss_mask_7: 0.3474  loss_dice_7: 2.624  loss_ce_8: 0.2422  loss_mask_8: 0.3464  loss_dice_8: 2.619  time: 1.5341  data_time: 0.0861  lr: 7.4054e-06  max_mem: 21478M
[01/17 15:32:14] d2.utils.events INFO:  eta: 1 day, 2:38:51  iter: 25559  total_loss: 33.41  loss_ce: 0.2337  loss_mask: 0.3535  loss_dice: 2.666  loss_ce_0: 0.5747  loss_mask_0: 0.3598  loss_dice_0: 2.775  loss_ce_1: 0.2839  loss_mask_1: 0.3617  loss_dice_1: 2.699  loss_ce_2: 0.2672  loss_mask_2: 0.3591  loss_dice_2: 2.687  loss_ce_3: 0.2629  loss_mask_3: 0.3542  loss_dice_3: 2.674  loss_ce_4: 0.2282  loss_mask_4: 0.3534  loss_dice_4: 2.667  loss_ce_5: 0.2471  loss_mask_5: 0.3547  loss_dice_5: 2.674  loss_ce_6: 0.2426  loss_mask_6: 0.3538  loss_dice_6: 2.673  loss_ce_7: 0.2259  loss_mask_7: 0.3554  loss_dice_7: 2.672  loss_ce_8: 0.2291  loss_mask_8: 0.3536  loss_dice_8: 2.671  time: 1.5341  data_time: 0.0919  lr: 7.4033e-06  max_mem: 21478M
[01/17 15:32:44] d2.utils.events INFO:  eta: 1 day, 2:38:21  iter: 25579  total_loss: 33.83  loss_ce: 0.2599  loss_mask: 0.3498  loss_dice: 2.701  loss_ce_0: 0.5657  loss_mask_0: 0.356  loss_dice_0: 2.794  loss_ce_1: 0.2847  loss_mask_1: 0.3558  loss_dice_1: 2.729  loss_ce_2: 0.2857  loss_mask_2: 0.3466  loss_dice_2: 2.717  loss_ce_3: 0.2899  loss_mask_3: 0.3449  loss_dice_3: 2.703  loss_ce_4: 0.2696  loss_mask_4: 0.3464  loss_dice_4: 2.713  loss_ce_5: 0.2646  loss_mask_5: 0.3463  loss_dice_5: 2.706  loss_ce_6: 0.2485  loss_mask_6: 0.3458  loss_dice_6: 2.707  loss_ce_7: 0.2491  loss_mask_7: 0.3454  loss_dice_7: 2.7  loss_ce_8: 0.2437  loss_mask_8: 0.3484  loss_dice_8: 2.707  time: 1.5340  data_time: 0.0820  lr: 7.4013e-06  max_mem: 21478M
[01/17 15:33:14] d2.utils.events INFO:  eta: 1 day, 2:36:58  iter: 25599  total_loss: 33.16  loss_ce: 0.2472  loss_mask: 0.3462  loss_dice: 2.656  loss_ce_0: 0.5547  loss_mask_0: 0.3514  loss_dice_0: 2.754  loss_ce_1: 0.2974  loss_mask_1: 0.3507  loss_dice_1: 2.684  loss_ce_2: 0.2781  loss_mask_2: 0.3476  loss_dice_2: 2.667  loss_ce_3: 0.2701  loss_mask_3: 0.3495  loss_dice_3: 2.654  loss_ce_4: 0.2737  loss_mask_4: 0.3473  loss_dice_4: 2.656  loss_ce_5: 0.2603  loss_mask_5: 0.3464  loss_dice_5: 2.65  loss_ce_6: 0.2427  loss_mask_6: 0.3474  loss_dice_6: 2.649  loss_ce_7: 0.2575  loss_mask_7: 0.3476  loss_dice_7: 2.653  loss_ce_8: 0.2415  loss_mask_8: 0.3467  loss_dice_8: 2.657  time: 1.5340  data_time: 0.0826  lr: 7.3992e-06  max_mem: 21478M
[01/17 15:33:44] d2.utils.events INFO:  eta: 1 day, 2:36:51  iter: 25619  total_loss: 34.01  loss_ce: 0.2662  loss_mask: 0.348  loss_dice: 2.719  loss_ce_0: 0.5663  loss_mask_0: 0.3505  loss_dice_0: 2.823  loss_ce_1: 0.3049  loss_mask_1: 0.3517  loss_dice_1: 2.753  loss_ce_2: 0.2898  loss_mask_2: 0.3504  loss_dice_2: 2.73  loss_ce_3: 0.2879  loss_mask_3: 0.3474  loss_dice_3: 2.722  loss_ce_4: 0.2731  loss_mask_4: 0.3477  loss_dice_4: 2.724  loss_ce_5: 0.2811  loss_mask_5: 0.3511  loss_dice_5: 2.718  loss_ce_6: 0.2635  loss_mask_6: 0.3502  loss_dice_6: 2.715  loss_ce_7: 0.2681  loss_mask_7: 0.348  loss_dice_7: 2.709  loss_ce_8: 0.2686  loss_mask_8: 0.3478  loss_dice_8: 2.705  time: 1.5340  data_time: 0.0889  lr: 7.3971e-06  max_mem: 21478M
[01/17 15:34:14] d2.utils.events INFO:  eta: 1 day, 2:36:56  iter: 25639  total_loss: 33.65  loss_ce: 0.2453  loss_mask: 0.3446  loss_dice: 2.727  loss_ce_0: 0.5504  loss_mask_0: 0.3458  loss_dice_0: 2.829  loss_ce_1: 0.2817  loss_mask_1: 0.3464  loss_dice_1: 2.764  loss_ce_2: 0.2842  loss_mask_2: 0.3459  loss_dice_2: 2.736  loss_ce_3: 0.2663  loss_mask_3: 0.3422  loss_dice_3: 2.726  loss_ce_4: 0.2647  loss_mask_4: 0.3415  loss_dice_4: 2.725  loss_ce_5: 0.2596  loss_mask_5: 0.3421  loss_dice_5: 2.717  loss_ce_6: 0.2606  loss_mask_6: 0.3425  loss_dice_6: 2.727  loss_ce_7: 0.2508  loss_mask_7: 0.3424  loss_dice_7: 2.713  loss_ce_8: 0.2464  loss_mask_8: 0.3449  loss_dice_8: 2.716  time: 1.5339  data_time: 0.0931  lr: 7.3951e-06  max_mem: 21478M
[01/17 15:34:43] d2.utils.events INFO:  eta: 1 day, 2:36:22  iter: 25659  total_loss: 33.61  loss_ce: 0.2512  loss_mask: 0.3508  loss_dice: 2.671  loss_ce_0: 0.5651  loss_mask_0: 0.3522  loss_dice_0: 2.769  loss_ce_1: 0.284  loss_mask_1: 0.3509  loss_dice_1: 2.713  loss_ce_2: 0.2873  loss_mask_2: 0.3485  loss_dice_2: 2.674  loss_ce_3: 0.2632  loss_mask_3: 0.3475  loss_dice_3: 2.661  loss_ce_4: 0.2524  loss_mask_4: 0.3507  loss_dice_4: 2.668  loss_ce_5: 0.2562  loss_mask_5: 0.3511  loss_dice_5: 2.676  loss_ce_6: 0.2636  loss_mask_6: 0.3513  loss_dice_6: 2.669  loss_ce_7: 0.2544  loss_mask_7: 0.3513  loss_dice_7: 2.669  loss_ce_8: 0.2544  loss_mask_8: 0.3511  loss_dice_8: 2.664  time: 1.5339  data_time: 0.0856  lr: 7.393e-06  max_mem: 21478M
[01/17 15:35:13] d2.utils.events INFO:  eta: 1 day, 2:35:57  iter: 25679  total_loss: 33.31  loss_ce: 0.2489  loss_mask: 0.3469  loss_dice: 2.686  loss_ce_0: 0.5729  loss_mask_0: 0.3516  loss_dice_0: 2.798  loss_ce_1: 0.2881  loss_mask_1: 0.3523  loss_dice_1: 2.733  loss_ce_2: 0.2614  loss_mask_2: 0.3478  loss_dice_2: 2.7  loss_ce_3: 0.2515  loss_mask_3: 0.3474  loss_dice_3: 2.7  loss_ce_4: 0.2532  loss_mask_4: 0.3461  loss_dice_4: 2.688  loss_ce_5: 0.2409  loss_mask_5: 0.3461  loss_dice_5: 2.698  loss_ce_6: 0.2476  loss_mask_6: 0.346  loss_dice_6: 2.691  loss_ce_7: 0.2378  loss_mask_7: 0.3457  loss_dice_7: 2.684  loss_ce_8: 0.2344  loss_mask_8: 0.3454  loss_dice_8: 2.684  time: 1.5339  data_time: 0.0869  lr: 7.3909e-06  max_mem: 21478M
[01/17 15:35:43] d2.utils.events INFO:  eta: 1 day, 2:35:35  iter: 25699  total_loss: 34.52  loss_ce: 0.2535  loss_mask: 0.3551  loss_dice: 2.785  loss_ce_0: 0.5773  loss_mask_0: 0.3554  loss_dice_0: 2.885  loss_ce_1: 0.2851  loss_mask_1: 0.3572  loss_dice_1: 2.823  loss_ce_2: 0.2952  loss_mask_2: 0.355  loss_dice_2: 2.803  loss_ce_3: 0.2633  loss_mask_3: 0.3498  loss_dice_3: 2.794  loss_ce_4: 0.2714  loss_mask_4: 0.3499  loss_dice_4: 2.798  loss_ce_5: 0.2684  loss_mask_5: 0.3508  loss_dice_5: 2.796  loss_ce_6: 0.2572  loss_mask_6: 0.353  loss_dice_6: 2.789  loss_ce_7: 0.2553  loss_mask_7: 0.3513  loss_dice_7: 2.781  loss_ce_8: 0.2593  loss_mask_8: 0.3528  loss_dice_8: 2.785  time: 1.5338  data_time: 0.0885  lr: 7.3889e-06  max_mem: 21478M
[01/17 15:36:13] d2.utils.events INFO:  eta: 1 day, 2:34:57  iter: 25719  total_loss: 33.83  loss_ce: 0.2414  loss_mask: 0.3566  loss_dice: 2.705  loss_ce_0: 0.5692  loss_mask_0: 0.3551  loss_dice_0: 2.803  loss_ce_1: 0.3074  loss_mask_1: 0.3603  loss_dice_1: 2.737  loss_ce_2: 0.2875  loss_mask_2: 0.3535  loss_dice_2: 2.724  loss_ce_3: 0.265  loss_mask_3: 0.3533  loss_dice_3: 2.714  loss_ce_4: 0.2669  loss_mask_4: 0.3549  loss_dice_4: 2.71  loss_ce_5: 0.2572  loss_mask_5: 0.355  loss_dice_5: 2.709  loss_ce_6: 0.2657  loss_mask_6: 0.3564  loss_dice_6: 2.706  loss_ce_7: 0.253  loss_mask_7: 0.3562  loss_dice_7: 2.702  loss_ce_8: 0.2518  loss_mask_8: 0.3552  loss_dice_8: 2.704  time: 1.5338  data_time: 0.0857  lr: 7.3868e-06  max_mem: 21478M
[01/17 15:36:43] d2.utils.events INFO:  eta: 1 day, 2:35:34  iter: 25739  total_loss: 33.88  loss_ce: 0.2485  loss_mask: 0.3443  loss_dice: 2.71  loss_ce_0: 0.5749  loss_mask_0: 0.3498  loss_dice_0: 2.829  loss_ce_1: 0.2834  loss_mask_1: 0.3501  loss_dice_1: 2.75  loss_ce_2: 0.272  loss_mask_2: 0.3496  loss_dice_2: 2.732  loss_ce_3: 0.2725  loss_mask_3: 0.3457  loss_dice_3: 2.722  loss_ce_4: 0.2486  loss_mask_4: 0.3474  loss_dice_4: 2.717  loss_ce_5: 0.2646  loss_mask_5: 0.3454  loss_dice_5: 2.718  loss_ce_6: 0.2493  loss_mask_6: 0.3429  loss_dice_6: 2.721  loss_ce_7: 0.2484  loss_mask_7: 0.343  loss_dice_7: 2.717  loss_ce_8: 0.2476  loss_mask_8: 0.344  loss_dice_8: 2.721  time: 1.5338  data_time: 0.0959  lr: 7.3847e-06  max_mem: 21478M
[01/17 15:37:12] d2.utils.events INFO:  eta: 1 day, 2:34:06  iter: 25759  total_loss: 33.11  loss_ce: 0.2382  loss_mask: 0.3579  loss_dice: 2.679  loss_ce_0: 0.5489  loss_mask_0: 0.3613  loss_dice_0: 2.792  loss_ce_1: 0.2689  loss_mask_1: 0.3648  loss_dice_1: 2.717  loss_ce_2: 0.2623  loss_mask_2: 0.3598  loss_dice_2: 2.692  loss_ce_3: 0.2509  loss_mask_3: 0.358  loss_dice_3: 2.694  loss_ce_4: 0.2423  loss_mask_4: 0.3577  loss_dice_4: 2.68  loss_ce_5: 0.2454  loss_mask_5: 0.357  loss_dice_5: 2.688  loss_ce_6: 0.2338  loss_mask_6: 0.3566  loss_dice_6: 2.674  loss_ce_7: 0.2397  loss_mask_7: 0.3571  loss_dice_7: 2.675  loss_ce_8: 0.2361  loss_mask_8: 0.3581  loss_dice_8: 2.674  time: 1.5337  data_time: 0.0883  lr: 7.3827e-06  max_mem: 21478M
[01/17 15:37:43] d2.utils.events INFO:  eta: 1 day, 2:35:12  iter: 25779  total_loss: 34.55  loss_ce: 0.237  loss_mask: 0.348  loss_dice: 2.732  loss_ce_0: 0.5775  loss_mask_0: 0.3513  loss_dice_0: 2.842  loss_ce_1: 0.2701  loss_mask_1: 0.3552  loss_dice_1: 2.778  loss_ce_2: 0.2604  loss_mask_2: 0.3523  loss_dice_2: 2.748  loss_ce_3: 0.2391  loss_mask_3: 0.3522  loss_dice_3: 2.75  loss_ce_4: 0.244  loss_mask_4: 0.3505  loss_dice_4: 2.744  loss_ce_5: 0.2226  loss_mask_5: 0.3498  loss_dice_5: 2.742  loss_ce_6: 0.2272  loss_mask_6: 0.3495  loss_dice_6: 2.737  loss_ce_7: 0.2258  loss_mask_7: 0.3489  loss_dice_7: 2.739  loss_ce_8: 0.236  loss_mask_8: 0.3492  loss_dice_8: 2.735  time: 1.5337  data_time: 0.0927  lr: 7.3806e-06  max_mem: 21478M
[01/17 15:38:13] d2.utils.events INFO:  eta: 1 day, 2:34:05  iter: 25799  total_loss: 33.57  loss_ce: 0.2415  loss_mask: 0.3503  loss_dice: 2.686  loss_ce_0: 0.5575  loss_mask_0: 0.3614  loss_dice_0: 2.789  loss_ce_1: 0.274  loss_mask_1: 0.3642  loss_dice_1: 2.714  loss_ce_2: 0.2703  loss_mask_2: 0.3556  loss_dice_2: 2.7  loss_ce_3: 0.2485  loss_mask_3: 0.3522  loss_dice_3: 2.688  loss_ce_4: 0.2672  loss_mask_4: 0.3523  loss_dice_4: 2.689  loss_ce_5: 0.2468  loss_mask_5: 0.3534  loss_dice_5: 2.692  loss_ce_6: 0.2474  loss_mask_6: 0.352  loss_dice_6: 2.684  loss_ce_7: 0.2267  loss_mask_7: 0.3513  loss_dice_7: 2.687  loss_ce_8: 0.2294  loss_mask_8: 0.3507  loss_dice_8: 2.687  time: 1.5337  data_time: 0.0880  lr: 7.3785e-06  max_mem: 21478M
[01/17 15:38:43] d2.utils.events INFO:  eta: 1 day, 2:33:13  iter: 25819  total_loss: 33.69  loss_ce: 0.2619  loss_mask: 0.3448  loss_dice: 2.708  loss_ce_0: 0.5788  loss_mask_0: 0.3484  loss_dice_0: 2.827  loss_ce_1: 0.2919  loss_mask_1: 0.3485  loss_dice_1: 2.756  loss_ce_2: 0.2905  loss_mask_2: 0.3435  loss_dice_2: 2.735  loss_ce_3: 0.257  loss_mask_3: 0.3446  loss_dice_3: 2.709  loss_ce_4: 0.2596  loss_mask_4: 0.3446  loss_dice_4: 2.708  loss_ce_5: 0.2532  loss_mask_5: 0.3446  loss_dice_5: 2.705  loss_ce_6: 0.2475  loss_mask_6: 0.3435  loss_dice_6: 2.702  loss_ce_7: 0.2512  loss_mask_7: 0.3431  loss_dice_7: 2.71  loss_ce_8: 0.2557  loss_mask_8: 0.3434  loss_dice_8: 2.707  time: 1.5337  data_time: 0.0918  lr: 7.3765e-06  max_mem: 21478M
[01/17 15:39:13] d2.utils.events INFO:  eta: 1 day, 2:33:05  iter: 25839  total_loss: 34.1  loss_ce: 0.2312  loss_mask: 0.3408  loss_dice: 2.735  loss_ce_0: 0.5349  loss_mask_0: 0.3471  loss_dice_0: 2.852  loss_ce_1: 0.2743  loss_mask_1: 0.3415  loss_dice_1: 2.772  loss_ce_2: 0.2764  loss_mask_2: 0.3403  loss_dice_2: 2.752  loss_ce_3: 0.2552  loss_mask_3: 0.3409  loss_dice_3: 2.754  loss_ce_4: 0.2517  loss_mask_4: 0.3408  loss_dice_4: 2.731  loss_ce_5: 0.2601  loss_mask_5: 0.3422  loss_dice_5: 2.74  loss_ce_6: 0.2447  loss_mask_6: 0.3402  loss_dice_6: 2.747  loss_ce_7: 0.2537  loss_mask_7: 0.3425  loss_dice_7: 2.735  loss_ce_8: 0.2448  loss_mask_8: 0.3415  loss_dice_8: 2.741  time: 1.5336  data_time: 0.0909  lr: 7.3744e-06  max_mem: 21478M
[01/17 15:39:43] d2.utils.events INFO:  eta: 1 day, 2:32:40  iter: 25859  total_loss: 34.22  loss_ce: 0.277  loss_mask: 0.3471  loss_dice: 2.7  loss_ce_0: 0.564  loss_mask_0: 0.3443  loss_dice_0: 2.805  loss_ce_1: 0.3106  loss_mask_1: 0.3499  loss_dice_1: 2.736  loss_ce_2: 0.3049  loss_mask_2: 0.3492  loss_dice_2: 2.722  loss_ce_3: 0.2887  loss_mask_3: 0.3488  loss_dice_3: 2.697  loss_ce_4: 0.271  loss_mask_4: 0.349  loss_dice_4: 2.708  loss_ce_5: 0.275  loss_mask_5: 0.3482  loss_dice_5: 2.708  loss_ce_6: 0.2663  loss_mask_6: 0.3469  loss_dice_6: 2.702  loss_ce_7: 0.2662  loss_mask_7: 0.3487  loss_dice_7: 2.711  loss_ce_8: 0.2697  loss_mask_8: 0.3478  loss_dice_8: 2.706  time: 1.5336  data_time: 0.0886  lr: 7.3723e-06  max_mem: 21478M
[01/17 15:40:13] d2.utils.events INFO:  eta: 1 day, 2:32:29  iter: 25879  total_loss: 33.55  loss_ce: 0.2592  loss_mask: 0.3468  loss_dice: 2.687  loss_ce_0: 0.5685  loss_mask_0: 0.3499  loss_dice_0: 2.8  loss_ce_1: 0.3039  loss_mask_1: 0.356  loss_dice_1: 2.727  loss_ce_2: 0.2882  loss_mask_2: 0.3523  loss_dice_2: 2.699  loss_ce_3: 0.276  loss_mask_3: 0.3464  loss_dice_3: 2.693  loss_ce_4: 0.2735  loss_mask_4: 0.3451  loss_dice_4: 2.689  loss_ce_5: 0.2699  loss_mask_5: 0.3454  loss_dice_5: 2.696  loss_ce_6: 0.2578  loss_mask_6: 0.3485  loss_dice_6: 2.686  loss_ce_7: 0.2565  loss_mask_7: 0.3475  loss_dice_7: 2.692  loss_ce_8: 0.2564  loss_mask_8: 0.3472  loss_dice_8: 2.678  time: 1.5336  data_time: 0.0896  lr: 7.3702e-06  max_mem: 21478M
[01/17 15:40:42] d2.utils.events INFO:  eta: 1 day, 2:30:40  iter: 25899  total_loss: 33.68  loss_ce: 0.2485  loss_mask: 0.358  loss_dice: 2.717  loss_ce_0: 0.5802  loss_mask_0: 0.3708  loss_dice_0: 2.815  loss_ce_1: 0.2996  loss_mask_1: 0.3658  loss_dice_1: 2.746  loss_ce_2: 0.2758  loss_mask_2: 0.3623  loss_dice_2: 2.719  loss_ce_3: 0.2639  loss_mask_3: 0.3598  loss_dice_3: 2.716  loss_ce_4: 0.2747  loss_mask_4: 0.3585  loss_dice_4: 2.706  loss_ce_5: 0.2662  loss_mask_5: 0.3593  loss_dice_5: 2.711  loss_ce_6: 0.2609  loss_mask_6: 0.3587  loss_dice_6: 2.709  loss_ce_7: 0.2524  loss_mask_7: 0.3572  loss_dice_7: 2.716  loss_ce_8: 0.2537  loss_mask_8: 0.3565  loss_dice_8: 2.722  time: 1.5335  data_time: 0.0844  lr: 7.3682e-06  max_mem: 21478M
[01/17 15:41:12] d2.utils.events INFO:  eta: 1 day, 2:30:44  iter: 25919  total_loss: 33.42  loss_ce: 0.2418  loss_mask: 0.3492  loss_dice: 2.671  loss_ce_0: 0.5404  loss_mask_0: 0.3544  loss_dice_0: 2.799  loss_ce_1: 0.2793  loss_mask_1: 0.3549  loss_dice_1: 2.721  loss_ce_2: 0.2705  loss_mask_2: 0.3517  loss_dice_2: 2.694  loss_ce_3: 0.2732  loss_mask_3: 0.3512  loss_dice_3: 2.685  loss_ce_4: 0.2552  loss_mask_4: 0.3489  loss_dice_4: 2.692  loss_ce_5: 0.256  loss_mask_5: 0.3487  loss_dice_5: 2.686  loss_ce_6: 0.2552  loss_mask_6: 0.3482  loss_dice_6: 2.676  loss_ce_7: 0.2412  loss_mask_7: 0.3495  loss_dice_7: 2.679  loss_ce_8: 0.2418  loss_mask_8: 0.3483  loss_dice_8: 2.672  time: 1.5335  data_time: 0.0883  lr: 7.3661e-06  max_mem: 21478M
[01/17 15:41:42] d2.utils.events INFO:  eta: 1 day, 2:30:29  iter: 25939  total_loss: 33.94  loss_ce: 0.2475  loss_mask: 0.3467  loss_dice: 2.72  loss_ce_0: 0.6004  loss_mask_0: 0.3505  loss_dice_0: 2.836  loss_ce_1: 0.3085  loss_mask_1: 0.351  loss_dice_1: 2.759  loss_ce_2: 0.2861  loss_mask_2: 0.3465  loss_dice_2: 2.739  loss_ce_3: 0.2592  loss_mask_3: 0.3482  loss_dice_3: 2.732  loss_ce_4: 0.2707  loss_mask_4: 0.3458  loss_dice_4: 2.731  loss_ce_5: 0.2523  loss_mask_5: 0.3472  loss_dice_5: 2.735  loss_ce_6: 0.2526  loss_mask_6: 0.3487  loss_dice_6: 2.724  loss_ce_7: 0.2533  loss_mask_7: 0.3496  loss_dice_7: 2.721  loss_ce_8: 0.2463  loss_mask_8: 0.3477  loss_dice_8: 2.724  time: 1.5335  data_time: 0.0923  lr: 7.364e-06  max_mem: 21478M
[01/17 15:42:12] d2.utils.events INFO:  eta: 1 day, 2:31:05  iter: 25959  total_loss: 33.39  loss_ce: 0.26  loss_mask: 0.3441  loss_dice: 2.677  loss_ce_0: 0.5958  loss_mask_0: 0.3439  loss_dice_0: 2.786  loss_ce_1: 0.3341  loss_mask_1: 0.3475  loss_dice_1: 2.711  loss_ce_2: 0.2995  loss_mask_2: 0.3457  loss_dice_2: 2.691  loss_ce_3: 0.2869  loss_mask_3: 0.3491  loss_dice_3: 2.678  loss_ce_4: 0.2809  loss_mask_4: 0.348  loss_dice_4: 2.674  loss_ce_5: 0.2639  loss_mask_5: 0.3491  loss_dice_5: 2.685  loss_ce_6: 0.2671  loss_mask_6: 0.347  loss_dice_6: 2.673  loss_ce_7: 0.258  loss_mask_7: 0.3469  loss_dice_7: 2.676  loss_ce_8: 0.2601  loss_mask_8: 0.3447  loss_dice_8: 2.663  time: 1.5334  data_time: 0.0846  lr: 7.362e-06  max_mem: 21478M
[01/17 15:42:42] d2.utils.events INFO:  eta: 1 day, 2:30:35  iter: 25979  total_loss: 33.34  loss_ce: 0.2471  loss_mask: 0.3448  loss_dice: 2.668  loss_ce_0: 0.5724  loss_mask_0: 0.3476  loss_dice_0: 2.767  loss_ce_1: 0.2944  loss_mask_1: 0.3504  loss_dice_1: 2.697  loss_ce_2: 0.2672  loss_mask_2: 0.3441  loss_dice_2: 2.691  loss_ce_3: 0.2471  loss_mask_3: 0.3434  loss_dice_3: 2.668  loss_ce_4: 0.2597  loss_mask_4: 0.3397  loss_dice_4: 2.67  loss_ce_5: 0.2503  loss_mask_5: 0.3416  loss_dice_5: 2.675  loss_ce_6: 0.2481  loss_mask_6: 0.342  loss_dice_6: 2.675  loss_ce_7: 0.249  loss_mask_7: 0.3425  loss_dice_7: 2.669  loss_ce_8: 0.2524  loss_mask_8: 0.3417  loss_dice_8: 2.664  time: 1.5334  data_time: 0.0840  lr: 7.3599e-06  max_mem: 21478M
[01/17 15:43:13] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 15:43:14] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 15:43:14] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 15:43:14] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 15:43:29] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0070 s/iter. Inference: 0.1469 s/iter. Eval: 0.2091 s/iter. Total: 0.3631 s/iter. ETA=0:06:32
[01/17 15:43:35] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0102 s/iter. Inference: 0.1546 s/iter. Eval: 0.2319 s/iter. Total: 0.3969 s/iter. ETA=0:07:04
[01/17 15:43:40] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0111 s/iter. Inference: 0.1558 s/iter. Eval: 0.2181 s/iter. Total: 0.3851 s/iter. ETA=0:06:46
[01/17 15:43:45] d2.evaluation.evaluator INFO: Inference done 51/1093. Dataloading: 0.0122 s/iter. Inference: 0.1559 s/iter. Eval: 0.2203 s/iter. Total: 0.3885 s/iter. ETA=0:06:44
[01/17 15:43:50] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0127 s/iter. Inference: 0.1579 s/iter. Eval: 0.2315 s/iter. Total: 0.4022 s/iter. ETA=0:06:54
[01/17 15:43:55] d2.evaluation.evaluator INFO: Inference done 74/1093. Dataloading: 0.0161 s/iter. Inference: 0.1589 s/iter. Eval: 0.2327 s/iter. Total: 0.4078 s/iter. ETA=0:06:55
[01/17 15:44:00] d2.evaluation.evaluator INFO: Inference done 86/1093. Dataloading: 0.0157 s/iter. Inference: 0.1602 s/iter. Eval: 0.2357 s/iter. Total: 0.4117 s/iter. ETA=0:06:54
[01/17 15:44:05] d2.evaluation.evaluator INFO: Inference done 98/1093. Dataloading: 0.0155 s/iter. Inference: 0.1609 s/iter. Eval: 0.2365 s/iter. Total: 0.4130 s/iter. ETA=0:06:50
[01/17 15:44:11] d2.evaluation.evaluator INFO: Inference done 113/1093. Dataloading: 0.0149 s/iter. Inference: 0.1596 s/iter. Eval: 0.2303 s/iter. Total: 0.4049 s/iter. ETA=0:06:36
[01/17 15:44:16] d2.evaluation.evaluator INFO: Inference done 125/1093. Dataloading: 0.0148 s/iter. Inference: 0.1598 s/iter. Eval: 0.2315 s/iter. Total: 0.4062 s/iter. ETA=0:06:33
[01/17 15:44:21] d2.evaluation.evaluator INFO: Inference done 137/1093. Dataloading: 0.0147 s/iter. Inference: 0.1622 s/iter. Eval: 0.2320 s/iter. Total: 0.4090 s/iter. ETA=0:06:31
[01/17 15:44:26] d2.evaluation.evaluator INFO: Inference done 151/1093. Dataloading: 0.0145 s/iter. Inference: 0.1624 s/iter. Eval: 0.2274 s/iter. Total: 0.4044 s/iter. ETA=0:06:20
[01/17 15:44:31] d2.evaluation.evaluator INFO: Inference done 163/1093. Dataloading: 0.0145 s/iter. Inference: 0.1635 s/iter. Eval: 0.2279 s/iter. Total: 0.4061 s/iter. ETA=0:06:17
[01/17 15:44:36] d2.evaluation.evaluator INFO: Inference done 175/1093. Dataloading: 0.0144 s/iter. Inference: 0.1648 s/iter. Eval: 0.2280 s/iter. Total: 0.4073 s/iter. ETA=0:06:13
[01/17 15:44:42] d2.evaluation.evaluator INFO: Inference done 189/1093. Dataloading: 0.0142 s/iter. Inference: 0.1650 s/iter. Eval: 0.2264 s/iter. Total: 0.4057 s/iter. ETA=0:06:06
[01/17 15:44:47] d2.evaluation.evaluator INFO: Inference done 203/1093. Dataloading: 0.0142 s/iter. Inference: 0.1644 s/iter. Eval: 0.2256 s/iter. Total: 0.4044 s/iter. ETA=0:05:59
[01/17 15:44:52] d2.evaluation.evaluator INFO: Inference done 215/1093. Dataloading: 0.0142 s/iter. Inference: 0.1646 s/iter. Eval: 0.2271 s/iter. Total: 0.4060 s/iter. ETA=0:05:56
[01/17 15:44:57] d2.evaluation.evaluator INFO: Inference done 228/1093. Dataloading: 0.0141 s/iter. Inference: 0.1642 s/iter. Eval: 0.2272 s/iter. Total: 0.4056 s/iter. ETA=0:05:50
[01/17 15:45:03] d2.evaluation.evaluator INFO: Inference done 241/1093. Dataloading: 0.0140 s/iter. Inference: 0.1642 s/iter. Eval: 0.2275 s/iter. Total: 0.4059 s/iter. ETA=0:05:45
[01/17 15:45:08] d2.evaluation.evaluator INFO: Inference done 254/1093. Dataloading: 0.0141 s/iter. Inference: 0.1640 s/iter. Eval: 0.2274 s/iter. Total: 0.4056 s/iter. ETA=0:05:40
[01/17 15:45:13] d2.evaluation.evaluator INFO: Inference done 266/1093. Dataloading: 0.0140 s/iter. Inference: 0.1650 s/iter. Eval: 0.2279 s/iter. Total: 0.4070 s/iter. ETA=0:05:36
[01/17 15:45:18] d2.evaluation.evaluator INFO: Inference done 279/1093. Dataloading: 0.0140 s/iter. Inference: 0.1648 s/iter. Eval: 0.2277 s/iter. Total: 0.4066 s/iter. ETA=0:05:30
[01/17 15:45:24] d2.evaluation.evaluator INFO: Inference done 292/1093. Dataloading: 0.0139 s/iter. Inference: 0.1655 s/iter. Eval: 0.2267 s/iter. Total: 0.4061 s/iter. ETA=0:05:25
[01/17 15:45:29] d2.evaluation.evaluator INFO: Inference done 304/1093. Dataloading: 0.0140 s/iter. Inference: 0.1660 s/iter. Eval: 0.2284 s/iter. Total: 0.4084 s/iter. ETA=0:05:22
[01/17 15:45:35] d2.evaluation.evaluator INFO: Inference done 316/1093. Dataloading: 0.0140 s/iter. Inference: 0.1659 s/iter. Eval: 0.2301 s/iter. Total: 0.4101 s/iter. ETA=0:05:18
[01/17 15:45:40] d2.evaluation.evaluator INFO: Inference done 329/1093. Dataloading: 0.0140 s/iter. Inference: 0.1660 s/iter. Eval: 0.2301 s/iter. Total: 0.4102 s/iter. ETA=0:05:13
[01/17 15:45:45] d2.evaluation.evaluator INFO: Inference done 344/1093. Dataloading: 0.0138 s/iter. Inference: 0.1660 s/iter. Eval: 0.2271 s/iter. Total: 0.4070 s/iter. ETA=0:05:04
[01/17 15:45:50] d2.evaluation.evaluator INFO: Inference done 358/1093. Dataloading: 0.0137 s/iter. Inference: 0.1659 s/iter. Eval: 0.2260 s/iter. Total: 0.4057 s/iter. ETA=0:04:58
[01/17 15:45:56] d2.evaluation.evaluator INFO: Inference done 371/1093. Dataloading: 0.0137 s/iter. Inference: 0.1655 s/iter. Eval: 0.2264 s/iter. Total: 0.4057 s/iter. ETA=0:04:52
[01/17 15:46:01] d2.evaluation.evaluator INFO: Inference done 386/1093. Dataloading: 0.0136 s/iter. Inference: 0.1650 s/iter. Eval: 0.2250 s/iter. Total: 0.4037 s/iter. ETA=0:04:45
[01/17 15:46:06] d2.evaluation.evaluator INFO: Inference done 399/1093. Dataloading: 0.0136 s/iter. Inference: 0.1647 s/iter. Eval: 0.2247 s/iter. Total: 0.4031 s/iter. ETA=0:04:39
[01/17 15:46:11] d2.evaluation.evaluator INFO: Inference done 412/1093. Dataloading: 0.0135 s/iter. Inference: 0.1647 s/iter. Eval: 0.2250 s/iter. Total: 0.4032 s/iter. ETA=0:04:34
[01/17 15:46:17] d2.evaluation.evaluator INFO: Inference done 424/1093. Dataloading: 0.0135 s/iter. Inference: 0.1648 s/iter. Eval: 0.2265 s/iter. Total: 0.4050 s/iter. ETA=0:04:30
[01/17 15:46:22] d2.evaluation.evaluator INFO: Inference done 437/1093. Dataloading: 0.0135 s/iter. Inference: 0.1651 s/iter. Eval: 0.2261 s/iter. Total: 0.4048 s/iter. ETA=0:04:25
[01/17 15:46:27] d2.evaluation.evaluator INFO: Inference done 451/1093. Dataloading: 0.0135 s/iter. Inference: 0.1651 s/iter. Eval: 0.2248 s/iter. Total: 0.4036 s/iter. ETA=0:04:19
[01/17 15:46:32] d2.evaluation.evaluator INFO: Inference done 462/1093. Dataloading: 0.0134 s/iter. Inference: 0.1657 s/iter. Eval: 0.2260 s/iter. Total: 0.4052 s/iter. ETA=0:04:15
[01/17 15:46:38] d2.evaluation.evaluator INFO: Inference done 476/1093. Dataloading: 0.0134 s/iter. Inference: 0.1663 s/iter. Eval: 0.2249 s/iter. Total: 0.4046 s/iter. ETA=0:04:09
[01/17 15:46:43] d2.evaluation.evaluator INFO: Inference done 488/1093. Dataloading: 0.0134 s/iter. Inference: 0.1665 s/iter. Eval: 0.2253 s/iter. Total: 0.4053 s/iter. ETA=0:04:05
[01/17 15:46:48] d2.evaluation.evaluator INFO: Inference done 502/1093. Dataloading: 0.0133 s/iter. Inference: 0.1664 s/iter. Eval: 0.2244 s/iter. Total: 0.4042 s/iter. ETA=0:03:58
[01/17 15:46:53] d2.evaluation.evaluator INFO: Inference done 516/1093. Dataloading: 0.0133 s/iter. Inference: 0.1667 s/iter. Eval: 0.2237 s/iter. Total: 0.4038 s/iter. ETA=0:03:52
[01/17 15:46:59] d2.evaluation.evaluator INFO: Inference done 527/1093. Dataloading: 0.0134 s/iter. Inference: 0.1668 s/iter. Eval: 0.2250 s/iter. Total: 0.4052 s/iter. ETA=0:03:49
[01/17 15:47:04] d2.evaluation.evaluator INFO: Inference done 541/1093. Dataloading: 0.0133 s/iter. Inference: 0.1663 s/iter. Eval: 0.2246 s/iter. Total: 0.4043 s/iter. ETA=0:03:43
[01/17 15:47:09] d2.evaluation.evaluator INFO: Inference done 551/1093. Dataloading: 0.0134 s/iter. Inference: 0.1667 s/iter. Eval: 0.2260 s/iter. Total: 0.4062 s/iter. ETA=0:03:40
[01/17 15:47:14] d2.evaluation.evaluator INFO: Inference done 564/1093. Dataloading: 0.0133 s/iter. Inference: 0.1670 s/iter. Eval: 0.2258 s/iter. Total: 0.4063 s/iter. ETA=0:03:34
[01/17 15:47:19] d2.evaluation.evaluator INFO: Inference done 577/1093. Dataloading: 0.0134 s/iter. Inference: 0.1681 s/iter. Eval: 0.2245 s/iter. Total: 0.4060 s/iter. ETA=0:03:29
[01/17 15:47:25] d2.evaluation.evaluator INFO: Inference done 590/1093. Dataloading: 0.0134 s/iter. Inference: 0.1695 s/iter. Eval: 0.2232 s/iter. Total: 0.4062 s/iter. ETA=0:03:24
[01/17 15:47:30] d2.evaluation.evaluator INFO: Inference done 603/1093. Dataloading: 0.0134 s/iter. Inference: 0.1689 s/iter. Eval: 0.2237 s/iter. Total: 0.4061 s/iter. ETA=0:03:18
[01/17 15:47:35] d2.evaluation.evaluator INFO: Inference done 618/1093. Dataloading: 0.0134 s/iter. Inference: 0.1686 s/iter. Eval: 0.2228 s/iter. Total: 0.4049 s/iter. ETA=0:03:12
[01/17 15:47:40] d2.evaluation.evaluator INFO: Inference done 633/1093. Dataloading: 0.0133 s/iter. Inference: 0.1680 s/iter. Eval: 0.2221 s/iter. Total: 0.4036 s/iter. ETA=0:03:05
[01/17 15:47:46] d2.evaluation.evaluator INFO: Inference done 648/1093. Dataloading: 0.0133 s/iter. Inference: 0.1676 s/iter. Eval: 0.2213 s/iter. Total: 0.4023 s/iter. ETA=0:02:59
[01/17 15:47:51] d2.evaluation.evaluator INFO: Inference done 661/1093. Dataloading: 0.0133 s/iter. Inference: 0.1679 s/iter. Eval: 0.2213 s/iter. Total: 0.4026 s/iter. ETA=0:02:53
[01/17 15:47:56] d2.evaluation.evaluator INFO: Inference done 674/1093. Dataloading: 0.0133 s/iter. Inference: 0.1676 s/iter. Eval: 0.2214 s/iter. Total: 0.4023 s/iter. ETA=0:02:48
[01/17 15:48:01] d2.evaluation.evaluator INFO: Inference done 688/1093. Dataloading: 0.0136 s/iter. Inference: 0.1677 s/iter. Eval: 0.2203 s/iter. Total: 0.4017 s/iter. ETA=0:02:42
[01/17 15:48:07] d2.evaluation.evaluator INFO: Inference done 701/1093. Dataloading: 0.0136 s/iter. Inference: 0.1675 s/iter. Eval: 0.2205 s/iter. Total: 0.4017 s/iter. ETA=0:02:37
[01/17 15:48:12] d2.evaluation.evaluator INFO: Inference done 712/1093. Dataloading: 0.0136 s/iter. Inference: 0.1679 s/iter. Eval: 0.2217 s/iter. Total: 0.4033 s/iter. ETA=0:02:33
[01/17 15:48:17] d2.evaluation.evaluator INFO: Inference done 724/1093. Dataloading: 0.0136 s/iter. Inference: 0.1682 s/iter. Eval: 0.2221 s/iter. Total: 0.4040 s/iter. ETA=0:02:29
[01/17 15:48:23] d2.evaluation.evaluator INFO: Inference done 737/1093. Dataloading: 0.0136 s/iter. Inference: 0.1685 s/iter. Eval: 0.2216 s/iter. Total: 0.4037 s/iter. ETA=0:02:23
[01/17 15:48:28] d2.evaluation.evaluator INFO: Inference done 751/1093. Dataloading: 0.0135 s/iter. Inference: 0.1686 s/iter. Eval: 0.2211 s/iter. Total: 0.4033 s/iter. ETA=0:02:17
[01/17 15:48:33] d2.evaluation.evaluator INFO: Inference done 764/1093. Dataloading: 0.0135 s/iter. Inference: 0.1684 s/iter. Eval: 0.2214 s/iter. Total: 0.4034 s/iter. ETA=0:02:12
[01/17 15:48:38] d2.evaluation.evaluator INFO: Inference done 777/1093. Dataloading: 0.0135 s/iter. Inference: 0.1683 s/iter. Eval: 0.2213 s/iter. Total: 0.4032 s/iter. ETA=0:02:07
[01/17 15:48:44] d2.evaluation.evaluator INFO: Inference done 792/1093. Dataloading: 0.0135 s/iter. Inference: 0.1681 s/iter. Eval: 0.2207 s/iter. Total: 0.4024 s/iter. ETA=0:02:01
[01/17 15:48:49] d2.evaluation.evaluator INFO: Inference done 806/1093. Dataloading: 0.0135 s/iter. Inference: 0.1681 s/iter. Eval: 0.2202 s/iter. Total: 0.4018 s/iter. ETA=0:01:55
[01/17 15:48:54] d2.evaluation.evaluator INFO: Inference done 819/1093. Dataloading: 0.0134 s/iter. Inference: 0.1682 s/iter. Eval: 0.2198 s/iter. Total: 0.4016 s/iter. ETA=0:01:50
[01/17 15:48:59] d2.evaluation.evaluator INFO: Inference done 834/1093. Dataloading: 0.0134 s/iter. Inference: 0.1681 s/iter. Eval: 0.2192 s/iter. Total: 0.4008 s/iter. ETA=0:01:43
[01/17 15:49:04] d2.evaluation.evaluator INFO: Inference done 848/1093. Dataloading: 0.0133 s/iter. Inference: 0.1680 s/iter. Eval: 0.2186 s/iter. Total: 0.4001 s/iter. ETA=0:01:38
[01/17 15:49:10] d2.evaluation.evaluator INFO: Inference done 859/1093. Dataloading: 0.0134 s/iter. Inference: 0.1681 s/iter. Eval: 0.2197 s/iter. Total: 0.4013 s/iter. ETA=0:01:33
[01/17 15:49:15] d2.evaluation.evaluator INFO: Inference done 872/1093. Dataloading: 0.0133 s/iter. Inference: 0.1684 s/iter. Eval: 0.2196 s/iter. Total: 0.4014 s/iter. ETA=0:01:28
[01/17 15:49:21] d2.evaluation.evaluator INFO: Inference done 883/1093. Dataloading: 0.0134 s/iter. Inference: 0.1693 s/iter. Eval: 0.2198 s/iter. Total: 0.4027 s/iter. ETA=0:01:24
[01/17 15:49:26] d2.evaluation.evaluator INFO: Inference done 894/1093. Dataloading: 0.0134 s/iter. Inference: 0.1695 s/iter. Eval: 0.2203 s/iter. Total: 0.4034 s/iter. ETA=0:01:20
[01/17 15:49:31] d2.evaluation.evaluator INFO: Inference done 909/1093. Dataloading: 0.0133 s/iter. Inference: 0.1693 s/iter. Eval: 0.2196 s/iter. Total: 0.4023 s/iter. ETA=0:01:14
[01/17 15:49:36] d2.evaluation.evaluator INFO: Inference done 921/1093. Dataloading: 0.0134 s/iter. Inference: 0.1692 s/iter. Eval: 0.2199 s/iter. Total: 0.4026 s/iter. ETA=0:01:09
[01/17 15:49:41] d2.evaluation.evaluator INFO: Inference done 934/1093. Dataloading: 0.0133 s/iter. Inference: 0.1691 s/iter. Eval: 0.2201 s/iter. Total: 0.4027 s/iter. ETA=0:01:04
[01/17 15:49:46] d2.evaluation.evaluator INFO: Inference done 947/1093. Dataloading: 0.0133 s/iter. Inference: 0.1688 s/iter. Eval: 0.2202 s/iter. Total: 0.4025 s/iter. ETA=0:00:58
[01/17 15:49:51] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0133 s/iter. Inference: 0.1690 s/iter. Eval: 0.2207 s/iter. Total: 0.4031 s/iter. ETA=0:00:54
[01/17 15:49:56] d2.evaluation.evaluator INFO: Inference done 970/1093. Dataloading: 0.0133 s/iter. Inference: 0.1690 s/iter. Eval: 0.2210 s/iter. Total: 0.4035 s/iter. ETA=0:00:49
[01/17 15:50:02] d2.evaluation.evaluator INFO: Inference done 984/1093. Dataloading: 0.0133 s/iter. Inference: 0.1690 s/iter. Eval: 0.2206 s/iter. Total: 0.4031 s/iter. ETA=0:00:43
[01/17 15:50:07] d2.evaluation.evaluator INFO: Inference done 997/1093. Dataloading: 0.0133 s/iter. Inference: 0.1691 s/iter. Eval: 0.2205 s/iter. Total: 0.4029 s/iter. ETA=0:00:38
[01/17 15:50:12] d2.evaluation.evaluator INFO: Inference done 1011/1093. Dataloading: 0.0132 s/iter. Inference: 0.1689 s/iter. Eval: 0.2201 s/iter. Total: 0.4023 s/iter. ETA=0:00:32
[01/17 15:50:17] d2.evaluation.evaluator INFO: Inference done 1024/1093. Dataloading: 0.0132 s/iter. Inference: 0.1688 s/iter. Eval: 0.2203 s/iter. Total: 0.4024 s/iter. ETA=0:00:27
[01/17 15:50:22] d2.evaluation.evaluator INFO: Inference done 1036/1093. Dataloading: 0.0132 s/iter. Inference: 0.1687 s/iter. Eval: 0.2205 s/iter. Total: 0.4026 s/iter. ETA=0:00:22
[01/17 15:50:27] d2.evaluation.evaluator INFO: Inference done 1051/1093. Dataloading: 0.0132 s/iter. Inference: 0.1685 s/iter. Eval: 0.2201 s/iter. Total: 0.4018 s/iter. ETA=0:00:16
[01/17 15:50:33] d2.evaluation.evaluator INFO: Inference done 1067/1093. Dataloading: 0.0132 s/iter. Inference: 0.1682 s/iter. Eval: 0.2194 s/iter. Total: 0.4009 s/iter. ETA=0:00:10
[01/17 15:50:38] d2.evaluation.evaluator INFO: Inference done 1082/1093. Dataloading: 0.0131 s/iter. Inference: 0.1680 s/iter. Eval: 0.2188 s/iter. Total: 0.4000 s/iter. ETA=0:00:04
[01/17 15:50:42] d2.evaluation.evaluator INFO: Total inference time: 0:07:14.775483 (0.399610 s / iter per device, on 4 devices)
[01/17 15:50:42] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:02 (0.167759 s / iter per device, on 4 devices)
[01/17 15:51:07] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 15.890862791244665, 'fwIoU': 44.748383140946316, 'IoU-1': nan, 'IoU-2': 95.47236393699703, 'IoU-3': 45.8080624077085, 'IoU-4': 59.981941108359834, 'IoU-5': 52.86064491420957, 'IoU-6': 45.85395110029932, 'IoU-7': 41.47109456275182, 'IoU-8': 33.96418123029687, 'IoU-9': 29.856846438312612, 'IoU-10': 38.00246214609045, 'IoU-11': 45.0687990592427, 'IoU-12': 54.962962356116975, 'IoU-13': 54.42520008821615, 'IoU-14': 53.11385905425109, 'IoU-15': 54.37758602493167, 'IoU-16': 54.49879898670618, 'IoU-17': 54.7618037395089, 'IoU-18': 50.51110160423286, 'IoU-19': 50.05734283484094, 'IoU-20': 50.013845252928505, 'IoU-21': 51.53587129781905, 'IoU-22': 50.98278305545105, 'IoU-23': 52.72736735686803, 'IoU-24': 50.25987391250565, 'IoU-25': 50.27336708462797, 'IoU-26': 49.84089093146299, 'IoU-27': 48.82237091071387, 'IoU-28': 49.797420803212425, 'IoU-29': 48.07264623781215, 'IoU-30': 48.355035843178264, 'IoU-31': 47.48181137770386, 'IoU-32': 47.77527824087197, 'IoU-33': 47.773954804457595, 'IoU-34': 46.21777311577455, 'IoU-35': 44.87404340508748, 'IoU-36': 46.53583580104466, 'IoU-37': 46.16633361835821, 'IoU-38': 44.756589226463454, 'IoU-39': 44.881202890183346, 'IoU-40': 43.73049477413279, 'IoU-41': 43.55706955284377, 'IoU-42': 42.702273512390214, 'IoU-43': 41.0413779449614, 'IoU-44': 40.22614930253922, 'IoU-45': 40.03116949767159, 'IoU-46': 39.81442182668714, 'IoU-47': 38.955756008790715, 'IoU-48': 37.72607354436303, 'IoU-49': 38.63252771565598, 'IoU-50': 39.00910568143328, 'IoU-51': 37.086699396595876, 'IoU-52': 36.36917776211818, 'IoU-53': 35.48534419539304, 'IoU-54': 34.13950339259892, 'IoU-55': 34.78312551666007, 'IoU-56': 32.963138608880215, 'IoU-57': 31.60650761494591, 'IoU-58': 30.960236818454277, 'IoU-59': 28.814970947851098, 'IoU-60': 27.665603478029656, 'IoU-61': 27.52362172291824, 'IoU-62': 25.831487654801816, 'IoU-63': 25.461971046743688, 'IoU-64': 24.514594696066197, 'IoU-65': 22.715601141245145, 'IoU-66': 22.0992640906014, 'IoU-67': 22.368051105614843, 'IoU-68': 20.559486596197267, 'IoU-69': 22.363046753688167, 'IoU-70': 23.095349268028926, 'IoU-71': 22.61555013268971, 'IoU-72': 20.16823449725588, 'IoU-73': 21.17704787406237, 'IoU-74': 21.304679455345234, 'IoU-75': 19.47989533373304, 'IoU-76': 20.26244846567995, 'IoU-77': 22.914220217027932, 'IoU-78': 19.322153919424377, 'IoU-79': 19.884214604483684, 'IoU-80': 19.577897827140927, 'IoU-81': 19.65934929289346, 'IoU-82': 18.740885363450733, 'IoU-83': 19.59942104106856, 'IoU-84': 19.393083034575913, 'IoU-85': 18.86494814676093, 'IoU-86': 17.58293977434401, 'IoU-87': 18.498410470965965, 'IoU-88': 16.692045383954028, 'IoU-89': 17.58742484170769, 'IoU-90': 16.971342859307136, 'IoU-91': 18.03119910986007, 'IoU-92': 16.960818804992286, 'IoU-93': 18.102146635864994, 'IoU-94': 18.772758789440804, 'IoU-95': 17.534007086917434, 'IoU-96': 16.780589442894943, 'IoU-97': 17.04542840290611, 'IoU-98': 17.97979652895269, 'IoU-99': 16.917137596596547, 'IoU-100': 15.567984530913648, 'IoU-101': 16.55689239049794, 'IoU-102': 16.167338596822045, 'IoU-103': 16.329388002307223, 'IoU-104': 15.25164592370938, 'IoU-105': 14.963963672796613, 'IoU-106': 13.533213332224078, 'IoU-107': 13.181026727546028, 'IoU-108': 15.05744590445, 'IoU-109': 14.25777014565621, 'IoU-110': 13.255221431771153, 'IoU-111': 12.654764325417615, 'IoU-112': 12.78647490519591, 'IoU-113': 12.464444377190475, 'IoU-114': 10.21639401815095, 'IoU-115': 11.227179044657758, 'IoU-116': 11.324781418952817, 'IoU-117': 11.478584759125003, 'IoU-118': 11.494219669258861, 'IoU-119': 11.273153536670591, 'IoU-120': 12.841723779508301, 'IoU-121': 11.886584780541018, 'IoU-122': 11.313111247736906, 'IoU-123': 9.354984946936499, 'IoU-124': 10.833113831020642, 'IoU-125': 9.377305689834609, 'IoU-126': 7.566336716296992, 'IoU-127': 9.60740040544621, 'IoU-128': 6.89848860526369, 'IoU-129': 8.643762255863829, 'IoU-130': 7.395681263379653, 'IoU-131': 8.604578309570746, 'IoU-132': 8.06924741524405, 'IoU-133': 8.12764770254779, 'IoU-134': 8.9506267214553, 'IoU-135': 8.105981557610852, 'IoU-136': 9.829665061640272, 'IoU-137': 8.65074586071857, 'IoU-138': 5.982089864537247, 'IoU-139': 6.650216767397659, 'IoU-140': 7.840854708513422, 'IoU-141': 8.557518039228972, 'IoU-142': 5.406904399039493, 'IoU-143': 8.208352153738758, 'IoU-144': 5.826363209877865, 'IoU-145': 5.472477520903196, 'IoU-146': 4.169845543751338, 'IoU-147': 5.114996245659043, 'IoU-148': 4.98106521694832, 'IoU-149': 3.7680054066275543, 'IoU-150': 5.582266146444172, 'IoU-151': 5.900907290112739, 'IoU-152': 2.5365870307167233, 'IoU-153': 5.4736179178586495, 'IoU-154': 3.77868416314736, 'IoU-155': 2.4531969293381506, 'IoU-156': 3.3021802525618047, 'IoU-157': 2.6374906927691035, 'IoU-158': 4.916030113202826, 'IoU-159': 1.326949835994402, 'IoU-160': 2.7310727680267877, 'IoU-161': 2.362373025681865, 'IoU-162': 1.9112555265540936, 'IoU-163': 2.9940219430157624, 'IoU-164': 2.2603489068494613, 'IoU-165': 1.216965142517213, 'IoU-166': 2.452321147727244, 'IoU-167': 2.483492532532173, 'IoU-168': 1.1861302444878883, 'IoU-169': 1.8567630297678506, 'IoU-170': 1.5235674870465665, 'IoU-171': 1.4496527339492686, 'IoU-172': 1.3477804423315274, 'IoU-173': 2.0226692362516023, 'IoU-174': 1.7160602732544907, 'IoU-175': 0.6801969287530644, 'IoU-176': 1.6268458593691562, 'IoU-177': 1.8241728117547082, 'IoU-178': 1.2302799938832392, 'IoU-179': 2.2714149052051216, 'IoU-180': 1.155784445553342, 'IoU-181': 1.7373741666284075, 'IoU-182': 1.181205410232176, 'IoU-183': 0.7590325042658465, 'IoU-184': 0.31103935309241637, 'IoU-185': 0.8267712863684549, 'IoU-186': 0.49505440187036065, 'IoU-187': 1.0684997950796558, 'IoU-188': 0.9163703294178139, 'IoU-189': 0.5949992808532865, 'IoU-190': 0.7782418939141004, 'IoU-191': 0.8639318012547008, 'IoU-192': 0.29288975257789474, 'IoU-193': 0.7185244409802507, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 23.79499543822212, 'pACC': 58.916747632449365, 'ACC-1': nan, 'ACC-2': 98.00278147475723, 'ACC-3': 69.57996083164709, 'ACC-4': 74.75673077589653, 'ACC-5': 71.07344814592813, 'ACC-6': 64.55227943810632, 'ACC-7': 61.73735032431528, 'ACC-8': 47.47494083886659, 'ACC-9': 39.625591432952795, 'ACC-10': 49.012955299521806, 'ACC-11': 64.09007257824199, 'ACC-12': 71.51578907896572, 'ACC-13': 72.83395090038938, 'ACC-14': 70.88613568308283, 'ACC-15': 67.49859713148983, 'ACC-16': 69.45474177435531, 'ACC-17': 71.12744516598775, 'ACC-18': 71.70482291547026, 'ACC-19': 68.68162389559022, 'ACC-20': 66.9363287604433, 'ACC-21': 65.26232000405957, 'ACC-22': 67.79224821069592, 'ACC-23': 70.10282606481896, 'ACC-24': 66.30235646057994, 'ACC-25': 64.39938889676546, 'ACC-26': 65.60442628556851, 'ACC-27': 66.02439003993544, 'ACC-28': 69.66433749010008, 'ACC-29': 65.21298935842134, 'ACC-30': 65.14534322147513, 'ACC-31': 62.324226710986686, 'ACC-32': 64.78958510025832, 'ACC-33': 65.68963173490981, 'ACC-34': 62.819509489937985, 'ACC-35': 62.507484309155934, 'ACC-36': 63.10878064618566, 'ACC-37': 63.88169356254808, 'ACC-38': 63.88991790617165, 'ACC-39': 61.35478631739355, 'ACC-40': 61.27722506617349, 'ACC-41': 59.49302807125723, 'ACC-42': 58.987942504099436, 'ACC-43': 57.57116224050138, 'ACC-44': 56.837310896994, 'ACC-45': 58.613686333513634, 'ACC-46': 59.45058659653676, 'ACC-47': 54.895368528048294, 'ACC-48': 54.59166574685628, 'ACC-49': 55.74268946550772, 'ACC-50': 56.37857606645278, 'ACC-51': 54.157321234796015, 'ACC-52': 53.32158553718892, 'ACC-53': 50.47802210436757, 'ACC-54': 47.56815968642585, 'ACC-55': 50.99373891992952, 'ACC-56': 47.32010719050505, 'ACC-57': 47.2741520262912, 'ACC-58': 47.04455522971905, 'ACC-59': 43.6177068640253, 'ACC-60': 43.536235306766, 'ACC-61': 41.93437492796963, 'ACC-62': 41.050415482499254, 'ACC-63': 41.53611613692874, 'ACC-64': 37.38469631839827, 'ACC-65': 36.924265429352246, 'ACC-66': 36.20777026609301, 'ACC-67': 36.18370944603728, 'ACC-68': 38.59350794150253, 'ACC-69': 38.09923345365691, 'ACC-70': 39.32470779963943, 'ACC-71': 35.89689362165395, 'ACC-72': 33.559603662150266, 'ACC-73': 35.11392965547594, 'ACC-74': 33.80689163512803, 'ACC-75': 37.36605402423693, 'ACC-76': 31.50980792562073, 'ACC-77': 37.83498950884091, 'ACC-78': 31.657001807071484, 'ACC-79': 32.90451848396093, 'ACC-80': 32.6805138663016, 'ACC-81': 35.12473583833035, 'ACC-82': 29.77346687098415, 'ACC-83': 31.48690287037164, 'ACC-84': 33.88940709369389, 'ACC-85': 32.35557211949217, 'ACC-86': 30.620909949807267, 'ACC-87': 31.760602921758558, 'ACC-88': 27.779404985274386, 'ACC-89': 28.89276191907794, 'ACC-90': 30.084563938801946, 'ACC-91': 31.87647072310075, 'ACC-92': 29.07506556851035, 'ACC-93': 31.755621374725752, 'ACC-94': 33.83989566156657, 'ACC-95': 28.803420986690213, 'ACC-96': 27.30980989646456, 'ACC-97': 29.366402497706446, 'ACC-98': 28.718303514792165, 'ACC-99': 27.767480460622558, 'ACC-100': 22.89136436319767, 'ACC-101': 26.449624416229373, 'ACC-102': 26.942110249651375, 'ACC-103': 28.213819664703216, 'ACC-104': 25.85281477283461, 'ACC-105': 24.253953419738817, 'ACC-106': 21.93561008399584, 'ACC-107': 22.998859064432835, 'ACC-108': 27.544259580782303, 'ACC-109': 26.855813313354115, 'ACC-110': 22.12915123397729, 'ACC-111': 19.82606124530248, 'ACC-112': 20.81177580737107, 'ACC-113': 21.162412189472548, 'ACC-114': 16.226937221387068, 'ACC-115': 22.9489682386679, 'ACC-116': 20.022965435328434, 'ACC-117': 21.908107487765115, 'ACC-118': 21.584157540582176, 'ACC-119': 19.37752787193333, 'ACC-120': 24.772456967228056, 'ACC-121': 20.810404565372465, 'ACC-122': 21.780483251704307, 'ACC-123': 15.186768188551426, 'ACC-124': 19.806076550963255, 'ACC-125': 14.381165424769366, 'ACC-126': 15.481752039183652, 'ACC-127': 19.72245876556253, 'ACC-128': 10.96064349727593, 'ACC-129': 16.588546526306548, 'ACC-130': 13.905101725132235, 'ACC-131': 15.978168056691336, 'ACC-132': 14.274084160899786, 'ACC-133': 19.89584904142916, 'ACC-134': 15.535528383922923, 'ACC-135': 14.154037052994397, 'ACC-136': 18.527806282424038, 'ACC-137': 14.74598455553485, 'ACC-138': 11.3466700554183, 'ACC-139': 16.05491852171485, 'ACC-140': 16.212790622363705, 'ACC-141': 20.0853256737349, 'ACC-142': 8.589656629172634, 'ACC-143': 17.457410565899984, 'ACC-144': 11.740274771241126, 'ACC-145': 9.872472924187726, 'ACC-146': 8.276750374561148, 'ACC-147': 9.561947408101254, 'ACC-148': 8.62483437528726, 'ACC-149': 5.813596489143587, 'ACC-150': 10.044936003464262, 'ACC-151': 12.57513740229662, 'ACC-152': 3.960532163537014, 'ACC-153': 9.487532874772405, 'ACC-154': 8.628204071162902, 'ACC-155': 3.664812655650624, 'ACC-156': 9.37489985529861, 'ACC-157': 4.822018396064819, 'ACC-158': 13.58302948145789, 'ACC-159': 1.8721305876358934, 'ACC-160': 4.218187798246037, 'ACC-161': 3.8753223544315247, 'ACC-162': 3.022941667111327, 'ACC-163': 6.957432438463439, 'ACC-164': 4.661653675590484, 'ACC-165': 2.6661677643574464, 'ACC-166': 4.603639459131522, 'ACC-167': 5.570506417305319, 'ACC-168': 2.3039361297416736, 'ACC-169': 6.765419509107533, 'ACC-170': 5.287132847745103, 'ACC-171': 3.566662407541491, 'ACC-172': 3.5254744135383507, 'ACC-173': 4.688483905619143, 'ACC-174': 4.21274642242985, 'ACC-175': 1.1931457032029094, 'ACC-176': 2.6941330685791094, 'ACC-177': 3.017616670536336, 'ACC-178': 2.6873653964334556, 'ACC-179': 6.086020965364847, 'ACC-180': 3.078024918793786, 'ACC-181': 9.691532053235802, 'ACC-182': 3.324403514132564, 'ACC-183': 1.0296406682003085, 'ACC-184': 0.43178238544464875, 'ACC-185': 1.25443083649948, 'ACC-186': 1.964995912032482, 'ACC-187': 2.683345411967999, 'ACC-188': 3.8925331303833097, 'ACC-189': 0.9911827609862395, 'ACC-190': 4.158954202725912, 'ACC-191': 1.6016537933491934, 'ACC-192': 0.6577487765089722, 'ACC-193': 4.048719334636968, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 15:51:07] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 15:51:07] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 15:51:07] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 15:51:07] d2.evaluation.testing INFO: copypaste: 15.8909,44.7484,23.7950,58.9167
[01/17 15:51:08] d2.utils.events INFO:  eta: 1 day, 2:30:28  iter: 25999  total_loss: 34.14  loss_ce: 0.2581  loss_mask: 0.3498  loss_dice: 2.728  loss_ce_0: 0.5847  loss_mask_0: 0.3522  loss_dice_0: 2.84  loss_ce_1: 0.2839  loss_mask_1: 0.3552  loss_dice_1: 2.77  loss_ce_2: 0.281  loss_mask_2: 0.3517  loss_dice_2: 2.737  loss_ce_3: 0.2609  loss_mask_3: 0.3509  loss_dice_3: 2.734  loss_ce_4: 0.2567  loss_mask_4: 0.3499  loss_dice_4: 2.735  loss_ce_5: 0.2522  loss_mask_5: 0.3516  loss_dice_5: 2.739  loss_ce_6: 0.2442  loss_mask_6: 0.3494  loss_dice_6: 2.726  loss_ce_7: 0.2466  loss_mask_7: 0.3505  loss_dice_7: 2.73  loss_ce_8: 0.2541  loss_mask_8: 0.3493  loss_dice_8: 2.733  time: 1.5334  data_time: 0.0967  lr: 7.3578e-06  max_mem: 21478M
[01/17 15:51:38] d2.utils.events INFO:  eta: 1 day, 2:30:12  iter: 26019  total_loss: 33.67  loss_ce: 0.2547  loss_mask: 0.3495  loss_dice: 2.72  loss_ce_0: 0.5442  loss_mask_0: 0.362  loss_dice_0: 2.803  loss_ce_1: 0.2845  loss_mask_1: 0.3613  loss_dice_1: 2.745  loss_ce_2: 0.2877  loss_mask_2: 0.3533  loss_dice_2: 2.723  loss_ce_3: 0.2718  loss_mask_3: 0.3491  loss_dice_3: 2.714  loss_ce_4: 0.2562  loss_mask_4: 0.3488  loss_dice_4: 2.706  loss_ce_5: 0.2588  loss_mask_5: 0.3507  loss_dice_5: 2.707  loss_ce_6: 0.2648  loss_mask_6: 0.3491  loss_dice_6: 2.713  loss_ce_7: 0.2704  loss_mask_7: 0.349  loss_dice_7: 2.709  loss_ce_8: 0.2748  loss_mask_8: 0.3502  loss_dice_8: 2.714  time: 1.5334  data_time: 0.0839  lr: 7.3558e-06  max_mem: 21478M
[01/17 15:52:08] d2.utils.events INFO:  eta: 1 day, 2:30:24  iter: 26039  total_loss: 33.62  loss_ce: 0.2417  loss_mask: 0.3426  loss_dice: 2.733  loss_ce_0: 0.575  loss_mask_0: 0.3513  loss_dice_0: 2.846  loss_ce_1: 0.2847  loss_mask_1: 0.3494  loss_dice_1: 2.763  loss_ce_2: 0.2854  loss_mask_2: 0.3438  loss_dice_2: 2.747  loss_ce_3: 0.2683  loss_mask_3: 0.3445  loss_dice_3: 2.729  loss_ce_4: 0.2547  loss_mask_4: 0.3449  loss_dice_4: 2.729  loss_ce_5: 0.2638  loss_mask_5: 0.3431  loss_dice_5: 2.738  loss_ce_6: 0.2545  loss_mask_6: 0.3428  loss_dice_6: 2.733  loss_ce_7: 0.2504  loss_mask_7: 0.3438  loss_dice_7: 2.73  loss_ce_8: 0.2456  loss_mask_8: 0.3411  loss_dice_8: 2.733  time: 1.5334  data_time: 0.0888  lr: 7.3537e-06  max_mem: 21478M
[01/17 15:52:38] d2.utils.events INFO:  eta: 1 day, 2:29:42  iter: 26059  total_loss: 33.71  loss_ce: 0.2408  loss_mask: 0.35  loss_dice: 2.695  loss_ce_0: 0.5654  loss_mask_0: 0.3544  loss_dice_0: 2.803  loss_ce_1: 0.268  loss_mask_1: 0.3537  loss_dice_1: 2.735  loss_ce_2: 0.2812  loss_mask_2: 0.3504  loss_dice_2: 2.716  loss_ce_3: 0.2556  loss_mask_3: 0.3484  loss_dice_3: 2.696  loss_ce_4: 0.2463  loss_mask_4: 0.3498  loss_dice_4: 2.699  loss_ce_5: 0.2333  loss_mask_5: 0.3498  loss_dice_5: 2.693  loss_ce_6: 0.2379  loss_mask_6: 0.3493  loss_dice_6: 2.693  loss_ce_7: 0.2404  loss_mask_7: 0.3501  loss_dice_7: 2.702  loss_ce_8: 0.2411  loss_mask_8: 0.3503  loss_dice_8: 2.699  time: 1.5333  data_time: 0.0972  lr: 7.3516e-06  max_mem: 21478M
[01/17 15:53:08] d2.utils.events INFO:  eta: 1 day, 2:28:06  iter: 26079  total_loss: 34.24  loss_ce: 0.2402  loss_mask: 0.3639  loss_dice: 2.745  loss_ce_0: 0.57  loss_mask_0: 0.3756  loss_dice_0: 2.818  loss_ce_1: 0.2797  loss_mask_1: 0.371  loss_dice_1: 2.762  loss_ce_2: 0.2829  loss_mask_2: 0.3685  loss_dice_2: 2.743  loss_ce_3: 0.24  loss_mask_3: 0.3664  loss_dice_3: 2.744  loss_ce_4: 0.2471  loss_mask_4: 0.3641  loss_dice_4: 2.741  loss_ce_5: 0.2535  loss_mask_5: 0.363  loss_dice_5: 2.745  loss_ce_6: 0.248  loss_mask_6: 0.3637  loss_dice_6: 2.738  loss_ce_7: 0.2401  loss_mask_7: 0.3619  loss_dice_7: 2.729  loss_ce_8: 0.2316  loss_mask_8: 0.3616  loss_dice_8: 2.734  time: 1.5333  data_time: 0.0789  lr: 7.3496e-06  max_mem: 21478M
[01/17 15:53:38] d2.utils.events INFO:  eta: 1 day, 2:28:13  iter: 26099  total_loss: 34.62  loss_ce: 0.2795  loss_mask: 0.3558  loss_dice: 2.738  loss_ce_0: 0.6107  loss_mask_0: 0.3583  loss_dice_0: 2.843  loss_ce_1: 0.3177  loss_mask_1: 0.3584  loss_dice_1: 2.775  loss_ce_2: 0.3112  loss_mask_2: 0.3557  loss_dice_2: 2.759  loss_ce_3: 0.288  loss_mask_3: 0.354  loss_dice_3: 2.751  loss_ce_4: 0.286  loss_mask_4: 0.3549  loss_dice_4: 2.75  loss_ce_5: 0.2826  loss_mask_5: 0.3558  loss_dice_5: 2.745  loss_ce_6: 0.2801  loss_mask_6: 0.3551  loss_dice_6: 2.746  loss_ce_7: 0.2731  loss_mask_7: 0.3566  loss_dice_7: 2.739  loss_ce_8: 0.2553  loss_mask_8: 0.3558  loss_dice_8: 2.744  time: 1.5333  data_time: 0.0855  lr: 7.3475e-06  max_mem: 21478M
[01/17 15:54:08] d2.utils.events INFO:  eta: 1 day, 2:27:06  iter: 26119  total_loss: 34.12  loss_ce: 0.266  loss_mask: 0.3526  loss_dice: 2.722  loss_ce_0: 0.5879  loss_mask_0: 0.3652  loss_dice_0: 2.832  loss_ce_1: 0.2904  loss_mask_1: 0.3563  loss_dice_1: 2.762  loss_ce_2: 0.2769  loss_mask_2: 0.3527  loss_dice_2: 2.743  loss_ce_3: 0.2659  loss_mask_3: 0.3498  loss_dice_3: 2.731  loss_ce_4: 0.2662  loss_mask_4: 0.3499  loss_dice_4: 2.738  loss_ce_5: 0.2524  loss_mask_5: 0.351  loss_dice_5: 2.731  loss_ce_6: 0.2556  loss_mask_6: 0.3532  loss_dice_6: 2.726  loss_ce_7: 0.2515  loss_mask_7: 0.352  loss_dice_7: 2.729  loss_ce_8: 0.2571  loss_mask_8: 0.3521  loss_dice_8: 2.722  time: 1.5333  data_time: 0.0919  lr: 7.3454e-06  max_mem: 21478M
[01/17 15:54:38] d2.utils.events INFO:  eta: 1 day, 2:27:30  iter: 26139  total_loss: 33.36  loss_ce: 0.2393  loss_mask: 0.3425  loss_dice: 2.664  loss_ce_0: 0.5423  loss_mask_0: 0.3498  loss_dice_0: 2.766  loss_ce_1: 0.2742  loss_mask_1: 0.3449  loss_dice_1: 2.704  loss_ce_2: 0.2735  loss_mask_2: 0.3446  loss_dice_2: 2.674  loss_ce_3: 0.2476  loss_mask_3: 0.3442  loss_dice_3: 2.665  loss_ce_4: 0.248  loss_mask_4: 0.3454  loss_dice_4: 2.66  loss_ce_5: 0.2451  loss_mask_5: 0.3429  loss_dice_5: 2.675  loss_ce_6: 0.2443  loss_mask_6: 0.3424  loss_dice_6: 2.661  loss_ce_7: 0.244  loss_mask_7: 0.3422  loss_dice_7: 2.668  loss_ce_8: 0.2385  loss_mask_8: 0.3426  loss_dice_8: 2.662  time: 1.5332  data_time: 0.0886  lr: 7.3433e-06  max_mem: 21478M
[01/17 15:55:08] d2.utils.events INFO:  eta: 1 day, 2:26:16  iter: 26159  total_loss: 33.11  loss_ce: 0.2572  loss_mask: 0.3522  loss_dice: 2.673  loss_ce_0: 0.5514  loss_mask_0: 0.352  loss_dice_0: 2.773  loss_ce_1: 0.2751  loss_mask_1: 0.3575  loss_dice_1: 2.702  loss_ce_2: 0.2824  loss_mask_2: 0.3547  loss_dice_2: 2.685  loss_ce_3: 0.2566  loss_mask_3: 0.3524  loss_dice_3: 2.664  loss_ce_4: 0.243  loss_mask_4: 0.3526  loss_dice_4: 2.67  loss_ce_5: 0.2517  loss_mask_5: 0.3512  loss_dice_5: 2.683  loss_ce_6: 0.2453  loss_mask_6: 0.3518  loss_dice_6: 2.672  loss_ce_7: 0.2427  loss_mask_7: 0.3526  loss_dice_7: 2.679  loss_ce_8: 0.2521  loss_mask_8: 0.3528  loss_dice_8: 2.671  time: 1.5332  data_time: 0.0892  lr: 7.3413e-06  max_mem: 21478M
[01/17 15:55:38] d2.utils.events INFO:  eta: 1 day, 2:25:24  iter: 26179  total_loss: 33.32  loss_ce: 0.2632  loss_mask: 0.3573  loss_dice: 2.656  loss_ce_0: 0.5678  loss_mask_0: 0.363  loss_dice_0: 2.765  loss_ce_1: 0.2943  loss_mask_1: 0.3626  loss_dice_1: 2.696  loss_ce_2: 0.2774  loss_mask_2: 0.3603  loss_dice_2: 2.674  loss_ce_3: 0.2711  loss_mask_3: 0.3581  loss_dice_3: 2.663  loss_ce_4: 0.265  loss_mask_4: 0.3558  loss_dice_4: 2.66  loss_ce_5: 0.2653  loss_mask_5: 0.356  loss_dice_5: 2.669  loss_ce_6: 0.2574  loss_mask_6: 0.3547  loss_dice_6: 2.664  loss_ce_7: 0.2566  loss_mask_7: 0.357  loss_dice_7: 2.664  loss_ce_8: 0.2644  loss_mask_8: 0.3568  loss_dice_8: 2.658  time: 1.5332  data_time: 0.0895  lr: 7.3392e-06  max_mem: 21478M
[01/17 15:56:08] d2.utils.events INFO:  eta: 1 day, 2:24:54  iter: 26199  total_loss: 34.08  loss_ce: 0.2625  loss_mask: 0.3567  loss_dice: 2.717  loss_ce_0: 0.5671  loss_mask_0: 0.3561  loss_dice_0: 2.813  loss_ce_1: 0.3032  loss_mask_1: 0.3579  loss_dice_1: 2.751  loss_ce_2: 0.2979  loss_mask_2: 0.3556  loss_dice_2: 2.737  loss_ce_3: 0.2854  loss_mask_3: 0.3557  loss_dice_3: 2.721  loss_ce_4: 0.2806  loss_mask_4: 0.3533  loss_dice_4: 2.733  loss_ce_5: 0.2607  loss_mask_5: 0.3562  loss_dice_5: 2.729  loss_ce_6: 0.2575  loss_mask_6: 0.3568  loss_dice_6: 2.718  loss_ce_7: 0.2741  loss_mask_7: 0.3569  loss_dice_7: 2.723  loss_ce_8: 0.254  loss_mask_8: 0.3558  loss_dice_8: 2.724  time: 1.5331  data_time: 0.0944  lr: 7.3371e-06  max_mem: 21478M
[01/17 15:56:38] d2.utils.events INFO:  eta: 1 day, 2:23:44  iter: 26219  total_loss: 35.12  loss_ce: 0.2575  loss_mask: 0.358  loss_dice: 2.78  loss_ce_0: 0.5676  loss_mask_0: 0.3647  loss_dice_0: 2.894  loss_ce_1: 0.2729  loss_mask_1: 0.3668  loss_dice_1: 2.818  loss_ce_2: 0.288  loss_mask_2: 0.3619  loss_dice_2: 2.805  loss_ce_3: 0.2707  loss_mask_3: 0.3631  loss_dice_3: 2.774  loss_ce_4: 0.2663  loss_mask_4: 0.3628  loss_dice_4: 2.779  loss_ce_5: 0.2385  loss_mask_5: 0.3615  loss_dice_5: 2.786  loss_ce_6: 0.2468  loss_mask_6: 0.359  loss_dice_6: 2.774  loss_ce_7: 0.2636  loss_mask_7: 0.3587  loss_dice_7: 2.786  loss_ce_8: 0.2488  loss_mask_8: 0.3584  loss_dice_8: 2.776  time: 1.5331  data_time: 0.0824  lr: 7.3351e-06  max_mem: 21478M
[01/17 15:57:09] d2.utils.events INFO:  eta: 1 day, 2:24:30  iter: 26239  total_loss: 34.07  loss_ce: 0.2432  loss_mask: 0.347  loss_dice: 2.767  loss_ce_0: 0.5945  loss_mask_0: 0.3532  loss_dice_0: 2.878  loss_ce_1: 0.2848  loss_mask_1: 0.3472  loss_dice_1: 2.808  loss_ce_2: 0.2798  loss_mask_2: 0.3442  loss_dice_2: 2.781  loss_ce_3: 0.2672  loss_mask_3: 0.3471  loss_dice_3: 2.783  loss_ce_4: 0.2649  loss_mask_4: 0.3477  loss_dice_4: 2.771  loss_ce_5: 0.2482  loss_mask_5: 0.3487  loss_dice_5: 2.776  loss_ce_6: 0.2549  loss_mask_6: 0.3473  loss_dice_6: 2.771  loss_ce_7: 0.2404  loss_mask_7: 0.3475  loss_dice_7: 2.759  loss_ce_8: 0.2425  loss_mask_8: 0.3465  loss_dice_8: 2.758  time: 1.5331  data_time: 0.0949  lr: 7.333e-06  max_mem: 21478M
[01/17 15:57:39] d2.utils.events INFO:  eta: 1 day, 2:23:47  iter: 26259  total_loss: 34.53  loss_ce: 0.2554  loss_mask: 0.3449  loss_dice: 2.773  loss_ce_0: 0.5744  loss_mask_0: 0.3503  loss_dice_0: 2.865  loss_ce_1: 0.3035  loss_mask_1: 0.3547  loss_dice_1: 2.795  loss_ce_2: 0.2764  loss_mask_2: 0.35  loss_dice_2: 2.774  loss_ce_3: 0.2671  loss_mask_3: 0.3472  loss_dice_3: 2.774  loss_ce_4: 0.2644  loss_mask_4: 0.3451  loss_dice_4: 2.775  loss_ce_5: 0.2512  loss_mask_5: 0.3444  loss_dice_5: 2.773  loss_ce_6: 0.2548  loss_mask_6: 0.3453  loss_dice_6: 2.775  loss_ce_7: 0.2559  loss_mask_7: 0.3447  loss_dice_7: 2.778  loss_ce_8: 0.2564  loss_mask_8: 0.3446  loss_dice_8: 2.772  time: 1.5331  data_time: 0.0837  lr: 7.3309e-06  max_mem: 21478M
[01/17 15:58:09] d2.utils.events INFO:  eta: 1 day, 2:22:44  iter: 26279  total_loss: 33.21  loss_ce: 0.2357  loss_mask: 0.344  loss_dice: 2.662  loss_ce_0: 0.5487  loss_mask_0: 0.3406  loss_dice_0: 2.768  loss_ce_1: 0.2806  loss_mask_1: 0.3469  loss_dice_1: 2.694  loss_ce_2: 0.2763  loss_mask_2: 0.3443  loss_dice_2: 2.68  loss_ce_3: 0.2507  loss_mask_3: 0.3456  loss_dice_3: 2.669  loss_ce_4: 0.2471  loss_mask_4: 0.3481  loss_dice_4: 2.657  loss_ce_5: 0.2471  loss_mask_5: 0.3449  loss_dice_5: 2.663  loss_ce_6: 0.2368  loss_mask_6: 0.3471  loss_dice_6: 2.664  loss_ce_7: 0.2467  loss_mask_7: 0.3462  loss_dice_7: 2.657  loss_ce_8: 0.2399  loss_mask_8: 0.3439  loss_dice_8: 2.662  time: 1.5331  data_time: 0.0915  lr: 7.3289e-06  max_mem: 21478M
[01/17 15:58:39] d2.utils.events INFO:  eta: 1 day, 2:23:21  iter: 26299  total_loss: 33.6  loss_ce: 0.2637  loss_mask: 0.3478  loss_dice: 2.717  loss_ce_0: 0.5695  loss_mask_0: 0.3472  loss_dice_0: 2.825  loss_ce_1: 0.3112  loss_mask_1: 0.3515  loss_dice_1: 2.745  loss_ce_2: 0.2943  loss_mask_2: 0.3492  loss_dice_2: 2.718  loss_ce_3: 0.2762  loss_mask_3: 0.3484  loss_dice_3: 2.708  loss_ce_4: 0.2794  loss_mask_4: 0.3491  loss_dice_4: 2.707  loss_ce_5: 0.2666  loss_mask_5: 0.3488  loss_dice_5: 2.708  loss_ce_6: 0.2544  loss_mask_6: 0.3488  loss_dice_6: 2.708  loss_ce_7: 0.2653  loss_mask_7: 0.3472  loss_dice_7: 2.703  loss_ce_8: 0.2614  loss_mask_8: 0.348  loss_dice_8: 2.706  time: 1.5330  data_time: 0.0919  lr: 7.3268e-06  max_mem: 21478M
[01/17 15:59:09] d2.utils.events INFO:  eta: 1 day, 2:22:18  iter: 26319  total_loss: 33.91  loss_ce: 0.2469  loss_mask: 0.3511  loss_dice: 2.729  loss_ce_0: 0.5275  loss_mask_0: 0.3517  loss_dice_0: 2.829  loss_ce_1: 0.2744  loss_mask_1: 0.3545  loss_dice_1: 2.767  loss_ce_2: 0.267  loss_mask_2: 0.3515  loss_dice_2: 2.745  loss_ce_3: 0.2486  loss_mask_3: 0.3497  loss_dice_3: 2.74  loss_ce_4: 0.2468  loss_mask_4: 0.3511  loss_dice_4: 2.731  loss_ce_5: 0.2397  loss_mask_5: 0.352  loss_dice_5: 2.743  loss_ce_6: 0.2463  loss_mask_6: 0.3502  loss_dice_6: 2.729  loss_ce_7: 0.2533  loss_mask_7: 0.3503  loss_dice_7: 2.731  loss_ce_8: 0.2424  loss_mask_8: 0.3508  loss_dice_8: 2.729  time: 1.5330  data_time: 0.0904  lr: 7.3247e-06  max_mem: 21478M
[01/17 15:59:39] d2.utils.events INFO:  eta: 1 day, 2:20:45  iter: 26339  total_loss: 33.55  loss_ce: 0.2564  loss_mask: 0.3488  loss_dice: 2.656  loss_ce_0: 0.5579  loss_mask_0: 0.3546  loss_dice_0: 2.768  loss_ce_1: 0.3009  loss_mask_1: 0.3536  loss_dice_1: 2.703  loss_ce_2: 0.3026  loss_mask_2: 0.3524  loss_dice_2: 2.681  loss_ce_3: 0.2627  loss_mask_3: 0.3518  loss_dice_3: 2.671  loss_ce_4: 0.2713  loss_mask_4: 0.3521  loss_dice_4: 2.672  loss_ce_5: 0.2614  loss_mask_5: 0.3506  loss_dice_5: 2.67  loss_ce_6: 0.261  loss_mask_6: 0.3517  loss_dice_6: 2.666  loss_ce_7: 0.2473  loss_mask_7: 0.3506  loss_dice_7: 2.658  loss_ce_8: 0.2703  loss_mask_8: 0.3493  loss_dice_8: 2.658  time: 1.5330  data_time: 0.0771  lr: 7.3226e-06  max_mem: 21478M
[01/17 16:00:08] d2.utils.events INFO:  eta: 1 day, 2:20:15  iter: 26359  total_loss: 33.32  loss_ce: 0.2409  loss_mask: 0.3648  loss_dice: 2.671  loss_ce_0: 0.5667  loss_mask_0: 0.3639  loss_dice_0: 2.767  loss_ce_1: 0.2935  loss_mask_1: 0.3668  loss_dice_1: 2.704  loss_ce_2: 0.2822  loss_mask_2: 0.3659  loss_dice_2: 2.689  loss_ce_3: 0.2608  loss_mask_3: 0.3654  loss_dice_3: 2.676  loss_ce_4: 0.2603  loss_mask_4: 0.3648  loss_dice_4: 2.661  loss_ce_5: 0.2504  loss_mask_5: 0.3635  loss_dice_5: 2.673  loss_ce_6: 0.242  loss_mask_6: 0.3635  loss_dice_6: 2.667  loss_ce_7: 0.2465  loss_mask_7: 0.3644  loss_dice_7: 2.668  loss_ce_8: 0.253  loss_mask_8: 0.3656  loss_dice_8: 2.662  time: 1.5329  data_time: 0.0919  lr: 7.3206e-06  max_mem: 21478M
[01/17 16:00:39] d2.utils.events INFO:  eta: 1 day, 2:21:21  iter: 26379  total_loss: 34.18  loss_ce: 0.2611  loss_mask: 0.3525  loss_dice: 2.745  loss_ce_0: 0.5777  loss_mask_0: 0.354  loss_dice_0: 2.866  loss_ce_1: 0.2831  loss_mask_1: 0.3545  loss_dice_1: 2.787  loss_ce_2: 0.2906  loss_mask_2: 0.3502  loss_dice_2: 2.763  loss_ce_3: 0.2791  loss_mask_3: 0.3504  loss_dice_3: 2.748  loss_ce_4: 0.2744  loss_mask_4: 0.3476  loss_dice_4: 2.752  loss_ce_5: 0.2611  loss_mask_5: 0.3502  loss_dice_5: 2.756  loss_ce_6: 0.2606  loss_mask_6: 0.3522  loss_dice_6: 2.743  loss_ce_7: 0.2631  loss_mask_7: 0.3525  loss_dice_7: 2.748  loss_ce_8: 0.2566  loss_mask_8: 0.3511  loss_dice_8: 2.749  time: 1.5329  data_time: 0.0878  lr: 7.3185e-06  max_mem: 21478M
[01/17 16:01:09] d2.utils.events INFO:  eta: 1 day, 2:22:36  iter: 26399  total_loss: 33.65  loss_ce: 0.2474  loss_mask: 0.3464  loss_dice: 2.707  loss_ce_0: 0.5874  loss_mask_0: 0.3538  loss_dice_0: 2.809  loss_ce_1: 0.2946  loss_mask_1: 0.3534  loss_dice_1: 2.748  loss_ce_2: 0.2793  loss_mask_2: 0.3492  loss_dice_2: 2.716  loss_ce_3: 0.2636  loss_mask_3: 0.3511  loss_dice_3: 2.704  loss_ce_4: 0.2589  loss_mask_4: 0.3494  loss_dice_4: 2.709  loss_ce_5: 0.2614  loss_mask_5: 0.3497  loss_dice_5: 2.713  loss_ce_6: 0.2503  loss_mask_6: 0.348  loss_dice_6: 2.704  loss_ce_7: 0.2556  loss_mask_7: 0.3467  loss_dice_7: 2.698  loss_ce_8: 0.2435  loss_mask_8: 0.3461  loss_dice_8: 2.712  time: 1.5329  data_time: 0.0815  lr: 7.3164e-06  max_mem: 21478M
[01/17 16:01:39] d2.utils.events INFO:  eta: 1 day, 2:23:13  iter: 26419  total_loss: 33.43  loss_ce: 0.2575  loss_mask: 0.3522  loss_dice: 2.677  loss_ce_0: 0.5596  loss_mask_0: 0.3471  loss_dice_0: 2.787  loss_ce_1: 0.2739  loss_mask_1: 0.3542  loss_dice_1: 2.72  loss_ce_2: 0.2939  loss_mask_2: 0.3543  loss_dice_2: 2.693  loss_ce_3: 0.2707  loss_mask_3: 0.3527  loss_dice_3: 2.694  loss_ce_4: 0.2646  loss_mask_4: 0.3526  loss_dice_4: 2.684  loss_ce_5: 0.268  loss_mask_5: 0.3536  loss_dice_5: 2.686  loss_ce_6: 0.2497  loss_mask_6: 0.3518  loss_dice_6: 2.673  loss_ce_7: 0.2584  loss_mask_7: 0.3512  loss_dice_7: 2.67  loss_ce_8: 0.2524  loss_mask_8: 0.3516  loss_dice_8: 2.686  time: 1.5329  data_time: 0.0877  lr: 7.3144e-06  max_mem: 21478M
[01/17 16:02:09] d2.utils.events INFO:  eta: 1 day, 2:24:14  iter: 26439  total_loss: 33.49  loss_ce: 0.2682  loss_mask: 0.3461  loss_dice: 2.674  loss_ce_0: 0.5974  loss_mask_0: 0.3465  loss_dice_0: 2.781  loss_ce_1: 0.2944  loss_mask_1: 0.3491  loss_dice_1: 2.716  loss_ce_2: 0.2804  loss_mask_2: 0.3462  loss_dice_2: 2.698  loss_ce_3: 0.2862  loss_mask_3: 0.3456  loss_dice_3: 2.684  loss_ce_4: 0.2685  loss_mask_4: 0.3449  loss_dice_4: 2.684  loss_ce_5: 0.259  loss_mask_5: 0.3458  loss_dice_5: 2.688  loss_ce_6: 0.2597  loss_mask_6: 0.3451  loss_dice_6: 2.674  loss_ce_7: 0.2679  loss_mask_7: 0.345  loss_dice_7: 2.681  loss_ce_8: 0.254  loss_mask_8: 0.3453  loss_dice_8: 2.676  time: 1.5329  data_time: 0.0937  lr: 7.3123e-06  max_mem: 21478M
[01/17 16:02:39] d2.utils.events INFO:  eta: 1 day, 2:24:00  iter: 26459  total_loss: 33.63  loss_ce: 0.2248  loss_mask: 0.3573  loss_dice: 2.728  loss_ce_0: 0.5872  loss_mask_0: 0.3558  loss_dice_0: 2.834  loss_ce_1: 0.2749  loss_mask_1: 0.3631  loss_dice_1: 2.762  loss_ce_2: 0.2709  loss_mask_2: 0.3579  loss_dice_2: 2.744  loss_ce_3: 0.2559  loss_mask_3: 0.3577  loss_dice_3: 2.736  loss_ce_4: 0.2471  loss_mask_4: 0.3555  loss_dice_4: 2.742  loss_ce_5: 0.2579  loss_mask_5: 0.3557  loss_dice_5: 2.73  loss_ce_6: 0.249  loss_mask_6: 0.3576  loss_dice_6: 2.723  loss_ce_7: 0.2377  loss_mask_7: 0.3576  loss_dice_7: 2.738  loss_ce_8: 0.2501  loss_mask_8: 0.3548  loss_dice_8: 2.732  time: 1.5328  data_time: 0.0883  lr: 7.3102e-06  max_mem: 21478M
[01/17 16:03:10] d2.utils.events INFO:  eta: 1 day, 2:24:34  iter: 26479  total_loss: 33.8  loss_ce: 0.2648  loss_mask: 0.3391  loss_dice: 2.724  loss_ce_0: 0.5933  loss_mask_0: 0.3438  loss_dice_0: 2.827  loss_ce_1: 0.3033  loss_mask_1: 0.3429  loss_dice_1: 2.767  loss_ce_2: 0.2772  loss_mask_2: 0.3431  loss_dice_2: 2.737  loss_ce_3: 0.266  loss_mask_3: 0.34  loss_dice_3: 2.734  loss_ce_4: 0.2637  loss_mask_4: 0.3396  loss_dice_4: 2.737  loss_ce_5: 0.27  loss_mask_5: 0.341  loss_dice_5: 2.73  loss_ce_6: 0.263  loss_mask_6: 0.3389  loss_dice_6: 2.722  loss_ce_7: 0.2642  loss_mask_7: 0.3387  loss_dice_7: 2.723  loss_ce_8: 0.2657  loss_mask_8: 0.34  loss_dice_8: 2.719  time: 1.5328  data_time: 0.0889  lr: 7.3081e-06  max_mem: 21478M
[01/17 16:03:40] d2.utils.events INFO:  eta: 1 day, 2:24:41  iter: 26499  total_loss: 32.64  loss_ce: 0.2256  loss_mask: 0.35  loss_dice: 2.627  loss_ce_0: 0.5456  loss_mask_0: 0.3534  loss_dice_0: 2.736  loss_ce_1: 0.2513  loss_mask_1: 0.3553  loss_dice_1: 2.667  loss_ce_2: 0.2549  loss_mask_2: 0.353  loss_dice_2: 2.647  loss_ce_3: 0.2375  loss_mask_3: 0.3509  loss_dice_3: 2.628  loss_ce_4: 0.2374  loss_mask_4: 0.3478  loss_dice_4: 2.63  loss_ce_5: 0.2419  loss_mask_5: 0.3495  loss_dice_5: 2.631  loss_ce_6: 0.2226  loss_mask_6: 0.3487  loss_dice_6: 2.631  loss_ce_7: 0.2327  loss_mask_7: 0.3491  loss_dice_7: 2.632  loss_ce_8: 0.2337  loss_mask_8: 0.3511  loss_dice_8: 2.626  time: 1.5328  data_time: 0.0848  lr: 7.3061e-06  max_mem: 21478M
[01/17 16:04:10] d2.utils.events INFO:  eta: 1 day, 2:24:11  iter: 26519  total_loss: 33.46  loss_ce: 0.2421  loss_mask: 0.3558  loss_dice: 2.658  loss_ce_0: 0.5592  loss_mask_0: 0.3633  loss_dice_0: 2.757  loss_ce_1: 0.26  loss_mask_1: 0.3633  loss_dice_1: 2.693  loss_ce_2: 0.2683  loss_mask_2: 0.3613  loss_dice_2: 2.679  loss_ce_3: 0.2633  loss_mask_3: 0.3586  loss_dice_3: 2.669  loss_ce_4: 0.2566  loss_mask_4: 0.3573  loss_dice_4: 2.661  loss_ce_5: 0.2344  loss_mask_5: 0.3543  loss_dice_5: 2.663  loss_ce_6: 0.2438  loss_mask_6: 0.3546  loss_dice_6: 2.655  loss_ce_7: 0.2436  loss_mask_7: 0.3533  loss_dice_7: 2.658  loss_ce_8: 0.2522  loss_mask_8: 0.3545  loss_dice_8: 2.658  time: 1.5328  data_time: 0.0821  lr: 7.304e-06  max_mem: 21478M
[01/17 16:04:40] d2.utils.events INFO:  eta: 1 day, 2:24:34  iter: 26539  total_loss: 33.77  loss_ce: 0.2728  loss_mask: 0.3652  loss_dice: 2.68  loss_ce_0: 0.591  loss_mask_0: 0.3742  loss_dice_0: 2.767  loss_ce_1: 0.3136  loss_mask_1: 0.3761  loss_dice_1: 2.717  loss_ce_2: 0.2985  loss_mask_2: 0.3711  loss_dice_2: 2.705  loss_ce_3: 0.2871  loss_mask_3: 0.3658  loss_dice_3: 2.688  loss_ce_4: 0.2846  loss_mask_4: 0.3651  loss_dice_4: 2.692  loss_ce_5: 0.2601  loss_mask_5: 0.3644  loss_dice_5: 2.688  loss_ce_6: 0.2696  loss_mask_6: 0.364  loss_dice_6: 2.687  loss_ce_7: 0.2591  loss_mask_7: 0.3643  loss_dice_7: 2.683  loss_ce_8: 0.2765  loss_mask_8: 0.3644  loss_dice_8: 2.691  time: 1.5328  data_time: 0.0845  lr: 7.3019e-06  max_mem: 21478M
[01/17 16:05:10] d2.utils.events INFO:  eta: 1 day, 2:23:50  iter: 26559  total_loss: 33.93  loss_ce: 0.2376  loss_mask: 0.3509  loss_dice: 2.734  loss_ce_0: 0.572  loss_mask_0: 0.3599  loss_dice_0: 2.84  loss_ce_1: 0.2909  loss_mask_1: 0.3595  loss_dice_1: 2.774  loss_ce_2: 0.2921  loss_mask_2: 0.3494  loss_dice_2: 2.747  loss_ce_3: 0.263  loss_mask_3: 0.351  loss_dice_3: 2.743  loss_ce_4: 0.2546  loss_mask_4: 0.3499  loss_dice_4: 2.739  loss_ce_5: 0.2513  loss_mask_5: 0.3506  loss_dice_5: 2.737  loss_ce_6: 0.2555  loss_mask_6: 0.3512  loss_dice_6: 2.739  loss_ce_7: 0.2471  loss_mask_7: 0.3523  loss_dice_7: 2.732  loss_ce_8: 0.2462  loss_mask_8: 0.352  loss_dice_8: 2.737  time: 1.5327  data_time: 0.0833  lr: 7.2999e-06  max_mem: 21478M
[01/17 16:05:40] d2.utils.events INFO:  eta: 1 day, 2:24:24  iter: 26579  total_loss: 32.92  loss_ce: 0.256  loss_mask: 0.3436  loss_dice: 2.663  loss_ce_0: 0.5626  loss_mask_0: 0.3476  loss_dice_0: 2.775  loss_ce_1: 0.2839  loss_mask_1: 0.3515  loss_dice_1: 2.706  loss_ce_2: 0.2731  loss_mask_2: 0.3479  loss_dice_2: 2.676  loss_ce_3: 0.2589  loss_mask_3: 0.3474  loss_dice_3: 2.659  loss_ce_4: 0.2495  loss_mask_4: 0.3446  loss_dice_4: 2.666  loss_ce_5: 0.2604  loss_mask_5: 0.3461  loss_dice_5: 2.666  loss_ce_6: 0.2398  loss_mask_6: 0.3448  loss_dice_6: 2.666  loss_ce_7: 0.2528  loss_mask_7: 0.3439  loss_dice_7: 2.667  loss_ce_8: 0.2421  loss_mask_8: 0.3442  loss_dice_8: 2.668  time: 1.5327  data_time: 0.0891  lr: 7.2978e-06  max_mem: 21478M
[01/17 16:06:10] d2.utils.events INFO:  eta: 1 day, 2:25:06  iter: 26599  total_loss: 33.14  loss_ce: 0.2505  loss_mask: 0.3522  loss_dice: 2.663  loss_ce_0: 0.5406  loss_mask_0: 0.3489  loss_dice_0: 2.783  loss_ce_1: 0.2935  loss_mask_1: 0.3512  loss_dice_1: 2.711  loss_ce_2: 0.286  loss_mask_2: 0.3502  loss_dice_2: 2.676  loss_ce_3: 0.2683  loss_mask_3: 0.3502  loss_dice_3: 2.673  loss_ce_4: 0.2628  loss_mask_4: 0.3486  loss_dice_4: 2.671  loss_ce_5: 0.2538  loss_mask_5: 0.3491  loss_dice_5: 2.667  loss_ce_6: 0.2519  loss_mask_6: 0.3499  loss_dice_6: 2.668  loss_ce_7: 0.2599  loss_mask_7: 0.3504  loss_dice_7: 2.667  loss_ce_8: 0.257  loss_mask_8: 0.3505  loss_dice_8: 2.659  time: 1.5327  data_time: 0.0904  lr: 7.2957e-06  max_mem: 21478M
[01/17 16:06:40] d2.utils.events INFO:  eta: 1 day, 2:24:36  iter: 26619  total_loss: 33.33  loss_ce: 0.2582  loss_mask: 0.3434  loss_dice: 2.662  loss_ce_0: 0.5952  loss_mask_0: 0.3401  loss_dice_0: 2.776  loss_ce_1: 0.3032  loss_mask_1: 0.3463  loss_dice_1: 2.706  loss_ce_2: 0.2768  loss_mask_2: 0.3464  loss_dice_2: 2.681  loss_ce_3: 0.2762  loss_mask_3: 0.3436  loss_dice_3: 2.669  loss_ce_4: 0.269  loss_mask_4: 0.3437  loss_dice_4: 2.673  loss_ce_5: 0.2553  loss_mask_5: 0.3417  loss_dice_5: 2.668  loss_ce_6: 0.2667  loss_mask_6: 0.3411  loss_dice_6: 2.666  loss_ce_7: 0.2553  loss_mask_7: 0.3422  loss_dice_7: 2.669  loss_ce_8: 0.2639  loss_mask_8: 0.3428  loss_dice_8: 2.661  time: 1.5327  data_time: 0.0937  lr: 7.2937e-06  max_mem: 21478M
[01/17 16:07:11] d2.utils.events INFO:  eta: 1 day, 2:24:21  iter: 26639  total_loss: 33.14  loss_ce: 0.257  loss_mask: 0.3535  loss_dice: 2.615  loss_ce_0: 0.543  loss_mask_0: 0.3519  loss_dice_0: 2.723  loss_ce_1: 0.2875  loss_mask_1: 0.3521  loss_dice_1: 2.658  loss_ce_2: 0.2852  loss_mask_2: 0.3521  loss_dice_2: 2.627  loss_ce_3: 0.2516  loss_mask_3: 0.3518  loss_dice_3: 2.617  loss_ce_4: 0.2718  loss_mask_4: 0.3534  loss_dice_4: 2.624  loss_ce_5: 0.2645  loss_mask_5: 0.3535  loss_dice_5: 2.618  loss_ce_6: 0.2545  loss_mask_6: 0.3529  loss_dice_6: 2.62  loss_ce_7: 0.2566  loss_mask_7: 0.3532  loss_dice_7: 2.621  loss_ce_8: 0.2703  loss_mask_8: 0.3536  loss_dice_8: 2.611  time: 1.5326  data_time: 0.0818  lr: 7.2916e-06  max_mem: 21478M
[01/17 16:07:40] d2.utils.events INFO:  eta: 1 day, 2:23:51  iter: 26659  total_loss: 33.45  loss_ce: 0.2516  loss_mask: 0.3433  loss_dice: 2.667  loss_ce_0: 0.5687  loss_mask_0: 0.3611  loss_dice_0: 2.756  loss_ce_1: 0.2891  loss_mask_1: 0.3512  loss_dice_1: 2.689  loss_ce_2: 0.2786  loss_mask_2: 0.3463  loss_dice_2: 2.669  loss_ce_3: 0.2819  loss_mask_3: 0.3422  loss_dice_3: 2.662  loss_ce_4: 0.2596  loss_mask_4: 0.3413  loss_dice_4: 2.66  loss_ce_5: 0.2741  loss_mask_5: 0.3438  loss_dice_5: 2.659  loss_ce_6: 0.2591  loss_mask_6: 0.3437  loss_dice_6: 2.662  loss_ce_7: 0.2503  loss_mask_7: 0.3435  loss_dice_7: 2.65  loss_ce_8: 0.2539  loss_mask_8: 0.344  loss_dice_8: 2.656  time: 1.5326  data_time: 0.0938  lr: 7.2895e-06  max_mem: 21478M
[01/17 16:08:10] d2.utils.events INFO:  eta: 1 day, 2:23:21  iter: 26679  total_loss: 33.05  loss_ce: 0.249  loss_mask: 0.3457  loss_dice: 2.644  loss_ce_0: 0.5713  loss_mask_0: 0.3511  loss_dice_0: 2.779  loss_ce_1: 0.2978  loss_mask_1: 0.3522  loss_dice_1: 2.697  loss_ce_2: 0.2852  loss_mask_2: 0.3509  loss_dice_2: 2.679  loss_ce_3: 0.2573  loss_mask_3: 0.3517  loss_dice_3: 2.672  loss_ce_4: 0.2508  loss_mask_4: 0.349  loss_dice_4: 2.661  loss_ce_5: 0.2494  loss_mask_5: 0.3481  loss_dice_5: 2.671  loss_ce_6: 0.2617  loss_mask_6: 0.3455  loss_dice_6: 2.665  loss_ce_7: 0.25  loss_mask_7: 0.3452  loss_dice_7: 2.652  loss_ce_8: 0.2583  loss_mask_8: 0.3468  loss_dice_8: 2.656  time: 1.5326  data_time: 0.0851  lr: 7.2874e-06  max_mem: 21478M
[01/17 16:08:40] d2.utils.events INFO:  eta: 1 day, 2:23:02  iter: 26699  total_loss: 33.29  loss_ce: 0.2486  loss_mask: 0.3448  loss_dice: 2.65  loss_ce_0: 0.5771  loss_mask_0: 0.3382  loss_dice_0: 2.778  loss_ce_1: 0.305  loss_mask_1: 0.3419  loss_dice_1: 2.691  loss_ce_2: 0.3049  loss_mask_2: 0.3409  loss_dice_2: 2.664  loss_ce_3: 0.2674  loss_mask_3: 0.3418  loss_dice_3: 2.661  loss_ce_4: 0.2681  loss_mask_4: 0.3397  loss_dice_4: 2.653  loss_ce_5: 0.2776  loss_mask_5: 0.3413  loss_dice_5: 2.66  loss_ce_6: 0.267  loss_mask_6: 0.3426  loss_dice_6: 2.653  loss_ce_7: 0.2605  loss_mask_7: 0.3428  loss_dice_7: 2.646  loss_ce_8: 0.2708  loss_mask_8: 0.3442  loss_dice_8: 2.649  time: 1.5325  data_time: 0.0889  lr: 7.2854e-06  max_mem: 21478M
[01/17 16:09:10] d2.utils.events INFO:  eta: 1 day, 2:22:21  iter: 26719  total_loss: 33.57  loss_ce: 0.2446  loss_mask: 0.3412  loss_dice: 2.673  loss_ce_0: 0.5607  loss_mask_0: 0.3509  loss_dice_0: 2.796  loss_ce_1: 0.2735  loss_mask_1: 0.3511  loss_dice_1: 2.708  loss_ce_2: 0.278  loss_mask_2: 0.3461  loss_dice_2: 2.686  loss_ce_3: 0.2668  loss_mask_3: 0.3426  loss_dice_3: 2.673  loss_ce_4: 0.2592  loss_mask_4: 0.3418  loss_dice_4: 2.671  loss_ce_5: 0.2479  loss_mask_5: 0.3417  loss_dice_5: 2.67  loss_ce_6: 0.239  loss_mask_6: 0.3426  loss_dice_6: 2.668  loss_ce_7: 0.245  loss_mask_7: 0.3421  loss_dice_7: 2.676  loss_ce_8: 0.2328  loss_mask_8: 0.3417  loss_dice_8: 2.669  time: 1.5325  data_time: 0.0938  lr: 7.2833e-06  max_mem: 21478M
[01/17 16:09:40] d2.utils.events INFO:  eta: 1 day, 2:21:40  iter: 26739  total_loss: 32.93  loss_ce: 0.2456  loss_mask: 0.3513  loss_dice: 2.606  loss_ce_0: 0.5488  loss_mask_0: 0.3521  loss_dice_0: 2.747  loss_ce_1: 0.2884  loss_mask_1: 0.3545  loss_dice_1: 2.662  loss_ce_2: 0.2879  loss_mask_2: 0.3518  loss_dice_2: 2.634  loss_ce_3: 0.2626  loss_mask_3: 0.3517  loss_dice_3: 2.628  loss_ce_4: 0.2546  loss_mask_4: 0.3506  loss_dice_4: 2.626  loss_ce_5: 0.2374  loss_mask_5: 0.3497  loss_dice_5: 2.628  loss_ce_6: 0.2347  loss_mask_6: 0.3515  loss_dice_6: 2.614  loss_ce_7: 0.2349  loss_mask_7: 0.3509  loss_dice_7: 2.614  loss_ce_8: 0.2352  loss_mask_8: 0.3517  loss_dice_8: 2.62  time: 1.5325  data_time: 0.0931  lr: 7.2812e-06  max_mem: 21478M
[01/17 16:10:10] d2.utils.events INFO:  eta: 1 day, 2:21:44  iter: 26759  total_loss: 33.85  loss_ce: 0.2607  loss_mask: 0.3458  loss_dice: 2.684  loss_ce_0: 0.5631  loss_mask_0: 0.3596  loss_dice_0: 2.779  loss_ce_1: 0.2619  loss_mask_1: 0.3543  loss_dice_1: 2.712  loss_ce_2: 0.2823  loss_mask_2: 0.3511  loss_dice_2: 2.695  loss_ce_3: 0.2453  loss_mask_3: 0.3491  loss_dice_3: 2.686  loss_ce_4: 0.246  loss_mask_4: 0.3468  loss_dice_4: 2.685  loss_ce_5: 0.2402  loss_mask_5: 0.3471  loss_dice_5: 2.688  loss_ce_6: 0.2542  loss_mask_6: 0.347  loss_dice_6: 2.681  loss_ce_7: 0.2504  loss_mask_7: 0.347  loss_dice_7: 2.68  loss_ce_8: 0.2542  loss_mask_8: 0.3455  loss_dice_8: 2.691  time: 1.5325  data_time: 0.0959  lr: 7.2791e-06  max_mem: 21478M
[01/17 16:10:40] d2.utils.events INFO:  eta: 1 day, 2:21:10  iter: 26779  total_loss: 32.84  loss_ce: 0.251  loss_mask: 0.3381  loss_dice: 2.64  loss_ce_0: 0.553  loss_mask_0: 0.3331  loss_dice_0: 2.749  loss_ce_1: 0.2825  loss_mask_1: 0.3394  loss_dice_1: 2.679  loss_ce_2: 0.2808  loss_mask_2: 0.3345  loss_dice_2: 2.672  loss_ce_3: 0.2535  loss_mask_3: 0.3336  loss_dice_3: 2.642  loss_ce_4: 0.2456  loss_mask_4: 0.3335  loss_dice_4: 2.651  loss_ce_5: 0.2475  loss_mask_5: 0.3364  loss_dice_5: 2.65  loss_ce_6: 0.2379  loss_mask_6: 0.3355  loss_dice_6: 2.642  loss_ce_7: 0.2508  loss_mask_7: 0.3361  loss_dice_7: 2.639  loss_ce_8: 0.244  loss_mask_8: 0.3376  loss_dice_8: 2.639  time: 1.5324  data_time: 0.0829  lr: 7.2771e-06  max_mem: 21478M
[01/17 16:11:10] d2.utils.events INFO:  eta: 1 day, 2:20:21  iter: 26799  total_loss: 33.13  loss_ce: 0.2518  loss_mask: 0.3451  loss_dice: 2.654  loss_ce_0: 0.5692  loss_mask_0: 0.3456  loss_dice_0: 2.778  loss_ce_1: 0.3034  loss_mask_1: 0.3472  loss_dice_1: 2.699  loss_ce_2: 0.2709  loss_mask_2: 0.3443  loss_dice_2: 2.681  loss_ce_3: 0.2702  loss_mask_3: 0.3442  loss_dice_3: 2.66  loss_ce_4: 0.2722  loss_mask_4: 0.3443  loss_dice_4: 2.656  loss_ce_5: 0.2687  loss_mask_5: 0.3415  loss_dice_5: 2.663  loss_ce_6: 0.2619  loss_mask_6: 0.3442  loss_dice_6: 2.656  loss_ce_7: 0.2633  loss_mask_7: 0.3457  loss_dice_7: 2.656  loss_ce_8: 0.2643  loss_mask_8: 0.3458  loss_dice_8: 2.657  time: 1.5324  data_time: 0.0899  lr: 7.275e-06  max_mem: 21478M
[01/17 16:11:40] d2.utils.events INFO:  eta: 1 day, 2:19:51  iter: 26819  total_loss: 33.4  loss_ce: 0.2515  loss_mask: 0.3481  loss_dice: 2.667  loss_ce_0: 0.5721  loss_mask_0: 0.3566  loss_dice_0: 2.781  loss_ce_1: 0.2795  loss_mask_1: 0.354  loss_dice_1: 2.706  loss_ce_2: 0.2717  loss_mask_2: 0.3502  loss_dice_2: 2.678  loss_ce_3: 0.247  loss_mask_3: 0.3508  loss_dice_3: 2.659  loss_ce_4: 0.2628  loss_mask_4: 0.3496  loss_dice_4: 2.663  loss_ce_5: 0.2426  loss_mask_5: 0.3503  loss_dice_5: 2.665  loss_ce_6: 0.2482  loss_mask_6: 0.3504  loss_dice_6: 2.666  loss_ce_7: 0.2514  loss_mask_7: 0.3521  loss_dice_7: 2.668  loss_ce_8: 0.2486  loss_mask_8: 0.3502  loss_dice_8: 2.663  time: 1.5324  data_time: 0.0825  lr: 7.2729e-06  max_mem: 21478M
[01/17 16:12:09] d2.utils.events INFO:  eta: 1 day, 2:18:36  iter: 26839  total_loss: 33.37  loss_ce: 0.2486  loss_mask: 0.3522  loss_dice: 2.647  loss_ce_0: 0.5506  loss_mask_0: 0.3504  loss_dice_0: 2.756  loss_ce_1: 0.288  loss_mask_1: 0.3528  loss_dice_1: 2.68  loss_ce_2: 0.2879  loss_mask_2: 0.3508  loss_dice_2: 2.661  loss_ce_3: 0.267  loss_mask_3: 0.3495  loss_dice_3: 2.647  loss_ce_4: 0.2669  loss_mask_4: 0.3508  loss_dice_4: 2.648  loss_ce_5: 0.2472  loss_mask_5: 0.3531  loss_dice_5: 2.655  loss_ce_6: 0.2402  loss_mask_6: 0.3499  loss_dice_6: 2.64  loss_ce_7: 0.2458  loss_mask_7: 0.3504  loss_dice_7: 2.639  loss_ce_8: 0.2346  loss_mask_8: 0.3522  loss_dice_8: 2.646  time: 1.5323  data_time: 0.0907  lr: 7.2709e-06  max_mem: 21478M
[01/17 16:12:40] d2.utils.events INFO:  eta: 1 day, 2:18:51  iter: 26859  total_loss: 33.8  loss_ce: 0.2439  loss_mask: 0.3422  loss_dice: 2.679  loss_ce_0: 0.5622  loss_mask_0: 0.3378  loss_dice_0: 2.769  loss_ce_1: 0.2944  loss_mask_1: 0.3503  loss_dice_1: 2.704  loss_ce_2: 0.2843  loss_mask_2: 0.3496  loss_dice_2: 2.687  loss_ce_3: 0.2609  loss_mask_3: 0.3443  loss_dice_3: 2.684  loss_ce_4: 0.245  loss_mask_4: 0.3424  loss_dice_4: 2.689  loss_ce_5: 0.2449  loss_mask_5: 0.3396  loss_dice_5: 2.691  loss_ce_6: 0.2401  loss_mask_6: 0.3422  loss_dice_6: 2.684  loss_ce_7: 0.2339  loss_mask_7: 0.3424  loss_dice_7: 2.678  loss_ce_8: 0.2407  loss_mask_8: 0.3416  loss_dice_8: 2.677  time: 1.5323  data_time: 0.0846  lr: 7.2688e-06  max_mem: 21478M
[01/17 16:13:09] d2.utils.events INFO:  eta: 1 day, 2:18:21  iter: 26879  total_loss: 32.83  loss_ce: 0.2417  loss_mask: 0.3421  loss_dice: 2.631  loss_ce_0: 0.5734  loss_mask_0: 0.3409  loss_dice_0: 2.744  loss_ce_1: 0.2963  loss_mask_1: 0.3492  loss_dice_1: 2.673  loss_ce_2: 0.2851  loss_mask_2: 0.3444  loss_dice_2: 2.661  loss_ce_3: 0.2787  loss_mask_3: 0.3416  loss_dice_3: 2.633  loss_ce_4: 0.2536  loss_mask_4: 0.3424  loss_dice_4: 2.634  loss_ce_5: 0.2442  loss_mask_5: 0.3437  loss_dice_5: 2.636  loss_ce_6: 0.2419  loss_mask_6: 0.3435  loss_dice_6: 2.63  loss_ce_7: 0.2452  loss_mask_7: 0.3414  loss_dice_7: 2.63  loss_ce_8: 0.2464  loss_mask_8: 0.3434  loss_dice_8: 2.633  time: 1.5323  data_time: 0.0883  lr: 7.2667e-06  max_mem: 21478M
[01/17 16:13:39] d2.utils.events INFO:  eta: 1 day, 2:18:17  iter: 26899  total_loss: 34.12  loss_ce: 0.2344  loss_mask: 0.3457  loss_dice: 2.752  loss_ce_0: 0.5481  loss_mask_0: 0.3514  loss_dice_0: 2.874  loss_ce_1: 0.2922  loss_mask_1: 0.3509  loss_dice_1: 2.802  loss_ce_2: 0.2667  loss_mask_2: 0.3456  loss_dice_2: 2.78  loss_ce_3: 0.2704  loss_mask_3: 0.3437  loss_dice_3: 2.757  loss_ce_4: 0.2589  loss_mask_4: 0.3462  loss_dice_4: 2.751  loss_ce_5: 0.2412  loss_mask_5: 0.3439  loss_dice_5: 2.763  loss_ce_6: 0.2367  loss_mask_6: 0.3436  loss_dice_6: 2.748  loss_ce_7: 0.2374  loss_mask_7: 0.3454  loss_dice_7: 2.751  loss_ce_8: 0.246  loss_mask_8: 0.3454  loss_dice_8: 2.746  time: 1.5323  data_time: 0.0820  lr: 7.2646e-06  max_mem: 21478M
[01/17 16:14:09] d2.utils.events INFO:  eta: 1 day, 2:17:47  iter: 26919  total_loss: 33.45  loss_ce: 0.2698  loss_mask: 0.3401  loss_dice: 2.652  loss_ce_0: 0.5715  loss_mask_0: 0.3433  loss_dice_0: 2.77  loss_ce_1: 0.3163  loss_mask_1: 0.3423  loss_dice_1: 2.691  loss_ce_2: 0.3213  loss_mask_2: 0.3399  loss_dice_2: 2.661  loss_ce_3: 0.2813  loss_mask_3: 0.3383  loss_dice_3: 2.652  loss_ce_4: 0.2924  loss_mask_4: 0.3391  loss_dice_4: 2.645  loss_ce_5: 0.26  loss_mask_5: 0.3402  loss_dice_5: 2.666  loss_ce_6: 0.2665  loss_mask_6: 0.339  loss_dice_6: 2.657  loss_ce_7: 0.2809  loss_mask_7: 0.3375  loss_dice_7: 2.648  loss_ce_8: 0.2754  loss_mask_8: 0.3395  loss_dice_8: 2.641  time: 1.5322  data_time: 0.0849  lr: 7.2626e-06  max_mem: 21478M
[01/17 16:14:39] d2.utils.events INFO:  eta: 1 day, 2:17:19  iter: 26939  total_loss: 33.2  loss_ce: 0.2343  loss_mask: 0.3487  loss_dice: 2.693  loss_ce_0: 0.53  loss_mask_0: 0.3516  loss_dice_0: 2.798  loss_ce_1: 0.2752  loss_mask_1: 0.355  loss_dice_1: 2.718  loss_ce_2: 0.2638  loss_mask_2: 0.3536  loss_dice_2: 2.693  loss_ce_3: 0.2581  loss_mask_3: 0.3499  loss_dice_3: 2.691  loss_ce_4: 0.2366  loss_mask_4: 0.3498  loss_dice_4: 2.69  loss_ce_5: 0.2292  loss_mask_5: 0.3496  loss_dice_5: 2.684  loss_ce_6: 0.2248  loss_mask_6: 0.3481  loss_dice_6: 2.691  loss_ce_7: 0.2317  loss_mask_7: 0.3494  loss_dice_7: 2.688  loss_ce_8: 0.2396  loss_mask_8: 0.3484  loss_dice_8: 2.692  time: 1.5322  data_time: 0.0800  lr: 7.2605e-06  max_mem: 21478M
[01/17 16:15:08] d2.utils.events INFO:  eta: 1 day, 2:16:10  iter: 26959  total_loss: 32.81  loss_ce: 0.2566  loss_mask: 0.3368  loss_dice: 2.628  loss_ce_0: 0.5536  loss_mask_0: 0.3425  loss_dice_0: 2.743  loss_ce_1: 0.2818  loss_mask_1: 0.3427  loss_dice_1: 2.665  loss_ce_2: 0.2745  loss_mask_2: 0.3398  loss_dice_2: 2.638  loss_ce_3: 0.2603  loss_mask_3: 0.3365  loss_dice_3: 2.634  loss_ce_4: 0.259  loss_mask_4: 0.3376  loss_dice_4: 2.628  loss_ce_5: 0.2528  loss_mask_5: 0.3381  loss_dice_5: 2.629  loss_ce_6: 0.2496  loss_mask_6: 0.3361  loss_dice_6: 2.619  loss_ce_7: 0.249  loss_mask_7: 0.3374  loss_dice_7: 2.629  loss_ce_8: 0.2509  loss_mask_8: 0.3369  loss_dice_8: 2.62  time: 1.5321  data_time: 0.0796  lr: 7.2584e-06  max_mem: 21478M
[01/17 16:15:38] d2.utils.events INFO:  eta: 1 day, 2:15:14  iter: 26979  total_loss: 33.2  loss_ce: 0.2629  loss_mask: 0.3456  loss_dice: 2.666  loss_ce_0: 0.605  loss_mask_0: 0.3474  loss_dice_0: 2.777  loss_ce_1: 0.293  loss_mask_1: 0.3529  loss_dice_1: 2.701  loss_ce_2: 0.2795  loss_mask_2: 0.3496  loss_dice_2: 2.688  loss_ce_3: 0.2668  loss_mask_3: 0.3481  loss_dice_3: 2.671  loss_ce_4: 0.2692  loss_mask_4: 0.3472  loss_dice_4: 2.667  loss_ce_5: 0.2599  loss_mask_5: 0.3474  loss_dice_5: 2.671  loss_ce_6: 0.2525  loss_mask_6: 0.3452  loss_dice_6: 2.68  loss_ce_7: 0.2526  loss_mask_7: 0.3436  loss_dice_7: 2.668  loss_ce_8: 0.2499  loss_mask_8: 0.3457  loss_dice_8: 2.667  time: 1.5321  data_time: 0.0871  lr: 7.2564e-06  max_mem: 21478M
[01/17 16:16:08] d2.utils.events INFO:  eta: 1 day, 2:12:55  iter: 26999  total_loss: 33.08  loss_ce: 0.248  loss_mask: 0.3468  loss_dice: 2.668  loss_ce_0: 0.5369  loss_mask_0: 0.3534  loss_dice_0: 2.786  loss_ce_1: 0.2929  loss_mask_1: 0.3536  loss_dice_1: 2.711  loss_ce_2: 0.2751  loss_mask_2: 0.3468  loss_dice_2: 2.701  loss_ce_3: 0.2602  loss_mask_3: 0.3461  loss_dice_3: 2.672  loss_ce_4: 0.2603  loss_mask_4: 0.3477  loss_dice_4: 2.683  loss_ce_5: 0.231  loss_mask_5: 0.3456  loss_dice_5: 2.679  loss_ce_6: 0.2413  loss_mask_6: 0.3474  loss_dice_6: 2.674  loss_ce_7: 0.2506  loss_mask_7: 0.3468  loss_dice_7: 2.673  loss_ce_8: 0.2436  loss_mask_8: 0.346  loss_dice_8: 2.669  time: 1.5321  data_time: 0.0819  lr: 7.2543e-06  max_mem: 21478M
[01/17 16:16:37] d2.utils.events INFO:  eta: 1 day, 2:12:25  iter: 27019  total_loss: 32.92  loss_ce: 0.2463  loss_mask: 0.3425  loss_dice: 2.659  loss_ce_0: 0.5462  loss_mask_0: 0.3506  loss_dice_0: 2.761  loss_ce_1: 0.261  loss_mask_1: 0.3486  loss_dice_1: 2.694  loss_ce_2: 0.2534  loss_mask_2: 0.3455  loss_dice_2: 2.685  loss_ce_3: 0.249  loss_mask_3: 0.342  loss_dice_3: 2.669  loss_ce_4: 0.2502  loss_mask_4: 0.3427  loss_dice_4: 2.668  loss_ce_5: 0.2396  loss_mask_5: 0.343  loss_dice_5: 2.674  loss_ce_6: 0.2262  loss_mask_6: 0.3414  loss_dice_6: 2.663  loss_ce_7: 0.2282  loss_mask_7: 0.3405  loss_dice_7: 2.665  loss_ce_8: 0.2354  loss_mask_8: 0.3413  loss_dice_8: 2.662  time: 1.5320  data_time: 0.0948  lr: 7.2522e-06  max_mem: 21478M
[01/17 16:17:07] d2.utils.events INFO:  eta: 1 day, 2:12:04  iter: 27039  total_loss: 33.16  loss_ce: 0.2337  loss_mask: 0.3333  loss_dice: 2.672  loss_ce_0: 0.5408  loss_mask_0: 0.3402  loss_dice_0: 2.782  loss_ce_1: 0.2887  loss_mask_1: 0.337  loss_dice_1: 2.708  loss_ce_2: 0.2635  loss_mask_2: 0.3346  loss_dice_2: 2.682  loss_ce_3: 0.2537  loss_mask_3: 0.3336  loss_dice_3: 2.674  loss_ce_4: 0.2719  loss_mask_4: 0.3333  loss_dice_4: 2.672  loss_ce_5: 0.2392  loss_mask_5: 0.3334  loss_dice_5: 2.669  loss_ce_6: 0.2388  loss_mask_6: 0.3325  loss_dice_6: 2.674  loss_ce_7: 0.2462  loss_mask_7: 0.332  loss_dice_7: 2.669  loss_ce_8: 0.2324  loss_mask_8: 0.3339  loss_dice_8: 2.672  time: 1.5320  data_time: 0.0905  lr: 7.2501e-06  max_mem: 21478M
[01/17 16:17:37] d2.utils.events INFO:  eta: 1 day, 2:11:18  iter: 27059  total_loss: 32.75  loss_ce: 0.2497  loss_mask: 0.346  loss_dice: 2.645  loss_ce_0: 0.5656  loss_mask_0: 0.3437  loss_dice_0: 2.729  loss_ce_1: 0.2913  loss_mask_1: 0.3502  loss_dice_1: 2.675  loss_ce_2: 0.2931  loss_mask_2: 0.3478  loss_dice_2: 2.648  loss_ce_3: 0.2672  loss_mask_3: 0.3442  loss_dice_3: 2.639  loss_ce_4: 0.2552  loss_mask_4: 0.3451  loss_dice_4: 2.641  loss_ce_5: 0.2482  loss_mask_5: 0.3463  loss_dice_5: 2.639  loss_ce_6: 0.2542  loss_mask_6: 0.3459  loss_dice_6: 2.635  loss_ce_7: 0.2532  loss_mask_7: 0.3444  loss_dice_7: 2.639  loss_ce_8: 0.2575  loss_mask_8: 0.3451  loss_dice_8: 2.638  time: 1.5320  data_time: 0.0850  lr: 7.2481e-06  max_mem: 21478M
[01/17 16:18:07] d2.utils.events INFO:  eta: 1 day, 2:10:34  iter: 27079  total_loss: 32.91  loss_ce: 0.248  loss_mask: 0.356  loss_dice: 2.623  loss_ce_0: 0.5499  loss_mask_0: 0.3587  loss_dice_0: 2.715  loss_ce_1: 0.2898  loss_mask_1: 0.3575  loss_dice_1: 2.659  loss_ce_2: 0.2739  loss_mask_2: 0.3573  loss_dice_2: 2.637  loss_ce_3: 0.2557  loss_mask_3: 0.3571  loss_dice_3: 2.624  loss_ce_4: 0.2679  loss_mask_4: 0.3572  loss_dice_4: 2.628  loss_ce_5: 0.2509  loss_mask_5: 0.3581  loss_dice_5: 2.629  loss_ce_6: 0.2474  loss_mask_6: 0.3577  loss_dice_6: 2.62  loss_ce_7: 0.2603  loss_mask_7: 0.3558  loss_dice_7: 2.623  loss_ce_8: 0.2471  loss_mask_8: 0.3557  loss_dice_8: 2.627  time: 1.5319  data_time: 0.0918  lr: 7.246e-06  max_mem: 21478M
[01/17 16:18:37] d2.utils.events INFO:  eta: 1 day, 2:10:04  iter: 27099  total_loss: 34.93  loss_ce: 0.2659  loss_mask: 0.3584  loss_dice: 2.8  loss_ce_0: 0.6093  loss_mask_0: 0.3628  loss_dice_0: 2.892  loss_ce_1: 0.3021  loss_mask_1: 0.3642  loss_dice_1: 2.826  loss_ce_2: 0.296  loss_mask_2: 0.3605  loss_dice_2: 2.81  loss_ce_3: 0.2788  loss_mask_3: 0.3615  loss_dice_3: 2.8  loss_ce_4: 0.277  loss_mask_4: 0.3591  loss_dice_4: 2.808  loss_ce_5: 0.2582  loss_mask_5: 0.3575  loss_dice_5: 2.801  loss_ce_6: 0.2745  loss_mask_6: 0.3574  loss_dice_6: 2.801  loss_ce_7: 0.2677  loss_mask_7: 0.3565  loss_dice_7: 2.795  loss_ce_8: 0.2607  loss_mask_8: 0.359  loss_dice_8: 2.799  time: 1.5319  data_time: 0.0896  lr: 7.2439e-06  max_mem: 21478M
[01/17 16:19:07] d2.utils.events INFO:  eta: 1 day, 2:09:29  iter: 27119  total_loss: 33.37  loss_ce: 0.2386  loss_mask: 0.3559  loss_dice: 2.652  loss_ce_0: 0.5652  loss_mask_0: 0.3592  loss_dice_0: 2.754  loss_ce_1: 0.2756  loss_mask_1: 0.3585  loss_dice_1: 2.686  loss_ce_2: 0.2882  loss_mask_2: 0.3537  loss_dice_2: 2.673  loss_ce_3: 0.2462  loss_mask_3: 0.3539  loss_dice_3: 2.656  loss_ce_4: 0.2534  loss_mask_4: 0.3539  loss_dice_4: 2.653  loss_ce_5: 0.231  loss_mask_5: 0.3564  loss_dice_5: 2.657  loss_ce_6: 0.2312  loss_mask_6: 0.356  loss_dice_6: 2.648  loss_ce_7: 0.2368  loss_mask_7: 0.3561  loss_dice_7: 2.652  loss_ce_8: 0.2281  loss_mask_8: 0.3564  loss_dice_8: 2.656  time: 1.5319  data_time: 0.0864  lr: 7.2418e-06  max_mem: 21478M
[01/17 16:19:37] d2.utils.events INFO:  eta: 1 day, 2:08:41  iter: 27139  total_loss: 33.69  loss_ce: 0.2452  loss_mask: 0.3429  loss_dice: 2.704  loss_ce_0: 0.6279  loss_mask_0: 0.3562  loss_dice_0: 2.808  loss_ce_1: 0.3131  loss_mask_1: 0.3525  loss_dice_1: 2.742  loss_ce_2: 0.2844  loss_mask_2: 0.3468  loss_dice_2: 2.722  loss_ce_3: 0.269  loss_mask_3: 0.345  loss_dice_3: 2.712  loss_ce_4: 0.2671  loss_mask_4: 0.3445  loss_dice_4: 2.715  loss_ce_5: 0.2385  loss_mask_5: 0.3461  loss_dice_5: 2.706  loss_ce_6: 0.2484  loss_mask_6: 0.3442  loss_dice_6: 2.705  loss_ce_7: 0.2435  loss_mask_7: 0.3443  loss_dice_7: 2.701  loss_ce_8: 0.2421  loss_mask_8: 0.3421  loss_dice_8: 2.702  time: 1.5319  data_time: 0.0976  lr: 7.2398e-06  max_mem: 21478M
[01/17 16:20:07] d2.utils.events INFO:  eta: 1 day, 2:08:48  iter: 27159  total_loss: 33.21  loss_ce: 0.2495  loss_mask: 0.3441  loss_dice: 2.661  loss_ce_0: 0.5316  loss_mask_0: 0.3405  loss_dice_0: 2.777  loss_ce_1: 0.283  loss_mask_1: 0.3427  loss_dice_1: 2.7  loss_ce_2: 0.2656  loss_mask_2: 0.3431  loss_dice_2: 2.668  loss_ce_3: 0.2502  loss_mask_3: 0.3452  loss_dice_3: 2.663  loss_ce_4: 0.2452  loss_mask_4: 0.3438  loss_dice_4: 2.658  loss_ce_5: 0.2499  loss_mask_5: 0.3446  loss_dice_5: 2.664  loss_ce_6: 0.2395  loss_mask_6: 0.3443  loss_dice_6: 2.65  loss_ce_7: 0.2294  loss_mask_7: 0.3468  loss_dice_7: 2.653  loss_ce_8: 0.2325  loss_mask_8: 0.3461  loss_dice_8: 2.653  time: 1.5318  data_time: 0.0854  lr: 7.2377e-06  max_mem: 21478M
[01/17 16:20:37] d2.utils.events INFO:  eta: 1 day, 2:08:59  iter: 27179  total_loss: 33.28  loss_ce: 0.2233  loss_mask: 0.3446  loss_dice: 2.715  loss_ce_0: 0.5669  loss_mask_0: 0.343  loss_dice_0: 2.827  loss_ce_1: 0.2719  loss_mask_1: 0.3468  loss_dice_1: 2.755  loss_ce_2: 0.2598  loss_mask_2: 0.3462  loss_dice_2: 2.738  loss_ce_3: 0.2342  loss_mask_3: 0.3459  loss_dice_3: 2.723  loss_ce_4: 0.2441  loss_mask_4: 0.3461  loss_dice_4: 2.72  loss_ce_5: 0.2331  loss_mask_5: 0.3462  loss_dice_5: 2.72  loss_ce_6: 0.2292  loss_mask_6: 0.3449  loss_dice_6: 2.717  loss_ce_7: 0.2291  loss_mask_7: 0.3434  loss_dice_7: 2.719  loss_ce_8: 0.2372  loss_mask_8: 0.3446  loss_dice_8: 2.718  time: 1.5318  data_time: 0.0931  lr: 7.2356e-06  max_mem: 21478M
[01/17 16:21:07] d2.utils.events INFO:  eta: 1 day, 2:07:39  iter: 27199  total_loss: 33.59  loss_ce: 0.2476  loss_mask: 0.3415  loss_dice: 2.685  loss_ce_0: 0.5731  loss_mask_0: 0.3457  loss_dice_0: 2.791  loss_ce_1: 0.2968  loss_mask_1: 0.349  loss_dice_1: 2.702  loss_ce_2: 0.2763  loss_mask_2: 0.3452  loss_dice_2: 2.695  loss_ce_3: 0.269  loss_mask_3: 0.3444  loss_dice_3: 2.68  loss_ce_4: 0.2511  loss_mask_4: 0.3451  loss_dice_4: 2.682  loss_ce_5: 0.2481  loss_mask_5: 0.344  loss_dice_5: 2.686  loss_ce_6: 0.2583  loss_mask_6: 0.3435  loss_dice_6: 2.682  loss_ce_7: 0.2573  loss_mask_7: 0.3441  loss_dice_7: 2.684  loss_ce_8: 0.2551  loss_mask_8: 0.3433  loss_dice_8: 2.686  time: 1.5318  data_time: 0.0898  lr: 7.2336e-06  max_mem: 21478M
[01/17 16:21:38] d2.utils.events INFO:  eta: 1 day, 2:08:32  iter: 27219  total_loss: 33.69  loss_ce: 0.2354  loss_mask: 0.3418  loss_dice: 2.705  loss_ce_0: 0.5722  loss_mask_0: 0.3511  loss_dice_0: 2.82  loss_ce_1: 0.2886  loss_mask_1: 0.3539  loss_dice_1: 2.745  loss_ce_2: 0.268  loss_mask_2: 0.3478  loss_dice_2: 2.727  loss_ce_3: 0.2575  loss_mask_3: 0.345  loss_dice_3: 2.715  loss_ce_4: 0.2629  loss_mask_4: 0.342  loss_dice_4: 2.717  loss_ce_5: 0.2502  loss_mask_5: 0.3418  loss_dice_5: 2.719  loss_ce_6: 0.2387  loss_mask_6: 0.3412  loss_dice_6: 2.713  loss_ce_7: 0.2338  loss_mask_7: 0.3428  loss_dice_7: 2.703  loss_ce_8: 0.2441  loss_mask_8: 0.3426  loss_dice_8: 2.719  time: 1.5318  data_time: 0.0914  lr: 7.2315e-06  max_mem: 21478M
[01/17 16:22:08] d2.utils.events INFO:  eta: 1 day, 2:07:34  iter: 27239  total_loss: 33.65  loss_ce: 0.2465  loss_mask: 0.3401  loss_dice: 2.665  loss_ce_0: 0.5615  loss_mask_0: 0.3473  loss_dice_0: 2.777  loss_ce_1: 0.298  loss_mask_1: 0.3457  loss_dice_1: 2.698  loss_ce_2: 0.2825  loss_mask_2: 0.3421  loss_dice_2: 2.684  loss_ce_3: 0.2564  loss_mask_3: 0.3409  loss_dice_3: 2.665  loss_ce_4: 0.261  loss_mask_4: 0.3408  loss_dice_4: 2.668  loss_ce_5: 0.2427  loss_mask_5: 0.3418  loss_dice_5: 2.664  loss_ce_6: 0.2626  loss_mask_6: 0.3407  loss_dice_6: 2.666  loss_ce_7: 0.2334  loss_mask_7: 0.34  loss_dice_7: 2.663  loss_ce_8: 0.2402  loss_mask_8: 0.3407  loss_dice_8: 2.662  time: 1.5318  data_time: 0.0941  lr: 7.2294e-06  max_mem: 21478M
[01/17 16:22:38] d2.utils.events INFO:  eta: 1 day, 2:07:04  iter: 27259  total_loss: 32.85  loss_ce: 0.2348  loss_mask: 0.3442  loss_dice: 2.633  loss_ce_0: 0.5471  loss_mask_0: 0.3486  loss_dice_0: 2.762  loss_ce_1: 0.2764  loss_mask_1: 0.3471  loss_dice_1: 2.681  loss_ce_2: 0.2651  loss_mask_2: 0.3473  loss_dice_2: 2.657  loss_ce_3: 0.2618  loss_mask_3: 0.3453  loss_dice_3: 2.647  loss_ce_4: 0.2459  loss_mask_4: 0.344  loss_dice_4: 2.64  loss_ce_5: 0.238  loss_mask_5: 0.3424  loss_dice_5: 2.644  loss_ce_6: 0.233  loss_mask_6: 0.344  loss_dice_6: 2.642  loss_ce_7: 0.2544  loss_mask_7: 0.3432  loss_dice_7: 2.638  loss_ce_8: 0.244  loss_mask_8: 0.3448  loss_dice_8: 2.638  time: 1.5317  data_time: 0.0857  lr: 7.2273e-06  max_mem: 21478M
[01/17 16:23:07] d2.utils.events INFO:  eta: 1 day, 2:05:49  iter: 27279  total_loss: 33.1  loss_ce: 0.2241  loss_mask: 0.3386  loss_dice: 2.643  loss_ce_0: 0.5528  loss_mask_0: 0.345  loss_dice_0: 2.748  loss_ce_1: 0.2654  loss_mask_1: 0.3432  loss_dice_1: 2.679  loss_ce_2: 0.2509  loss_mask_2: 0.3369  loss_dice_2: 2.663  loss_ce_3: 0.2414  loss_mask_3: 0.3373  loss_dice_3: 2.649  loss_ce_4: 0.2445  loss_mask_4: 0.3391  loss_dice_4: 2.646  loss_ce_5: 0.2316  loss_mask_5: 0.34  loss_dice_5: 2.645  loss_ce_6: 0.2269  loss_mask_6: 0.339  loss_dice_6: 2.645  loss_ce_7: 0.2345  loss_mask_7: 0.3404  loss_dice_7: 2.644  loss_ce_8: 0.2195  loss_mask_8: 0.3379  loss_dice_8: 2.642  time: 1.5317  data_time: 0.0863  lr: 7.2253e-06  max_mem: 21478M
[01/17 16:23:37] d2.utils.events INFO:  eta: 1 day, 2:04:45  iter: 27299  total_loss: 33.52  loss_ce: 0.2655  loss_mask: 0.3468  loss_dice: 2.699  loss_ce_0: 0.5937  loss_mask_0: 0.3435  loss_dice_0: 2.788  loss_ce_1: 0.2804  loss_mask_1: 0.349  loss_dice_1: 2.738  loss_ce_2: 0.281  loss_mask_2: 0.3504  loss_dice_2: 2.714  loss_ce_3: 0.2771  loss_mask_3: 0.3488  loss_dice_3: 2.705  loss_ce_4: 0.2663  loss_mask_4: 0.3475  loss_dice_4: 2.702  loss_ce_5: 0.2722  loss_mask_5: 0.3467  loss_dice_5: 2.711  loss_ce_6: 0.2678  loss_mask_6: 0.3474  loss_dice_6: 2.707  loss_ce_7: 0.2712  loss_mask_7: 0.347  loss_dice_7: 2.701  loss_ce_8: 0.2578  loss_mask_8: 0.3463  loss_dice_8: 2.705  time: 1.5317  data_time: 0.0871  lr: 7.2232e-06  max_mem: 21478M
[01/17 16:24:07] d2.utils.events INFO:  eta: 1 day, 2:04:21  iter: 27319  total_loss: 33.52  loss_ce: 0.2699  loss_mask: 0.3447  loss_dice: 2.657  loss_ce_0: 0.5674  loss_mask_0: 0.3499  loss_dice_0: 2.751  loss_ce_1: 0.2873  loss_mask_1: 0.3504  loss_dice_1: 2.694  loss_ce_2: 0.2917  loss_mask_2: 0.3465  loss_dice_2: 2.661  loss_ce_3: 0.2801  loss_mask_3: 0.344  loss_dice_3: 2.656  loss_ce_4: 0.2781  loss_mask_4: 0.3445  loss_dice_4: 2.661  loss_ce_5: 0.2692  loss_mask_5: 0.3437  loss_dice_5: 2.651  loss_ce_6: 0.2808  loss_mask_6: 0.3438  loss_dice_6: 2.652  loss_ce_7: 0.271  loss_mask_7: 0.3443  loss_dice_7: 2.667  loss_ce_8: 0.2716  loss_mask_8: 0.3435  loss_dice_8: 2.658  time: 1.5316  data_time: 0.0916  lr: 7.2211e-06  max_mem: 21478M
[01/17 16:24:38] d2.utils.events INFO:  eta: 1 day, 2:05:04  iter: 27339  total_loss: 33.42  loss_ce: 0.2359  loss_mask: 0.3404  loss_dice: 2.687  loss_ce_0: 0.5576  loss_mask_0: 0.348  loss_dice_0: 2.797  loss_ce_1: 0.2813  loss_mask_1: 0.3449  loss_dice_1: 2.731  loss_ce_2: 0.2756  loss_mask_2: 0.3444  loss_dice_2: 2.707  loss_ce_3: 0.2643  loss_mask_3: 0.3407  loss_dice_3: 2.694  loss_ce_4: 0.2559  loss_mask_4: 0.3411  loss_dice_4: 2.686  loss_ce_5: 0.257  loss_mask_5: 0.3407  loss_dice_5: 2.695  loss_ce_6: 0.2322  loss_mask_6: 0.3412  loss_dice_6: 2.683  loss_ce_7: 0.2429  loss_mask_7: 0.3409  loss_dice_7: 2.688  loss_ce_8: 0.244  loss_mask_8: 0.3404  loss_dice_8: 2.688  time: 1.5316  data_time: 0.0966  lr: 7.219e-06  max_mem: 21478M
[01/17 16:25:09] d2.utils.events INFO:  eta: 1 day, 2:05:36  iter: 27359  total_loss: 33.06  loss_ce: 0.2583  loss_mask: 0.3351  loss_dice: 2.647  loss_ce_0: 0.5762  loss_mask_0: 0.3392  loss_dice_0: 2.775  loss_ce_1: 0.2947  loss_mask_1: 0.3374  loss_dice_1: 2.697  loss_ce_2: 0.2861  loss_mask_2: 0.3352  loss_dice_2: 2.673  loss_ce_3: 0.2748  loss_mask_3: 0.3319  loss_dice_3: 2.656  loss_ce_4: 0.2705  loss_mask_4: 0.3326  loss_dice_4: 2.653  loss_ce_5: 0.2699  loss_mask_5: 0.3343  loss_dice_5: 2.654  loss_ce_6: 0.2731  loss_mask_6: 0.3338  loss_dice_6: 2.656  loss_ce_7: 0.2538  loss_mask_7: 0.3324  loss_dice_7: 2.655  loss_ce_8: 0.2522  loss_mask_8: 0.334  loss_dice_8: 2.654  time: 1.5316  data_time: 0.0896  lr: 7.217e-06  max_mem: 21478M
[01/17 16:25:40] d2.utils.events INFO:  eta: 1 day, 2:05:34  iter: 27379  total_loss: 33.3  loss_ce: 0.2513  loss_mask: 0.3395  loss_dice: 2.672  loss_ce_0: 0.5264  loss_mask_0: 0.3487  loss_dice_0: 2.784  loss_ce_1: 0.2914  loss_mask_1: 0.3448  loss_dice_1: 2.716  loss_ce_2: 0.2728  loss_mask_2: 0.3408  loss_dice_2: 2.698  loss_ce_3: 0.2624  loss_mask_3: 0.341  loss_dice_3: 2.686  loss_ce_4: 0.247  loss_mask_4: 0.3379  loss_dice_4: 2.684  loss_ce_5: 0.2526  loss_mask_5: 0.3369  loss_dice_5: 2.686  loss_ce_6: 0.2382  loss_mask_6: 0.339  loss_dice_6: 2.673  loss_ce_7: 0.2419  loss_mask_7: 0.3375  loss_dice_7: 2.673  loss_ce_8: 0.2414  loss_mask_8: 0.339  loss_dice_8: 2.677  time: 1.5317  data_time: 0.0874  lr: 7.2149e-06  max_mem: 21478M
[01/17 16:26:11] d2.utils.events INFO:  eta: 1 day, 2:05:05  iter: 27399  total_loss: 33.2  loss_ce: 0.2513  loss_mask: 0.343  loss_dice: 2.646  loss_ce_0: 0.56  loss_mask_0: 0.3466  loss_dice_0: 2.754  loss_ce_1: 0.2814  loss_mask_1: 0.3473  loss_dice_1: 2.684  loss_ce_2: 0.2809  loss_mask_2: 0.3452  loss_dice_2: 2.661  loss_ce_3: 0.2684  loss_mask_3: 0.3451  loss_dice_3: 2.644  loss_ce_4: 0.253  loss_mask_4: 0.3457  loss_dice_4: 2.646  loss_ce_5: 0.2719  loss_mask_5: 0.3427  loss_dice_5: 2.647  loss_ce_6: 0.2587  loss_mask_6: 0.3441  loss_dice_6: 2.643  loss_ce_7: 0.2527  loss_mask_7: 0.3448  loss_dice_7: 2.646  loss_ce_8: 0.2461  loss_mask_8: 0.3435  loss_dice_8: 2.643  time: 1.5317  data_time: 0.0917  lr: 7.2128e-06  max_mem: 21478M
[01/17 16:26:41] d2.utils.events INFO:  eta: 1 day, 2:04:46  iter: 27419  total_loss: 33.14  loss_ce: 0.2312  loss_mask: 0.3508  loss_dice: 2.647  loss_ce_0: 0.5568  loss_mask_0: 0.3546  loss_dice_0: 2.764  loss_ce_1: 0.2722  loss_mask_1: 0.3593  loss_dice_1: 2.686  loss_ce_2: 0.2626  loss_mask_2: 0.3571  loss_dice_2: 2.66  loss_ce_3: 0.253  loss_mask_3: 0.3536  loss_dice_3: 2.645  loss_ce_4: 0.2516  loss_mask_4: 0.3533  loss_dice_4: 2.655  loss_ce_5: 0.243  loss_mask_5: 0.3527  loss_dice_5: 2.656  loss_ce_6: 0.2362  loss_mask_6: 0.3513  loss_dice_6: 2.651  loss_ce_7: 0.2493  loss_mask_7: 0.3505  loss_dice_7: 2.646  loss_ce_8: 0.2394  loss_mask_8: 0.3513  loss_dice_8: 2.654  time: 1.5317  data_time: 0.1046  lr: 7.2107e-06  max_mem: 21478M
[01/17 16:27:11] d2.utils.events INFO:  eta: 1 day, 2:04:00  iter: 27439  total_loss: 32.21  loss_ce: 0.2441  loss_mask: 0.348  loss_dice: 2.554  loss_ce_0: 0.5707  loss_mask_0: 0.3524  loss_dice_0: 2.655  loss_ce_1: 0.2821  loss_mask_1: 0.3509  loss_dice_1: 2.594  loss_ce_2: 0.2797  loss_mask_2: 0.3484  loss_dice_2: 2.56  loss_ce_3: 0.2602  loss_mask_3: 0.3478  loss_dice_3: 2.55  loss_ce_4: 0.2405  loss_mask_4: 0.3452  loss_dice_4: 2.563  loss_ce_5: 0.2338  loss_mask_5: 0.3465  loss_dice_5: 2.565  loss_ce_6: 0.2489  loss_mask_6: 0.3472  loss_dice_6: 2.556  loss_ce_7: 0.2409  loss_mask_7: 0.3475  loss_dice_7: 2.556  loss_ce_8: 0.2501  loss_mask_8: 0.3472  loss_dice_8: 2.543  time: 1.5316  data_time: 0.0865  lr: 7.2087e-06  max_mem: 21478M
[01/17 16:27:42] d2.utils.events INFO:  eta: 1 day, 2:04:05  iter: 27459  total_loss: 32.95  loss_ce: 0.2413  loss_mask: 0.3477  loss_dice: 2.68  loss_ce_0: 0.5487  loss_mask_0: 0.3464  loss_dice_0: 2.802  loss_ce_1: 0.2858  loss_mask_1: 0.3505  loss_dice_1: 2.731  loss_ce_2: 0.2696  loss_mask_2: 0.3502  loss_dice_2: 2.71  loss_ce_3: 0.2641  loss_mask_3: 0.3489  loss_dice_3: 2.698  loss_ce_4: 0.2493  loss_mask_4: 0.3492  loss_dice_4: 2.699  loss_ce_5: 0.2512  loss_mask_5: 0.3495  loss_dice_5: 2.693  loss_ce_6: 0.2457  loss_mask_6: 0.3476  loss_dice_6: 2.68  loss_ce_7: 0.2343  loss_mask_7: 0.3474  loss_dice_7: 2.695  loss_ce_8: 0.2379  loss_mask_8: 0.3467  loss_dice_8: 2.683  time: 1.5316  data_time: 0.0973  lr: 7.2066e-06  max_mem: 21478M
[01/17 16:28:13] d2.utils.events INFO:  eta: 1 day, 2:04:02  iter: 27479  total_loss: 32.95  loss_ce: 0.249  loss_mask: 0.3385  loss_dice: 2.643  loss_ce_0: 0.527  loss_mask_0: 0.343  loss_dice_0: 2.763  loss_ce_1: 0.2744  loss_mask_1: 0.345  loss_dice_1: 2.67  loss_ce_2: 0.2863  loss_mask_2: 0.3408  loss_dice_2: 2.674  loss_ce_3: 0.251  loss_mask_3: 0.3397  loss_dice_3: 2.656  loss_ce_4: 0.2513  loss_mask_4: 0.3417  loss_dice_4: 2.648  loss_ce_5: 0.2568  loss_mask_5: 0.3407  loss_dice_5: 2.652  loss_ce_6: 0.2423  loss_mask_6: 0.3405  loss_dice_6: 2.642  loss_ce_7: 0.2583  loss_mask_7: 0.3398  loss_dice_7: 2.643  loss_ce_8: 0.2582  loss_mask_8: 0.3401  loss_dice_8: 2.636  time: 1.5316  data_time: 0.0901  lr: 7.2045e-06  max_mem: 21478M
[01/17 16:28:43] d2.utils.events INFO:  eta: 1 day, 2:03:44  iter: 27499  total_loss: 32.66  loss_ce: 0.2322  loss_mask: 0.3412  loss_dice: 2.623  loss_ce_0: 0.554  loss_mask_0: 0.3453  loss_dice_0: 2.729  loss_ce_1: 0.2635  loss_mask_1: 0.3458  loss_dice_1: 2.666  loss_ce_2: 0.2757  loss_mask_2: 0.3454  loss_dice_2: 2.648  loss_ce_3: 0.2533  loss_mask_3: 0.3421  loss_dice_3: 2.631  loss_ce_4: 0.2441  loss_mask_4: 0.3425  loss_dice_4: 2.635  loss_ce_5: 0.2419  loss_mask_5: 0.3404  loss_dice_5: 2.639  loss_ce_6: 0.2317  loss_mask_6: 0.3415  loss_dice_6: 2.628  loss_ce_7: 0.2332  loss_mask_7: 0.3403  loss_dice_7: 2.626  loss_ce_8: 0.2213  loss_mask_8: 0.3418  loss_dice_8: 2.629  time: 1.5316  data_time: 0.0893  lr: 7.2024e-06  max_mem: 21478M
[01/17 16:29:14] d2.utils.events INFO:  eta: 1 day, 2:03:37  iter: 27519  total_loss: 33.56  loss_ce: 0.2605  loss_mask: 0.3519  loss_dice: 2.674  loss_ce_0: 0.5665  loss_mask_0: 0.357  loss_dice_0: 2.768  loss_ce_1: 0.293  loss_mask_1: 0.3549  loss_dice_1: 2.706  loss_ce_2: 0.2959  loss_mask_2: 0.3493  loss_dice_2: 2.686  loss_ce_3: 0.2814  loss_mask_3: 0.3493  loss_dice_3: 2.685  loss_ce_4: 0.2798  loss_mask_4: 0.3496  loss_dice_4: 2.667  loss_ce_5: 0.2671  loss_mask_5: 0.3491  loss_dice_5: 2.676  loss_ce_6: 0.2478  loss_mask_6: 0.3499  loss_dice_6: 2.676  loss_ce_7: 0.2669  loss_mask_7: 0.3502  loss_dice_7: 2.671  loss_ce_8: 0.2534  loss_mask_8: 0.3498  loss_dice_8: 2.662  time: 1.5316  data_time: 0.0931  lr: 7.2004e-06  max_mem: 21478M
[01/17 16:29:44] d2.utils.events INFO:  eta: 1 day, 2:03:07  iter: 27539  total_loss: 33.38  loss_ce: 0.2589  loss_mask: 0.3503  loss_dice: 2.683  loss_ce_0: 0.5693  loss_mask_0: 0.3582  loss_dice_0: 2.792  loss_ce_1: 0.2866  loss_mask_1: 0.3593  loss_dice_1: 2.728  loss_ce_2: 0.2774  loss_mask_2: 0.3508  loss_dice_2: 2.707  loss_ce_3: 0.2624  loss_mask_3: 0.3517  loss_dice_3: 2.687  loss_ce_4: 0.2484  loss_mask_4: 0.3483  loss_dice_4: 2.701  loss_ce_5: 0.2494  loss_mask_5: 0.3482  loss_dice_5: 2.692  loss_ce_6: 0.25  loss_mask_6: 0.3466  loss_dice_6: 2.694  loss_ce_7: 0.2471  loss_mask_7: 0.3463  loss_dice_7: 2.691  loss_ce_8: 0.2572  loss_mask_8: 0.348  loss_dice_8: 2.696  time: 1.5316  data_time: 0.0954  lr: 7.1983e-06  max_mem: 21478M
[01/17 16:30:15] d2.utils.events INFO:  eta: 1 day, 2:03:21  iter: 27559  total_loss: 33.92  loss_ce: 0.2693  loss_mask: 0.354  loss_dice: 2.71  loss_ce_0: 0.5669  loss_mask_0: 0.3685  loss_dice_0: 2.815  loss_ce_1: 0.2908  loss_mask_1: 0.3631  loss_dice_1: 2.752  loss_ce_2: 0.299  loss_mask_2: 0.3583  loss_dice_2: 2.735  loss_ce_3: 0.274  loss_mask_3: 0.3586  loss_dice_3: 2.726  loss_ce_4: 0.2821  loss_mask_4: 0.3563  loss_dice_4: 2.715  loss_ce_5: 0.271  loss_mask_5: 0.3557  loss_dice_5: 2.718  loss_ce_6: 0.2663  loss_mask_6: 0.3575  loss_dice_6: 2.713  loss_ce_7: 0.2651  loss_mask_7: 0.3594  loss_dice_7: 2.72  loss_ce_8: 0.2789  loss_mask_8: 0.3556  loss_dice_8: 2.703  time: 1.5316  data_time: 0.0995  lr: 7.1962e-06  max_mem: 21478M
[01/17 16:30:46] d2.utils.events INFO:  eta: 1 day, 2:03:08  iter: 27579  total_loss: 33.47  loss_ce: 0.2475  loss_mask: 0.3501  loss_dice: 2.693  loss_ce_0: 0.5709  loss_mask_0: 0.3659  loss_dice_0: 2.791  loss_ce_1: 0.2801  loss_mask_1: 0.3631  loss_dice_1: 2.728  loss_ce_2: 0.2859  loss_mask_2: 0.3579  loss_dice_2: 2.708  loss_ce_3: 0.2603  loss_mask_3: 0.3533  loss_dice_3: 2.699  loss_ce_4: 0.2482  loss_mask_4: 0.352  loss_dice_4: 2.699  loss_ce_5: 0.2417  loss_mask_5: 0.3538  loss_dice_5: 2.7  loss_ce_6: 0.2426  loss_mask_6: 0.3535  loss_dice_6: 2.698  loss_ce_7: 0.2436  loss_mask_7: 0.3509  loss_dice_7: 2.698  loss_ce_8: 0.2461  loss_mask_8: 0.3495  loss_dice_8: 2.696  time: 1.5316  data_time: 0.0899  lr: 7.1941e-06  max_mem: 21478M
[01/17 16:31:16] d2.utils.events INFO:  eta: 1 day, 2:02:38  iter: 27599  total_loss: 32.53  loss_ce: 0.2535  loss_mask: 0.3447  loss_dice: 2.579  loss_ce_0: 0.5299  loss_mask_0: 0.3503  loss_dice_0: 2.688  loss_ce_1: 0.2851  loss_mask_1: 0.3501  loss_dice_1: 2.616  loss_ce_2: 0.2661  loss_mask_2: 0.3475  loss_dice_2: 2.593  loss_ce_3: 0.2641  loss_mask_3: 0.3456  loss_dice_3: 2.585  loss_ce_4: 0.2453  loss_mask_4: 0.3465  loss_dice_4: 2.59  loss_ce_5: 0.255  loss_mask_5: 0.3468  loss_dice_5: 2.596  loss_ce_6: 0.2451  loss_mask_6: 0.347  loss_dice_6: 2.583  loss_ce_7: 0.2432  loss_mask_7: 0.3455  loss_dice_7: 2.584  loss_ce_8: 0.236  loss_mask_8: 0.3451  loss_dice_8: 2.579  time: 1.5316  data_time: 0.0891  lr: 7.1921e-06  max_mem: 21478M
[01/17 16:31:46] d2.utils.events INFO:  eta: 1 day, 2:02:02  iter: 27619  total_loss: 32.31  loss_ce: 0.2267  loss_mask: 0.344  loss_dice: 2.576  loss_ce_0: 0.533  loss_mask_0: 0.3418  loss_dice_0: 2.679  loss_ce_1: 0.2752  loss_mask_1: 0.3446  loss_dice_1: 2.618  loss_ce_2: 0.2483  loss_mask_2: 0.3427  loss_dice_2: 2.603  loss_ce_3: 0.2347  loss_mask_3: 0.3417  loss_dice_3: 2.584  loss_ce_4: 0.2217  loss_mask_4: 0.3451  loss_dice_4: 2.592  loss_ce_5: 0.2377  loss_mask_5: 0.3437  loss_dice_5: 2.58  loss_ce_6: 0.2203  loss_mask_6: 0.3426  loss_dice_6: 2.578  loss_ce_7: 0.2228  loss_mask_7: 0.3438  loss_dice_7: 2.579  loss_ce_8: 0.2364  loss_mask_8: 0.3437  loss_dice_8: 2.579  time: 1.5316  data_time: 0.0978  lr: 7.19e-06  max_mem: 21478M
[01/17 16:32:16] d2.utils.events INFO:  eta: 1 day, 2:01:03  iter: 27639  total_loss: 33.98  loss_ce: 0.2611  loss_mask: 0.3395  loss_dice: 2.737  loss_ce_0: 0.6  loss_mask_0: 0.3481  loss_dice_0: 2.837  loss_ce_1: 0.3141  loss_mask_1: 0.3457  loss_dice_1: 2.776  loss_ce_2: 0.2891  loss_mask_2: 0.3427  loss_dice_2: 2.752  loss_ce_3: 0.2706  loss_mask_3: 0.3411  loss_dice_3: 2.744  loss_ce_4: 0.2758  loss_mask_4: 0.3398  loss_dice_4: 2.746  loss_ce_5: 0.2545  loss_mask_5: 0.341  loss_dice_5: 2.743  loss_ce_6: 0.2616  loss_mask_6: 0.3412  loss_dice_6: 2.737  loss_ce_7: 0.2529  loss_mask_7: 0.3408  loss_dice_7: 2.73  loss_ce_8: 0.267  loss_mask_8: 0.3398  loss_dice_8: 2.732  time: 1.5315  data_time: 0.0968  lr: 7.1879e-06  max_mem: 21478M
[01/17 16:32:45] d2.utils.events INFO:  eta: 1 day, 2:00:51  iter: 27659  total_loss: 33.87  loss_ce: 0.2919  loss_mask: 0.3443  loss_dice: 2.69  loss_ce_0: 0.6132  loss_mask_0: 0.3469  loss_dice_0: 2.81  loss_ce_1: 0.3162  loss_mask_1: 0.3503  loss_dice_1: 2.729  loss_ce_2: 0.3066  loss_mask_2: 0.3424  loss_dice_2: 2.712  loss_ce_3: 0.2802  loss_mask_3: 0.3402  loss_dice_3: 2.693  loss_ce_4: 0.2811  loss_mask_4: 0.3413  loss_dice_4: 2.692  loss_ce_5: 0.2874  loss_mask_5: 0.3458  loss_dice_5: 2.699  loss_ce_6: 0.2831  loss_mask_6: 0.3434  loss_dice_6: 2.691  loss_ce_7: 0.2893  loss_mask_7: 0.3449  loss_dice_7: 2.683  loss_ce_8: 0.2725  loss_mask_8: 0.3438  loss_dice_8: 2.696  time: 1.5315  data_time: 0.0909  lr: 7.1858e-06  max_mem: 21478M
[01/17 16:33:16] d2.utils.events INFO:  eta: 1 day, 2:00:56  iter: 27679  total_loss: 33.75  loss_ce: 0.2411  loss_mask: 0.3384  loss_dice: 2.735  loss_ce_0: 0.5838  loss_mask_0: 0.3482  loss_dice_0: 2.843  loss_ce_1: 0.2885  loss_mask_1: 0.3471  loss_dice_1: 2.78  loss_ce_2: 0.2685  loss_mask_2: 0.3422  loss_dice_2: 2.751  loss_ce_3: 0.2466  loss_mask_3: 0.3396  loss_dice_3: 2.733  loss_ce_4: 0.2411  loss_mask_4: 0.3357  loss_dice_4: 2.739  loss_ce_5: 0.2552  loss_mask_5: 0.3371  loss_dice_5: 2.74  loss_ce_6: 0.2556  loss_mask_6: 0.3363  loss_dice_6: 2.729  loss_ce_7: 0.2422  loss_mask_7: 0.337  loss_dice_7: 2.733  loss_ce_8: 0.2481  loss_mask_8: 0.3363  loss_dice_8: 2.735  time: 1.5315  data_time: 0.0948  lr: 7.1838e-06  max_mem: 21478M
[01/17 16:33:46] d2.utils.events INFO:  eta: 1 day, 2:01:21  iter: 27699  total_loss: 34.13  loss_ce: 0.2444  loss_mask: 0.346  loss_dice: 2.73  loss_ce_0: 0.6008  loss_mask_0: 0.3651  loss_dice_0: 2.818  loss_ce_1: 0.2822  loss_mask_1: 0.3592  loss_dice_1: 2.772  loss_ce_2: 0.2806  loss_mask_2: 0.3532  loss_dice_2: 2.742  loss_ce_3: 0.2621  loss_mask_3: 0.3495  loss_dice_3: 2.729  loss_ce_4: 0.2625  loss_mask_4: 0.3485  loss_dice_4: 2.729  loss_ce_5: 0.2575  loss_mask_5: 0.3491  loss_dice_5: 2.736  loss_ce_6: 0.2508  loss_mask_6: 0.3473  loss_dice_6: 2.735  loss_ce_7: 0.2552  loss_mask_7: 0.3454  loss_dice_7: 2.729  loss_ce_8: 0.2497  loss_mask_8: 0.3451  loss_dice_8: 2.731  time: 1.5315  data_time: 0.0855  lr: 7.1817e-06  max_mem: 21478M
[01/17 16:34:17] d2.utils.events INFO:  eta: 1 day, 2:01:21  iter: 27719  total_loss: 34.23  loss_ce: 0.2592  loss_mask: 0.3432  loss_dice: 2.737  loss_ce_0: 0.5551  loss_mask_0: 0.3552  loss_dice_0: 2.821  loss_ce_1: 0.2897  loss_mask_1: 0.3481  loss_dice_1: 2.768  loss_ce_2: 0.2947  loss_mask_2: 0.3409  loss_dice_2: 2.745  loss_ce_3: 0.2839  loss_mask_3: 0.3399  loss_dice_3: 2.741  loss_ce_4: 0.2815  loss_mask_4: 0.3407  loss_dice_4: 2.738  loss_ce_5: 0.2712  loss_mask_5: 0.342  loss_dice_5: 2.734  loss_ce_6: 0.2775  loss_mask_6: 0.3404  loss_dice_6: 2.726  loss_ce_7: 0.2646  loss_mask_7: 0.3423  loss_dice_7: 2.729  loss_ce_8: 0.2715  loss_mask_8: 0.3419  loss_dice_8: 2.726  time: 1.5315  data_time: 0.0981  lr: 7.1796e-06  max_mem: 21478M
[01/17 16:34:46] d2.utils.events INFO:  eta: 1 day, 2:00:15  iter: 27739  total_loss: 33.05  loss_ce: 0.247  loss_mask: 0.342  loss_dice: 2.647  loss_ce_0: 0.5745  loss_mask_0: 0.3528  loss_dice_0: 2.764  loss_ce_1: 0.3021  loss_mask_1: 0.3492  loss_dice_1: 2.693  loss_ce_2: 0.2795  loss_mask_2: 0.3452  loss_dice_2: 2.672  loss_ce_3: 0.2684  loss_mask_3: 0.3448  loss_dice_3: 2.66  loss_ce_4: 0.2596  loss_mask_4: 0.3436  loss_dice_4: 2.661  loss_ce_5: 0.251  loss_mask_5: 0.3413  loss_dice_5: 2.669  loss_ce_6: 0.2519  loss_mask_6: 0.3409  loss_dice_6: 2.659  loss_ce_7: 0.251  loss_mask_7: 0.3404  loss_dice_7: 2.661  loss_ce_8: 0.2588  loss_mask_8: 0.3415  loss_dice_8: 2.656  time: 1.5314  data_time: 0.0911  lr: 7.1775e-06  max_mem: 21478M
[01/17 16:35:18] d2.utils.events INFO:  eta: 1 day, 2:00:57  iter: 27759  total_loss: 33.76  loss_ce: 0.2646  loss_mask: 0.3409  loss_dice: 2.697  loss_ce_0: 0.5578  loss_mask_0: 0.3504  loss_dice_0: 2.803  loss_ce_1: 0.307  loss_mask_1: 0.3468  loss_dice_1: 2.732  loss_ce_2: 0.3028  loss_mask_2: 0.3422  loss_dice_2: 2.719  loss_ce_3: 0.2887  loss_mask_3: 0.3378  loss_dice_3: 2.699  loss_ce_4: 0.2669  loss_mask_4: 0.339  loss_dice_4: 2.702  loss_ce_5: 0.2561  loss_mask_5: 0.3424  loss_dice_5: 2.708  loss_ce_6: 0.2648  loss_mask_6: 0.3423  loss_dice_6: 2.708  loss_ce_7: 0.2603  loss_mask_7: 0.3413  loss_dice_7: 2.699  loss_ce_8: 0.2787  loss_mask_8: 0.3414  loss_dice_8: 2.7  time: 1.5315  data_time: 0.1094  lr: 7.1755e-06  max_mem: 21478M
[01/17 16:35:47] d2.utils.events INFO:  eta: 1 day, 1:59:42  iter: 27779  total_loss: 32.91  loss_ce: 0.2348  loss_mask: 0.3401  loss_dice: 2.645  loss_ce_0: 0.5537  loss_mask_0: 0.3425  loss_dice_0: 2.764  loss_ce_1: 0.2758  loss_mask_1: 0.3402  loss_dice_1: 2.687  loss_ce_2: 0.2588  loss_mask_2: 0.3349  loss_dice_2: 2.659  loss_ce_3: 0.253  loss_mask_3: 0.3396  loss_dice_3: 2.65  loss_ce_4: 0.2465  loss_mask_4: 0.3377  loss_dice_4: 2.648  loss_ce_5: 0.2357  loss_mask_5: 0.3376  loss_dice_5: 2.655  loss_ce_6: 0.2357  loss_mask_6: 0.3366  loss_dice_6: 2.641  loss_ce_7: 0.2354  loss_mask_7: 0.3402  loss_dice_7: 2.649  loss_ce_8: 0.2274  loss_mask_8: 0.3389  loss_dice_8: 2.641  time: 1.5314  data_time: 0.0907  lr: 7.1734e-06  max_mem: 21478M
[01/17 16:36:18] d2.utils.events INFO:  eta: 1 day, 1:59:43  iter: 27799  total_loss: 33.16  loss_ce: 0.2333  loss_mask: 0.3341  loss_dice: 2.725  loss_ce_0: 0.5637  loss_mask_0: 0.3408  loss_dice_0: 2.821  loss_ce_1: 0.2678  loss_mask_1: 0.3378  loss_dice_1: 2.758  loss_ce_2: 0.2576  loss_mask_2: 0.3359  loss_dice_2: 2.746  loss_ce_3: 0.2435  loss_mask_3: 0.3341  loss_dice_3: 2.729  loss_ce_4: 0.2491  loss_mask_4: 0.3337  loss_dice_4: 2.729  loss_ce_5: 0.2382  loss_mask_5: 0.3343  loss_dice_5: 2.72  loss_ce_6: 0.2418  loss_mask_6: 0.3351  loss_dice_6: 2.719  loss_ce_7: 0.241  loss_mask_7: 0.3361  loss_dice_7: 2.727  loss_ce_8: 0.2426  loss_mask_8: 0.3343  loss_dice_8: 2.716  time: 1.5314  data_time: 0.0940  lr: 7.1713e-06  max_mem: 21478M
[01/17 16:36:48] d2.utils.events INFO:  eta: 1 day, 2:00:06  iter: 27819  total_loss: 33.4  loss_ce: 0.2536  loss_mask: 0.3458  loss_dice: 2.649  loss_ce_0: 0.5758  loss_mask_0: 0.3528  loss_dice_0: 2.762  loss_ce_1: 0.2956  loss_mask_1: 0.3534  loss_dice_1: 2.689  loss_ce_2: 0.2931  loss_mask_2: 0.3503  loss_dice_2: 2.672  loss_ce_3: 0.2706  loss_mask_3: 0.348  loss_dice_3: 2.65  loss_ce_4: 0.2761  loss_mask_4: 0.349  loss_dice_4: 2.649  loss_ce_5: 0.269  loss_mask_5: 0.3479  loss_dice_5: 2.66  loss_ce_6: 0.2573  loss_mask_6: 0.3471  loss_dice_6: 2.656  loss_ce_7: 0.2449  loss_mask_7: 0.3464  loss_dice_7: 2.653  loss_ce_8: 0.2537  loss_mask_8: 0.3453  loss_dice_8: 2.653  time: 1.5314  data_time: 0.0930  lr: 7.1692e-06  max_mem: 21478M
[01/17 16:37:18] d2.utils.events INFO:  eta: 1 day, 1:59:50  iter: 27839  total_loss: 33.15  loss_ce: 0.2434  loss_mask: 0.3471  loss_dice: 2.654  loss_ce_0: 0.5933  loss_mask_0: 0.358  loss_dice_0: 2.752  loss_ce_1: 0.2919  loss_mask_1: 0.3573  loss_dice_1: 2.694  loss_ce_2: 0.2714  loss_mask_2: 0.3537  loss_dice_2: 2.673  loss_ce_3: 0.2651  loss_mask_3: 0.3497  loss_dice_3: 2.663  loss_ce_4: 0.2552  loss_mask_4: 0.3464  loss_dice_4: 2.661  loss_ce_5: 0.2527  loss_mask_5: 0.3476  loss_dice_5: 2.661  loss_ce_6: 0.2387  loss_mask_6: 0.3477  loss_dice_6: 2.653  loss_ce_7: 0.2475  loss_mask_7: 0.3475  loss_dice_7: 2.656  loss_ce_8: 0.2532  loss_mask_8: 0.3464  loss_dice_8: 2.655  time: 1.5314  data_time: 0.0841  lr: 7.1672e-06  max_mem: 21478M
[01/17 16:37:48] d2.utils.events INFO:  eta: 1 day, 1:59:33  iter: 27859  total_loss: 33.88  loss_ce: 0.2405  loss_mask: 0.3454  loss_dice: 2.725  loss_ce_0: 0.5661  loss_mask_0: 0.3503  loss_dice_0: 2.803  loss_ce_1: 0.2816  loss_mask_1: 0.3462  loss_dice_1: 2.753  loss_ce_2: 0.2715  loss_mask_2: 0.3436  loss_dice_2: 2.74  loss_ce_3: 0.265  loss_mask_3: 0.3419  loss_dice_3: 2.726  loss_ce_4: 0.2483  loss_mask_4: 0.3442  loss_dice_4: 2.726  loss_ce_5: 0.2437  loss_mask_5: 0.3436  loss_dice_5: 2.721  loss_ce_6: 0.236  loss_mask_6: 0.3439  loss_dice_6: 2.717  loss_ce_7: 0.2384  loss_mask_7: 0.3452  loss_dice_7: 2.718  loss_ce_8: 0.2352  loss_mask_8: 0.3448  loss_dice_8: 2.723  time: 1.5314  data_time: 0.0931  lr: 7.1651e-06  max_mem: 21478M
[01/17 16:38:18] d2.utils.events INFO:  eta: 1 day, 1:59:02  iter: 27879  total_loss: 33.48  loss_ce: 0.2602  loss_mask: 0.3453  loss_dice: 2.662  loss_ce_0: 0.5465  loss_mask_0: 0.3578  loss_dice_0: 2.758  loss_ce_1: 0.285  loss_mask_1: 0.3559  loss_dice_1: 2.693  loss_ce_2: 0.2826  loss_mask_2: 0.35  loss_dice_2: 2.678  loss_ce_3: 0.2934  loss_mask_3: 0.3453  loss_dice_3: 2.664  loss_ce_4: 0.2722  loss_mask_4: 0.3431  loss_dice_4: 2.662  loss_ce_5: 0.2724  loss_mask_5: 0.346  loss_dice_5: 2.674  loss_ce_6: 0.2688  loss_mask_6: 0.3444  loss_dice_6: 2.662  loss_ce_7: 0.2671  loss_mask_7: 0.3428  loss_dice_7: 2.657  loss_ce_8: 0.2603  loss_mask_8: 0.3458  loss_dice_8: 2.668  time: 1.5313  data_time: 0.0852  lr: 7.163e-06  max_mem: 21478M
[01/17 16:38:48] d2.utils.events INFO:  eta: 1 day, 1:58:06  iter: 27899  total_loss: 33.1  loss_ce: 0.2506  loss_mask: 0.3467  loss_dice: 2.667  loss_ce_0: 0.5798  loss_mask_0: 0.3555  loss_dice_0: 2.776  loss_ce_1: 0.2986  loss_mask_1: 0.3514  loss_dice_1: 2.718  loss_ce_2: 0.2822  loss_mask_2: 0.3499  loss_dice_2: 2.694  loss_ce_3: 0.2709  loss_mask_3: 0.3501  loss_dice_3: 2.678  loss_ce_4: 0.2603  loss_mask_4: 0.3496  loss_dice_4: 2.684  loss_ce_5: 0.2558  loss_mask_5: 0.3482  loss_dice_5: 2.682  loss_ce_6: 0.2578  loss_mask_6: 0.3495  loss_dice_6: 2.673  loss_ce_7: 0.2602  loss_mask_7: 0.3479  loss_dice_7: 2.665  loss_ce_8: 0.264  loss_mask_8: 0.3469  loss_dice_8: 2.67  time: 1.5313  data_time: 0.0835  lr: 7.1609e-06  max_mem: 21478M
[01/17 16:39:18] d2.utils.events INFO:  eta: 1 day, 1:57:28  iter: 27919  total_loss: 32.78  loss_ce: 0.2366  loss_mask: 0.3392  loss_dice: 2.599  loss_ce_0: 0.5647  loss_mask_0: 0.3464  loss_dice_0: 2.691  loss_ce_1: 0.2796  loss_mask_1: 0.3454  loss_dice_1: 2.625  loss_ce_2: 0.2686  loss_mask_2: 0.3427  loss_dice_2: 2.605  loss_ce_3: 0.2501  loss_mask_3: 0.3413  loss_dice_3: 2.595  loss_ce_4: 0.2447  loss_mask_4: 0.3417  loss_dice_4: 2.597  loss_ce_5: 0.2475  loss_mask_5: 0.3395  loss_dice_5: 2.595  loss_ce_6: 0.2411  loss_mask_6: 0.3395  loss_dice_6: 2.587  loss_ce_7: 0.2343  loss_mask_7: 0.3401  loss_dice_7: 2.589  loss_ce_8: 0.2454  loss_mask_8: 0.3404  loss_dice_8: 2.592  time: 1.5313  data_time: 0.0875  lr: 7.1589e-06  max_mem: 21478M
[01/17 16:39:48] d2.utils.events INFO:  eta: 1 day, 1:57:32  iter: 27939  total_loss: 33.2  loss_ce: 0.2402  loss_mask: 0.3321  loss_dice: 2.67  loss_ce_0: 0.5632  loss_mask_0: 0.3339  loss_dice_0: 2.771  loss_ce_1: 0.2769  loss_mask_1: 0.3376  loss_dice_1: 2.707  loss_ce_2: 0.2722  loss_mask_2: 0.333  loss_dice_2: 2.688  loss_ce_3: 0.2482  loss_mask_3: 0.3313  loss_dice_3: 2.689  loss_ce_4: 0.2556  loss_mask_4: 0.3325  loss_dice_4: 2.681  loss_ce_5: 0.2599  loss_mask_5: 0.3333  loss_dice_5: 2.682  loss_ce_6: 0.2325  loss_mask_6: 0.3328  loss_dice_6: 2.677  loss_ce_7: 0.2459  loss_mask_7: 0.3316  loss_dice_7: 2.672  loss_ce_8: 0.2528  loss_mask_8: 0.331  loss_dice_8: 2.667  time: 1.5313  data_time: 0.0969  lr: 7.1568e-06  max_mem: 21478M
[01/17 16:40:19] d2.utils.events INFO:  eta: 1 day, 1:57:49  iter: 27959  total_loss: 33.05  loss_ce: 0.2592  loss_mask: 0.3383  loss_dice: 2.635  loss_ce_0: 0.579  loss_mask_0: 0.3418  loss_dice_0: 2.751  loss_ce_1: 0.2832  loss_mask_1: 0.3422  loss_dice_1: 2.682  loss_ce_2: 0.2856  loss_mask_2: 0.3404  loss_dice_2: 2.65  loss_ce_3: 0.2648  loss_mask_3: 0.3359  loss_dice_3: 2.638  loss_ce_4: 0.2612  loss_mask_4: 0.3367  loss_dice_4: 2.648  loss_ce_5: 0.2628  loss_mask_5: 0.3388  loss_dice_5: 2.643  loss_ce_6: 0.2609  loss_mask_6: 0.3384  loss_dice_6: 2.636  loss_ce_7: 0.2446  loss_mask_7: 0.3373  loss_dice_7: 2.641  loss_ce_8: 0.2571  loss_mask_8: 0.3378  loss_dice_8: 2.633  time: 1.5313  data_time: 0.0986  lr: 7.1547e-06  max_mem: 21478M
[01/17 16:40:49] d2.utils.events INFO:  eta: 1 day, 1:57:18  iter: 27979  total_loss: 32.79  loss_ce: 0.2506  loss_mask: 0.3488  loss_dice: 2.619  loss_ce_0: 0.5462  loss_mask_0: 0.353  loss_dice_0: 2.717  loss_ce_1: 0.3063  loss_mask_1: 0.3544  loss_dice_1: 2.65  loss_ce_2: 0.2733  loss_mask_2: 0.3512  loss_dice_2: 2.639  loss_ce_3: 0.2706  loss_mask_3: 0.351  loss_dice_3: 2.623  loss_ce_4: 0.271  loss_mask_4: 0.3499  loss_dice_4: 2.618  loss_ce_5: 0.2558  loss_mask_5: 0.3499  loss_dice_5: 2.622  loss_ce_6: 0.246  loss_mask_6: 0.3519  loss_dice_6: 2.612  loss_ce_7: 0.2529  loss_mask_7: 0.3521  loss_dice_7: 2.615  loss_ce_8: 0.2597  loss_mask_8: 0.3499  loss_dice_8: 2.62  time: 1.5312  data_time: 0.0882  lr: 7.1526e-06  max_mem: 21478M
[01/17 16:41:19] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 16:41:19] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 16:41:19] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 16:41:20] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 16:41:34] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0102 s/iter. Inference: 0.1690 s/iter. Eval: 0.2502 s/iter. Total: 0.4294 s/iter. ETA=0:07:44
[01/17 16:41:40] d2.evaluation.evaluator INFO: Inference done 22/1093. Dataloading: 0.0134 s/iter. Inference: 0.1787 s/iter. Eval: 0.2647 s/iter. Total: 0.4569 s/iter. ETA=0:08:09
[01/17 16:41:45] d2.evaluation.evaluator INFO: Inference done 35/1093. Dataloading: 0.0125 s/iter. Inference: 0.1697 s/iter. Eval: 0.2480 s/iter. Total: 0.4303 s/iter. ETA=0:07:35
[01/17 16:41:50] d2.evaluation.evaluator INFO: Inference done 47/1093. Dataloading: 0.0133 s/iter. Inference: 0.1727 s/iter. Eval: 0.2473 s/iter. Total: 0.4333 s/iter. ETA=0:07:33
[01/17 16:41:55] d2.evaluation.evaluator INFO: Inference done 58/1093. Dataloading: 0.0136 s/iter. Inference: 0.1702 s/iter. Eval: 0.2546 s/iter. Total: 0.4385 s/iter. ETA=0:07:33
[01/17 16:42:00] d2.evaluation.evaluator INFO: Inference done 68/1093. Dataloading: 0.0142 s/iter. Inference: 0.1715 s/iter. Eval: 0.2630 s/iter. Total: 0.4488 s/iter. ETA=0:07:40
[01/17 16:42:05] d2.evaluation.evaluator INFO: Inference done 81/1093. Dataloading: 0.0139 s/iter. Inference: 0.1700 s/iter. Eval: 0.2578 s/iter. Total: 0.4418 s/iter. ETA=0:07:27
[01/17 16:42:11] d2.evaluation.evaluator INFO: Inference done 92/1093. Dataloading: 0.0142 s/iter. Inference: 0.1702 s/iter. Eval: 0.2607 s/iter. Total: 0.4452 s/iter. ETA=0:07:25
[01/17 16:42:16] d2.evaluation.evaluator INFO: Inference done 105/1093. Dataloading: 0.0141 s/iter. Inference: 0.1700 s/iter. Eval: 0.2552 s/iter. Total: 0.4394 s/iter. ETA=0:07:14
[01/17 16:42:21] d2.evaluation.evaluator INFO: Inference done 118/1093. Dataloading: 0.0140 s/iter. Inference: 0.1703 s/iter. Eval: 0.2518 s/iter. Total: 0.4362 s/iter. ETA=0:07:05
[01/17 16:42:26] d2.evaluation.evaluator INFO: Inference done 129/1093. Dataloading: 0.0140 s/iter. Inference: 0.1708 s/iter. Eval: 0.2539 s/iter. Total: 0.4388 s/iter. ETA=0:07:03
[01/17 16:42:32] d2.evaluation.evaluator INFO: Inference done 143/1093. Dataloading: 0.0137 s/iter. Inference: 0.1709 s/iter. Eval: 0.2477 s/iter. Total: 0.4324 s/iter. ETA=0:06:50
[01/17 16:42:37] d2.evaluation.evaluator INFO: Inference done 157/1093. Dataloading: 0.0137 s/iter. Inference: 0.1708 s/iter. Eval: 0.2432 s/iter. Total: 0.4277 s/iter. ETA=0:06:40
[01/17 16:42:42] d2.evaluation.evaluator INFO: Inference done 169/1093. Dataloading: 0.0137 s/iter. Inference: 0.1711 s/iter. Eval: 0.2438 s/iter. Total: 0.4288 s/iter. ETA=0:06:36
[01/17 16:42:47] d2.evaluation.evaluator INFO: Inference done 183/1093. Dataloading: 0.0136 s/iter. Inference: 0.1699 s/iter. Eval: 0.2405 s/iter. Total: 0.4241 s/iter. ETA=0:06:25
[01/17 16:42:53] d2.evaluation.evaluator INFO: Inference done 196/1093. Dataloading: 0.0136 s/iter. Inference: 0.1699 s/iter. Eval: 0.2391 s/iter. Total: 0.4226 s/iter. ETA=0:06:19
[01/17 16:42:58] d2.evaluation.evaluator INFO: Inference done 208/1093. Dataloading: 0.0136 s/iter. Inference: 0.1697 s/iter. Eval: 0.2395 s/iter. Total: 0.4229 s/iter. ETA=0:06:14
[01/17 16:43:03] d2.evaluation.evaluator INFO: Inference done 221/1093. Dataloading: 0.0135 s/iter. Inference: 0.1695 s/iter. Eval: 0.2381 s/iter. Total: 0.4213 s/iter. ETA=0:06:07
[01/17 16:43:08] d2.evaluation.evaluator INFO: Inference done 233/1093. Dataloading: 0.0134 s/iter. Inference: 0.1690 s/iter. Eval: 0.2390 s/iter. Total: 0.4216 s/iter. ETA=0:06:02
[01/17 16:43:13] d2.evaluation.evaluator INFO: Inference done 246/1093. Dataloading: 0.0135 s/iter. Inference: 0.1683 s/iter. Eval: 0.2379 s/iter. Total: 0.4199 s/iter. ETA=0:05:55
[01/17 16:43:18] d2.evaluation.evaluator INFO: Inference done 257/1093. Dataloading: 0.0137 s/iter. Inference: 0.1693 s/iter. Eval: 0.2399 s/iter. Total: 0.4230 s/iter. ETA=0:05:53
[01/17 16:43:24] d2.evaluation.evaluator INFO: Inference done 271/1093. Dataloading: 0.0136 s/iter. Inference: 0.1681 s/iter. Eval: 0.2384 s/iter. Total: 0.4202 s/iter. ETA=0:05:45
[01/17 16:43:29] d2.evaluation.evaluator INFO: Inference done 285/1093. Dataloading: 0.0136 s/iter. Inference: 0.1671 s/iter. Eval: 0.2370 s/iter. Total: 0.4178 s/iter. ETA=0:05:37
[01/17 16:43:34] d2.evaluation.evaluator INFO: Inference done 297/1093. Dataloading: 0.0137 s/iter. Inference: 0.1681 s/iter. Eval: 0.2367 s/iter. Total: 0.4186 s/iter. ETA=0:05:33
[01/17 16:43:39] d2.evaluation.evaluator INFO: Inference done 308/1093. Dataloading: 0.0137 s/iter. Inference: 0.1678 s/iter. Eval: 0.2384 s/iter. Total: 0.4200 s/iter. ETA=0:05:29
[01/17 16:43:44] d2.evaluation.evaluator INFO: Inference done 319/1093. Dataloading: 0.0137 s/iter. Inference: 0.1679 s/iter. Eval: 0.2399 s/iter. Total: 0.4217 s/iter. ETA=0:05:26
[01/17 16:43:49] d2.evaluation.evaluator INFO: Inference done 333/1093. Dataloading: 0.0137 s/iter. Inference: 0.1677 s/iter. Eval: 0.2376 s/iter. Total: 0.4191 s/iter. ETA=0:05:18
[01/17 16:43:55] d2.evaluation.evaluator INFO: Inference done 349/1093. Dataloading: 0.0135 s/iter. Inference: 0.1672 s/iter. Eval: 0.2339 s/iter. Total: 0.4148 s/iter. ETA=0:05:08
[01/17 16:44:00] d2.evaluation.evaluator INFO: Inference done 362/1093. Dataloading: 0.0135 s/iter. Inference: 0.1671 s/iter. Eval: 0.2336 s/iter. Total: 0.4142 s/iter. ETA=0:05:02
[01/17 16:44:05] d2.evaluation.evaluator INFO: Inference done 376/1093. Dataloading: 0.0134 s/iter. Inference: 0.1672 s/iter. Eval: 0.2322 s/iter. Total: 0.4129 s/iter. ETA=0:04:56
[01/17 16:44:10] d2.evaluation.evaluator INFO: Inference done 388/1093. Dataloading: 0.0134 s/iter. Inference: 0.1672 s/iter. Eval: 0.2324 s/iter. Total: 0.4131 s/iter. ETA=0:04:51
[01/17 16:44:15] d2.evaluation.evaluator INFO: Inference done 401/1093. Dataloading: 0.0134 s/iter. Inference: 0.1669 s/iter. Eval: 0.2319 s/iter. Total: 0.4124 s/iter. ETA=0:04:45
[01/17 16:44:20] d2.evaluation.evaluator INFO: Inference done 415/1093. Dataloading: 0.0134 s/iter. Inference: 0.1665 s/iter. Eval: 0.2310 s/iter. Total: 0.4110 s/iter. ETA=0:04:38
[01/17 16:44:26] d2.evaluation.evaluator INFO: Inference done 426/1093. Dataloading: 0.0134 s/iter. Inference: 0.1665 s/iter. Eval: 0.2328 s/iter. Total: 0.4128 s/iter. ETA=0:04:35
[01/17 16:44:31] d2.evaluation.evaluator INFO: Inference done 440/1093. Dataloading: 0.0134 s/iter. Inference: 0.1666 s/iter. Eval: 0.2311 s/iter. Total: 0.4112 s/iter. ETA=0:04:28
[01/17 16:44:36] d2.evaluation.evaluator INFO: Inference done 455/1093. Dataloading: 0.0133 s/iter. Inference: 0.1660 s/iter. Eval: 0.2296 s/iter. Total: 0.4090 s/iter. ETA=0:04:20
[01/17 16:44:41] d2.evaluation.evaluator INFO: Inference done 469/1093. Dataloading: 0.0132 s/iter. Inference: 0.1658 s/iter. Eval: 0.2289 s/iter. Total: 0.4080 s/iter. ETA=0:04:14
[01/17 16:44:46] d2.evaluation.evaluator INFO: Inference done 484/1093. Dataloading: 0.0131 s/iter. Inference: 0.1654 s/iter. Eval: 0.2276 s/iter. Total: 0.4062 s/iter. ETA=0:04:07
[01/17 16:44:52] d2.evaluation.evaluator INFO: Inference done 499/1093. Dataloading: 0.0131 s/iter. Inference: 0.1652 s/iter. Eval: 0.2260 s/iter. Total: 0.4044 s/iter. ETA=0:04:00
[01/17 16:44:57] d2.evaluation.evaluator INFO: Inference done 515/1093. Dataloading: 0.0130 s/iter. Inference: 0.1650 s/iter. Eval: 0.2241 s/iter. Total: 0.4022 s/iter. ETA=0:03:52
[01/17 16:45:02] d2.evaluation.evaluator INFO: Inference done 527/1093. Dataloading: 0.0130 s/iter. Inference: 0.1650 s/iter. Eval: 0.2248 s/iter. Total: 0.4029 s/iter. ETA=0:03:48
[01/17 16:45:08] d2.evaluation.evaluator INFO: Inference done 541/1093. Dataloading: 0.0130 s/iter. Inference: 0.1648 s/iter. Eval: 0.2244 s/iter. Total: 0.4024 s/iter. ETA=0:03:42
[01/17 16:45:13] d2.evaluation.evaluator INFO: Inference done 554/1093. Dataloading: 0.0130 s/iter. Inference: 0.1645 s/iter. Eval: 0.2244 s/iter. Total: 0.4021 s/iter. ETA=0:03:36
[01/17 16:45:18] d2.evaluation.evaluator INFO: Inference done 568/1093. Dataloading: 0.0130 s/iter. Inference: 0.1644 s/iter. Eval: 0.2238 s/iter. Total: 0.4013 s/iter. ETA=0:03:30
[01/17 16:45:23] d2.evaluation.evaluator INFO: Inference done 585/1093. Dataloading: 0.0131 s/iter. Inference: 0.1641 s/iter. Eval: 0.2214 s/iter. Total: 0.3987 s/iter. ETA=0:03:22
[01/17 16:45:28] d2.evaluation.evaluator INFO: Inference done 598/1093. Dataloading: 0.0130 s/iter. Inference: 0.1641 s/iter. Eval: 0.2212 s/iter. Total: 0.3985 s/iter. ETA=0:03:17
[01/17 16:45:33] d2.evaluation.evaluator INFO: Inference done 610/1093. Dataloading: 0.0131 s/iter. Inference: 0.1645 s/iter. Eval: 0.2214 s/iter. Total: 0.3990 s/iter. ETA=0:03:12
[01/17 16:45:39] d2.evaluation.evaluator INFO: Inference done 625/1093. Dataloading: 0.0130 s/iter. Inference: 0.1643 s/iter. Eval: 0.2208 s/iter. Total: 0.3982 s/iter. ETA=0:03:06
[01/17 16:45:44] d2.evaluation.evaluator INFO: Inference done 639/1093. Dataloading: 0.0130 s/iter. Inference: 0.1642 s/iter. Eval: 0.2200 s/iter. Total: 0.3973 s/iter. ETA=0:03:00
[01/17 16:45:49] d2.evaluation.evaluator INFO: Inference done 655/1093. Dataloading: 0.0130 s/iter. Inference: 0.1639 s/iter. Eval: 0.2187 s/iter. Total: 0.3957 s/iter. ETA=0:02:53
[01/17 16:45:54] d2.evaluation.evaluator INFO: Inference done 669/1093. Dataloading: 0.0129 s/iter. Inference: 0.1638 s/iter. Eval: 0.2182 s/iter. Total: 0.3951 s/iter. ETA=0:02:47
[01/17 16:45:59] d2.evaluation.evaluator INFO: Inference done 686/1093. Dataloading: 0.0128 s/iter. Inference: 0.1635 s/iter. Eval: 0.2165 s/iter. Total: 0.3929 s/iter. ETA=0:02:39
[01/17 16:46:05] d2.evaluation.evaluator INFO: Inference done 700/1093. Dataloading: 0.0128 s/iter. Inference: 0.1633 s/iter. Eval: 0.2161 s/iter. Total: 0.3924 s/iter. ETA=0:02:34
[01/17 16:46:10] d2.evaluation.evaluator INFO: Inference done 710/1093. Dataloading: 0.0129 s/iter. Inference: 0.1634 s/iter. Eval: 0.2177 s/iter. Total: 0.3941 s/iter. ETA=0:02:30
[01/17 16:46:15] d2.evaluation.evaluator INFO: Inference done 723/1093. Dataloading: 0.0129 s/iter. Inference: 0.1634 s/iter. Eval: 0.2179 s/iter. Total: 0.3943 s/iter. ETA=0:02:25
[01/17 16:46:21] d2.evaluation.evaluator INFO: Inference done 738/1093. Dataloading: 0.0132 s/iter. Inference: 0.1635 s/iter. Eval: 0.2169 s/iter. Total: 0.3938 s/iter. ETA=0:02:19
[01/17 16:46:26] d2.evaluation.evaluator INFO: Inference done 753/1093. Dataloading: 0.0132 s/iter. Inference: 0.1634 s/iter. Eval: 0.2161 s/iter. Total: 0.3928 s/iter. ETA=0:02:13
[01/17 16:46:31] d2.evaluation.evaluator INFO: Inference done 766/1093. Dataloading: 0.0132 s/iter. Inference: 0.1635 s/iter. Eval: 0.2163 s/iter. Total: 0.3932 s/iter. ETA=0:02:08
[01/17 16:46:36] d2.evaluation.evaluator INFO: Inference done 780/1093. Dataloading: 0.0131 s/iter. Inference: 0.1637 s/iter. Eval: 0.2158 s/iter. Total: 0.3927 s/iter. ETA=0:02:02
[01/17 16:46:41] d2.evaluation.evaluator INFO: Inference done 794/1093. Dataloading: 0.0131 s/iter. Inference: 0.1637 s/iter. Eval: 0.2153 s/iter. Total: 0.3922 s/iter. ETA=0:01:57
[01/17 16:46:47] d2.evaluation.evaluator INFO: Inference done 808/1093. Dataloading: 0.0131 s/iter. Inference: 0.1639 s/iter. Eval: 0.2150 s/iter. Total: 0.3921 s/iter. ETA=0:01:51
[01/17 16:46:52] d2.evaluation.evaluator INFO: Inference done 822/1093. Dataloading: 0.0131 s/iter. Inference: 0.1640 s/iter. Eval: 0.2143 s/iter. Total: 0.3915 s/iter. ETA=0:01:46
[01/17 16:46:57] d2.evaluation.evaluator INFO: Inference done 838/1093. Dataloading: 0.0130 s/iter. Inference: 0.1638 s/iter. Eval: 0.2134 s/iter. Total: 0.3903 s/iter. ETA=0:01:39
[01/17 16:47:02] d2.evaluation.evaluator INFO: Inference done 852/1093. Dataloading: 0.0130 s/iter. Inference: 0.1639 s/iter. Eval: 0.2130 s/iter. Total: 0.3900 s/iter. ETA=0:01:34
[01/17 16:47:08] d2.evaluation.evaluator INFO: Inference done 864/1093. Dataloading: 0.0130 s/iter. Inference: 0.1640 s/iter. Eval: 0.2136 s/iter. Total: 0.3908 s/iter. ETA=0:01:29
[01/17 16:47:13] d2.evaluation.evaluator INFO: Inference done 878/1093. Dataloading: 0.0130 s/iter. Inference: 0.1640 s/iter. Eval: 0.2136 s/iter. Total: 0.3907 s/iter. ETA=0:01:23
[01/17 16:47:18] d2.evaluation.evaluator INFO: Inference done 889/1093. Dataloading: 0.0130 s/iter. Inference: 0.1640 s/iter. Eval: 0.2148 s/iter. Total: 0.3919 s/iter. ETA=0:01:19
[01/17 16:47:24] d2.evaluation.evaluator INFO: Inference done 902/1093. Dataloading: 0.0130 s/iter. Inference: 0.1641 s/iter. Eval: 0.2148 s/iter. Total: 0.3920 s/iter. ETA=0:01:14
[01/17 16:47:29] d2.evaluation.evaluator INFO: Inference done 916/1093. Dataloading: 0.0130 s/iter. Inference: 0.1641 s/iter. Eval: 0.2145 s/iter. Total: 0.3917 s/iter. ETA=0:01:09
[01/17 16:47:34] d2.evaluation.evaluator INFO: Inference done 930/1093. Dataloading: 0.0129 s/iter. Inference: 0.1641 s/iter. Eval: 0.2144 s/iter. Total: 0.3916 s/iter. ETA=0:01:03
[01/17 16:47:39] d2.evaluation.evaluator INFO: Inference done 943/1093. Dataloading: 0.0129 s/iter. Inference: 0.1641 s/iter. Eval: 0.2147 s/iter. Total: 0.3919 s/iter. ETA=0:00:58
[01/17 16:47:45] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0129 s/iter. Inference: 0.1638 s/iter. Eval: 0.2144 s/iter. Total: 0.3913 s/iter. ETA=0:00:52
[01/17 16:47:50] d2.evaluation.evaluator INFO: Inference done 970/1093. Dataloading: 0.0129 s/iter. Inference: 0.1641 s/iter. Eval: 0.2145 s/iter. Total: 0.3916 s/iter. ETA=0:00:48
[01/17 16:47:55] d2.evaluation.evaluator INFO: Inference done 985/1093. Dataloading: 0.0129 s/iter. Inference: 0.1640 s/iter. Eval: 0.2139 s/iter. Total: 0.3909 s/iter. ETA=0:00:42
[01/17 16:48:00] d2.evaluation.evaluator INFO: Inference done 999/1093. Dataloading: 0.0129 s/iter. Inference: 0.1641 s/iter. Eval: 0.2135 s/iter. Total: 0.3906 s/iter. ETA=0:00:36
[01/17 16:48:05] d2.evaluation.evaluator INFO: Inference done 1013/1093. Dataloading: 0.0128 s/iter. Inference: 0.1642 s/iter. Eval: 0.2132 s/iter. Total: 0.3904 s/iter. ETA=0:00:31
[01/17 16:48:11] d2.evaluation.evaluator INFO: Inference done 1026/1093. Dataloading: 0.0128 s/iter. Inference: 0.1641 s/iter. Eval: 0.2135 s/iter. Total: 0.3906 s/iter. ETA=0:00:26
[01/17 16:48:16] d2.evaluation.evaluator INFO: Inference done 1040/1093. Dataloading: 0.0128 s/iter. Inference: 0.1642 s/iter. Eval: 0.2131 s/iter. Total: 0.3902 s/iter. ETA=0:00:20
[01/17 16:48:21] d2.evaluation.evaluator INFO: Inference done 1053/1093. Dataloading: 0.0128 s/iter. Inference: 0.1644 s/iter. Eval: 0.2129 s/iter. Total: 0.3902 s/iter. ETA=0:00:15
[01/17 16:48:26] d2.evaluation.evaluator INFO: Inference done 1069/1093. Dataloading: 0.0127 s/iter. Inference: 0.1643 s/iter. Eval: 0.2121 s/iter. Total: 0.3893 s/iter. ETA=0:00:09
[01/17 16:48:31] d2.evaluation.evaluator INFO: Inference done 1085/1093. Dataloading: 0.0127 s/iter. Inference: 0.1640 s/iter. Eval: 0.2115 s/iter. Total: 0.3883 s/iter. ETA=0:00:03
[01/17 16:48:34] d2.evaluation.evaluator INFO: Total inference time: 0:07:02.351281 (0.388191 s / iter per device, on 4 devices)
[01/17 16:48:34] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:58 (0.163867 s / iter per device, on 4 devices)
[01/17 16:48:58] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 16.089537895070904, 'fwIoU': 44.662276185767865, 'IoU-1': nan, 'IoU-2': 95.39621500916124, 'IoU-3': 45.03737303876954, 'IoU-4': 59.69776536829357, 'IoU-5': 53.25296173039865, 'IoU-6': 47.43463988323258, 'IoU-7': 43.92114854831532, 'IoU-8': 37.057084375710716, 'IoU-9': 23.860993528701503, 'IoU-10': 30.301402893199864, 'IoU-11': 36.047043920905, 'IoU-12': 51.65641185397901, 'IoU-13': 52.926486933363094, 'IoU-14': 54.300840426304, 'IoU-15': 55.55035215803402, 'IoU-16': 55.06355279788029, 'IoU-17': 55.93022648477041, 'IoU-18': 50.74420283045759, 'IoU-19': 52.32761382014425, 'IoU-20': 51.40072188782958, 'IoU-21': 51.5612962161072, 'IoU-22': 48.8983258488903, 'IoU-23': 50.30226783910285, 'IoU-24': 46.821321599249025, 'IoU-25': 44.32606708017058, 'IoU-26': 45.84864308860847, 'IoU-27': 44.119996883285204, 'IoU-28': 46.34270923283063, 'IoU-29': 46.660316806057246, 'IoU-30': 47.41688942150405, 'IoU-31': 46.609629447294374, 'IoU-32': 47.659084906172126, 'IoU-33': 46.80025455021892, 'IoU-34': 46.122358282179974, 'IoU-35': 46.201420446233385, 'IoU-36': 46.85473384917876, 'IoU-37': 46.872344176295464, 'IoU-38': 45.86080759661564, 'IoU-39': 46.28673357199942, 'IoU-40': 45.94046142936076, 'IoU-41': 44.942626049762, 'IoU-42': 43.96689647784904, 'IoU-43': 42.75990177979207, 'IoU-44': 41.891342608295716, 'IoU-45': 42.52678128630459, 'IoU-46': 41.34051550566987, 'IoU-47': 40.54417985930103, 'IoU-48': 38.74114564426955, 'IoU-49': 38.45583669239307, 'IoU-50': 38.0047262396718, 'IoU-51': 37.25149333203921, 'IoU-52': 35.95344585458429, 'IoU-53': 35.437329209288016, 'IoU-54': 34.792907981552155, 'IoU-55': 34.65949731968207, 'IoU-56': 33.423763264547, 'IoU-57': 32.26971682283283, 'IoU-58': 31.41452564995569, 'IoU-59': 29.801806501926976, 'IoU-60': 29.574983693822364, 'IoU-61': 28.229217201556455, 'IoU-62': 28.20053627880808, 'IoU-63': 28.240601394242166, 'IoU-64': 28.17037174310363, 'IoU-65': 26.946555351460717, 'IoU-66': 26.281872184426025, 'IoU-67': 25.85566852658226, 'IoU-68': 23.714795245378557, 'IoU-69': 23.28307816885461, 'IoU-70': 24.031413595293635, 'IoU-71': 22.554963659694273, 'IoU-72': 20.548883425897372, 'IoU-73': 20.00843906419612, 'IoU-74': 20.290484422051776, 'IoU-75': 20.501624145205117, 'IoU-76': 20.222883825253124, 'IoU-77': 20.296176957327248, 'IoU-78': 19.474530092440688, 'IoU-79': 19.099187199628584, 'IoU-80': 18.490150512603837, 'IoU-81': 18.599188008590776, 'IoU-82': 18.900756434831344, 'IoU-83': 17.449427955144024, 'IoU-84': 18.697860878304272, 'IoU-85': 18.280035334571647, 'IoU-86': 18.343336851217614, 'IoU-87': 17.477955738411115, 'IoU-88': 17.74361650000026, 'IoU-89': 17.351229395586458, 'IoU-90': 17.872052284617173, 'IoU-91': 17.692661857263328, 'IoU-92': 17.183521533056155, 'IoU-93': 16.00829187213125, 'IoU-94': 17.45959589778899, 'IoU-95': 16.39933997561441, 'IoU-96': 18.039890527578205, 'IoU-97': 17.625983511132986, 'IoU-98': 18.160271047245274, 'IoU-99': 17.51522193765781, 'IoU-100': 17.89031260566756, 'IoU-101': 17.502007025519934, 'IoU-102': 16.37750590567158, 'IoU-103': 16.275289649196655, 'IoU-104': 15.98911110808818, 'IoU-105': 15.438960051480134, 'IoU-106': 14.45883297950875, 'IoU-107': 15.005134332907208, 'IoU-108': 14.87857816091474, 'IoU-109': 13.785107803204118, 'IoU-110': 15.094705138537865, 'IoU-111': 15.070066562094697, 'IoU-112': 14.44262122053441, 'IoU-113': 15.08455996928005, 'IoU-114': 14.189705436814998, 'IoU-115': 13.49173296218453, 'IoU-116': 12.223615194514565, 'IoU-117': 12.796199443791284, 'IoU-118': 13.071777007936685, 'IoU-119': 12.015599962469576, 'IoU-120': 12.86672644589466, 'IoU-121': 8.934181580097214, 'IoU-122': 10.93083098745486, 'IoU-123': 10.882684123224768, 'IoU-124': 10.720691161695932, 'IoU-125': 9.82615929770745, 'IoU-126': 9.291052437388366, 'IoU-127': 9.200985462102047, 'IoU-128': 8.610071726377088, 'IoU-129': 8.370000457027043, 'IoU-130': 8.979182071212586, 'IoU-131': 9.1676403155824, 'IoU-132': 8.443951932976525, 'IoU-133': 8.897398198964446, 'IoU-134': 9.216709417369, 'IoU-135': 9.509656275700044, 'IoU-136': 8.080772826610804, 'IoU-137': 8.62205039002733, 'IoU-138': 8.017759587802159, 'IoU-139': 8.590971027218512, 'IoU-140': 7.4295320191288425, 'IoU-141': 7.566696841333571, 'IoU-142': 7.729774062714169, 'IoU-143': 5.973154886263409, 'IoU-144': 5.373057368954719, 'IoU-145': 8.072763542720265, 'IoU-146': 5.713658118752429, 'IoU-147': 5.491457279782633, 'IoU-148': 5.421523535313428, 'IoU-149': 6.144227630481303, 'IoU-150': 4.06985217113687, 'IoU-151': 4.500542276434215, 'IoU-152': 4.985248851419855, 'IoU-153': 3.812745044691569, 'IoU-154': 4.137114981161935, 'IoU-155': 4.836700329017921, 'IoU-156': 3.8552522366000304, 'IoU-157': 2.6963761593306375, 'IoU-158': 4.818923140438593, 'IoU-159': 2.178511074518339, 'IoU-160': 3.16222860041742, 'IoU-161': 3.1189820948944345, 'IoU-162': 1.8802624052658539, 'IoU-163': 3.643004134299857, 'IoU-164': 3.2957191751638097, 'IoU-165': 1.7125767051628233, 'IoU-166': 2.0335653858962734, 'IoU-167': 1.2809440169239876, 'IoU-168': 1.519270514929917, 'IoU-169': 2.6604597388786217, 'IoU-170': 2.1071542697724652, 'IoU-171': 1.943272739189163, 'IoU-172': 1.4370927273712273, 'IoU-173': 1.8864973123019264, 'IoU-174': 1.9593442383761266, 'IoU-175': 1.4666581080298278, 'IoU-176': 2.481137924527737, 'IoU-177': 1.9992950590674747, 'IoU-178': 1.7431575948111184, 'IoU-179': 2.9302064991195773, 'IoU-180': 1.4983695251368883, 'IoU-181': 1.2744190082632263, 'IoU-182': 0.8950526718592077, 'IoU-183': 2.155195925597366, 'IoU-184': 0.5996040909368675, 'IoU-185': 1.0338744760995278, 'IoU-186': 0.7275738449149628, 'IoU-187': 0.9035126722987995, 'IoU-188': 1.7140115122986865, 'IoU-189': 1.014242054182125, 'IoU-190': 1.8753676129152093, 'IoU-191': 1.381612254192475, 'IoU-192': 1.023935536165546, 'IoU-193': 0.9292230567126131, 'IoU-194': 0.0, 'IoU-195': 0.0, 'IoU-196': 0.0, 'IoU-197': 0.0, 'IoU-198': 0.0, 'IoU-199': 0.0, 'IoU-200': 0.0, 'IoU-201': 0.0, 'IoU-202': 0.0, 'IoU-203': 0.0, 'IoU-204': 0.0, 'IoU-205': 0.0, 'IoU-206': 0.0, 'IoU-207': 0.0, 'IoU-208': 0.0, 'IoU-209': 0.0, 'IoU-210': 0.0, 'IoU-211': 0.0, 'IoU-212': 0.0, 'IoU-213': 0.0, 'IoU-214': 0.0, 'IoU-215': 0.0, 'IoU-216': 0.0, 'IoU-217': 0.0, 'IoU-218': 0.0, 'IoU-219': 0.0, 'IoU-220': 0.0, 'IoU-221': 0.0, 'IoU-222': 0.0, 'IoU-223': 0.0, 'IoU-224': 0.0, 'IoU-225': 0.0, 'IoU-226': 0.0, 'IoU-227': 0.0, 'IoU-228': 0.0, 'IoU-229': 0.0, 'IoU-230': 0.0, 'IoU-231': 0.0, 'IoU-232': 0.0, 'IoU-233': 0.0, 'IoU-234': 0.0, 'IoU-235': 0.0, 'IoU-236': 0.0, 'IoU-237': 0.0, 'IoU-238': 0.0, 'IoU-239': 0.0, 'IoU-240': 0.0, 'IoU-241': 0.0, 'IoU-242': 0.0, 'IoU-243': 0.0, 'IoU-244': 0.0, 'IoU-245': 0.0, 'IoU-246': 0.0, 'IoU-247': 0.0, 'IoU-248': 0.0, 'IoU-249': 0.0, 'IoU-250': 0.0, 'IoU-251': 0.0, 'IoU-252': 0.0, 'IoU-253': 0.0, 'IoU-254': 0.0, 'IoU-255': 0.0, 'mACC': 24.031497001996698, 'pACC': 59.0293135278001, 'ACC-1': nan, 'ACC-2': 98.72727625129401, 'ACC-3': 57.768920030293344, 'ACC-4': 74.60375597052347, 'ACC-5': 70.44588105458091, 'ACC-6': 66.93137934136413, 'ACC-7': 60.600651562421845, 'ACC-8': 51.98613678710379, 'ACC-9': 28.811681973689907, 'ACC-10': 38.28772230242581, 'ACC-11': 47.87816805086088, 'ACC-12': 66.826323743222, 'ACC-13': 70.90666248715807, 'ACC-14': 73.22035986086875, 'ACC-15': 71.26657028034033, 'ACC-16': 73.15043241451636, 'ACC-17': 71.26319301015654, 'ACC-18': 68.74053203882585, 'ACC-19': 69.23404854172315, 'ACC-20': 68.33425279741125, 'ACC-21': 66.9131121142909, 'ACC-22': 64.01653255790538, 'ACC-23': 65.75009072920832, 'ACC-24': 62.68853190322324, 'ACC-25': 59.44543912435768, 'ACC-26': 64.34763761721256, 'ACC-27': 60.628314554122674, 'ACC-28': 63.89551044943437, 'ACC-29': 65.18296839043086, 'ACC-30': 65.90618864458752, 'ACC-31': 63.886621924345924, 'ACC-32': 66.13858943202932, 'ACC-33': 65.35142015191762, 'ACC-34': 63.50620860864489, 'ACC-35': 64.25267603648035, 'ACC-36': 64.05735205289442, 'ACC-37': 64.37217903443153, 'ACC-38': 63.45272938174206, 'ACC-39': 62.408464946776554, 'ACC-40': 62.75949455939234, 'ACC-41': 62.148917298011966, 'ACC-42': 59.817479232969475, 'ACC-43': 59.22584255089487, 'ACC-44': 59.344389636666136, 'ACC-45': 61.414533480026364, 'ACC-46': 58.159096797576495, 'ACC-47': 58.25165380318148, 'ACC-48': 55.9989607243167, 'ACC-49': 57.626156235660694, 'ACC-50': 56.26005243189568, 'ACC-51': 55.453379147730516, 'ACC-52': 52.50930029671657, 'ACC-53': 51.86247170059808, 'ACC-54': 50.15329076269358, 'ACC-55': 53.11921349200801, 'ACC-56': 50.27930812110212, 'ACC-57': 48.808125330505895, 'ACC-58': 47.16805807678946, 'ACC-59': 44.299015561885554, 'ACC-60': 46.179844782089916, 'ACC-61': 43.171852695253996, 'ACC-62': 41.6996319287149, 'ACC-63': 43.49216591670797, 'ACC-64': 42.85798740384289, 'ACC-65': 42.304715555915216, 'ACC-66': 41.42572755055055, 'ACC-67': 40.57544650905896, 'ACC-68': 42.06517366687101, 'ACC-69': 38.429514704031085, 'ACC-70': 38.579652091258176, 'ACC-71': 36.44601994612521, 'ACC-72': 35.39641227471971, 'ACC-73': 32.840793373762864, 'ACC-74': 33.734065126853594, 'ACC-75': 32.76864873614518, 'ACC-76': 33.57041689826468, 'ACC-77': 32.608028437332, 'ACC-78': 31.97470754893906, 'ACC-79': 34.752005036633385, 'ACC-80': 31.000220201683355, 'ACC-81': 31.545307202702222, 'ACC-82': 32.17635789685956, 'ACC-83': 29.14134125894551, 'ACC-84': 30.045200492950592, 'ACC-85': 31.142530922887378, 'ACC-86': 29.924180878990757, 'ACC-87': 29.723684577196437, 'ACC-88': 29.05863477046342, 'ACC-89': 30.06445340042527, 'ACC-90': 31.22088986443555, 'ACC-91': 29.254415035193137, 'ACC-92': 30.686076451644773, 'ACC-93': 26.784188439819758, 'ACC-94': 28.79492674550388, 'ACC-95': 24.9129974139462, 'ACC-96': 30.759532721165744, 'ACC-97': 29.232361272160972, 'ACC-98': 29.439279937830744, 'ACC-99': 28.001972848420863, 'ACC-100': 29.06814758019665, 'ACC-101': 28.595941927442553, 'ACC-102': 26.36943726561331, 'ACC-103': 28.566153149071138, 'ACC-104': 28.588268631340323, 'ACC-105': 24.369149799562233, 'ACC-106': 26.00586737345523, 'ACC-107': 24.373803112438665, 'ACC-108': 26.158407634165766, 'ACC-109': 24.878379472627195, 'ACC-110': 26.728298201374095, 'ACC-111': 24.89250020098345, 'ACC-112': 27.616824319051698, 'ACC-113': 29.215555951496757, 'ACC-114': 23.371669432206136, 'ACC-115': 23.789093063877257, 'ACC-116': 20.269909641724173, 'ACC-117': 23.08238915501084, 'ACC-118': 23.018949199044947, 'ACC-119': 21.937929132043244, 'ACC-120': 22.37116545866525, 'ACC-121': 14.451009744287136, 'ACC-122': 17.53462325816317, 'ACC-123': 22.281789276043288, 'ACC-124': 18.185886643943086, 'ACC-125': 17.706665757463238, 'ACC-126': 16.36567928011025, 'ACC-127': 15.946126558529677, 'ACC-128': 16.610051490996085, 'ACC-129': 14.18408056680705, 'ACC-130': 18.746215232696123, 'ACC-131': 16.238717066992052, 'ACC-132': 15.708510526592114, 'ACC-133': 16.714352256771654, 'ACC-134': 17.114318180271763, 'ACC-135': 21.25132270573029, 'ACC-136': 15.509235505387378, 'ACC-137': 15.945303764442789, 'ACC-138': 16.746080992690523, 'ACC-139': 14.458032972053712, 'ACC-140': 15.899366511088719, 'ACC-141': 13.216203098625613, 'ACC-142': 14.474811449845912, 'ACC-143': 12.029752126742594, 'ACC-144': 9.4947850255155, 'ACC-145': 17.136823104693143, 'ACC-146': 9.26013551286925, 'ACC-147': 9.101825409517717, 'ACC-148': 7.82231474405817, 'ACC-149': 12.881694898821443, 'ACC-150': 6.089026224331158, 'ACC-151': 7.993045718770618, 'ACC-152': 8.725512452000089, 'ACC-153': 5.61104929529975, 'ACC-154': 7.178699683028271, 'ACC-155': 9.956003931113921, 'ACC-156': 8.169465788104905, 'ACC-157': 5.584233421887827, 'ACC-158': 11.072865925445026, 'ACC-159': 3.413162919818051, 'ACC-160': 5.26685134410965, 'ACC-161': 4.536290322580645, 'ACC-162': 2.7371888664530424, 'ACC-163': 6.723425398912827, 'ACC-164': 6.503232355040839, 'ACC-165': 2.261549535608624, 'ACC-166': 3.423308674287783, 'ACC-167': 1.9944430920583223, 'ACC-168': 2.1541967839045837, 'ACC-169': 5.95558759406428, 'ACC-170': 5.449794961510319, 'ACC-171': 3.3675483055780346, 'ACC-172': 3.2646411062271197, 'ACC-173': 3.864133051820573, 'ACC-174': 5.2552558202541455, 'ACC-175': 3.4923883604451977, 'ACC-176': 7.657720118547771, 'ACC-177': 4.743186963083352, 'ACC-178': 5.690305221372693, 'ACC-179': 8.4858154452967, 'ACC-180': 2.844297102140273, 'ACC-181': 1.8028886856988044, 'ACC-182': 1.2354890437987498, 'ACC-183': 5.335604576286044, 'ACC-184': 0.762328988042406, 'ACC-185': 1.4706957504062337, 'ACC-186': 1.8425878451099134, 'ACC-187': 3.7175440791477126, 'ACC-188': 5.937496224887955, 'ACC-189': 5.180127920584319, 'ACC-190': 8.553839202507696, 'ACC-191': 5.269763249735372, 'ACC-192': 4.708384991843393, 'ACC-193': 4.811090893140637, 'ACC-194': 0.0, 'ACC-195': 0.0, 'ACC-196': 0.0, 'ACC-197': 0.0, 'ACC-198': 0.0, 'ACC-199': 0.0, 'ACC-200': 0.0, 'ACC-201': 0.0, 'ACC-202': 0.0, 'ACC-203': 0.0, 'ACC-204': 0.0, 'ACC-205': 0.0, 'ACC-206': 0.0, 'ACC-207': 0.0, 'ACC-208': 0.0, 'ACC-209': 0.0, 'ACC-210': 0.0, 'ACC-211': 0.0, 'ACC-212': 0.0, 'ACC-213': 0.0, 'ACC-214': 0.0, 'ACC-215': 0.0, 'ACC-216': 0.0, 'ACC-217': 0.0, 'ACC-218': 0.0, 'ACC-219': 0.0, 'ACC-220': 0.0, 'ACC-221': 0.0, 'ACC-222': 0.0, 'ACC-223': 0.0, 'ACC-224': 0.0, 'ACC-225': 0.0, 'ACC-226': 0.0, 'ACC-227': 0.0, 'ACC-228': 0.0, 'ACC-229': 0.0, 'ACC-230': 0.0, 'ACC-231': 0.0, 'ACC-232': 0.0, 'ACC-233': 0.0, 'ACC-234': 0.0, 'ACC-235': 0.0, 'ACC-236': 0.0, 'ACC-237': 0.0, 'ACC-238': 0.0, 'ACC-239': 0.0, 'ACC-240': 0.0, 'ACC-241': 0.0, 'ACC-242': 0.0, 'ACC-243': 0.0, 'ACC-244': 0.0, 'ACC-245': 0.0, 'ACC-246': 0.0, 'ACC-247': 0.0, 'ACC-248': 0.0, 'ACC-249': 0.0, 'ACC-250': 0.0, 'ACC-251': 0.0, 'ACC-252': 0.0, 'ACC-253': 0.0, 'ACC-254': 0.0, 'ACC-255': 0.0})])
[01/17 16:48:58] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 16:48:58] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 16:48:58] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 16:48:58] d2.evaluation.testing INFO: copypaste: 16.0895,44.6623,24.0315,59.0293
[01/17 16:48:58] d2.utils.events INFO:  eta: 1 day, 1:57:35  iter: 27999  total_loss: 33.07  loss_ce: 0.241  loss_mask: 0.3348  loss_dice: 2.657  loss_ce_0: 0.5462  loss_mask_0: 0.3468  loss_dice_0: 2.745  loss_ce_1: 0.2928  loss_mask_1: 0.3412  loss_dice_1: 2.685  loss_ce_2: 0.2695  loss_mask_2: 0.3369  loss_dice_2: 2.672  loss_ce_3: 0.2686  loss_mask_3: 0.3369  loss_dice_3: 2.662  loss_ce_4: 0.2593  loss_mask_4: 0.3343  loss_dice_4: 2.658  loss_ce_5: 0.2487  loss_mask_5: 0.3325  loss_dice_5: 2.656  loss_ce_6: 0.2462  loss_mask_6: 0.3327  loss_dice_6: 2.663  loss_ce_7: 0.2331  loss_mask_7: 0.3337  loss_dice_7: 2.656  loss_ce_8: 0.2404  loss_mask_8: 0.3331  loss_dice_8: 2.658  time: 1.5312  data_time: 0.0935  lr: 7.1506e-06  max_mem: 21478M
[01/17 16:49:28] d2.utils.events INFO:  eta: 1 day, 1:57:13  iter: 28019  total_loss: 33.26  loss_ce: 0.235  loss_mask: 0.3358  loss_dice: 2.671  loss_ce_0: 0.5878  loss_mask_0: 0.3429  loss_dice_0: 2.787  loss_ce_1: 0.2883  loss_mask_1: 0.3429  loss_dice_1: 2.721  loss_ce_2: 0.2747  loss_mask_2: 0.3409  loss_dice_2: 2.692  loss_ce_3: 0.278  loss_mask_3: 0.3408  loss_dice_3: 2.675  loss_ce_4: 0.2571  loss_mask_4: 0.3373  loss_dice_4: 2.674  loss_ce_5: 0.2621  loss_mask_5: 0.3365  loss_dice_5: 2.68  loss_ce_6: 0.2578  loss_mask_6: 0.3371  loss_dice_6: 2.67  loss_ce_7: 0.2511  loss_mask_7: 0.3373  loss_dice_7: 2.675  loss_ce_8: 0.2354  loss_mask_8: 0.338  loss_dice_8: 2.674  time: 1.5312  data_time: 0.0851  lr: 7.1485e-06  max_mem: 21478M
[01/17 16:49:59] d2.utils.events INFO:  eta: 1 day, 1:58:14  iter: 28039  total_loss: 33.63  loss_ce: 0.2331  loss_mask: 0.3442  loss_dice: 2.679  loss_ce_0: 0.5465  loss_mask_0: 0.3499  loss_dice_0: 2.787  loss_ce_1: 0.2711  loss_mask_1: 0.3518  loss_dice_1: 2.725  loss_ce_2: 0.2696  loss_mask_2: 0.3506  loss_dice_2: 2.698  loss_ce_3: 0.2625  loss_mask_3: 0.3452  loss_dice_3: 2.69  loss_ce_4: 0.2545  loss_mask_4: 0.3441  loss_dice_4: 2.691  loss_ce_5: 0.239  loss_mask_5: 0.3458  loss_dice_5: 2.69  loss_ce_6: 0.2438  loss_mask_6: 0.3442  loss_dice_6: 2.685  loss_ce_7: 0.2188  loss_mask_7: 0.3436  loss_dice_7: 2.687  loss_ce_8: 0.2304  loss_mask_8: 0.3443  loss_dice_8: 2.687  time: 1.5312  data_time: 0.0939  lr: 7.1464e-06  max_mem: 21478M
[01/17 16:50:29] d2.utils.events INFO:  eta: 1 day, 1:57:59  iter: 28059  total_loss: 33.46  loss_ce: 0.2488  loss_mask: 0.338  loss_dice: 2.663  loss_ce_0: 0.5827  loss_mask_0: 0.3434  loss_dice_0: 2.773  loss_ce_1: 0.2972  loss_mask_1: 0.3437  loss_dice_1: 2.698  loss_ce_2: 0.2867  loss_mask_2: 0.3406  loss_dice_2: 2.679  loss_ce_3: 0.2613  loss_mask_3: 0.3415  loss_dice_3: 2.669  loss_ce_4: 0.2528  loss_mask_4: 0.3402  loss_dice_4: 2.671  loss_ce_5: 0.2575  loss_mask_5: 0.3382  loss_dice_5: 2.67  loss_ce_6: 0.2547  loss_mask_6: 0.3381  loss_dice_6: 2.672  loss_ce_7: 0.2468  loss_mask_7: 0.3393  loss_dice_7: 2.667  loss_ce_8: 0.2364  loss_mask_8: 0.3389  loss_dice_8: 2.663  time: 1.5312  data_time: 0.0850  lr: 7.1443e-06  max_mem: 21478M
[01/17 16:50:59] d2.utils.events INFO:  eta: 1 day, 1:59:07  iter: 28079  total_loss: 33.01  loss_ce: 0.2577  loss_mask: 0.3368  loss_dice: 2.637  loss_ce_0: 0.5653  loss_mask_0: 0.3458  loss_dice_0: 2.72  loss_ce_1: 0.305  loss_mask_1: 0.3455  loss_dice_1: 2.684  loss_ce_2: 0.307  loss_mask_2: 0.3405  loss_dice_2: 2.66  loss_ce_3: 0.278  loss_mask_3: 0.3378  loss_dice_3: 2.652  loss_ce_4: 0.2752  loss_mask_4: 0.3367  loss_dice_4: 2.641  loss_ce_5: 0.2621  loss_mask_5: 0.3364  loss_dice_5: 2.643  loss_ce_6: 0.2677  loss_mask_6: 0.3375  loss_dice_6: 2.641  loss_ce_7: 0.2533  loss_mask_7: 0.3368  loss_dice_7: 2.639  loss_ce_8: 0.2546  loss_mask_8: 0.3373  loss_dice_8: 2.641  time: 1.5312  data_time: 0.0971  lr: 7.1423e-06  max_mem: 21478M
[01/17 16:51:30] d2.utils.events INFO:  eta: 1 day, 1:58:37  iter: 28099  total_loss: 31.86  loss_ce: 0.2382  loss_mask: 0.3416  loss_dice: 2.535  loss_ce_0: 0.5705  loss_mask_0: 0.3452  loss_dice_0: 2.646  loss_ce_1: 0.2803  loss_mask_1: 0.3453  loss_dice_1: 2.576  loss_ce_2: 0.2522  loss_mask_2: 0.3431  loss_dice_2: 2.554  loss_ce_3: 0.2531  loss_mask_3: 0.344  loss_dice_3: 2.542  loss_ce_4: 0.2426  loss_mask_4: 0.3425  loss_dice_4: 2.552  loss_ce_5: 0.2385  loss_mask_5: 0.3436  loss_dice_5: 2.553  loss_ce_6: 0.2126  loss_mask_6: 0.3445  loss_dice_6: 2.541  loss_ce_7: 0.226  loss_mask_7: 0.3419  loss_dice_7: 2.533  loss_ce_8: 0.2336  loss_mask_8: 0.3416  loss_dice_8: 2.537  time: 1.5311  data_time: 0.0928  lr: 7.1402e-06  max_mem: 21478M
[01/17 16:52:01] d2.utils.events INFO:  eta: 1 day, 1:59:11  iter: 28119  total_loss: 33.68  loss_ce: 0.269  loss_mask: 0.3337  loss_dice: 2.681  loss_ce_0: 0.5886  loss_mask_0: 0.3343  loss_dice_0: 2.799  loss_ce_1: 0.3101  loss_mask_1: 0.3365  loss_dice_1: 2.715  loss_ce_2: 0.2964  loss_mask_2: 0.3311  loss_dice_2: 2.702  loss_ce_3: 0.2892  loss_mask_3: 0.3315  loss_dice_3: 2.689  loss_ce_4: 0.2813  loss_mask_4: 0.3307  loss_dice_4: 2.698  loss_ce_5: 0.2678  loss_mask_5: 0.3317  loss_dice_5: 2.683  loss_ce_6: 0.2633  loss_mask_6: 0.3305  loss_dice_6: 2.685  loss_ce_7: 0.2691  loss_mask_7: 0.3313  loss_dice_7: 2.685  loss_ce_8: 0.2626  loss_mask_8: 0.3323  loss_dice_8: 2.676  time: 1.5312  data_time: 0.0987  lr: 7.1381e-06  max_mem: 21478M
[01/17 16:52:31] d2.utils.events INFO:  eta: 1 day, 1:58:54  iter: 28139  total_loss: 32.34  loss_ce: 0.2342  loss_mask: 0.341  loss_dice: 2.606  loss_ce_0: 0.5687  loss_mask_0: 0.3489  loss_dice_0: 2.718  loss_ce_1: 0.2739  loss_mask_1: 0.3466  loss_dice_1: 2.644  loss_ce_2: 0.2581  loss_mask_2: 0.3398  loss_dice_2: 2.631  loss_ce_3: 0.2473  loss_mask_3: 0.339  loss_dice_3: 2.615  loss_ce_4: 0.2405  loss_mask_4: 0.3393  loss_dice_4: 2.612  loss_ce_5: 0.2356  loss_mask_5: 0.3397  loss_dice_5: 2.616  loss_ce_6: 0.2355  loss_mask_6: 0.3412  loss_dice_6: 2.604  loss_ce_7: 0.2336  loss_mask_7: 0.3403  loss_dice_7: 2.608  loss_ce_8: 0.2325  loss_mask_8: 0.3403  loss_dice_8: 2.605  time: 1.5311  data_time: 0.0942  lr: 7.136e-06  max_mem: 21478M
[01/17 16:53:01] d2.utils.events INFO:  eta: 1 day, 1:58:05  iter: 28159  total_loss: 33.2  loss_ce: 0.2408  loss_mask: 0.3459  loss_dice: 2.688  loss_ce_0: 0.5758  loss_mask_0: 0.3608  loss_dice_0: 2.769  loss_ce_1: 0.2896  loss_mask_1: 0.3565  loss_dice_1: 2.708  loss_ce_2: 0.2839  loss_mask_2: 0.3527  loss_dice_2: 2.685  loss_ce_3: 0.2573  loss_mask_3: 0.3479  loss_dice_3: 2.68  loss_ce_4: 0.2528  loss_mask_4: 0.3454  loss_dice_4: 2.685  loss_ce_5: 0.2447  loss_mask_5: 0.3454  loss_dice_5: 2.692  loss_ce_6: 0.2529  loss_mask_6: 0.3447  loss_dice_6: 2.683  loss_ce_7: 0.241  loss_mask_7: 0.3432  loss_dice_7: 2.684  loss_ce_8: 0.2364  loss_mask_8: 0.346  loss_dice_8: 2.68  time: 1.5311  data_time: 0.0909  lr: 7.134e-06  max_mem: 21478M
[01/17 16:53:31] d2.utils.events INFO:  eta: 1 day, 1:57:40  iter: 28179  total_loss: 32.8  loss_ce: 0.2278  loss_mask: 0.3471  loss_dice: 2.6  loss_ce_0: 0.577  loss_mask_0: 0.3449  loss_dice_0: 2.707  loss_ce_1: 0.2853  loss_mask_1: 0.3513  loss_dice_1: 2.632  loss_ce_2: 0.2577  loss_mask_2: 0.3484  loss_dice_2: 2.622  loss_ce_3: 0.2603  loss_mask_3: 0.3475  loss_dice_3: 2.595  loss_ce_4: 0.2471  loss_mask_4: 0.3468  loss_dice_4: 2.599  loss_ce_5: 0.2373  loss_mask_5: 0.348  loss_dice_5: 2.6  loss_ce_6: 0.2321  loss_mask_6: 0.3478  loss_dice_6: 2.597  loss_ce_7: 0.2279  loss_mask_7: 0.3469  loss_dice_7: 2.601  loss_ce_8: 0.2303  loss_mask_8: 0.3473  loss_dice_8: 2.592  time: 1.5311  data_time: 0.0885  lr: 7.1319e-06  max_mem: 21478M
[01/17 16:54:01] d2.utils.events INFO:  eta: 1 day, 1:57:10  iter: 28199  total_loss: 33.07  loss_ce: 0.2344  loss_mask: 0.3327  loss_dice: 2.647  loss_ce_0: 0.5558  loss_mask_0: 0.3417  loss_dice_0: 2.766  loss_ce_1: 0.2687  loss_mask_1: 0.3445  loss_dice_1: 2.684  loss_ce_2: 0.2768  loss_mask_2: 0.341  loss_dice_2: 2.666  loss_ce_3: 0.2483  loss_mask_3: 0.3371  loss_dice_3: 2.651  loss_ce_4: 0.2387  loss_mask_4: 0.3379  loss_dice_4: 2.655  loss_ce_5: 0.2425  loss_mask_5: 0.3366  loss_dice_5: 2.65  loss_ce_6: 0.2288  loss_mask_6: 0.3351  loss_dice_6: 2.654  loss_ce_7: 0.235  loss_mask_7: 0.335  loss_dice_7: 2.647  loss_ce_8: 0.2238  loss_mask_8: 0.335  loss_dice_8: 2.655  time: 1.5311  data_time: 0.0844  lr: 7.1298e-06  max_mem: 21478M
[01/17 16:54:31] d2.utils.events INFO:  eta: 1 day, 1:55:44  iter: 28219  total_loss: 33.13  loss_ce: 0.2508  loss_mask: 0.3343  loss_dice: 2.653  loss_ce_0: 0.5718  loss_mask_0: 0.3339  loss_dice_0: 2.758  loss_ce_1: 0.2882  loss_mask_1: 0.3326  loss_dice_1: 2.7  loss_ce_2: 0.3001  loss_mask_2: 0.3313  loss_dice_2: 2.679  loss_ce_3: 0.2647  loss_mask_3: 0.3328  loss_dice_3: 2.666  loss_ce_4: 0.2661  loss_mask_4: 0.331  loss_dice_4: 2.662  loss_ce_5: 0.2693  loss_mask_5: 0.3315  loss_dice_5: 2.663  loss_ce_6: 0.2589  loss_mask_6: 0.3343  loss_dice_6: 2.652  loss_ce_7: 0.2619  loss_mask_7: 0.334  loss_dice_7: 2.652  loss_ce_8: 0.2471  loss_mask_8: 0.334  loss_dice_8: 2.654  time: 1.5310  data_time: 0.0863  lr: 7.1277e-06  max_mem: 21478M
[01/17 16:55:01] d2.utils.events INFO:  eta: 1 day, 1:54:55  iter: 28239  total_loss: 32.67  loss_ce: 0.253  loss_mask: 0.3508  loss_dice: 2.597  loss_ce_0: 0.5598  loss_mask_0: 0.354  loss_dice_0: 2.722  loss_ce_1: 0.2935  loss_mask_1: 0.3525  loss_dice_1: 2.65  loss_ce_2: 0.2768  loss_mask_2: 0.3519  loss_dice_2: 2.615  loss_ce_3: 0.2529  loss_mask_3: 0.3509  loss_dice_3: 2.601  loss_ce_4: 0.2521  loss_mask_4: 0.3512  loss_dice_4: 2.596  loss_ce_5: 0.2389  loss_mask_5: 0.3544  loss_dice_5: 2.61  loss_ce_6: 0.2452  loss_mask_6: 0.3529  loss_dice_6: 2.609  loss_ce_7: 0.2367  loss_mask_7: 0.3528  loss_dice_7: 2.603  loss_ce_8: 0.2474  loss_mask_8: 0.3521  loss_dice_8: 2.603  time: 1.5310  data_time: 0.0881  lr: 7.1257e-06  max_mem: 21478M
[01/17 16:55:31] d2.utils.events INFO:  eta: 1 day, 1:54:38  iter: 28259  total_loss: 33.66  loss_ce: 0.2754  loss_mask: 0.3494  loss_dice: 2.716  loss_ce_0: 0.5784  loss_mask_0: 0.3543  loss_dice_0: 2.814  loss_ce_1: 0.2995  loss_mask_1: 0.3527  loss_dice_1: 2.752  loss_ce_2: 0.2896  loss_mask_2: 0.3488  loss_dice_2: 2.722  loss_ce_3: 0.2779  loss_mask_3: 0.3477  loss_dice_3: 2.725  loss_ce_4: 0.2546  loss_mask_4: 0.3498  loss_dice_4: 2.723  loss_ce_5: 0.2609  loss_mask_5: 0.3513  loss_dice_5: 2.712  loss_ce_6: 0.2637  loss_mask_6: 0.3474  loss_dice_6: 2.713  loss_ce_7: 0.2585  loss_mask_7: 0.3506  loss_dice_7: 2.71  loss_ce_8: 0.2605  loss_mask_8: 0.3507  loss_dice_8: 2.711  time: 1.5310  data_time: 0.0910  lr: 7.1236e-06  max_mem: 21478M
[01/17 16:56:01] d2.utils.events INFO:  eta: 1 day, 1:56:03  iter: 28279  total_loss: 33.16  loss_ce: 0.2462  loss_mask: 0.3293  loss_dice: 2.709  loss_ce_0: 0.5597  loss_mask_0: 0.336  loss_dice_0: 2.81  loss_ce_1: 0.2986  loss_mask_1: 0.3358  loss_dice_1: 2.747  loss_ce_2: 0.2783  loss_mask_2: 0.334  loss_dice_2: 2.719  loss_ce_3: 0.2721  loss_mask_3: 0.3326  loss_dice_3: 2.711  loss_ce_4: 0.2615  loss_mask_4: 0.3296  loss_dice_4: 2.707  loss_ce_5: 0.26  loss_mask_5: 0.3296  loss_dice_5: 2.712  loss_ce_6: 0.2628  loss_mask_6: 0.3293  loss_dice_6: 2.704  loss_ce_7: 0.2518  loss_mask_7: 0.33  loss_dice_7: 2.705  loss_ce_8: 0.2528  loss_mask_8: 0.3282  loss_dice_8: 2.705  time: 1.5310  data_time: 0.0858  lr: 7.1215e-06  max_mem: 21478M
[01/17 16:56:31] d2.utils.events INFO:  eta: 1 day, 1:55:55  iter: 28299  total_loss: 31.65  loss_ce: 0.2148  loss_mask: 0.336  loss_dice: 2.551  loss_ce_0: 0.5396  loss_mask_0: 0.338  loss_dice_0: 2.662  loss_ce_1: 0.2467  loss_mask_1: 0.3442  loss_dice_1: 2.593  loss_ce_2: 0.2351  loss_mask_2: 0.3406  loss_dice_2: 2.574  loss_ce_3: 0.2246  loss_mask_3: 0.339  loss_dice_3: 2.56  loss_ce_4: 0.2287  loss_mask_4: 0.3357  loss_dice_4: 2.566  loss_ce_5: 0.2248  loss_mask_5: 0.3351  loss_dice_5: 2.566  loss_ce_6: 0.2094  loss_mask_6: 0.3369  loss_dice_6: 2.556  loss_ce_7: 0.2079  loss_mask_7: 0.3358  loss_dice_7: 2.555  loss_ce_8: 0.2031  loss_mask_8: 0.3355  loss_dice_8: 2.56  time: 1.5310  data_time: 0.0907  lr: 7.1194e-06  max_mem: 21478M
[01/17 16:57:01] d2.utils.events INFO:  eta: 1 day, 1:54:22  iter: 28319  total_loss: 31.66  loss_ce: 0.224  loss_mask: 0.3475  loss_dice: 2.551  loss_ce_0: 0.5321  loss_mask_0: 0.3504  loss_dice_0: 2.643  loss_ce_1: 0.2724  loss_mask_1: 0.353  loss_dice_1: 2.573  loss_ce_2: 0.2597  loss_mask_2: 0.3498  loss_dice_2: 2.563  loss_ce_3: 0.2348  loss_mask_3: 0.3486  loss_dice_3: 2.556  loss_ce_4: 0.2358  loss_mask_4: 0.3476  loss_dice_4: 2.555  loss_ce_5: 0.2445  loss_mask_5: 0.3481  loss_dice_5: 2.55  loss_ce_6: 0.2282  loss_mask_6: 0.3498  loss_dice_6: 2.55  loss_ce_7: 0.2335  loss_mask_7: 0.3475  loss_dice_7: 2.555  loss_ce_8: 0.2342  loss_mask_8: 0.3464  loss_dice_8: 2.556  time: 1.5309  data_time: 0.0936  lr: 7.1173e-06  max_mem: 21478M
[01/17 16:57:32] d2.utils.events INFO:  eta: 1 day, 1:54:03  iter: 28339  total_loss: 33.31  loss_ce: 0.2451  loss_mask: 0.334  loss_dice: 2.681  loss_ce_0: 0.5721  loss_mask_0: 0.3422  loss_dice_0: 2.782  loss_ce_1: 0.271  loss_mask_1: 0.3432  loss_dice_1: 2.725  loss_ce_2: 0.2737  loss_mask_2: 0.3368  loss_dice_2: 2.705  loss_ce_3: 0.2566  loss_mask_3: 0.3363  loss_dice_3: 2.696  loss_ce_4: 0.2446  loss_mask_4: 0.3356  loss_dice_4: 2.689  loss_ce_5: 0.2451  loss_mask_5: 0.335  loss_dice_5: 2.693  loss_ce_6: 0.2435  loss_mask_6: 0.3346  loss_dice_6: 2.688  loss_ce_7: 0.2438  loss_mask_7: 0.334  loss_dice_7: 2.691  loss_ce_8: 0.2543  loss_mask_8: 0.3333  loss_dice_8: 2.677  time: 1.5309  data_time: 0.0861  lr: 7.1153e-06  max_mem: 21478M
[01/17 16:58:02] d2.utils.events INFO:  eta: 1 day, 1:52:54  iter: 28359  total_loss: 33.27  loss_ce: 0.2316  loss_mask: 0.3408  loss_dice: 2.673  loss_ce_0: 0.5692  loss_mask_0: 0.3469  loss_dice_0: 2.79  loss_ce_1: 0.2875  loss_mask_1: 0.3463  loss_dice_1: 2.72  loss_ce_2: 0.2723  loss_mask_2: 0.3408  loss_dice_2: 2.699  loss_ce_3: 0.2512  loss_mask_3: 0.3398  loss_dice_3: 2.681  loss_ce_4: 0.2578  loss_mask_4: 0.3393  loss_dice_4: 2.686  loss_ce_5: 0.2494  loss_mask_5: 0.3405  loss_dice_5: 2.682  loss_ce_6: 0.241  loss_mask_6: 0.3406  loss_dice_6: 2.678  loss_ce_7: 0.2424  loss_mask_7: 0.3393  loss_dice_7: 2.668  loss_ce_8: 0.2357  loss_mask_8: 0.339  loss_dice_8: 2.675  time: 1.5309  data_time: 0.0967  lr: 7.1132e-06  max_mem: 21478M
[01/17 16:58:32] d2.utils.events INFO:  eta: 1 day, 1:51:07  iter: 28379  total_loss: 33.03  loss_ce: 0.2583  loss_mask: 0.3371  loss_dice: 2.648  loss_ce_0: 0.5834  loss_mask_0: 0.3575  loss_dice_0: 2.759  loss_ce_1: 0.2941  loss_mask_1: 0.3465  loss_dice_1: 2.695  loss_ce_2: 0.2811  loss_mask_2: 0.3371  loss_dice_2: 2.673  loss_ce_3: 0.2638  loss_mask_3: 0.339  loss_dice_3: 2.663  loss_ce_4: 0.2639  loss_mask_4: 0.3377  loss_dice_4: 2.661  loss_ce_5: 0.2515  loss_mask_5: 0.3388  loss_dice_5: 2.666  loss_ce_6: 0.2445  loss_mask_6: 0.3375  loss_dice_6: 2.662  loss_ce_7: 0.2506  loss_mask_7: 0.3364  loss_dice_7: 2.658  loss_ce_8: 0.2537  loss_mask_8: 0.3357  loss_dice_8: 2.664  time: 1.5309  data_time: 0.0844  lr: 7.1111e-06  max_mem: 21478M
[01/17 16:59:02] d2.utils.events INFO:  eta: 1 day, 1:50:44  iter: 28399  total_loss: 33.8  loss_ce: 0.2513  loss_mask: 0.3487  loss_dice: 2.695  loss_ce_0: 0.5786  loss_mask_0: 0.3572  loss_dice_0: 2.803  loss_ce_1: 0.2729  loss_mask_1: 0.3544  loss_dice_1: 2.73  loss_ce_2: 0.2623  loss_mask_2: 0.3489  loss_dice_2: 2.719  loss_ce_3: 0.2646  loss_mask_3: 0.3464  loss_dice_3: 2.708  loss_ce_4: 0.2514  loss_mask_4: 0.346  loss_dice_4: 2.705  loss_ce_5: 0.2533  loss_mask_5: 0.3469  loss_dice_5: 2.706  loss_ce_6: 0.247  loss_mask_6: 0.3476  loss_dice_6: 2.7  loss_ce_7: 0.2492  loss_mask_7: 0.3466  loss_dice_7: 2.692  loss_ce_8: 0.2518  loss_mask_8: 0.3476  loss_dice_8: 2.703  time: 1.5309  data_time: 0.0944  lr: 7.109e-06  max_mem: 21478M
[01/17 16:59:32] d2.utils.events INFO:  eta: 1 day, 1:50:01  iter: 28419  total_loss: 32.61  loss_ce: 0.2303  loss_mask: 0.3436  loss_dice: 2.626  loss_ce_0: 0.5242  loss_mask_0: 0.3485  loss_dice_0: 2.73  loss_ce_1: 0.2898  loss_mask_1: 0.3466  loss_dice_1: 2.657  loss_ce_2: 0.2719  loss_mask_2: 0.3446  loss_dice_2: 2.647  loss_ce_3: 0.2552  loss_mask_3: 0.3447  loss_dice_3: 2.628  loss_ce_4: 0.241  loss_mask_4: 0.3431  loss_dice_4: 2.626  loss_ce_5: 0.2521  loss_mask_5: 0.3432  loss_dice_5: 2.625  loss_ce_6: 0.2472  loss_mask_6: 0.344  loss_dice_6: 2.616  loss_ce_7: 0.229  loss_mask_7: 0.3426  loss_dice_7: 2.627  loss_ce_8: 0.2346  loss_mask_8: 0.3429  loss_dice_8: 2.629  time: 1.5308  data_time: 0.0858  lr: 7.107e-06  max_mem: 21478M
[01/17 17:00:01] d2.utils.events INFO:  eta: 1 day, 1:48:23  iter: 28439  total_loss: 32.1  loss_ce: 0.2194  loss_mask: 0.3326  loss_dice: 2.595  loss_ce_0: 0.5421  loss_mask_0: 0.3408  loss_dice_0: 2.718  loss_ce_1: 0.2784  loss_mask_1: 0.3382  loss_dice_1: 2.627  loss_ce_2: 0.2558  loss_mask_2: 0.3372  loss_dice_2: 2.618  loss_ce_3: 0.2268  loss_mask_3: 0.3358  loss_dice_3: 2.606  loss_ce_4: 0.2275  loss_mask_4: 0.3342  loss_dice_4: 2.597  loss_ce_5: 0.2392  loss_mask_5: 0.3337  loss_dice_5: 2.599  loss_ce_6: 0.222  loss_mask_6: 0.3338  loss_dice_6: 2.593  loss_ce_7: 0.2153  loss_mask_7: 0.3339  loss_dice_7: 2.589  loss_ce_8: 0.2337  loss_mask_8: 0.3324  loss_dice_8: 2.581  time: 1.5308  data_time: 0.0863  lr: 7.1049e-06  max_mem: 21478M
[01/17 17:00:31] d2.utils.events INFO:  eta: 1 day, 1:46:10  iter: 28459  total_loss: 32.39  loss_ce: 0.2302  loss_mask: 0.3377  loss_dice: 2.612  loss_ce_0: 0.5558  loss_mask_0: 0.3441  loss_dice_0: 2.711  loss_ce_1: 0.2794  loss_mask_1: 0.3449  loss_dice_1: 2.653  loss_ce_2: 0.2645  loss_mask_2: 0.3402  loss_dice_2: 2.634  loss_ce_3: 0.2553  loss_mask_3: 0.3394  loss_dice_3: 2.623  loss_ce_4: 0.2509  loss_mask_4: 0.3393  loss_dice_4: 2.623  loss_ce_5: 0.2444  loss_mask_5: 0.3389  loss_dice_5: 2.618  loss_ce_6: 0.241  loss_mask_6: 0.3378  loss_dice_6: 2.616  loss_ce_7: 0.2382  loss_mask_7: 0.3373  loss_dice_7: 2.614  loss_ce_8: 0.2448  loss_mask_8: 0.3379  loss_dice_8: 2.615  time: 1.5308  data_time: 0.0906  lr: 7.1028e-06  max_mem: 21478M
[01/17 17:01:01] d2.utils.events INFO:  eta: 1 day, 1:45:34  iter: 28479  total_loss: 33.14  loss_ce: 0.2619  loss_mask: 0.3531  loss_dice: 2.601  loss_ce_0: 0.5746  loss_mask_0: 0.3661  loss_dice_0: 2.713  loss_ce_1: 0.2982  loss_mask_1: 0.3642  loss_dice_1: 2.642  loss_ce_2: 0.2844  loss_mask_2: 0.3602  loss_dice_2: 2.62  loss_ce_3: 0.2756  loss_mask_3: 0.3577  loss_dice_3: 2.608  loss_ce_4: 0.2531  loss_mask_4: 0.3567  loss_dice_4: 2.612  loss_ce_5: 0.2723  loss_mask_5: 0.3544  loss_dice_5: 2.612  loss_ce_6: 0.244  loss_mask_6: 0.3557  loss_dice_6: 2.611  loss_ce_7: 0.2655  loss_mask_7: 0.3551  loss_dice_7: 2.6  loss_ce_8: 0.2527  loss_mask_8: 0.3539  loss_dice_8: 2.612  time: 1.5307  data_time: 0.0850  lr: 7.1007e-06  max_mem: 21478M
[01/17 17:01:31] d2.utils.events INFO:  eta: 1 day, 1:43:22  iter: 28499  total_loss: 32.53  loss_ce: 0.2454  loss_mask: 0.3535  loss_dice: 2.604  loss_ce_0: 0.5614  loss_mask_0: 0.3572  loss_dice_0: 2.709  loss_ce_1: 0.2887  loss_mask_1: 0.3567  loss_dice_1: 2.647  loss_ce_2: 0.2672  loss_mask_2: 0.3497  loss_dice_2: 2.615  loss_ce_3: 0.2617  loss_mask_3: 0.3482  loss_dice_3: 2.613  loss_ce_4: 0.2509  loss_mask_4: 0.35  loss_dice_4: 2.615  loss_ce_5: 0.2306  loss_mask_5: 0.3479  loss_dice_5: 2.614  loss_ce_6: 0.2388  loss_mask_6: 0.3488  loss_dice_6: 2.619  loss_ce_7: 0.2487  loss_mask_7: 0.3496  loss_dice_7: 2.613  loss_ce_8: 0.2529  loss_mask_8: 0.3514  loss_dice_8: 2.599  time: 1.5307  data_time: 0.0861  lr: 7.0986e-06  max_mem: 21478M
[01/17 17:02:00] d2.utils.events INFO:  eta: 1 day, 1:40:54  iter: 28519  total_loss: 32.05  loss_ce: 0.2126  loss_mask: 0.3271  loss_dice: 2.58  loss_ce_0: 0.5418  loss_mask_0: 0.3349  loss_dice_0: 2.679  loss_ce_1: 0.2725  loss_mask_1: 0.3292  loss_dice_1: 2.613  loss_ce_2: 0.2527  loss_mask_2: 0.3267  loss_dice_2: 2.604  loss_ce_3: 0.2558  loss_mask_3: 0.3262  loss_dice_3: 2.592  loss_ce_4: 0.2239  loss_mask_4: 0.3273  loss_dice_4: 2.582  loss_ce_5: 0.2309  loss_mask_5: 0.327  loss_dice_5: 2.582  loss_ce_6: 0.2147  loss_mask_6: 0.327  loss_dice_6: 2.577  loss_ce_7: 0.209  loss_mask_7: 0.3271  loss_dice_7: 2.579  loss_ce_8: 0.2266  loss_mask_8: 0.3269  loss_dice_8: 2.581  time: 1.5307  data_time: 0.0893  lr: 7.0966e-06  max_mem: 21478M
[01/17 17:02:31] d2.utils.events INFO:  eta: 1 day, 1:40:24  iter: 28539  total_loss: 33.06  loss_ce: 0.2332  loss_mask: 0.3396  loss_dice: 2.632  loss_ce_0: 0.5563  loss_mask_0: 0.3422  loss_dice_0: 2.74  loss_ce_1: 0.2611  loss_mask_1: 0.3436  loss_dice_1: 2.677  loss_ce_2: 0.2683  loss_mask_2: 0.3437  loss_dice_2: 2.65  loss_ce_3: 0.2666  loss_mask_3: 0.3407  loss_dice_3: 2.645  loss_ce_4: 0.2596  loss_mask_4: 0.3405  loss_dice_4: 2.639  loss_ce_5: 0.2396  loss_mask_5: 0.3407  loss_dice_5: 2.644  loss_ce_6: 0.2544  loss_mask_6: 0.341  loss_dice_6: 2.635  loss_ce_7: 0.2437  loss_mask_7: 0.3416  loss_dice_7: 2.637  loss_ce_8: 0.2393  loss_mask_8: 0.3407  loss_dice_8: 2.63  time: 1.5307  data_time: 0.0884  lr: 7.0945e-06  max_mem: 21478M
[01/17 17:02:51] d2.engine.hooks INFO: Overall training speed: 28551 iterations in 12:08:23 (1.5307 s / it)
[01/17 17:02:51] d2.engine.hooks INFO: Total training time: 13:58:43 (1:50:20 on hooks)
[01/17 17:02:51] d2.utils.events INFO:  eta: 1 day, 1:39:24  iter: 28553  total_loss: 32.51  loss_ce: 0.258  loss_mask: 0.3458  loss_dice: 2.603  loss_ce_0: 0.5832  loss_mask_0: 0.3524  loss_dice_0: 2.719  loss_ce_1: 0.3142  loss_mask_1: 0.3502  loss_dice_1: 2.651  loss_ce_2: 0.2719  loss_mask_2: 0.3493  loss_dice_2: 2.622  loss_ce_3: 0.2598  loss_mask_3: 0.3463  loss_dice_3: 2.61  loss_ce_4: 0.2746  loss_mask_4: 0.3475  loss_dice_4: 2.61  loss_ce_5: 0.2663  loss_mask_5: 0.3485  loss_dice_5: 2.607  loss_ce_6: 0.2626  loss_mask_6: 0.3461  loss_dice_6: 2.606  loss_ce_7: 0.2584  loss_mask_7: 0.347  loss_dice_7: 2.608  loss_ce_8: 0.2589  loss_mask_8: 0.346  loss_dice_8: 2.597  time: 1.5306  data_time: 0.0889  lr: 7.0931e-06  max_mem: 21478M
