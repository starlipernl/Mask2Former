[01/17 17:03:56] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 17:04:00] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 17:04:00] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 17:04:00] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 17:04:00] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 17:04:00] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 17:04:00] d2.utils.env INFO: Using a generated random seed 662939
[01/17 17:04:02] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 17:04:02] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:04:05] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 17:04:07] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 17:04:07] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 17:04:07] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 17:04:07] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 17:04:07] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 17:04:07] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 17:04:07] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 17:04:07] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 17:05:04] d2.utils.events INFO:  eta: 1 day, 0:10:43  iter: 19  total_loss: 149.1  loss_ce: 9.87  loss_mask: 0.57  loss_dice: 4.91  loss_ce_0: 9.207  loss_mask_0: 0.3451  loss_dice_0: 4.882  loss_ce_1: 9.342  loss_mask_1: 0.3394  loss_dice_1: 4.872  loss_ce_2: 9.42  loss_mask_2: 0.3826  loss_dice_2: 4.879  loss_ce_3: 9.683  loss_mask_3: 0.4845  loss_dice_3: 4.838  loss_ce_4: 9.788  loss_mask_4: 0.5369  loss_dice_4: 4.87  loss_ce_5: 9.814  loss_mask_5: 0.4696  loss_dice_5: 4.895  loss_ce_6: 9.908  loss_mask_6: 0.5051  loss_dice_6: 4.891  loss_ce_7: 10.01  loss_mask_7: 0.5898  loss_dice_7: 4.88  loss_ce_8: 9.933  loss_mask_8: 0.5693  loss_dice_8: 4.909  time: 2.2178  data_time: 0.5906  lr: 9.9957e-06  max_mem: 20535M
[01/17 17:05:31] d2.engine.hooks INFO: Overall training speed: 34 iterations in 0:01:06 (1.9544 s / it)
[01/17 17:05:31] d2.engine.hooks INFO: Total training time: 0:01:06 (0:00:00 on hooks)
[01/17 17:05:31] d2.utils.events INFO:  eta: 22:38:26  iter: 36  total_loss: 134.5  loss_ce: 9.194  loss_mask: 0.5377  loss_dice: 4.547  loss_ce_0: 9.084  loss_mask_0: 0.3095  loss_dice_0: 4.485  loss_ce_1: 8.158  loss_mask_1: 0.299  loss_dice_1: 4.473  loss_ce_2: 8.075  loss_mask_2: 0.3069  loss_dice_2: 4.483  loss_ce_3: 8.06  loss_mask_3: 0.3062  loss_dice_3: 4.488  loss_ce_4: 8.153  loss_mask_4: 0.3082  loss_dice_4: 4.473  loss_ce_5: 8.468  loss_mask_5: 0.318  loss_dice_5: 4.483  loss_ce_6: 8.724  loss_mask_6: 0.3284  loss_dice_6: 4.495  loss_ce_7: 8.963  loss_mask_7: 0.4237  loss_dice_7: 4.525  loss_ce_8: 9.12  loss_mask_8: 0.4848  loss_dice_8: 4.545  time: 1.9371  data_time: 0.0910  lr: 9.9921e-06  max_mem: 20535M
[01/17 17:06:34] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 17:06:38] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 17:06:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 17:06:38] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 17:06:38] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 17:06:38] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 17:06:38] d2.utils.env INFO: Using a generated random seed 38360034
[01/17 17:06:39] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 17:06:39] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:06:43] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 17:06:43] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 17:06:43] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 17:06:43] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 17:06:43] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 17:06:44] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 17:06:44] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 17:06:44] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 17:06:44] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 17:07:40] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:07:41] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:07:41] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:07:42] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 17:07:54] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0165 s/iter. Inference: 0.2251 s/iter. Eval: 0.0213 s/iter. Total: 0.2630 s/iter. ETA=0:04:44
[01/17 17:07:59] d2.evaluation.evaluator INFO: Inference done 33/1093. Dataloading: 0.0162 s/iter. Inference: 0.2015 s/iter. Eval: 0.0219 s/iter. Total: 0.2397 s/iter. ETA=0:04:14
[01/17 17:08:04] d2.evaluation.evaluator INFO: Inference done 55/1093. Dataloading: 0.0170 s/iter. Inference: 0.1973 s/iter. Eval: 0.0223 s/iter. Total: 0.2367 s/iter. ETA=0:04:05
[01/17 17:08:09] d2.evaluation.evaluator INFO: Inference done 77/1093. Dataloading: 0.0174 s/iter. Inference: 0.1955 s/iter. Eval: 0.0225 s/iter. Total: 0.2355 s/iter. ETA=0:03:59
[01/17 17:08:14] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0172 s/iter. Inference: 0.1928 s/iter. Eval: 0.0227 s/iter. Total: 0.2328 s/iter. ETA=0:03:51
[01/17 17:08:19] d2.evaluation.evaluator INFO: Inference done 123/1093. Dataloading: 0.0167 s/iter. Inference: 0.1913 s/iter. Eval: 0.0225 s/iter. Total: 0.2306 s/iter. ETA=0:03:43
[01/17 17:08:24] d2.evaluation.evaluator INFO: Inference done 147/1093. Dataloading: 0.0164 s/iter. Inference: 0.1903 s/iter. Eval: 0.0220 s/iter. Total: 0.2289 s/iter. ETA=0:03:36
[01/17 17:08:30] d2.evaluation.evaluator INFO: Inference done 170/1093. Dataloading: 0.0164 s/iter. Inference: 0.1903 s/iter. Eval: 0.0217 s/iter. Total: 0.2285 s/iter. ETA=0:03:30
[01/17 17:08:35] d2.evaluation.evaluator INFO: Inference done 194/1093. Dataloading: 0.0163 s/iter. Inference: 0.1888 s/iter. Eval: 0.0217 s/iter. Total: 0.2269 s/iter. ETA=0:03:23
[01/17 17:08:40] d2.evaluation.evaluator INFO: Inference done 216/1093. Dataloading: 0.0164 s/iter. Inference: 0.1888 s/iter. Eval: 0.0218 s/iter. Total: 0.2271 s/iter. ETA=0:03:19
[01/17 17:08:45] d2.evaluation.evaluator INFO: Inference done 238/1093. Dataloading: 0.0165 s/iter. Inference: 0.1893 s/iter. Eval: 0.0220 s/iter. Total: 0.2280 s/iter. ETA=0:03:14
[01/17 17:08:50] d2.evaluation.evaluator INFO: Inference done 260/1093. Dataloading: 0.0168 s/iter. Inference: 0.1892 s/iter. Eval: 0.0218 s/iter. Total: 0.2279 s/iter. ETA=0:03:09
[01/17 17:08:55] d2.evaluation.evaluator INFO: Inference done 283/1093. Dataloading: 0.0167 s/iter. Inference: 0.1887 s/iter. Eval: 0.0219 s/iter. Total: 0.2273 s/iter. ETA=0:03:04
[01/17 17:09:00] d2.evaluation.evaluator INFO: Inference done 303/1093. Dataloading: 0.0167 s/iter. Inference: 0.1905 s/iter. Eval: 0.0218 s/iter. Total: 0.2290 s/iter. ETA=0:03:00
[01/17 17:09:05] d2.evaluation.evaluator INFO: Inference done 329/1093. Dataloading: 0.0165 s/iter. Inference: 0.1881 s/iter. Eval: 0.0216 s/iter. Total: 0.2263 s/iter. ETA=0:02:52
[01/17 17:09:10] d2.evaluation.evaluator INFO: Inference done 352/1093. Dataloading: 0.0162 s/iter. Inference: 0.1880 s/iter. Eval: 0.0215 s/iter. Total: 0.2258 s/iter. ETA=0:02:47
[01/17 17:09:15] d2.evaluation.evaluator INFO: Inference done 374/1093. Dataloading: 0.0165 s/iter. Inference: 0.1877 s/iter. Eval: 0.0218 s/iter. Total: 0.2261 s/iter. ETA=0:02:42
[01/17 17:09:21] d2.evaluation.evaluator INFO: Inference done 396/1093. Dataloading: 0.0164 s/iter. Inference: 0.1881 s/iter. Eval: 0.0219 s/iter. Total: 0.2266 s/iter. ETA=0:02:37
[01/17 17:09:26] d2.evaluation.evaluator INFO: Inference done 417/1093. Dataloading: 0.0164 s/iter. Inference: 0.1890 s/iter. Eval: 0.0218 s/iter. Total: 0.2274 s/iter. ETA=0:02:33
[01/17 17:09:31] d2.evaluation.evaluator INFO: Inference done 440/1093. Dataloading: 0.0165 s/iter. Inference: 0.1887 s/iter. Eval: 0.0218 s/iter. Total: 0.2272 s/iter. ETA=0:02:28
[01/17 17:09:36] d2.evaluation.evaluator INFO: Inference done 462/1093. Dataloading: 0.0165 s/iter. Inference: 0.1889 s/iter. Eval: 0.0218 s/iter. Total: 0.2272 s/iter. ETA=0:02:23
[01/17 17:09:41] d2.evaluation.evaluator INFO: Inference done 486/1093. Dataloading: 0.0163 s/iter. Inference: 0.1885 s/iter. Eval: 0.0217 s/iter. Total: 0.2266 s/iter. ETA=0:02:17
[01/17 17:09:46] d2.evaluation.evaluator INFO: Inference done 507/1093. Dataloading: 0.0163 s/iter. Inference: 0.1889 s/iter. Eval: 0.0218 s/iter. Total: 0.2271 s/iter. ETA=0:02:13
[01/17 17:09:51] d2.evaluation.evaluator INFO: Inference done 528/1093. Dataloading: 0.0162 s/iter. Inference: 0.1897 s/iter. Eval: 0.0218 s/iter. Total: 0.2278 s/iter. ETA=0:02:08
[01/17 17:09:56] d2.evaluation.evaluator INFO: Inference done 551/1093. Dataloading: 0.0162 s/iter. Inference: 0.1896 s/iter. Eval: 0.0218 s/iter. Total: 0.2277 s/iter. ETA=0:02:03
[01/17 17:10:01] d2.evaluation.evaluator INFO: Inference done 575/1093. Dataloading: 0.0161 s/iter. Inference: 0.1892 s/iter. Eval: 0.0217 s/iter. Total: 0.2272 s/iter. ETA=0:01:57
[01/17 17:10:07] d2.evaluation.evaluator INFO: Inference done 597/1093. Dataloading: 0.0161 s/iter. Inference: 0.1895 s/iter. Eval: 0.0217 s/iter. Total: 0.2274 s/iter. ETA=0:01:52
[01/17 17:10:12] d2.evaluation.evaluator INFO: Inference done 617/1093. Dataloading: 0.0162 s/iter. Inference: 0.1903 s/iter. Eval: 0.0218 s/iter. Total: 0.2284 s/iter. ETA=0:01:48
[01/17 17:10:17] d2.evaluation.evaluator INFO: Inference done 639/1093. Dataloading: 0.0161 s/iter. Inference: 0.1904 s/iter. Eval: 0.0217 s/iter. Total: 0.2283 s/iter. ETA=0:01:43
[01/17 17:10:22] d2.evaluation.evaluator INFO: Inference done 663/1093. Dataloading: 0.0161 s/iter. Inference: 0.1899 s/iter. Eval: 0.0217 s/iter. Total: 0.2279 s/iter. ETA=0:01:37
[01/17 17:10:27] d2.evaluation.evaluator INFO: Inference done 684/1093. Dataloading: 0.0160 s/iter. Inference: 0.1906 s/iter. Eval: 0.0217 s/iter. Total: 0.2284 s/iter. ETA=0:01:33
[01/17 17:10:32] d2.evaluation.evaluator INFO: Inference done 707/1093. Dataloading: 0.0160 s/iter. Inference: 0.1905 s/iter. Eval: 0.0217 s/iter. Total: 0.2282 s/iter. ETA=0:01:28
[01/17 17:10:37] d2.evaluation.evaluator INFO: Inference done 732/1093. Dataloading: 0.0159 s/iter. Inference: 0.1896 s/iter. Eval: 0.0216 s/iter. Total: 0.2272 s/iter. ETA=0:01:22
[01/17 17:10:42] d2.evaluation.evaluator INFO: Inference done 755/1093. Dataloading: 0.0159 s/iter. Inference: 0.1893 s/iter. Eval: 0.0216 s/iter. Total: 0.2270 s/iter. ETA=0:01:16
[01/17 17:10:47] d2.evaluation.evaluator INFO: Inference done 778/1093. Dataloading: 0.0159 s/iter. Inference: 0.1893 s/iter. Eval: 0.0216 s/iter. Total: 0.2269 s/iter. ETA=0:01:11
[01/17 17:10:52] d2.evaluation.evaluator INFO: Inference done 800/1093. Dataloading: 0.0158 s/iter. Inference: 0.1894 s/iter. Eval: 0.0217 s/iter. Total: 0.2271 s/iter. ETA=0:01:06
[01/17 17:10:58] d2.evaluation.evaluator INFO: Inference done 822/1093. Dataloading: 0.0157 s/iter. Inference: 0.1897 s/iter. Eval: 0.0217 s/iter. Total: 0.2273 s/iter. ETA=0:01:01
[01/17 17:11:03] d2.evaluation.evaluator INFO: Inference done 845/1093. Dataloading: 0.0157 s/iter. Inference: 0.1896 s/iter. Eval: 0.0217 s/iter. Total: 0.2271 s/iter. ETA=0:00:56
[01/17 17:11:08] d2.evaluation.evaluator INFO: Inference done 866/1093. Dataloading: 0.0157 s/iter. Inference: 0.1900 s/iter. Eval: 0.0217 s/iter. Total: 0.2275 s/iter. ETA=0:00:51
[01/17 17:11:13] d2.evaluation.evaluator INFO: Inference done 888/1093. Dataloading: 0.0157 s/iter. Inference: 0.1900 s/iter. Eval: 0.0218 s/iter. Total: 0.2276 s/iter. ETA=0:00:46
[01/17 17:11:18] d2.evaluation.evaluator INFO: Inference done 911/1093. Dataloading: 0.0157 s/iter. Inference: 0.1898 s/iter. Eval: 0.0218 s/iter. Total: 0.2275 s/iter. ETA=0:00:41
[01/17 17:11:23] d2.evaluation.evaluator INFO: Inference done 934/1093. Dataloading: 0.0157 s/iter. Inference: 0.1898 s/iter. Eval: 0.0218 s/iter. Total: 0.2275 s/iter. ETA=0:00:36
[01/17 17:11:28] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0158 s/iter. Inference: 0.1895 s/iter. Eval: 0.0218 s/iter. Total: 0.2271 s/iter. ETA=0:00:30
[01/17 17:11:34] d2.evaluation.evaluator INFO: Inference done 983/1093. Dataloading: 0.0157 s/iter. Inference: 0.1890 s/iter. Eval: 0.0218 s/iter. Total: 0.2266 s/iter. ETA=0:00:24
[01/17 17:11:39] d2.evaluation.evaluator INFO: Inference done 1007/1093. Dataloading: 0.0157 s/iter. Inference: 0.1887 s/iter. Eval: 0.0218 s/iter. Total: 0.2263 s/iter. ETA=0:00:19
[01/17 17:11:44] d2.evaluation.evaluator INFO: Inference done 1028/1093. Dataloading: 0.0157 s/iter. Inference: 0.1888 s/iter. Eval: 0.0220 s/iter. Total: 0.2266 s/iter. ETA=0:00:14
[01/17 17:11:49] d2.evaluation.evaluator INFO: Inference done 1051/1093. Dataloading: 0.0157 s/iter. Inference: 0.1888 s/iter. Eval: 0.0219 s/iter. Total: 0.2265 s/iter. ETA=0:00:09
[01/17 17:11:54] d2.evaluation.evaluator INFO: Inference done 1074/1093. Dataloading: 0.0156 s/iter. Inference: 0.1889 s/iter. Eval: 0.0219 s/iter. Total: 0.2265 s/iter. ETA=0:00:04
[01/17 17:11:59] d2.evaluation.evaluator INFO: Total inference time: 0:04:06.921721 (0.226950 s / iter per device, on 4 devices)
[01/17 17:11:59] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:25 (0.188932 s / iter per device, on 4 devices)
[01/17 17:12:04] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 0.006613895334947786, 'fwIoU': 0.015958106911914783, 'IoU-0': nan, 'IoU-1': 0.0, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 0.0, 'IoU-20': 0.0, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 0.0, 'IoU-28': 0.0, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.0, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 0.0, 'IoU-42': 1.2632540089750273, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.0, 'IoU-50': 0.0, 'IoU-51': 0.0, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.0, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 0.5235602094240838, 'pACC': 1.2632540089750273, 'ACC-0': nan, 'ACC-1': 0.0, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 0.0, 'ACC-20': 0.0, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 0.0, 'ACC-28': 0.0, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 0.0, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 0.0, 'ACC-42': 100.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 0.0, 'ACC-50': 0.0, 'ACC-51': 0.0, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 0.0, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 17:12:04] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 17:12:04] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 17:12:04] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 17:12:04] d2.evaluation.testing INFO: copypaste: 0.0066,0.0160,0.5236,1.2633
[01/17 17:12:04] d2.utils.events INFO:  eta: 1 day, 0:00:13  iter: 19  total_loss: 147.5  loss_ce: 9.666  loss_mask: 0.3554  loss_dice: 4.911  loss_ce_0: 9.305  loss_mask_0: 0.3354  loss_dice_0: 4.856  loss_ce_1: 9.167  loss_mask_1: 0.3542  loss_dice_1: 4.869  loss_ce_2: 9.413  loss_mask_2: 0.3427  loss_dice_2: 4.896  loss_ce_3: 9.532  loss_mask_3: 0.3509  loss_dice_3: 4.902  loss_ce_4: 9.59  loss_mask_4: 0.3798  loss_dice_4: 4.904  loss_ce_5: 9.671  loss_mask_5: 0.4251  loss_dice_5: 4.907  loss_ce_6: 9.742  loss_mask_6: 0.4559  loss_dice_6: 4.911  loss_ce_7: 9.757  loss_mask_7: 0.4541  loss_dice_7: 4.912  loss_ce_8: 9.682  loss_mask_8: 0.5972  loss_dice_8: 4.91  time: 2.1796  data_time: 0.5928  lr: 9.9957e-06  max_mem: 20298M
[01/17 17:12:39] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:12:39] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:12:39] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:12:40] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 17:12:53] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0276 s/iter. Inference: 0.2107 s/iter. Eval: 0.0205 s/iter. Total: 0.2588 s/iter. ETA=0:04:40
[01/17 17:12:58] d2.evaluation.evaluator INFO: Inference done 30/1093. Dataloading: 0.0188 s/iter. Inference: 0.2201 s/iter. Eval: 0.0243 s/iter. Total: 0.2633 s/iter. ETA=0:04:39
[01/17 17:13:03] d2.evaluation.evaluator INFO: Inference done 53/1093. Dataloading: 0.0181 s/iter. Inference: 0.2032 s/iter. Eval: 0.0245 s/iter. Total: 0.2460 s/iter. ETA=0:04:15
[01/17 17:13:08] d2.evaluation.evaluator INFO: Inference done 76/1093. Dataloading: 0.0181 s/iter. Inference: 0.1963 s/iter. Eval: 0.0241 s/iter. Total: 0.2386 s/iter. ETA=0:04:02
[01/17 17:13:14] d2.evaluation.evaluator INFO: Inference done 96/1093. Dataloading: 0.0197 s/iter. Inference: 0.1994 s/iter. Eval: 0.0241 s/iter. Total: 0.2432 s/iter. ETA=0:04:02
[01/17 17:13:19] d2.evaluation.evaluator INFO: Inference done 114/1093. Dataloading: 0.0193 s/iter. Inference: 0.2068 s/iter. Eval: 0.0239 s/iter. Total: 0.2501 s/iter. ETA=0:04:04
[01/17 17:13:24] d2.evaluation.evaluator INFO: Inference done 133/1093. Dataloading: 0.0189 s/iter. Inference: 0.2108 s/iter. Eval: 0.0236 s/iter. Total: 0.2533 s/iter. ETA=0:04:03
[01/17 17:13:29] d2.evaluation.evaluator INFO: Inference done 154/1093. Dataloading: 0.0182 s/iter. Inference: 0.2109 s/iter. Eval: 0.0234 s/iter. Total: 0.2525 s/iter. ETA=0:03:57
[01/17 17:13:34] d2.evaluation.evaluator INFO: Inference done 176/1093. Dataloading: 0.0181 s/iter. Inference: 0.2083 s/iter. Eval: 0.0237 s/iter. Total: 0.2501 s/iter. ETA=0:03:49
[01/17 17:13:39] d2.evaluation.evaluator INFO: Inference done 198/1093. Dataloading: 0.0177 s/iter. Inference: 0.2067 s/iter. Eval: 0.0235 s/iter. Total: 0.2480 s/iter. ETA=0:03:41
[01/17 17:13:44] d2.evaluation.evaluator INFO: Inference done 218/1093. Dataloading: 0.0176 s/iter. Inference: 0.2082 s/iter. Eval: 0.0234 s/iter. Total: 0.2493 s/iter. ETA=0:03:38
[01/17 17:13:50] d2.evaluation.evaluator INFO: Inference done 238/1093. Dataloading: 0.0177 s/iter. Inference: 0.2088 s/iter. Eval: 0.0234 s/iter. Total: 0.2500 s/iter. ETA=0:03:33
[01/17 17:13:55] d2.evaluation.evaluator INFO: Inference done 257/1093. Dataloading: 0.0180 s/iter. Inference: 0.2104 s/iter. Eval: 0.0235 s/iter. Total: 0.2519 s/iter. ETA=0:03:30
[01/17 17:14:00] d2.evaluation.evaluator INFO: Inference done 271/1093. Dataloading: 0.0183 s/iter. Inference: 0.2155 s/iter. Eval: 0.0241 s/iter. Total: 0.2581 s/iter. ETA=0:03:32
[01/17 17:14:05] d2.evaluation.evaluator INFO: Inference done 285/1093. Dataloading: 0.0185 s/iter. Inference: 0.2217 s/iter. Eval: 0.0243 s/iter. Total: 0.2646 s/iter. ETA=0:03:33
[01/17 17:14:11] d2.evaluation.evaluator INFO: Inference done 298/1093. Dataloading: 0.0184 s/iter. Inference: 0.2271 s/iter. Eval: 0.0245 s/iter. Total: 0.2701 s/iter. ETA=0:03:34
[01/17 17:14:16] d2.evaluation.evaluator INFO: Inference done 312/1093. Dataloading: 0.0186 s/iter. Inference: 0.2320 s/iter. Eval: 0.0246 s/iter. Total: 0.2753 s/iter. ETA=0:03:35
[01/17 17:14:21] d2.evaluation.evaluator INFO: Inference done 325/1093. Dataloading: 0.0184 s/iter. Inference: 0.2370 s/iter. Eval: 0.0245 s/iter. Total: 0.2800 s/iter. ETA=0:03:35
[01/17 17:14:26] d2.evaluation.evaluator INFO: Inference done 343/1093. Dataloading: 0.0182 s/iter. Inference: 0.2373 s/iter. Eval: 0.0245 s/iter. Total: 0.2801 s/iter. ETA=0:03:30
[01/17 17:14:31] d2.evaluation.evaluator INFO: Inference done 362/1093. Dataloading: 0.0179 s/iter. Inference: 0.2368 s/iter. Eval: 0.0245 s/iter. Total: 0.2794 s/iter. ETA=0:03:24
[01/17 17:14:36] d2.evaluation.evaluator INFO: Inference done 380/1093. Dataloading: 0.0179 s/iter. Inference: 0.2369 s/iter. Eval: 0.0247 s/iter. Total: 0.2797 s/iter. ETA=0:03:19
[01/17 17:14:41] d2.evaluation.evaluator INFO: Inference done 402/1093. Dataloading: 0.0178 s/iter. Inference: 0.2343 s/iter. Eval: 0.0245 s/iter. Total: 0.2768 s/iter. ETA=0:03:11
[01/17 17:14:46] d2.evaluation.evaluator INFO: Inference done 425/1093. Dataloading: 0.0177 s/iter. Inference: 0.2315 s/iter. Eval: 0.0243 s/iter. Total: 0.2737 s/iter. ETA=0:03:02
[01/17 17:14:51] d2.evaluation.evaluator INFO: Inference done 444/1093. Dataloading: 0.0176 s/iter. Inference: 0.2316 s/iter. Eval: 0.0243 s/iter. Total: 0.2735 s/iter. ETA=0:02:57
[01/17 17:14:57] d2.evaluation.evaluator INFO: Inference done 463/1093. Dataloading: 0.0175 s/iter. Inference: 0.2313 s/iter. Eval: 0.0242 s/iter. Total: 0.2732 s/iter. ETA=0:02:52
[01/17 17:15:02] d2.evaluation.evaluator INFO: Inference done 484/1093. Dataloading: 0.0173 s/iter. Inference: 0.2300 s/iter. Eval: 0.0242 s/iter. Total: 0.2717 s/iter. ETA=0:02:45
[01/17 17:15:07] d2.evaluation.evaluator INFO: Inference done 502/1093. Dataloading: 0.0172 s/iter. Inference: 0.2303 s/iter. Eval: 0.0242 s/iter. Total: 0.2719 s/iter. ETA=0:02:40
[01/17 17:15:12] d2.evaluation.evaluator INFO: Inference done 523/1093. Dataloading: 0.0171 s/iter. Inference: 0.2297 s/iter. Eval: 0.0241 s/iter. Total: 0.2710 s/iter. ETA=0:02:34
[01/17 17:15:17] d2.evaluation.evaluator INFO: Inference done 544/1093. Dataloading: 0.0170 s/iter. Inference: 0.2288 s/iter. Eval: 0.0241 s/iter. Total: 0.2700 s/iter. ETA=0:02:28
[01/17 17:15:22] d2.evaluation.evaluator INFO: Inference done 562/1093. Dataloading: 0.0169 s/iter. Inference: 0.2292 s/iter. Eval: 0.0241 s/iter. Total: 0.2703 s/iter. ETA=0:02:23
[01/17 17:15:27] d2.evaluation.evaluator INFO: Inference done 583/1093. Dataloading: 0.0169 s/iter. Inference: 0.2285 s/iter. Eval: 0.0242 s/iter. Total: 0.2697 s/iter. ETA=0:02:17
[01/17 17:15:32] d2.evaluation.evaluator INFO: Inference done 603/1093. Dataloading: 0.0167 s/iter. Inference: 0.2283 s/iter. Eval: 0.0241 s/iter. Total: 0.2693 s/iter. ETA=0:02:11
[01/17 17:15:37] d2.evaluation.evaluator INFO: Inference done 621/1093. Dataloading: 0.0167 s/iter. Inference: 0.2285 s/iter. Eval: 0.0241 s/iter. Total: 0.2696 s/iter. ETA=0:02:07
[01/17 17:15:42] d2.evaluation.evaluator INFO: Inference done 639/1093. Dataloading: 0.0167 s/iter. Inference: 0.2289 s/iter. Eval: 0.0241 s/iter. Total: 0.2699 s/iter. ETA=0:02:02
[01/17 17:15:48] d2.evaluation.evaluator INFO: Inference done 659/1093. Dataloading: 0.0166 s/iter. Inference: 0.2288 s/iter. Eval: 0.0240 s/iter. Total: 0.2695 s/iter. ETA=0:01:56
[01/17 17:15:53] d2.evaluation.evaluator INFO: Inference done 679/1093. Dataloading: 0.0166 s/iter. Inference: 0.2285 s/iter. Eval: 0.0240 s/iter. Total: 0.2692 s/iter. ETA=0:01:51
[01/17 17:15:58] d2.evaluation.evaluator INFO: Inference done 699/1093. Dataloading: 0.0166 s/iter. Inference: 0.2282 s/iter. Eval: 0.0240 s/iter. Total: 0.2690 s/iter. ETA=0:01:45
[01/17 17:16:03] d2.evaluation.evaluator INFO: Inference done 718/1093. Dataloading: 0.0166 s/iter. Inference: 0.2284 s/iter. Eval: 0.0240 s/iter. Total: 0.2691 s/iter. ETA=0:01:40
[01/17 17:16:09] d2.evaluation.evaluator INFO: Inference done 738/1093. Dataloading: 0.0166 s/iter. Inference: 0.2282 s/iter. Eval: 0.0240 s/iter. Total: 0.2689 s/iter. ETA=0:01:35
[01/17 17:16:14] d2.evaluation.evaluator INFO: Inference done 758/1093. Dataloading: 0.0165 s/iter. Inference: 0.2279 s/iter. Eval: 0.0241 s/iter. Total: 0.2686 s/iter. ETA=0:01:29
[01/17 17:16:19] d2.evaluation.evaluator INFO: Inference done 778/1093. Dataloading: 0.0165 s/iter. Inference: 0.2276 s/iter. Eval: 0.0241 s/iter. Total: 0.2683 s/iter. ETA=0:01:24
[01/17 17:16:24] d2.evaluation.evaluator INFO: Inference done 796/1093. Dataloading: 0.0164 s/iter. Inference: 0.2279 s/iter. Eval: 0.0241 s/iter. Total: 0.2685 s/iter. ETA=0:01:19
[01/17 17:16:29] d2.evaluation.evaluator INFO: Inference done 816/1093. Dataloading: 0.0163 s/iter. Inference: 0.2278 s/iter. Eval: 0.0240 s/iter. Total: 0.2683 s/iter. ETA=0:01:14
[01/17 17:16:34] d2.evaluation.evaluator INFO: Inference done 836/1093. Dataloading: 0.0162 s/iter. Inference: 0.2277 s/iter. Eval: 0.0240 s/iter. Total: 0.2680 s/iter. ETA=0:01:08
[01/17 17:16:39] d2.evaluation.evaluator INFO: Inference done 855/1093. Dataloading: 0.0162 s/iter. Inference: 0.2279 s/iter. Eval: 0.0239 s/iter. Total: 0.2681 s/iter. ETA=0:01:03
[01/17 17:16:45] d2.evaluation.evaluator INFO: Inference done 873/1093. Dataloading: 0.0161 s/iter. Inference: 0.2285 s/iter. Eval: 0.0239 s/iter. Total: 0.2686 s/iter. ETA=0:00:59
[01/17 17:16:50] d2.evaluation.evaluator INFO: Inference done 893/1093. Dataloading: 0.0161 s/iter. Inference: 0.2282 s/iter. Eval: 0.0240 s/iter. Total: 0.2684 s/iter. ETA=0:00:53
[01/17 17:16:55] d2.evaluation.evaluator INFO: Inference done 912/1093. Dataloading: 0.0160 s/iter. Inference: 0.2283 s/iter. Eval: 0.0240 s/iter. Total: 0.2683 s/iter. ETA=0:00:48
[01/17 17:17:00] d2.evaluation.evaluator INFO: Inference done 932/1093. Dataloading: 0.0159 s/iter. Inference: 0.2280 s/iter. Eval: 0.0239 s/iter. Total: 0.2679 s/iter. ETA=0:00:43
[01/17 17:17:05] d2.evaluation.evaluator INFO: Inference done 953/1093. Dataloading: 0.0159 s/iter. Inference: 0.2276 s/iter. Eval: 0.0239 s/iter. Total: 0.2675 s/iter. ETA=0:00:37
[01/17 17:17:10] d2.evaluation.evaluator INFO: Inference done 972/1093. Dataloading: 0.0159 s/iter. Inference: 0.2274 s/iter. Eval: 0.0240 s/iter. Total: 0.2675 s/iter. ETA=0:00:32
[01/17 17:17:15] d2.evaluation.evaluator INFO: Inference done 989/1093. Dataloading: 0.0159 s/iter. Inference: 0.2280 s/iter. Eval: 0.0240 s/iter. Total: 0.2681 s/iter. ETA=0:00:27
[01/17 17:17:20] d2.evaluation.evaluator INFO: Inference done 1008/1093. Dataloading: 0.0159 s/iter. Inference: 0.2282 s/iter. Eval: 0.0240 s/iter. Total: 0.2682 s/iter. ETA=0:00:22
[01/17 17:17:26] d2.evaluation.evaluator INFO: Inference done 1028/1093. Dataloading: 0.0159 s/iter. Inference: 0.2283 s/iter. Eval: 0.0240 s/iter. Total: 0.2682 s/iter. ETA=0:00:17
[01/17 17:17:31] d2.evaluation.evaluator INFO: Inference done 1042/1093. Dataloading: 0.0159 s/iter. Inference: 0.2295 s/iter. Eval: 0.0241 s/iter. Total: 0.2696 s/iter. ETA=0:00:13
[01/17 17:17:36] d2.evaluation.evaluator INFO: Inference done 1054/1093. Dataloading: 0.0159 s/iter. Inference: 0.2312 s/iter. Eval: 0.0241 s/iter. Total: 0.2713 s/iter. ETA=0:00:10
[01/17 17:17:41] d2.evaluation.evaluator INFO: Inference done 1065/1093. Dataloading: 0.0160 s/iter. Inference: 0.2332 s/iter. Eval: 0.0242 s/iter. Total: 0.2734 s/iter. ETA=0:00:07
[01/17 17:17:46] d2.evaluation.evaluator INFO: Inference done 1081/1093. Dataloading: 0.0159 s/iter. Inference: 0.2340 s/iter. Eval: 0.0242 s/iter. Total: 0.2742 s/iter. ETA=0:00:03
[01/17 17:17:50] d2.evaluation.evaluator INFO: Total inference time: 0:04:58.110345 (0.273998 s / iter per device, on 4 devices)
[01/17 17:17:50] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:04:14 (0.233510 s / iter per device, on 4 devices)
[01/17 17:17:51] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 0.0600866135404526, 'fwIoU': 1.0778680530809304, 'IoU-0': nan, 'IoU-1': 10.287278512336059, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 0.0, 'IoU-20': 0.0, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 0.5125502536301989, 'IoU-28': 0.6765337408734687, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.0, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 0.0, 'IoU-42': 0.0, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.0, 'IoU-50': 0.0, 'IoU-51': 0.0001806793867233759, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.0, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 0.5233415243107844, 'pACC': 10.11469027783687, 'ACC-0': nan, 'ACC-1': 97.9220232317638, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 0.0, 'ACC-20': 0.0, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 0.7662937333844224, 'ACC-28': 1.2697281521663355, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 0.0, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 0.0, 'ACC-42': 0.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 0.0, 'ACC-50': 0.0, 'ACC-51': 0.00018602604525520965, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 0.0, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 17:17:51] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 17:17:51] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 17:17:51] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 17:17:51] d2.evaluation.testing INFO: copypaste: 0.0601,1.0779,0.5233,10.1147
[01/17 17:17:51] d2.utils.events INFO:  eta: 21:56:26  iter: 39  total_loss: 133.6  loss_ce: 9.228  loss_mask: 0.4768  loss_dice: 4.511  loss_ce_0: 9.016  loss_mask_0: 0.2944  loss_dice_0: 4.446  loss_ce_1: 8.038  loss_mask_1: 0.2946  loss_dice_1: 4.43  loss_ce_2: 7.978  loss_mask_2: 0.2957  loss_dice_2: 4.436  loss_ce_3: 7.995  loss_mask_3: 0.2984  loss_dice_3: 4.448  loss_ce_4: 7.971  loss_mask_4: 0.2971  loss_dice_4: 4.454  loss_ce_5: 8.225  loss_mask_5: 0.3157  loss_dice_5: 4.451  loss_ce_6: 8.742  loss_mask_6: 0.307  loss_dice_6: 4.474  loss_ce_7: 9.119  loss_mask_7: 0.3783  loss_dice_7: 4.467  loss_ce_8: 9.185  loss_mask_8: 0.4462  loss_dice_8: 4.487  time: 1.9391  data_time: 0.1096  lr: 9.9912e-06  max_mem: 20411M
[01/17 17:18:24] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:18:25] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:18:25] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:18:26] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 17:18:38] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0162 s/iter. Inference: 0.2238 s/iter. Eval: 0.0166 s/iter. Total: 0.2566 s/iter. ETA=0:04:37
[01/17 17:18:43] d2.evaluation.evaluator INFO: Inference done 31/1093. Dataloading: 0.0180 s/iter. Inference: 0.2126 s/iter. Eval: 0.0210 s/iter. Total: 0.2518 s/iter. ETA=0:04:27
[01/17 17:18:48] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0179 s/iter. Inference: 0.2213 s/iter. Eval: 0.0204 s/iter. Total: 0.2598 s/iter. ETA=0:04:30
[01/17 17:18:53] d2.evaluation.evaluator INFO: Inference done 70/1093. Dataloading: 0.0188 s/iter. Inference: 0.2190 s/iter. Eval: 0.0209 s/iter. Total: 0.2588 s/iter. ETA=0:04:24
[01/17 17:18:58] d2.evaluation.evaluator INFO: Inference done 89/1093. Dataloading: 0.0192 s/iter. Inference: 0.2225 s/iter. Eval: 0.0205 s/iter. Total: 0.2623 s/iter. ETA=0:04:23
[01/17 17:19:03] d2.evaluation.evaluator INFO: Inference done 108/1093. Dataloading: 0.0197 s/iter. Inference: 0.2238 s/iter. Eval: 0.0203 s/iter. Total: 0.2639 s/iter. ETA=0:04:19
[01/17 17:19:08] d2.evaluation.evaluator INFO: Inference done 128/1093. Dataloading: 0.0188 s/iter. Inference: 0.2241 s/iter. Eval: 0.0200 s/iter. Total: 0.2629 s/iter. ETA=0:04:13
[01/17 17:19:14] d2.evaluation.evaluator INFO: Inference done 148/1093. Dataloading: 0.0183 s/iter. Inference: 0.2238 s/iter. Eval: 0.0204 s/iter. Total: 0.2626 s/iter. ETA=0:04:08
[01/17 17:19:19] d2.evaluation.evaluator INFO: Inference done 167/1093. Dataloading: 0.0186 s/iter. Inference: 0.2236 s/iter. Eval: 0.0206 s/iter. Total: 0.2629 s/iter. ETA=0:04:03
[01/17 17:19:24] d2.evaluation.evaluator INFO: Inference done 187/1093. Dataloading: 0.0183 s/iter. Inference: 0.2229 s/iter. Eval: 0.0205 s/iter. Total: 0.2619 s/iter. ETA=0:03:57
[01/17 17:19:29] d2.evaluation.evaluator INFO: Inference done 206/1093. Dataloading: 0.0185 s/iter. Inference: 0.2231 s/iter. Eval: 0.0205 s/iter. Total: 0.2622 s/iter. ETA=0:03:52
[01/17 17:19:34] d2.evaluation.evaluator INFO: Inference done 226/1093. Dataloading: 0.0188 s/iter. Inference: 0.2222 s/iter. Eval: 0.0207 s/iter. Total: 0.2618 s/iter. ETA=0:03:46
[01/17 17:19:39] d2.evaluation.evaluator INFO: Inference done 244/1093. Dataloading: 0.0194 s/iter. Inference: 0.2232 s/iter. Eval: 0.0207 s/iter. Total: 0.2634 s/iter. ETA=0:03:43
[01/17 17:19:44] d2.evaluation.evaluator INFO: Inference done 263/1093. Dataloading: 0.0193 s/iter. Inference: 0.2237 s/iter. Eval: 0.0208 s/iter. Total: 0.2640 s/iter. ETA=0:03:39
[01/17 17:19:49] d2.evaluation.evaluator INFO: Inference done 283/1093. Dataloading: 0.0190 s/iter. Inference: 0.2236 s/iter. Eval: 0.0212 s/iter. Total: 0.2638 s/iter. ETA=0:03:33
[01/17 17:19:55] d2.evaluation.evaluator INFO: Inference done 304/1093. Dataloading: 0.0188 s/iter. Inference: 0.2227 s/iter. Eval: 0.0210 s/iter. Total: 0.2626 s/iter. ETA=0:03:27
[01/17 17:20:00] d2.evaluation.evaluator INFO: Inference done 325/1093. Dataloading: 0.0187 s/iter. Inference: 0.2221 s/iter. Eval: 0.0210 s/iter. Total: 0.2619 s/iter. ETA=0:03:21
[01/17 17:20:05] d2.evaluation.evaluator INFO: Inference done 344/1093. Dataloading: 0.0184 s/iter. Inference: 0.2223 s/iter. Eval: 0.0211 s/iter. Total: 0.2620 s/iter. ETA=0:03:16
[01/17 17:20:10] d2.evaluation.evaluator INFO: Inference done 362/1093. Dataloading: 0.0181 s/iter. Inference: 0.2238 s/iter. Eval: 0.0210 s/iter. Total: 0.2630 s/iter. ETA=0:03:12
[01/17 17:20:15] d2.evaluation.evaluator INFO: Inference done 382/1093. Dataloading: 0.0178 s/iter. Inference: 0.2243 s/iter. Eval: 0.0208 s/iter. Total: 0.2631 s/iter. ETA=0:03:07
[01/17 17:20:20] d2.evaluation.evaluator INFO: Inference done 400/1093. Dataloading: 0.0177 s/iter. Inference: 0.2251 s/iter. Eval: 0.0209 s/iter. Total: 0.2640 s/iter. ETA=0:03:02
[01/17 17:20:25] d2.evaluation.evaluator INFO: Inference done 420/1093. Dataloading: 0.0178 s/iter. Inference: 0.2247 s/iter. Eval: 0.0208 s/iter. Total: 0.2636 s/iter. ETA=0:02:57
[01/17 17:20:31] d2.evaluation.evaluator INFO: Inference done 440/1093. Dataloading: 0.0178 s/iter. Inference: 0.2247 s/iter. Eval: 0.0208 s/iter. Total: 0.2636 s/iter. ETA=0:02:52
[01/17 17:20:36] d2.evaluation.evaluator INFO: Inference done 459/1093. Dataloading: 0.0177 s/iter. Inference: 0.2251 s/iter. Eval: 0.0208 s/iter. Total: 0.2639 s/iter. ETA=0:02:47
[01/17 17:20:41] d2.evaluation.evaluator INFO: Inference done 479/1093. Dataloading: 0.0175 s/iter. Inference: 0.2250 s/iter. Eval: 0.0208 s/iter. Total: 0.2635 s/iter. ETA=0:02:41
[01/17 17:20:46] d2.evaluation.evaluator INFO: Inference done 497/1093. Dataloading: 0.0175 s/iter. Inference: 0.2262 s/iter. Eval: 0.0207 s/iter. Total: 0.2646 s/iter. ETA=0:02:37
[01/17 17:20:51] d2.evaluation.evaluator INFO: Inference done 515/1093. Dataloading: 0.0175 s/iter. Inference: 0.2270 s/iter. Eval: 0.0206 s/iter. Total: 0.2652 s/iter. ETA=0:02:33
[01/17 17:20:56] d2.evaluation.evaluator INFO: Inference done 534/1093. Dataloading: 0.0175 s/iter. Inference: 0.2272 s/iter. Eval: 0.0206 s/iter. Total: 0.2656 s/iter. ETA=0:02:28
[01/17 17:21:02] d2.evaluation.evaluator INFO: Inference done 552/1093. Dataloading: 0.0176 s/iter. Inference: 0.2279 s/iter. Eval: 0.0206 s/iter. Total: 0.2663 s/iter. ETA=0:02:24
[01/17 17:21:07] d2.evaluation.evaluator INFO: Inference done 572/1093. Dataloading: 0.0176 s/iter. Inference: 0.2278 s/iter. Eval: 0.0206 s/iter. Total: 0.2663 s/iter. ETA=0:02:18
[01/17 17:21:12] d2.evaluation.evaluator INFO: Inference done 591/1093. Dataloading: 0.0177 s/iter. Inference: 0.2280 s/iter. Eval: 0.0205 s/iter. Total: 0.2665 s/iter. ETA=0:02:13
[01/17 17:21:17] d2.evaluation.evaluator INFO: Inference done 607/1093. Dataloading: 0.0179 s/iter. Inference: 0.2292 s/iter. Eval: 0.0204 s/iter. Total: 0.2678 s/iter. ETA=0:02:10
[01/17 17:21:22] d2.evaluation.evaluator INFO: Inference done 625/1093. Dataloading: 0.0179 s/iter. Inference: 0.2296 s/iter. Eval: 0.0203 s/iter. Total: 0.2681 s/iter. ETA=0:02:05
[01/17 17:21:27] d2.evaluation.evaluator INFO: Inference done 643/1093. Dataloading: 0.0179 s/iter. Inference: 0.2300 s/iter. Eval: 0.0204 s/iter. Total: 0.2686 s/iter. ETA=0:02:00
[01/17 17:21:32] d2.evaluation.evaluator INFO: Inference done 660/1093. Dataloading: 0.0179 s/iter. Inference: 0.2309 s/iter. Eval: 0.0204 s/iter. Total: 0.2694 s/iter. ETA=0:01:56
[01/17 17:21:38] d2.evaluation.evaluator INFO: Inference done 679/1093. Dataloading: 0.0178 s/iter. Inference: 0.2311 s/iter. Eval: 0.0203 s/iter. Total: 0.2694 s/iter. ETA=0:01:51
[01/17 17:21:43] d2.evaluation.evaluator INFO: Inference done 696/1093. Dataloading: 0.0179 s/iter. Inference: 0.2316 s/iter. Eval: 0.0205 s/iter. Total: 0.2702 s/iter. ETA=0:01:47
[01/17 17:21:48] d2.evaluation.evaluator INFO: Inference done 715/1093. Dataloading: 0.0180 s/iter. Inference: 0.2313 s/iter. Eval: 0.0206 s/iter. Total: 0.2701 s/iter. ETA=0:01:42
[01/17 17:21:53] d2.evaluation.evaluator INFO: Inference done 734/1093. Dataloading: 0.0179 s/iter. Inference: 0.2315 s/iter. Eval: 0.0205 s/iter. Total: 0.2701 s/iter. ETA=0:01:36
[01/17 17:21:58] d2.evaluation.evaluator INFO: Inference done 751/1093. Dataloading: 0.0178 s/iter. Inference: 0.2323 s/iter. Eval: 0.0204 s/iter. Total: 0.2708 s/iter. ETA=0:01:32
[01/17 17:22:03] d2.evaluation.evaluator INFO: Inference done 769/1093. Dataloading: 0.0178 s/iter. Inference: 0.2328 s/iter. Eval: 0.0205 s/iter. Total: 0.2713 s/iter. ETA=0:01:27
[01/17 17:22:08] d2.evaluation.evaluator INFO: Inference done 789/1093. Dataloading: 0.0176 s/iter. Inference: 0.2327 s/iter. Eval: 0.0204 s/iter. Total: 0.2710 s/iter. ETA=0:01:22
[01/17 17:22:14] d2.evaluation.evaluator INFO: Inference done 807/1093. Dataloading: 0.0175 s/iter. Inference: 0.2334 s/iter. Eval: 0.0204 s/iter. Total: 0.2716 s/iter. ETA=0:01:17
[01/17 17:22:19] d2.evaluation.evaluator INFO: Inference done 826/1093. Dataloading: 0.0174 s/iter. Inference: 0.2334 s/iter. Eval: 0.0204 s/iter. Total: 0.2714 s/iter. ETA=0:01:12
[01/17 17:22:24] d2.evaluation.evaluator INFO: Inference done 846/1093. Dataloading: 0.0173 s/iter. Inference: 0.2331 s/iter. Eval: 0.0203 s/iter. Total: 0.2709 s/iter. ETA=0:01:06
[01/17 17:22:29] d2.evaluation.evaluator INFO: Inference done 865/1093. Dataloading: 0.0172 s/iter. Inference: 0.2331 s/iter. Eval: 0.0203 s/iter. Total: 0.2708 s/iter. ETA=0:01:01
[01/17 17:22:34] d2.evaluation.evaluator INFO: Inference done 884/1093. Dataloading: 0.0173 s/iter. Inference: 0.2331 s/iter. Eval: 0.0203 s/iter. Total: 0.2709 s/iter. ETA=0:00:56
[01/17 17:22:39] d2.evaluation.evaluator INFO: Inference done 903/1093. Dataloading: 0.0173 s/iter. Inference: 0.2332 s/iter. Eval: 0.0203 s/iter. Total: 0.2710 s/iter. ETA=0:00:51
[01/17 17:22:45] d2.evaluation.evaluator INFO: Inference done 921/1093. Dataloading: 0.0173 s/iter. Inference: 0.2335 s/iter. Eval: 0.0203 s/iter. Total: 0.2713 s/iter. ETA=0:00:46
[01/17 17:22:50] d2.evaluation.evaluator INFO: Inference done 940/1093. Dataloading: 0.0173 s/iter. Inference: 0.2334 s/iter. Eval: 0.0203 s/iter. Total: 0.2712 s/iter. ETA=0:00:41
[01/17 17:22:55] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0173 s/iter. Inference: 0.2337 s/iter. Eval: 0.0203 s/iter. Total: 0.2715 s/iter. ETA=0:00:36
[01/17 17:23:00] d2.evaluation.evaluator INFO: Inference done 977/1093. Dataloading: 0.0172 s/iter. Inference: 0.2338 s/iter. Eval: 0.0203 s/iter. Total: 0.2716 s/iter. ETA=0:00:31
[01/17 17:23:05] d2.evaluation.evaluator INFO: Inference done 996/1093. Dataloading: 0.0173 s/iter. Inference: 0.2340 s/iter. Eval: 0.0203 s/iter. Total: 0.2717 s/iter. ETA=0:00:26
[01/17 17:23:10] d2.evaluation.evaluator INFO: Inference done 1015/1093. Dataloading: 0.0172 s/iter. Inference: 0.2340 s/iter. Eval: 0.0203 s/iter. Total: 0.2717 s/iter. ETA=0:00:21
[01/17 17:23:16] d2.evaluation.evaluator INFO: Inference done 1035/1093. Dataloading: 0.0172 s/iter. Inference: 0.2336 s/iter. Eval: 0.0204 s/iter. Total: 0.2715 s/iter. ETA=0:00:15
[01/17 17:23:21] d2.evaluation.evaluator INFO: Inference done 1055/1093. Dataloading: 0.0171 s/iter. Inference: 0.2335 s/iter. Eval: 0.0204 s/iter. Total: 0.2712 s/iter. ETA=0:00:10
[01/17 17:23:26] d2.evaluation.evaluator INFO: Inference done 1076/1093. Dataloading: 0.0170 s/iter. Inference: 0.2332 s/iter. Eval: 0.0204 s/iter. Total: 0.2708 s/iter. ETA=0:00:04
[01/17 17:23:30] d2.evaluation.evaluator INFO: Total inference time: 0:04:54.380321 (0.270570 s / iter per device, on 4 devices)
[01/17 17:23:30] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:04:13 (0.232724 s / iter per device, on 4 devices)
[01/17 17:23:33] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'mIoU': 0.05391016729867564, 'fwIoU': 1.0602495422662344, 'IoU-0': nan, 'IoU-1': 10.296841954047048, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 0.0, 'IoU-20': 0.0, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 0.0, 'IoU-28': 0.0, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.0, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 0.0, 'IoU-42': 0.0, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.0, 'IoU-50': 0.0, 'IoU-51': 0.0, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.0, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 0.5235602094240838, 'pACC': 10.296841954047048, 'ACC-0': nan, 'ACC-1': 100.0, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 0.0, 'ACC-20': 0.0, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 0.0, 'ACC-28': 0.0, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 0.0, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 0.0, 'ACC-42': 0.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 0.0, 'ACC-50': 0.0, 'ACC-51': 0.0, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 0.0, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 17:23:33] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 17:23:33] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 17:23:33] d2.evaluation.testing INFO: copypaste: mIoU,fwIoU,mACC,pACC
[01/17 17:23:33] d2.evaluation.testing INFO: copypaste: 0.0539,1.0602,0.5236,10.2968
[01/17 17:23:33] d2.utils.events INFO:  eta: 19:17:59  iter: 59  total_loss: 112.3  loss_ce: 6.89  loss_mask: 0.3033  loss_dice: 4.529  loss_ce_0: 8.871  loss_mask_0: 0.3184  loss_dice_0: 4.523  loss_ce_1: 7.301  loss_mask_1: 0.3176  loss_dice_1: 4.508  loss_ce_2: 6.59  loss_mask_2: 0.3195  loss_dice_2: 4.505  loss_ce_3: 6.015  loss_mask_3: 0.3285  loss_dice_3: 4.52  loss_ce_4: 5.575  loss_mask_4: 0.3232  loss_dice_4: 4.521  loss_ce_5: 5.31  loss_mask_5: 0.3337  loss_dice_5: 4.519  loss_ce_6: 5.421  loss_mask_6: 0.3262  loss_dice_6: 4.536  loss_ce_7: 5.593  loss_mask_7: 0.3279  loss_dice_7: 4.525  loss_ce_8: 5.998  loss_mask_8: 0.3153  loss_dice_8: 4.514  time: 1.8416  data_time: 0.1129  lr: 9.9867e-06  max_mem: 20895M
[01/17 17:24:07] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:24:07] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:24:07] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:24:08] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 17:24:21] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0167 s/iter. Inference: 0.2403 s/iter. Eval: 0.0178 s/iter. Total: 0.2748 s/iter. ETA=0:04:57
[01/17 17:24:26] d2.evaluation.evaluator INFO: Inference done 30/1093. Dataloading: 0.0190 s/iter. Inference: 0.2310 s/iter. Eval: 0.0219 s/iter. Total: 0.2719 s/iter. ETA=0:04:49
[01/17 17:24:31] d2.evaluation.evaluator INFO: Inference done 49/1093. Dataloading: 0.0193 s/iter. Inference: 0.2321 s/iter. Eval: 0.0213 s/iter. Total: 0.2728 s/iter. ETA=0:04:44
[01/17 17:24:36] d2.evaluation.evaluator INFO: Inference done 65/1093. Dataloading: 0.0193 s/iter. Inference: 0.2432 s/iter. Eval: 0.0207 s/iter. Total: 0.2834 s/iter. ETA=0:04:51
[01/17 17:24:41] d2.evaluation.evaluator INFO: Inference done 83/1093. Dataloading: 0.0184 s/iter. Inference: 0.2433 s/iter. Eval: 0.0207 s/iter. Total: 0.2826 s/iter. ETA=0:04:45
[01/17 17:24:46] d2.evaluation.evaluator INFO: Inference done 102/1093. Dataloading: 0.0179 s/iter. Inference: 0.2402 s/iter. Eval: 0.0223 s/iter. Total: 0.2806 s/iter. ETA=0:04:38
[01/17 17:24:51] d2.evaluation.evaluator INFO: Inference done 120/1093. Dataloading: 0.0179 s/iter. Inference: 0.2406 s/iter. Eval: 0.0224 s/iter. Total: 0.2810 s/iter. ETA=0:04:33
[01/17 17:24:56] d2.evaluation.evaluator INFO: Inference done 139/1093. Dataloading: 0.0174 s/iter. Inference: 0.2405 s/iter. Eval: 0.0218 s/iter. Total: 0.2798 s/iter. ETA=0:04:26
[01/17 17:25:02] d2.evaluation.evaluator INFO: Inference done 158/1093. Dataloading: 0.0176 s/iter. Inference: 0.2404 s/iter. Eval: 0.0214 s/iter. Total: 0.2796 s/iter. ETA=0:04:21
[01/17 17:25:07] d2.evaluation.evaluator INFO: Inference done 178/1093. Dataloading: 0.0173 s/iter. Inference: 0.2384 s/iter. Eval: 0.0209 s/iter. Total: 0.2769 s/iter. ETA=0:04:13
[01/17 17:25:12] d2.evaluation.evaluator INFO: Inference done 195/1093. Dataloading: 0.0175 s/iter. Inference: 0.2399 s/iter. Eval: 0.0209 s/iter. Total: 0.2784 s/iter. ETA=0:04:10
[01/17 17:25:17] d2.evaluation.evaluator INFO: Inference done 214/1093. Dataloading: 0.0175 s/iter. Inference: 0.2390 s/iter. Eval: 0.0207 s/iter. Total: 0.2774 s/iter. ETA=0:04:03
[01/17 17:25:22] d2.evaluation.evaluator INFO: Inference done 234/1093. Dataloading: 0.0175 s/iter. Inference: 0.2374 s/iter. Eval: 0.0209 s/iter. Total: 0.2760 s/iter. ETA=0:03:57
[01/17 17:25:27] d2.evaluation.evaluator INFO: Inference done 253/1093. Dataloading: 0.0175 s/iter. Inference: 0.2363 s/iter. Eval: 0.0211 s/iter. Total: 0.2751 s/iter. ETA=0:03:51
[01/17 17:25:32] d2.evaluation.evaluator INFO: Inference done 273/1093. Dataloading: 0.0174 s/iter. Inference: 0.2354 s/iter. Eval: 0.0208 s/iter. Total: 0.2737 s/iter. ETA=0:03:44
[01/17 17:25:38] d2.evaluation.evaluator INFO: Inference done 291/1093. Dataloading: 0.0172 s/iter. Inference: 0.2367 s/iter. Eval: 0.0208 s/iter. Total: 0.2748 s/iter. ETA=0:03:40
[01/17 17:25:43] d2.evaluation.evaluator INFO: Inference done 308/1093. Dataloading: 0.0180 s/iter. Inference: 0.2369 s/iter. Eval: 0.0208 s/iter. Total: 0.2760 s/iter. ETA=0:03:36
[01/17 17:25:44] d2.engine.hooks INFO: Overall training speed: 77 iterations in 0:02:20 (1.8184 s / it)
[01/17 17:25:44] d2.engine.hooks INFO: Total training time: 0:18:43 (0:16:23 on hooks)
[01/17 17:25:44] d2.utils.events INFO:  eta: 18:45:09  iter: 79  total_loss: 91.27  loss_ce: 3.034  loss_mask: 0.3607  loss_dice: 4.534  loss_ce_0: 8.801  loss_mask_0: 0.3255  loss_dice_0: 4.477  loss_ce_1: 6.452  loss_mask_1: 0.3283  loss_dice_1: 4.484  loss_ce_2: 5.043  loss_mask_2: 0.3415  loss_dice_2: 4.498  loss_ce_3: 4.109  loss_mask_3: 0.3486  loss_dice_3: 4.507  loss_ce_4: 3.453  loss_mask_4: 0.3487  loss_dice_4: 4.513  loss_ce_5: 3.143  loss_mask_5: 0.364  loss_dice_5: 4.523  loss_ce_6: 3.019  loss_mask_6: 0.3593  loss_dice_6: 4.518  loss_ce_7: 2.934  loss_mask_7: 0.3625  loss_dice_7: 4.526  loss_ce_8: 2.931  loss_mask_8: 0.371  loss_dice_8: 4.517  time: 1.7951  data_time: 0.1162  lr: 9.9822e-06  max_mem: 20895M
[01/17 17:26:22] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 17:26:26] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 17:26:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 17:26:26] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 17:26:26] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 17:26:26] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 17:26:26] d2.utils.env INFO: Using a generated random seed 26493929
[01/17 17:26:28] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 17:26:29] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:26:32] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 17:26:32] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 17:26:32] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 17:26:33] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 17:26:33] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 17:26:33] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 17:26:33] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 17:26:33] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 17:26:33] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 17:27:43] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:27:44] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:27:44] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:27:45] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 17:27:58] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0148 s/iter. Inference: 0.2437 s/iter. Eval: 0.0353 s/iter. Total: 0.2939 s/iter. ETA=0:05:17
[01/17 17:28:03] d2.evaluation.evaluator INFO: Inference done 29/1093. Dataloading: 0.0186 s/iter. Inference: 0.2312 s/iter. Eval: 0.0323 s/iter. Total: 0.2822 s/iter. ETA=0:05:00
[01/17 17:28:08] d2.evaluation.evaluator INFO: Inference done 47/1093. Dataloading: 0.0177 s/iter. Inference: 0.2345 s/iter. Eval: 0.0302 s/iter. Total: 0.2826 s/iter. ETA=0:04:55
[01/17 17:28:13] d2.evaluation.evaluator INFO: Inference done 65/1093. Dataloading: 0.0178 s/iter. Inference: 0.2346 s/iter. Eval: 0.0292 s/iter. Total: 0.2818 s/iter. ETA=0:04:49
[01/17 17:28:18] d2.evaluation.evaluator INFO: Inference done 84/1093. Dataloading: 0.0190 s/iter. Inference: 0.2302 s/iter. Eval: 0.0310 s/iter. Total: 0.2804 s/iter. ETA=0:04:42
[01/17 17:28:24] d2.evaluation.evaluator INFO: Inference done 104/1093. Dataloading: 0.0195 s/iter. Inference: 0.2272 s/iter. Eval: 0.0301 s/iter. Total: 0.2769 s/iter. ETA=0:04:33
[01/17 17:28:29] d2.evaluation.evaluator INFO: Inference done 123/1093. Dataloading: 0.0191 s/iter. Inference: 0.2269 s/iter. Eval: 0.0298 s/iter. Total: 0.2759 s/iter. ETA=0:04:27
[01/17 17:28:34] d2.evaluation.evaluator INFO: Inference done 143/1093. Dataloading: 0.0184 s/iter. Inference: 0.2252 s/iter. Eval: 0.0297 s/iter. Total: 0.2734 s/iter. ETA=0:04:19
[01/17 17:28:39] d2.evaluation.evaluator INFO: Inference done 163/1093. Dataloading: 0.0183 s/iter. Inference: 0.2238 s/iter. Eval: 0.0299 s/iter. Total: 0.2721 s/iter. ETA=0:04:13
[01/17 17:28:44] d2.evaluation.evaluator INFO: Inference done 184/1093. Dataloading: 0.0181 s/iter. Inference: 0.2216 s/iter. Eval: 0.0293 s/iter. Total: 0.2691 s/iter. ETA=0:04:04
[01/17 17:28:49] d2.evaluation.evaluator INFO: Inference done 205/1093. Dataloading: 0.0176 s/iter. Inference: 0.2190 s/iter. Eval: 0.0292 s/iter. Total: 0.2659 s/iter. ETA=0:03:56
[01/17 17:28:55] d2.evaluation.evaluator INFO: Inference done 227/1093. Dataloading: 0.0172 s/iter. Inference: 0.2172 s/iter. Eval: 0.0287 s/iter. Total: 0.2632 s/iter. ETA=0:03:47
[01/17 17:29:00] d2.evaluation.evaluator INFO: Inference done 249/1093. Dataloading: 0.0169 s/iter. Inference: 0.2146 s/iter. Eval: 0.0285 s/iter. Total: 0.2601 s/iter. ETA=0:03:39
[01/17 17:29:05] d2.evaluation.evaluator INFO: Inference done 269/1093. Dataloading: 0.0167 s/iter. Inference: 0.2144 s/iter. Eval: 0.0284 s/iter. Total: 0.2596 s/iter. ETA=0:03:33
[01/17 17:29:10] d2.evaluation.evaluator INFO: Inference done 291/1093. Dataloading: 0.0165 s/iter. Inference: 0.2127 s/iter. Eval: 0.0282 s/iter. Total: 0.2575 s/iter. ETA=0:03:26
[01/17 17:29:15] d2.evaluation.evaluator INFO: Inference done 313/1093. Dataloading: 0.0168 s/iter. Inference: 0.2104 s/iter. Eval: 0.0281 s/iter. Total: 0.2555 s/iter. ETA=0:03:19
[01/17 17:29:20] d2.evaluation.evaluator INFO: Inference done 332/1093. Dataloading: 0.0169 s/iter. Inference: 0.2107 s/iter. Eval: 0.0284 s/iter. Total: 0.2562 s/iter. ETA=0:03:14
[01/17 17:29:25] d2.evaluation.evaluator INFO: Inference done 353/1093. Dataloading: 0.0166 s/iter. Inference: 0.2102 s/iter. Eval: 0.0284 s/iter. Total: 0.2553 s/iter. ETA=0:03:08
[01/17 17:29:30] d2.evaluation.evaluator INFO: Inference done 373/1093. Dataloading: 0.0166 s/iter. Inference: 0.2101 s/iter. Eval: 0.0282 s/iter. Total: 0.2551 s/iter. ETA=0:03:03
[01/17 17:29:35] d2.evaluation.evaluator INFO: Inference done 395/1093. Dataloading: 0.0165 s/iter. Inference: 0.2091 s/iter. Eval: 0.0282 s/iter. Total: 0.2539 s/iter. ETA=0:02:57
[01/17 17:29:40] d2.evaluation.evaluator INFO: Inference done 417/1093. Dataloading: 0.0164 s/iter. Inference: 0.2081 s/iter. Eval: 0.0282 s/iter. Total: 0.2529 s/iter. ETA=0:02:50
[01/17 17:29:46] d2.evaluation.evaluator INFO: Inference done 439/1093. Dataloading: 0.0164 s/iter. Inference: 0.2073 s/iter. Eval: 0.0281 s/iter. Total: 0.2518 s/iter. ETA=0:02:44
[01/17 17:29:51] d2.evaluation.evaluator INFO: Inference done 460/1093. Dataloading: 0.0163 s/iter. Inference: 0.2066 s/iter. Eval: 0.0282 s/iter. Total: 0.2512 s/iter. ETA=0:02:39
[01/17 17:29:56] d2.evaluation.evaluator INFO: Inference done 483/1093. Dataloading: 0.0161 s/iter. Inference: 0.2056 s/iter. Eval: 0.0281 s/iter. Total: 0.2500 s/iter. ETA=0:02:32
[01/17 17:30:01] d2.evaluation.evaluator INFO: Inference done 506/1093. Dataloading: 0.0159 s/iter. Inference: 0.2050 s/iter. Eval: 0.0279 s/iter. Total: 0.2489 s/iter. ETA=0:02:26
[01/17 17:30:06] d2.evaluation.evaluator INFO: Inference done 528/1093. Dataloading: 0.0158 s/iter. Inference: 0.2047 s/iter. Eval: 0.0278 s/iter. Total: 0.2484 s/iter. ETA=0:02:20
[01/17 17:30:11] d2.evaluation.evaluator INFO: Inference done 549/1093. Dataloading: 0.0158 s/iter. Inference: 0.2045 s/iter. Eval: 0.0277 s/iter. Total: 0.2481 s/iter. ETA=0:02:14
[01/17 17:30:16] d2.evaluation.evaluator INFO: Inference done 571/1093. Dataloading: 0.0157 s/iter. Inference: 0.2040 s/iter. Eval: 0.0276 s/iter. Total: 0.2474 s/iter. ETA=0:02:09
[01/17 17:30:21] d2.evaluation.evaluator INFO: Inference done 591/1093. Dataloading: 0.0156 s/iter. Inference: 0.2042 s/iter. Eval: 0.0277 s/iter. Total: 0.2476 s/iter. ETA=0:02:04
[01/17 17:30:26] d2.evaluation.evaluator INFO: Inference done 613/1093. Dataloading: 0.0158 s/iter. Inference: 0.2035 s/iter. Eval: 0.0277 s/iter. Total: 0.2471 s/iter. ETA=0:01:58
[01/17 17:30:32] d2.evaluation.evaluator INFO: Inference done 633/1093. Dataloading: 0.0157 s/iter. Inference: 0.2037 s/iter. Eval: 0.0277 s/iter. Total: 0.2473 s/iter. ETA=0:01:53
[01/17 17:30:37] d2.evaluation.evaluator INFO: Inference done 656/1093. Dataloading: 0.0156 s/iter. Inference: 0.2032 s/iter. Eval: 0.0276 s/iter. Total: 0.2464 s/iter. ETA=0:01:47
[01/17 17:30:42] d2.evaluation.evaluator INFO: Inference done 677/1093. Dataloading: 0.0156 s/iter. Inference: 0.2031 s/iter. Eval: 0.0275 s/iter. Total: 0.2464 s/iter. ETA=0:01:42
[01/17 17:30:47] d2.evaluation.evaluator INFO: Inference done 699/1093. Dataloading: 0.0156 s/iter. Inference: 0.2027 s/iter. Eval: 0.0275 s/iter. Total: 0.2459 s/iter. ETA=0:01:36
[01/17 17:30:52] d2.evaluation.evaluator INFO: Inference done 720/1093. Dataloading: 0.0155 s/iter. Inference: 0.2026 s/iter. Eval: 0.0275 s/iter. Total: 0.2457 s/iter. ETA=0:01:31
[01/17 17:30:57] d2.evaluation.evaluator INFO: Inference done 744/1093. Dataloading: 0.0154 s/iter. Inference: 0.2016 s/iter. Eval: 0.0276 s/iter. Total: 0.2447 s/iter. ETA=0:01:25
[01/17 17:31:02] d2.evaluation.evaluator INFO: Inference done 767/1093. Dataloading: 0.0154 s/iter. Inference: 0.2010 s/iter. Eval: 0.0277 s/iter. Total: 0.2442 s/iter. ETA=0:01:19
[01/17 17:31:07] d2.evaluation.evaluator INFO: Inference done 788/1093. Dataloading: 0.0153 s/iter. Inference: 0.2012 s/iter. Eval: 0.0276 s/iter. Total: 0.2442 s/iter. ETA=0:01:14
[01/17 17:31:13] d2.evaluation.evaluator INFO: Inference done 809/1093. Dataloading: 0.0152 s/iter. Inference: 0.2014 s/iter. Eval: 0.0275 s/iter. Total: 0.2442 s/iter. ETA=0:01:09
[01/17 17:31:18] d2.evaluation.evaluator INFO: Inference done 830/1093. Dataloading: 0.0151 s/iter. Inference: 0.2016 s/iter. Eval: 0.0274 s/iter. Total: 0.2442 s/iter. ETA=0:01:04
[01/17 17:31:23] d2.evaluation.evaluator INFO: Inference done 852/1093. Dataloading: 0.0150 s/iter. Inference: 0.2016 s/iter. Eval: 0.0273 s/iter. Total: 0.2440 s/iter. ETA=0:00:58
[01/17 17:31:28] d2.evaluation.evaluator INFO: Inference done 873/1093. Dataloading: 0.0151 s/iter. Inference: 0.2014 s/iter. Eval: 0.0273 s/iter. Total: 0.2439 s/iter. ETA=0:00:53
[01/17 17:31:33] d2.evaluation.evaluator INFO: Inference done 893/1093. Dataloading: 0.0151 s/iter. Inference: 0.2015 s/iter. Eval: 0.0274 s/iter. Total: 0.2441 s/iter. ETA=0:00:48
[01/17 17:31:38] d2.evaluation.evaluator INFO: Inference done 916/1093. Dataloading: 0.0151 s/iter. Inference: 0.2012 s/iter. Eval: 0.0273 s/iter. Total: 0.2437 s/iter. ETA=0:00:43
[01/17 17:31:43] d2.evaluation.evaluator INFO: Inference done 937/1093. Dataloading: 0.0151 s/iter. Inference: 0.2009 s/iter. Eval: 0.0274 s/iter. Total: 0.2436 s/iter. ETA=0:00:37
[01/17 17:31:48] d2.evaluation.evaluator INFO: Inference done 959/1093. Dataloading: 0.0151 s/iter. Inference: 0.2009 s/iter. Eval: 0.0273 s/iter. Total: 0.2434 s/iter. ETA=0:00:32
[01/17 17:31:54] d2.evaluation.evaluator INFO: Inference done 980/1093. Dataloading: 0.0150 s/iter. Inference: 0.2010 s/iter. Eval: 0.0273 s/iter. Total: 0.2435 s/iter. ETA=0:00:27
[01/17 17:31:59] d2.evaluation.evaluator INFO: Inference done 999/1093. Dataloading: 0.0149 s/iter. Inference: 0.2016 s/iter. Eval: 0.0273 s/iter. Total: 0.2439 s/iter. ETA=0:00:22
[01/17 17:32:04] d2.evaluation.evaluator INFO: Inference done 1021/1093. Dataloading: 0.0149 s/iter. Inference: 0.2014 s/iter. Eval: 0.0272 s/iter. Total: 0.2436 s/iter. ETA=0:00:17
[01/17 17:32:09] d2.evaluation.evaluator INFO: Inference done 1043/1093. Dataloading: 0.0148 s/iter. Inference: 0.2013 s/iter. Eval: 0.0272 s/iter. Total: 0.2434 s/iter. ETA=0:00:12
[01/17 17:32:14] d2.evaluation.evaluator INFO: Inference done 1065/1093. Dataloading: 0.0148 s/iter. Inference: 0.2010 s/iter. Eval: 0.0271 s/iter. Total: 0.2431 s/iter. ETA=0:00:06
[01/17 17:32:19] d2.evaluation.evaluator INFO: Inference done 1089/1093. Dataloading: 0.0147 s/iter. Inference: 0.2005 s/iter. Eval: 0.0271 s/iter. Total: 0.2424 s/iter. ETA=0:00:00
[01/17 17:32:20] d2.evaluation.evaluator INFO: Total inference time: 0:04:23.877665 (0.242535 s / iter per device, on 4 devices)
[01/17 17:32:20] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:37 (0.200339 s / iter per device, on 4 devices)
[01/17 17:32:23] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': nan, 'mIoU': 0.008292619682136834, 'fwIoU': 0.025087086702458985, 'IoU-0': nan, 'IoU-1': 0.0, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 0.0, 'IoU-20': 0.0, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 1.5838903592881353, 'IoU-28': 0.0, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.0, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 0.0, 'IoU-42': 0.0, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.0, 'IoU-50': 0.0, 'IoU-51': 0.0, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.0, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 0.5235602094240838, 'pACC': 1.5838903592881353, 'ACC-0': nan, 'ACC-1': 0.0, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 0.0, 'ACC-20': 0.0, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 100.0, 'ACC-28': 0.0, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 0.0, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 0.0, 'ACC-42': 0.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 0.0, 'ACC-50': 0.0, 'ACC-51': 0.0, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 0.0, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 17:32:23] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 17:32:23] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 17:32:23] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 17:32:23] d2.evaluation.testing INFO: copypaste: nan,0.0083,0.0251,0.5236,1.5839
[01/17 17:32:23] d2.utils.events INFO:  eta: 1 day, 0:46:35  iter: 19  total_loss: 148.6  loss_ce: 9.96  loss_mask: 0.5032  loss_dice: 4.916  loss_ce_0: 9.191  loss_mask_0: 0.3495  loss_dice_0: 4.883  loss_ce_1: 9.174  loss_mask_1: 0.3879  loss_dice_1: 4.886  loss_ce_2: 9.416  loss_mask_2: 0.4412  loss_dice_2: 4.901  loss_ce_3: 9.718  loss_mask_3: 0.4018  loss_dice_3: 4.902  loss_ce_4: 9.709  loss_mask_4: 0.5202  loss_dice_4: 4.891  loss_ce_5: 9.774  loss_mask_5: 0.4704  loss_dice_5: 4.911  loss_ce_6: 9.723  loss_mask_6: 0.4933  loss_dice_6: 4.915  loss_ce_7: 9.869  loss_mask_7: 0.5512  loss_dice_7: 4.909  loss_ce_8: 9.868  loss_mask_8: 0.506  loss_dice_8: 4.915  time: 2.6683  data_time: 0.6104  lr: 9.9957e-06  max_mem: 20872M
[01/17 17:32:57] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:32:58] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:32:58] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:32:59] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 17:33:12] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0089 s/iter. Inference: 0.1967 s/iter. Eval: 0.0281 s/iter. Total: 0.2338 s/iter. ETA=0:04:12
[01/17 17:33:17] d2.evaluation.evaluator INFO: Inference done 32/1093. Dataloading: 0.0114 s/iter. Inference: 0.2083 s/iter. Eval: 0.0286 s/iter. Total: 0.2484 s/iter. ETA=0:04:23
[01/17 17:33:23] d2.evaluation.evaluator INFO: Inference done 52/1093. Dataloading: 0.0159 s/iter. Inference: 0.2061 s/iter. Eval: 0.0296 s/iter. Total: 0.2517 s/iter. ETA=0:04:22
[01/17 17:33:28] d2.evaluation.evaluator INFO: Inference done 73/1093. Dataloading: 0.0154 s/iter. Inference: 0.2048 s/iter. Eval: 0.0294 s/iter. Total: 0.2497 s/iter. ETA=0:04:14
[01/17 17:33:33] d2.evaluation.evaluator INFO: Inference done 93/1093. Dataloading: 0.0155 s/iter. Inference: 0.2080 s/iter. Eval: 0.0289 s/iter. Total: 0.2525 s/iter. ETA=0:04:12
[01/17 17:33:38] d2.evaluation.evaluator INFO: Inference done 114/1093. Dataloading: 0.0151 s/iter. Inference: 0.2069 s/iter. Eval: 0.0297 s/iter. Total: 0.2518 s/iter. ETA=0:04:06
[01/17 17:33:43] d2.evaluation.evaluator INFO: Inference done 135/1093. Dataloading: 0.0150 s/iter. Inference: 0.2056 s/iter. Eval: 0.0293 s/iter. Total: 0.2500 s/iter. ETA=0:03:59
[01/17 17:33:48] d2.evaluation.evaluator INFO: Inference done 155/1093. Dataloading: 0.0148 s/iter. Inference: 0.2071 s/iter. Eval: 0.0291 s/iter. Total: 0.2511 s/iter. ETA=0:03:55
[01/17 17:33:54] d2.evaluation.evaluator INFO: Inference done 176/1093. Dataloading: 0.0146 s/iter. Inference: 0.2064 s/iter. Eval: 0.0289 s/iter. Total: 0.2501 s/iter. ETA=0:03:49
[01/17 17:33:59] d2.evaluation.evaluator INFO: Inference done 196/1093. Dataloading: 0.0146 s/iter. Inference: 0.2079 s/iter. Eval: 0.0287 s/iter. Total: 0.2513 s/iter. ETA=0:03:45
[01/17 17:34:04] d2.evaluation.evaluator INFO: Inference done 217/1093. Dataloading: 0.0146 s/iter. Inference: 0.2072 s/iter. Eval: 0.0288 s/iter. Total: 0.2507 s/iter. ETA=0:03:39
[01/17 17:34:09] d2.evaluation.evaluator INFO: Inference done 238/1093. Dataloading: 0.0149 s/iter. Inference: 0.2061 s/iter. Eval: 0.0289 s/iter. Total: 0.2499 s/iter. ETA=0:03:33
[01/17 17:34:14] d2.evaluation.evaluator INFO: Inference done 259/1093. Dataloading: 0.0146 s/iter. Inference: 0.2060 s/iter. Eval: 0.0285 s/iter. Total: 0.2492 s/iter. ETA=0:03:27
[01/17 17:34:19] d2.evaluation.evaluator INFO: Inference done 282/1093. Dataloading: 0.0147 s/iter. Inference: 0.2035 s/iter. Eval: 0.0286 s/iter. Total: 0.2469 s/iter. ETA=0:03:20
[01/17 17:34:24] d2.evaluation.evaluator INFO: Inference done 301/1093. Dataloading: 0.0149 s/iter. Inference: 0.2044 s/iter. Eval: 0.0287 s/iter. Total: 0.2481 s/iter. ETA=0:03:16
[01/17 17:34:29] d2.evaluation.evaluator INFO: Inference done 323/1093. Dataloading: 0.0149 s/iter. Inference: 0.2036 s/iter. Eval: 0.0285 s/iter. Total: 0.2471 s/iter. ETA=0:03:10
[01/17 17:34:34] d2.evaluation.evaluator INFO: Inference done 345/1093. Dataloading: 0.0147 s/iter. Inference: 0.2029 s/iter. Eval: 0.0285 s/iter. Total: 0.2462 s/iter. ETA=0:03:04
[01/17 17:34:39] d2.evaluation.evaluator INFO: Inference done 363/1093. Dataloading: 0.0145 s/iter. Inference: 0.2050 s/iter. Eval: 0.0284 s/iter. Total: 0.2479 s/iter. ETA=0:03:00
[01/17 17:34:45] d2.evaluation.evaluator INFO: Inference done 382/1093. Dataloading: 0.0145 s/iter. Inference: 0.2060 s/iter. Eval: 0.0282 s/iter. Total: 0.2488 s/iter. ETA=0:02:56
[01/17 17:34:50] d2.evaluation.evaluator INFO: Inference done 402/1093. Dataloading: 0.0146 s/iter. Inference: 0.2059 s/iter. Eval: 0.0283 s/iter. Total: 0.2489 s/iter. ETA=0:02:51
[01/17 17:34:55] d2.evaluation.evaluator INFO: Inference done 424/1093. Dataloading: 0.0145 s/iter. Inference: 0.2052 s/iter. Eval: 0.0282 s/iter. Total: 0.2480 s/iter. ETA=0:02:45
[01/17 17:35:00] d2.evaluation.evaluator INFO: Inference done 445/1093. Dataloading: 0.0145 s/iter. Inference: 0.2050 s/iter. Eval: 0.0283 s/iter. Total: 0.2479 s/iter. ETA=0:02:40
[01/17 17:35:05] d2.evaluation.evaluator INFO: Inference done 467/1093. Dataloading: 0.0144 s/iter. Inference: 0.2043 s/iter. Eval: 0.0286 s/iter. Total: 0.2473 s/iter. ETA=0:02:34
[01/17 17:35:10] d2.evaluation.evaluator INFO: Inference done 485/1093. Dataloading: 0.0143 s/iter. Inference: 0.2057 s/iter. Eval: 0.0285 s/iter. Total: 0.2486 s/iter. ETA=0:02:31
[01/17 17:35:15] d2.evaluation.evaluator INFO: Inference done 506/1093. Dataloading: 0.0142 s/iter. Inference: 0.2054 s/iter. Eval: 0.0285 s/iter. Total: 0.2482 s/iter. ETA=0:02:25
[01/17 17:35:20] d2.evaluation.evaluator INFO: Inference done 528/1093. Dataloading: 0.0142 s/iter. Inference: 0.2050 s/iter. Eval: 0.0284 s/iter. Total: 0.2477 s/iter. ETA=0:02:19
[01/17 17:35:25] d2.evaluation.evaluator INFO: Inference done 547/1093. Dataloading: 0.0142 s/iter. Inference: 0.2057 s/iter. Eval: 0.0285 s/iter. Total: 0.2486 s/iter. ETA=0:02:15
[01/17 17:35:31] d2.evaluation.evaluator INFO: Inference done 569/1093. Dataloading: 0.0143 s/iter. Inference: 0.2049 s/iter. Eval: 0.0285 s/iter. Total: 0.2478 s/iter. ETA=0:02:09
[01/17 17:35:36] d2.evaluation.evaluator INFO: Inference done 591/1093. Dataloading: 0.0142 s/iter. Inference: 0.2044 s/iter. Eval: 0.0285 s/iter. Total: 0.2471 s/iter. ETA=0:02:04
[01/17 17:35:41] d2.evaluation.evaluator INFO: Inference done 613/1093. Dataloading: 0.0143 s/iter. Inference: 0.2037 s/iter. Eval: 0.0284 s/iter. Total: 0.2465 s/iter. ETA=0:01:58
[01/17 17:35:46] d2.evaluation.evaluator INFO: Inference done 635/1093. Dataloading: 0.0143 s/iter. Inference: 0.2031 s/iter. Eval: 0.0285 s/iter. Total: 0.2460 s/iter. ETA=0:01:52
[01/17 17:35:51] d2.evaluation.evaluator INFO: Inference done 657/1093. Dataloading: 0.0142 s/iter. Inference: 0.2029 s/iter. Eval: 0.0284 s/iter. Total: 0.2457 s/iter. ETA=0:01:47
[01/17 17:35:56] d2.evaluation.evaluator INFO: Inference done 678/1093. Dataloading: 0.0141 s/iter. Inference: 0.2030 s/iter. Eval: 0.0284 s/iter. Total: 0.2456 s/iter. ETA=0:01:41
[01/17 17:36:01] d2.evaluation.evaluator INFO: Inference done 698/1093. Dataloading: 0.0141 s/iter. Inference: 0.2034 s/iter. Eval: 0.0283 s/iter. Total: 0.2459 s/iter. ETA=0:01:37
[01/17 17:36:06] d2.evaluation.evaluator INFO: Inference done 721/1093. Dataloading: 0.0141 s/iter. Inference: 0.2026 s/iter. Eval: 0.0284 s/iter. Total: 0.2453 s/iter. ETA=0:01:31
[01/17 17:36:11] d2.evaluation.evaluator INFO: Inference done 741/1093. Dataloading: 0.0141 s/iter. Inference: 0.2030 s/iter. Eval: 0.0284 s/iter. Total: 0.2456 s/iter. ETA=0:01:26
[01/17 17:36:16] d2.evaluation.evaluator INFO: Inference done 762/1093. Dataloading: 0.0140 s/iter. Inference: 0.2029 s/iter. Eval: 0.0284 s/iter. Total: 0.2454 s/iter. ETA=0:01:21
[01/17 17:36:22] d2.evaluation.evaluator INFO: Inference done 784/1093. Dataloading: 0.0139 s/iter. Inference: 0.2027 s/iter. Eval: 0.0284 s/iter. Total: 0.2451 s/iter. ETA=0:01:15
[01/17 17:36:27] d2.evaluation.evaluator INFO: Inference done 805/1093. Dataloading: 0.0139 s/iter. Inference: 0.2026 s/iter. Eval: 0.0285 s/iter. Total: 0.2451 s/iter. ETA=0:01:10
[01/17 17:36:32] d2.evaluation.evaluator INFO: Inference done 826/1093. Dataloading: 0.0138 s/iter. Inference: 0.2028 s/iter. Eval: 0.0284 s/iter. Total: 0.2451 s/iter. ETA=0:01:05
[01/17 17:36:37] d2.evaluation.evaluator INFO: Inference done 848/1093. Dataloading: 0.0137 s/iter. Inference: 0.2027 s/iter. Eval: 0.0284 s/iter. Total: 0.2449 s/iter. ETA=0:00:59
[01/17 17:36:42] d2.evaluation.evaluator INFO: Inference done 869/1093. Dataloading: 0.0137 s/iter. Inference: 0.2026 s/iter. Eval: 0.0283 s/iter. Total: 0.2447 s/iter. ETA=0:00:54
[01/17 17:36:47] d2.evaluation.evaluator INFO: Inference done 892/1093. Dataloading: 0.0137 s/iter. Inference: 0.2021 s/iter. Eval: 0.0283 s/iter. Total: 0.2442 s/iter. ETA=0:00:49
[01/17 17:36:52] d2.evaluation.evaluator INFO: Inference done 915/1093. Dataloading: 0.0136 s/iter. Inference: 0.2016 s/iter. Eval: 0.0283 s/iter. Total: 0.2436 s/iter. ETA=0:00:43
[01/17 17:36:58] d2.evaluation.evaluator INFO: Inference done 935/1093. Dataloading: 0.0136 s/iter. Inference: 0.2020 s/iter. Eval: 0.0282 s/iter. Total: 0.2440 s/iter. ETA=0:00:38
[01/17 17:37:03] d2.evaluation.evaluator INFO: Inference done 957/1093. Dataloading: 0.0136 s/iter. Inference: 0.2019 s/iter. Eval: 0.0282 s/iter. Total: 0.2438 s/iter. ETA=0:00:33
[01/17 17:37:08] d2.evaluation.evaluator INFO: Inference done 977/1093. Dataloading: 0.0136 s/iter. Inference: 0.2022 s/iter. Eval: 0.0282 s/iter. Total: 0.2441 s/iter. ETA=0:00:28
[01/17 17:37:13] d2.evaluation.evaluator INFO: Inference done 1000/1093. Dataloading: 0.0135 s/iter. Inference: 0.2018 s/iter. Eval: 0.0281 s/iter. Total: 0.2435 s/iter. ETA=0:00:22
[01/17 17:37:18] d2.evaluation.evaluator INFO: Inference done 1023/1093. Dataloading: 0.0134 s/iter. Inference: 0.2015 s/iter. Eval: 0.0281 s/iter. Total: 0.2431 s/iter. ETA=0:00:17
[01/17 17:37:23] d2.evaluation.evaluator INFO: Inference done 1044/1093. Dataloading: 0.0134 s/iter. Inference: 0.2017 s/iter. Eval: 0.0281 s/iter. Total: 0.2433 s/iter. ETA=0:00:11
[01/17 17:37:29] d2.evaluation.evaluator INFO: Inference done 1066/1093. Dataloading: 0.0133 s/iter. Inference: 0.2016 s/iter. Eval: 0.0280 s/iter. Total: 0.2431 s/iter. ETA=0:00:06
[01/17 17:37:34] d2.evaluation.evaluator INFO: Inference done 1092/1093. Dataloading: 0.0132 s/iter. Inference: 0.2007 s/iter. Eval: 0.0279 s/iter. Total: 0.2420 s/iter. ETA=0:00:00
[01/17 17:37:34] d2.evaluation.evaluator INFO: Total inference time: 0:04:23.607762 (0.242287 s / iter per device, on 4 devices)
[01/17 17:37:34] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:38 (0.200636 s / iter per device, on 4 devices)
[01/17 17:37:39] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': nan, 'mIoU': 0.012536674780845908, 'fwIoU': 0.03651677790215452, 'IoU-0': nan, 'IoU-1': 0.0, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 1.4918440366320045, 'IoU-20': 0.03572066333337771, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 0.0, 'IoU-28': 0.0, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.866940183176186, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 0.0, 'IoU-42': 0.0, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.0, 'IoU-50': 0.0, 'IoU-51': 0.0, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.0, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 0.5229455217641845, 'pACC': 1.4794315238952032, 'ACC-0': nan, 'ACC-1': 0.0, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 96.71482227494099, 'ACC-20': 0.03668522708013921, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 0.0, 'ACC-28': 0.0, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 3.131087154938135, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 0.0, 'ACC-42': 0.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 0.0, 'ACC-50': 0.0, 'ACC-51': 0.0, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 0.0, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 17:37:39] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 17:37:39] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 17:37:39] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 17:37:39] d2.evaluation.testing INFO: copypaste: nan,0.0125,0.0365,0.5229,1.4794
[01/17 17:37:39] d2.utils.events INFO:  eta: 21:01:54  iter: 39  total_loss: 129.6  loss_ce: 9.083  loss_mask: 0.4643  loss_dice: 4.45  loss_ce_0: 8.941  loss_mask_0: 0.304  loss_dice_0: 4.424  loss_ce_1: 7.827  loss_mask_1: 0.2872  loss_dice_1: 4.398  loss_ce_2: 7.625  loss_mask_2: 0.2862  loss_dice_2: 4.398  loss_ce_3: 7.542  loss_mask_3: 0.3044  loss_dice_3: 4.399  loss_ce_4: 7.62  loss_mask_4: 0.2971  loss_dice_4: 4.416  loss_ce_5: 7.749  loss_mask_5: 0.2906  loss_dice_5: 4.406  loss_ce_6: 8.11  loss_mask_6: 0.3085  loss_dice_6: 4.406  loss_ce_7: 8.566  loss_mask_7: 0.3086  loss_dice_7: 4.403  loss_ce_8: 8.919  loss_mask_8: 0.4098  loss_dice_8: 4.437  time: 2.1626  data_time: 0.0948  lr: 9.9912e-06  max_mem: 20872M
[01/17 17:37:51] d2.engine.hooks INFO: Overall training speed: 45 iterations in 0:01:34 (2.1020 s / it)
[01/17 17:37:51] d2.engine.hooks INFO: Total training time: 0:10:56 (0:09:21 on hooks)
[01/17 17:37:51] d2.utils.events INFO:  eta: 19:20:35  iter: 47  total_loss: 121.6  loss_ce: 8.669  loss_mask: 0.3097  loss_dice: 4.442  loss_ce_0: 8.885  loss_mask_0: 0.2999  loss_dice_0: 4.428  loss_ce_1: 7.603  loss_mask_1: 0.2893  loss_dice_1: 4.402  loss_ce_2: 7.126  loss_mask_2: 0.2906  loss_dice_2: 4.398  loss_ce_3: 6.861  loss_mask_3: 0.3137  loss_dice_3: 4.411  loss_ce_4: 6.749  loss_mask_4: 0.3047  loss_dice_4: 4.414  loss_ce_5: 6.791  loss_mask_5: 0.3043  loss_dice_5: 4.4  loss_ce_6: 6.96  loss_mask_6: 0.2965  loss_dice_6: 4.39  loss_ce_7: 7.283  loss_mask_7: 0.2976  loss_dice_7: 4.404  loss_ce_8: 7.97  loss_mask_8: 0.2942  loss_dice_8: 4.432  time: 2.0840  data_time: 0.1013  lr: 9.9896e-06  max_mem: 20872M
[01/17 17:39:50] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 17:39:56] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3,4,5,6,7     Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 17:39:56] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 17:39:56] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBase-SceneFlow-SemanticSegmentationStereo.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerStereo[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerHead[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mGN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;242m# pixel decoder[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMSDeformAttnPixelDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMultiScaleMaskedTransformerDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmulti_scale_pixel_decoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m[38;5;15m  [39m[38;5;242m# 9 decoder layers, add one for the loss on learnable query[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m

[01/17 17:39:56] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/torchvision/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 17:39:56] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 17:39:57] d2.utils.env INFO: Using a generated random seed 57351232
[01/17 17:39:59] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 17:39:59] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:40:09] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 17:40:09] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 17:40:10] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 17:40:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[01/17 17:40:10] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[01/17 17:40:11] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
[01/17 17:40:11] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 17:40:11] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[01/17 17:40:11] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 17:41:27] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:41:29] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:41:29] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:41:31] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 17:54:52] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 17:54:57] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3,4,5,6,7     Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 17:54:57] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 17:54:57] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R50_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBase-SceneFlow-SemanticSegmentationStereo.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerStereo[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerHead[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mGN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;242m# pixel decoder[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMSDeformAttnPixelDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMultiScaleMaskedTransformerDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmulti_scale_pixel_decoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m[38;5;15m  [39m[38;5;242m# 9 decoder layers, add one for the loss on learnable query[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m

[01/17 17:54:57] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mdetectron2://ImageNetPretrained/torchvision/R-50.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 17:54:57] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 17:54:58] d2.utils.env INFO: Using a generated random seed 58481263
[01/17 17:55:00] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 17:55:00] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:55:11] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 17:55:11] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 17:55:11] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 17:55:12] fvcore.common.checkpoint INFO: [Checkpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[01/17 17:55:12] fvcore.common.checkpoint INFO: Reading a file from 'torchvision'
[01/17 17:55:12] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint                                                               | Shapes                                          |
|:------------------|:----------------------------------------------------------------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.*      | stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |
[01/17 17:55:13] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 17:55:13] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mstem.fc.{bias, weight}[0m
[01/17 17:55:13] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 17:56:28] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 17:56:31] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 17:56:31] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 17:56:32] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 18:11:08] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 18:11:11] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 18:11:11] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 18:11:11] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 18:11:11] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 18:11:11] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 18:11:11] d2.utils.env INFO: Using a generated random seed 11596970
[01/17 18:11:13] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 18:11:13] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 18:11:16] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 18:11:18] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 18:11:18] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 18:11:18] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 18:11:18] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 18:11:18] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 18:11:18] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 18:11:18] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 18:11:18] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 18:12:27] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 18:12:28] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 18:12:28] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 18:12:28] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 18:12:40] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0096 s/iter. Inference: 0.1849 s/iter. Eval: 0.0274 s/iter. Total: 0.2220 s/iter. ETA=0:04:00
[01/17 18:12:46] d2.evaluation.evaluator INFO: Inference done 33/1093. Dataloading: 0.0099 s/iter. Inference: 0.1998 s/iter. Eval: 0.0252 s/iter. Total: 0.2350 s/iter. ETA=0:04:09
[01/17 18:12:51] d2.evaluation.evaluator INFO: Inference done 55/1093. Dataloading: 0.0124 s/iter. Inference: 0.1970 s/iter. Eval: 0.0248 s/iter. Total: 0.2344 s/iter. ETA=0:04:03
[01/17 18:12:56] d2.evaluation.evaluator INFO: Inference done 78/1093. Dataloading: 0.0129 s/iter. Inference: 0.1914 s/iter. Eval: 0.0247 s/iter. Total: 0.2292 s/iter. ETA=0:03:52
[01/17 18:13:01] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0128 s/iter. Inference: 0.1917 s/iter. Eval: 0.0245 s/iter. Total: 0.2291 s/iter. ETA=0:03:47
[01/17 18:13:06] d2.evaluation.evaluator INFO: Inference done 124/1093. Dataloading: 0.0123 s/iter. Inference: 0.1899 s/iter. Eval: 0.0241 s/iter. Total: 0.2264 s/iter. ETA=0:03:39
[01/17 18:13:11] d2.evaluation.evaluator INFO: Inference done 146/1093. Dataloading: 0.0125 s/iter. Inference: 0.1907 s/iter. Eval: 0.0244 s/iter. Total: 0.2277 s/iter. ETA=0:03:35
[01/17 18:13:16] d2.evaluation.evaluator INFO: Inference done 168/1093. Dataloading: 0.0125 s/iter. Inference: 0.1908 s/iter. Eval: 0.0242 s/iter. Total: 0.2277 s/iter. ETA=0:03:30
[01/17 18:13:21] d2.evaluation.evaluator INFO: Inference done 191/1093. Dataloading: 0.0123 s/iter. Inference: 0.1908 s/iter. Eval: 0.0244 s/iter. Total: 0.2275 s/iter. ETA=0:03:25
[01/17 18:13:26] d2.evaluation.evaluator INFO: Inference done 212/1093. Dataloading: 0.0124 s/iter. Inference: 0.1917 s/iter. Eval: 0.0248 s/iter. Total: 0.2290 s/iter. ETA=0:03:21
[01/17 18:13:32] d2.evaluation.evaluator INFO: Inference done 236/1093. Dataloading: 0.0122 s/iter. Inference: 0.1907 s/iter. Eval: 0.0245 s/iter. Total: 0.2275 s/iter. ETA=0:03:14
[01/17 18:13:37] d2.evaluation.evaluator INFO: Inference done 259/1093. Dataloading: 0.0120 s/iter. Inference: 0.1903 s/iter. Eval: 0.0244 s/iter. Total: 0.2268 s/iter. ETA=0:03:09
[01/17 18:13:42] d2.evaluation.evaluator INFO: Inference done 281/1093. Dataloading: 0.0120 s/iter. Inference: 0.1904 s/iter. Eval: 0.0244 s/iter. Total: 0.2269 s/iter. ETA=0:03:04
[01/17 18:13:47] d2.evaluation.evaluator INFO: Inference done 303/1093. Dataloading: 0.0119 s/iter. Inference: 0.1909 s/iter. Eval: 0.0243 s/iter. Total: 0.2273 s/iter. ETA=0:02:59
[01/17 18:13:52] d2.evaluation.evaluator INFO: Inference done 326/1093. Dataloading: 0.0119 s/iter. Inference: 0.1910 s/iter. Eval: 0.0243 s/iter. Total: 0.2273 s/iter. ETA=0:02:54
[01/17 18:13:57] d2.evaluation.evaluator INFO: Inference done 350/1093. Dataloading: 0.0117 s/iter. Inference: 0.1904 s/iter. Eval: 0.0244 s/iter. Total: 0.2266 s/iter. ETA=0:02:48
[01/17 18:14:02] d2.evaluation.evaluator INFO: Inference done 373/1093. Dataloading: 0.0118 s/iter. Inference: 0.1901 s/iter. Eval: 0.0243 s/iter. Total: 0.2262 s/iter. ETA=0:02:42
[01/17 18:14:07] d2.evaluation.evaluator INFO: Inference done 396/1093. Dataloading: 0.0117 s/iter. Inference: 0.1904 s/iter. Eval: 0.0241 s/iter. Total: 0.2263 s/iter. ETA=0:02:37
[01/17 18:14:13] d2.evaluation.evaluator INFO: Inference done 419/1093. Dataloading: 0.0116 s/iter. Inference: 0.1903 s/iter. Eval: 0.0241 s/iter. Total: 0.2261 s/iter. ETA=0:02:32
[01/17 18:14:18] d2.evaluation.evaluator INFO: Inference done 441/1093. Dataloading: 0.0117 s/iter. Inference: 0.1903 s/iter. Eval: 0.0242 s/iter. Total: 0.2263 s/iter. ETA=0:02:27
[01/17 18:14:23] d2.evaluation.evaluator INFO: Inference done 465/1093. Dataloading: 0.0117 s/iter. Inference: 0.1899 s/iter. Eval: 0.0241 s/iter. Total: 0.2258 s/iter. ETA=0:02:21
[01/17 18:14:28] d2.evaluation.evaluator INFO: Inference done 487/1093. Dataloading: 0.0116 s/iter. Inference: 0.1901 s/iter. Eval: 0.0241 s/iter. Total: 0.2259 s/iter. ETA=0:02:16
[01/17 18:14:33] d2.evaluation.evaluator INFO: Inference done 511/1093. Dataloading: 0.0115 s/iter. Inference: 0.1898 s/iter. Eval: 0.0240 s/iter. Total: 0.2254 s/iter. ETA=0:02:11
[01/17 18:14:38] d2.evaluation.evaluator INFO: Inference done 532/1093. Dataloading: 0.0117 s/iter. Inference: 0.1900 s/iter. Eval: 0.0242 s/iter. Total: 0.2260 s/iter. ETA=0:02:06
[01/17 18:14:43] d2.evaluation.evaluator INFO: Inference done 553/1093. Dataloading: 0.0118 s/iter. Inference: 0.1905 s/iter. Eval: 0.0243 s/iter. Total: 0.2266 s/iter. ETA=0:02:02
[01/17 18:14:48] d2.evaluation.evaluator INFO: Inference done 575/1093. Dataloading: 0.0118 s/iter. Inference: 0.1906 s/iter. Eval: 0.0242 s/iter. Total: 0.2267 s/iter. ETA=0:01:57
[01/17 18:14:53] d2.evaluation.evaluator INFO: Inference done 597/1093. Dataloading: 0.0118 s/iter. Inference: 0.1909 s/iter. Eval: 0.0242 s/iter. Total: 0.2270 s/iter. ETA=0:01:52
[01/17 18:14:58] d2.evaluation.evaluator INFO: Inference done 618/1093. Dataloading: 0.0118 s/iter. Inference: 0.1913 s/iter. Eval: 0.0242 s/iter. Total: 0.2274 s/iter. ETA=0:01:48
[01/17 18:15:04] d2.evaluation.evaluator INFO: Inference done 639/1093. Dataloading: 0.0118 s/iter. Inference: 0.1921 s/iter. Eval: 0.0241 s/iter. Total: 0.2281 s/iter. ETA=0:01:43
[01/17 18:15:09] d2.evaluation.evaluator INFO: Inference done 661/1093. Dataloading: 0.0118 s/iter. Inference: 0.1921 s/iter. Eval: 0.0242 s/iter. Total: 0.2282 s/iter. ETA=0:01:38
[01/17 18:15:14] d2.evaluation.evaluator INFO: Inference done 682/1093. Dataloading: 0.0117 s/iter. Inference: 0.1928 s/iter. Eval: 0.0242 s/iter. Total: 0.2288 s/iter. ETA=0:01:34
[01/17 18:15:19] d2.evaluation.evaluator INFO: Inference done 705/1093. Dataloading: 0.0117 s/iter. Inference: 0.1928 s/iter. Eval: 0.0241 s/iter. Total: 0.2287 s/iter. ETA=0:01:28
[01/17 18:15:24] d2.evaluation.evaluator INFO: Inference done 728/1093. Dataloading: 0.0116 s/iter. Inference: 0.1927 s/iter. Eval: 0.0241 s/iter. Total: 0.2285 s/iter. ETA=0:01:23
[01/17 18:15:29] d2.evaluation.evaluator INFO: Inference done 750/1093. Dataloading: 0.0116 s/iter. Inference: 0.1928 s/iter. Eval: 0.0241 s/iter. Total: 0.2286 s/iter. ETA=0:01:18
[01/17 18:15:34] d2.evaluation.evaluator INFO: Inference done 773/1093. Dataloading: 0.0116 s/iter. Inference: 0.1925 s/iter. Eval: 0.0241 s/iter. Total: 0.2283 s/iter. ETA=0:01:13
[01/17 18:15:40] d2.evaluation.evaluator INFO: Inference done 797/1093. Dataloading: 0.0116 s/iter. Inference: 0.1922 s/iter. Eval: 0.0241 s/iter. Total: 0.2280 s/iter. ETA=0:01:07
[01/17 18:15:45] d2.evaluation.evaluator INFO: Inference done 820/1093. Dataloading: 0.0115 s/iter. Inference: 0.1922 s/iter. Eval: 0.0240 s/iter. Total: 0.2278 s/iter. ETA=0:01:02
[01/17 18:15:50] d2.evaluation.evaluator INFO: Inference done 844/1093. Dataloading: 0.0114 s/iter. Inference: 0.1918 s/iter. Eval: 0.0239 s/iter. Total: 0.2273 s/iter. ETA=0:00:56
[01/17 18:15:55] d2.evaluation.evaluator INFO: Inference done 865/1093. Dataloading: 0.0115 s/iter. Inference: 0.1922 s/iter. Eval: 0.0240 s/iter. Total: 0.2278 s/iter. ETA=0:00:51
[01/17 18:16:00] d2.evaluation.evaluator INFO: Inference done 887/1093. Dataloading: 0.0116 s/iter. Inference: 0.1922 s/iter. Eval: 0.0239 s/iter. Total: 0.2278 s/iter. ETA=0:00:46
[01/17 18:16:05] d2.evaluation.evaluator INFO: Inference done 910/1093. Dataloading: 0.0115 s/iter. Inference: 0.1922 s/iter. Eval: 0.0238 s/iter. Total: 0.2277 s/iter. ETA=0:00:41
[01/17 18:16:10] d2.evaluation.evaluator INFO: Inference done 931/1093. Dataloading: 0.0115 s/iter. Inference: 0.1926 s/iter. Eval: 0.0238 s/iter. Total: 0.2279 s/iter. ETA=0:00:36
[01/17 18:16:15] d2.evaluation.evaluator INFO: Inference done 954/1093. Dataloading: 0.0115 s/iter. Inference: 0.1925 s/iter. Eval: 0.0238 s/iter. Total: 0.2278 s/iter. ETA=0:00:31
[01/17 18:16:20] d2.evaluation.evaluator INFO: Inference done 976/1093. Dataloading: 0.0115 s/iter. Inference: 0.1926 s/iter. Eval: 0.0238 s/iter. Total: 0.2279 s/iter. ETA=0:00:26
[01/17 18:16:26] d2.evaluation.evaluator INFO: Inference done 997/1093. Dataloading: 0.0115 s/iter. Inference: 0.1932 s/iter. Eval: 0.0237 s/iter. Total: 0.2285 s/iter. ETA=0:00:21
[01/17 18:16:31] d2.evaluation.evaluator INFO: Inference done 1019/1093. Dataloading: 0.0115 s/iter. Inference: 0.1931 s/iter. Eval: 0.0238 s/iter. Total: 0.2285 s/iter. ETA=0:00:16
[01/17 18:16:36] d2.evaluation.evaluator INFO: Inference done 1041/1093. Dataloading: 0.0115 s/iter. Inference: 0.1932 s/iter. Eval: 0.0237 s/iter. Total: 0.2285 s/iter. ETA=0:00:11
[01/17 18:16:41] d2.evaluation.evaluator INFO: Inference done 1064/1093. Dataloading: 0.0114 s/iter. Inference: 0.1932 s/iter. Eval: 0.0238 s/iter. Total: 0.2284 s/iter. ETA=0:00:06
[01/17 18:16:46] d2.evaluation.evaluator INFO: Inference done 1084/1093. Dataloading: 0.0115 s/iter. Inference: 0.1936 s/iter. Eval: 0.0237 s/iter. Total: 0.2289 s/iter. ETA=0:00:02
[01/17 18:16:49] d2.evaluation.evaluator INFO: Total inference time: 0:04:09.589235 (0.229402 s / iter per device, on 4 devices)
[01/17 18:16:49] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:30 (0.193596 s / iter per device, on 4 devices)
[01/17 18:17:19] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 30235.365231162978, 'mIoU': 0.005149680378771825, 'fwIoU': 0.00967447227175958, 'IoU-0': nan, 'IoU-1': 0.0, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 0.0, 'IoU-20': 0.0, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 0.0, 'IoU-28': 0.0, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.0, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 0.0, 'IoU-42': 0.0, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.9835889523454185, 'IoU-50': 0.0, 'IoU-51': 0.0, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.0, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 0.5235602094240838, 'pACC': 0.9835889523454185, 'ACC-0': nan, 'ACC-1': 0.0, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 0.0, 'ACC-20': 0.0, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 0.0, 'ACC-28': 0.0, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 0.0, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 0.0, 'ACC-42': 0.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 100.0, 'ACC-50': 0.0, 'ACC-51': 0.0, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 0.0, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 18:17:19] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 18:17:19] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 18:17:19] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 18:17:19] d2.evaluation.testing INFO: copypaste: 30235.3652,0.0051,0.0097,0.5236,0.9836
[01/17 18:17:19] d2.utils.events INFO:  eta: 22:56:56  iter: 19  total_loss: 148.7  loss_ce: 10.02  loss_mask: 0.5914  loss_dice: 4.899  loss_ce_0: 9.229  loss_mask_0: 0.3417  loss_dice_0: 4.852  loss_ce_1: 9.188  loss_mask_1: 0.3819  loss_dice_1: 4.859  loss_ce_2: 9.475  loss_mask_2: 0.5003  loss_dice_2: 4.851  loss_ce_3: 9.722  loss_mask_3: 0.5458  loss_dice_3: 4.782  loss_ce_4: 9.828  loss_mask_4: 0.4826  loss_dice_4: 4.906  loss_ce_5: 9.904  loss_mask_5: 0.5085  loss_dice_5: 4.903  loss_ce_6: 9.924  loss_mask_6: 0.5962  loss_dice_6: 4.885  loss_ce_7: 9.936  loss_mask_7: 0.6141  loss_dice_7: 4.873  loss_ce_8: 9.993  loss_mask_8: 0.6252  loss_dice_8: 4.901  time: 2.6169  data_time: 0.5671  lr: 9.9957e-06  max_mem: 20858M
[01/17 18:17:52] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 18:17:52] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 18:17:52] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 18:17:53] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 18:18:06] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0360 s/iter. Inference: 0.1916 s/iter. Eval: 0.0225 s/iter. Total: 0.2501 s/iter. ETA=0:04:30
[01/17 18:18:11] d2.evaluation.evaluator INFO: Inference done 33/1093. Dataloading: 0.0174 s/iter. Inference: 0.1967 s/iter. Eval: 0.0221 s/iter. Total: 0.2364 s/iter. ETA=0:04:10
[01/17 18:18:16] d2.evaluation.evaluator INFO: Inference done 57/1093. Dataloading: 0.0168 s/iter. Inference: 0.1899 s/iter. Eval: 0.0219 s/iter. Total: 0.2288 s/iter. ETA=0:03:57
[01/17 18:18:21] d2.evaluation.evaluator INFO: Inference done 79/1093. Dataloading: 0.0157 s/iter. Inference: 0.1925 s/iter. Eval: 0.0222 s/iter. Total: 0.2305 s/iter. ETA=0:03:53
[01/17 18:18:26] d2.evaluation.evaluator INFO: Inference done 102/1093. Dataloading: 0.0153 s/iter. Inference: 0.1905 s/iter. Eval: 0.0224 s/iter. Total: 0.2283 s/iter. ETA=0:03:46
[01/17 18:18:31] d2.evaluation.evaluator INFO: Inference done 125/1093. Dataloading: 0.0153 s/iter. Inference: 0.1878 s/iter. Eval: 0.0230 s/iter. Total: 0.2262 s/iter. ETA=0:03:38
[01/17 18:18:36] d2.evaluation.evaluator INFO: Inference done 146/1093. Dataloading: 0.0153 s/iter. Inference: 0.1909 s/iter. Eval: 0.0238 s/iter. Total: 0.2301 s/iter. ETA=0:03:37
[01/17 18:18:42] d2.evaluation.evaluator INFO: Inference done 166/1093. Dataloading: 0.0157 s/iter. Inference: 0.1933 s/iter. Eval: 0.0240 s/iter. Total: 0.2330 s/iter. ETA=0:03:36
[01/17 18:18:47] d2.evaluation.evaluator INFO: Inference done 189/1093. Dataloading: 0.0155 s/iter. Inference: 0.1920 s/iter. Eval: 0.0244 s/iter. Total: 0.2320 s/iter. ETA=0:03:29
[01/17 18:18:52] d2.evaluation.evaluator INFO: Inference done 211/1093. Dataloading: 0.0159 s/iter. Inference: 0.1918 s/iter. Eval: 0.0244 s/iter. Total: 0.2322 s/iter. ETA=0:03:24
[01/17 18:18:57] d2.evaluation.evaluator INFO: Inference done 234/1093. Dataloading: 0.0157 s/iter. Inference: 0.1913 s/iter. Eval: 0.0241 s/iter. Total: 0.2312 s/iter. ETA=0:03:18
[01/17 18:19:02] d2.evaluation.evaluator INFO: Inference done 255/1093. Dataloading: 0.0155 s/iter. Inference: 0.1924 s/iter. Eval: 0.0241 s/iter. Total: 0.2322 s/iter. ETA=0:03:14
[01/17 18:19:07] d2.evaluation.evaluator INFO: Inference done 277/1093. Dataloading: 0.0153 s/iter. Inference: 0.1925 s/iter. Eval: 0.0240 s/iter. Total: 0.2319 s/iter. ETA=0:03:09
[01/17 18:19:12] d2.evaluation.evaluator INFO: Inference done 297/1093. Dataloading: 0.0152 s/iter. Inference: 0.1939 s/iter. Eval: 0.0240 s/iter. Total: 0.2332 s/iter. ETA=0:03:05
[01/17 18:19:17] d2.evaluation.evaluator INFO: Inference done 319/1093. Dataloading: 0.0153 s/iter. Inference: 0.1939 s/iter. Eval: 0.0241 s/iter. Total: 0.2334 s/iter. ETA=0:03:00
[01/17 18:19:22] d2.evaluation.evaluator INFO: Inference done 342/1093. Dataloading: 0.0150 s/iter. Inference: 0.1933 s/iter. Eval: 0.0242 s/iter. Total: 0.2326 s/iter. ETA=0:02:54
[01/17 18:19:28] d2.evaluation.evaluator INFO: Inference done 364/1093. Dataloading: 0.0150 s/iter. Inference: 0.1936 s/iter. Eval: 0.0241 s/iter. Total: 0.2327 s/iter. ETA=0:02:49
[01/17 18:19:33] d2.evaluation.evaluator INFO: Inference done 385/1093. Dataloading: 0.0149 s/iter. Inference: 0.1943 s/iter. Eval: 0.0240 s/iter. Total: 0.2333 s/iter. ETA=0:02:45
[01/17 18:19:38] d2.evaluation.evaluator INFO: Inference done 407/1093. Dataloading: 0.0148 s/iter. Inference: 0.1943 s/iter. Eval: 0.0241 s/iter. Total: 0.2333 s/iter. ETA=0:02:40
[01/17 18:19:43] d2.evaluation.evaluator INFO: Inference done 429/1093. Dataloading: 0.0149 s/iter. Inference: 0.1942 s/iter. Eval: 0.0242 s/iter. Total: 0.2334 s/iter. ETA=0:02:34
[01/17 18:19:48] d2.evaluation.evaluator INFO: Inference done 452/1093. Dataloading: 0.0149 s/iter. Inference: 0.1936 s/iter. Eval: 0.0240 s/iter. Total: 0.2326 s/iter. ETA=0:02:29
[01/17 18:19:53] d2.evaluation.evaluator INFO: Inference done 474/1093. Dataloading: 0.0147 s/iter. Inference: 0.1936 s/iter. Eval: 0.0240 s/iter. Total: 0.2324 s/iter. ETA=0:02:23
[01/17 18:19:58] d2.evaluation.evaluator INFO: Inference done 496/1093. Dataloading: 0.0147 s/iter. Inference: 0.1934 s/iter. Eval: 0.0240 s/iter. Total: 0.2322 s/iter. ETA=0:02:18
[01/17 18:20:03] d2.evaluation.evaluator INFO: Inference done 519/1093. Dataloading: 0.0146 s/iter. Inference: 0.1933 s/iter. Eval: 0.0240 s/iter. Total: 0.2320 s/iter. ETA=0:02:13
[01/17 18:20:08] d2.evaluation.evaluator INFO: Inference done 540/1093. Dataloading: 0.0146 s/iter. Inference: 0.1937 s/iter. Eval: 0.0239 s/iter. Total: 0.2323 s/iter. ETA=0:02:08
[01/17 18:20:13] d2.evaluation.evaluator INFO: Inference done 561/1093. Dataloading: 0.0146 s/iter. Inference: 0.1942 s/iter. Eval: 0.0238 s/iter. Total: 0.2327 s/iter. ETA=0:02:03
[01/17 18:20:18] d2.evaluation.evaluator INFO: Inference done 581/1093. Dataloading: 0.0147 s/iter. Inference: 0.1949 s/iter. Eval: 0.0238 s/iter. Total: 0.2334 s/iter. ETA=0:01:59
[01/17 18:20:24] d2.evaluation.evaluator INFO: Inference done 603/1093. Dataloading: 0.0146 s/iter. Inference: 0.1948 s/iter. Eval: 0.0237 s/iter. Total: 0.2333 s/iter. ETA=0:01:54
[01/17 18:20:29] d2.evaluation.evaluator INFO: Inference done 626/1093. Dataloading: 0.0146 s/iter. Inference: 0.1946 s/iter. Eval: 0.0237 s/iter. Total: 0.2329 s/iter. ETA=0:01:48
[01/17 18:20:34] d2.evaluation.evaluator INFO: Inference done 648/1093. Dataloading: 0.0145 s/iter. Inference: 0.1946 s/iter. Eval: 0.0237 s/iter. Total: 0.2329 s/iter. ETA=0:01:43
[01/17 18:20:39] d2.evaluation.evaluator INFO: Inference done 669/1093. Dataloading: 0.0145 s/iter. Inference: 0.1949 s/iter. Eval: 0.0237 s/iter. Total: 0.2332 s/iter. ETA=0:01:38
[01/17 18:20:44] d2.evaluation.evaluator INFO: Inference done 691/1093. Dataloading: 0.0144 s/iter. Inference: 0.1950 s/iter. Eval: 0.0237 s/iter. Total: 0.2332 s/iter. ETA=0:01:33
[01/17 18:20:49] d2.evaluation.evaluator INFO: Inference done 711/1093. Dataloading: 0.0146 s/iter. Inference: 0.1957 s/iter. Eval: 0.0238 s/iter. Total: 0.2342 s/iter. ETA=0:01:29
[01/17 18:20:55] d2.evaluation.evaluator INFO: Inference done 731/1093. Dataloading: 0.0147 s/iter. Inference: 0.1964 s/iter. Eval: 0.0238 s/iter. Total: 0.2350 s/iter. ETA=0:01:25
[01/17 18:21:00] d2.evaluation.evaluator INFO: Inference done 751/1093. Dataloading: 0.0146 s/iter. Inference: 0.1969 s/iter. Eval: 0.0239 s/iter. Total: 0.2355 s/iter. ETA=0:01:20
[01/17 18:21:05] d2.evaluation.evaluator INFO: Inference done 771/1093. Dataloading: 0.0147 s/iter. Inference: 0.1973 s/iter. Eval: 0.0239 s/iter. Total: 0.2360 s/iter. ETA=0:01:16
[01/17 18:21:10] d2.evaluation.evaluator INFO: Inference done 793/1093. Dataloading: 0.0146 s/iter. Inference: 0.1972 s/iter. Eval: 0.0239 s/iter. Total: 0.2358 s/iter. ETA=0:01:10
[01/17 18:21:15] d2.evaluation.evaluator INFO: Inference done 814/1093. Dataloading: 0.0145 s/iter. Inference: 0.1976 s/iter. Eval: 0.0239 s/iter. Total: 0.2362 s/iter. ETA=0:01:05
[01/17 18:21:20] d2.evaluation.evaluator INFO: Inference done 837/1093. Dataloading: 0.0144 s/iter. Inference: 0.1974 s/iter. Eval: 0.0239 s/iter. Total: 0.2358 s/iter. ETA=0:01:00
[01/17 18:21:25] d2.evaluation.evaluator INFO: Inference done 858/1093. Dataloading: 0.0144 s/iter. Inference: 0.1977 s/iter. Eval: 0.0239 s/iter. Total: 0.2361 s/iter. ETA=0:00:55
[01/17 18:21:31] d2.evaluation.evaluator INFO: Inference done 878/1093. Dataloading: 0.0144 s/iter. Inference: 0.1981 s/iter. Eval: 0.0239 s/iter. Total: 0.2366 s/iter. ETA=0:00:50
[01/17 18:21:36] d2.evaluation.evaluator INFO: Inference done 898/1093. Dataloading: 0.0145 s/iter. Inference: 0.1984 s/iter. Eval: 0.0239 s/iter. Total: 0.2369 s/iter. ETA=0:00:46
[01/17 18:21:41] d2.evaluation.evaluator INFO: Inference done 919/1093. Dataloading: 0.0145 s/iter. Inference: 0.1986 s/iter. Eval: 0.0240 s/iter. Total: 0.2371 s/iter. ETA=0:00:41
[01/17 18:21:46] d2.evaluation.evaluator INFO: Inference done 939/1093. Dataloading: 0.0146 s/iter. Inference: 0.1989 s/iter. Eval: 0.0240 s/iter. Total: 0.2376 s/iter. ETA=0:00:36
[01/17 18:21:51] d2.evaluation.evaluator INFO: Inference done 961/1093. Dataloading: 0.0145 s/iter. Inference: 0.1988 s/iter. Eval: 0.0240 s/iter. Total: 0.2374 s/iter. ETA=0:00:31
[01/17 18:21:56] d2.evaluation.evaluator INFO: Inference done 983/1093. Dataloading: 0.0144 s/iter. Inference: 0.1989 s/iter. Eval: 0.0240 s/iter. Total: 0.2374 s/iter. ETA=0:00:26
[01/17 18:22:01] d2.evaluation.evaluator INFO: Inference done 1004/1093. Dataloading: 0.0144 s/iter. Inference: 0.1989 s/iter. Eval: 0.0240 s/iter. Total: 0.2374 s/iter. ETA=0:00:21
[01/17 18:22:06] d2.evaluation.evaluator INFO: Inference done 1024/1093. Dataloading: 0.0145 s/iter. Inference: 0.1991 s/iter. Eval: 0.0241 s/iter. Total: 0.2378 s/iter. ETA=0:00:16
[01/17 18:22:12] d2.evaluation.evaluator INFO: Inference done 1045/1093. Dataloading: 0.0145 s/iter. Inference: 0.1992 s/iter. Eval: 0.0242 s/iter. Total: 0.2380 s/iter. ETA=0:00:11
[01/17 18:22:17] d2.evaluation.evaluator INFO: Inference done 1067/1093. Dataloading: 0.0144 s/iter. Inference: 0.1991 s/iter. Eval: 0.0242 s/iter. Total: 0.2379 s/iter. ETA=0:00:06
[01/17 18:22:18] d2.engine.hooks INFO: Overall training speed: 37 iterations in 0:01:19 (2.1581 s / it)
[01/17 18:22:18] d2.engine.hooks INFO: Total training time: 0:10:38 (0:09:18 on hooks)
[01/17 18:22:18] d2.utils.events INFO:  eta: 21:09:54  iter: 39  total_loss: 133.7  loss_ce: 9.295  loss_mask: 0.4944  loss_dice: 4.503  loss_ce_0: 9  loss_mask_0: 0.2756  loss_dice_0: 4.434  loss_ce_1: 7.957  loss_mask_1: 0.282  loss_dice_1: 4.42  loss_ce_2: 7.857  loss_mask_2: 0.2901  loss_dice_2: 4.424  loss_ce_3: 7.817  loss_mask_3: 0.2898  loss_dice_3: 4.426  loss_ce_4: 7.953  loss_mask_4: 0.2887  loss_dice_4: 4.431  loss_ce_5: 8.291  loss_mask_5: 0.3003  loss_dice_5: 4.463  loss_ce_6: 8.852  loss_mask_6: 0.3173  loss_dice_6: 4.483  loss_ce_7: 9.095  loss_mask_7: 0.4116  loss_dice_7: 4.498  loss_ce_8: 9.213  loss_mask_8: 0.4585  loss_dice_8: 4.494  time: 2.1012  data_time: 0.0875  lr: 9.9912e-06  max_mem: 20858M
[01/17 18:22:58] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 18:23:02] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 18:23:02] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 18:23:02] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 18:23:02] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m20[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 18:23:02] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 18:23:02] d2.utils.env INFO: Using a generated random seed 2952674
[01/17 18:23:04] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 18:23:04] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 18:23:08] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 18:23:08] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 18:23:08] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 18:23:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 18:23:08] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 18:23:08] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 18:23:08] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 18:23:08] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 18:23:08] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 18:24:21] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 18:24:22] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 18:24:22] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 18:24:23] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 18:24:35] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0218 s/iter. Inference: 0.2135 s/iter. Eval: 0.0308 s/iter. Total: 0.2662 s/iter. ETA=0:04:48
[01/17 18:24:40] d2.evaluation.evaluator INFO: Inference done 30/1093. Dataloading: 0.0187 s/iter. Inference: 0.2251 s/iter. Eval: 0.0297 s/iter. Total: 0.2736 s/iter. ETA=0:04:50
[01/17 18:24:46] d2.evaluation.evaluator INFO: Inference done 52/1093. Dataloading: 0.0170 s/iter. Inference: 0.2071 s/iter. Eval: 0.0283 s/iter. Total: 0.2525 s/iter. ETA=0:04:22
[01/17 18:24:51] d2.evaluation.evaluator INFO: Inference done 74/1093. Dataloading: 0.0172 s/iter. Inference: 0.2037 s/iter. Eval: 0.0284 s/iter. Total: 0.2494 s/iter. ETA=0:04:14
[01/17 18:24:56] d2.evaluation.evaluator INFO: Inference done 96/1093. Dataloading: 0.0175 s/iter. Inference: 0.2013 s/iter. Eval: 0.0284 s/iter. Total: 0.2474 s/iter. ETA=0:04:06
[01/17 18:25:01] d2.evaluation.evaluator INFO: Inference done 119/1093. Dataloading: 0.0173 s/iter. Inference: 0.1978 s/iter. Eval: 0.0279 s/iter. Total: 0.2433 s/iter. ETA=0:03:56
[01/17 18:25:07] d2.evaluation.evaluator INFO: Inference done 141/1093. Dataloading: 0.0167 s/iter. Inference: 0.1973 s/iter. Eval: 0.0280 s/iter. Total: 0.2422 s/iter. ETA=0:03:50
[01/17 18:25:12] d2.evaluation.evaluator INFO: Inference done 162/1093. Dataloading: 0.0164 s/iter. Inference: 0.1978 s/iter. Eval: 0.0277 s/iter. Total: 0.2420 s/iter. ETA=0:03:45
[01/17 18:25:17] d2.evaluation.evaluator INFO: Inference done 182/1093. Dataloading: 0.0162 s/iter. Inference: 0.1997 s/iter. Eval: 0.0275 s/iter. Total: 0.2436 s/iter. ETA=0:03:41
[01/17 18:25:22] d2.evaluation.evaluator INFO: Inference done 202/1093. Dataloading: 0.0160 s/iter. Inference: 0.2007 s/iter. Eval: 0.0277 s/iter. Total: 0.2446 s/iter. ETA=0:03:37
[01/17 18:25:27] d2.evaluation.evaluator INFO: Inference done 222/1093. Dataloading: 0.0160 s/iter. Inference: 0.2020 s/iter. Eval: 0.0278 s/iter. Total: 0.2460 s/iter. ETA=0:03:34
[01/17 18:25:32] d2.evaluation.evaluator INFO: Inference done 242/1093. Dataloading: 0.0159 s/iter. Inference: 0.2024 s/iter. Eval: 0.0279 s/iter. Total: 0.2463 s/iter. ETA=0:03:29
[01/17 18:25:37] d2.evaluation.evaluator INFO: Inference done 262/1093. Dataloading: 0.0160 s/iter. Inference: 0.2032 s/iter. Eval: 0.0277 s/iter. Total: 0.2470 s/iter. ETA=0:03:25
[01/17 18:25:42] d2.evaluation.evaluator INFO: Inference done 283/1093. Dataloading: 0.0159 s/iter. Inference: 0.2026 s/iter. Eval: 0.0278 s/iter. Total: 0.2465 s/iter. ETA=0:03:19
[01/17 18:25:47] d2.evaluation.evaluator INFO: Inference done 304/1093. Dataloading: 0.0159 s/iter. Inference: 0.2023 s/iter. Eval: 0.0278 s/iter. Total: 0.2461 s/iter. ETA=0:03:14
[01/17 18:25:52] d2.evaluation.evaluator INFO: Inference done 326/1093. Dataloading: 0.0158 s/iter. Inference: 0.2017 s/iter. Eval: 0.0275 s/iter. Total: 0.2452 s/iter. ETA=0:03:08
[01/17 18:25:58] d2.evaluation.evaluator INFO: Inference done 346/1093. Dataloading: 0.0158 s/iter. Inference: 0.2023 s/iter. Eval: 0.0276 s/iter. Total: 0.2459 s/iter. ETA=0:03:03
[01/17 18:26:03] d2.evaluation.evaluator INFO: Inference done 368/1093. Dataloading: 0.0158 s/iter. Inference: 0.2018 s/iter. Eval: 0.0275 s/iter. Total: 0.2451 s/iter. ETA=0:02:57
[01/17 18:26:08] d2.evaluation.evaluator INFO: Inference done 391/1093. Dataloading: 0.0156 s/iter. Inference: 0.2010 s/iter. Eval: 0.0273 s/iter. Total: 0.2441 s/iter. ETA=0:02:51
[01/17 18:26:13] d2.evaluation.evaluator INFO: Inference done 410/1093. Dataloading: 0.0155 s/iter. Inference: 0.2021 s/iter. Eval: 0.0273 s/iter. Total: 0.2450 s/iter. ETA=0:02:47
[01/17 18:26:18] d2.evaluation.evaluator INFO: Inference done 432/1093. Dataloading: 0.0155 s/iter. Inference: 0.2019 s/iter. Eval: 0.0272 s/iter. Total: 0.2448 s/iter. ETA=0:02:41
[01/17 18:26:23] d2.evaluation.evaluator INFO: Inference done 455/1093. Dataloading: 0.0154 s/iter. Inference: 0.2011 s/iter. Eval: 0.0272 s/iter. Total: 0.2437 s/iter. ETA=0:02:35
[01/17 18:26:28] d2.evaluation.evaluator INFO: Inference done 477/1093. Dataloading: 0.0152 s/iter. Inference: 0.2006 s/iter. Eval: 0.0272 s/iter. Total: 0.2430 s/iter. ETA=0:02:29
[01/17 18:26:34] d2.evaluation.evaluator INFO: Inference done 498/1093. Dataloading: 0.0151 s/iter. Inference: 0.2008 s/iter. Eval: 0.0272 s/iter. Total: 0.2431 s/iter. ETA=0:02:24
[01/17 18:26:39] d2.evaluation.evaluator INFO: Inference done 520/1093. Dataloading: 0.0151 s/iter. Inference: 0.2002 s/iter. Eval: 0.0272 s/iter. Total: 0.2425 s/iter. ETA=0:02:18
[01/17 18:26:44] d2.evaluation.evaluator INFO: Inference done 541/1093. Dataloading: 0.0151 s/iter. Inference: 0.2001 s/iter. Eval: 0.0272 s/iter. Total: 0.2425 s/iter. ETA=0:02:13
[01/17 18:26:49] d2.evaluation.evaluator INFO: Inference done 563/1093. Dataloading: 0.0151 s/iter. Inference: 0.1998 s/iter. Eval: 0.0272 s/iter. Total: 0.2422 s/iter. ETA=0:02:08
[01/17 18:26:54] d2.evaluation.evaluator INFO: Inference done 585/1093. Dataloading: 0.0151 s/iter. Inference: 0.1996 s/iter. Eval: 0.0270 s/iter. Total: 0.2418 s/iter. ETA=0:02:02
[01/17 18:26:59] d2.evaluation.evaluator INFO: Inference done 606/1093. Dataloading: 0.0151 s/iter. Inference: 0.1998 s/iter. Eval: 0.0270 s/iter. Total: 0.2420 s/iter. ETA=0:01:57
[01/17 18:27:04] d2.evaluation.evaluator INFO: Inference done 627/1093. Dataloading: 0.0151 s/iter. Inference: 0.2000 s/iter. Eval: 0.0269 s/iter. Total: 0.2421 s/iter. ETA=0:01:52
[01/17 18:27:09] d2.evaluation.evaluator INFO: Inference done 650/1093. Dataloading: 0.0151 s/iter. Inference: 0.1994 s/iter. Eval: 0.0269 s/iter. Total: 0.2415 s/iter. ETA=0:01:46
[01/17 18:27:15] d2.evaluation.evaluator INFO: Inference done 673/1093. Dataloading: 0.0150 s/iter. Inference: 0.1989 s/iter. Eval: 0.0269 s/iter. Total: 0.2409 s/iter. ETA=0:01:41
[01/17 18:27:20] d2.evaluation.evaluator INFO: Inference done 695/1093. Dataloading: 0.0150 s/iter. Inference: 0.1988 s/iter. Eval: 0.0268 s/iter. Total: 0.2406 s/iter. ETA=0:01:35
[01/17 18:27:25] d2.evaluation.evaluator INFO: Inference done 718/1093. Dataloading: 0.0150 s/iter. Inference: 0.1982 s/iter. Eval: 0.0266 s/iter. Total: 0.2400 s/iter. ETA=0:01:29
[01/17 18:27:30] d2.evaluation.evaluator INFO: Inference done 741/1093. Dataloading: 0.0149 s/iter. Inference: 0.1977 s/iter. Eval: 0.0266 s/iter. Total: 0.2394 s/iter. ETA=0:01:24
[01/17 18:27:35] d2.evaluation.evaluator INFO: Inference done 763/1093. Dataloading: 0.0149 s/iter. Inference: 0.1976 s/iter. Eval: 0.0265 s/iter. Total: 0.2392 s/iter. ETA=0:01:18
[01/17 18:27:40] d2.evaluation.evaluator INFO: Inference done 785/1093. Dataloading: 0.0148 s/iter. Inference: 0.1974 s/iter. Eval: 0.0265 s/iter. Total: 0.2389 s/iter. ETA=0:01:13
[01/17 18:27:45] d2.evaluation.evaluator INFO: Inference done 807/1093. Dataloading: 0.0147 s/iter. Inference: 0.1973 s/iter. Eval: 0.0264 s/iter. Total: 0.2386 s/iter. ETA=0:01:08
[01/17 18:27:50] d2.evaluation.evaluator INFO: Inference done 830/1093. Dataloading: 0.0147 s/iter. Inference: 0.1971 s/iter. Eval: 0.0264 s/iter. Total: 0.2383 s/iter. ETA=0:01:02
[01/17 18:27:55] d2.evaluation.evaluator INFO: Inference done 853/1093. Dataloading: 0.0146 s/iter. Inference: 0.1969 s/iter. Eval: 0.0263 s/iter. Total: 0.2380 s/iter. ETA=0:00:57
[01/17 18:28:00] d2.evaluation.evaluator INFO: Inference done 875/1093. Dataloading: 0.0146 s/iter. Inference: 0.1968 s/iter. Eval: 0.0262 s/iter. Total: 0.2377 s/iter. ETA=0:00:51
[01/17 18:28:05] d2.evaluation.evaluator INFO: Inference done 897/1093. Dataloading: 0.0146 s/iter. Inference: 0.1965 s/iter. Eval: 0.0263 s/iter. Total: 0.2375 s/iter. ETA=0:00:46
[01/17 18:28:11] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0145 s/iter. Inference: 0.1970 s/iter. Eval: 0.0263 s/iter. Total: 0.2379 s/iter. ETA=0:00:41
[01/17 18:28:16] d2.evaluation.evaluator INFO: Inference done 938/1093. Dataloading: 0.0145 s/iter. Inference: 0.1971 s/iter. Eval: 0.0263 s/iter. Total: 0.2381 s/iter. ETA=0:00:36
[01/17 18:28:21] d2.evaluation.evaluator INFO: Inference done 961/1093. Dataloading: 0.0145 s/iter. Inference: 0.1969 s/iter. Eval: 0.0262 s/iter. Total: 0.2377 s/iter. ETA=0:00:31
[01/17 18:28:26] d2.evaluation.evaluator INFO: Inference done 982/1093. Dataloading: 0.0145 s/iter. Inference: 0.1970 s/iter. Eval: 0.0262 s/iter. Total: 0.2378 s/iter. ETA=0:00:26
[01/17 18:28:31] d2.evaluation.evaluator INFO: Inference done 1005/1093. Dataloading: 0.0144 s/iter. Inference: 0.1968 s/iter. Eval: 0.0262 s/iter. Total: 0.2376 s/iter. ETA=0:00:20
[01/17 18:28:36] d2.evaluation.evaluator INFO: Inference done 1027/1093. Dataloading: 0.0144 s/iter. Inference: 0.1968 s/iter. Eval: 0.0261 s/iter. Total: 0.2375 s/iter. ETA=0:00:15
[01/17 18:28:41] d2.evaluation.evaluator INFO: Inference done 1050/1093. Dataloading: 0.0144 s/iter. Inference: 0.1965 s/iter. Eval: 0.0262 s/iter. Total: 0.2372 s/iter. ETA=0:00:10
[01/17 18:28:47] d2.evaluation.evaluator INFO: Inference done 1073/1093. Dataloading: 0.0143 s/iter. Inference: 0.1962 s/iter. Eval: 0.0262 s/iter. Total: 0.2367 s/iter. ETA=0:00:04
[01/17 18:28:51] d2.evaluation.evaluator INFO: Total inference time: 0:04:17.143659 (0.236345 s / iter per device, on 4 devices)
[01/17 18:28:51] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:32 (0.195525 s / iter per device, on 4 devices)
[01/17 18:28:52] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 25.315528156648348, 'mIoU': 0.006905027497732623, 'fwIoU': 0.017393923644820487, 'IoU-0': nan, 'IoU-1': 0.0, 'IoU-2': 0.0, 'IoU-3': 0.0, 'IoU-4': 0.0, 'IoU-5': 0.0, 'IoU-6': 0.0, 'IoU-7': 0.0, 'IoU-8': 0.0, 'IoU-9': 0.0, 'IoU-10': 0.0, 'IoU-11': 0.0, 'IoU-12': 0.0, 'IoU-13': 0.0, 'IoU-14': 0.0, 'IoU-15': 0.0, 'IoU-16': 0.0, 'IoU-17': 0.0, 'IoU-18': 0.0, 'IoU-19': 0.0, 'IoU-20': 0.0, 'IoU-21': 0.0, 'IoU-22': 0.0, 'IoU-23': 0.0, 'IoU-24': 0.0, 'IoU-25': 0.0, 'IoU-26': 0.0, 'IoU-27': 0.0, 'IoU-28': 0.0, 'IoU-29': 0.0, 'IoU-30': 0.0, 'IoU-31': 0.0, 'IoU-32': 0.0, 'IoU-33': 0.0, 'IoU-34': 0.0, 'IoU-35': 0.0, 'IoU-36': 0.0, 'IoU-37': 0.0, 'IoU-38': 0.0, 'IoU-39': 0.0, 'IoU-40': 0.0, 'IoU-41': 1.318860252066931, 'IoU-42': 0.0, 'IoU-43': 0.0, 'IoU-44': 0.0, 'IoU-45': 0.0, 'IoU-46': 0.0, 'IoU-47': 0.0, 'IoU-48': 0.0, 'IoU-49': 0.0, 'IoU-50': 0.0, 'IoU-51': 0.0, 'IoU-52': 0.0, 'IoU-53': 0.0, 'IoU-54': 0.0, 'IoU-55': 0.0, 'IoU-56': 0.0, 'IoU-57': 0.0, 'IoU-58': 0.0, 'IoU-59': 0.0, 'IoU-60': 0.0, 'IoU-61': 0.0, 'IoU-62': 0.0, 'IoU-63': 0.0, 'IoU-64': 0.0, 'IoU-65': 0.0, 'IoU-66': 0.0, 'IoU-67': 0.0, 'IoU-68': 0.0, 'IoU-69': 0.0, 'IoU-70': 0.0, 'IoU-71': 0.0, 'IoU-72': 0.0, 'IoU-73': 0.0, 'IoU-74': 0.0, 'IoU-75': 0.0, 'IoU-76': 0.0, 'IoU-77': 0.0, 'IoU-78': 0.0, 'IoU-79': 0.0, 'IoU-80': 0.0, 'IoU-81': 0.0, 'IoU-82': 0.0, 'IoU-83': 0.0, 'IoU-84': 0.0, 'IoU-85': 0.0, 'IoU-86': 0.0, 'IoU-87': 0.0, 'IoU-88': 0.0, 'IoU-89': 0.0, 'IoU-90': 0.0, 'IoU-91': 0.0, 'IoU-92': 0.0, 'IoU-93': 0.0, 'IoU-94': 0.0, 'IoU-95': 0.0, 'IoU-96': 0.0, 'IoU-97': 0.0, 'IoU-98': 0.0, 'IoU-99': 0.0, 'IoU-100': 0.0, 'IoU-101': 0.0, 'IoU-102': 0.0, 'IoU-103': 0.0, 'IoU-104': 0.0, 'IoU-105': 0.0, 'IoU-106': 0.0, 'IoU-107': 0.0, 'IoU-108': 0.0, 'IoU-109': 0.0, 'IoU-110': 0.0, 'IoU-111': 0.0, 'IoU-112': 0.0, 'IoU-113': 0.0, 'IoU-114': 0.0, 'IoU-115': 0.0, 'IoU-116': 0.0, 'IoU-117': 0.0, 'IoU-118': 0.0, 'IoU-119': 0.0, 'IoU-120': 0.0, 'IoU-121': 0.0, 'IoU-122': 0.0, 'IoU-123': 0.0, 'IoU-124': 0.0, 'IoU-125': 0.0, 'IoU-126': 0.0, 'IoU-127': 0.0, 'IoU-128': 0.0, 'IoU-129': 0.0, 'IoU-130': 0.0, 'IoU-131': 0.0, 'IoU-132': 0.0, 'IoU-133': 0.0, 'IoU-134': 0.0, 'IoU-135': 0.0, 'IoU-136': 0.0, 'IoU-137': 0.0, 'IoU-138': 0.0, 'IoU-139': 0.0, 'IoU-140': 0.0, 'IoU-141': 0.0, 'IoU-142': 0.0, 'IoU-143': 0.0, 'IoU-144': 0.0, 'IoU-145': 0.0, 'IoU-146': 0.0, 'IoU-147': 0.0, 'IoU-148': 0.0, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.0, 'IoU-152': 0.0, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.0, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.0, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 0.5235602094240838, 'pACC': 1.318860252066931, 'ACC-0': nan, 'ACC-1': 0.0, 'ACC-2': 0.0, 'ACC-3': 0.0, 'ACC-4': 0.0, 'ACC-5': 0.0, 'ACC-6': 0.0, 'ACC-7': 0.0, 'ACC-8': 0.0, 'ACC-9': 0.0, 'ACC-10': 0.0, 'ACC-11': 0.0, 'ACC-12': 0.0, 'ACC-13': 0.0, 'ACC-14': 0.0, 'ACC-15': 0.0, 'ACC-16': 0.0, 'ACC-17': 0.0, 'ACC-18': 0.0, 'ACC-19': 0.0, 'ACC-20': 0.0, 'ACC-21': 0.0, 'ACC-22': 0.0, 'ACC-23': 0.0, 'ACC-24': 0.0, 'ACC-25': 0.0, 'ACC-26': 0.0, 'ACC-27': 0.0, 'ACC-28': 0.0, 'ACC-29': 0.0, 'ACC-30': 0.0, 'ACC-31': 0.0, 'ACC-32': 0.0, 'ACC-33': 0.0, 'ACC-34': 0.0, 'ACC-35': 0.0, 'ACC-36': 0.0, 'ACC-37': 0.0, 'ACC-38': 0.0, 'ACC-39': 0.0, 'ACC-40': 0.0, 'ACC-41': 100.0, 'ACC-42': 0.0, 'ACC-43': 0.0, 'ACC-44': 0.0, 'ACC-45': 0.0, 'ACC-46': 0.0, 'ACC-47': 0.0, 'ACC-48': 0.0, 'ACC-49': 0.0, 'ACC-50': 0.0, 'ACC-51': 0.0, 'ACC-52': 0.0, 'ACC-53': 0.0, 'ACC-54': 0.0, 'ACC-55': 0.0, 'ACC-56': 0.0, 'ACC-57': 0.0, 'ACC-58': 0.0, 'ACC-59': 0.0, 'ACC-60': 0.0, 'ACC-61': 0.0, 'ACC-62': 0.0, 'ACC-63': 0.0, 'ACC-64': 0.0, 'ACC-65': 0.0, 'ACC-66': 0.0, 'ACC-67': 0.0, 'ACC-68': 0.0, 'ACC-69': 0.0, 'ACC-70': 0.0, 'ACC-71': 0.0, 'ACC-72': 0.0, 'ACC-73': 0.0, 'ACC-74': 0.0, 'ACC-75': 0.0, 'ACC-76': 0.0, 'ACC-77': 0.0, 'ACC-78': 0.0, 'ACC-79': 0.0, 'ACC-80': 0.0, 'ACC-81': 0.0, 'ACC-82': 0.0, 'ACC-83': 0.0, 'ACC-84': 0.0, 'ACC-85': 0.0, 'ACC-86': 0.0, 'ACC-87': 0.0, 'ACC-88': 0.0, 'ACC-89': 0.0, 'ACC-90': 0.0, 'ACC-91': 0.0, 'ACC-92': 0.0, 'ACC-93': 0.0, 'ACC-94': 0.0, 'ACC-95': 0.0, 'ACC-96': 0.0, 'ACC-97': 0.0, 'ACC-98': 0.0, 'ACC-99': 0.0, 'ACC-100': 0.0, 'ACC-101': 0.0, 'ACC-102': 0.0, 'ACC-103': 0.0, 'ACC-104': 0.0, 'ACC-105': 0.0, 'ACC-106': 0.0, 'ACC-107': 0.0, 'ACC-108': 0.0, 'ACC-109': 0.0, 'ACC-110': 0.0, 'ACC-111': 0.0, 'ACC-112': 0.0, 'ACC-113': 0.0, 'ACC-114': 0.0, 'ACC-115': 0.0, 'ACC-116': 0.0, 'ACC-117': 0.0, 'ACC-118': 0.0, 'ACC-119': 0.0, 'ACC-120': 0.0, 'ACC-121': 0.0, 'ACC-122': 0.0, 'ACC-123': 0.0, 'ACC-124': 0.0, 'ACC-125': 0.0, 'ACC-126': 0.0, 'ACC-127': 0.0, 'ACC-128': 0.0, 'ACC-129': 0.0, 'ACC-130': 0.0, 'ACC-131': 0.0, 'ACC-132': 0.0, 'ACC-133': 0.0, 'ACC-134': 0.0, 'ACC-135': 0.0, 'ACC-136': 0.0, 'ACC-137': 0.0, 'ACC-138': 0.0, 'ACC-139': 0.0, 'ACC-140': 0.0, 'ACC-141': 0.0, 'ACC-142': 0.0, 'ACC-143': 0.0, 'ACC-144': 0.0, 'ACC-145': 0.0, 'ACC-146': 0.0, 'ACC-147': 0.0, 'ACC-148': 0.0, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.0, 'ACC-152': 0.0, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.0, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.0, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 18:28:52] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 18:28:52] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 18:28:52] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 18:28:52] d2.evaluation.testing INFO: copypaste: 25.3155,0.0069,0.0174,0.5236,1.3189
[01/17 18:28:52] d2.utils.events INFO:  eta: 1 day, 1:24:35  iter: 19  total_loss: 150.3  loss_ce: 9.962  loss_mask: 0.7194  loss_dice: 4.883  loss_ce_0: 9.274  loss_mask_0: 0.354  loss_dice_0: 4.887  loss_ce_1: 9.653  loss_mask_1: 0.4227  loss_dice_1: 4.894  loss_ce_2: 9.526  loss_mask_2: 0.4446  loss_dice_2: 4.916  loss_ce_3: 9.838  loss_mask_3: 0.4897  loss_dice_3: 4.916  loss_ce_4: 9.987  loss_mask_4: 0.5344  loss_dice_4: 4.91  loss_ce_5: 10.06  loss_mask_5: 0.6044  loss_dice_5: 4.905  loss_ce_6: 9.86  loss_mask_6: 0.4977  loss_dice_6: 4.907  loss_ce_7: 9.984  loss_mask_7: 0.5657  loss_dice_7: 4.906  loss_ce_8: 9.981  loss_mask_8: 0.6491  loss_dice_8: 4.906  time: 2.7959  data_time: 0.6103  lr: 9.9957e-06  max_mem: 20821M
[01/17 18:29:25] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 18:29:26] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 18:29:26] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 18:29:26] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 18:29:39] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0101 s/iter. Inference: 0.1972 s/iter. Eval: 0.0240 s/iter. Total: 0.2313 s/iter. ETA=0:04:10
[01/17 18:29:44] d2.evaluation.evaluator INFO: Inference done 31/1093. Dataloading: 0.0134 s/iter. Inference: 0.2115 s/iter. Eval: 0.0237 s/iter. Total: 0.2487 s/iter. ETA=0:04:24
[01/17 18:29:49] d2.evaluation.evaluator INFO: Inference done 52/1093. Dataloading: 0.0158 s/iter. Inference: 0.2058 s/iter. Eval: 0.0248 s/iter. Total: 0.2465 s/iter. ETA=0:04:16
[01/17 18:29:54] d2.evaluation.evaluator INFO: Inference done 74/1093. Dataloading: 0.0163 s/iter. Inference: 0.1987 s/iter. Eval: 0.0259 s/iter. Total: 0.2410 s/iter. ETA=0:04:05
[01/17 18:29:59] d2.evaluation.evaluator INFO: Inference done 95/1093. Dataloading: 0.0161 s/iter. Inference: 0.1988 s/iter. Eval: 0.0256 s/iter. Total: 0.2405 s/iter. ETA=0:04:00
[01/17 18:30:04] d2.evaluation.evaluator INFO: Inference done 118/1093. Dataloading: 0.0155 s/iter. Inference: 0.1966 s/iter. Eval: 0.0254 s/iter. Total: 0.2376 s/iter. ETA=0:03:51
[01/17 18:30:09] d2.evaluation.evaluator INFO: Inference done 141/1093. Dataloading: 0.0152 s/iter. Inference: 0.1942 s/iter. Eval: 0.0248 s/iter. Total: 0.2343 s/iter. ETA=0:03:43
[01/17 18:30:14] d2.evaluation.evaluator INFO: Inference done 162/1093. Dataloading: 0.0148 s/iter. Inference: 0.1954 s/iter. Eval: 0.0248 s/iter. Total: 0.2351 s/iter. ETA=0:03:38
[01/17 18:30:16] d2.engine.hooks INFO: Overall training speed: 37 iterations in 0:01:23 (2.2519 s / it)
[01/17 18:30:16] d2.engine.hooks INFO: Total training time: 0:06:45 (0:05:21 on hooks)
[01/17 18:30:16] d2.utils.events INFO:  eta: 20:42:03  iter: 39  total_loss: 132.8  loss_ce: 9.241  loss_mask: 0.5132  loss_dice: 4.434  loss_ce_0: 9.03  loss_mask_0: 0.2835  loss_dice_0: 4.456  loss_ce_1: 7.956  loss_mask_1: 0.2941  loss_dice_1: 4.429  loss_ce_2: 7.857  loss_mask_2: 0.2948  loss_dice_2: 4.426  loss_ce_3: 7.823  loss_mask_3: 0.2996  loss_dice_3: 4.428  loss_ce_4: 7.831  loss_mask_4: 0.3014  loss_dice_4: 4.426  loss_ce_5: 8.116  loss_mask_5: 0.2951  loss_dice_5: 4.419  loss_ce_6: 8.673  loss_mask_6: 0.3026  loss_dice_6: 4.418  loss_ce_7: 8.95  loss_mask_7: 0.3815  loss_dice_7: 4.41  loss_ce_8: 9.053  loss_mask_8: 0.4488  loss_dice_8: 4.439  time: 2.1926  data_time: 0.0953  lr: 9.9912e-06  max_mem: 20821M
[01/17 18:30:59] detectron2 INFO: Rank of current process: 0. World size: 4
[01/17 18:31:02] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/17 18:31:02] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:61200', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[01/17 18:31:02] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[01/17 18:31:02] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mROOT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/Datasets/sceneflow[39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_test[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141msceneflow_train[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmask_former_sceneflow[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m300[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerStereo[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m193[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcheckpoints/R-101.pkl[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./work_dirs/sceneflow_vanilla_disp192[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m40000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[01/17 18:31:03] detectron2 INFO: Full config saved to ./work_dirs/sceneflow_vanilla_disp192/config.yaml
[01/17 18:31:03] d2.utils.env INFO: Using a generated random seed 3183536
[01/17 18:31:04] d2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(300, 256)
      (query_embed): Embedding(300, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=194, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 193
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[01/17 18:31:05] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 18:31:08] d2.data.build INFO: Using training sampler TrainingSampler
[01/17 18:31:08] d2.data.common INFO: Serializing 22390 elements to byte tensors and concatenating them all ...
[01/17 18:31:08] d2.data.common INFO: Serialized dataset takes 7.73 MiB
[01/17 18:31:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[01/17 18:31:09] d2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[01/17 18:31:09] d2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[01/17 18:31:09] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[01/17 18:31:09] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[01/17 18:31:09] d2.engine.train_loop INFO: Starting training from iteration 0
[01/17 18:32:19] d2.utils.events INFO:  eta: 1 day, 0:03:25  iter: 19  total_loss: 148.2  loss_ce: 9.814  loss_mask: 0.54  loss_dice: 4.905  loss_ce_0: 9.183  loss_mask_0: 0.378  loss_dice_0: 4.878  loss_ce_1: 9.144  loss_mask_1: 0.3824  loss_dice_1: 4.883  loss_ce_2: 9.313  loss_mask_2: 0.3369  loss_dice_2: 4.893  loss_ce_3: 9.499  loss_mask_3: 0.3493  loss_dice_3: 4.906  loss_ce_4: 9.745  loss_mask_4: 0.4139  loss_dice_4: 4.908  loss_ce_5: 9.736  loss_mask_5: 0.4838  loss_dice_5: 4.906  loss_ce_6: 9.814  loss_mask_6: 0.5148  loss_dice_6: 4.91  loss_ce_7: 9.845  loss_mask_7: 0.5363  loss_dice_7: 4.905  loss_ce_8: 9.843  loss_mask_8: 0.5671  loss_dice_8: 4.904  time: 2.6628  data_time: 0.6137  lr: 9.9957e-06  max_mem: 20274M
[01/17 18:32:53] d2.utils.events INFO:  eta: 22:26:15  iter: 39  total_loss: 132  loss_ce: 9.185  loss_mask: 0.4386  loss_dice: 4.491  loss_ce_0: 9.02  loss_mask_0: 0.2948  loss_dice_0: 4.447  loss_ce_1: 8.084  loss_mask_1: 0.2896  loss_dice_1: 4.421  loss_ce_2: 7.938  loss_mask_2: 0.3008  loss_dice_2: 4.421  loss_ce_3: 7.931  loss_mask_3: 0.303  loss_dice_3: 4.432  loss_ce_4: 7.963  loss_mask_4: 0.3084  loss_dice_4: 4.441  loss_ce_5: 8.171  loss_mask_5: 0.304  loss_dice_5: 4.444  loss_ce_6: 8.479  loss_mask_6: 0.3185  loss_dice_6: 4.441  loss_ce_7: 8.852  loss_mask_7: 0.318  loss_dice_7: 4.462  loss_ce_8: 9.04  loss_mask_8: 0.3954  loss_dice_8: 4.459  time: 2.1561  data_time: 0.0880  lr: 9.9912e-06  max_mem: 20344M
[01/17 18:33:26] d2.utils.events INFO:  eta: 18:48:07  iter: 59  total_loss: 110.3  loss_ce: 6.188  loss_mask: 0.3175  loss_dice: 4.494  loss_ce_0: 8.901  loss_mask_0: 0.3092  loss_dice_0: 4.485  loss_ce_1: 7.501  loss_mask_1: 0.305  loss_dice_1: 4.472  loss_ce_2: 6.631  loss_mask_2: 0.3123  loss_dice_2: 4.476  loss_ce_3: 6.06  loss_mask_3: 0.3199  loss_dice_3: 4.496  loss_ce_4: 5.567  loss_mask_4: 0.3212  loss_dice_4: 4.513  loss_ce_5: 5.284  loss_mask_5: 0.3242  loss_dice_5: 4.51  loss_ce_6: 5.24  loss_mask_6: 0.3247  loss_dice_6: 4.514  loss_ce_7: 5.344  loss_mask_7: 0.3239  loss_dice_7: 4.525  loss_ce_8: 5.586  loss_mask_8: 0.3217  loss_dice_8: 4.509  time: 1.9791  data_time: 0.1024  lr: 9.9867e-06  max_mem: 21212M
[01/17 18:33:58] d2.utils.events INFO:  eta: 18:19:40  iter: 79  total_loss: 91.86  loss_ce: 2.895  loss_mask: 0.3696  loss_dice: 4.532  loss_ce_0: 8.784  loss_mask_0: 0.3037  loss_dice_0: 4.472  loss_ce_1: 6.675  loss_mask_1: 0.3162  loss_dice_1: 4.479  loss_ce_2: 5.25  loss_mask_2: 0.3303  loss_dice_2: 4.49  loss_ce_3: 4.185  loss_mask_3: 0.3416  loss_dice_3: 4.519  loss_ce_4: 3.575  loss_mask_4: 0.3522  loss_dice_4: 4.526  loss_ce_5: 3.156  loss_mask_5: 0.361  loss_dice_5: 4.533  loss_ce_6: 2.952  loss_mask_6: 0.365  loss_dice_6: 4.536  loss_ce_7: 2.855  loss_mask_7: 0.3637  loss_dice_7: 4.548  loss_ce_8: 2.795  loss_mask_8: 0.3672  loss_dice_8: 4.538  time: 1.8815  data_time: 0.1017  lr: 9.9822e-06  max_mem: 21212M
[01/17 18:34:29] d2.utils.events INFO:  eta: 18:05:50  iter: 99  total_loss: 80.68  loss_ce: 1.735  loss_mask: 0.4004  loss_dice: 4.598  loss_ce_0: 8.7  loss_mask_0: 0.333  loss_dice_0: 4.503  loss_ce_1: 5.673  loss_mask_1: 0.3447  loss_dice_1: 4.535  loss_ce_2: 3.816  loss_mask_2: 0.3577  loss_dice_2: 4.548  loss_ce_3: 2.624  loss_mask_3: 0.3644  loss_dice_3: 4.586  loss_ce_4: 2.091  loss_mask_4: 0.3742  loss_dice_4: 4.583  loss_ce_5: 1.899  loss_mask_5: 0.3768  loss_dice_5: 4.598  loss_ce_6: 1.8  loss_mask_6: 0.3816  loss_dice_6: 4.586  loss_ce_7: 1.747  loss_mask_7: 0.38  loss_dice_7: 4.594  loss_ce_8: 1.69  loss_mask_8: 0.3908  loss_dice_8: 4.596  time: 1.8148  data_time: 0.0810  lr: 9.9777e-06  max_mem: 21212M
[01/17 18:35:01] d2.utils.events INFO:  eta: 17:56:55  iter: 119  total_loss: 74.53  loss_ce: 1.277  loss_mask: 0.3883  loss_dice: 4.612  loss_ce_0: 8.52  loss_mask_0: 0.3473  loss_dice_0: 4.513  loss_ce_1: 4.517  loss_mask_1: 0.3654  loss_dice_1: 4.55  loss_ce_2: 2.521  loss_mask_2: 0.3687  loss_dice_2: 4.588  loss_ce_3: 1.766  loss_mask_3: 0.3787  loss_dice_3: 4.607  loss_ce_4: 1.493  loss_mask_4: 0.3788  loss_dice_4: 4.605  loss_ce_5: 1.322  loss_mask_5: 0.3817  loss_dice_5: 4.621  loss_ce_6: 1.253  loss_mask_6: 0.3881  loss_dice_6: 4.618  loss_ce_7: 1.24  loss_mask_7: 0.3887  loss_dice_7: 4.616  loss_ce_8: 1.255  loss_mask_8: 0.3924  loss_dice_8: 4.62  time: 1.7721  data_time: 0.0926  lr: 9.9732e-06  max_mem: 21212M
[01/17 18:35:32] d2.utils.events INFO:  eta: 17:50:18  iter: 139  total_loss: 72.62  loss_ce: 1.254  loss_mask: 0.4144  loss_dice: 4.627  loss_ce_0: 8.274  loss_mask_0: 0.3539  loss_dice_0: 4.505  loss_ce_1: 3.587  loss_mask_1: 0.3685  loss_dice_1: 4.566  loss_ce_2: 2.038  loss_mask_2: 0.3893  loss_dice_2: 4.591  loss_ce_3: 1.575  loss_mask_3: 0.3915  loss_dice_3: 4.616  loss_ce_4: 1.4  loss_mask_4: 0.399  loss_dice_4: 4.611  loss_ce_5: 1.297  loss_mask_5: 0.4012  loss_dice_5: 4.614  loss_ce_6: 1.321  loss_mask_6: 0.3963  loss_dice_6: 4.62  loss_ce_7: 1.264  loss_mask_7: 0.4028  loss_dice_7: 4.619  loss_ce_8: 1.281  loss_mask_8: 0.4032  loss_dice_8: 4.624  time: 1.7436  data_time: 0.1017  lr: 9.9687e-06  max_mem: 21212M
[01/17 18:36:04] d2.utils.events INFO:  eta: 17:46:04  iter: 159  total_loss: 68.99  loss_ce: 0.9345  loss_mask: 0.4076  loss_dice: 4.643  loss_ce_0: 7.943  loss_mask_0: 0.3686  loss_dice_0: 4.536  loss_ce_1: 2.69  loss_mask_1: 0.3845  loss_dice_1: 4.6  loss_ce_2: 1.4  loss_mask_2: 0.3937  loss_dice_2: 4.636  loss_ce_3: 1.103  loss_mask_3: 0.4055  loss_dice_3: 4.635  loss_ce_4: 0.97  loss_mask_4: 0.4104  loss_dice_4: 4.641  loss_ce_5: 0.9169  loss_mask_5: 0.4152  loss_dice_5: 4.643  loss_ce_6: 0.9059  loss_mask_6: 0.4198  loss_dice_6: 4.642  loss_ce_7: 0.8876  loss_mask_7: 0.421  loss_dice_7: 4.637  loss_ce_8: 0.8877  loss_mask_8: 0.4148  loss_dice_8: 4.649  time: 1.7205  data_time: 0.0898  lr: 9.9642e-06  max_mem: 21212M
[01/17 18:36:35] d2.utils.events INFO:  eta: 17:40:49  iter: 179  total_loss: 68.48  loss_ce: 1.063  loss_mask: 0.4272  loss_dice: 4.656  loss_ce_0: 7.562  loss_mask_0: 0.3774  loss_dice_0: 4.551  loss_ce_1: 2.067  loss_mask_1: 0.4073  loss_dice_1: 4.633  loss_ce_2: 1.272  loss_mask_2: 0.4274  loss_dice_2: 4.645  loss_ce_3: 1.096  loss_mask_3: 0.435  loss_dice_3: 4.658  loss_ce_4: 1.035  loss_mask_4: 0.4335  loss_dice_4: 4.657  loss_ce_5: 0.9963  loss_mask_5: 0.4356  loss_dice_5: 4.658  loss_ce_6: 0.9424  loss_mask_6: 0.4316  loss_dice_6: 4.661  loss_ce_7: 0.9616  loss_mask_7: 0.4302  loss_dice_7: 4.656  loss_ce_8: 0.9878  loss_mask_8: 0.4273  loss_dice_8: 4.663  time: 1.7037  data_time: 0.0909  lr: 9.9597e-06  max_mem: 21212M
[01/17 18:37:07] d2.utils.events INFO:  eta: 17:37:32  iter: 199  total_loss: 66.2  loss_ce: 0.818  loss_mask: 0.433  loss_dice: 4.656  loss_ce_0: 7.219  loss_mask_0: 0.3837  loss_dice_0: 4.539  loss_ce_1: 1.733  loss_mask_1: 0.416  loss_dice_1: 4.628  loss_ce_2: 1.044  loss_mask_2: 0.4295  loss_dice_2: 4.639  loss_ce_3: 0.8641  loss_mask_3: 0.438  loss_dice_3: 4.638  loss_ce_4: 0.7828  loss_mask_4: 0.4354  loss_dice_4: 4.647  loss_ce_5: 0.7754  loss_mask_5: 0.4374  loss_dice_5: 4.648  loss_ce_6: 0.7792  loss_mask_6: 0.4412  loss_dice_6: 4.647  loss_ce_7: 0.7544  loss_mask_7: 0.4409  loss_dice_7: 4.64  loss_ce_8: 0.7892  loss_mask_8: 0.4393  loss_dice_8: 4.649  time: 1.6921  data_time: 0.0849  lr: 9.9552e-06  max_mem: 21212M
[01/17 18:37:38] d2.utils.events INFO:  eta: 17:35:27  iter: 219  total_loss: 64.91  loss_ce: 0.8234  loss_mask: 0.4372  loss_dice: 4.631  loss_ce_0: 6.778  loss_mask_0: 0.3851  loss_dice_0: 4.497  loss_ce_1: 1.466  loss_mask_1: 0.4242  loss_dice_1: 4.601  loss_ce_2: 0.9562  loss_mask_2: 0.4412  loss_dice_2: 4.601  loss_ce_3: 0.8443  loss_mask_3: 0.4458  loss_dice_3: 4.614  loss_ce_4: 0.7528  loss_mask_4: 0.4473  loss_dice_4: 4.619  loss_ce_5: 0.7325  loss_mask_5: 0.4513  loss_dice_5: 4.615  loss_ce_6: 0.7305  loss_mask_6: 0.4484  loss_dice_6: 4.62  loss_ce_7: 0.708  loss_mask_7: 0.4473  loss_dice_7: 4.616  loss_ce_8: 0.7538  loss_mask_8: 0.4411  loss_dice_8: 4.622  time: 1.6813  data_time: 0.0935  lr: 9.9507e-06  max_mem: 21212M
[01/17 18:38:11] d2.utils.events INFO:  eta: 17:35:07  iter: 239  total_loss: 64.34  loss_ce: 0.7352  loss_mask: 0.4522  loss_dice: 4.654  loss_ce_0: 6.435  loss_mask_0: 0.4021  loss_dice_0: 4.539  loss_ce_1: 1.256  loss_mask_1: 0.4307  loss_dice_1: 4.632  loss_ce_2: 0.8537  loss_mask_2: 0.4477  loss_dice_2: 4.648  loss_ce_3: 0.748  loss_mask_3: 0.4483  loss_dice_3: 4.636  loss_ce_4: 0.7255  loss_mask_4: 0.4482  loss_dice_4: 4.649  loss_ce_5: 0.6909  loss_mask_5: 0.4535  loss_dice_5: 4.647  loss_ce_6: 0.6923  loss_mask_6: 0.4553  loss_dice_6: 4.654  loss_ce_7: 0.7036  loss_mask_7: 0.4576  loss_dice_7: 4.645  loss_ce_8: 0.713  loss_mask_8: 0.4591  loss_dice_8: 4.644  time: 1.6747  data_time: 0.0965  lr: 9.9462e-06  max_mem: 21212M
[01/17 18:38:42] d2.utils.events INFO:  eta: 17:34:44  iter: 259  total_loss: 63.7  loss_ce: 0.7368  loss_mask: 0.4518  loss_dice: 4.653  loss_ce_0: 5.826  loss_mask_0: 0.3953  loss_dice_0: 4.55  loss_ce_1: 1.089  loss_mask_1: 0.4384  loss_dice_1: 4.642  loss_ce_2: 0.8157  loss_mask_2: 0.4499  loss_dice_2: 4.646  loss_ce_3: 0.7355  loss_mask_3: 0.4551  loss_dice_3: 4.648  loss_ce_4: 0.7282  loss_mask_4: 0.4547  loss_dice_4: 4.652  loss_ce_5: 0.7181  loss_mask_5: 0.4576  loss_dice_5: 4.656  loss_ce_6: 0.6987  loss_mask_6: 0.4546  loss_dice_6: 4.66  loss_ce_7: 0.6997  loss_mask_7: 0.4483  loss_dice_7: 4.655  loss_ce_8: 0.676  loss_mask_8: 0.4553  loss_dice_8: 4.665  time: 1.6679  data_time: 0.0970  lr: 9.9417e-06  max_mem: 21212M
[01/17 18:39:14] d2.utils.events INFO:  eta: 17:33:53  iter: 279  total_loss: 62.61  loss_ce: 0.6771  loss_mask: 0.4554  loss_dice: 4.634  loss_ce_0: 5.409  loss_mask_0: 0.396  loss_dice_0: 4.531  loss_ce_1: 1.09  loss_mask_1: 0.4441  loss_dice_1: 4.626  loss_ce_2: 0.8209  loss_mask_2: 0.4596  loss_dice_2: 4.626  loss_ce_3: 0.7308  loss_mask_3: 0.4658  loss_dice_3: 4.628  loss_ce_4: 0.702  loss_mask_4: 0.4608  loss_dice_4: 4.629  loss_ce_5: 0.6625  loss_mask_5: 0.4639  loss_dice_5: 4.623  loss_ce_6: 0.6464  loss_mask_6: 0.4628  loss_dice_6: 4.63  loss_ce_7: 0.6585  loss_mask_7: 0.4609  loss_dice_7: 4.627  loss_ce_8: 0.6674  loss_mask_8: 0.4584  loss_dice_8: 4.64  time: 1.6622  data_time: 0.0873  lr: 9.9372e-06  max_mem: 21212M
[01/17 18:39:46] d2.utils.events INFO:  eta: 17:33:31  iter: 299  total_loss: 62.43  loss_ce: 0.6808  loss_mask: 0.4544  loss_dice: 4.631  loss_ce_0: 5.02  loss_mask_0: 0.4044  loss_dice_0: 4.536  loss_ce_1: 1.015  loss_mask_1: 0.4478  loss_dice_1: 4.622  loss_ce_2: 0.8078  loss_mask_2: 0.4515  loss_dice_2: 4.63  loss_ce_3: 0.7389  loss_mask_3: 0.4539  loss_dice_3: 4.631  loss_ce_4: 0.7393  loss_mask_4: 0.4523  loss_dice_4: 4.634  loss_ce_5: 0.7002  loss_mask_5: 0.4588  loss_dice_5: 4.623  loss_ce_6: 0.6984  loss_mask_6: 0.454  loss_dice_6: 4.633  loss_ce_7: 0.6786  loss_mask_7: 0.4528  loss_dice_7: 4.639  loss_ce_8: 0.7021  loss_mask_8: 0.4555  loss_dice_8: 4.637  time: 1.6578  data_time: 0.1027  lr: 9.9327e-06  max_mem: 21212M
[01/17 18:40:17] d2.utils.events INFO:  eta: 17:32:08  iter: 319  total_loss: 61.95  loss_ce: 0.6384  loss_mask: 0.4677  loss_dice: 4.664  loss_ce_0: 4.634  loss_mask_0: 0.4313  loss_dice_0: 4.577  loss_ce_1: 0.8667  loss_mask_1: 0.475  loss_dice_1: 4.658  loss_ce_2: 0.6977  loss_mask_2: 0.4783  loss_dice_2: 4.663  loss_ce_3: 0.6622  loss_mask_3: 0.4788  loss_dice_3: 4.666  loss_ce_4: 0.6922  loss_mask_4: 0.4786  loss_dice_4: 4.674  loss_ce_5: 0.662  loss_mask_5: 0.4762  loss_dice_5: 4.669  loss_ce_6: 0.6658  loss_mask_6: 0.476  loss_dice_6: 4.654  loss_ce_7: 0.6331  loss_mask_7: 0.473  loss_dice_7: 4.664  loss_ce_8: 0.6633  loss_mask_8: 0.4671  loss_dice_8: 4.661  time: 1.6514  data_time: 0.0958  lr: 9.9282e-06  max_mem: 21212M
[01/17 18:40:49] d2.utils.events INFO:  eta: 17:30:45  iter: 339  total_loss: 61.74  loss_ce: 0.6958  loss_mask: 0.45  loss_dice: 4.619  loss_ce_0: 4.23  loss_mask_0: 0.4167  loss_dice_0: 4.55  loss_ce_1: 0.9289  loss_mask_1: 0.4611  loss_dice_1: 4.607  loss_ce_2: 0.8183  loss_mask_2: 0.4626  loss_dice_2: 4.629  loss_ce_3: 0.7419  loss_mask_3: 0.4654  loss_dice_3: 4.624  loss_ce_4: 0.7298  loss_mask_4: 0.4574  loss_dice_4: 4.626  loss_ce_5: 0.6752  loss_mask_5: 0.4577  loss_dice_5: 4.627  loss_ce_6: 0.6919  loss_mask_6: 0.4517  loss_dice_6: 4.629  loss_ce_7: 0.6661  loss_mask_7: 0.454  loss_dice_7: 4.624  loss_ce_8: 0.6656  loss_mask_8: 0.4503  loss_dice_8: 4.619  time: 1.6466  data_time: 0.0866  lr: 9.9237e-06  max_mem: 21212M
[01/17 18:41:21] d2.utils.events INFO:  eta: 17:30:13  iter: 359  total_loss: 61.1  loss_ce: 0.6258  loss_mask: 0.4466  loss_dice: 4.668  loss_ce_0: 3.979  loss_mask_0: 0.4212  loss_dice_0: 4.586  loss_ce_1: 0.8548  loss_mask_1: 0.46  loss_dice_1: 4.647  loss_ce_2: 0.7368  loss_mask_2: 0.4575  loss_dice_2: 4.654  loss_ce_3: 0.6871  loss_mask_3: 0.4548  loss_dice_3: 4.657  loss_ce_4: 0.6611  loss_mask_4: 0.4498  loss_dice_4: 4.656  loss_ce_5: 0.6444  loss_mask_5: 0.4511  loss_dice_5: 4.66  loss_ce_6: 0.6619  loss_mask_6: 0.4476  loss_dice_6: 4.653  loss_ce_7: 0.6319  loss_mask_7: 0.4487  loss_dice_7: 4.668  loss_ce_8: 0.62  loss_mask_8: 0.4476  loss_dice_8: 4.67  time: 1.6429  data_time: 0.0889  lr: 9.9192e-06  max_mem: 21212M
[01/17 18:41:52] d2.utils.events INFO:  eta: 17:29:21  iter: 379  total_loss: 60.59  loss_ce: 0.6231  loss_mask: 0.4687  loss_dice: 4.659  loss_ce_0: 3.693  loss_mask_0: 0.4257  loss_dice_0: 4.574  loss_ce_1: 0.7679  loss_mask_1: 0.4714  loss_dice_1: 4.645  loss_ce_2: 0.6946  loss_mask_2: 0.4655  loss_dice_2: 4.653  loss_ce_3: 0.6555  loss_mask_3: 0.465  loss_dice_3: 4.652  loss_ce_4: 0.6465  loss_mask_4: 0.4645  loss_dice_4: 4.656  loss_ce_5: 0.6185  loss_mask_5: 0.4689  loss_dice_5: 4.654  loss_ce_6: 0.6301  loss_mask_6: 0.4654  loss_dice_6: 4.662  loss_ce_7: 0.6213  loss_mask_7: 0.4707  loss_dice_7: 4.657  loss_ce_8: 0.6321  loss_mask_8: 0.4659  loss_dice_8: 4.663  time: 1.6403  data_time: 0.1043  lr: 9.9147e-06  max_mem: 21212M
[01/17 18:42:25] d2.utils.events INFO:  eta: 17:29:09  iter: 399  total_loss: 60.31  loss_ce: 0.6061  loss_mask: 0.4589  loss_dice: 4.659  loss_ce_0: 3.532  loss_mask_0: 0.4351  loss_dice_0: 4.591  loss_ce_1: 0.7619  loss_mask_1: 0.468  loss_dice_1: 4.663  loss_ce_2: 0.6676  loss_mask_2: 0.4651  loss_dice_2: 4.66  loss_ce_3: 0.6363  loss_mask_3: 0.4601  loss_dice_3: 4.664  loss_ce_4: 0.6126  loss_mask_4: 0.4598  loss_dice_4: 4.668  loss_ce_5: 0.6108  loss_mask_5: 0.4606  loss_dice_5: 4.659  loss_ce_6: 0.5933  loss_mask_6: 0.4596  loss_dice_6: 4.655  loss_ce_7: 0.5959  loss_mask_7: 0.4598  loss_dice_7: 4.662  loss_ce_8: 0.5848  loss_mask_8: 0.4646  loss_dice_8: 4.662  time: 1.6389  data_time: 0.1100  lr: 9.9102e-06  max_mem: 21212M
[01/17 18:42:56] d2.utils.events INFO:  eta: 17:27:51  iter: 419  total_loss: 59.96  loss_ce: 0.6067  loss_mask: 0.4573  loss_dice: 4.646  loss_ce_0: 3.303  loss_mask_0: 0.4248  loss_dice_0: 4.582  loss_ce_1: 0.7745  loss_mask_1: 0.4516  loss_dice_1: 4.633  loss_ce_2: 0.6875  loss_mask_2: 0.4492  loss_dice_2: 4.642  loss_ce_3: 0.6348  loss_mask_3: 0.4506  loss_dice_3: 4.629  loss_ce_4: 0.6037  loss_mask_4: 0.4548  loss_dice_4: 4.633  loss_ce_5: 0.6124  loss_mask_5: 0.4517  loss_dice_5: 4.635  loss_ce_6: 0.5958  loss_mask_6: 0.4485  loss_dice_6: 4.638  loss_ce_7: 0.6097  loss_mask_7: 0.4557  loss_dice_7: 4.648  loss_ce_8: 0.6269  loss_mask_8: 0.4608  loss_dice_8: 4.642  time: 1.6339  data_time: 0.0935  lr: 9.9057e-06  max_mem: 21212M
[01/17 18:43:27] d2.utils.events INFO:  eta: 17:26:57  iter: 439  total_loss: 59.34  loss_ce: 0.5492  loss_mask: 0.464  loss_dice: 4.655  loss_ce_0: 3.104  loss_mask_0: 0.4428  loss_dice_0: 4.613  loss_ce_1: 0.6912  loss_mask_1: 0.4663  loss_dice_1: 4.662  loss_ce_2: 0.6256  loss_mask_2: 0.4649  loss_dice_2: 4.66  loss_ce_3: 0.5772  loss_mask_3: 0.4659  loss_dice_3: 4.661  loss_ce_4: 0.5658  loss_mask_4: 0.4669  loss_dice_4: 4.659  loss_ce_5: 0.5468  loss_mask_5: 0.4716  loss_dice_5: 4.656  loss_ce_6: 0.5398  loss_mask_6: 0.4676  loss_dice_6: 4.654  loss_ce_7: 0.5337  loss_mask_7: 0.4671  loss_dice_7: 4.659  loss_ce_8: 0.5327  loss_mask_8: 0.4671  loss_dice_8: 4.659  time: 1.6308  data_time: 0.0906  lr: 9.9012e-06  max_mem: 21212M
[01/17 18:43:58] d2.utils.events INFO:  eta: 17:25:39  iter: 459  total_loss: 59.39  loss_ce: 0.5446  loss_mask: 0.4738  loss_dice: 4.673  loss_ce_0: 2.945  loss_mask_0: 0.447  loss_dice_0: 4.61  loss_ce_1: 0.673  loss_mask_1: 0.4711  loss_dice_1: 4.666  loss_ce_2: 0.622  loss_mask_2: 0.4675  loss_dice_2: 4.665  loss_ce_3: 0.5812  loss_mask_3: 0.4693  loss_dice_3: 4.664  loss_ce_4: 0.5769  loss_mask_4: 0.4732  loss_dice_4: 4.663  loss_ce_5: 0.5422  loss_mask_5: 0.4768  loss_dice_5: 4.655  loss_ce_6: 0.5468  loss_mask_6: 0.4716  loss_dice_6: 4.654  loss_ce_7: 0.5292  loss_mask_7: 0.4775  loss_dice_7: 4.66  loss_ce_8: 0.5403  loss_mask_8: 0.473  loss_dice_8: 4.661  time: 1.6267  data_time: 0.1014  lr: 9.8967e-06  max_mem: 21212M
[01/17 18:44:29] d2.utils.events INFO:  eta: 17:23:42  iter: 479  total_loss: 59.08  loss_ce: 0.5522  loss_mask: 0.4797  loss_dice: 4.655  loss_ce_0: 2.858  loss_mask_0: 0.4514  loss_dice_0: 4.611  loss_ce_1: 0.702  loss_mask_1: 0.4697  loss_dice_1: 4.658  loss_ce_2: 0.6217  loss_mask_2: 0.4698  loss_dice_2: 4.655  loss_ce_3: 0.5734  loss_mask_3: 0.4703  loss_dice_3: 4.658  loss_ce_4: 0.5508  loss_mask_4: 0.4684  loss_dice_4: 4.649  loss_ce_5: 0.5375  loss_mask_5: 0.4723  loss_dice_5: 4.649  loss_ce_6: 0.5461  loss_mask_6: 0.4736  loss_dice_6: 4.654  loss_ce_7: 0.5303  loss_mask_7: 0.4752  loss_dice_7: 4.646  loss_ce_8: 0.5186  loss_mask_8: 0.4793  loss_dice_8: 4.647  time: 1.6236  data_time: 0.0835  lr: 9.8922e-06  max_mem: 21212M
[01/17 18:45:00] d2.utils.events INFO:  eta: 17:22:07  iter: 499  total_loss: 59.28  loss_ce: 0.5832  loss_mask: 0.4707  loss_dice: 4.629  loss_ce_0: 2.774  loss_mask_0: 0.4565  loss_dice_0: 4.608  loss_ce_1: 0.7537  loss_mask_1: 0.4732  loss_dice_1: 4.641  loss_ce_2: 0.6471  loss_mask_2: 0.4726  loss_dice_2: 4.635  loss_ce_3: 0.6269  loss_mask_3: 0.4745  loss_dice_3: 4.638  loss_ce_4: 0.5954  loss_mask_4: 0.4736  loss_dice_4: 4.631  loss_ce_5: 0.5688  loss_mask_5: 0.4733  loss_dice_5: 4.636  loss_ce_6: 0.5807  loss_mask_6: 0.4747  loss_dice_6: 4.637  loss_ce_7: 0.559  loss_mask_7: 0.4757  loss_dice_7: 4.628  loss_ce_8: 0.5734  loss_mask_8: 0.4762  loss_dice_8: 4.637  time: 1.6206  data_time: 0.0869  lr: 9.8877e-06  max_mem: 21212M
[01/17 18:45:31] d2.utils.events INFO:  eta: 17:20:21  iter: 519  total_loss: 58.99  loss_ce: 0.5272  loss_mask: 0.4776  loss_dice: 4.643  loss_ce_0: 2.62  loss_mask_0: 0.4551  loss_dice_0: 4.616  loss_ce_1: 0.7129  loss_mask_1: 0.4718  loss_dice_1: 4.643  loss_ce_2: 0.6222  loss_mask_2: 0.4714  loss_dice_2: 4.646  loss_ce_3: 0.5972  loss_mask_3: 0.4717  loss_dice_3: 4.642  loss_ce_4: 0.5722  loss_mask_4: 0.4693  loss_dice_4: 4.637  loss_ce_5: 0.5554  loss_mask_5: 0.4736  loss_dice_5: 4.634  loss_ce_6: 0.5342  loss_mask_6: 0.4771  loss_dice_6: 4.639  loss_ce_7: 0.5217  loss_mask_7: 0.4777  loss_dice_7: 4.642  loss_ce_8: 0.5215  loss_mask_8: 0.4755  loss_dice_8: 4.626  time: 1.6175  data_time: 0.0777  lr: 9.8831e-06  max_mem: 21212M
[01/17 18:46:02] d2.utils.events INFO:  eta: 17:18:51  iter: 539  total_loss: 58.86  loss_ce: 0.5638  loss_mask: 0.4837  loss_dice: 4.632  loss_ce_0: 2.609  loss_mask_0: 0.4688  loss_dice_0: 4.609  loss_ce_1: 0.6487  loss_mask_1: 0.479  loss_dice_1: 4.642  loss_ce_2: 0.5768  loss_mask_2: 0.4832  loss_dice_2: 4.635  loss_ce_3: 0.5616  loss_mask_3: 0.4829  loss_dice_3: 4.63  loss_ce_4: 0.5638  loss_mask_4: 0.484  loss_dice_4: 4.626  loss_ce_5: 0.575  loss_mask_5: 0.4833  loss_dice_5: 4.634  loss_ce_6: 0.5544  loss_mask_6: 0.4818  loss_dice_6: 4.635  loss_ce_7: 0.5502  loss_mask_7: 0.4838  loss_dice_7: 4.632  loss_ce_8: 0.5359  loss_mask_8: 0.4854  loss_dice_8: 4.639  time: 1.6156  data_time: 0.0988  lr: 9.8786e-06  max_mem: 21212M
[01/17 18:46:33] d2.utils.events INFO:  eta: 17:15:54  iter: 559  total_loss: 58.31  loss_ce: 0.509  loss_mask: 0.4771  loss_dice: 4.626  loss_ce_0: 2.45  loss_mask_0: 0.4611  loss_dice_0: 4.609  loss_ce_1: 0.6553  loss_mask_1: 0.481  loss_dice_1: 4.635  loss_ce_2: 0.567  loss_mask_2: 0.4817  loss_dice_2: 4.631  loss_ce_3: 0.5316  loss_mask_3: 0.4819  loss_dice_3: 4.619  loss_ce_4: 0.5215  loss_mask_4: 0.4829  loss_dice_4: 4.617  loss_ce_5: 0.5174  loss_mask_5: 0.4833  loss_dice_5: 4.614  loss_ce_6: 0.5095  loss_mask_6: 0.4808  loss_dice_6: 4.621  loss_ce_7: 0.4969  loss_mask_7: 0.4779  loss_dice_7: 4.639  loss_ce_8: 0.4995  loss_mask_8: 0.4803  loss_dice_8: 4.617  time: 1.6130  data_time: 0.0897  lr: 9.8741e-06  max_mem: 21212M
[01/17 18:47:04] d2.utils.events INFO:  eta: 17:14:51  iter: 579  total_loss: 58.24  loss_ce: 0.5273  loss_mask: 0.4805  loss_dice: 4.63  loss_ce_0: 2.399  loss_mask_0: 0.4671  loss_dice_0: 4.617  loss_ce_1: 0.6626  loss_mask_1: 0.4818  loss_dice_1: 4.636  loss_ce_2: 0.5721  loss_mask_2: 0.479  loss_dice_2: 4.641  loss_ce_3: 0.5461  loss_mask_3: 0.4828  loss_dice_3: 4.636  loss_ce_4: 0.5517  loss_mask_4: 0.4807  loss_dice_4: 4.63  loss_ce_5: 0.5528  loss_mask_5: 0.4817  loss_dice_5: 4.633  loss_ce_6: 0.5364  loss_mask_6: 0.4791  loss_dice_6: 4.633  loss_ce_7: 0.5199  loss_mask_7: 0.4776  loss_dice_7: 4.632  loss_ce_8: 0.515  loss_mask_8: 0.4775  loss_dice_8: 4.631  time: 1.6109  data_time: 0.0948  lr: 9.8696e-06  max_mem: 21212M
[01/17 18:47:35] d2.utils.events INFO:  eta: 17:13:39  iter: 599  total_loss: 58.54  loss_ce: 0.5217  loss_mask: 0.484  loss_dice: 4.654  loss_ce_0: 2.324  loss_mask_0: 0.473  loss_dice_0: 4.627  loss_ce_1: 0.6379  loss_mask_1: 0.4899  loss_dice_1: 4.654  loss_ce_2: 0.5542  loss_mask_2: 0.4879  loss_dice_2: 4.641  loss_ce_3: 0.5317  loss_mask_3: 0.4879  loss_dice_3: 4.652  loss_ce_4: 0.5276  loss_mask_4: 0.4893  loss_dice_4: 4.647  loss_ce_5: 0.5057  loss_mask_5: 0.4891  loss_dice_5: 4.639  loss_ce_6: 0.5135  loss_mask_6: 0.489  loss_dice_6: 4.647  loss_ce_7: 0.5118  loss_mask_7: 0.4892  loss_dice_7: 4.65  loss_ce_8: 0.5253  loss_mask_8: 0.4885  loss_dice_8: 4.647  time: 1.6088  data_time: 0.0908  lr: 9.8651e-06  max_mem: 21212M
[01/17 18:48:06] d2.utils.events INFO:  eta: 17:12:49  iter: 619  total_loss: 58.44  loss_ce: 0.5391  loss_mask: 0.5  loss_dice: 4.63  loss_ce_0: 2.355  loss_mask_0: 0.4883  loss_dice_0: 4.628  loss_ce_1: 0.6152  loss_mask_1: 0.4989  loss_dice_1: 4.646  loss_ce_2: 0.5544  loss_mask_2: 0.4965  loss_dice_2: 4.641  loss_ce_3: 0.5295  loss_mask_3: 0.4943  loss_dice_3: 4.63  loss_ce_4: 0.5341  loss_mask_4: 0.4958  loss_dice_4: 4.625  loss_ce_5: 0.5351  loss_mask_5: 0.4995  loss_dice_5: 4.628  loss_ce_6: 0.5396  loss_mask_6: 0.4989  loss_dice_6: 4.626  loss_ce_7: 0.5207  loss_mask_7: 0.4989  loss_dice_7: 4.628  loss_ce_8: 0.5182  loss_mask_8: 0.4993  loss_dice_8: 4.634  time: 1.6070  data_time: 0.0970  lr: 9.8606e-06  max_mem: 21212M
[01/17 18:48:37] d2.utils.events INFO:  eta: 17:11:20  iter: 639  total_loss: 58.39  loss_ce: 0.544  loss_mask: 0.4879  loss_dice: 4.623  loss_ce_0: 2.299  loss_mask_0: 0.4764  loss_dice_0: 4.62  loss_ce_1: 0.6349  loss_mask_1: 0.49  loss_dice_1: 4.639  loss_ce_2: 0.5807  loss_mask_2: 0.4888  loss_dice_2: 4.627  loss_ce_3: 0.5637  loss_mask_3: 0.4905  loss_dice_3: 4.624  loss_ce_4: 0.5569  loss_mask_4: 0.4884  loss_dice_4: 4.626  loss_ce_5: 0.5419  loss_mask_5: 0.4879  loss_dice_5: 4.618  loss_ce_6: 0.5423  loss_mask_6: 0.4878  loss_dice_6: 4.622  loss_ce_7: 0.5329  loss_mask_7: 0.4865  loss_dice_7: 4.635  loss_ce_8: 0.5338  loss_mask_8: 0.4861  loss_dice_8: 4.63  time: 1.6053  data_time: 0.0933  lr: 9.8561e-06  max_mem: 21212M
[01/17 18:49:08] d2.utils.events INFO:  eta: 17:08:36  iter: 659  total_loss: 58.1  loss_ce: 0.4923  loss_mask: 0.5111  loss_dice: 4.619  loss_ce_0: 2.259  loss_mask_0: 0.4963  loss_dice_0: 4.609  loss_ce_1: 0.5472  loss_mask_1: 0.5072  loss_dice_1: 4.627  loss_ce_2: 0.4962  loss_mask_2: 0.5093  loss_dice_2: 4.621  loss_ce_3: 0.4863  loss_mask_3: 0.5103  loss_dice_3: 4.621  loss_ce_4: 0.4881  loss_mask_4: 0.5097  loss_dice_4: 4.618  loss_ce_5: 0.491  loss_mask_5: 0.5104  loss_dice_5: 4.619  loss_ce_6: 0.4891  loss_mask_6: 0.515  loss_dice_6: 4.62  loss_ce_7: 0.4776  loss_mask_7: 0.5144  loss_dice_7: 4.619  loss_ce_8: 0.4811  loss_mask_8: 0.5159  loss_dice_8: 4.62  time: 1.6025  data_time: 0.0897  lr: 9.8516e-06  max_mem: 21212M
[01/17 18:49:39] d2.utils.events INFO:  eta: 17:07:39  iter: 679  total_loss: 57.82  loss_ce: 0.4839  loss_mask: 0.4817  loss_dice: 4.625  loss_ce_0: 2.173  loss_mask_0: 0.4811  loss_dice_0: 4.61  loss_ce_1: 0.5915  loss_mask_1: 0.4913  loss_dice_1: 4.638  loss_ce_2: 0.5108  loss_mask_2: 0.4869  loss_dice_2: 4.637  loss_ce_3: 0.502  loss_mask_3: 0.4894  loss_dice_3: 4.616  loss_ce_4: 0.4962  loss_mask_4: 0.4869  loss_dice_4: 4.622  loss_ce_5: 0.4984  loss_mask_5: 0.4883  loss_dice_5: 4.619  loss_ce_6: 0.4889  loss_mask_6: 0.4892  loss_dice_6: 4.62  loss_ce_7: 0.4663  loss_mask_7: 0.4876  loss_dice_7: 4.626  loss_ce_8: 0.4667  loss_mask_8: 0.487  loss_dice_8: 4.619  time: 1.6014  data_time: 0.0914  lr: 9.8471e-06  max_mem: 21212M
[01/17 18:50:10] d2.utils.events INFO:  eta: 17:06:15  iter: 699  total_loss: 57.81  loss_ce: 0.4715  loss_mask: 0.4965  loss_dice: 4.613  loss_ce_0: 2.154  loss_mask_0: 0.4974  loss_dice_0: 4.621  loss_ce_1: 0.5553  loss_mask_1: 0.4973  loss_dice_1: 4.629  loss_ce_2: 0.4995  loss_mask_2: 0.4979  loss_dice_2: 4.618  loss_ce_3: 0.4821  loss_mask_3: 0.4989  loss_dice_3: 4.617  loss_ce_4: 0.4623  loss_mask_4: 0.4977  loss_dice_4: 4.619  loss_ce_5: 0.4558  loss_mask_5: 0.4971  loss_dice_5: 4.615  loss_ce_6: 0.447  loss_mask_6: 0.496  loss_dice_6: 4.62  loss_ce_7: 0.4617  loss_mask_7: 0.4992  loss_dice_7: 4.615  loss_ce_8: 0.457  loss_mask_8: 0.4966  loss_dice_8: 4.613  time: 1.5999  data_time: 0.0746  lr: 9.8426e-06  max_mem: 21212M
[01/17 18:50:41] d2.utils.events INFO:  eta: 17:05:31  iter: 719  total_loss: 57.78  loss_ce: 0.4743  loss_mask: 0.4979  loss_dice: 4.624  loss_ce_0: 2.125  loss_mask_0: 0.4855  loss_dice_0: 4.624  loss_ce_1: 0.537  loss_mask_1: 0.4934  loss_dice_1: 4.627  loss_ce_2: 0.4782  loss_mask_2: 0.4944  loss_dice_2: 4.626  loss_ce_3: 0.4755  loss_mask_3: 0.4943  loss_dice_3: 4.622  loss_ce_4: 0.4789  loss_mask_4: 0.4965  loss_dice_4: 4.619  loss_ce_5: 0.4744  loss_mask_5: 0.4974  loss_dice_5: 4.617  loss_ce_6: 0.4635  loss_mask_6: 0.4953  loss_dice_6: 4.619  loss_ce_7: 0.4572  loss_mask_7: 0.4982  loss_dice_7: 4.621  loss_ce_8: 0.4595  loss_mask_8: 0.4962  loss_dice_8: 4.624  time: 1.5984  data_time: 0.0863  lr: 9.8381e-06  max_mem: 21212M
[01/17 18:51:12] d2.utils.events INFO:  eta: 17:04:45  iter: 739  total_loss: 57.55  loss_ce: 0.4912  loss_mask: 0.4944  loss_dice: 4.601  loss_ce_0: 2.109  loss_mask_0: 0.49  loss_dice_0: 4.598  loss_ce_1: 0.5841  loss_mask_1: 0.4963  loss_dice_1: 4.606  loss_ce_2: 0.5218  loss_mask_2: 0.4945  loss_dice_2: 4.601  loss_ce_3: 0.5155  loss_mask_3: 0.4955  loss_dice_3: 4.598  loss_ce_4: 0.4977  loss_mask_4: 0.4954  loss_dice_4: 4.596  loss_ce_5: 0.4834  loss_mask_5: 0.4954  loss_dice_5: 4.594  loss_ce_6: 0.5095  loss_mask_6: 0.4947  loss_dice_6: 4.598  loss_ce_7: 0.4813  loss_mask_7: 0.4963  loss_dice_7: 4.602  loss_ce_8: 0.4932  loss_mask_8: 0.4922  loss_dice_8: 4.607  time: 1.5973  data_time: 0.0903  lr: 9.8336e-06  max_mem: 21212M
[01/17 18:51:43] d2.utils.events INFO:  eta: 17:03:41  iter: 759  total_loss: 57.72  loss_ce: 0.4801  loss_mask: 0.5065  loss_dice: 4.595  loss_ce_0: 2.134  loss_mask_0: 0.5006  loss_dice_0: 4.6  loss_ce_1: 0.5761  loss_mask_1: 0.5096  loss_dice_1: 4.6  loss_ce_2: 0.5045  loss_mask_2: 0.5091  loss_dice_2: 4.599  loss_ce_3: 0.4861  loss_mask_3: 0.5082  loss_dice_3: 4.597  loss_ce_4: 0.4777  loss_mask_4: 0.5069  loss_dice_4: 4.593  loss_ce_5: 0.4796  loss_mask_5: 0.5088  loss_dice_5: 4.588  loss_ce_6: 0.4746  loss_mask_6: 0.5119  loss_dice_6: 4.589  loss_ce_7: 0.4726  loss_mask_7: 0.509  loss_dice_7: 4.591  loss_ce_8: 0.4689  loss_mask_8: 0.5092  loss_dice_8: 4.596  time: 1.5956  data_time: 0.0942  lr: 9.8291e-06  max_mem: 21212M
[01/17 18:52:14] d2.utils.events INFO:  eta: 17:03:25  iter: 779  total_loss: 57.6  loss_ce: 0.489  loss_mask: 0.5104  loss_dice: 4.597  loss_ce_0: 2.014  loss_mask_0: 0.5021  loss_dice_0: 4.612  loss_ce_1: 0.5819  loss_mask_1: 0.5089  loss_dice_1: 4.617  loss_ce_2: 0.5022  loss_mask_2: 0.505  loss_dice_2: 4.605  loss_ce_3: 0.4842  loss_mask_3: 0.5075  loss_dice_3: 4.605  loss_ce_4: 0.4963  loss_mask_4: 0.5059  loss_dice_4: 4.593  loss_ce_5: 0.4924  loss_mask_5: 0.5087  loss_dice_5: 4.593  loss_ce_6: 0.4926  loss_mask_6: 0.5083  loss_dice_6: 4.593  loss_ce_7: 0.4808  loss_mask_7: 0.5092  loss_dice_7: 4.592  loss_ce_8: 0.4917  loss_mask_8: 0.509  loss_dice_8: 4.588  time: 1.5948  data_time: 0.0997  lr: 9.8246e-06  max_mem: 21212M
[01/17 18:52:45] d2.utils.events INFO:  eta: 17:02:37  iter: 799  total_loss: 57.68  loss_ce: 0.4549  loss_mask: 0.5273  loss_dice: 4.583  loss_ce_0: 2.049  loss_mask_0: 0.5195  loss_dice_0: 4.614  loss_ce_1: 0.5047  loss_mask_1: 0.5278  loss_dice_1: 4.601  loss_ce_2: 0.4763  loss_mask_2: 0.5286  loss_dice_2: 4.585  loss_ce_3: 0.4593  loss_mask_3: 0.5282  loss_dice_3: 4.582  loss_ce_4: 0.4511  loss_mask_4: 0.5286  loss_dice_4: 4.582  loss_ce_5: 0.4499  loss_mask_5: 0.5307  loss_dice_5: 4.571  loss_ce_6: 0.4496  loss_mask_6: 0.532  loss_dice_6: 4.58  loss_ce_7: 0.4385  loss_mask_7: 0.5298  loss_dice_7: 4.58  loss_ce_8: 0.4338  loss_mask_8: 0.5283  loss_dice_8: 4.583  time: 1.5934  data_time: 0.0878  lr: 9.82e-06  max_mem: 21212M
[01/17 18:53:16] d2.utils.events INFO:  eta: 17:01:53  iter: 819  total_loss: 57.58  loss_ce: 0.467  loss_mask: 0.5139  loss_dice: 4.599  loss_ce_0: 2.005  loss_mask_0: 0.5088  loss_dice_0: 4.605  loss_ce_1: 0.5384  loss_mask_1: 0.5131  loss_dice_1: 4.604  loss_ce_2: 0.4799  loss_mask_2: 0.5105  loss_dice_2: 4.598  loss_ce_3: 0.4623  loss_mask_3: 0.5116  loss_dice_3: 4.593  loss_ce_4: 0.4609  loss_mask_4: 0.5114  loss_dice_4: 4.591  loss_ce_5: 0.4574  loss_mask_5: 0.5133  loss_dice_5: 4.591  loss_ce_6: 0.4602  loss_mask_6: 0.5127  loss_dice_6: 4.596  loss_ce_7: 0.4569  loss_mask_7: 0.5154  loss_dice_7: 4.587  loss_ce_8: 0.4692  loss_mask_8: 0.5172  loss_dice_8: 4.599  time: 1.5919  data_time: 0.0848  lr: 9.8155e-06  max_mem: 21212M
[01/17 18:53:47] d2.utils.events INFO:  eta: 17:00:40  iter: 839  total_loss: 57.4  loss_ce: 0.4742  loss_mask: 0.5236  loss_dice: 4.594  loss_ce_0: 1.997  loss_mask_0: 0.5052  loss_dice_0: 4.616  loss_ce_1: 0.5434  loss_mask_1: 0.5185  loss_dice_1: 4.608  loss_ce_2: 0.4832  loss_mask_2: 0.5168  loss_dice_2: 4.605  loss_ce_3: 0.4766  loss_mask_3: 0.5186  loss_dice_3: 4.593  loss_ce_4: 0.4825  loss_mask_4: 0.5204  loss_dice_4: 4.598  loss_ce_5: 0.4751  loss_mask_5: 0.5176  loss_dice_5: 4.595  loss_ce_6: 0.476  loss_mask_6: 0.522  loss_dice_6: 4.594  loss_ce_7: 0.4654  loss_mask_7: 0.5233  loss_dice_7: 4.597  loss_ce_8: 0.4637  loss_mask_8: 0.5195  loss_dice_8: 4.598  time: 1.5908  data_time: 0.0867  lr: 9.811e-06  max_mem: 21212M
[01/17 18:54:18] d2.utils.events INFO:  eta: 16:59:52  iter: 859  total_loss: 57.18  loss_ce: 0.4423  loss_mask: 0.5045  loss_dice: 4.591  loss_ce_0: 1.939  loss_mask_0: 0.4991  loss_dice_0: 4.609  loss_ce_1: 0.5202  loss_mask_1: 0.5093  loss_dice_1: 4.609  loss_ce_2: 0.4694  loss_mask_2: 0.5078  loss_dice_2: 4.595  loss_ce_3: 0.4577  loss_mask_3: 0.5069  loss_dice_3: 4.593  loss_ce_4: 0.4404  loss_mask_4: 0.5085  loss_dice_4: 4.592  loss_ce_5: 0.4421  loss_mask_5: 0.5094  loss_dice_5: 4.591  loss_ce_6: 0.4398  loss_mask_6: 0.5093  loss_dice_6: 4.588  loss_ce_7: 0.4379  loss_mask_7: 0.5117  loss_dice_7: 4.597  loss_ce_8: 0.4345  loss_mask_8: 0.5094  loss_dice_8: 4.596  time: 1.5902  data_time: 0.0957  lr: 9.8065e-06  max_mem: 21212M
[01/17 18:54:49] d2.utils.events INFO:  eta: 16:58:49  iter: 879  total_loss: 57.25  loss_ce: 0.4774  loss_mask: 0.5097  loss_dice: 4.583  loss_ce_0: 1.952  loss_mask_0: 0.5051  loss_dice_0: 4.609  loss_ce_1: 0.5239  loss_mask_1: 0.5116  loss_dice_1: 4.609  loss_ce_2: 0.488  loss_mask_2: 0.5107  loss_dice_2: 4.595  loss_ce_3: 0.4771  loss_mask_3: 0.5116  loss_dice_3: 4.587  loss_ce_4: 0.4805  loss_mask_4: 0.5107  loss_dice_4: 4.585  loss_ce_5: 0.4762  loss_mask_5: 0.5099  loss_dice_5: 4.59  loss_ce_6: 0.4695  loss_mask_6: 0.5068  loss_dice_6: 4.592  loss_ce_7: 0.4672  loss_mask_7: 0.5063  loss_dice_7: 4.59  loss_ce_8: 0.4728  loss_mask_8: 0.5106  loss_dice_8: 4.591  time: 1.5889  data_time: 0.0892  lr: 9.802e-06  max_mem: 21212M
[01/17 18:55:20] d2.utils.events INFO:  eta: 16:56:57  iter: 899  total_loss: 56.99  loss_ce: 0.4446  loss_mask: 0.5251  loss_dice: 4.586  loss_ce_0: 1.903  loss_mask_0: 0.5126  loss_dice_0: 4.6  loss_ce_1: 0.5055  loss_mask_1: 0.5216  loss_dice_1: 4.579  loss_ce_2: 0.4672  loss_mask_2: 0.5226  loss_dice_2: 4.591  loss_ce_3: 0.4369  loss_mask_3: 0.524  loss_dice_3: 4.583  loss_ce_4: 0.445  loss_mask_4: 0.5239  loss_dice_4: 4.582  loss_ce_5: 0.4355  loss_mask_5: 0.5238  loss_dice_5: 4.568  loss_ce_6: 0.4362  loss_mask_6: 0.5258  loss_dice_6: 4.579  loss_ce_7: 0.4421  loss_mask_7: 0.525  loss_dice_7: 4.583  loss_ce_8: 0.4429  loss_mask_8: 0.5242  loss_dice_8: 4.583  time: 1.5878  data_time: 0.0794  lr: 9.7975e-06  max_mem: 21212M
[01/17 18:55:51] d2.utils.events INFO:  eta: 16:56:25  iter: 919  total_loss: 57.1  loss_ce: 0.4592  loss_mask: 0.5101  loss_dice: 4.582  loss_ce_0: 1.885  loss_mask_0: 0.5024  loss_dice_0: 4.6  loss_ce_1: 0.5257  loss_mask_1: 0.5081  loss_dice_1: 4.6  loss_ce_2: 0.4836  loss_mask_2: 0.5067  loss_dice_2: 4.587  loss_ce_3: 0.4661  loss_mask_3: 0.5088  loss_dice_3: 4.584  loss_ce_4: 0.4676  loss_mask_4: 0.511  loss_dice_4: 4.577  loss_ce_5: 0.4684  loss_mask_5: 0.5106  loss_dice_5: 4.579  loss_ce_6: 0.4781  loss_mask_6: 0.5085  loss_dice_6: 4.585  loss_ce_7: 0.4596  loss_mask_7: 0.51  loss_dice_7: 4.577  loss_ce_8: 0.4642  loss_mask_8: 0.5116  loss_dice_8: 4.584  time: 1.5871  data_time: 0.0802  lr: 9.793e-06  max_mem: 21212M
[01/17 18:56:22] d2.utils.events INFO:  eta: 16:56:01  iter: 939  total_loss: 57.04  loss_ce: 0.4523  loss_mask: 0.5134  loss_dice: 4.594  loss_ce_0: 1.838  loss_mask_0: 0.5046  loss_dice_0: 4.607  loss_ce_1: 0.5104  loss_mask_1: 0.5127  loss_dice_1: 4.601  loss_ce_2: 0.4665  loss_mask_2: 0.5156  loss_dice_2: 4.592  loss_ce_3: 0.4555  loss_mask_3: 0.5145  loss_dice_3: 4.587  loss_ce_4: 0.4491  loss_mask_4: 0.5144  loss_dice_4: 4.583  loss_ce_5: 0.4502  loss_mask_5: 0.5145  loss_dice_5: 4.586  loss_ce_6: 0.4545  loss_mask_6: 0.513  loss_dice_6: 4.584  loss_ce_7: 0.4473  loss_mask_7: 0.514  loss_dice_7: 4.589  loss_ce_8: 0.4333  loss_mask_8: 0.5143  loss_dice_8: 4.588  time: 1.5863  data_time: 0.0832  lr: 9.7885e-06  max_mem: 21212M
[01/17 18:56:53] d2.utils.events INFO:  eta: 16:55:06  iter: 959  total_loss: 57.12  loss_ce: 0.471  loss_mask: 0.4986  loss_dice: 4.609  loss_ce_0: 1.81  loss_mask_0: 0.4988  loss_dice_0: 4.624  loss_ce_1: 0.5107  loss_mask_1: 0.4993  loss_dice_1: 4.618  loss_ce_2: 0.4705  loss_mask_2: 0.4982  loss_dice_2: 4.617  loss_ce_3: 0.4631  loss_mask_3: 0.4954  loss_dice_3: 4.608  loss_ce_4: 0.4752  loss_mask_4: 0.4963  loss_dice_4: 4.605  loss_ce_5: 0.4616  loss_mask_5: 0.4985  loss_dice_5: 4.606  loss_ce_6: 0.4601  loss_mask_6: 0.4998  loss_dice_6: 4.608  loss_ce_7: 0.4655  loss_mask_7: 0.4948  loss_dice_7: 4.612  loss_ce_8: 0.4605  loss_mask_8: 0.4969  loss_dice_8: 4.613  time: 1.5857  data_time: 0.0851  lr: 9.784e-06  max_mem: 21212M
[01/17 18:57:24] d2.utils.events INFO:  eta: 16:54:35  iter: 979  total_loss: 57.06  loss_ce: 0.5026  loss_mask: 0.5189  loss_dice: 4.567  loss_ce_0: 1.799  loss_mask_0: 0.5104  loss_dice_0: 4.594  loss_ce_1: 0.5313  loss_mask_1: 0.5158  loss_dice_1: 4.583  loss_ce_2: 0.5032  loss_mask_2: 0.5125  loss_dice_2: 4.569  loss_ce_3: 0.5079  loss_mask_3: 0.5093  loss_dice_3: 4.563  loss_ce_4: 0.5053  loss_mask_4: 0.5116  loss_dice_4: 4.551  loss_ce_5: 0.5004  loss_mask_5: 0.5126  loss_dice_5: 4.559  loss_ce_6: 0.4889  loss_mask_6: 0.5156  loss_dice_6: 4.552  loss_ce_7: 0.4867  loss_mask_7: 0.5166  loss_dice_7: 4.558  loss_ce_8: 0.4794  loss_mask_8: 0.5186  loss_dice_8: 4.558  time: 1.5850  data_time: 0.0965  lr: 9.7795e-06  max_mem: 21212M
[01/17 18:57:56] d2.utils.events INFO:  eta: 16:53:46  iter: 999  total_loss: 57.08  loss_ce: 0.4602  loss_mask: 0.5096  loss_dice: 4.565  loss_ce_0: 1.807  loss_mask_0: 0.5025  loss_dice_0: 4.589  loss_ce_1: 0.5441  loss_mask_1: 0.5086  loss_dice_1: 4.581  loss_ce_2: 0.4899  loss_mask_2: 0.5094  loss_dice_2: 4.564  loss_ce_3: 0.4891  loss_mask_3: 0.5086  loss_dice_3: 4.558  loss_ce_4: 0.4765  loss_mask_4: 0.5117  loss_dice_4: 4.561  loss_ce_5: 0.473  loss_mask_5: 0.5133  loss_dice_5: 4.563  loss_ce_6: 0.4882  loss_mask_6: 0.5102  loss_dice_6: 4.552  loss_ce_7: 0.4568  loss_mask_7: 0.5122  loss_dice_7: 4.565  loss_ce_8: 0.4641  loss_mask_8: 0.5132  loss_dice_8: 4.568  time: 1.5844  data_time: 0.1008  lr: 9.7749e-06  max_mem: 21212M
[01/17 18:58:26] d2.utils.events INFO:  eta: 16:52:08  iter: 1019  total_loss: 56.94  loss_ce: 0.464  loss_mask: 0.5228  loss_dice: 4.56  loss_ce_0: 1.786  loss_mask_0: 0.5217  loss_dice_0: 4.59  loss_ce_1: 0.5202  loss_mask_1: 0.5182  loss_dice_1: 4.571  loss_ce_2: 0.4854  loss_mask_2: 0.517  loss_dice_2: 4.556  loss_ce_3: 0.4795  loss_mask_3: 0.5163  loss_dice_3: 4.554  loss_ce_4: 0.4833  loss_mask_4: 0.5193  loss_dice_4: 4.556  loss_ce_5: 0.4773  loss_mask_5: 0.5208  loss_dice_5: 4.549  loss_ce_6: 0.4828  loss_mask_6: 0.5171  loss_dice_6: 4.556  loss_ce_7: 0.4725  loss_mask_7: 0.5201  loss_dice_7: 4.553  loss_ce_8: 0.4589  loss_mask_8: 0.5226  loss_dice_8: 4.561  time: 1.5833  data_time: 0.0867  lr: 9.7704e-06  max_mem: 21212M
[01/17 18:58:58] d2.utils.events INFO:  eta: 16:50:54  iter: 1039  total_loss: 56.75  loss_ce: 0.4638  loss_mask: 0.5247  loss_dice: 4.539  loss_ce_0: 1.745  loss_mask_0: 0.5184  loss_dice_0: 4.57  loss_ce_1: 0.5258  loss_mask_1: 0.5218  loss_dice_1: 4.562  loss_ce_2: 0.4749  loss_mask_2: 0.5208  loss_dice_2: 4.556  loss_ce_3: 0.4693  loss_mask_3: 0.516  loss_dice_3: 4.549  loss_ce_4: 0.4611  loss_mask_4: 0.5187  loss_dice_4: 4.546  loss_ce_5: 0.4648  loss_mask_5: 0.5199  loss_dice_5: 4.535  loss_ce_6: 0.4485  loss_mask_6: 0.5188  loss_dice_6: 4.537  loss_ce_7: 0.4565  loss_mask_7: 0.5196  loss_dice_7: 4.536  loss_ce_8: 0.4615  loss_mask_8: 0.5227  loss_dice_8: 4.542  time: 1.5830  data_time: 0.0931  lr: 9.7659e-06  max_mem: 21212M
[01/17 18:59:29] d2.utils.events INFO:  eta: 16:48:34  iter: 1059  total_loss: 56.65  loss_ce: 0.4413  loss_mask: 0.5199  loss_dice: 4.55  loss_ce_0: 1.71  loss_mask_0: 0.5091  loss_dice_0: 4.592  loss_ce_1: 0.5118  loss_mask_1: 0.5158  loss_dice_1: 4.574  loss_ce_2: 0.4549  loss_mask_2: 0.5182  loss_dice_2: 4.566  loss_ce_3: 0.453  loss_mask_3: 0.5211  loss_dice_3: 4.557  loss_ce_4: 0.4461  loss_mask_4: 0.5233  loss_dice_4: 4.551  loss_ce_5: 0.4482  loss_mask_5: 0.5217  loss_dice_5: 4.544  loss_ce_6: 0.4444  loss_mask_6: 0.5204  loss_dice_6: 4.552  loss_ce_7: 0.4436  loss_mask_7: 0.5212  loss_dice_7: 4.553  loss_ce_8: 0.4349  loss_mask_8: 0.5204  loss_dice_8: 4.545  time: 1.5824  data_time: 0.0929  lr: 9.7614e-06  max_mem: 21212M
[01/17 19:00:00] d2.utils.events INFO:  eta: 16:47:29  iter: 1079  total_loss: 56.81  loss_ce: 0.4756  loss_mask: 0.5199  loss_dice: 4.535  loss_ce_0: 1.716  loss_mask_0: 0.5152  loss_dice_0: 4.573  loss_ce_1: 0.5195  loss_mask_1: 0.517  loss_dice_1: 4.546  loss_ce_2: 0.4833  loss_mask_2: 0.5186  loss_dice_2: 4.542  loss_ce_3: 0.4898  loss_mask_3: 0.5183  loss_dice_3: 4.543  loss_ce_4: 0.4852  loss_mask_4: 0.5198  loss_dice_4: 4.534  loss_ce_5: 0.4706  loss_mask_5: 0.521  loss_dice_5: 4.537  loss_ce_6: 0.4795  loss_mask_6: 0.5206  loss_dice_6: 4.533  loss_ce_7: 0.4642  loss_mask_7: 0.5232  loss_dice_7: 4.545  loss_ce_8: 0.4639  loss_mask_8: 0.5239  loss_dice_8: 4.532  time: 1.5818  data_time: 0.0878  lr: 9.7569e-06  max_mem: 21212M
[01/17 19:00:31] d2.utils.events INFO:  eta: 16:46:58  iter: 1099  total_loss: 56.65  loss_ce: 0.4671  loss_mask: 0.5297  loss_dice: 4.534  loss_ce_0: 1.69  loss_mask_0: 0.5236  loss_dice_0: 4.574  loss_ce_1: 0.5104  loss_mask_1: 0.5299  loss_dice_1: 4.556  loss_ce_2: 0.4679  loss_mask_2: 0.5331  loss_dice_2: 4.549  loss_ce_3: 0.464  loss_mask_3: 0.5329  loss_dice_3: 4.536  loss_ce_4: 0.4649  loss_mask_4: 0.5324  loss_dice_4: 4.536  loss_ce_5: 0.4712  loss_mask_5: 0.5323  loss_dice_5: 4.537  loss_ce_6: 0.4722  loss_mask_6: 0.5267  loss_dice_6: 4.535  loss_ce_7: 0.4635  loss_mask_7: 0.5296  loss_dice_7: 4.532  loss_ce_8: 0.4583  loss_mask_8: 0.5301  loss_dice_8: 4.537  time: 1.5815  data_time: 0.0929  lr: 9.7524e-06  max_mem: 21212M
[01/17 19:01:02] d2.utils.events INFO:  eta: 16:46:05  iter: 1119  total_loss: 56.35  loss_ce: 0.4092  loss_mask: 0.5287  loss_dice: 4.556  loss_ce_0: 1.644  loss_mask_0: 0.5166  loss_dice_0: 4.597  loss_ce_1: 0.4538  loss_mask_1: 0.5227  loss_dice_1: 4.575  loss_ce_2: 0.4292  loss_mask_2: 0.5252  loss_dice_2: 4.558  loss_ce_3: 0.4302  loss_mask_3: 0.526  loss_dice_3: 4.558  loss_ce_4: 0.4255  loss_mask_4: 0.5256  loss_dice_4: 4.548  loss_ce_5: 0.4135  loss_mask_5: 0.5288  loss_dice_5: 4.545  loss_ce_6: 0.4197  loss_mask_6: 0.529  loss_dice_6: 4.55  loss_ce_7: 0.4105  loss_mask_7: 0.5311  loss_dice_7: 4.556  loss_ce_8: 0.4056  loss_mask_8: 0.5284  loss_dice_8: 4.559  time: 1.5810  data_time: 0.0881  lr: 9.7479e-06  max_mem: 21212M
[01/17 19:01:33] d2.utils.events INFO:  eta: 16:45:15  iter: 1139  total_loss: 56.55  loss_ce: 0.4371  loss_mask: 0.5448  loss_dice: 4.549  loss_ce_0: 1.673  loss_mask_0: 0.5285  loss_dice_0: 4.59  loss_ce_1: 0.5029  loss_mask_1: 0.5368  loss_dice_1: 4.571  loss_ce_2: 0.4622  loss_mask_2: 0.5382  loss_dice_2: 4.552  loss_ce_3: 0.4602  loss_mask_3: 0.5397  loss_dice_3: 4.55  loss_ce_4: 0.4392  loss_mask_4: 0.5424  loss_dice_4: 4.544  loss_ce_5: 0.4277  loss_mask_5: 0.5427  loss_dice_5: 4.551  loss_ce_6: 0.438  loss_mask_6: 0.5453  loss_dice_6: 4.549  loss_ce_7: 0.423  loss_mask_7: 0.5467  loss_dice_7: 4.557  loss_ce_8: 0.442  loss_mask_8: 0.5439  loss_dice_8: 4.548  time: 1.5806  data_time: 0.0898  lr: 9.7434e-06  max_mem: 21212M
[01/17 19:02:04] d2.utils.events INFO:  eta: 16:44:31  iter: 1159  total_loss: 56.38  loss_ce: 0.4573  loss_mask: 0.5302  loss_dice: 4.522  loss_ce_0: 1.687  loss_mask_0: 0.5215  loss_dice_0: 4.566  loss_ce_1: 0.532  loss_mask_1: 0.5261  loss_dice_1: 4.541  loss_ce_2: 0.476  loss_mask_2: 0.5257  loss_dice_2: 4.533  loss_ce_3: 0.4747  loss_mask_3: 0.53  loss_dice_3: 4.52  loss_ce_4: 0.4659  loss_mask_4: 0.5319  loss_dice_4: 4.518  loss_ce_5: 0.445  loss_mask_5: 0.5303  loss_dice_5: 4.512  loss_ce_6: 0.4596  loss_mask_6: 0.5283  loss_dice_6: 4.517  loss_ce_7: 0.4395  loss_mask_7: 0.5289  loss_dice_7: 4.522  loss_ce_8: 0.4332  loss_mask_8: 0.5308  loss_dice_8: 4.515  time: 1.5800  data_time: 0.0834  lr: 9.7388e-06  max_mem: 21212M
[01/17 19:02:36] d2.utils.events INFO:  eta: 16:44:07  iter: 1179  total_loss: 56.38  loss_ce: 0.4607  loss_mask: 0.5265  loss_dice: 4.509  loss_ce_0: 1.664  loss_mask_0: 0.5168  loss_dice_0: 4.56  loss_ce_1: 0.4958  loss_mask_1: 0.5232  loss_dice_1: 4.528  loss_ce_2: 0.4608  loss_mask_2: 0.5263  loss_dice_2: 4.522  loss_ce_3: 0.4608  loss_mask_3: 0.5276  loss_dice_3: 4.515  loss_ce_4: 0.4726  loss_mask_4: 0.5278  loss_dice_4: 4.506  loss_ce_5: 0.4662  loss_mask_5: 0.526  loss_dice_5: 4.507  loss_ce_6: 0.4671  loss_mask_6: 0.5225  loss_dice_6: 4.51  loss_ce_7: 0.4513  loss_mask_7: 0.5252  loss_dice_7: 4.519  loss_ce_8: 0.4485  loss_mask_8: 0.5257  loss_dice_8: 4.512  time: 1.5796  data_time: 0.0891  lr: 9.7343e-06  max_mem: 21212M
[01/17 19:03:07] d2.utils.events INFO:  eta: 16:43:23  iter: 1199  total_loss: 56.21  loss_ce: 0.4406  loss_mask: 0.5372  loss_dice: 4.515  loss_ce_0: 1.568  loss_mask_0: 0.5273  loss_dice_0: 4.566  loss_ce_1: 0.4834  loss_mask_1: 0.5328  loss_dice_1: 4.538  loss_ce_2: 0.4483  loss_mask_2: 0.534  loss_dice_2: 4.53  loss_ce_3: 0.4433  loss_mask_3: 0.5343  loss_dice_3: 4.521  loss_ce_4: 0.4541  loss_mask_4: 0.5353  loss_dice_4: 4.519  loss_ce_5: 0.4359  loss_mask_5: 0.5324  loss_dice_5: 4.517  loss_ce_6: 0.4378  loss_mask_6: 0.5356  loss_dice_6: 4.521  loss_ce_7: 0.4384  loss_mask_7: 0.5364  loss_dice_7: 4.518  loss_ce_8: 0.436  loss_mask_8: 0.5365  loss_dice_8: 4.524  time: 1.5792  data_time: 0.0902  lr: 9.7298e-06  max_mem: 21212M
[01/17 19:03:38] d2.utils.events INFO:  eta: 16:42:36  iter: 1219  total_loss: 56.16  loss_ce: 0.43  loss_mask: 0.5263  loss_dice: 4.523  loss_ce_0: 1.568  loss_mask_0: 0.5172  loss_dice_0: 4.557  loss_ce_1: 0.5152  loss_mask_1: 0.5183  loss_dice_1: 4.536  loss_ce_2: 0.4522  loss_mask_2: 0.5211  loss_dice_2: 4.528  loss_ce_3: 0.4496  loss_mask_3: 0.5226  loss_dice_3: 4.521  loss_ce_4: 0.4301  loss_mask_4: 0.5258  loss_dice_4: 4.52  loss_ce_5: 0.4152  loss_mask_5: 0.5286  loss_dice_5: 4.523  loss_ce_6: 0.4197  loss_mask_6: 0.5289  loss_dice_6: 4.522  loss_ce_7: 0.4266  loss_mask_7: 0.5259  loss_dice_7: 4.524  loss_ce_8: 0.4241  loss_mask_8: 0.5275  loss_dice_8: 4.526  time: 1.5790  data_time: 0.0879  lr: 9.7253e-06  max_mem: 21212M
[01/17 19:04:09] d2.utils.events INFO:  eta: 16:41:13  iter: 1239  total_loss: 56.15  loss_ce: 0.4257  loss_mask: 0.5416  loss_dice: 4.514  loss_ce_0: 1.572  loss_mask_0: 0.5345  loss_dice_0: 4.549  loss_ce_1: 0.4786  loss_mask_1: 0.5423  loss_dice_1: 4.527  loss_ce_2: 0.4197  loss_mask_2: 0.5411  loss_dice_2: 4.513  loss_ce_3: 0.4366  loss_mask_3: 0.5393  loss_dice_3: 4.512  loss_ce_4: 0.4382  loss_mask_4: 0.5404  loss_dice_4: 4.509  loss_ce_5: 0.4384  loss_mask_5: 0.541  loss_dice_5: 4.507  loss_ce_6: 0.4366  loss_mask_6: 0.5417  loss_dice_6: 4.505  loss_ce_7: 0.4336  loss_mask_7: 0.5423  loss_dice_7: 4.511  loss_ce_8: 0.4371  loss_mask_8: 0.5417  loss_dice_8: 4.512  time: 1.5783  data_time: 0.0892  lr: 9.7208e-06  max_mem: 21212M
[01/17 19:04:40] d2.utils.events INFO:  eta: 16:40:13  iter: 1259  total_loss: 56.34  loss_ce: 0.4815  loss_mask: 0.5497  loss_dice: 4.503  loss_ce_0: 1.586  loss_mask_0: 0.5468  loss_dice_0: 4.54  loss_ce_1: 0.5215  loss_mask_1: 0.5477  loss_dice_1: 4.524  loss_ce_2: 0.4892  loss_mask_2: 0.5487  loss_dice_2: 4.509  loss_ce_3: 0.4772  loss_mask_3: 0.5503  loss_dice_3: 4.503  loss_ce_4: 0.4781  loss_mask_4: 0.5496  loss_dice_4: 4.492  loss_ce_5: 0.4713  loss_mask_5: 0.5498  loss_dice_5: 4.494  loss_ce_6: 0.4722  loss_mask_6: 0.551  loss_dice_6: 4.496  loss_ce_7: 0.4707  loss_mask_7: 0.5544  loss_dice_7: 4.503  loss_ce_8: 0.4721  loss_mask_8: 0.5509  loss_dice_8: 4.507  time: 1.5779  data_time: 0.0837  lr: 9.7163e-06  max_mem: 21212M
[01/17 19:05:11] d2.utils.events INFO:  eta: 16:39:09  iter: 1279  total_loss: 55.89  loss_ce: 0.4175  loss_mask: 0.5379  loss_dice: 4.511  loss_ce_0: 1.491  loss_mask_0: 0.5284  loss_dice_0: 4.552  loss_ce_1: 0.4743  loss_mask_1: 0.5295  loss_dice_1: 4.52  loss_ce_2: 0.4475  loss_mask_2: 0.5311  loss_dice_2: 4.518  loss_ce_3: 0.4469  loss_mask_3: 0.5341  loss_dice_3: 4.504  loss_ce_4: 0.4272  loss_mask_4: 0.5356  loss_dice_4: 4.513  loss_ce_5: 0.4216  loss_mask_5: 0.5366  loss_dice_5: 4.508  loss_ce_6: 0.4143  loss_mask_6: 0.5339  loss_dice_6: 4.511  loss_ce_7: 0.4209  loss_mask_7: 0.536  loss_dice_7: 4.516  loss_ce_8: 0.4177  loss_mask_8: 0.5378  loss_dice_8: 4.516  time: 1.5774  data_time: 0.0888  lr: 9.7118e-06  max_mem: 21212M
[01/17 19:05:42] d2.utils.events INFO:  eta: 16:37:58  iter: 1299  total_loss: 56.01  loss_ce: 0.4354  loss_mask: 0.5452  loss_dice: 4.506  loss_ce_0: 1.525  loss_mask_0: 0.5318  loss_dice_0: 4.541  loss_ce_1: 0.4892  loss_mask_1: 0.5352  loss_dice_1: 4.525  loss_ce_2: 0.4651  loss_mask_2: 0.5374  loss_dice_2: 4.509  loss_ce_3: 0.4435  loss_mask_3: 0.5406  loss_dice_3: 4.504  loss_ce_4: 0.4408  loss_mask_4: 0.5447  loss_dice_4: 4.496  loss_ce_5: 0.4386  loss_mask_5: 0.5459  loss_dice_5: 4.503  loss_ce_6: 0.4272  loss_mask_6: 0.5454  loss_dice_6: 4.504  loss_ce_7: 0.432  loss_mask_7: 0.5455  loss_dice_7: 4.505  loss_ce_8: 0.4327  loss_mask_8: 0.5444  loss_dice_8: 4.504  time: 1.5770  data_time: 0.1012  lr: 9.7072e-06  max_mem: 21212M
[01/17 19:06:13] d2.utils.events INFO:  eta: 16:37:21  iter: 1319  total_loss: 56.09  loss_ce: 0.4561  loss_mask: 0.5424  loss_dice: 4.488  loss_ce_0: 1.518  loss_mask_0: 0.535  loss_dice_0: 4.523  loss_ce_1: 0.4826  loss_mask_1: 0.541  loss_dice_1: 4.495  loss_ce_2: 0.4658  loss_mask_2: 0.5415  loss_dice_2: 4.483  loss_ce_3: 0.4637  loss_mask_3: 0.5417  loss_dice_3: 4.485  loss_ce_4: 0.4641  loss_mask_4: 0.5426  loss_dice_4: 4.473  loss_ce_5: 0.4524  loss_mask_5: 0.5426  loss_dice_5: 4.481  loss_ce_6: 0.4536  loss_mask_6: 0.5433  loss_dice_6: 4.478  loss_ce_7: 0.4563  loss_mask_7: 0.5431  loss_dice_7: 4.482  loss_ce_8: 0.4436  loss_mask_8: 0.5433  loss_dice_8: 4.494  time: 1.5765  data_time: 0.0831  lr: 9.7027e-06  max_mem: 21212M
[01/17 19:06:43] d2.utils.events INFO:  eta: 16:36:24  iter: 1339  total_loss: 56.12  loss_ce: 0.4644  loss_mask: 0.542  loss_dice: 4.486  loss_ce_0: 1.52  loss_mask_0: 0.533  loss_dice_0: 4.533  loss_ce_1: 0.485  loss_mask_1: 0.538  loss_dice_1: 4.511  loss_ce_2: 0.4691  loss_mask_2: 0.5412  loss_dice_2: 4.499  loss_ce_3: 0.4572  loss_mask_3: 0.5413  loss_dice_3: 4.488  loss_ce_4: 0.4497  loss_mask_4: 0.5425  loss_dice_4: 4.487  loss_ce_5: 0.4651  loss_mask_5: 0.544  loss_dice_5: 4.483  loss_ce_6: 0.4535  loss_mask_6: 0.5413  loss_dice_6: 4.479  loss_ce_7: 0.4562  loss_mask_7: 0.5445  loss_dice_7: 4.489  loss_ce_8: 0.4545  loss_mask_8: 0.5421  loss_dice_8: 4.491  time: 1.5756  data_time: 0.0784  lr: 9.6982e-06  max_mem: 21212M
[01/17 19:07:14] d2.utils.events INFO:  eta: 16:35:08  iter: 1359  total_loss: 55.92  loss_ce: 0.4121  loss_mask: 0.5487  loss_dice: 4.51  loss_ce_0: 1.475  loss_mask_0: 0.5331  loss_dice_0: 4.549  loss_ce_1: 0.5021  loss_mask_1: 0.5401  loss_dice_1: 4.521  loss_ce_2: 0.4526  loss_mask_2: 0.5453  loss_dice_2: 4.513  loss_ce_3: 0.4401  loss_mask_3: 0.5443  loss_dice_3: 4.51  loss_ce_4: 0.4388  loss_mask_4: 0.5473  loss_dice_4: 4.502  loss_ce_5: 0.4359  loss_mask_5: 0.5478  loss_dice_5: 4.504  loss_ce_6: 0.4248  loss_mask_6: 0.5463  loss_dice_6: 4.505  loss_ce_7: 0.4268  loss_mask_7: 0.5493  loss_dice_7: 4.506  loss_ce_8: 0.422  loss_mask_8: 0.5502  loss_dice_8: 4.505  time: 1.5748  data_time: 0.0902  lr: 9.6937e-06  max_mem: 21212M
[01/17 19:07:44] d2.utils.events INFO:  eta: 16:33:49  iter: 1379  total_loss: 55.91  loss_ce: 0.4257  loss_mask: 0.5511  loss_dice: 4.474  loss_ce_0: 1.44  loss_mask_0: 0.5331  loss_dice_0: 4.526  loss_ce_1: 0.4975  loss_mask_1: 0.5376  loss_dice_1: 4.494  loss_ce_2: 0.4561  loss_mask_2: 0.5408  loss_dice_2: 4.479  loss_ce_3: 0.4423  loss_mask_3: 0.5468  loss_dice_3: 4.469  loss_ce_4: 0.4441  loss_mask_4: 0.5503  loss_dice_4: 4.46  loss_ce_5: 0.4401  loss_mask_5: 0.5482  loss_dice_5: 4.469  loss_ce_6: 0.438  loss_mask_6: 0.5492  loss_dice_6: 4.469  loss_ce_7: 0.4235  loss_mask_7: 0.5478  loss_dice_7: 4.473  loss_ce_8: 0.4188  loss_mask_8: 0.5467  loss_dice_8: 4.471  time: 1.5739  data_time: 0.0797  lr: 9.6892e-06  max_mem: 21212M
[01/17 19:08:15] d2.utils.events INFO:  eta: 16:33:06  iter: 1399  total_loss: 56.06  loss_ce: 0.4511  loss_mask: 0.5436  loss_dice: 4.504  loss_ce_0: 1.429  loss_mask_0: 0.53  loss_dice_0: 4.55  loss_ce_1: 0.4778  loss_mask_1: 0.5362  loss_dice_1: 4.511  loss_ce_2: 0.457  loss_mask_2: 0.5396  loss_dice_2: 4.502  loss_ce_3: 0.4404  loss_mask_3: 0.5381  loss_dice_3: 4.498  loss_ce_4: 0.4443  loss_mask_4: 0.5408  loss_dice_4: 4.504  loss_ce_5: 0.4457  loss_mask_5: 0.5459  loss_dice_5: 4.496  loss_ce_6: 0.4379  loss_mask_6: 0.5408  loss_dice_6: 4.497  loss_ce_7: 0.44  loss_mask_7: 0.544  loss_dice_7: 4.502  loss_ce_8: 0.4484  loss_mask_8: 0.547  loss_dice_8: 4.497  time: 1.5730  data_time: 0.0853  lr: 9.6847e-06  max_mem: 21212M
[01/17 19:08:45] d2.utils.events INFO:  eta: 16:32:42  iter: 1419  total_loss: 55.5  loss_ce: 0.4154  loss_mask: 0.5258  loss_dice: 4.469  loss_ce_0: 1.383  loss_mask_0: 0.5239  loss_dice_0: 4.535  loss_ce_1: 0.4775  loss_mask_1: 0.5202  loss_dice_1: 4.504  loss_ce_2: 0.4524  loss_mask_2: 0.5236  loss_dice_2: 4.487  loss_ce_3: 0.4243  loss_mask_3: 0.5267  loss_dice_3: 4.48  loss_ce_4: 0.4188  loss_mask_4: 0.5295  loss_dice_4: 4.478  loss_ce_5: 0.4098  loss_mask_5: 0.5274  loss_dice_5: 4.476  loss_ce_6: 0.4135  loss_mask_6: 0.5258  loss_dice_6: 4.477  loss_ce_7: 0.4021  loss_mask_7: 0.5289  loss_dice_7: 4.481  loss_ce_8: 0.4192  loss_mask_8: 0.529  loss_dice_8: 4.482  time: 1.5724  data_time: 0.0864  lr: 9.6802e-06  max_mem: 21212M
[01/17 19:09:16] d2.utils.events INFO:  eta: 16:31:40  iter: 1439  total_loss: 55.56  loss_ce: 0.4126  loss_mask: 0.5431  loss_dice: 4.49  loss_ce_0: 1.373  loss_mask_0: 0.53  loss_dice_0: 4.538  loss_ce_1: 0.4861  loss_mask_1: 0.5351  loss_dice_1: 4.506  loss_ce_2: 0.452  loss_mask_2: 0.5365  loss_dice_2: 4.492  loss_ce_3: 0.4397  loss_mask_3: 0.5364  loss_dice_3: 4.492  loss_ce_4: 0.4317  loss_mask_4: 0.5386  loss_dice_4: 4.489  loss_ce_5: 0.4332  loss_mask_5: 0.5412  loss_dice_5: 4.49  loss_ce_6: 0.421  loss_mask_6: 0.5408  loss_dice_6: 4.484  loss_ce_7: 0.4163  loss_mask_7: 0.5439  loss_dice_7: 4.493  loss_ce_8: 0.4176  loss_mask_8: 0.5414  loss_dice_8: 4.493  time: 1.5717  data_time: 0.0903  lr: 9.6756e-06  max_mem: 21212M
[01/17 19:09:46] d2.utils.events INFO:  eta: 16:31:22  iter: 1459  total_loss: 55.36  loss_ce: 0.4183  loss_mask: 0.5407  loss_dice: 4.453  loss_ce_0: 1.353  loss_mask_0: 0.5283  loss_dice_0: 4.508  loss_ce_1: 0.4686  loss_mask_1: 0.5352  loss_dice_1: 4.469  loss_ce_2: 0.4354  loss_mask_2: 0.535  loss_dice_2: 4.462  loss_ce_3: 0.4309  loss_mask_3: 0.5374  loss_dice_3: 4.458  loss_ce_4: 0.4412  loss_mask_4: 0.5399  loss_dice_4: 4.457  loss_ce_5: 0.4328  loss_mask_5: 0.5439  loss_dice_5: 4.449  loss_ce_6: 0.428  loss_mask_6: 0.5425  loss_dice_6: 4.445  loss_ce_7: 0.4227  loss_mask_7: 0.5424  loss_dice_7: 4.448  loss_ce_8: 0.4094  loss_mask_8: 0.5429  loss_dice_8: 4.453  time: 1.5712  data_time: 0.0922  lr: 9.6711e-06  max_mem: 21212M
[01/17 19:10:17] d2.utils.events INFO:  eta: 16:30:43  iter: 1479  total_loss: 55.63  loss_ce: 0.4502  loss_mask: 0.5605  loss_dice: 4.459  loss_ce_0: 1.397  loss_mask_0: 0.5387  loss_dice_0: 4.502  loss_ce_1: 0.4777  loss_mask_1: 0.5511  loss_dice_1: 4.477  loss_ce_2: 0.4477  loss_mask_2: 0.5528  loss_dice_2: 4.464  loss_ce_3: 0.4334  loss_mask_3: 0.5565  loss_dice_3: 4.47  loss_ce_4: 0.4405  loss_mask_4: 0.5554  loss_dice_4: 4.457  loss_ce_5: 0.4447  loss_mask_5: 0.5593  loss_dice_5: 4.464  loss_ce_6: 0.4421  loss_mask_6: 0.5567  loss_dice_6: 4.458  loss_ce_7: 0.4336  loss_mask_7: 0.5593  loss_dice_7: 4.462  loss_ce_8: 0.4401  loss_mask_8: 0.5598  loss_dice_8: 4.46  time: 1.5708  data_time: 0.0977  lr: 9.6666e-06  max_mem: 21212M
[01/17 19:10:48] d2.utils.events INFO:  eta: 16:30:23  iter: 1499  total_loss: 55.37  loss_ce: 0.4098  loss_mask: 0.5588  loss_dice: 4.481  loss_ce_0: 1.334  loss_mask_0: 0.5534  loss_dice_0: 4.505  loss_ce_1: 0.4651  loss_mask_1: 0.5606  loss_dice_1: 4.49  loss_ce_2: 0.4316  loss_mask_2: 0.5623  loss_dice_2: 4.48  loss_ce_3: 0.4165  loss_mask_3: 0.5657  loss_dice_3: 4.471  loss_ce_4: 0.4127  loss_mask_4: 0.5645  loss_dice_4: 4.477  loss_ce_5: 0.4152  loss_mask_5: 0.5647  loss_dice_5: 4.474  loss_ce_6: 0.4126  loss_mask_6: 0.5639  loss_dice_6: 4.472  loss_ce_7: 0.3977  loss_mask_7: 0.565  loss_dice_7: 4.476  loss_ce_8: 0.3982  loss_mask_8: 0.5656  loss_dice_8: 4.475  time: 1.5702  data_time: 0.0809  lr: 9.6621e-06  max_mem: 21212M
[01/17 19:11:18] d2.utils.events INFO:  eta: 16:29:41  iter: 1519  total_loss: 55.26  loss_ce: 0.4083  loss_mask: 0.5462  loss_dice: 4.455  loss_ce_0: 1.341  loss_mask_0: 0.5316  loss_dice_0: 4.501  loss_ce_1: 0.4833  loss_mask_1: 0.5363  loss_dice_1: 4.468  loss_ce_2: 0.4377  loss_mask_2: 0.539  loss_dice_2: 4.463  loss_ce_3: 0.4126  loss_mask_3: 0.5427  loss_dice_3: 4.453  loss_ce_4: 0.4126  loss_mask_4: 0.5421  loss_dice_4: 4.447  loss_ce_5: 0.4181  loss_mask_5: 0.5423  loss_dice_5: 4.451  loss_ce_6: 0.4084  loss_mask_6: 0.5432  loss_dice_6: 4.449  loss_ce_7: 0.3992  loss_mask_7: 0.5447  loss_dice_7: 4.452  loss_ce_8: 0.4132  loss_mask_8: 0.5458  loss_dice_8: 4.452  time: 1.5695  data_time: 0.0892  lr: 9.6576e-06  max_mem: 21212M
[01/17 19:11:49] d2.utils.events INFO:  eta: 16:29:10  iter: 1539  total_loss: 55.27  loss_ce: 0.426  loss_mask: 0.5411  loss_dice: 4.47  loss_ce_0: 1.31  loss_mask_0: 0.5299  loss_dice_0: 4.507  loss_ce_1: 0.4738  loss_mask_1: 0.5383  loss_dice_1: 4.48  loss_ce_2: 0.4271  loss_mask_2: 0.5388  loss_dice_2: 4.469  loss_ce_3: 0.4223  loss_mask_3: 0.5398  loss_dice_3: 4.473  loss_ce_4: 0.4304  loss_mask_4: 0.5414  loss_dice_4: 4.461  loss_ce_5: 0.4215  loss_mask_5: 0.5422  loss_dice_5: 4.46  loss_ce_6: 0.4217  loss_mask_6: 0.5444  loss_dice_6: 4.458  loss_ce_7: 0.424  loss_mask_7: 0.5409  loss_dice_7: 4.467  loss_ce_8: 0.4234  loss_mask_8: 0.5408  loss_dice_8: 4.467  time: 1.5692  data_time: 0.0889  lr: 9.653e-06  max_mem: 21212M
[01/17 19:12:20] d2.utils.events INFO:  eta: 16:27:41  iter: 1559  total_loss: 55.12  loss_ce: 0.4388  loss_mask: 0.5545  loss_dice: 4.427  loss_ce_0: 1.3  loss_mask_0: 0.5386  loss_dice_0: 4.479  loss_ce_1: 0.4793  loss_mask_1: 0.5451  loss_dice_1: 4.444  loss_ce_2: 0.4416  loss_mask_2: 0.5441  loss_dice_2: 4.443  loss_ce_3: 0.4305  loss_mask_3: 0.548  loss_dice_3: 4.423  loss_ce_4: 0.4307  loss_mask_4: 0.5469  loss_dice_4: 4.428  loss_ce_5: 0.4335  loss_mask_5: 0.5527  loss_dice_5: 4.425  loss_ce_6: 0.4346  loss_mask_6: 0.5498  loss_dice_6: 4.426  loss_ce_7: 0.4154  loss_mask_7: 0.5537  loss_dice_7: 4.429  loss_ce_8: 0.4203  loss_mask_8: 0.5536  loss_dice_8: 4.427  time: 1.5687  data_time: 0.0852  lr: 9.6485e-06  max_mem: 21212M
[01/17 19:12:51] d2.utils.events INFO:  eta: 16:27:18  iter: 1579  total_loss: 55  loss_ce: 0.4116  loss_mask: 0.5565  loss_dice: 4.439  loss_ce_0: 1.306  loss_mask_0: 0.5344  loss_dice_0: 4.501  loss_ce_1: 0.4446  loss_mask_1: 0.544  loss_dice_1: 4.462  loss_ce_2: 0.4302  loss_mask_2: 0.549  loss_dice_2: 4.454  loss_ce_3: 0.4036  loss_mask_3: 0.5502  loss_dice_3: 4.444  loss_ce_4: 0.399  loss_mask_4: 0.5496  loss_dice_4: 4.448  loss_ce_5: 0.4136  loss_mask_5: 0.5504  loss_dice_5: 4.443  loss_ce_6: 0.4105  loss_mask_6: 0.5557  loss_dice_6: 4.439  loss_ce_7: 0.3973  loss_mask_7: 0.5556  loss_dice_7: 4.443  loss_ce_8: 0.4072  loss_mask_8: 0.5561  loss_dice_8: 4.434  time: 1.5684  data_time: 0.0870  lr: 9.644e-06  max_mem: 21212M
[01/17 19:13:22] d2.utils.events INFO:  eta: 16:26:47  iter: 1599  total_loss: 55.27  loss_ce: 0.4413  loss_mask: 0.5561  loss_dice: 4.432  loss_ce_0: 1.268  loss_mask_0: 0.5385  loss_dice_0: 4.489  loss_ce_1: 0.4846  loss_mask_1: 0.5446  loss_dice_1: 4.45  loss_ce_2: 0.4826  loss_mask_2: 0.5485  loss_dice_2: 4.437  loss_ce_3: 0.4683  loss_mask_3: 0.552  loss_dice_3: 4.434  loss_ce_4: 0.4632  loss_mask_4: 0.5542  loss_dice_4: 4.428  loss_ce_5: 0.462  loss_mask_5: 0.5494  loss_dice_5: 4.431  loss_ce_6: 0.452  loss_mask_6: 0.5515  loss_dice_6: 4.434  loss_ce_7: 0.4443  loss_mask_7: 0.5533  loss_dice_7: 4.434  loss_ce_8: 0.4439  loss_mask_8: 0.5537  loss_dice_8: 4.429  time: 1.5680  data_time: 0.0881  lr: 9.6395e-06  max_mem: 21212M
[01/17 19:13:52] d2.utils.events INFO:  eta: 16:26:07  iter: 1619  total_loss: 55.01  loss_ce: 0.4619  loss_mask: 0.5533  loss_dice: 4.429  loss_ce_0: 1.268  loss_mask_0: 0.5435  loss_dice_0: 4.482  loss_ce_1: 0.4906  loss_mask_1: 0.5474  loss_dice_1: 4.449  loss_ce_2: 0.4661  loss_mask_2: 0.553  loss_dice_2: 4.438  loss_ce_3: 0.4383  loss_mask_3: 0.5544  loss_dice_3: 4.434  loss_ce_4: 0.444  loss_mask_4: 0.5557  loss_dice_4: 4.43  loss_ce_5: 0.4464  loss_mask_5: 0.5551  loss_dice_5: 4.433  loss_ce_6: 0.4438  loss_mask_6: 0.5563  loss_dice_6: 4.428  loss_ce_7: 0.4464  loss_mask_7: 0.5547  loss_dice_7: 4.434  loss_ce_8: 0.459  loss_mask_8: 0.5536  loss_dice_8: 4.432  time: 1.5675  data_time: 0.0943  lr: 9.635e-06  max_mem: 21212M
[01/17 19:14:23] d2.utils.events INFO:  eta: 16:24:59  iter: 1639  total_loss: 54.93  loss_ce: 0.4197  loss_mask: 0.5403  loss_dice: 4.425  loss_ce_0: 1.262  loss_mask_0: 0.5311  loss_dice_0: 4.486  loss_ce_1: 0.4576  loss_mask_1: 0.5351  loss_dice_1: 4.451  loss_ce_2: 0.4591  loss_mask_2: 0.5357  loss_dice_2: 4.438  loss_ce_3: 0.4338  loss_mask_3: 0.5365  loss_dice_3: 4.432  loss_ce_4: 0.4348  loss_mask_4: 0.5342  loss_dice_4: 4.425  loss_ce_5: 0.4275  loss_mask_5: 0.5375  loss_dice_5: 4.422  loss_ce_6: 0.4392  loss_mask_6: 0.5386  loss_dice_6: 4.429  loss_ce_7: 0.4123  loss_mask_7: 0.5379  loss_dice_7: 4.425  loss_ce_8: 0.4131  loss_mask_8: 0.539  loss_dice_8: 4.421  time: 1.5668  data_time: 0.0781  lr: 9.6305e-06  max_mem: 21212M
[01/17 19:14:53] d2.utils.events INFO:  eta: 16:24:41  iter: 1659  total_loss: 54.94  loss_ce: 0.4556  loss_mask: 0.5425  loss_dice: 4.41  loss_ce_0: 1.256  loss_mask_0: 0.5302  loss_dice_0: 4.458  loss_ce_1: 0.4905  loss_mask_1: 0.5341  loss_dice_1: 4.431  loss_ce_2: 0.4663  loss_mask_2: 0.5371  loss_dice_2: 4.42  loss_ce_3: 0.4608  loss_mask_3: 0.5394  loss_dice_3: 4.412  loss_ce_4: 0.4506  loss_mask_4: 0.5414  loss_dice_4: 4.41  loss_ce_5: 0.4615  loss_mask_5: 0.5413  loss_dice_5: 4.404  loss_ce_6: 0.4458  loss_mask_6: 0.5417  loss_dice_6: 4.416  loss_ce_7: 0.457  loss_mask_7: 0.5441  loss_dice_7: 4.412  loss_ce_8: 0.4403  loss_mask_8: 0.5435  loss_dice_8: 4.41  time: 1.5663  data_time: 0.0856  lr: 9.6259e-06  max_mem: 21212M
[01/17 19:15:24] d2.utils.events INFO:  eta: 16:23:59  iter: 1679  total_loss: 55.11  loss_ce: 0.4333  loss_mask: 0.5456  loss_dice: 4.441  loss_ce_0: 1.208  loss_mask_0: 0.5387  loss_dice_0: 4.494  loss_ce_1: 0.4817  loss_mask_1: 0.5409  loss_dice_1: 4.453  loss_ce_2: 0.4467  loss_mask_2: 0.5409  loss_dice_2: 4.452  loss_ce_3: 0.4353  loss_mask_3: 0.5427  loss_dice_3: 4.437  loss_ce_4: 0.4348  loss_mask_4: 0.5439  loss_dice_4: 4.433  loss_ce_5: 0.4175  loss_mask_5: 0.5454  loss_dice_5: 4.436  loss_ce_6: 0.4219  loss_mask_6: 0.5438  loss_dice_6: 4.437  loss_ce_7: 0.411  loss_mask_7: 0.5463  loss_dice_7: 4.447  loss_ce_8: 0.4278  loss_mask_8: 0.5476  loss_dice_8: 4.442  time: 1.5657  data_time: 0.0864  lr: 9.6214e-06  max_mem: 21212M
[01/17 19:15:54] d2.utils.events INFO:  eta: 16:22:58  iter: 1699  total_loss: 55.02  loss_ce: 0.4423  loss_mask: 0.5596  loss_dice: 4.419  loss_ce_0: 1.207  loss_mask_0: 0.5433  loss_dice_0: 4.483  loss_ce_1: 0.4732  loss_mask_1: 0.5507  loss_dice_1: 4.444  loss_ce_2: 0.4553  loss_mask_2: 0.5521  loss_dice_2: 4.433  loss_ce_3: 0.4469  loss_mask_3: 0.5525  loss_dice_3: 4.425  loss_ce_4: 0.4375  loss_mask_4: 0.5559  loss_dice_4: 4.414  loss_ce_5: 0.4431  loss_mask_5: 0.5604  loss_dice_5: 4.415  loss_ce_6: 0.4282  loss_mask_6: 0.5593  loss_dice_6: 4.416  loss_ce_7: 0.4226  loss_mask_7: 0.5615  loss_dice_7: 4.418  loss_ce_8: 0.4195  loss_mask_8: 0.561  loss_dice_8: 4.415  time: 1.5652  data_time: 0.0871  lr: 9.6169e-06  max_mem: 21212M
[01/17 19:16:25] d2.utils.events INFO:  eta: 16:21:57  iter: 1719  total_loss: 54.98  loss_ce: 0.4203  loss_mask: 0.5485  loss_dice: 4.419  loss_ce_0: 1.199  loss_mask_0: 0.5346  loss_dice_0: 4.475  loss_ce_1: 0.46  loss_mask_1: 0.5375  loss_dice_1: 4.438  loss_ce_2: 0.4387  loss_mask_2: 0.5409  loss_dice_2: 4.427  loss_ce_3: 0.4363  loss_mask_3: 0.5427  loss_dice_3: 4.419  loss_ce_4: 0.4371  loss_mask_4: 0.5429  loss_dice_4: 4.42  loss_ce_5: 0.4339  loss_mask_5: 0.5475  loss_dice_5: 4.419  loss_ce_6: 0.4266  loss_mask_6: 0.5465  loss_dice_6: 4.413  loss_ce_7: 0.4083  loss_mask_7: 0.5496  loss_dice_7: 4.42  loss_ce_8: 0.4164  loss_mask_8: 0.5504  loss_dice_8: 4.416  time: 1.5648  data_time: 0.0834  lr: 9.6124e-06  max_mem: 21212M
[01/17 19:16:54] d2.utils.events INFO:  eta: 16:19:57  iter: 1739  total_loss: 55.13  loss_ce: 0.4345  loss_mask: 0.5713  loss_dice: 4.417  loss_ce_0: 1.212  loss_mask_0: 0.5525  loss_dice_0: 4.467  loss_ce_1: 0.4706  loss_mask_1: 0.5628  loss_dice_1: 4.433  loss_ce_2: 0.4419  loss_mask_2: 0.5637  loss_dice_2: 4.429  loss_ce_3: 0.4447  loss_mask_3: 0.567  loss_dice_3: 4.416  loss_ce_4: 0.4342  loss_mask_4: 0.5676  loss_dice_4: 4.415  loss_ce_5: 0.4426  loss_mask_5: 0.5682  loss_dice_5: 4.42  loss_ce_6: 0.4368  loss_mask_6: 0.5684  loss_dice_6: 4.416  loss_ce_7: 0.4347  loss_mask_7: 0.5671  loss_dice_7: 4.419  loss_ce_8: 0.4341  loss_mask_8: 0.5692  loss_dice_8: 4.42  time: 1.5639  data_time: 0.0781  lr: 9.6079e-06  max_mem: 21212M
[01/17 19:17:25] d2.utils.events INFO:  eta: 16:18:55  iter: 1759  total_loss: 54.85  loss_ce: 0.4305  loss_mask: 0.5647  loss_dice: 4.414  loss_ce_0: 1.169  loss_mask_0: 0.547  loss_dice_0: 4.462  loss_ce_1: 0.4868  loss_mask_1: 0.5517  loss_dice_1: 4.434  loss_ce_2: 0.4475  loss_mask_2: 0.5541  loss_dice_2: 4.415  loss_ce_3: 0.4532  loss_mask_3: 0.5552  loss_dice_3: 4.417  loss_ce_4: 0.4341  loss_mask_4: 0.5586  loss_dice_4: 4.408  loss_ce_5: 0.4449  loss_mask_5: 0.5624  loss_dice_5: 4.407  loss_ce_6: 0.4504  loss_mask_6: 0.5592  loss_dice_6: 4.402  loss_ce_7: 0.4321  loss_mask_7: 0.5609  loss_dice_7: 4.41  loss_ce_8: 0.4357  loss_mask_8: 0.5622  loss_dice_8: 4.407  time: 1.5632  data_time: 0.0803  lr: 9.6033e-06  max_mem: 21212M
[01/17 19:17:55] d2.utils.events INFO:  eta: 16:17:17  iter: 1779  total_loss: 54.59  loss_ce: 0.4311  loss_mask: 0.5501  loss_dice: 4.405  loss_ce_0: 1.152  loss_mask_0: 0.5325  loss_dice_0: 4.459  loss_ce_1: 0.4455  loss_mask_1: 0.5437  loss_dice_1: 4.43  loss_ce_2: 0.4196  loss_mask_2: 0.5471  loss_dice_2: 4.423  loss_ce_3: 0.411  loss_mask_3: 0.5485  loss_dice_3: 4.406  loss_ce_4: 0.4205  loss_mask_4: 0.551  loss_dice_4: 4.402  loss_ce_5: 0.4219  loss_mask_5: 0.5475  loss_dice_5: 4.414  loss_ce_6: 0.428  loss_mask_6: 0.5487  loss_dice_6: 4.409  loss_ce_7: 0.4005  loss_mask_7: 0.5523  loss_dice_7: 4.409  loss_ce_8: 0.4048  loss_mask_8: 0.5498  loss_dice_8: 4.408  time: 1.5627  data_time: 0.0770  lr: 9.5988e-06  max_mem: 21212M
[01/17 19:18:25] d2.utils.events INFO:  eta: 16:16:21  iter: 1799  total_loss: 54.43  loss_ce: 0.4093  loss_mask: 0.5567  loss_dice: 4.375  loss_ce_0: 1.163  loss_mask_0: 0.5371  loss_dice_0: 4.444  loss_ce_1: 0.457  loss_mask_1: 0.5435  loss_dice_1: 4.407  loss_ce_2: 0.4211  loss_mask_2: 0.549  loss_dice_2: 4.388  loss_ce_3: 0.4195  loss_mask_3: 0.5527  loss_dice_3: 4.384  loss_ce_4: 0.4096  loss_mask_4: 0.5508  loss_dice_4: 4.375  loss_ce_5: 0.4223  loss_mask_5: 0.5515  loss_dice_5: 4.385  loss_ce_6: 0.4007  loss_mask_6: 0.5543  loss_dice_6: 4.381  loss_ce_7: 0.4065  loss_mask_7: 0.5559  loss_dice_7: 4.382  loss_ce_8: 0.3982  loss_mask_8: 0.5572  loss_dice_8: 4.374  time: 1.5622  data_time: 0.0794  lr: 9.5943e-06  max_mem: 21212M
[01/17 19:18:56] d2.utils.events INFO:  eta: 16:15:47  iter: 1819  total_loss: 54.58  loss_ce: 0.4259  loss_mask: 0.5537  loss_dice: 4.389  loss_ce_0: 1.142  loss_mask_0: 0.5453  loss_dice_0: 4.452  loss_ce_1: 0.4718  loss_mask_1: 0.5485  loss_dice_1: 4.408  loss_ce_2: 0.4346  loss_mask_2: 0.5471  loss_dice_2: 4.399  loss_ce_3: 0.4276  loss_mask_3: 0.5477  loss_dice_3: 4.388  loss_ce_4: 0.4329  loss_mask_4: 0.5492  loss_dice_4: 4.388  loss_ce_5: 0.4313  loss_mask_5: 0.549  loss_dice_5: 4.393  loss_ce_6: 0.4206  loss_mask_6: 0.5513  loss_dice_6: 4.393  loss_ce_7: 0.4067  loss_mask_7: 0.5503  loss_dice_7: 4.396  loss_ce_8: 0.4161  loss_mask_8: 0.5519  loss_dice_8: 4.39  time: 1.5616  data_time: 0.0804  lr: 9.5898e-06  max_mem: 21212M
[01/17 19:19:25] d2.utils.events INFO:  eta: 16:14:50  iter: 1839  total_loss: 54.83  loss_ce: 0.4487  loss_mask: 0.5509  loss_dice: 4.392  loss_ce_0: 1.151  loss_mask_0: 0.537  loss_dice_0: 4.447  loss_ce_1: 0.487  loss_mask_1: 0.5422  loss_dice_1: 4.412  loss_ce_2: 0.4653  loss_mask_2: 0.544  loss_dice_2: 4.405  loss_ce_3: 0.466  loss_mask_3: 0.5466  loss_dice_3: 4.393  loss_ce_4: 0.4637  loss_mask_4: 0.5489  loss_dice_4: 4.389  loss_ce_5: 0.4651  loss_mask_5: 0.5518  loss_dice_5: 4.392  loss_ce_6: 0.4518  loss_mask_6: 0.5514  loss_dice_6: 4.388  loss_ce_7: 0.4376  loss_mask_7: 0.5524  loss_dice_7: 4.389  loss_ce_8: 0.4434  loss_mask_8: 0.5515  loss_dice_8: 4.394  time: 1.5608  data_time: 0.0798  lr: 9.5853e-06  max_mem: 21212M
[01/17 19:19:57] d2.utils.events INFO:  eta: 16:14:19  iter: 1859  total_loss: 54.75  loss_ce: 0.4575  loss_mask: 0.5449  loss_dice: 4.386  loss_ce_0: 1.162  loss_mask_0: 0.5331  loss_dice_0: 4.452  loss_ce_1: 0.4981  loss_mask_1: 0.5413  loss_dice_1: 4.417  loss_ce_2: 0.4689  loss_mask_2: 0.5414  loss_dice_2: 4.401  loss_ce_3: 0.4676  loss_mask_3: 0.5448  loss_dice_3: 4.389  loss_ce_4: 0.4636  loss_mask_4: 0.5432  loss_dice_4: 4.39  loss_ce_5: 0.4671  loss_mask_5: 0.5438  loss_dice_5: 4.383  loss_ce_6: 0.4693  loss_mask_6: 0.5444  loss_dice_6: 4.387  loss_ce_7: 0.4501  loss_mask_7: 0.5461  loss_dice_7: 4.395  loss_ce_8: 0.454  loss_mask_8: 0.5468  loss_dice_8: 4.389  time: 1.5607  data_time: 0.0900  lr: 9.5807e-06  max_mem: 21212M
[01/17 19:20:26] d2.utils.events INFO:  eta: 16:13:18  iter: 1879  total_loss: 54.48  loss_ce: 0.4044  loss_mask: 0.5566  loss_dice: 4.386  loss_ce_0: 1.073  loss_mask_0: 0.5468  loss_dice_0: 4.452  loss_ce_1: 0.4545  loss_mask_1: 0.552  loss_dice_1: 4.412  loss_ce_2: 0.4325  loss_mask_2: 0.5578  loss_dice_2: 4.392  loss_ce_3: 0.4214  loss_mask_3: 0.5593  loss_dice_3: 4.391  loss_ce_4: 0.4134  loss_mask_4: 0.5576  loss_dice_4: 4.384  loss_ce_5: 0.4138  loss_mask_5: 0.5566  loss_dice_5: 4.387  loss_ce_6: 0.4101  loss_mask_6: 0.5567  loss_dice_6: 4.38  loss_ce_7: 0.4035  loss_mask_7: 0.5578  loss_dice_7: 4.383  loss_ce_8: 0.3986  loss_mask_8: 0.5548  loss_dice_8: 4.39  time: 1.5598  data_time: 0.0777  lr: 9.5762e-06  max_mem: 21212M
[01/17 19:20:56] d2.utils.events INFO:  eta: 16:12:26  iter: 1899  total_loss: 54.54  loss_ce: 0.4572  loss_mask: 0.5402  loss_dice: 4.381  loss_ce_0: 1.106  loss_mask_0: 0.528  loss_dice_0: 4.447  loss_ce_1: 0.4842  loss_mask_1: 0.5385  loss_dice_1: 4.406  loss_ce_2: 0.4597  loss_mask_2: 0.5453  loss_dice_2: 4.389  loss_ce_3: 0.4614  loss_mask_3: 0.5427  loss_dice_3: 4.383  loss_ce_4: 0.4554  loss_mask_4: 0.5427  loss_dice_4: 4.385  loss_ce_5: 0.4552  loss_mask_5: 0.5406  loss_dice_5: 4.386  loss_ce_6: 0.4573  loss_mask_6: 0.5403  loss_dice_6: 4.378  loss_ce_7: 0.4515  loss_mask_7: 0.5415  loss_dice_7: 4.385  loss_ce_8: 0.4565  loss_mask_8: 0.544  loss_dice_8: 4.382  time: 1.5590  data_time: 0.0721  lr: 9.5717e-06  max_mem: 21212M
[01/17 19:21:26] d2.utils.events INFO:  eta: 16:11:23  iter: 1919  total_loss: 54.65  loss_ce: 0.4208  loss_mask: 0.5604  loss_dice: 4.405  loss_ce_0: 1.091  loss_mask_0: 0.5519  loss_dice_0: 4.453  loss_ce_1: 0.4567  loss_mask_1: 0.5521  loss_dice_1: 4.422  loss_ce_2: 0.4521  loss_mask_2: 0.5567  loss_dice_2: 4.41  loss_ce_3: 0.434  loss_mask_3: 0.5574  loss_dice_3: 4.41  loss_ce_4: 0.4345  loss_mask_4: 0.5588  loss_dice_4: 4.416  loss_ce_5: 0.431  loss_mask_5: 0.559  loss_dice_5: 4.406  loss_ce_6: 0.4213  loss_mask_6: 0.5592  loss_dice_6: 4.408  loss_ce_7: 0.4237  loss_mask_7: 0.5603  loss_dice_7: 4.411  loss_ce_8: 0.4232  loss_mask_8: 0.5649  loss_dice_8: 4.4  time: 1.5585  data_time: 0.0856  lr: 9.5672e-06  max_mem: 21212M
[01/17 19:21:56] d2.utils.events INFO:  eta: 16:09:56  iter: 1939  total_loss: 54.44  loss_ce: 0.4213  loss_mask: 0.5566  loss_dice: 4.38  loss_ce_0: 1.067  loss_mask_0: 0.536  loss_dice_0: 4.44  loss_ce_1: 0.4658  loss_mask_1: 0.5522  loss_dice_1: 4.399  loss_ce_2: 0.4394  loss_mask_2: 0.5467  loss_dice_2: 4.397  loss_ce_3: 0.4348  loss_mask_3: 0.551  loss_dice_3: 4.389  loss_ce_4: 0.4405  loss_mask_4: 0.5513  loss_dice_4: 4.384  loss_ce_5: 0.4428  loss_mask_5: 0.5526  loss_dice_5: 4.387  loss_ce_6: 0.4329  loss_mask_6: 0.5569  loss_dice_6: 4.381  loss_ce_7: 0.4167  loss_mask_7: 0.5555  loss_dice_7: 4.385  loss_ce_8: 0.4208  loss_mask_8: 0.5544  loss_dice_8: 4.382  time: 1.5579  data_time: 0.0836  lr: 9.5626e-06  max_mem: 21212M
[01/17 19:22:26] d2.utils.events INFO:  eta: 16:08:32  iter: 1959  total_loss: 54.27  loss_ce: 0.4253  loss_mask: 0.5677  loss_dice: 4.369  loss_ce_0: 1.088  loss_mask_0: 0.5542  loss_dice_0: 4.432  loss_ce_1: 0.467  loss_mask_1: 0.555  loss_dice_1: 4.4  loss_ce_2: 0.4323  loss_mask_2: 0.5598  loss_dice_2: 4.387  loss_ce_3: 0.4211  loss_mask_3: 0.562  loss_dice_3: 4.374  loss_ce_4: 0.4392  loss_mask_4: 0.5632  loss_dice_4: 4.368  loss_ce_5: 0.4379  loss_mask_5: 0.563  loss_dice_5: 4.369  loss_ce_6: 0.4363  loss_mask_6: 0.5613  loss_dice_6: 4.369  loss_ce_7: 0.4178  loss_mask_7: 0.5623  loss_dice_7: 4.373  loss_ce_8: 0.4239  loss_mask_8: 0.5656  loss_dice_8: 4.365  time: 1.5572  data_time: 0.0888  lr: 9.5581e-06  max_mem: 21212M
[01/17 19:22:56] d2.utils.events INFO:  eta: 16:06:30  iter: 1979  total_loss: 54.04  loss_ce: 0.4162  loss_mask: 0.5596  loss_dice: 4.351  loss_ce_0: 1.061  loss_mask_0: 0.5405  loss_dice_0: 4.418  loss_ce_1: 0.4626  loss_mask_1: 0.552  loss_dice_1: 4.376  loss_ce_2: 0.4341  loss_mask_2: 0.5521  loss_dice_2: 4.367  loss_ce_3: 0.4236  loss_mask_3: 0.5579  loss_dice_3: 4.361  loss_ce_4: 0.4236  loss_mask_4: 0.5588  loss_dice_4: 4.357  loss_ce_5: 0.4293  loss_mask_5: 0.5558  loss_dice_5: 4.349  loss_ce_6: 0.4159  loss_mask_6: 0.5596  loss_dice_6: 4.352  loss_ce_7: 0.4166  loss_mask_7: 0.5599  loss_dice_7: 4.354  loss_ce_8: 0.4153  loss_mask_8: 0.5597  loss_dice_8: 4.352  time: 1.5565  data_time: 0.0672  lr: 9.5536e-06  max_mem: 21212M
[01/17 19:23:25] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 19:23:26] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 19:23:26] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 19:23:26] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 19:23:39] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0058 s/iter. Inference: 0.1883 s/iter. Eval: 0.1440 s/iter. Total: 0.3381 s/iter. ETA=0:06:05
[01/17 19:23:44] d2.evaluation.evaluator INFO: Inference done 26/1093. Dataloading: 0.0088 s/iter. Inference: 0.1745 s/iter. Eval: 0.1627 s/iter. Total: 0.3460 s/iter. ETA=0:06:09
[01/17 19:23:50] d2.evaluation.evaluator INFO: Inference done 42/1093. Dataloading: 0.0087 s/iter. Inference: 0.1682 s/iter. Eval: 0.1615 s/iter. Total: 0.3385 s/iter. ETA=0:05:55
[01/17 19:23:55] d2.evaluation.evaluator INFO: Inference done 58/1093. Dataloading: 0.0093 s/iter. Inference: 0.1609 s/iter. Eval: 0.1635 s/iter. Total: 0.3338 s/iter. ETA=0:05:45
[01/17 19:24:00] d2.evaluation.evaluator INFO: Inference done 73/1093. Dataloading: 0.0092 s/iter. Inference: 0.1598 s/iter. Eval: 0.1646 s/iter. Total: 0.3337 s/iter. ETA=0:05:40
[01/17 19:24:05] d2.evaluation.evaluator INFO: Inference done 88/1093. Dataloading: 0.0096 s/iter. Inference: 0.1609 s/iter. Eval: 0.1671 s/iter. Total: 0.3377 s/iter. ETA=0:05:39
[01/17 19:24:10] d2.evaluation.evaluator INFO: Inference done 104/1093. Dataloading: 0.0095 s/iter. Inference: 0.1611 s/iter. Eval: 0.1631 s/iter. Total: 0.3339 s/iter. ETA=0:05:30
[01/17 19:24:15] d2.evaluation.evaluator INFO: Inference done 119/1093. Dataloading: 0.0093 s/iter. Inference: 0.1616 s/iter. Eval: 0.1638 s/iter. Total: 0.3348 s/iter. ETA=0:05:26
[01/17 19:24:21] d2.evaluation.evaluator INFO: Inference done 135/1093. Dataloading: 0.0091 s/iter. Inference: 0.1601 s/iter. Eval: 0.1645 s/iter. Total: 0.3339 s/iter. ETA=0:05:19
[01/17 19:24:26] d2.evaluation.evaluator INFO: Inference done 152/1093. Dataloading: 0.0091 s/iter. Inference: 0.1603 s/iter. Eval: 0.1615 s/iter. Total: 0.3310 s/iter. ETA=0:05:11
[01/17 19:24:31] d2.evaluation.evaluator INFO: Inference done 168/1093. Dataloading: 0.0091 s/iter. Inference: 0.1595 s/iter. Eval: 0.1615 s/iter. Total: 0.3301 s/iter. ETA=0:05:05
[01/17 19:24:36] d2.evaluation.evaluator INFO: Inference done 183/1093. Dataloading: 0.0092 s/iter. Inference: 0.1594 s/iter. Eval: 0.1617 s/iter. Total: 0.3304 s/iter. ETA=0:05:00
[01/17 19:24:41] d2.evaluation.evaluator INFO: Inference done 199/1093. Dataloading: 0.0092 s/iter. Inference: 0.1596 s/iter. Eval: 0.1610 s/iter. Total: 0.3298 s/iter. ETA=0:04:54
[01/17 19:24:46] d2.evaluation.evaluator INFO: Inference done 214/1093. Dataloading: 0.0093 s/iter. Inference: 0.1599 s/iter. Eval: 0.1610 s/iter. Total: 0.3303 s/iter. ETA=0:04:50
[01/17 19:24:51] d2.evaluation.evaluator INFO: Inference done 230/1093. Dataloading: 0.0094 s/iter. Inference: 0.1601 s/iter. Eval: 0.1608 s/iter. Total: 0.3304 s/iter. ETA=0:04:45
[01/17 19:24:57] d2.evaluation.evaluator INFO: Inference done 246/1093. Dataloading: 0.0093 s/iter. Inference: 0.1594 s/iter. Eval: 0.1611 s/iter. Total: 0.3299 s/iter. ETA=0:04:39
[01/17 19:25:02] d2.evaluation.evaluator INFO: Inference done 262/1093. Dataloading: 0.0093 s/iter. Inference: 0.1587 s/iter. Eval: 0.1610 s/iter. Total: 0.3291 s/iter. ETA=0:04:33
[01/17 19:25:07] d2.evaluation.evaluator INFO: Inference done 278/1093. Dataloading: 0.0093 s/iter. Inference: 0.1585 s/iter. Eval: 0.1609 s/iter. Total: 0.3288 s/iter. ETA=0:04:27
[01/17 19:25:12] d2.evaluation.evaluator INFO: Inference done 294/1093. Dataloading: 0.0092 s/iter. Inference: 0.1585 s/iter. Eval: 0.1602 s/iter. Total: 0.3280 s/iter. ETA=0:04:22
[01/17 19:25:17] d2.evaluation.evaluator INFO: Inference done 309/1093. Dataloading: 0.0093 s/iter. Inference: 0.1589 s/iter. Eval: 0.1606 s/iter. Total: 0.3289 s/iter. ETA=0:04:17
[01/17 19:25:22] d2.evaluation.evaluator INFO: Inference done 324/1093. Dataloading: 0.0094 s/iter. Inference: 0.1585 s/iter. Eval: 0.1615 s/iter. Total: 0.3294 s/iter. ETA=0:04:13
[01/17 19:25:27] d2.evaluation.evaluator INFO: Inference done 341/1093. Dataloading: 0.0093 s/iter. Inference: 0.1581 s/iter. Eval: 0.1602 s/iter. Total: 0.3277 s/iter. ETA=0:04:06
[01/17 19:25:32] d2.evaluation.evaluator INFO: Inference done 357/1093. Dataloading: 0.0092 s/iter. Inference: 0.1592 s/iter. Eval: 0.1591 s/iter. Total: 0.3276 s/iter. ETA=0:04:01
[01/17 19:25:38] d2.evaluation.evaluator INFO: Inference done 373/1093. Dataloading: 0.0092 s/iter. Inference: 0.1588 s/iter. Eval: 0.1594 s/iter. Total: 0.3274 s/iter. ETA=0:03:55
[01/17 19:25:43] d2.evaluation.evaluator INFO: Inference done 390/1093. Dataloading: 0.0092 s/iter. Inference: 0.1584 s/iter. Eval: 0.1591 s/iter. Total: 0.3268 s/iter. ETA=0:03:49
[01/17 19:25:48] d2.evaluation.evaluator INFO: Inference done 405/1093. Dataloading: 0.0093 s/iter. Inference: 0.1590 s/iter. Eval: 0.1591 s/iter. Total: 0.3275 s/iter. ETA=0:03:45
[01/17 19:25:53] d2.evaluation.evaluator INFO: Inference done 421/1093. Dataloading: 0.0093 s/iter. Inference: 0.1588 s/iter. Eval: 0.1596 s/iter. Total: 0.3277 s/iter. ETA=0:03:40
[01/17 19:25:59] d2.evaluation.evaluator INFO: Inference done 437/1093. Dataloading: 0.0093 s/iter. Inference: 0.1585 s/iter. Eval: 0.1594 s/iter. Total: 0.3273 s/iter. ETA=0:03:34
[01/17 19:26:04] d2.evaluation.evaluator INFO: Inference done 453/1093. Dataloading: 0.0092 s/iter. Inference: 0.1581 s/iter. Eval: 0.1596 s/iter. Total: 0.3270 s/iter. ETA=0:03:29
[01/17 19:26:09] d2.evaluation.evaluator INFO: Inference done 470/1093. Dataloading: 0.0092 s/iter. Inference: 0.1573 s/iter. Eval: 0.1597 s/iter. Total: 0.3263 s/iter. ETA=0:03:23
[01/17 19:26:14] d2.evaluation.evaluator INFO: Inference done 487/1093. Dataloading: 0.0092 s/iter. Inference: 0.1568 s/iter. Eval: 0.1593 s/iter. Total: 0.3253 s/iter. ETA=0:03:17
[01/17 19:26:19] d2.evaluation.evaluator INFO: Inference done 504/1093. Dataloading: 0.0092 s/iter. Inference: 0.1567 s/iter. Eval: 0.1585 s/iter. Total: 0.3244 s/iter. ETA=0:03:11
[01/17 19:26:24] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0091 s/iter. Inference: 0.1566 s/iter. Eval: 0.1582 s/iter. Total: 0.3240 s/iter. ETA=0:03:05
[01/17 19:26:29] d2.evaluation.evaluator INFO: Inference done 536/1093. Dataloading: 0.0092 s/iter. Inference: 0.1565 s/iter. Eval: 0.1587 s/iter. Total: 0.3244 s/iter. ETA=0:03:00
[01/17 19:26:35] d2.evaluation.evaluator INFO: Inference done 551/1093. Dataloading: 0.0092 s/iter. Inference: 0.1568 s/iter. Eval: 0.1591 s/iter. Total: 0.3251 s/iter. ETA=0:02:56
[01/17 19:26:40] d2.evaluation.evaluator INFO: Inference done 567/1093. Dataloading: 0.0092 s/iter. Inference: 0.1569 s/iter. Eval: 0.1589 s/iter. Total: 0.3251 s/iter. ETA=0:02:51
[01/17 19:26:45] d2.evaluation.evaluator INFO: Inference done 585/1093. Dataloading: 0.0093 s/iter. Inference: 0.1569 s/iter. Eval: 0.1577 s/iter. Total: 0.3240 s/iter. ETA=0:02:44
[01/17 19:26:50] d2.evaluation.evaluator INFO: Inference done 600/1093. Dataloading: 0.0093 s/iter. Inference: 0.1575 s/iter. Eval: 0.1579 s/iter. Total: 0.3248 s/iter. ETA=0:02:40
[01/17 19:26:56] d2.evaluation.evaluator INFO: Inference done 615/1093. Dataloading: 0.0094 s/iter. Inference: 0.1572 s/iter. Eval: 0.1586 s/iter. Total: 0.3254 s/iter. ETA=0:02:35
[01/17 19:27:01] d2.evaluation.evaluator INFO: Inference done 632/1093. Dataloading: 0.0094 s/iter. Inference: 0.1570 s/iter. Eval: 0.1585 s/iter. Total: 0.3249 s/iter. ETA=0:02:29
[01/17 19:27:06] d2.evaluation.evaluator INFO: Inference done 648/1093. Dataloading: 0.0093 s/iter. Inference: 0.1569 s/iter. Eval: 0.1587 s/iter. Total: 0.3250 s/iter. ETA=0:02:24
[01/17 19:27:11] d2.evaluation.evaluator INFO: Inference done 662/1093. Dataloading: 0.0094 s/iter. Inference: 0.1572 s/iter. Eval: 0.1591 s/iter. Total: 0.3257 s/iter. ETA=0:02:20
[01/17 19:27:16] d2.evaluation.evaluator INFO: Inference done 678/1093. Dataloading: 0.0093 s/iter. Inference: 0.1572 s/iter. Eval: 0.1589 s/iter. Total: 0.3255 s/iter. ETA=0:02:15
[01/17 19:27:21] d2.evaluation.evaluator INFO: Inference done 694/1093. Dataloading: 0.0093 s/iter. Inference: 0.1578 s/iter. Eval: 0.1583 s/iter. Total: 0.3255 s/iter. ETA=0:02:09
[01/17 19:27:26] d2.evaluation.evaluator INFO: Inference done 708/1093. Dataloading: 0.0093 s/iter. Inference: 0.1582 s/iter. Eval: 0.1586 s/iter. Total: 0.3262 s/iter. ETA=0:02:05
[01/17 19:27:32] d2.evaluation.evaluator INFO: Inference done 724/1093. Dataloading: 0.0094 s/iter. Inference: 0.1583 s/iter. Eval: 0.1585 s/iter. Total: 0.3264 s/iter. ETA=0:02:00
[01/17 19:27:37] d2.evaluation.evaluator INFO: Inference done 741/1093. Dataloading: 0.0094 s/iter. Inference: 0.1583 s/iter. Eval: 0.1580 s/iter. Total: 0.3257 s/iter. ETA=0:01:54
[01/17 19:27:42] d2.evaluation.evaluator INFO: Inference done 756/1093. Dataloading: 0.0093 s/iter. Inference: 0.1583 s/iter. Eval: 0.1581 s/iter. Total: 0.3259 s/iter. ETA=0:01:49
[01/17 19:27:47] d2.evaluation.evaluator INFO: Inference done 771/1093. Dataloading: 0.0094 s/iter. Inference: 0.1584 s/iter. Eval: 0.1585 s/iter. Total: 0.3263 s/iter. ETA=0:01:45
[01/17 19:27:52] d2.evaluation.evaluator INFO: Inference done 787/1093. Dataloading: 0.0094 s/iter. Inference: 0.1581 s/iter. Eval: 0.1587 s/iter. Total: 0.3263 s/iter. ETA=0:01:39
[01/17 19:27:57] d2.evaluation.evaluator INFO: Inference done 804/1093. Dataloading: 0.0093 s/iter. Inference: 0.1580 s/iter. Eval: 0.1585 s/iter. Total: 0.3258 s/iter. ETA=0:01:34
[01/17 19:28:03] d2.evaluation.evaluator INFO: Inference done 819/1093. Dataloading: 0.0095 s/iter. Inference: 0.1581 s/iter. Eval: 0.1585 s/iter. Total: 0.3262 s/iter. ETA=0:01:29
[01/17 19:28:08] d2.evaluation.evaluator INFO: Inference done 835/1093. Dataloading: 0.0095 s/iter. Inference: 0.1581 s/iter. Eval: 0.1586 s/iter. Total: 0.3262 s/iter. ETA=0:01:24
[01/17 19:28:13] d2.evaluation.evaluator INFO: Inference done 851/1093. Dataloading: 0.0094 s/iter. Inference: 0.1580 s/iter. Eval: 0.1586 s/iter. Total: 0.3261 s/iter. ETA=0:01:18
[01/17 19:28:18] d2.evaluation.evaluator INFO: Inference done 866/1093. Dataloading: 0.0095 s/iter. Inference: 0.1582 s/iter. Eval: 0.1589 s/iter. Total: 0.3266 s/iter. ETA=0:01:14
[01/17 19:28:23] d2.evaluation.evaluator INFO: Inference done 881/1093. Dataloading: 0.0095 s/iter. Inference: 0.1584 s/iter. Eval: 0.1589 s/iter. Total: 0.3269 s/iter. ETA=0:01:09
[01/17 19:28:29] d2.evaluation.evaluator INFO: Inference done 895/1093. Dataloading: 0.0095 s/iter. Inference: 0.1585 s/iter. Eval: 0.1593 s/iter. Total: 0.3274 s/iter. ETA=0:01:04
[01/17 19:28:34] d2.evaluation.evaluator INFO: Inference done 911/1093. Dataloading: 0.0095 s/iter. Inference: 0.1585 s/iter. Eval: 0.1592 s/iter. Total: 0.3273 s/iter. ETA=0:00:59
[01/17 19:28:39] d2.evaluation.evaluator INFO: Inference done 926/1093. Dataloading: 0.0095 s/iter. Inference: 0.1586 s/iter. Eval: 0.1594 s/iter. Total: 0.3277 s/iter. ETA=0:00:54
[01/17 19:28:44] d2.evaluation.evaluator INFO: Inference done 942/1093. Dataloading: 0.0095 s/iter. Inference: 0.1586 s/iter. Eval: 0.1595 s/iter. Total: 0.3277 s/iter. ETA=0:00:49
[01/17 19:28:49] d2.evaluation.evaluator INFO: Inference done 957/1093. Dataloading: 0.0095 s/iter. Inference: 0.1586 s/iter. Eval: 0.1597 s/iter. Total: 0.3279 s/iter. ETA=0:00:44
[01/17 19:28:54] d2.evaluation.evaluator INFO: Inference done 973/1093. Dataloading: 0.0095 s/iter. Inference: 0.1586 s/iter. Eval: 0.1596 s/iter. Total: 0.3278 s/iter. ETA=0:00:39
[01/17 19:29:00] d2.evaluation.evaluator INFO: Inference done 991/1093. Dataloading: 0.0095 s/iter. Inference: 0.1585 s/iter. Eval: 0.1591 s/iter. Total: 0.3271 s/iter. ETA=0:00:33
[01/17 19:29:05] d2.evaluation.evaluator INFO: Inference done 1006/1093. Dataloading: 0.0094 s/iter. Inference: 0.1588 s/iter. Eval: 0.1591 s/iter. Total: 0.3274 s/iter. ETA=0:00:28
[01/17 19:29:10] d2.evaluation.evaluator INFO: Inference done 1022/1093. Dataloading: 0.0094 s/iter. Inference: 0.1588 s/iter. Eval: 0.1590 s/iter. Total: 0.3273 s/iter. ETA=0:00:23
[01/17 19:29:15] d2.evaluation.evaluator INFO: Inference done 1039/1093. Dataloading: 0.0094 s/iter. Inference: 0.1587 s/iter. Eval: 0.1589 s/iter. Total: 0.3271 s/iter. ETA=0:00:17
[01/17 19:29:21] d2.evaluation.evaluator INFO: Inference done 1055/1093. Dataloading: 0.0094 s/iter. Inference: 0.1587 s/iter. Eval: 0.1590 s/iter. Total: 0.3272 s/iter. ETA=0:00:12
[01/17 19:29:26] d2.evaluation.evaluator INFO: Inference done 1071/1093. Dataloading: 0.0094 s/iter. Inference: 0.1588 s/iter. Eval: 0.1588 s/iter. Total: 0.3270 s/iter. ETA=0:00:07
[01/17 19:29:31] d2.evaluation.evaluator INFO: Inference done 1087/1093. Dataloading: 0.0093 s/iter. Inference: 0.1588 s/iter. Eval: 0.1587 s/iter. Total: 0.3269 s/iter. ETA=0:00:01
[01/17 19:29:33] d2.evaluation.evaluator INFO: Total inference time: 0:05:56.156793 (0.327350 s / iter per device, on 4 devices)
[01/17 19:29:33] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:52 (0.158752 s / iter per device, on 4 devices)
[01/17 19:30:10] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 6.75634394233159, 'mIoU': 3.8356307033617227, 'fwIoU': 14.246557687282854, 'IoU-0': nan, 'IoU-1': 93.01490240146313, 'IoU-2': 33.29743294192854, 'IoU-3': 32.013424103317604, 'IoU-4': 25.06342957486317, 'IoU-5': 15.526740394114604, 'IoU-6': 8.366321257564293, 'IoU-7': 0.0, 'IoU-8': 3.5369272114791834, 'IoU-9': 0.3584028772800891, 'IoU-10': 13.899319405531665, 'IoU-11': 11.3056445960432, 'IoU-12': 14.317511513203932, 'IoU-13': 8.150065227652755, 'IoU-14': 6.7003542938885925, 'IoU-15': 6.919348144080292, 'IoU-16': 5.954350178046945, 'IoU-17': 5.834159588254658, 'IoU-18': 1.333434876841447, 'IoU-19': 6.35658995398371, 'IoU-20': 5.594802276801072, 'IoU-21': 3.111997144019421, 'IoU-22': 5.581941270076121, 'IoU-23': 2.255945984408277, 'IoU-24': 7.268024782642148, 'IoU-25': 4.308077192248598, 'IoU-26': 1.4507557114988412, 'IoU-27': 4.405756637969977, 'IoU-28': 7.405438681513115, 'IoU-29': 5.3047470373579255, 'IoU-30': 6.012969760219792, 'IoU-31': 5.720674408181507, 'IoU-32': 5.8050366477478645, 'IoU-33': 5.12880481542842, 'IoU-34': 5.962023896404393, 'IoU-35': 5.82986633459977, 'IoU-36': 5.790882856977139, 'IoU-37': 6.496336489675563, 'IoU-38': 5.1706362125345695, 'IoU-39': 5.6245582568238355, 'IoU-40': 4.439008497633608, 'IoU-41': 4.984336095874466, 'IoU-42': 6.139010298050091, 'IoU-43': 2.836891661129695, 'IoU-44': 4.767608092197646, 'IoU-45': 5.409125493921924, 'IoU-46': 3.200604581077932, 'IoU-47': 7.076079350445758, 'IoU-48': 3.994732918265962, 'IoU-49': 3.517525375617959, 'IoU-50': 5.546320155549077, 'IoU-51': 5.402067412139655, 'IoU-52': 4.721986176208754, 'IoU-53': 3.539070722748768, 'IoU-54': 3.4002192912458886, 'IoU-55': 5.425424769464784, 'IoU-56': 3.5735411979066045, 'IoU-57': 4.1123858284120125, 'IoU-58': 3.1483465314179253, 'IoU-59': 3.919165052677312, 'IoU-60': 4.251906653322127, 'IoU-61': 5.0687937383007675, 'IoU-62': 4.010888780390682, 'IoU-63': 3.602025777876245, 'IoU-64': 3.5478261165165703, 'IoU-65': 4.169119980462501, 'IoU-66': 3.2426732783061354, 'IoU-67': 2.6963149475932644, 'IoU-68': 4.381572389684051, 'IoU-69': 1.596740088452656, 'IoU-70': 2.8755358508471702, 'IoU-71': 5.859059366221246, 'IoU-72': 3.5745894038152404, 'IoU-73': 3.5986107325342056, 'IoU-74': 3.5704231917248497, 'IoU-75': 3.1365826458865786, 'IoU-76': 4.779654510894864, 'IoU-77': 4.674461243769552, 'IoU-78': 2.360741548743989, 'IoU-79': 4.686040043226038, 'IoU-80': 3.308484428271256, 'IoU-81': 4.067473167962594, 'IoU-82': 4.024279549882424, 'IoU-83': 4.396666596362947, 'IoU-84': 5.566937411018969, 'IoU-85': 3.0171514065432743, 'IoU-86': 4.850123862942437, 'IoU-87': 3.8162679319061783, 'IoU-88': 3.6353703382780727, 'IoU-89': 3.80170934459015, 'IoU-90': 4.116108960811336, 'IoU-91': 4.424107664751223, 'IoU-92': 4.145903255443099, 'IoU-93': 3.875650386051314, 'IoU-94': 3.302998547573334, 'IoU-95': 4.050353376363066, 'IoU-96': 2.7810356787836743, 'IoU-97': 3.028320001173896, 'IoU-98': 6.010836781811879, 'IoU-99': 3.076951219106598, 'IoU-100': 3.496747485470617, 'IoU-101': 2.1520776230202445, 'IoU-102': 4.393266568985608, 'IoU-103': 3.6906824121027695, 'IoU-104': 1.3173922869182728, 'IoU-105': 1.8502344561671986, 'IoU-106': 3.2331524830265397, 'IoU-107': 3.5975895726762355, 'IoU-108': 4.034630041015221, 'IoU-109': 4.0977049457306025, 'IoU-110': 2.3423225013356466, 'IoU-111': 1.189402899225107, 'IoU-112': 1.954060544373463, 'IoU-113': 4.03566601753939, 'IoU-114': 3.753132046389535, 'IoU-115': 2.131189681081232, 'IoU-116': 2.948098611778415, 'IoU-117': 2.3074221511052406, 'IoU-118': 0.27249634002306067, 'IoU-119': 2.054657949493222, 'IoU-120': 1.7135017259093492, 'IoU-121': 1.6839608056888384, 'IoU-122': 3.0406766919722528, 'IoU-123': 1.4601302205600424, 'IoU-124': 0.22356580719186822, 'IoU-125': 0.3315076615392443, 'IoU-126': 1.8334173227548867, 'IoU-127': 1.890605529591743, 'IoU-128': 2.460086360639318, 'IoU-129': 2.18363972376228, 'IoU-130': 2.8744496016714964, 'IoU-131': 0.2675443909756607, 'IoU-132': 0.06740018093026873, 'IoU-133': 1.925214352278088, 'IoU-134': 0.08286900651608507, 'IoU-135': 0.10602024174281446, 'IoU-136': 0.0, 'IoU-137': 0.10237957452429901, 'IoU-138': 2.6317436426862795, 'IoU-139': 0.03443707110285124, 'IoU-140': 2.147223643840478, 'IoU-141': 1.2065325703444698, 'IoU-142': 0.08769397755911959, 'IoU-143': 1.1777296945161817, 'IoU-144': 1.3438296359713022, 'IoU-145': 2.0164215107452517, 'IoU-146': 0.9253961369844044, 'IoU-147': 0.44636618882363027, 'IoU-148': 0.0004736081839494186, 'IoU-149': 0.0, 'IoU-150': 0.0, 'IoU-151': 0.02165084543398384, 'IoU-152': 0.1784735464450149, 'IoU-153': 0.0, 'IoU-154': 0.0, 'IoU-155': 0.0, 'IoU-156': 0.022539985935048776, 'IoU-157': 0.0, 'IoU-158': 0.0, 'IoU-159': 0.0, 'IoU-160': 0.0, 'IoU-161': 0.14700076666760983, 'IoU-162': 0.6827044495757215, 'IoU-163': 1.3616299916571504, 'IoU-164': 0.0, 'IoU-165': 0.0, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.0, 'IoU-169': 0.0, 'IoU-170': 0.0, 'IoU-171': 0.0, 'IoU-172': 0.0, 'IoU-173': 0.0, 'IoU-174': 0.0, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.0, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.7633466230620252, 'IoU-186': 0.0, 'IoU-187': 0.0, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 7.073557711693391, 'pACC': 19.402403670635284, 'ACC-0': nan, 'ACC-1': 98.44050462254819, 'ACC-2': 42.16368540475937, 'ACC-3': 50.24679319054144, 'ACC-4': 41.302764915781296, 'ACC-5': 23.73702542283273, 'ACC-6': 13.843545686115236, 'ACC-7': 0.0, 'ACC-8': 4.5836678448133235, 'ACC-9': 0.36984254356784696, 'ACC-10': 38.23931992653651, 'ACC-11': 18.857210077269404, 'ACC-12': 34.73777352923656, 'ACC-13': 15.762348864306485, 'ACC-14': 10.642624037448078, 'ACC-15': 13.49392620920266, 'ACC-16': 10.454464238400968, 'ACC-17': 13.3587328078108, 'ACC-18': 1.5594222199869836, 'ACC-19': 13.03908608233742, 'ACC-20': 12.687213514056777, 'ACC-21': 4.713384500946956, 'ACC-22': 11.326956545129766, 'ACC-23': 3.197603855230614, 'ACC-24': 23.616548279472248, 'ACC-25': 7.45544046659272, 'ACC-26': 1.8639458161297213, 'ACC-27': 8.346445348371338, 'ACC-28': 16.0980776407306, 'ACC-29': 11.012756361015711, 'ACC-30': 10.974330774159636, 'ACC-31': 11.421045104585371, 'ACC-32': 12.575899127103533, 'ACC-33': 7.853369551204971, 'ACC-34': 11.435229596373713, 'ACC-35': 10.400880327762454, 'ACC-36': 11.02015289362095, 'ACC-37': 14.461405625192755, 'ACC-38': 10.180818637963581, 'ACC-39': 11.975437794335733, 'ACC-40': 7.176725678914372, 'ACC-41': 10.705752451287214, 'ACC-42': 12.409863707318324, 'ACC-43': 4.049337995443059, 'ACC-44': 8.056273851718815, 'ACC-45': 11.500612239098336, 'ACC-46': 5.179404980648188, 'ACC-47': 16.75478883248095, 'ACC-48': 6.199547548225629, 'ACC-49': 5.458080340272932, 'ACC-50': 12.134077658737494, 'ACC-51': 10.933042288043936, 'ACC-52': 10.29224946536864, 'ACC-53': 6.9365165266646045, 'ACC-54': 5.0423619438902545, 'ACC-55': 12.571279756779855, 'ACC-56': 7.142735443849631, 'ACC-57': 6.628893187206655, 'ACC-58': 4.798044866173658, 'ACC-59': 7.444251741953909, 'ACC-60': 7.699695017503577, 'ACC-61': 10.28037890241036, 'ACC-62': 7.240883953500925, 'ACC-63': 6.114811920899203, 'ACC-64': 5.557663961071533, 'ACC-65': 9.328078176475737, 'ACC-66': 5.170437332969207, 'ACC-67': 3.74386630843023, 'ACC-68': 8.35884047096307, 'ACC-69': 2.0616692717824807, 'ACC-70': 4.668982263003678, 'ACC-71': 13.620312397493366, 'ACC-72': 6.096468251953099, 'ACC-73': 6.650526459082575, 'ACC-74': 6.3704544313266025, 'ACC-75': 4.401942038069799, 'ACC-76': 9.766426088057955, 'ACC-77': 12.505537253901949, 'ACC-78': 3.170157414758906, 'ACC-79': 8.12817096828483, 'ACC-80': 6.0548602595687315, 'ACC-81': 7.678758405658841, 'ACC-82': 6.599937578603367, 'ACC-83': 7.77225219722609, 'ACC-84': 13.000718043627701, 'ACC-85': 4.999831039873201, 'ACC-86': 9.747116514001352, 'ACC-87': 6.02565947171791, 'ACC-88': 6.5201724488546775, 'ACC-89': 7.055596635175518, 'ACC-90': 6.825783534570981, 'ACC-91': 10.757644579347764, 'ACC-92': 8.911141134056813, 'ACC-93': 6.091638502626213, 'ACC-94': 5.448092207536291, 'ACC-95': 11.713119630810686, 'ACC-96': 4.6627162057989935, 'ACC-97': 4.3813643576964845, 'ACC-98': 13.341640190215228, 'ACC-99': 5.709009593437863, 'ACC-100': 7.194641028198584, 'ACC-101': 3.316598489264423, 'ACC-102': 10.617263253285001, 'ACC-103': 8.06430055728724, 'ACC-104': 1.6386709623472124, 'ACC-105': 2.4319438056099894, 'ACC-106': 8.113360379091642, 'ACC-107': 7.376859696530596, 'ACC-108': 8.464506814701526, 'ACC-109': 11.715973311901209, 'ACC-110': 3.368874032004996, 'ACC-111': 1.3842258611336493, 'ACC-112': 2.8415343210954136, 'ACC-113': 8.102701212502666, 'ACC-114': 13.154460445838682, 'ACC-115': 3.5587403966141955, 'ACC-116': 4.989616265597218, 'ACC-117': 5.031836486711291, 'ACC-118': 0.2936141751028783, 'ACC-119': 4.101414646218353, 'ACC-120': 2.529543849292107, 'ACC-121': 3.7032158490062126, 'ACC-122': 6.65959032711559, 'ACC-123': 2.008658512756849, 'ACC-124': 0.22960938170082484, 'ACC-125': 0.3472513033480791, 'ACC-126': 5.76963360322043, 'ACC-127': 3.284945658776365, 'ACC-128': 6.860180194684415, 'ACC-129': 5.38160181078956, 'ACC-130': 10.423443735602088, 'ACC-131': 0.2759330184560112, 'ACC-132': 0.06795950578560586, 'ACC-133': 3.501602400538897, 'ACC-134': 0.08410168031021112, 'ACC-135': 0.10881648708739523, 'ACC-136': 0.0, 'ACC-137': 0.10410489254833359, 'ACC-138': 12.174705561337158, 'ACC-139': 0.034925103689025565, 'ACC-140': 24.20310525156248, 'ACC-141': 3.583673180854985, 'ACC-142': 0.09073752402710424, 'ACC-143': 2.27433452576396, 'ACC-144': 1.9379963898916968, 'ACC-145': 6.843086831100874, 'ACC-146': 2.8215028215028215, 'ACC-147': 0.5754036971799926, 'ACC-148': 0.00047526213082825833, 'ACC-149': 0.0, 'ACC-150': 0.0, 'ACC-151': 0.02195499652023963, 'ACC-152': 0.18629037696405693, 'ACC-153': 0.0, 'ACC-154': 0.0, 'ACC-155': 0.0, 'ACC-156': 0.022623410705397945, 'ACC-157': 0.0, 'ACC-158': 0.0, 'ACC-159': 0.0, 'ACC-160': 0.0, 'ACC-161': 0.15319720172883955, 'ACC-162': 0.8211817281114904, 'ACC-163': 4.22139912285395, 'ACC-164': 0.0, 'ACC-165': 0.0, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.0, 'ACC-169': 0.0, 'ACC-170': 0.0, 'ACC-171': 0.0, 'ACC-172': 0.0, 'ACC-173': 0.0, 'ACC-174': 0.0, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.0, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.885726295562361, 'ACC-186': 0.0, 'ACC-187': 0.0, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 19:30:10] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 19:30:10] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 19:30:10] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 19:30:10] d2.evaluation.testing INFO: copypaste: 6.7563,3.8356,14.2466,7.0736,19.4024
[01/17 19:30:10] d2.utils.events INFO:  eta: 16:04:27  iter: 1999  total_loss: 53.97  loss_ce: 0.4231  loss_mask: 0.5644  loss_dice: 4.365  loss_ce_0: 1.047  loss_mask_0: 0.5373  loss_dice_0: 4.432  loss_ce_1: 0.4484  loss_mask_1: 0.5523  loss_dice_1: 4.403  loss_ce_2: 0.4449  loss_mask_2: 0.5526  loss_dice_2: 4.381  loss_ce_3: 0.4312  loss_mask_3: 0.5548  loss_dice_3: 4.377  loss_ce_4: 0.4262  loss_mask_4: 0.557  loss_dice_4: 4.367  loss_ce_5: 0.4351  loss_mask_5: 0.5597  loss_dice_5: 4.371  loss_ce_6: 0.4263  loss_mask_6: 0.5568  loss_dice_6: 4.366  loss_ce_7: 0.4211  loss_mask_7: 0.5594  loss_dice_7: 4.369  loss_ce_8: 0.4201  loss_mask_8: 0.5613  loss_dice_8: 4.364  time: 1.5555  data_time: 0.0734  lr: 9.5491e-06  max_mem: 21212M
[01/17 19:30:40] d2.utils.events INFO:  eta: 16:03:03  iter: 2019  total_loss: 54.18  loss_ce: 0.4353  loss_mask: 0.5472  loss_dice: 4.349  loss_ce_0: 1.059  loss_mask_0: 0.5293  loss_dice_0: 4.419  loss_ce_1: 0.4685  loss_mask_1: 0.5402  loss_dice_1: 4.373  loss_ce_2: 0.4255  loss_mask_2: 0.5411  loss_dice_2: 4.362  loss_ce_3: 0.4315  loss_mask_3: 0.5424  loss_dice_3: 4.35  loss_ce_4: 0.4315  loss_mask_4: 0.5448  loss_dice_4: 4.349  loss_ce_5: 0.4573  loss_mask_5: 0.5441  loss_dice_5: 4.35  loss_ce_6: 0.443  loss_mask_6: 0.5472  loss_dice_6: 4.34  loss_ce_7: 0.4353  loss_mask_7: 0.5502  loss_dice_7: 4.339  loss_ce_8: 0.4421  loss_mask_8: 0.5474  loss_dice_8: 4.354  time: 1.5548  data_time: 0.0824  lr: 9.5446e-06  max_mem: 21212M
[01/17 19:31:10] d2.utils.events INFO:  eta: 16:01:52  iter: 2039  total_loss: 54.11  loss_ce: 0.4437  loss_mask: 0.5563  loss_dice: 4.366  loss_ce_0: 0.9999  loss_mask_0: 0.5419  loss_dice_0: 4.419  loss_ce_1: 0.488  loss_mask_1: 0.5519  loss_dice_1: 4.382  loss_ce_2: 0.4553  loss_mask_2: 0.555  loss_dice_2: 4.364  loss_ce_3: 0.428  loss_mask_3: 0.5544  loss_dice_3: 4.372  loss_ce_4: 0.4313  loss_mask_4: 0.557  loss_dice_4: 4.363  loss_ce_5: 0.426  loss_mask_5: 0.5572  loss_dice_5: 4.36  loss_ce_6: 0.4362  loss_mask_6: 0.5581  loss_dice_6: 4.359  loss_ce_7: 0.4143  loss_mask_7: 0.5576  loss_dice_7: 4.364  loss_ce_8: 0.4296  loss_mask_8: 0.5592  loss_dice_8: 4.363  time: 1.5543  data_time: 0.0736  lr: 9.54e-06  max_mem: 21212M
[01/17 19:31:41] d2.utils.events INFO:  eta: 16:01:05  iter: 2059  total_loss: 54.11  loss_ce: 0.4168  loss_mask: 0.5603  loss_dice: 4.368  loss_ce_0: 1.018  loss_mask_0: 0.5464  loss_dice_0: 4.435  loss_ce_1: 0.4508  loss_mask_1: 0.5526  loss_dice_1: 4.388  loss_ce_2: 0.4288  loss_mask_2: 0.5547  loss_dice_2: 4.38  loss_ce_3: 0.4347  loss_mask_3: 0.555  loss_dice_3: 4.366  loss_ce_4: 0.4293  loss_mask_4: 0.5561  loss_dice_4: 4.362  loss_ce_5: 0.4237  loss_mask_5: 0.5573  loss_dice_5: 4.363  loss_ce_6: 0.4278  loss_mask_6: 0.5577  loss_dice_6: 4.36  loss_ce_7: 0.428  loss_mask_7: 0.5617  loss_dice_7: 4.36  loss_ce_8: 0.4232  loss_mask_8: 0.5634  loss_dice_8: 4.362  time: 1.5540  data_time: 0.0850  lr: 9.5355e-06  max_mem: 21212M
[01/17 19:32:11] d2.utils.events INFO:  eta: 15:59:26  iter: 2079  total_loss: 54.04  loss_ce: 0.4216  loss_mask: 0.5478  loss_dice: 4.356  loss_ce_0: 1.024  loss_mask_0: 0.5342  loss_dice_0: 4.425  loss_ce_1: 0.4628  loss_mask_1: 0.5442  loss_dice_1: 4.385  loss_ce_2: 0.4296  loss_mask_2: 0.5423  loss_dice_2: 4.366  loss_ce_3: 0.438  loss_mask_3: 0.5452  loss_dice_3: 4.36  loss_ce_4: 0.4421  loss_mask_4: 0.5452  loss_dice_4: 4.354  loss_ce_5: 0.4337  loss_mask_5: 0.5447  loss_dice_5: 4.354  loss_ce_6: 0.4335  loss_mask_6: 0.5478  loss_dice_6: 4.364  loss_ce_7: 0.4055  loss_mask_7: 0.5475  loss_dice_7: 4.363  loss_ce_8: 0.4172  loss_mask_8: 0.5501  loss_dice_8: 4.356  time: 1.5534  data_time: 0.0840  lr: 9.531e-06  max_mem: 21212M
[01/17 19:32:41] d2.utils.events INFO:  eta: 15:57:41  iter: 2099  total_loss: 54.21  loss_ce: 0.4555  loss_mask: 0.5531  loss_dice: 4.342  loss_ce_0: 1.02  loss_mask_0: 0.5344  loss_dice_0: 4.405  loss_ce_1: 0.4819  loss_mask_1: 0.5408  loss_dice_1: 4.365  loss_ce_2: 0.4532  loss_mask_2: 0.544  loss_dice_2: 4.359  loss_ce_3: 0.4568  loss_mask_3: 0.5462  loss_dice_3: 4.354  loss_ce_4: 0.4659  loss_mask_4: 0.5492  loss_dice_4: 4.342  loss_ce_5: 0.4633  loss_mask_5: 0.5507  loss_dice_5: 4.345  loss_ce_6: 0.4715  loss_mask_6: 0.5489  loss_dice_6: 4.341  loss_ce_7: 0.465  loss_mask_7: 0.5507  loss_dice_7: 4.339  loss_ce_8: 0.471  loss_mask_8: 0.5517  loss_dice_8: 4.347  time: 1.5530  data_time: 0.0785  lr: 9.5265e-06  max_mem: 21212M
[01/17 19:33:11] d2.utils.events INFO:  eta: 15:56:57  iter: 2119  total_loss: 53.85  loss_ce: 0.4297  loss_mask: 0.5463  loss_dice: 4.346  loss_ce_0: 0.974  loss_mask_0: 0.5381  loss_dice_0: 4.4  loss_ce_1: 0.4745  loss_mask_1: 0.5426  loss_dice_1: 4.371  loss_ce_2: 0.4336  loss_mask_2: 0.545  loss_dice_2: 4.358  loss_ce_3: 0.4413  loss_mask_3: 0.5435  loss_dice_3: 4.346  loss_ce_4: 0.4383  loss_mask_4: 0.5448  loss_dice_4: 4.344  loss_ce_5: 0.4301  loss_mask_5: 0.5475  loss_dice_5: 4.338  loss_ce_6: 0.4358  loss_mask_6: 0.5467  loss_dice_6: 4.355  loss_ce_7: 0.4247  loss_mask_7: 0.5454  loss_dice_7: 4.348  loss_ce_8: 0.4309  loss_mask_8: 0.5474  loss_dice_8: 4.347  time: 1.5526  data_time: 0.0776  lr: 9.5219e-06  max_mem: 21212M
[01/17 19:33:42] d2.utils.events INFO:  eta: 15:56:19  iter: 2139  total_loss: 53.88  loss_ce: 0.4203  loss_mask: 0.555  loss_dice: 4.339  loss_ce_0: 1.005  loss_mask_0: 0.5334  loss_dice_0: 4.4  loss_ce_1: 0.4835  loss_mask_1: 0.5461  loss_dice_1: 4.358  loss_ce_2: 0.435  loss_mask_2: 0.5526  loss_dice_2: 4.347  loss_ce_3: 0.4394  loss_mask_3: 0.5532  loss_dice_3: 4.34  loss_ce_4: 0.4371  loss_mask_4: 0.5514  loss_dice_4: 4.331  loss_ce_5: 0.4276  loss_mask_5: 0.5542  loss_dice_5: 4.324  loss_ce_6: 0.4329  loss_mask_6: 0.5531  loss_dice_6: 4.335  loss_ce_7: 0.4291  loss_mask_7: 0.5572  loss_dice_7: 4.334  loss_ce_8: 0.433  loss_mask_8: 0.5555  loss_dice_8: 4.336  time: 1.5522  data_time: 0.0771  lr: 9.5174e-06  max_mem: 21212M
[01/17 19:34:12] d2.utils.events INFO:  eta: 15:55:44  iter: 2159  total_loss: 53.79  loss_ce: 0.4193  loss_mask: 0.5569  loss_dice: 4.328  loss_ce_0: 1.001  loss_mask_0: 0.5393  loss_dice_0: 4.393  loss_ce_1: 0.4536  loss_mask_1: 0.5467  loss_dice_1: 4.35  loss_ce_2: 0.435  loss_mask_2: 0.5505  loss_dice_2: 4.339  loss_ce_3: 0.4169  loss_mask_3: 0.5526  loss_dice_3: 4.329  loss_ce_4: 0.4284  loss_mask_4: 0.5553  loss_dice_4: 4.327  loss_ce_5: 0.4219  loss_mask_5: 0.5532  loss_dice_5: 4.33  loss_ce_6: 0.4112  loss_mask_6: 0.5534  loss_dice_6: 4.328  loss_ce_7: 0.4038  loss_mask_7: 0.5565  loss_dice_7: 4.329  loss_ce_8: 0.4097  loss_mask_8: 0.5578  loss_dice_8: 4.332  time: 1.5519  data_time: 0.0731  lr: 9.5129e-06  max_mem: 21212M
[01/17 19:34:42] d2.utils.events INFO:  eta: 15:54:52  iter: 2179  total_loss: 53.92  loss_ce: 0.4479  loss_mask: 0.5505  loss_dice: 4.329  loss_ce_0: 0.9763  loss_mask_0: 0.5352  loss_dice_0: 4.404  loss_ce_1: 0.4749  loss_mask_1: 0.545  loss_dice_1: 4.354  loss_ce_2: 0.4609  loss_mask_2: 0.5441  loss_dice_2: 4.338  loss_ce_3: 0.4389  loss_mask_3: 0.5435  loss_dice_3: 4.327  loss_ce_4: 0.442  loss_mask_4: 0.5449  loss_dice_4: 4.33  loss_ce_5: 0.4419  loss_mask_5: 0.5483  loss_dice_5: 4.33  loss_ce_6: 0.4644  loss_mask_6: 0.5478  loss_dice_6: 4.321  loss_ce_7: 0.4368  loss_mask_7: 0.5511  loss_dice_7: 4.33  loss_ce_8: 0.4417  loss_mask_8: 0.5499  loss_dice_8: 4.326  time: 1.5514  data_time: 0.0837  lr: 9.5084e-06  max_mem: 21212M
[01/17 19:35:13] d2.utils.events INFO:  eta: 15:53:43  iter: 2199  total_loss: 53.82  loss_ce: 0.4335  loss_mask: 0.5608  loss_dice: 4.309  loss_ce_0: 0.9505  loss_mask_0: 0.5401  loss_dice_0: 4.385  loss_ce_1: 0.4596  loss_mask_1: 0.5502  loss_dice_1: 4.339  loss_ce_2: 0.4518  loss_mask_2: 0.5525  loss_dice_2: 4.329  loss_ce_3: 0.4336  loss_mask_3: 0.556  loss_dice_3: 4.318  loss_ce_4: 0.4455  loss_mask_4: 0.5583  loss_dice_4: 4.313  loss_ce_5: 0.4489  loss_mask_5: 0.5553  loss_dice_5: 4.314  loss_ce_6: 0.4319  loss_mask_6: 0.558  loss_dice_6: 4.318  loss_ce_7: 0.4215  loss_mask_7: 0.5583  loss_dice_7: 4.316  loss_ce_8: 0.4416  loss_mask_8: 0.5597  loss_dice_8: 4.312  time: 1.5512  data_time: 0.0882  lr: 9.5038e-06  max_mem: 21212M
[01/17 19:35:42] d2.utils.events INFO:  eta: 15:51:52  iter: 2219  total_loss: 53.86  loss_ce: 0.4424  loss_mask: 0.5577  loss_dice: 4.341  loss_ce_0: 0.961  loss_mask_0: 0.5397  loss_dice_0: 4.398  loss_ce_1: 0.4637  loss_mask_1: 0.5498  loss_dice_1: 4.357  loss_ce_2: 0.4241  loss_mask_2: 0.5542  loss_dice_2: 4.347  loss_ce_3: 0.4269  loss_mask_3: 0.5557  loss_dice_3: 4.346  loss_ce_4: 0.4269  loss_mask_4: 0.5551  loss_dice_4: 4.339  loss_ce_5: 0.4352  loss_mask_5: 0.5548  loss_dice_5: 4.337  loss_ce_6: 0.4253  loss_mask_6: 0.5563  loss_dice_6: 4.336  loss_ce_7: 0.4407  loss_mask_7: 0.5568  loss_dice_7: 4.337  loss_ce_8: 0.4319  loss_mask_8: 0.5552  loss_dice_8: 4.335  time: 1.5506  data_time: 0.0733  lr: 9.4993e-06  max_mem: 21212M
[01/17 19:36:12] d2.utils.events INFO:  eta: 15:50:20  iter: 2239  total_loss: 53.81  loss_ce: 0.4776  loss_mask: 0.5412  loss_dice: 4.301  loss_ce_0: 0.9857  loss_mask_0: 0.532  loss_dice_0: 4.384  loss_ce_1: 0.5019  loss_mask_1: 0.5389  loss_dice_1: 4.333  loss_ce_2: 0.4702  loss_mask_2: 0.5435  loss_dice_2: 4.307  loss_ce_3: 0.4734  loss_mask_3: 0.5381  loss_dice_3: 4.313  loss_ce_4: 0.4619  loss_mask_4: 0.541  loss_dice_4: 4.304  loss_ce_5: 0.4767  loss_mask_5: 0.5382  loss_dice_5: 4.301  loss_ce_6: 0.4734  loss_mask_6: 0.5408  loss_dice_6: 4.295  loss_ce_7: 0.4655  loss_mask_7: 0.541  loss_dice_7: 4.302  loss_ce_8: 0.4681  loss_mask_8: 0.5419  loss_dice_8: 4.304  time: 1.5499  data_time: 0.0787  lr: 9.4948e-06  max_mem: 21212M
[01/17 19:36:42] d2.utils.events INFO:  eta: 15:49:19  iter: 2259  total_loss: 54  loss_ce: 0.4327  loss_mask: 0.5656  loss_dice: 4.322  loss_ce_0: 0.9318  loss_mask_0: 0.5514  loss_dice_0: 4.392  loss_ce_1: 0.4724  loss_mask_1: 0.5628  loss_dice_1: 4.349  loss_ce_2: 0.4162  loss_mask_2: 0.5628  loss_dice_2: 4.339  loss_ce_3: 0.438  loss_mask_3: 0.5648  loss_dice_3: 4.328  loss_ce_4: 0.4199  loss_mask_4: 0.566  loss_dice_4: 4.33  loss_ce_5: 0.4487  loss_mask_5: 0.5676  loss_dice_5: 4.313  loss_ce_6: 0.4277  loss_mask_6: 0.566  loss_dice_6: 4.333  loss_ce_7: 0.4266  loss_mask_7: 0.5692  loss_dice_7: 4.325  loss_ce_8: 0.4171  loss_mask_8: 0.5673  loss_dice_8: 4.328  time: 1.5494  data_time: 0.0770  lr: 9.4903e-06  max_mem: 21212M
[01/17 19:37:12] d2.utils.events INFO:  eta: 15:47:38  iter: 2279  total_loss: 53.34  loss_ce: 0.4224  loss_mask: 0.562  loss_dice: 4.311  loss_ce_0: 0.9401  loss_mask_0: 0.54  loss_dice_0: 4.387  loss_ce_1: 0.4417  loss_mask_1: 0.5489  loss_dice_1: 4.34  loss_ce_2: 0.4202  loss_mask_2: 0.5543  loss_dice_2: 4.318  loss_ce_3: 0.4284  loss_mask_3: 0.5564  loss_dice_3: 4.313  loss_ce_4: 0.4295  loss_mask_4: 0.5574  loss_dice_4: 4.309  loss_ce_5: 0.4103  loss_mask_5: 0.5591  loss_dice_5: 4.305  loss_ce_6: 0.4182  loss_mask_6: 0.5589  loss_dice_6: 4.307  loss_ce_7: 0.414  loss_mask_7: 0.5619  loss_dice_7: 4.304  loss_ce_8: 0.4134  loss_mask_8: 0.5624  loss_dice_8: 4.305  time: 1.5489  data_time: 0.0860  lr: 9.4857e-06  max_mem: 21212M
[01/17 19:37:42] d2.utils.events INFO:  eta: 15:45:54  iter: 2299  total_loss: 53.42  loss_ce: 0.4368  loss_mask: 0.5602  loss_dice: 4.3  loss_ce_0: 0.966  loss_mask_0: 0.5449  loss_dice_0: 4.368  loss_ce_1: 0.4675  loss_mask_1: 0.5514  loss_dice_1: 4.332  loss_ce_2: 0.4526  loss_mask_2: 0.5541  loss_dice_2: 4.308  loss_ce_3: 0.4352  loss_mask_3: 0.5548  loss_dice_3: 4.31  loss_ce_4: 0.4414  loss_mask_4: 0.5532  loss_dice_4: 4.304  loss_ce_5: 0.4414  loss_mask_5: 0.5577  loss_dice_5: 4.296  loss_ce_6: 0.4428  loss_mask_6: 0.5618  loss_dice_6: 4.304  loss_ce_7: 0.4246  loss_mask_7: 0.5632  loss_dice_7: 4.309  loss_ce_8: 0.4242  loss_mask_8: 0.559  loss_dice_8: 4.3  time: 1.5484  data_time: 0.0802  lr: 9.4812e-06  max_mem: 21212M
[01/17 19:38:12] d2.utils.events INFO:  eta: 15:44:44  iter: 2319  total_loss: 53.8  loss_ce: 0.4395  loss_mask: 0.563  loss_dice: 4.316  loss_ce_0: 0.9351  loss_mask_0: 0.5519  loss_dice_0: 4.384  loss_ce_1: 0.4679  loss_mask_1: 0.5562  loss_dice_1: 4.332  loss_ce_2: 0.4349  loss_mask_2: 0.559  loss_dice_2: 4.318  loss_ce_3: 0.4409  loss_mask_3: 0.5615  loss_dice_3: 4.317  loss_ce_4: 0.4495  loss_mask_4: 0.5629  loss_dice_4: 4.307  loss_ce_5: 0.4377  loss_mask_5: 0.5636  loss_dice_5: 4.311  loss_ce_6: 0.4398  loss_mask_6: 0.5625  loss_dice_6: 4.3  loss_ce_7: 0.4383  loss_mask_7: 0.5639  loss_dice_7: 4.313  loss_ce_8: 0.4269  loss_mask_8: 0.5625  loss_dice_8: 4.318  time: 1.5479  data_time: 0.0720  lr: 9.4767e-06  max_mem: 21212M
[01/17 19:38:42] d2.utils.events INFO:  eta: 15:44:25  iter: 2339  total_loss: 53.5  loss_ce: 0.4396  loss_mask: 0.5483  loss_dice: 4.322  loss_ce_0: 0.9135  loss_mask_0: 0.5393  loss_dice_0: 4.396  loss_ce_1: 0.454  loss_mask_1: 0.5469  loss_dice_1: 4.353  loss_ce_2: 0.4407  loss_mask_2: 0.5495  loss_dice_2: 4.33  loss_ce_3: 0.4276  loss_mask_3: 0.5503  loss_dice_3: 4.319  loss_ce_4: 0.4194  loss_mask_4: 0.5513  loss_dice_4: 4.326  loss_ce_5: 0.4268  loss_mask_5: 0.5506  loss_dice_5: 4.324  loss_ce_6: 0.431  loss_mask_6: 0.5517  loss_dice_6: 4.329  loss_ce_7: 0.4205  loss_mask_7: 0.5504  loss_dice_7: 4.333  loss_ce_8: 0.419  loss_mask_8: 0.553  loss_dice_8: 4.33  time: 1.5477  data_time: 0.0815  lr: 9.4722e-06  max_mem: 21212M
[01/17 19:39:12] d2.utils.events INFO:  eta: 15:43:49  iter: 2359  total_loss: 53.18  loss_ce: 0.3981  loss_mask: 0.5563  loss_dice: 4.299  loss_ce_0: 0.8912  loss_mask_0: 0.5412  loss_dice_0: 4.375  loss_ce_1: 0.393  loss_mask_1: 0.5485  loss_dice_1: 4.321  loss_ce_2: 0.3927  loss_mask_2: 0.5441  loss_dice_2: 4.311  loss_ce_3: 0.3862  loss_mask_3: 0.5497  loss_dice_3: 4.303  loss_ce_4: 0.3951  loss_mask_4: 0.551  loss_dice_4: 4.291  loss_ce_5: 0.3905  loss_mask_5: 0.5537  loss_dice_5: 4.292  loss_ce_6: 0.3909  loss_mask_6: 0.5507  loss_dice_6: 4.295  loss_ce_7: 0.393  loss_mask_7: 0.5564  loss_dice_7: 4.293  loss_ce_8: 0.3865  loss_mask_8: 0.5557  loss_dice_8: 4.296  time: 1.5472  data_time: 0.0759  lr: 9.4676e-06  max_mem: 21212M
[01/17 19:39:42] d2.utils.events INFO:  eta: 15:43:11  iter: 2379  total_loss: 53.48  loss_ce: 0.4404  loss_mask: 0.5542  loss_dice: 4.288  loss_ce_0: 0.9186  loss_mask_0: 0.532  loss_dice_0: 4.364  loss_ce_1: 0.4564  loss_mask_1: 0.5422  loss_dice_1: 4.312  loss_ce_2: 0.4487  loss_mask_2: 0.546  loss_dice_2: 4.292  loss_ce_3: 0.4319  loss_mask_3: 0.547  loss_dice_3: 4.286  loss_ce_4: 0.4343  loss_mask_4: 0.548  loss_dice_4: 4.288  loss_ce_5: 0.4367  loss_mask_5: 0.5523  loss_dice_5: 4.286  loss_ce_6: 0.4274  loss_mask_6: 0.554  loss_dice_6: 4.287  loss_ce_7: 0.4393  loss_mask_7: 0.5527  loss_dice_7: 4.286  loss_ce_8: 0.4477  loss_mask_8: 0.5523  loss_dice_8: 4.291  time: 1.5468  data_time: 0.0759  lr: 9.4631e-06  max_mem: 21212M
[01/17 19:40:12] d2.utils.events INFO:  eta: 15:42:30  iter: 2399  total_loss: 53.41  loss_ce: 0.4266  loss_mask: 0.5534  loss_dice: 4.306  loss_ce_0: 0.8902  loss_mask_0: 0.54  loss_dice_0: 4.371  loss_ce_1: 0.4451  loss_mask_1: 0.5496  loss_dice_1: 4.323  loss_ce_2: 0.448  loss_mask_2: 0.5505  loss_dice_2: 4.312  loss_ce_3: 0.4402  loss_mask_3: 0.5544  loss_dice_3: 4.306  loss_ce_4: 0.4363  loss_mask_4: 0.5552  loss_dice_4: 4.301  loss_ce_5: 0.4319  loss_mask_5: 0.5526  loss_dice_5: 4.302  loss_ce_6: 0.4354  loss_mask_6: 0.5525  loss_dice_6: 4.288  loss_ce_7: 0.4129  loss_mask_7: 0.554  loss_dice_7: 4.301  loss_ce_8: 0.434  loss_mask_8: 0.5529  loss_dice_8: 4.299  time: 1.5464  data_time: 0.0874  lr: 9.4586e-06  max_mem: 21212M
[01/17 19:40:42] d2.utils.events INFO:  eta: 15:41:40  iter: 2419  total_loss: 53.45  loss_ce: 0.4089  loss_mask: 0.5656  loss_dice: 4.29  loss_ce_0: 0.9008  loss_mask_0: 0.5453  loss_dice_0: 4.368  loss_ce_1: 0.4563  loss_mask_1: 0.5534  loss_dice_1: 4.317  loss_ce_2: 0.4352  loss_mask_2: 0.5587  loss_dice_2: 4.3  loss_ce_3: 0.4369  loss_mask_3: 0.5597  loss_dice_3: 4.296  loss_ce_4: 0.4344  loss_mask_4: 0.562  loss_dice_4: 4.282  loss_ce_5: 0.4183  loss_mask_5: 0.5628  loss_dice_5: 4.292  loss_ce_6: 0.4234  loss_mask_6: 0.5628  loss_dice_6: 4.286  loss_ce_7: 0.4066  loss_mask_7: 0.5642  loss_dice_7: 4.292  loss_ce_8: 0.4134  loss_mask_8: 0.5662  loss_dice_8: 4.29  time: 1.5460  data_time: 0.0746  lr: 9.454e-06  max_mem: 21212M
[01/17 19:41:12] d2.utils.events INFO:  eta: 15:40:30  iter: 2439  total_loss: 53.16  loss_ce: 0.4323  loss_mask: 0.5582  loss_dice: 4.242  loss_ce_0: 0.8814  loss_mask_0: 0.5441  loss_dice_0: 4.339  loss_ce_1: 0.4529  loss_mask_1: 0.5505  loss_dice_1: 4.272  loss_ce_2: 0.4325  loss_mask_2: 0.5534  loss_dice_2: 4.256  loss_ce_3: 0.4241  loss_mask_3: 0.5558  loss_dice_3: 4.249  loss_ce_4: 0.4384  loss_mask_4: 0.5545  loss_dice_4: 4.244  loss_ce_5: 0.4406  loss_mask_5: 0.5588  loss_dice_5: 4.24  loss_ce_6: 0.4334  loss_mask_6: 0.558  loss_dice_6: 4.243  loss_ce_7: 0.4354  loss_mask_7: 0.5573  loss_dice_7: 4.248  loss_ce_8: 0.425  loss_mask_8: 0.56  loss_dice_8: 4.253  time: 1.5455  data_time: 0.0747  lr: 9.4495e-06  max_mem: 21212M
[01/17 19:41:42] d2.utils.events INFO:  eta: 15:39:48  iter: 2459  total_loss: 53.23  loss_ce: 0.4331  loss_mask: 0.5656  loss_dice: 4.268  loss_ce_0: 0.8592  loss_mask_0: 0.5467  loss_dice_0: 4.355  loss_ce_1: 0.4488  loss_mask_1: 0.5535  loss_dice_1: 4.298  loss_ce_2: 0.4183  loss_mask_2: 0.5577  loss_dice_2: 4.28  loss_ce_3: 0.4237  loss_mask_3: 0.5641  loss_dice_3: 4.268  loss_ce_4: 0.4016  loss_mask_4: 0.5614  loss_dice_4: 4.268  loss_ce_5: 0.421  loss_mask_5: 0.5619  loss_dice_5: 4.268  loss_ce_6: 0.4116  loss_mask_6: 0.5626  loss_dice_6: 4.273  loss_ce_7: 0.3924  loss_mask_7: 0.5663  loss_dice_7: 4.275  loss_ce_8: 0.4207  loss_mask_8: 0.5631  loss_dice_8: 4.274  time: 1.5450  data_time: 0.0684  lr: 9.445e-06  max_mem: 21212M
[01/17 19:42:12] d2.utils.events INFO:  eta: 15:38:54  iter: 2479  total_loss: 53.23  loss_ce: 0.4261  loss_mask: 0.5596  loss_dice: 4.27  loss_ce_0: 0.8774  loss_mask_0: 0.5359  loss_dice_0: 4.355  loss_ce_1: 0.4533  loss_mask_1: 0.5457  loss_dice_1: 4.302  loss_ce_2: 0.4308  loss_mask_2: 0.5473  loss_dice_2: 4.291  loss_ce_3: 0.4234  loss_mask_3: 0.5462  loss_dice_3: 4.283  loss_ce_4: 0.4364  loss_mask_4: 0.5466  loss_dice_4: 4.272  loss_ce_5: 0.4308  loss_mask_5: 0.554  loss_dice_5: 4.276  loss_ce_6: 0.4337  loss_mask_6: 0.5535  loss_dice_6: 4.277  loss_ce_7: 0.4184  loss_mask_7: 0.5589  loss_dice_7: 4.271  loss_ce_8: 0.4226  loss_mask_8: 0.5603  loss_dice_8: 4.266  time: 1.5447  data_time: 0.0747  lr: 9.4405e-06  max_mem: 21212M
[01/17 19:42:41] d2.utils.events INFO:  eta: 15:38:01  iter: 2499  total_loss: 53.4  loss_ce: 0.4729  loss_mask: 0.5798  loss_dice: 4.236  loss_ce_0: 0.8968  loss_mask_0: 0.5581  loss_dice_0: 4.31  loss_ce_1: 0.4959  loss_mask_1: 0.5749  loss_dice_1: 4.271  loss_ce_2: 0.4706  loss_mask_2: 0.5772  loss_dice_2: 4.253  loss_ce_3: 0.4683  loss_mask_3: 0.5785  loss_dice_3: 4.241  loss_ce_4: 0.4649  loss_mask_4: 0.5777  loss_dice_4: 4.231  loss_ce_5: 0.4475  loss_mask_5: 0.5827  loss_dice_5: 4.237  loss_ce_6: 0.4609  loss_mask_6: 0.5826  loss_dice_6: 4.234  loss_ce_7: 0.4627  loss_mask_7: 0.5841  loss_dice_7: 4.24  loss_ce_8: 0.4515  loss_mask_8: 0.5853  loss_dice_8: 4.245  time: 1.5442  data_time: 0.0781  lr: 9.4359e-06  max_mem: 21212M
[01/17 19:43:11] d2.utils.events INFO:  eta: 15:37:08  iter: 2519  total_loss: 53.39  loss_ce: 0.4241  loss_mask: 0.5402  loss_dice: 4.299  loss_ce_0: 0.9395  loss_mask_0: 0.5278  loss_dice_0: 4.365  loss_ce_1: 0.4713  loss_mask_1: 0.5302  loss_dice_1: 4.318  loss_ce_2: 0.455  loss_mask_2: 0.5328  loss_dice_2: 4.3  loss_ce_3: 0.442  loss_mask_3: 0.5344  loss_dice_3: 4.302  loss_ce_4: 0.4469  loss_mask_4: 0.5389  loss_dice_4: 4.295  loss_ce_5: 0.4395  loss_mask_5: 0.5377  loss_dice_5: 4.29  loss_ce_6: 0.4436  loss_mask_6: 0.5363  loss_dice_6: 4.29  loss_ce_7: 0.4254  loss_mask_7: 0.5404  loss_dice_7: 4.297  loss_ce_8: 0.4282  loss_mask_8: 0.5364  loss_dice_8: 4.298  time: 1.5438  data_time: 0.0770  lr: 9.4314e-06  max_mem: 21408M
[01/17 19:43:41] d2.utils.events INFO:  eta: 15:35:53  iter: 2539  total_loss: 53.27  loss_ce: 0.4281  loss_mask: 0.5714  loss_dice: 4.273  loss_ce_0: 0.8482  loss_mask_0: 0.5529  loss_dice_0: 4.355  loss_ce_1: 0.4705  loss_mask_1: 0.5607  loss_dice_1: 4.3  loss_ce_2: 0.4317  loss_mask_2: 0.5657  loss_dice_2: 4.284  loss_ce_3: 0.4392  loss_mask_3: 0.5687  loss_dice_3: 4.264  loss_ce_4: 0.4559  loss_mask_4: 0.5715  loss_dice_4: 4.266  loss_ce_5: 0.4332  loss_mask_5: 0.5715  loss_dice_5: 4.266  loss_ce_6: 0.4339  loss_mask_6: 0.5725  loss_dice_6: 4.27  loss_ce_7: 0.4113  loss_mask_7: 0.5704  loss_dice_7: 4.276  loss_ce_8: 0.4192  loss_mask_8: 0.5711  loss_dice_8: 4.273  time: 1.5434  data_time: 0.0820  lr: 9.4269e-06  max_mem: 21408M
[01/17 19:44:11] d2.utils.events INFO:  eta: 15:34:23  iter: 2559  total_loss: 52.89  loss_ce: 0.4029  loss_mask: 0.5647  loss_dice: 4.249  loss_ce_0: 0.8672  loss_mask_0: 0.5466  loss_dice_0: 4.322  loss_ce_1: 0.4481  loss_mask_1: 0.5552  loss_dice_1: 4.273  loss_ce_2: 0.4253  loss_mask_2: 0.5549  loss_dice_2: 4.264  loss_ce_3: 0.4106  loss_mask_3: 0.5575  loss_dice_3: 4.252  loss_ce_4: 0.4205  loss_mask_4: 0.5567  loss_dice_4: 4.256  loss_ce_5: 0.4185  loss_mask_5: 0.5577  loss_dice_5: 4.249  loss_ce_6: 0.4104  loss_mask_6: 0.5595  loss_dice_6: 4.251  loss_ce_7: 0.4073  loss_mask_7: 0.5625  loss_dice_7: 4.245  loss_ce_8: 0.3977  loss_mask_8: 0.5646  loss_dice_8: 4.249  time: 1.5429  data_time: 0.0793  lr: 9.4223e-06  max_mem: 21408M
[01/17 19:44:40] d2.utils.events INFO:  eta: 15:32:16  iter: 2579  total_loss: 52.92  loss_ce: 0.414  loss_mask: 0.5735  loss_dice: 4.24  loss_ce_0: 0.8427  loss_mask_0: 0.5524  loss_dice_0: 4.328  loss_ce_1: 0.4439  loss_mask_1: 0.5659  loss_dice_1: 4.275  loss_ce_2: 0.4258  loss_mask_2: 0.5689  loss_dice_2: 4.253  loss_ce_3: 0.4407  loss_mask_3: 0.5706  loss_dice_3: 4.238  loss_ce_4: 0.4307  loss_mask_4: 0.5711  loss_dice_4: 4.234  loss_ce_5: 0.4249  loss_mask_5: 0.5718  loss_dice_5: 4.237  loss_ce_6: 0.4261  loss_mask_6: 0.5694  loss_dice_6: 4.232  loss_ce_7: 0.4141  loss_mask_7: 0.574  loss_dice_7: 4.247  loss_ce_8: 0.4083  loss_mask_8: 0.5739  loss_dice_8: 4.238  time: 1.5421  data_time: 0.0615  lr: 9.4178e-06  max_mem: 21408M
[01/17 19:45:09] d2.utils.events INFO:  eta: 15:31:09  iter: 2599  total_loss: 53.37  loss_ce: 0.4312  loss_mask: 0.5572  loss_dice: 4.272  loss_ce_0: 0.9125  loss_mask_0: 0.5403  loss_dice_0: 4.327  loss_ce_1: 0.4706  loss_mask_1: 0.5525  loss_dice_1: 4.292  loss_ce_2: 0.4369  loss_mask_2: 0.5579  loss_dice_2: 4.273  loss_ce_3: 0.4378  loss_mask_3: 0.5592  loss_dice_3: 4.261  loss_ce_4: 0.4395  loss_mask_4: 0.5615  loss_dice_4: 4.262  loss_ce_5: 0.4273  loss_mask_5: 0.5614  loss_dice_5: 4.264  loss_ce_6: 0.4296  loss_mask_6: 0.5606  loss_dice_6: 4.258  loss_ce_7: 0.4167  loss_mask_7: 0.5644  loss_dice_7: 4.263  loss_ce_8: 0.4141  loss_mask_8: 0.5607  loss_dice_8: 4.266  time: 1.5416  data_time: 0.0701  lr: 9.4133e-06  max_mem: 21408M
[01/17 19:45:39] d2.utils.events INFO:  eta: 15:30:25  iter: 2619  total_loss: 52.78  loss_ce: 0.3973  loss_mask: 0.5686  loss_dice: 4.253  loss_ce_0: 0.8218  loss_mask_0: 0.5458  loss_dice_0: 4.328  loss_ce_1: 0.4286  loss_mask_1: 0.5596  loss_dice_1: 4.279  loss_ce_2: 0.4047  loss_mask_2: 0.563  loss_dice_2: 4.27  loss_ce_3: 0.4202  loss_mask_3: 0.5666  loss_dice_3: 4.252  loss_ce_4: 0.4151  loss_mask_4: 0.5652  loss_dice_4: 4.252  loss_ce_5: 0.4098  loss_mask_5: 0.5682  loss_dice_5: 4.247  loss_ce_6: 0.4107  loss_mask_6: 0.5667  loss_dice_6: 4.252  loss_ce_7: 0.3863  loss_mask_7: 0.5692  loss_dice_7: 4.253  loss_ce_8: 0.3837  loss_mask_8: 0.5701  loss_dice_8: 4.256  time: 1.5411  data_time: 0.0806  lr: 9.4087e-06  max_mem: 21408M
[01/17 19:46:08] d2.utils.events INFO:  eta: 15:29:20  iter: 2639  total_loss: 53  loss_ce: 0.4404  loss_mask: 0.5602  loss_dice: 4.229  loss_ce_0: 0.8503  loss_mask_0: 0.5441  loss_dice_0: 4.304  loss_ce_1: 0.4621  loss_mask_1: 0.5561  loss_dice_1: 4.264  loss_ce_2: 0.4533  loss_mask_2: 0.5572  loss_dice_2: 4.241  loss_ce_3: 0.4415  loss_mask_3: 0.5601  loss_dice_3: 4.233  loss_ce_4: 0.4452  loss_mask_4: 0.5594  loss_dice_4: 4.226  loss_ce_5: 0.4475  loss_mask_5: 0.5591  loss_dice_5: 4.22  loss_ce_6: 0.4577  loss_mask_6: 0.5585  loss_dice_6: 4.218  loss_ce_7: 0.4428  loss_mask_7: 0.5612  loss_dice_7: 4.221  loss_ce_8: 0.438  loss_mask_8: 0.56  loss_dice_8: 4.237  time: 1.5404  data_time: 0.0684  lr: 9.4042e-06  max_mem: 21408M
[01/17 19:46:37] d2.utils.events INFO:  eta: 15:27:00  iter: 2659  total_loss: 52.81  loss_ce: 0.4152  loss_mask: 0.5622  loss_dice: 4.242  loss_ce_0: 0.8335  loss_mask_0: 0.5433  loss_dice_0: 4.327  loss_ce_1: 0.455  loss_mask_1: 0.5569  loss_dice_1: 4.262  loss_ce_2: 0.4331  loss_mask_2: 0.5584  loss_dice_2: 4.248  loss_ce_3: 0.4425  loss_mask_3: 0.5618  loss_dice_3: 4.242  loss_ce_4: 0.442  loss_mask_4: 0.5631  loss_dice_4: 4.243  loss_ce_5: 0.4442  loss_mask_5: 0.5624  loss_dice_5: 4.237  loss_ce_6: 0.4228  loss_mask_6: 0.5636  loss_dice_6: 4.237  loss_ce_7: 0.4153  loss_mask_7: 0.5633  loss_dice_7: 4.246  loss_ce_8: 0.4262  loss_mask_8: 0.5621  loss_dice_8: 4.241  time: 1.5397  data_time: 0.0624  lr: 9.3997e-06  max_mem: 21408M
[01/17 19:47:06] d2.utils.events INFO:  eta: 15:25:22  iter: 2679  total_loss: 52.75  loss_ce: 0.4278  loss_mask: 0.5641  loss_dice: 4.227  loss_ce_0: 0.8325  loss_mask_0: 0.5452  loss_dice_0: 4.308  loss_ce_1: 0.4454  loss_mask_1: 0.5531  loss_dice_1: 4.255  loss_ce_2: 0.4435  loss_mask_2: 0.5528  loss_dice_2: 4.24  loss_ce_3: 0.4229  loss_mask_3: 0.5572  loss_dice_3: 4.228  loss_ce_4: 0.4361  loss_mask_4: 0.5545  loss_dice_4: 4.23  loss_ce_5: 0.4289  loss_mask_5: 0.557  loss_dice_5: 4.231  loss_ce_6: 0.4193  loss_mask_6: 0.5585  loss_dice_6: 4.227  loss_ce_7: 0.4066  loss_mask_7: 0.5611  loss_dice_7: 4.233  loss_ce_8: 0.415  loss_mask_8: 0.5629  loss_dice_8: 4.225  time: 1.5392  data_time: 0.0741  lr: 9.3952e-06  max_mem: 21408M
[01/17 19:47:36] d2.utils.events INFO:  eta: 15:24:29  iter: 2699  total_loss: 52.9  loss_ce: 0.4256  loss_mask: 0.5562  loss_dice: 4.261  loss_ce_0: 0.8709  loss_mask_0: 0.5403  loss_dice_0: 4.338  loss_ce_1: 0.4826  loss_mask_1: 0.5485  loss_dice_1: 4.283  loss_ce_2: 0.4513  loss_mask_2: 0.5487  loss_dice_2: 4.268  loss_ce_3: 0.4499  loss_mask_3: 0.552  loss_dice_3: 4.259  loss_ce_4: 0.4348  loss_mask_4: 0.5491  loss_dice_4: 4.26  loss_ce_5: 0.4351  loss_mask_5: 0.5528  loss_dice_5: 4.263  loss_ce_6: 0.4381  loss_mask_6: 0.552  loss_dice_6: 4.259  loss_ce_7: 0.4299  loss_mask_7: 0.5559  loss_dice_7: 4.268  loss_ce_8: 0.4288  loss_mask_8: 0.5535  loss_dice_8: 4.265  time: 1.5388  data_time: 0.0671  lr: 9.3906e-06  max_mem: 21408M
[01/17 19:48:06] d2.utils.events INFO:  eta: 15:22:56  iter: 2719  total_loss: 52.7  loss_ce: 0.4274  loss_mask: 0.5565  loss_dice: 4.225  loss_ce_0: 0.8147  loss_mask_0: 0.5389  loss_dice_0: 4.306  loss_ce_1: 0.4615  loss_mask_1: 0.5487  loss_dice_1: 4.242  loss_ce_2: 0.4466  loss_mask_2: 0.5534  loss_dice_2: 4.23  loss_ce_3: 0.4407  loss_mask_3: 0.5553  loss_dice_3: 4.231  loss_ce_4: 0.4307  loss_mask_4: 0.5542  loss_dice_4: 4.218  loss_ce_5: 0.435  loss_mask_5: 0.5564  loss_dice_5: 4.214  loss_ce_6: 0.4366  loss_mask_6: 0.5587  loss_dice_6: 4.211  loss_ce_7: 0.4009  loss_mask_7: 0.5596  loss_dice_7: 4.223  loss_ce_8: 0.406  loss_mask_8: 0.5594  loss_dice_8: 4.222  time: 1.5383  data_time: 0.0727  lr: 9.3861e-06  max_mem: 21408M
[01/17 19:48:35] d2.utils.events INFO:  eta: 15:22:05  iter: 2739  total_loss: 52.73  loss_ce: 0.453  loss_mask: 0.5631  loss_dice: 4.194  loss_ce_0: 0.8948  loss_mask_0: 0.5507  loss_dice_0: 4.28  loss_ce_1: 0.4709  loss_mask_1: 0.5565  loss_dice_1: 4.223  loss_ce_2: 0.4675  loss_mask_2: 0.5588  loss_dice_2: 4.204  loss_ce_3: 0.4481  loss_mask_3: 0.5622  loss_dice_3: 4.202  loss_ce_4: 0.4565  loss_mask_4: 0.5623  loss_dice_4: 4.197  loss_ce_5: 0.4472  loss_mask_5: 0.5599  loss_dice_5: 4.183  loss_ce_6: 0.4627  loss_mask_6: 0.5587  loss_dice_6: 4.193  loss_ce_7: 0.4501  loss_mask_7: 0.5654  loss_dice_7: 4.2  loss_ce_8: 0.4247  loss_mask_8: 0.5634  loss_dice_8: 4.204  time: 1.5376  data_time: 0.0712  lr: 9.3816e-06  max_mem: 21408M
[01/17 19:49:04] d2.utils.events INFO:  eta: 15:20:51  iter: 2759  total_loss: 52.74  loss_ce: 0.4126  loss_mask: 0.56  loss_dice: 4.223  loss_ce_0: 0.844  loss_mask_0: 0.5441  loss_dice_0: 4.304  loss_ce_1: 0.4448  loss_mask_1: 0.5545  loss_dice_1: 4.257  loss_ce_2: 0.4333  loss_mask_2: 0.5576  loss_dice_2: 4.241  loss_ce_3: 0.4353  loss_mask_3: 0.5574  loss_dice_3: 4.23  loss_ce_4: 0.4184  loss_mask_4: 0.5547  loss_dice_4: 4.237  loss_ce_5: 0.4157  loss_mask_5: 0.5592  loss_dice_5: 4.234  loss_ce_6: 0.4127  loss_mask_6: 0.5612  loss_dice_6: 4.231  loss_ce_7: 0.4084  loss_mask_7: 0.5636  loss_dice_7: 4.227  loss_ce_8: 0.4135  loss_mask_8: 0.5622  loss_dice_8: 4.229  time: 1.5371  data_time: 0.0631  lr: 9.377e-06  max_mem: 21408M
[01/17 19:49:34] d2.utils.events INFO:  eta: 15:20:04  iter: 2779  total_loss: 52.9  loss_ce: 0.4307  loss_mask: 0.551  loss_dice: 4.248  loss_ce_0: 0.7983  loss_mask_0: 0.5312  loss_dice_0: 4.335  loss_ce_1: 0.4571  loss_mask_1: 0.5442  loss_dice_1: 4.272  loss_ce_2: 0.456  loss_mask_2: 0.5527  loss_dice_2: 4.256  loss_ce_3: 0.4447  loss_mask_3: 0.5504  loss_dice_3: 4.255  loss_ce_4: 0.4428  loss_mask_4: 0.5492  loss_dice_4: 4.243  loss_ce_5: 0.4427  loss_mask_5: 0.5491  loss_dice_5: 4.247  loss_ce_6: 0.4419  loss_mask_6: 0.5481  loss_dice_6: 4.242  loss_ce_7: 0.4198  loss_mask_7: 0.553  loss_dice_7: 4.249  loss_ce_8: 0.4261  loss_mask_8: 0.5511  loss_dice_8: 4.251  time: 1.5368  data_time: 0.0809  lr: 9.3725e-06  max_mem: 21408M
[01/17 19:50:04] d2.utils.events INFO:  eta: 15:19:23  iter: 2799  total_loss: 52.43  loss_ce: 0.4324  loss_mask: 0.5527  loss_dice: 4.2  loss_ce_0: 0.8428  loss_mask_0: 0.5364  loss_dice_0: 4.291  loss_ce_1: 0.4719  loss_mask_1: 0.5441  loss_dice_1: 4.226  loss_ce_2: 0.4479  loss_mask_2: 0.5475  loss_dice_2: 4.199  loss_ce_3: 0.4361  loss_mask_3: 0.5507  loss_dice_3: 4.202  loss_ce_4: 0.4414  loss_mask_4: 0.5475  loss_dice_4: 4.195  loss_ce_5: 0.4362  loss_mask_5: 0.5476  loss_dice_5: 4.193  loss_ce_6: 0.4253  loss_mask_6: 0.5488  loss_dice_6: 4.187  loss_ce_7: 0.4322  loss_mask_7: 0.5533  loss_dice_7: 4.203  loss_ce_8: 0.4221  loss_mask_8: 0.5539  loss_dice_8: 4.199  time: 1.5364  data_time: 0.0738  lr: 9.368e-06  max_mem: 21408M
[01/17 19:50:33] d2.utils.events INFO:  eta: 15:18:36  iter: 2819  total_loss: 52.2  loss_ce: 0.415  loss_mask: 0.5674  loss_dice: 4.19  loss_ce_0: 0.8051  loss_mask_0: 0.5437  loss_dice_0: 4.289  loss_ce_1: 0.4343  loss_mask_1: 0.5588  loss_dice_1: 4.214  loss_ce_2: 0.4096  loss_mask_2: 0.5604  loss_dice_2: 4.209  loss_ce_3: 0.4072  loss_mask_3: 0.5616  loss_dice_3: 4.191  loss_ce_4: 0.4091  loss_mask_4: 0.5628  loss_dice_4: 4.185  loss_ce_5: 0.3963  loss_mask_5: 0.5648  loss_dice_5: 4.197  loss_ce_6: 0.4034  loss_mask_6: 0.5673  loss_dice_6: 4.186  loss_ce_7: 0.3768  loss_mask_7: 0.5655  loss_dice_7: 4.19  loss_ce_8: 0.3882  loss_mask_8: 0.5663  loss_dice_8: 4.193  time: 1.5360  data_time: 0.0764  lr: 9.3634e-06  max_mem: 21408M
[01/17 19:51:03] d2.utils.events INFO:  eta: 15:18:02  iter: 2839  total_loss: 52.33  loss_ce: 0.4278  loss_mask: 0.5608  loss_dice: 4.166  loss_ce_0: 0.8207  loss_mask_0: 0.5374  loss_dice_0: 4.252  loss_ce_1: 0.4733  loss_mask_1: 0.5528  loss_dice_1: 4.19  loss_ce_2: 0.4519  loss_mask_2: 0.5518  loss_dice_2: 4.173  loss_ce_3: 0.4519  loss_mask_3: 0.5559  loss_dice_3: 4.158  loss_ce_4: 0.4479  loss_mask_4: 0.5576  loss_dice_4: 4.16  loss_ce_5: 0.4464  loss_mask_5: 0.5605  loss_dice_5: 4.161  loss_ce_6: 0.443  loss_mask_6: 0.5614  loss_dice_6: 4.162  loss_ce_7: 0.4331  loss_mask_7: 0.5586  loss_dice_7: 4.162  loss_ce_8: 0.4315  loss_mask_8: 0.5593  loss_dice_8: 4.164  time: 1.5355  data_time: 0.0704  lr: 9.3589e-06  max_mem: 21408M
[01/17 19:51:32] d2.utils.events INFO:  eta: 15:17:05  iter: 2859  total_loss: 52.24  loss_ce: 0.4199  loss_mask: 0.5525  loss_dice: 4.172  loss_ce_0: 0.795  loss_mask_0: 0.5317  loss_dice_0: 4.265  loss_ce_1: 0.438  loss_mask_1: 0.5466  loss_dice_1: 4.204  loss_ce_2: 0.3985  loss_mask_2: 0.5501  loss_dice_2: 4.195  loss_ce_3: 0.4128  loss_mask_3: 0.5492  loss_dice_3: 4.184  loss_ce_4: 0.4083  loss_mask_4: 0.5482  loss_dice_4: 4.177  loss_ce_5: 0.4049  loss_mask_5: 0.5529  loss_dice_5: 4.17  loss_ce_6: 0.4098  loss_mask_6: 0.5494  loss_dice_6: 4.169  loss_ce_7: 0.3989  loss_mask_7: 0.5542  loss_dice_7: 4.171  loss_ce_8: 0.4101  loss_mask_8: 0.5541  loss_dice_8: 4.17  time: 1.5351  data_time: 0.0732  lr: 9.3544e-06  max_mem: 21408M
[01/17 19:52:02] d2.utils.events INFO:  eta: 15:16:49  iter: 2879  total_loss: 51.94  loss_ce: 0.4246  loss_mask: 0.5433  loss_dice: 4.189  loss_ce_0: 0.7947  loss_mask_0: 0.5261  loss_dice_0: 4.291  loss_ce_1: 0.4326  loss_mask_1: 0.5378  loss_dice_1: 4.215  loss_ce_2: 0.4136  loss_mask_2: 0.544  loss_dice_2: 4.199  loss_ce_3: 0.4224  loss_mask_3: 0.5446  loss_dice_3: 4.182  loss_ce_4: 0.4349  loss_mask_4: 0.5437  loss_dice_4: 4.181  loss_ce_5: 0.4241  loss_mask_5: 0.5411  loss_dice_5: 4.19  loss_ce_6: 0.4145  loss_mask_6: 0.5411  loss_dice_6: 4.184  loss_ce_7: 0.424  loss_mask_7: 0.5485  loss_dice_7: 4.192  loss_ce_8: 0.4097  loss_mask_8: 0.547  loss_dice_8: 4.185  time: 1.5348  data_time: 0.0711  lr: 9.3498e-06  max_mem: 21408M
[01/17 19:52:31] d2.utils.events INFO:  eta: 15:16:04  iter: 2899  total_loss: 52.03  loss_ce: 0.4175  loss_mask: 0.5641  loss_dice: 4.173  loss_ce_0: 0.8032  loss_mask_0: 0.5405  loss_dice_0: 4.267  loss_ce_1: 0.4386  loss_mask_1: 0.5528  loss_dice_1: 4.203  loss_ce_2: 0.4199  loss_mask_2: 0.5574  loss_dice_2: 4.185  loss_ce_3: 0.4223  loss_mask_3: 0.56  loss_dice_3: 4.173  loss_ce_4: 0.4254  loss_mask_4: 0.5585  loss_dice_4: 4.177  loss_ce_5: 0.4238  loss_mask_5: 0.5616  loss_dice_5: 4.163  loss_ce_6: 0.423  loss_mask_6: 0.5613  loss_dice_6: 4.172  loss_ce_7: 0.4109  loss_mask_7: 0.563  loss_dice_7: 4.168  loss_ce_8: 0.4115  loss_mask_8: 0.5642  loss_dice_8: 4.174  time: 1.5343  data_time: 0.0780  lr: 9.3453e-06  max_mem: 21408M
[01/17 19:53:01] d2.utils.events INFO:  eta: 15:15:07  iter: 2919  total_loss: 52.29  loss_ce: 0.4236  loss_mask: 0.5578  loss_dice: 4.217  loss_ce_0: 0.82  loss_mask_0: 0.5358  loss_dice_0: 4.284  loss_ce_1: 0.4825  loss_mask_1: 0.5519  loss_dice_1: 4.234  loss_ce_2: 0.4424  loss_mask_2: 0.5547  loss_dice_2: 4.224  loss_ce_3: 0.4408  loss_mask_3: 0.5513  loss_dice_3: 4.226  loss_ce_4: 0.4406  loss_mask_4: 0.5546  loss_dice_4: 4.215  loss_ce_5: 0.4268  loss_mask_5: 0.557  loss_dice_5: 4.217  loss_ce_6: 0.4273  loss_mask_6: 0.5559  loss_dice_6: 4.215  loss_ce_7: 0.4106  loss_mask_7: 0.5565  loss_dice_7: 4.22  loss_ce_8: 0.4253  loss_mask_8: 0.5557  loss_dice_8: 4.216  time: 1.5338  data_time: 0.0670  lr: 9.3408e-06  max_mem: 21408M
[01/17 19:53:31] d2.utils.events INFO:  eta: 15:14:29  iter: 2939  total_loss: 52.29  loss_ce: 0.4315  loss_mask: 0.5342  loss_dice: 4.195  loss_ce_0: 0.815  loss_mask_0: 0.5238  loss_dice_0: 4.282  loss_ce_1: 0.4627  loss_mask_1: 0.5295  loss_dice_1: 4.224  loss_ce_2: 0.4578  loss_mask_2: 0.5294  loss_dice_2: 4.202  loss_ce_3: 0.45  loss_mask_3: 0.5324  loss_dice_3: 4.197  loss_ce_4: 0.4591  loss_mask_4: 0.5323  loss_dice_4: 4.192  loss_ce_5: 0.4567  loss_mask_5: 0.534  loss_dice_5: 4.193  loss_ce_6: 0.4567  loss_mask_6: 0.5335  loss_dice_6: 4.191  loss_ce_7: 0.4472  loss_mask_7: 0.5377  loss_dice_7: 4.192  loss_ce_8: 0.4541  loss_mask_8: 0.5383  loss_dice_8: 4.197  time: 1.5335  data_time: 0.0685  lr: 9.3362e-06  max_mem: 21408M
[01/17 19:54:00] d2.utils.events INFO:  eta: 15:13:28  iter: 2959  total_loss: 52.03  loss_ce: 0.4109  loss_mask: 0.5487  loss_dice: 4.179  loss_ce_0: 0.8147  loss_mask_0: 0.5343  loss_dice_0: 4.263  loss_ce_1: 0.4549  loss_mask_1: 0.5418  loss_dice_1: 4.201  loss_ce_2: 0.4379  loss_mask_2: 0.5472  loss_dice_2: 4.181  loss_ce_3: 0.4296  loss_mask_3: 0.5463  loss_dice_3: 4.174  loss_ce_4: 0.4285  loss_mask_4: 0.5476  loss_dice_4: 4.183  loss_ce_5: 0.4291  loss_mask_5: 0.5458  loss_dice_5: 4.175  loss_ce_6: 0.4185  loss_mask_6: 0.5459  loss_dice_6: 4.173  loss_ce_7: 0.4161  loss_mask_7: 0.5481  loss_dice_7: 4.176  loss_ce_8: 0.4149  loss_mask_8: 0.5459  loss_dice_8: 4.177  time: 1.5331  data_time: 0.0748  lr: 9.3317e-06  max_mem: 21408M
[01/17 19:54:29] d2.utils.events INFO:  eta: 15:12:48  iter: 2979  total_loss: 52.02  loss_ce: 0.4086  loss_mask: 0.5565  loss_dice: 4.184  loss_ce_0: 0.7825  loss_mask_0: 0.5304  loss_dice_0: 4.271  loss_ce_1: 0.4474  loss_mask_1: 0.5475  loss_dice_1: 4.207  loss_ce_2: 0.4043  loss_mask_2: 0.551  loss_dice_2: 4.191  loss_ce_3: 0.4139  loss_mask_3: 0.5538  loss_dice_3: 4.183  loss_ce_4: 0.418  loss_mask_4: 0.5528  loss_dice_4: 4.182  loss_ce_5: 0.4136  loss_mask_5: 0.5563  loss_dice_5: 4.168  loss_ce_6: 0.4051  loss_mask_6: 0.5571  loss_dice_6: 4.171  loss_ce_7: 0.4028  loss_mask_7: 0.5583  loss_dice_7: 4.175  loss_ce_8: 0.3994  loss_mask_8: 0.5578  loss_dice_8: 4.177  time: 1.5325  data_time: 0.0731  lr: 9.3272e-06  max_mem: 21408M
[01/17 19:54:59] d2.utils.events INFO:  eta: 15:13:02  iter: 2999  total_loss: 51.95  loss_ce: 0.4136  loss_mask: 0.5539  loss_dice: 4.152  loss_ce_0: 0.7897  loss_mask_0: 0.5354  loss_dice_0: 4.243  loss_ce_1: 0.4549  loss_mask_1: 0.5441  loss_dice_1: 4.174  loss_ce_2: 0.4303  loss_mask_2: 0.5455  loss_dice_2: 4.162  loss_ce_3: 0.4206  loss_mask_3: 0.5488  loss_dice_3: 4.151  loss_ce_4: 0.4336  loss_mask_4: 0.5467  loss_dice_4: 4.148  loss_ce_5: 0.4286  loss_mask_5: 0.5543  loss_dice_5: 4.146  loss_ce_6: 0.4301  loss_mask_6: 0.5544  loss_dice_6: 4.143  loss_ce_7: 0.3966  loss_mask_7: 0.5559  loss_dice_7: 4.148  loss_ce_8: 0.4142  loss_mask_8: 0.5544  loss_dice_8: 4.149  time: 1.5322  data_time: 0.0745  lr: 9.3226e-06  max_mem: 21408M
[01/17 19:55:28] d2.utils.events INFO:  eta: 15:12:37  iter: 3019  total_loss: 52.28  loss_ce: 0.4336  loss_mask: 0.5617  loss_dice: 4.196  loss_ce_0: 0.7848  loss_mask_0: 0.5359  loss_dice_0: 4.283  loss_ce_1: 0.4724  loss_mask_1: 0.5536  loss_dice_1: 4.225  loss_ce_2: 0.4483  loss_mask_2: 0.5581  loss_dice_2: 4.204  loss_ce_3: 0.4375  loss_mask_3: 0.5579  loss_dice_3: 4.19  loss_ce_4: 0.4321  loss_mask_4: 0.5587  loss_dice_4: 4.197  loss_ce_5: 0.4251  loss_mask_5: 0.5636  loss_dice_5: 4.198  loss_ce_6: 0.42  loss_mask_6: 0.5571  loss_dice_6: 4.197  loss_ce_7: 0.4113  loss_mask_7: 0.5627  loss_dice_7: 4.19  loss_ce_8: 0.4226  loss_mask_8: 0.5619  loss_dice_8: 4.197  time: 1.5318  data_time: 0.0637  lr: 9.3181e-06  max_mem: 21408M
[01/17 19:55:58] d2.utils.events INFO:  eta: 15:11:38  iter: 3039  total_loss: 52.26  loss_ce: 0.4365  loss_mask: 0.5434  loss_dice: 4.21  loss_ce_0: 0.7488  loss_mask_0: 0.5319  loss_dice_0: 4.295  loss_ce_1: 0.4358  loss_mask_1: 0.5412  loss_dice_1: 4.223  loss_ce_2: 0.4179  loss_mask_2: 0.54  loss_dice_2: 4.215  loss_ce_3: 0.4291  loss_mask_3: 0.5434  loss_dice_3: 4.21  loss_ce_4: 0.4488  loss_mask_4: 0.5421  loss_dice_4: 4.204  loss_ce_5: 0.4522  loss_mask_5: 0.5449  loss_dice_5: 4.2  loss_ce_6: 0.4329  loss_mask_6: 0.5464  loss_dice_6: 4.199  loss_ce_7: 0.4279  loss_mask_7: 0.5451  loss_dice_7: 4.207  loss_ce_8: 0.4263  loss_mask_8: 0.5455  loss_dice_8: 4.212  time: 1.5315  data_time: 0.0701  lr: 9.3136e-06  max_mem: 21408M
[01/17 19:56:27] d2.utils.events INFO:  eta: 15:10:30  iter: 3059  total_loss: 52.02  loss_ce: 0.398  loss_mask: 0.5654  loss_dice: 4.15  loss_ce_0: 0.7796  loss_mask_0: 0.5455  loss_dice_0: 4.251  loss_ce_1: 0.4258  loss_mask_1: 0.5612  loss_dice_1: 4.181  loss_ce_2: 0.4146  loss_mask_2: 0.564  loss_dice_2: 4.161  loss_ce_3: 0.4048  loss_mask_3: 0.5643  loss_dice_3: 4.153  loss_ce_4: 0.407  loss_mask_4: 0.562  loss_dice_4: 4.154  loss_ce_5: 0.4136  loss_mask_5: 0.5642  loss_dice_5: 4.158  loss_ce_6: 0.4027  loss_mask_6: 0.5622  loss_dice_6: 4.148  loss_ce_7: 0.3922  loss_mask_7: 0.5672  loss_dice_7: 4.162  loss_ce_8: 0.4073  loss_mask_8: 0.5666  loss_dice_8: 4.165  time: 1.5310  data_time: 0.0761  lr: 9.309e-06  max_mem: 21408M
[01/17 19:56:57] d2.utils.events INFO:  eta: 15:09:28  iter: 3079  total_loss: 51.81  loss_ce: 0.4402  loss_mask: 0.5667  loss_dice: 4.146  loss_ce_0: 0.7781  loss_mask_0: 0.538  loss_dice_0: 4.252  loss_ce_1: 0.4402  loss_mask_1: 0.5565  loss_dice_1: 4.179  loss_ce_2: 0.4349  loss_mask_2: 0.5602  loss_dice_2: 4.163  loss_ce_3: 0.4333  loss_mask_3: 0.5605  loss_dice_3: 4.153  loss_ce_4: 0.4413  loss_mask_4: 0.5632  loss_dice_4: 4.144  loss_ce_5: 0.4296  loss_mask_5: 0.5681  loss_dice_5: 4.142  loss_ce_6: 0.425  loss_mask_6: 0.5687  loss_dice_6: 4.143  loss_ce_7: 0.4095  loss_mask_7: 0.5648  loss_dice_7: 4.163  loss_ce_8: 0.4227  loss_mask_8: 0.5656  loss_dice_8: 4.158  time: 1.5306  data_time: 0.0667  lr: 9.3045e-06  max_mem: 21408M
[01/17 19:57:27] d2.utils.events INFO:  eta: 15:09:06  iter: 3099  total_loss: 51.61  loss_ce: 0.396  loss_mask: 0.5583  loss_dice: 4.157  loss_ce_0: 0.7822  loss_mask_0: 0.5464  loss_dice_0: 4.258  loss_ce_1: 0.4304  loss_mask_1: 0.5501  loss_dice_1: 4.188  loss_ce_2: 0.4029  loss_mask_2: 0.554  loss_dice_2: 4.176  loss_ce_3: 0.3997  loss_mask_3: 0.5552  loss_dice_3: 4.159  loss_ce_4: 0.4145  loss_mask_4: 0.5551  loss_dice_4: 4.161  loss_ce_5: 0.3907  loss_mask_5: 0.5597  loss_dice_5: 4.163  loss_ce_6: 0.393  loss_mask_6: 0.5602  loss_dice_6: 4.165  loss_ce_7: 0.3891  loss_mask_7: 0.5611  loss_dice_7: 4.164  loss_ce_8: 0.3915  loss_mask_8: 0.559  loss_dice_8: 4.161  time: 1.5303  data_time: 0.0810  lr: 9.2999e-06  max_mem: 21408M
[01/17 19:57:56] d2.utils.events INFO:  eta: 15:08:05  iter: 3119  total_loss: 51.79  loss_ce: 0.391  loss_mask: 0.5767  loss_dice: 4.161  loss_ce_0: 0.7865  loss_mask_0: 0.5495  loss_dice_0: 4.243  loss_ce_1: 0.4222  loss_mask_1: 0.5633  loss_dice_1: 4.185  loss_ce_2: 0.4237  loss_mask_2: 0.5671  loss_dice_2: 4.166  loss_ce_3: 0.4177  loss_mask_3: 0.5713  loss_dice_3: 4.159  loss_ce_4: 0.3906  loss_mask_4: 0.5697  loss_dice_4: 4.157  loss_ce_5: 0.4045  loss_mask_5: 0.5718  loss_dice_5: 4.156  loss_ce_6: 0.3895  loss_mask_6: 0.5721  loss_dice_6: 4.157  loss_ce_7: 0.3698  loss_mask_7: 0.575  loss_dice_7: 4.152  loss_ce_8: 0.3755  loss_mask_8: 0.5779  loss_dice_8: 4.151  time: 1.5299  data_time: 0.0709  lr: 9.2954e-06  max_mem: 21408M
[01/17 19:58:25] d2.utils.events INFO:  eta: 15:07:19  iter: 3139  total_loss: 51.9  loss_ce: 0.4088  loss_mask: 0.5632  loss_dice: 4.169  loss_ce_0: 0.7505  loss_mask_0: 0.5396  loss_dice_0: 4.257  loss_ce_1: 0.4423  loss_mask_1: 0.555  loss_dice_1: 4.196  loss_ce_2: 0.4288  loss_mask_2: 0.5588  loss_dice_2: 4.181  loss_ce_3: 0.4249  loss_mask_3: 0.5571  loss_dice_3: 4.165  loss_ce_4: 0.4214  loss_mask_4: 0.5588  loss_dice_4: 4.165  loss_ce_5: 0.4128  loss_mask_5: 0.5629  loss_dice_5: 4.165  loss_ce_6: 0.4247  loss_mask_6: 0.5619  loss_dice_6: 4.161  loss_ce_7: 0.4066  loss_mask_7: 0.5638  loss_dice_7: 4.178  loss_ce_8: 0.4116  loss_mask_8: 0.5632  loss_dice_8: 4.171  time: 1.5295  data_time: 0.0822  lr: 9.2909e-06  max_mem: 21408M
[01/17 19:58:54] d2.utils.events INFO:  eta: 15:06:27  iter: 3159  total_loss: 51.83  loss_ce: 0.4034  loss_mask: 0.5729  loss_dice: 4.147  loss_ce_0: 0.7667  loss_mask_0: 0.5495  loss_dice_0: 4.247  loss_ce_1: 0.4382  loss_mask_1: 0.5627  loss_dice_1: 4.173  loss_ce_2: 0.4067  loss_mask_2: 0.5657  loss_dice_2: 4.156  loss_ce_3: 0.4193  loss_mask_3: 0.5672  loss_dice_3: 4.145  loss_ce_4: 0.429  loss_mask_4: 0.5653  loss_dice_4: 4.141  loss_ce_5: 0.4138  loss_mask_5: 0.5681  loss_dice_5: 4.15  loss_ce_6: 0.3986  loss_mask_6: 0.5676  loss_dice_6: 4.143  loss_ce_7: 0.3991  loss_mask_7: 0.5718  loss_dice_7: 4.15  loss_ce_8: 0.4013  loss_mask_8: 0.5739  loss_dice_8: 4.151  time: 1.5290  data_time: 0.0627  lr: 9.2863e-06  max_mem: 21408M
[01/17 19:59:24] d2.utils.events INFO:  eta: 15:05:35  iter: 3179  total_loss: 51.87  loss_ce: 0.4107  loss_mask: 0.5644  loss_dice: 4.153  loss_ce_0: 0.7799  loss_mask_0: 0.5405  loss_dice_0: 4.245  loss_ce_1: 0.4674  loss_mask_1: 0.5491  loss_dice_1: 4.176  loss_ce_2: 0.4383  loss_mask_2: 0.556  loss_dice_2: 4.16  loss_ce_3: 0.4359  loss_mask_3: 0.5616  loss_dice_3: 4.144  loss_ce_4: 0.4348  loss_mask_4: 0.563  loss_dice_4: 4.145  loss_ce_5: 0.4191  loss_mask_5: 0.5629  loss_dice_5: 4.15  loss_ce_6: 0.4224  loss_mask_6: 0.5653  loss_dice_6: 4.148  loss_ce_7: 0.4126  loss_mask_7: 0.5668  loss_dice_7: 4.154  loss_ce_8: 0.4121  loss_mask_8: 0.5671  loss_dice_8: 4.147  time: 1.5287  data_time: 0.0737  lr: 9.2818e-06  max_mem: 21408M
[01/17 19:59:53] d2.utils.events INFO:  eta: 15:04:57  iter: 3199  total_loss: 52.04  loss_ce: 0.4127  loss_mask: 0.5544  loss_dice: 4.155  loss_ce_0: 0.7908  loss_mask_0: 0.5381  loss_dice_0: 4.239  loss_ce_1: 0.4412  loss_mask_1: 0.5455  loss_dice_1: 4.184  loss_ce_2: 0.4205  loss_mask_2: 0.5509  loss_dice_2: 4.165  loss_ce_3: 0.4218  loss_mask_3: 0.5472  loss_dice_3: 4.161  loss_ce_4: 0.4178  loss_mask_4: 0.5545  loss_dice_4: 4.164  loss_ce_5: 0.4087  loss_mask_5: 0.5551  loss_dice_5: 4.162  loss_ce_6: 0.3941  loss_mask_6: 0.5523  loss_dice_6: 4.16  loss_ce_7: 0.3919  loss_mask_7: 0.5592  loss_dice_7: 4.157  loss_ce_8: 0.3916  loss_mask_8: 0.5562  loss_dice_8: 4.16  time: 1.5283  data_time: 0.0776  lr: 9.2773e-06  max_mem: 21408M
[01/17 20:00:23] d2.utils.events INFO:  eta: 15:04:42  iter: 3219  total_loss: 51.42  loss_ce: 0.4057  loss_mask: 0.535  loss_dice: 4.142  loss_ce_0: 0.776  loss_mask_0: 0.5207  loss_dice_0: 4.233  loss_ce_1: 0.4544  loss_mask_1: 0.5292  loss_dice_1: 4.163  loss_ce_2: 0.4304  loss_mask_2: 0.5308  loss_dice_2: 4.148  loss_ce_3: 0.4099  loss_mask_3: 0.5335  loss_dice_3: 4.142  loss_ce_4: 0.4293  loss_mask_4: 0.5312  loss_dice_4: 4.141  loss_ce_5: 0.4206  loss_mask_5: 0.5306  loss_dice_5: 4.136  loss_ce_6: 0.4241  loss_mask_6: 0.5316  loss_dice_6: 4.135  loss_ce_7: 0.3955  loss_mask_7: 0.5341  loss_dice_7: 4.14  loss_ce_8: 0.3978  loss_mask_8: 0.533  loss_dice_8: 4.148  time: 1.5280  data_time: 0.0751  lr: 9.2727e-06  max_mem: 21408M
[01/17 20:00:52] d2.utils.events INFO:  eta: 15:04:21  iter: 3239  total_loss: 51.78  loss_ce: 0.4297  loss_mask: 0.5529  loss_dice: 4.146  loss_ce_0: 0.7789  loss_mask_0: 0.5341  loss_dice_0: 4.237  loss_ce_1: 0.4615  loss_mask_1: 0.5467  loss_dice_1: 4.168  loss_ce_2: 0.4408  loss_mask_2: 0.5501  loss_dice_2: 4.148  loss_ce_3: 0.441  loss_mask_3: 0.551  loss_dice_3: 4.141  loss_ce_4: 0.4282  loss_mask_4: 0.5517  loss_dice_4: 4.136  loss_ce_5: 0.4333  loss_mask_5: 0.5537  loss_dice_5: 4.139  loss_ce_6: 0.4239  loss_mask_6: 0.5517  loss_dice_6: 4.134  loss_ce_7: 0.4108  loss_mask_7: 0.554  loss_dice_7: 4.144  loss_ce_8: 0.4205  loss_mask_8: 0.5553  loss_dice_8: 4.142  time: 1.5276  data_time: 0.0701  lr: 9.2682e-06  max_mem: 21408M
[01/17 20:01:22] d2.utils.events INFO:  eta: 15:02:58  iter: 3259  total_loss: 51.42  loss_ce: 0.4055  loss_mask: 0.5514  loss_dice: 4.118  loss_ce_0: 0.7581  loss_mask_0: 0.5313  loss_dice_0: 4.212  loss_ce_1: 0.4388  loss_mask_1: 0.5433  loss_dice_1: 4.149  loss_ce_2: 0.4254  loss_mask_2: 0.5486  loss_dice_2: 4.118  loss_ce_3: 0.4246  loss_mask_3: 0.5499  loss_dice_3: 4.12  loss_ce_4: 0.4271  loss_mask_4: 0.5483  loss_dice_4: 4.114  loss_ce_5: 0.4093  loss_mask_5: 0.5492  loss_dice_5: 4.123  loss_ce_6: 0.425  loss_mask_6: 0.5517  loss_dice_6: 4.121  loss_ce_7: 0.4175  loss_mask_7: 0.5517  loss_dice_7: 4.127  loss_ce_8: 0.4022  loss_mask_8: 0.5529  loss_dice_8: 4.127  time: 1.5272  data_time: 0.0732  lr: 9.2636e-06  max_mem: 21408M
[01/17 20:01:51] d2.utils.events INFO:  eta: 15:02:06  iter: 3279  total_loss: 51.48  loss_ce: 0.3694  loss_mask: 0.5581  loss_dice: 4.165  loss_ce_0: 0.7354  loss_mask_0: 0.5357  loss_dice_0: 4.25  loss_ce_1: 0.425  loss_mask_1: 0.5528  loss_dice_1: 4.188  loss_ce_2: 0.3973  loss_mask_2: 0.5539  loss_dice_2: 4.173  loss_ce_3: 0.3944  loss_mask_3: 0.5553  loss_dice_3: 4.165  loss_ce_4: 0.3843  loss_mask_4: 0.5562  loss_dice_4: 4.163  loss_ce_5: 0.3898  loss_mask_5: 0.5584  loss_dice_5: 4.162  loss_ce_6: 0.372  loss_mask_6: 0.5613  loss_dice_6: 4.161  loss_ce_7: 0.3803  loss_mask_7: 0.5575  loss_dice_7: 4.169  loss_ce_8: 0.3695  loss_mask_8: 0.5577  loss_dice_8: 4.165  time: 1.5269  data_time: 0.0662  lr: 9.2591e-06  max_mem: 21408M
[01/17 20:02:21] d2.utils.events INFO:  eta: 15:01:41  iter: 3299  total_loss: 51.6  loss_ce: 0.4206  loss_mask: 0.54  loss_dice: 4.164  loss_ce_0: 0.7427  loss_mask_0: 0.5268  loss_dice_0: 4.254  loss_ce_1: 0.4423  loss_mask_1: 0.5322  loss_dice_1: 4.189  loss_ce_2: 0.4412  loss_mask_2: 0.5336  loss_dice_2: 4.17  loss_ce_3: 0.4296  loss_mask_3: 0.5364  loss_dice_3: 4.159  loss_ce_4: 0.4232  loss_mask_4: 0.5339  loss_dice_4: 4.158  loss_ce_5: 0.4167  loss_mask_5: 0.5362  loss_dice_5: 4.164  loss_ce_6: 0.4202  loss_mask_6: 0.5402  loss_dice_6: 4.154  loss_ce_7: 0.4037  loss_mask_7: 0.5424  loss_dice_7: 4.16  loss_ce_8: 0.4106  loss_mask_8: 0.5416  loss_dice_8: 4.166  time: 1.5266  data_time: 0.0757  lr: 9.2546e-06  max_mem: 21408M
[01/17 20:02:51] d2.utils.events INFO:  eta: 15:01:11  iter: 3319  total_loss: 51.63  loss_ce: 0.4376  loss_mask: 0.5438  loss_dice: 4.125  loss_ce_0: 0.7311  loss_mask_0: 0.5217  loss_dice_0: 4.226  loss_ce_1: 0.4515  loss_mask_1: 0.5325  loss_dice_1: 4.164  loss_ce_2: 0.4584  loss_mask_2: 0.5338  loss_dice_2: 4.14  loss_ce_3: 0.4272  loss_mask_3: 0.5355  loss_dice_3: 4.136  loss_ce_4: 0.44  loss_mask_4: 0.5371  loss_dice_4: 4.131  loss_ce_5: 0.4205  loss_mask_5: 0.5377  loss_dice_5: 4.135  loss_ce_6: 0.4347  loss_mask_6: 0.5398  loss_dice_6: 4.127  loss_ce_7: 0.4162  loss_mask_7: 0.5397  loss_dice_7: 4.14  loss_ce_8: 0.4104  loss_mask_8: 0.5391  loss_dice_8: 4.129  time: 1.5264  data_time: 0.0714  lr: 9.25e-06  max_mem: 21408M
[01/17 20:03:20] d2.utils.events INFO:  eta: 15:00:22  iter: 3339  total_loss: 51.58  loss_ce: 0.4233  loss_mask: 0.5633  loss_dice: 4.13  loss_ce_0: 0.7812  loss_mask_0: 0.5463  loss_dice_0: 4.213  loss_ce_1: 0.4485  loss_mask_1: 0.5556  loss_dice_1: 4.144  loss_ce_2: 0.4236  loss_mask_2: 0.5593  loss_dice_2: 4.124  loss_ce_3: 0.436  loss_mask_3: 0.5548  loss_dice_3: 4.124  loss_ce_4: 0.4284  loss_mask_4: 0.5546  loss_dice_4: 4.131  loss_ce_5: 0.4172  loss_mask_5: 0.5554  loss_dice_5: 4.127  loss_ce_6: 0.4186  loss_mask_6: 0.5601  loss_dice_6: 4.135  loss_ce_7: 0.4118  loss_mask_7: 0.5647  loss_dice_7: 4.133  loss_ce_8: 0.4094  loss_mask_8: 0.5635  loss_dice_8: 4.127  time: 1.5261  data_time: 0.0724  lr: 9.2455e-06  max_mem: 21408M
[01/17 20:03:50] d2.utils.events INFO:  eta: 14:59:31  iter: 3359  total_loss: 51.32  loss_ce: 0.4178  loss_mask: 0.5715  loss_dice: 4.125  loss_ce_0: 0.7433  loss_mask_0: 0.551  loss_dice_0: 4.214  loss_ce_1: 0.4156  loss_mask_1: 0.5656  loss_dice_1: 4.153  loss_ce_2: 0.4172  loss_mask_2: 0.5674  loss_dice_2: 4.14  loss_ce_3: 0.4151  loss_mask_3: 0.5676  loss_dice_3: 4.131  loss_ce_4: 0.4267  loss_mask_4: 0.5671  loss_dice_4: 4.124  loss_ce_5: 0.4167  loss_mask_5: 0.5691  loss_dice_5: 4.117  loss_ce_6: 0.4063  loss_mask_6: 0.57  loss_dice_6: 4.122  loss_ce_7: 0.3774  loss_mask_7: 0.5706  loss_dice_7: 4.126  loss_ce_8: 0.4138  loss_mask_8: 0.5695  loss_dice_8: 4.134  time: 1.5257  data_time: 0.0674  lr: 9.2409e-06  max_mem: 21408M
[01/17 20:04:19] d2.utils.events INFO:  eta: 14:58:54  iter: 3379  total_loss: 51.58  loss_ce: 0.4332  loss_mask: 0.5406  loss_dice: 4.147  loss_ce_0: 0.7788  loss_mask_0: 0.5188  loss_dice_0: 4.229  loss_ce_1: 0.4626  loss_mask_1: 0.5329  loss_dice_1: 4.173  loss_ce_2: 0.4606  loss_mask_2: 0.5384  loss_dice_2: 4.156  loss_ce_3: 0.4511  loss_mask_3: 0.5381  loss_dice_3: 4.141  loss_ce_4: 0.4393  loss_mask_4: 0.5363  loss_dice_4: 4.147  loss_ce_5: 0.4334  loss_mask_5: 0.542  loss_dice_5: 4.151  loss_ce_6: 0.432  loss_mask_6: 0.5416  loss_dice_6: 4.148  loss_ce_7: 0.4178  loss_mask_7: 0.5439  loss_dice_7: 4.146  loss_ce_8: 0.4337  loss_mask_8: 0.5443  loss_dice_8: 4.143  time: 1.5255  data_time: 0.0714  lr: 9.2364e-06  max_mem: 21408M
[01/17 20:04:49] d2.utils.events INFO:  eta: 14:57:50  iter: 3399  total_loss: 51.19  loss_ce: 0.4113  loss_mask: 0.5478  loss_dice: 4.114  loss_ce_0: 0.7573  loss_mask_0: 0.5205  loss_dice_0: 4.213  loss_ce_1: 0.4227  loss_mask_1: 0.5376  loss_dice_1: 4.144  loss_ce_2: 0.412  loss_mask_2: 0.5443  loss_dice_2: 4.125  loss_ce_3: 0.42  loss_mask_3: 0.5464  loss_dice_3: 4.118  loss_ce_4: 0.4201  loss_mask_4: 0.549  loss_dice_4: 4.108  loss_ce_5: 0.4197  loss_mask_5: 0.5489  loss_dice_5: 4.114  loss_ce_6: 0.4121  loss_mask_6: 0.5482  loss_dice_6: 4.112  loss_ce_7: 0.4091  loss_mask_7: 0.5477  loss_dice_7: 4.114  loss_ce_8: 0.4162  loss_mask_8: 0.5476  loss_dice_8: 4.118  time: 1.5251  data_time: 0.0699  lr: 9.2319e-06  max_mem: 21408M
[01/17 20:05:18] d2.utils.events INFO:  eta: 14:56:13  iter: 3419  total_loss: 51.43  loss_ce: 0.4165  loss_mask: 0.5739  loss_dice: 4.095  loss_ce_0: 0.7337  loss_mask_0: 0.5428  loss_dice_0: 4.206  loss_ce_1: 0.4332  loss_mask_1: 0.565  loss_dice_1: 4.135  loss_ce_2: 0.4212  loss_mask_2: 0.573  loss_dice_2: 4.122  loss_ce_3: 0.4204  loss_mask_3: 0.5743  loss_dice_3: 4.099  loss_ce_4: 0.4164  loss_mask_4: 0.5717  loss_dice_4: 4.102  loss_ce_5: 0.4222  loss_mask_5: 0.5765  loss_dice_5: 4.107  loss_ce_6: 0.4023  loss_mask_6: 0.5776  loss_dice_6: 4.104  loss_ce_7: 0.4144  loss_mask_7: 0.5732  loss_dice_7: 4.109  loss_ce_8: 0.4029  loss_mask_8: 0.5743  loss_dice_8: 4.109  time: 1.5247  data_time: 0.0713  lr: 9.2273e-06  max_mem: 21408M
[01/17 20:05:47] d2.utils.events INFO:  eta: 14:55:40  iter: 3439  total_loss: 51.45  loss_ce: 0.4386  loss_mask: 0.5497  loss_dice: 4.135  loss_ce_0: 0.775  loss_mask_0: 0.5255  loss_dice_0: 4.213  loss_ce_1: 0.4782  loss_mask_1: 0.5374  loss_dice_1: 4.153  loss_ce_2: 0.4337  loss_mask_2: 0.5402  loss_dice_2: 4.134  loss_ce_3: 0.4363  loss_mask_3: 0.542  loss_dice_3: 4.131  loss_ce_4: 0.4597  loss_mask_4: 0.5441  loss_dice_4: 4.128  loss_ce_5: 0.4483  loss_mask_5: 0.5472  loss_dice_5: 4.129  loss_ce_6: 0.4602  loss_mask_6: 0.5513  loss_dice_6: 4.123  loss_ce_7: 0.4394  loss_mask_7: 0.5527  loss_dice_7: 4.125  loss_ce_8: 0.4198  loss_mask_8: 0.5525  loss_dice_8: 4.127  time: 1.5243  data_time: 0.0647  lr: 9.2228e-06  max_mem: 21408M
[01/17 20:06:16] d2.utils.events INFO:  eta: 14:55:00  iter: 3459  total_loss: 51.33  loss_ce: 0.4134  loss_mask: 0.5436  loss_dice: 4.128  loss_ce_0: 0.7265  loss_mask_0: 0.5334  loss_dice_0: 4.222  loss_ce_1: 0.4366  loss_mask_1: 0.5374  loss_dice_1: 4.152  loss_ce_2: 0.4273  loss_mask_2: 0.5425  loss_dice_2: 4.133  loss_ce_3: 0.4298  loss_mask_3: 0.543  loss_dice_3: 4.132  loss_ce_4: 0.4365  loss_mask_4: 0.5428  loss_dice_4: 4.124  loss_ce_5: 0.41  loss_mask_5: 0.5415  loss_dice_5: 4.127  loss_ce_6: 0.4068  loss_mask_6: 0.5402  loss_dice_6: 4.132  loss_ce_7: 0.4049  loss_mask_7: 0.543  loss_dice_7: 4.124  loss_ce_8: 0.4104  loss_mask_8: 0.5458  loss_dice_8: 4.13  time: 1.5240  data_time: 0.0701  lr: 9.2182e-06  max_mem: 21408M
[01/17 20:06:46] d2.utils.events INFO:  eta: 14:54:13  iter: 3479  total_loss: 51.4  loss_ce: 0.43  loss_mask: 0.5343  loss_dice: 4.101  loss_ce_0: 0.7459  loss_mask_0: 0.5192  loss_dice_0: 4.2  loss_ce_1: 0.4803  loss_mask_1: 0.5307  loss_dice_1: 4.128  loss_ce_2: 0.4523  loss_mask_2: 0.5312  loss_dice_2: 4.111  loss_ce_3: 0.4424  loss_mask_3: 0.5343  loss_dice_3: 4.105  loss_ce_4: 0.4585  loss_mask_4: 0.5337  loss_dice_4: 4.104  loss_ce_5: 0.457  loss_mask_5: 0.536  loss_dice_5: 4.107  loss_ce_6: 0.4464  loss_mask_6: 0.5385  loss_dice_6: 4.102  loss_ce_7: 0.4385  loss_mask_7: 0.534  loss_dice_7: 4.106  loss_ce_8: 0.4231  loss_mask_8: 0.5372  loss_dice_8: 4.104  time: 1.5237  data_time: 0.0706  lr: 9.2137e-06  max_mem: 21408M
[01/17 20:07:15] d2.utils.events INFO:  eta: 14:53:26  iter: 3499  total_loss: 50.9  loss_ce: 0.3929  loss_mask: 0.5484  loss_dice: 4.085  loss_ce_0: 0.739  loss_mask_0: 0.5329  loss_dice_0: 4.182  loss_ce_1: 0.4022  loss_mask_1: 0.5448  loss_dice_1: 4.111  loss_ce_2: 0.4032  loss_mask_2: 0.5483  loss_dice_2: 4.096  loss_ce_3: 0.4066  loss_mask_3: 0.5498  loss_dice_3: 4.085  loss_ce_4: 0.3926  loss_mask_4: 0.5483  loss_dice_4: 4.085  loss_ce_5: 0.3802  loss_mask_5: 0.55  loss_dice_5: 4.09  loss_ce_6: 0.3998  loss_mask_6: 0.5494  loss_dice_6: 4.083  loss_ce_7: 0.3838  loss_mask_7: 0.5496  loss_dice_7: 4.085  loss_ce_8: 0.3787  loss_mask_8: 0.5499  loss_dice_8: 4.087  time: 1.5234  data_time: 0.0650  lr: 9.2092e-06  max_mem: 21408M
[01/17 20:07:45] d2.utils.events INFO:  eta: 14:52:09  iter: 3519  total_loss: 50.93  loss_ce: 0.4316  loss_mask: 0.5494  loss_dice: 4.076  loss_ce_0: 0.7527  loss_mask_0: 0.5302  loss_dice_0: 4.179  loss_ce_1: 0.4453  loss_mask_1: 0.5421  loss_dice_1: 4.097  loss_ce_2: 0.4435  loss_mask_2: 0.5438  loss_dice_2: 4.087  loss_ce_3: 0.4361  loss_mask_3: 0.5415  loss_dice_3: 4.078  loss_ce_4: 0.4426  loss_mask_4: 0.5435  loss_dice_4: 4.081  loss_ce_5: 0.4505  loss_mask_5: 0.5469  loss_dice_5: 4.073  loss_ce_6: 0.4356  loss_mask_6: 0.5502  loss_dice_6: 4.066  loss_ce_7: 0.425  loss_mask_7: 0.5517  loss_dice_7: 4.076  loss_ce_8: 0.4195  loss_mask_8: 0.5497  loss_dice_8: 4.08  time: 1.5230  data_time: 0.0677  lr: 9.2046e-06  max_mem: 21408M
[01/17 20:08:14] d2.utils.events INFO:  eta: 14:52:06  iter: 3539  total_loss: 51.11  loss_ce: 0.4353  loss_mask: 0.5442  loss_dice: 4.088  loss_ce_0: 0.7592  loss_mask_0: 0.5287  loss_dice_0: 4.184  loss_ce_1: 0.4411  loss_mask_1: 0.5376  loss_dice_1: 4.122  loss_ce_2: 0.4376  loss_mask_2: 0.5427  loss_dice_2: 4.088  loss_ce_3: 0.4438  loss_mask_3: 0.5451  loss_dice_3: 4.083  loss_ce_4: 0.4358  loss_mask_4: 0.5451  loss_dice_4: 4.082  loss_ce_5: 0.4474  loss_mask_5: 0.5397  loss_dice_5: 4.081  loss_ce_6: 0.4337  loss_mask_6: 0.544  loss_dice_6: 4.081  loss_ce_7: 0.4224  loss_mask_7: 0.5453  loss_dice_7: 4.09  loss_ce_8: 0.4382  loss_mask_8: 0.5435  loss_dice_8: 4.083  time: 1.5227  data_time: 0.0737  lr: 9.2001e-06  max_mem: 21408M
[01/17 20:08:43] d2.utils.events INFO:  eta: 14:50:50  iter: 3559  total_loss: 50.86  loss_ce: 0.4252  loss_mask: 0.5566  loss_dice: 4.057  loss_ce_0: 0.7605  loss_mask_0: 0.5369  loss_dice_0: 4.155  loss_ce_1: 0.4425  loss_mask_1: 0.5527  loss_dice_1: 4.079  loss_ce_2: 0.4193  loss_mask_2: 0.5534  loss_dice_2: 4.067  loss_ce_3: 0.416  loss_mask_3: 0.5537  loss_dice_3: 4.063  loss_ce_4: 0.4225  loss_mask_4: 0.5559  loss_dice_4: 4.059  loss_ce_5: 0.4316  loss_mask_5: 0.5572  loss_dice_5: 4.052  loss_ce_6: 0.4157  loss_mask_6: 0.5525  loss_dice_6: 4.071  loss_ce_7: 0.4157  loss_mask_7: 0.5584  loss_dice_7: 4.061  loss_ce_8: 0.4132  loss_mask_8: 0.5572  loss_dice_8: 4.064  time: 1.5223  data_time: 0.0616  lr: 9.1955e-06  max_mem: 21408M
[01/17 20:09:12] d2.utils.events INFO:  eta: 14:50:39  iter: 3579  total_loss: 50.96  loss_ce: 0.4362  loss_mask: 0.5502  loss_dice: 4.07  loss_ce_0: 0.7591  loss_mask_0: 0.532  loss_dice_0: 4.167  loss_ce_1: 0.5121  loss_mask_1: 0.5464  loss_dice_1: 4.095  loss_ce_2: 0.4547  loss_mask_2: 0.5492  loss_dice_2: 4.08  loss_ce_3: 0.4526  loss_mask_3: 0.5548  loss_dice_3: 4.057  loss_ce_4: 0.4486  loss_mask_4: 0.5503  loss_dice_4: 4.061  loss_ce_5: 0.444  loss_mask_5: 0.554  loss_dice_5: 4.067  loss_ce_6: 0.4468  loss_mask_6: 0.557  loss_dice_6: 4.056  loss_ce_7: 0.4231  loss_mask_7: 0.5541  loss_dice_7: 4.07  loss_ce_8: 0.4337  loss_mask_8: 0.5527  loss_dice_8: 4.069  time: 1.5219  data_time: 0.0639  lr: 9.191e-06  max_mem: 21408M
[01/17 20:09:42] d2.utils.events INFO:  eta: 14:50:12  iter: 3599  total_loss: 51.17  loss_ce: 0.4412  loss_mask: 0.5671  loss_dice: 4.045  loss_ce_0: 0.7356  loss_mask_0: 0.5533  loss_dice_0: 4.15  loss_ce_1: 0.474  loss_mask_1: 0.5635  loss_dice_1: 4.068  loss_ce_2: 0.4568  loss_mask_2: 0.567  loss_dice_2: 4.044  loss_ce_3: 0.4479  loss_mask_3: 0.5669  loss_dice_3: 4.052  loss_ce_4: 0.4443  loss_mask_4: 0.5643  loss_dice_4: 4.059  loss_ce_5: 0.448  loss_mask_5: 0.5688  loss_dice_5: 4.047  loss_ce_6: 0.4431  loss_mask_6: 0.5675  loss_dice_6: 4.048  loss_ce_7: 0.4442  loss_mask_7: 0.5677  loss_dice_7: 4.045  loss_ce_8: 0.431  loss_mask_8: 0.567  loss_dice_8: 4.046  time: 1.5216  data_time: 0.0746  lr: 9.1865e-06  max_mem: 21408M
[01/17 20:10:11] d2.utils.events INFO:  eta: 14:49:55  iter: 3619  total_loss: 50.85  loss_ce: 0.4126  loss_mask: 0.5472  loss_dice: 4.091  loss_ce_0: 0.7272  loss_mask_0: 0.5311  loss_dice_0: 4.186  loss_ce_1: 0.4367  loss_mask_1: 0.5422  loss_dice_1: 4.115  loss_ce_2: 0.4303  loss_mask_2: 0.5447  loss_dice_2: 4.102  loss_ce_3: 0.4155  loss_mask_3: 0.5466  loss_dice_3: 4.098  loss_ce_4: 0.4113  loss_mask_4: 0.5457  loss_dice_4: 4.086  loss_ce_5: 0.4088  loss_mask_5: 0.5477  loss_dice_5: 4.089  loss_ce_6: 0.4316  loss_mask_6: 0.5467  loss_dice_6: 4.084  loss_ce_7: 0.4052  loss_mask_7: 0.5492  loss_dice_7: 4.087  loss_ce_8: 0.4032  loss_mask_8: 0.5487  loss_dice_8: 4.096  time: 1.5213  data_time: 0.0813  lr: 9.1819e-06  max_mem: 21408M
[01/17 20:10:41] d2.utils.events INFO:  eta: 14:49:48  iter: 3639  total_loss: 51.1  loss_ce: 0.4211  loss_mask: 0.5368  loss_dice: 4.108  loss_ce_0: 0.731  loss_mask_0: 0.5264  loss_dice_0: 4.191  loss_ce_1: 0.4763  loss_mask_1: 0.5309  loss_dice_1: 4.124  loss_ce_2: 0.4295  loss_mask_2: 0.5323  loss_dice_2: 4.11  loss_ce_3: 0.4492  loss_mask_3: 0.5361  loss_dice_3: 4.106  loss_ce_4: 0.4371  loss_mask_4: 0.5359  loss_dice_4: 4.107  loss_ce_5: 0.4371  loss_mask_5: 0.5401  loss_dice_5: 4.098  loss_ce_6: 0.4277  loss_mask_6: 0.5408  loss_dice_6: 4.099  loss_ce_7: 0.4186  loss_mask_7: 0.5411  loss_dice_7: 4.108  loss_ce_8: 0.432  loss_mask_8: 0.5368  loss_dice_8: 4.104  time: 1.5211  data_time: 0.0738  lr: 9.1774e-06  max_mem: 21408M
[01/17 20:11:10] d2.utils.events INFO:  eta: 14:49:26  iter: 3659  total_loss: 50.91  loss_ce: 0.4216  loss_mask: 0.562  loss_dice: 4.063  loss_ce_0: 0.7084  loss_mask_0: 0.5407  loss_dice_0: 4.163  loss_ce_1: 0.4396  loss_mask_1: 0.5479  loss_dice_1: 4.095  loss_ce_2: 0.4346  loss_mask_2: 0.5517  loss_dice_2: 4.063  loss_ce_3: 0.4277  loss_mask_3: 0.5582  loss_dice_3: 4.058  loss_ce_4: 0.4325  loss_mask_4: 0.5556  loss_dice_4: 4.057  loss_ce_5: 0.4383  loss_mask_5: 0.5563  loss_dice_5: 4.065  loss_ce_6: 0.4367  loss_mask_6: 0.558  loss_dice_6: 4.059  loss_ce_7: 0.426  loss_mask_7: 0.5648  loss_dice_7: 4.069  loss_ce_8: 0.4195  loss_mask_8: 0.562  loss_dice_8: 4.067  time: 1.5208  data_time: 0.0662  lr: 9.1728e-06  max_mem: 21408M
[01/17 20:11:40] d2.utils.events INFO:  eta: 14:48:57  iter: 3679  total_loss: 51  loss_ce: 0.4237  loss_mask: 0.55  loss_dice: 4.089  loss_ce_0: 0.7545  loss_mask_0: 0.5216  loss_dice_0: 4.186  loss_ce_1: 0.4481  loss_mask_1: 0.5488  loss_dice_1: 4.124  loss_ce_2: 0.4263  loss_mask_2: 0.554  loss_dice_2: 4.106  loss_ce_3: 0.4352  loss_mask_3: 0.551  loss_dice_3: 4.094  loss_ce_4: 0.4238  loss_mask_4: 0.5533  loss_dice_4: 4.088  loss_ce_5: 0.4389  loss_mask_5: 0.5502  loss_dice_5: 4.091  loss_ce_6: 0.4413  loss_mask_6: 0.5513  loss_dice_6: 4.091  loss_ce_7: 0.4259  loss_mask_7: 0.5534  loss_dice_7: 4.09  loss_ce_8: 0.423  loss_mask_8: 0.5543  loss_dice_8: 4.091  time: 1.5205  data_time: 0.0741  lr: 9.1683e-06  max_mem: 21408M
[01/17 20:12:10] d2.utils.events INFO:  eta: 14:48:33  iter: 3699  total_loss: 50.6  loss_ce: 0.383  loss_mask: 0.5551  loss_dice: 4.071  loss_ce_0: 0.715  loss_mask_0: 0.539  loss_dice_0: 4.15  loss_ce_1: 0.4367  loss_mask_1: 0.5472  loss_dice_1: 4.097  loss_ce_2: 0.4017  loss_mask_2: 0.5526  loss_dice_2: 4.082  loss_ce_3: 0.4063  loss_mask_3: 0.5544  loss_dice_3: 4.07  loss_ce_4: 0.4038  loss_mask_4: 0.5546  loss_dice_4: 4.062  loss_ce_5: 0.397  loss_mask_5: 0.5576  loss_dice_5: 4.063  loss_ce_6: 0.3962  loss_mask_6: 0.5575  loss_dice_6: 4.058  loss_ce_7: 0.3734  loss_mask_7: 0.5554  loss_dice_7: 4.073  loss_ce_8: 0.3872  loss_mask_8: 0.5564  loss_dice_8: 4.061  time: 1.5204  data_time: 0.0816  lr: 9.1637e-06  max_mem: 21408M
[01/17 20:12:40] d2.utils.events INFO:  eta: 14:48:45  iter: 3719  total_loss: 50.96  loss_ce: 0.4137  loss_mask: 0.5546  loss_dice: 4.079  loss_ce_0: 0.7122  loss_mask_0: 0.5325  loss_dice_0: 4.189  loss_ce_1: 0.4408  loss_mask_1: 0.5459  loss_dice_1: 4.112  loss_ce_2: 0.4189  loss_mask_2: 0.5484  loss_dice_2: 4.1  loss_ce_3: 0.4195  loss_mask_3: 0.5513  loss_dice_3: 4.086  loss_ce_4: 0.4186  loss_mask_4: 0.5528  loss_dice_4: 4.084  loss_ce_5: 0.4056  loss_mask_5: 0.5506  loss_dice_5: 4.088  loss_ce_6: 0.4043  loss_mask_6: 0.5532  loss_dice_6: 4.08  loss_ce_7: 0.3875  loss_mask_7: 0.5553  loss_dice_7: 4.093  loss_ce_8: 0.402  loss_mask_8: 0.5537  loss_dice_8: 4.086  time: 1.5204  data_time: 0.0838  lr: 9.1592e-06  max_mem: 21408M
[01/17 20:13:10] d2.utils.events INFO:  eta: 14:48:54  iter: 3739  total_loss: 51.12  loss_ce: 0.441  loss_mask: 0.5472  loss_dice: 4.074  loss_ce_0: 0.7584  loss_mask_0: 0.5192  loss_dice_0: 4.173  loss_ce_1: 0.4672  loss_mask_1: 0.5378  loss_dice_1: 4.097  loss_ce_2: 0.4296  loss_mask_2: 0.5427  loss_dice_2: 4.077  loss_ce_3: 0.4286  loss_mask_3: 0.5436  loss_dice_3: 4.074  loss_ce_4: 0.4439  loss_mask_4: 0.5434  loss_dice_4: 4.071  loss_ce_5: 0.4324  loss_mask_5: 0.5467  loss_dice_5: 4.08  loss_ce_6: 0.444  loss_mask_6: 0.5442  loss_dice_6: 4.079  loss_ce_7: 0.4255  loss_mask_7: 0.551  loss_dice_7: 4.088  loss_ce_8: 0.4256  loss_mask_8: 0.5496  loss_dice_8: 4.086  time: 1.5203  data_time: 0.0749  lr: 9.1547e-06  max_mem: 21408M
[01/17 20:13:40] d2.utils.events INFO:  eta: 14:48:57  iter: 3759  total_loss: 50.74  loss_ce: 0.4392  loss_mask: 0.5362  loss_dice: 4.07  loss_ce_0: 0.7354  loss_mask_0: 0.5186  loss_dice_0: 4.169  loss_ce_1: 0.4574  loss_mask_1: 0.5331  loss_dice_1: 4.081  loss_ce_2: 0.4479  loss_mask_2: 0.5337  loss_dice_2: 4.072  loss_ce_3: 0.4373  loss_mask_3: 0.5356  loss_dice_3: 4.059  loss_ce_4: 0.4327  loss_mask_4: 0.5356  loss_dice_4: 4.058  loss_ce_5: 0.4419  loss_mask_5: 0.5378  loss_dice_5: 4.058  loss_ce_6: 0.4323  loss_mask_6: 0.5397  loss_dice_6: 4.061  loss_ce_7: 0.4375  loss_mask_7: 0.5389  loss_dice_7: 4.063  loss_ce_8: 0.4263  loss_mask_8: 0.5384  loss_dice_8: 4.063  time: 1.5202  data_time: 0.0750  lr: 9.1501e-06  max_mem: 21408M
[01/17 20:14:10] d2.utils.events INFO:  eta: 14:48:41  iter: 3779  total_loss: 51.32  loss_ce: 0.4502  loss_mask: 0.5504  loss_dice: 4.075  loss_ce_0: 0.7295  loss_mask_0: 0.5368  loss_dice_0: 4.177  loss_ce_1: 0.4686  loss_mask_1: 0.5451  loss_dice_1: 4.113  loss_ce_2: 0.4425  loss_mask_2: 0.5508  loss_dice_2: 4.083  loss_ce_3: 0.4593  loss_mask_3: 0.5529  loss_dice_3: 4.085  loss_ce_4: 0.4597  loss_mask_4: 0.5517  loss_dice_4: 4.087  loss_ce_5: 0.4534  loss_mask_5: 0.5482  loss_dice_5: 4.084  loss_ce_6: 0.4648  loss_mask_6: 0.5495  loss_dice_6: 4.078  loss_ce_7: 0.4366  loss_mask_7: 0.5533  loss_dice_7: 4.091  loss_ce_8: 0.4402  loss_mask_8: 0.5537  loss_dice_8: 4.084  time: 1.5201  data_time: 0.0844  lr: 9.1456e-06  max_mem: 21408M
[01/17 20:14:41] d2.utils.events INFO:  eta: 14:48:20  iter: 3799  total_loss: 50.71  loss_ce: 0.4225  loss_mask: 0.5361  loss_dice: 4.097  loss_ce_0: 0.7056  loss_mask_0: 0.522  loss_dice_0: 4.187  loss_ce_1: 0.4318  loss_mask_1: 0.5333  loss_dice_1: 4.116  loss_ce_2: 0.4046  loss_mask_2: 0.5373  loss_dice_2: 4.096  loss_ce_3: 0.408  loss_mask_3: 0.5375  loss_dice_3: 4.08  loss_ce_4: 0.4193  loss_mask_4: 0.5388  loss_dice_4: 4.087  loss_ce_5: 0.4155  loss_mask_5: 0.5406  loss_dice_5: 4.083  loss_ce_6: 0.4019  loss_mask_6: 0.541  loss_dice_6: 4.087  loss_ce_7: 0.4012  loss_mask_7: 0.5411  loss_dice_7: 4.095  loss_ce_8: 0.4144  loss_mask_8: 0.5372  loss_dice_8: 4.093  time: 1.5200  data_time: 0.0781  lr: 9.141e-06  max_mem: 21408M
[01/17 20:15:11] d2.utils.events INFO:  eta: 14:47:56  iter: 3819  total_loss: 50.71  loss_ce: 0.4121  loss_mask: 0.5482  loss_dice: 4.051  loss_ce_0: 0.6765  loss_mask_0: 0.5308  loss_dice_0: 4.15  loss_ce_1: 0.4319  loss_mask_1: 0.5361  loss_dice_1: 4.095  loss_ce_2: 0.4158  loss_mask_2: 0.5379  loss_dice_2: 4.075  loss_ce_3: 0.4008  loss_mask_3: 0.5371  loss_dice_3: 4.057  loss_ce_4: 0.4153  loss_mask_4: 0.5385  loss_dice_4: 4.053  loss_ce_5: 0.4038  loss_mask_5: 0.5412  loss_dice_5: 4.059  loss_ce_6: 0.4072  loss_mask_6: 0.5412  loss_dice_6: 4.05  loss_ce_7: 0.3953  loss_mask_7: 0.5428  loss_dice_7: 4.066  loss_ce_8: 0.3898  loss_mask_8: 0.5458  loss_dice_8: 4.06  time: 1.5200  data_time: 0.0886  lr: 9.1365e-06  max_mem: 21408M
[01/17 20:15:41] d2.utils.events INFO:  eta: 14:47:36  iter: 3839  total_loss: 50.97  loss_ce: 0.4151  loss_mask: 0.5585  loss_dice: 4.107  loss_ce_0: 0.6958  loss_mask_0: 0.5327  loss_dice_0: 4.179  loss_ce_1: 0.4322  loss_mask_1: 0.5474  loss_dice_1: 4.134  loss_ce_2: 0.413  loss_mask_2: 0.55  loss_dice_2: 4.11  loss_ce_3: 0.3946  loss_mask_3: 0.5501  loss_dice_3: 4.101  loss_ce_4: 0.4126  loss_mask_4: 0.5464  loss_dice_4: 4.107  loss_ce_5: 0.4128  loss_mask_5: 0.5495  loss_dice_5: 4.1  loss_ce_6: 0.4054  loss_mask_6: 0.5507  loss_dice_6: 4.114  loss_ce_7: 0.3923  loss_mask_7: 0.5538  loss_dice_7: 4.108  loss_ce_8: 0.4016  loss_mask_8: 0.5582  loss_dice_8: 4.112  time: 1.5200  data_time: 0.0905  lr: 9.1319e-06  max_mem: 21408M
[01/17 20:16:11] d2.utils.events INFO:  eta: 14:47:11  iter: 3859  total_loss: 50.74  loss_ce: 0.4269  loss_mask: 0.5433  loss_dice: 4.059  loss_ce_0: 0.6992  loss_mask_0: 0.5243  loss_dice_0: 4.156  loss_ce_1: 0.4407  loss_mask_1: 0.5358  loss_dice_1: 4.098  loss_ce_2: 0.4429  loss_mask_2: 0.54  loss_dice_2: 4.074  loss_ce_3: 0.4279  loss_mask_3: 0.5393  loss_dice_3: 4.07  loss_ce_4: 0.4439  loss_mask_4: 0.5398  loss_dice_4: 4.057  loss_ce_5: 0.4377  loss_mask_5: 0.5394  loss_dice_5: 4.053  loss_ce_6: 0.4286  loss_mask_6: 0.5416  loss_dice_6: 4.057  loss_ce_7: 0.4217  loss_mask_7: 0.544  loss_dice_7: 4.061  loss_ce_8: 0.4232  loss_mask_8: 0.5434  loss_dice_8: 4.061  time: 1.5199  data_time: 0.0841  lr: 9.1274e-06  max_mem: 21408M
[01/17 20:16:41] d2.utils.events INFO:  eta: 14:46:48  iter: 3879  total_loss: 50.76  loss_ce: 0.4427  loss_mask: 0.5358  loss_dice: 4.043  loss_ce_0: 0.7283  loss_mask_0: 0.5221  loss_dice_0: 4.167  loss_ce_1: 0.4717  loss_mask_1: 0.5269  loss_dice_1: 4.082  loss_ce_2: 0.4432  loss_mask_2: 0.5298  loss_dice_2: 4.068  loss_ce_3: 0.4547  loss_mask_3: 0.5331  loss_dice_3: 4.043  loss_ce_4: 0.4359  loss_mask_4: 0.5295  loss_dice_4: 4.057  loss_ce_5: 0.4484  loss_mask_5: 0.5346  loss_dice_5: 4.051  loss_ce_6: 0.4573  loss_mask_6: 0.5395  loss_dice_6: 4.042  loss_ce_7: 0.4342  loss_mask_7: 0.5394  loss_dice_7: 4.049  loss_ce_8: 0.4352  loss_mask_8: 0.5374  loss_dice_8: 4.055  time: 1.5197  data_time: 0.0616  lr: 9.1228e-06  max_mem: 21408M
[01/17 20:17:12] d2.utils.events INFO:  eta: 14:47:09  iter: 3899  total_loss: 50.62  loss_ce: 0.4193  loss_mask: 0.5404  loss_dice: 4.046  loss_ce_0: 0.6901  loss_mask_0: 0.5194  loss_dice_0: 4.15  loss_ce_1: 0.435  loss_mask_1: 0.5351  loss_dice_1: 4.078  loss_ce_2: 0.4157  loss_mask_2: 0.5355  loss_dice_2: 4.06  loss_ce_3: 0.4146  loss_mask_3: 0.5391  loss_dice_3: 4.04  loss_ce_4: 0.4217  loss_mask_4: 0.5397  loss_dice_4: 4.041  loss_ce_5: 0.4125  loss_mask_5: 0.5394  loss_dice_5: 4.045  loss_ce_6: 0.4058  loss_mask_6: 0.538  loss_dice_6: 4.055  loss_ce_7: 0.3926  loss_mask_7: 0.5422  loss_dice_7: 4.054  loss_ce_8: 0.4119  loss_mask_8: 0.5414  loss_dice_8: 4.054  time: 1.5197  data_time: 0.0796  lr: 9.1183e-06  max_mem: 21408M
[01/17 20:17:42] d2.utils.events INFO:  eta: 14:47:35  iter: 3919  total_loss: 50.54  loss_ce: 0.4223  loss_mask: 0.5488  loss_dice: 4.062  loss_ce_0: 0.7029  loss_mask_0: 0.526  loss_dice_0: 4.162  loss_ce_1: 0.4172  loss_mask_1: 0.5481  loss_dice_1: 4.078  loss_ce_2: 0.4321  loss_mask_2: 0.5484  loss_dice_2: 4.063  loss_ce_3: 0.4291  loss_mask_3: 0.5483  loss_dice_3: 4.057  loss_ce_4: 0.4258  loss_mask_4: 0.5471  loss_dice_4: 4.056  loss_ce_5: 0.4328  loss_mask_5: 0.5497  loss_dice_5: 4.059  loss_ce_6: 0.4196  loss_mask_6: 0.5509  loss_dice_6: 4.052  loss_ce_7: 0.4117  loss_mask_7: 0.5513  loss_dice_7: 4.061  loss_ce_8: 0.4086  loss_mask_8: 0.5494  loss_dice_8: 4.071  time: 1.5197  data_time: 0.0901  lr: 9.1137e-06  max_mem: 21408M
[01/17 20:18:12] d2.utils.events INFO:  eta: 14:46:29  iter: 3939  total_loss: 50.55  loss_ce: 0.4453  loss_mask: 0.5437  loss_dice: 4.024  loss_ce_0: 0.7262  loss_mask_0: 0.5314  loss_dice_0: 4.125  loss_ce_1: 0.4553  loss_mask_1: 0.5434  loss_dice_1: 4.057  loss_ce_2: 0.4405  loss_mask_2: 0.5436  loss_dice_2: 4.032  loss_ce_3: 0.4527  loss_mask_3: 0.5416  loss_dice_3: 4.033  loss_ce_4: 0.4477  loss_mask_4: 0.5422  loss_dice_4: 4.025  loss_ce_5: 0.4583  loss_mask_5: 0.5433  loss_dice_5: 4.019  loss_ce_6: 0.4536  loss_mask_6: 0.5405  loss_dice_6: 4.02  loss_ce_7: 0.4288  loss_mask_7: 0.5464  loss_dice_7: 4.016  loss_ce_8: 0.4275  loss_mask_8: 0.5446  loss_dice_8: 4.019  time: 1.5195  data_time: 0.0770  lr: 9.1092e-06  max_mem: 21408M
[01/17 20:18:42] d2.utils.events INFO:  eta: 14:46:28  iter: 3959  total_loss: 50.67  loss_ce: 0.4545  loss_mask: 0.5458  loss_dice: 4.037  loss_ce_0: 0.7227  loss_mask_0: 0.5224  loss_dice_0: 4.147  loss_ce_1: 0.4814  loss_mask_1: 0.5392  loss_dice_1: 4.067  loss_ce_2: 0.4529  loss_mask_2: 0.538  loss_dice_2: 4.053  loss_ce_3: 0.4446  loss_mask_3: 0.5396  loss_dice_3: 4.041  loss_ce_4: 0.4606  loss_mask_4: 0.5413  loss_dice_4: 4.042  loss_ce_5: 0.4606  loss_mask_5: 0.5416  loss_dice_5: 4.038  loss_ce_6: 0.4579  loss_mask_6: 0.5435  loss_dice_6: 4.039  loss_ce_7: 0.44  loss_mask_7: 0.546  loss_dice_7: 4.039  loss_ce_8: 0.441  loss_mask_8: 0.5506  loss_dice_8: 4.044  time: 1.5194  data_time: 0.0849  lr: 9.1046e-06  max_mem: 21408M
[01/17 20:19:12] d2.utils.events INFO:  eta: 14:46:24  iter: 3979  total_loss: 50.41  loss_ce: 0.4241  loss_mask: 0.5505  loss_dice: 4.032  loss_ce_0: 0.698  loss_mask_0: 0.5334  loss_dice_0: 4.119  loss_ce_1: 0.4245  loss_mask_1: 0.5411  loss_dice_1: 4.068  loss_ce_2: 0.4337  loss_mask_2: 0.5461  loss_dice_2: 4.034  loss_ce_3: 0.4328  loss_mask_3: 0.5442  loss_dice_3: 4.039  loss_ce_4: 0.4351  loss_mask_4: 0.5462  loss_dice_4: 4.029  loss_ce_5: 0.4392  loss_mask_5: 0.5517  loss_dice_5: 4.029  loss_ce_6: 0.4251  loss_mask_6: 0.5502  loss_dice_6: 4.026  loss_ce_7: 0.4144  loss_mask_7: 0.554  loss_dice_7: 4.031  loss_ce_8: 0.4182  loss_mask_8: 0.5539  loss_dice_8: 4.03  time: 1.5194  data_time: 0.0796  lr: 9.1001e-06  max_mem: 21408M
[01/17 20:19:42] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 20:19:42] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 20:19:42] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 20:19:43] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 20:19:56] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0090 s/iter. Inference: 0.1497 s/iter. Eval: 0.1680 s/iter. Total: 0.3268 s/iter. ETA=0:05:53
[01/17 20:20:01] d2.evaluation.evaluator INFO: Inference done 26/1093. Dataloading: 0.0100 s/iter. Inference: 0.1441 s/iter. Eval: 0.1872 s/iter. Total: 0.3413 s/iter. ETA=0:06:04
[01/17 20:20:06] d2.evaluation.evaluator INFO: Inference done 39/1093. Dataloading: 0.0106 s/iter. Inference: 0.1545 s/iter. Eval: 0.1984 s/iter. Total: 0.3636 s/iter. ETA=0:06:23
[01/17 20:20:11] d2.evaluation.evaluator INFO: Inference done 53/1093. Dataloading: 0.0116 s/iter. Inference: 0.1596 s/iter. Eval: 0.1954 s/iter. Total: 0.3666 s/iter. ETA=0:06:21
[01/17 20:20:17] d2.evaluation.evaluator INFO: Inference done 63/1093. Dataloading: 0.0131 s/iter. Inference: 0.1812 s/iter. Eval: 0.2017 s/iter. Total: 0.3962 s/iter. ETA=0:06:48
[01/17 20:20:22] d2.evaluation.evaluator INFO: Inference done 74/1093. Dataloading: 0.0138 s/iter. Inference: 0.1927 s/iter. Eval: 0.2055 s/iter. Total: 0.4122 s/iter. ETA=0:07:00
[01/17 20:20:27] d2.evaluation.evaluator INFO: Inference done 84/1093. Dataloading: 0.0137 s/iter. Inference: 0.2024 s/iter. Eval: 0.2090 s/iter. Total: 0.4253 s/iter. ETA=0:07:09
[01/17 20:20:32] d2.evaluation.evaluator INFO: Inference done 99/1093. Dataloading: 0.0135 s/iter. Inference: 0.1951 s/iter. Eval: 0.2026 s/iter. Total: 0.4114 s/iter. ETA=0:06:48
[01/17 20:20:38] d2.evaluation.evaluator INFO: Inference done 115/1093. Dataloading: 0.0129 s/iter. Inference: 0.1866 s/iter. Eval: 0.1992 s/iter. Total: 0.3989 s/iter. ETA=0:06:30
[01/17 20:20:43] d2.evaluation.evaluator INFO: Inference done 131/1093. Dataloading: 0.0124 s/iter. Inference: 0.1817 s/iter. Eval: 0.1946 s/iter. Total: 0.3888 s/iter. ETA=0:06:14
[01/17 20:20:48] d2.evaluation.evaluator INFO: Inference done 147/1093. Dataloading: 0.0121 s/iter. Inference: 0.1802 s/iter. Eval: 0.1895 s/iter. Total: 0.3820 s/iter. ETA=0:06:01
[01/17 20:20:53] d2.evaluation.evaluator INFO: Inference done 162/1093. Dataloading: 0.0122 s/iter. Inference: 0.1792 s/iter. Eval: 0.1887 s/iter. Total: 0.3802 s/iter. ETA=0:05:53
[01/17 20:20:59] d2.evaluation.evaluator INFO: Inference done 177/1093. Dataloading: 0.0121 s/iter. Inference: 0.1775 s/iter. Eval: 0.1870 s/iter. Total: 0.3768 s/iter. ETA=0:05:45
[01/17 20:21:04] d2.evaluation.evaluator INFO: Inference done 193/1093. Dataloading: 0.0119 s/iter. Inference: 0.1756 s/iter. Eval: 0.1842 s/iter. Total: 0.3720 s/iter. ETA=0:05:34
[01/17 20:21:09] d2.evaluation.evaluator INFO: Inference done 208/1093. Dataloading: 0.0118 s/iter. Inference: 0.1744 s/iter. Eval: 0.1828 s/iter. Total: 0.3692 s/iter. ETA=0:05:26
[01/17 20:21:14] d2.evaluation.evaluator INFO: Inference done 223/1093. Dataloading: 0.0117 s/iter. Inference: 0.1736 s/iter. Eval: 0.1815 s/iter. Total: 0.3670 s/iter. ETA=0:05:19
[01/17 20:21:19] d2.evaluation.evaluator INFO: Inference done 238/1093. Dataloading: 0.0117 s/iter. Inference: 0.1726 s/iter. Eval: 0.1810 s/iter. Total: 0.3654 s/iter. ETA=0:05:12
[01/17 20:21:24] d2.evaluation.evaluator INFO: Inference done 254/1093. Dataloading: 0.0117 s/iter. Inference: 0.1711 s/iter. Eval: 0.1810 s/iter. Total: 0.3639 s/iter. ETA=0:05:05
[01/17 20:21:30] d2.evaluation.evaluator INFO: Inference done 270/1093. Dataloading: 0.0116 s/iter. Inference: 0.1692 s/iter. Eval: 0.1804 s/iter. Total: 0.3613 s/iter. ETA=0:04:57
[01/17 20:21:35] d2.evaluation.evaluator INFO: Inference done 286/1093. Dataloading: 0.0114 s/iter. Inference: 0.1676 s/iter. Eval: 0.1800 s/iter. Total: 0.3592 s/iter. ETA=0:04:49
[01/17 20:21:40] d2.evaluation.evaluator INFO: Inference done 301/1093. Dataloading: 0.0113 s/iter. Inference: 0.1678 s/iter. Eval: 0.1794 s/iter. Total: 0.3586 s/iter. ETA=0:04:44
[01/17 20:21:45] d2.evaluation.evaluator INFO: Inference done 315/1093. Dataloading: 0.0114 s/iter. Inference: 0.1672 s/iter. Eval: 0.1802 s/iter. Total: 0.3589 s/iter. ETA=0:04:39
[01/17 20:21:50] d2.evaluation.evaluator INFO: Inference done 331/1093. Dataloading: 0.0114 s/iter. Inference: 0.1659 s/iter. Eval: 0.1798 s/iter. Total: 0.3572 s/iter. ETA=0:04:32
[01/17 20:21:55] d2.evaluation.evaluator INFO: Inference done 349/1093. Dataloading: 0.0113 s/iter. Inference: 0.1650 s/iter. Eval: 0.1771 s/iter. Total: 0.3534 s/iter. ETA=0:04:22
[01/17 20:22:00] d2.evaluation.evaluator INFO: Inference done 364/1093. Dataloading: 0.0112 s/iter. Inference: 0.1645 s/iter. Eval: 0.1770 s/iter. Total: 0.3528 s/iter. ETA=0:04:17
[01/17 20:22:05] d2.evaluation.evaluator INFO: Inference done 379/1093. Dataloading: 0.0112 s/iter. Inference: 0.1640 s/iter. Eval: 0.1769 s/iter. Total: 0.3521 s/iter. ETA=0:04:11
[01/17 20:22:11] d2.evaluation.evaluator INFO: Inference done 395/1093. Dataloading: 0.0111 s/iter. Inference: 0.1630 s/iter. Eval: 0.1774 s/iter. Total: 0.3516 s/iter. ETA=0:04:05
[01/17 20:22:16] d2.evaluation.evaluator INFO: Inference done 411/1093. Dataloading: 0.0111 s/iter. Inference: 0.1625 s/iter. Eval: 0.1770 s/iter. Total: 0.3506 s/iter. ETA=0:03:59
[01/17 20:22:21] d2.evaluation.evaluator INFO: Inference done 425/1093. Dataloading: 0.0111 s/iter. Inference: 0.1625 s/iter. Eval: 0.1775 s/iter. Total: 0.3512 s/iter. ETA=0:03:54
[01/17 20:22:26] d2.evaluation.evaluator INFO: Inference done 440/1093. Dataloading: 0.0110 s/iter. Inference: 0.1632 s/iter. Eval: 0.1765 s/iter. Total: 0.3508 s/iter. ETA=0:03:49
[01/17 20:22:32] d2.evaluation.evaluator INFO: Inference done 455/1093. Dataloading: 0.0110 s/iter. Inference: 0.1627 s/iter. Eval: 0.1770 s/iter. Total: 0.3509 s/iter. ETA=0:03:43
[01/17 20:22:37] d2.evaluation.evaluator INFO: Inference done 472/1093. Dataloading: 0.0109 s/iter. Inference: 0.1620 s/iter. Eval: 0.1762 s/iter. Total: 0.3493 s/iter. ETA=0:03:36
[01/17 20:22:42] d2.evaluation.evaluator INFO: Inference done 488/1093. Dataloading: 0.0109 s/iter. Inference: 0.1621 s/iter. Eval: 0.1752 s/iter. Total: 0.3483 s/iter. ETA=0:03:30
[01/17 20:22:47] d2.evaluation.evaluator INFO: Inference done 506/1093. Dataloading: 0.0108 s/iter. Inference: 0.1615 s/iter. Eval: 0.1738 s/iter. Total: 0.3462 s/iter. ETA=0:03:23
[01/17 20:22:52] d2.evaluation.evaluator INFO: Inference done 522/1093. Dataloading: 0.0108 s/iter. Inference: 0.1612 s/iter. Eval: 0.1734 s/iter. Total: 0.3455 s/iter. ETA=0:03:17
[01/17 20:22:58] d2.evaluation.evaluator INFO: Inference done 538/1093. Dataloading: 0.0108 s/iter. Inference: 0.1607 s/iter. Eval: 0.1734 s/iter. Total: 0.3450 s/iter. ETA=0:03:11
[01/17 20:23:03] d2.evaluation.evaluator INFO: Inference done 553/1093. Dataloading: 0.0108 s/iter. Inference: 0.1605 s/iter. Eval: 0.1738 s/iter. Total: 0.3452 s/iter. ETA=0:03:06
[01/17 20:23:08] d2.evaluation.evaluator INFO: Inference done 570/1093. Dataloading: 0.0107 s/iter. Inference: 0.1598 s/iter. Eval: 0.1732 s/iter. Total: 0.3438 s/iter. ETA=0:02:59
[01/17 20:23:13] d2.evaluation.evaluator INFO: Inference done 589/1093. Dataloading: 0.0107 s/iter. Inference: 0.1593 s/iter. Eval: 0.1715 s/iter. Total: 0.3416 s/iter. ETA=0:02:52
[01/17 20:23:19] d2.evaluation.evaluator INFO: Inference done 603/1093. Dataloading: 0.0107 s/iter. Inference: 0.1594 s/iter. Eval: 0.1724 s/iter. Total: 0.3426 s/iter. ETA=0:02:47
[01/17 20:23:24] d2.evaluation.evaluator INFO: Inference done 618/1093. Dataloading: 0.0107 s/iter. Inference: 0.1591 s/iter. Eval: 0.1728 s/iter. Total: 0.3427 s/iter. ETA=0:02:42
[01/17 20:23:29] d2.evaluation.evaluator INFO: Inference done 633/1093. Dataloading: 0.0107 s/iter. Inference: 0.1592 s/iter. Eval: 0.1728 s/iter. Total: 0.3428 s/iter. ETA=0:02:37
[01/17 20:23:34] d2.evaluation.evaluator INFO: Inference done 650/1093. Dataloading: 0.0107 s/iter. Inference: 0.1589 s/iter. Eval: 0.1721 s/iter. Total: 0.3417 s/iter. ETA=0:02:31
[01/17 20:23:40] d2.evaluation.evaluator INFO: Inference done 665/1093. Dataloading: 0.0107 s/iter. Inference: 0.1593 s/iter. Eval: 0.1722 s/iter. Total: 0.3422 s/iter. ETA=0:02:26
[01/17 20:23:45] d2.evaluation.evaluator INFO: Inference done 682/1093. Dataloading: 0.0106 s/iter. Inference: 0.1591 s/iter. Eval: 0.1715 s/iter. Total: 0.3413 s/iter. ETA=0:02:20
[01/17 20:23:50] d2.evaluation.evaluator INFO: Inference done 698/1093. Dataloading: 0.0106 s/iter. Inference: 0.1593 s/iter. Eval: 0.1710 s/iter. Total: 0.3410 s/iter. ETA=0:02:14
[01/17 20:23:56] d2.evaluation.evaluator INFO: Inference done 712/1093. Dataloading: 0.0106 s/iter. Inference: 0.1592 s/iter. Eval: 0.1720 s/iter. Total: 0.3419 s/iter. ETA=0:02:10
[01/17 20:24:01] d2.evaluation.evaluator INFO: Inference done 729/1093. Dataloading: 0.0106 s/iter. Inference: 0.1590 s/iter. Eval: 0.1715 s/iter. Total: 0.3411 s/iter. ETA=0:02:04
[01/17 20:24:06] d2.evaluation.evaluator INFO: Inference done 746/1093. Dataloading: 0.0105 s/iter. Inference: 0.1588 s/iter. Eval: 0.1709 s/iter. Total: 0.3403 s/iter. ETA=0:01:58
[01/17 20:24:11] d2.evaluation.evaluator INFO: Inference done 762/1093. Dataloading: 0.0105 s/iter. Inference: 0.1584 s/iter. Eval: 0.1707 s/iter. Total: 0.3398 s/iter. ETA=0:01:52
[01/17 20:24:16] d2.evaluation.evaluator INFO: Inference done 778/1093. Dataloading: 0.0105 s/iter. Inference: 0.1581 s/iter. Eval: 0.1708 s/iter. Total: 0.3395 s/iter. ETA=0:01:46
[01/17 20:24:21] d2.evaluation.evaluator INFO: Inference done 794/1093. Dataloading: 0.0104 s/iter. Inference: 0.1579 s/iter. Eval: 0.1705 s/iter. Total: 0.3390 s/iter. ETA=0:01:41
[01/17 20:24:26] d2.evaluation.evaluator INFO: Inference done 810/1093. Dataloading: 0.0104 s/iter. Inference: 0.1577 s/iter. Eval: 0.1703 s/iter. Total: 0.3385 s/iter. ETA=0:01:35
[01/17 20:24:31] d2.evaluation.evaluator INFO: Inference done 828/1093. Dataloading: 0.0103 s/iter. Inference: 0.1573 s/iter. Eval: 0.1695 s/iter. Total: 0.3373 s/iter. ETA=0:01:29
[01/17 20:24:36] d2.evaluation.evaluator INFO: Inference done 844/1093. Dataloading: 0.0103 s/iter. Inference: 0.1575 s/iter. Eval: 0.1691 s/iter. Total: 0.3369 s/iter. ETA=0:01:23
[01/17 20:24:42] d2.evaluation.evaluator INFO: Inference done 859/1093. Dataloading: 0.0103 s/iter. Inference: 0.1574 s/iter. Eval: 0.1693 s/iter. Total: 0.3371 s/iter. ETA=0:01:18
[01/17 20:24:47] d2.evaluation.evaluator INFO: Inference done 875/1093. Dataloading: 0.0103 s/iter. Inference: 0.1574 s/iter. Eval: 0.1691 s/iter. Total: 0.3369 s/iter. ETA=0:01:13
[01/17 20:24:52] d2.evaluation.evaluator INFO: Inference done 889/1093. Dataloading: 0.0103 s/iter. Inference: 0.1574 s/iter. Eval: 0.1695 s/iter. Total: 0.3372 s/iter. ETA=0:01:08
[01/17 20:24:57] d2.evaluation.evaluator INFO: Inference done 905/1093. Dataloading: 0.0103 s/iter. Inference: 0.1574 s/iter. Eval: 0.1691 s/iter. Total: 0.3369 s/iter. ETA=0:01:03
[01/17 20:25:02] d2.evaluation.evaluator INFO: Inference done 920/1093. Dataloading: 0.0103 s/iter. Inference: 0.1574 s/iter. Eval: 0.1691 s/iter. Total: 0.3369 s/iter. ETA=0:00:58
[01/17 20:25:07] d2.evaluation.evaluator INFO: Inference done 937/1093. Dataloading: 0.0103 s/iter. Inference: 0.1572 s/iter. Eval: 0.1689 s/iter. Total: 0.3364 s/iter. ETA=0:00:52
[01/17 20:25:12] d2.evaluation.evaluator INFO: Inference done 952/1093. Dataloading: 0.0103 s/iter. Inference: 0.1570 s/iter. Eval: 0.1692 s/iter. Total: 0.3365 s/iter. ETA=0:00:47
[01/17 20:25:18] d2.evaluation.evaluator INFO: Inference done 967/1093. Dataloading: 0.0103 s/iter. Inference: 0.1569 s/iter. Eval: 0.1694 s/iter. Total: 0.3366 s/iter. ETA=0:00:42
[01/17 20:25:23] d2.evaluation.evaluator INFO: Inference done 984/1093. Dataloading: 0.0102 s/iter. Inference: 0.1569 s/iter. Eval: 0.1690 s/iter. Total: 0.3362 s/iter. ETA=0:00:36
[01/17 20:25:28] d2.evaluation.evaluator INFO: Inference done 1000/1093. Dataloading: 0.0102 s/iter. Inference: 0.1567 s/iter. Eval: 0.1689 s/iter. Total: 0.3359 s/iter. ETA=0:00:31
[01/17 20:25:33] d2.evaluation.evaluator INFO: Inference done 1017/1093. Dataloading: 0.0102 s/iter. Inference: 0.1564 s/iter. Eval: 0.1686 s/iter. Total: 0.3352 s/iter. ETA=0:00:25
[01/17 20:25:38] d2.evaluation.evaluator INFO: Inference done 1033/1093. Dataloading: 0.0101 s/iter. Inference: 0.1564 s/iter. Eval: 0.1687 s/iter. Total: 0.3353 s/iter. ETA=0:00:20
[01/17 20:25:44] d2.evaluation.evaluator INFO: Inference done 1049/1093. Dataloading: 0.0101 s/iter. Inference: 0.1563 s/iter. Eval: 0.1686 s/iter. Total: 0.3351 s/iter. ETA=0:00:14
[01/17 20:25:49] d2.evaluation.evaluator INFO: Inference done 1066/1093. Dataloading: 0.0101 s/iter. Inference: 0.1562 s/iter. Eval: 0.1682 s/iter. Total: 0.3346 s/iter. ETA=0:00:09
[01/17 20:25:54] d2.evaluation.evaluator INFO: Inference done 1084/1093. Dataloading: 0.0101 s/iter. Inference: 0.1559 s/iter. Eval: 0.1678 s/iter. Total: 0.3339 s/iter. ETA=0:00:03
[01/17 20:25:57] d2.evaluation.evaluator INFO: Total inference time: 0:06:03.034034 (0.333671 s / iter per device, on 4 devices)
[01/17 20:25:57] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:49 (0.155615 s / iter per device, on 4 devices)
[01/17 20:26:21] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 4.329472839186732, 'mIoU': 6.668480661403575, 'fwIoU': 18.84939661541428, 'IoU-0': nan, 'IoU-1': 94.29735498944181, 'IoU-2': 41.912687236265434, 'IoU-3': 37.211556177219286, 'IoU-4': 32.105703952685424, 'IoU-5': 29.976778846422214, 'IoU-6': 25.563195093411174, 'IoU-7': 19.532582856365092, 'IoU-8': 7.1837329751106624, 'IoU-9': 3.1297906858263986, 'IoU-10': 13.455642617399524, 'IoU-11': 20.005261227462654, 'IoU-12': 16.89692916141339, 'IoU-13': 15.092930491773773, 'IoU-14': 7.962844581508645, 'IoU-15': 14.760156356129622, 'IoU-16': 9.671752368212319, 'IoU-17': 8.906539267896518, 'IoU-18': 9.04993042226903, 'IoU-19': 8.769313902247585, 'IoU-20': 14.53195368301323, 'IoU-21': 6.3695894630323675, 'IoU-22': 11.393412310551028, 'IoU-23': 11.063170443603623, 'IoU-24': 11.391296890820506, 'IoU-25': 14.181873879125487, 'IoU-26': 11.08345995705286, 'IoU-27': 11.290926114106245, 'IoU-28': 8.920413408733273, 'IoU-29': 11.915631713590738, 'IoU-30': 12.871280894767729, 'IoU-31': 14.266520754218353, 'IoU-32': 9.10314834442426, 'IoU-33': 8.904148852683553, 'IoU-34': 12.371529158183485, 'IoU-35': 11.6464338738646, 'IoU-36': 10.008742154765164, 'IoU-37': 13.201090701908056, 'IoU-38': 13.773902384609437, 'IoU-39': 10.034785747710174, 'IoU-40': 9.85633501367267, 'IoU-41': 11.383082825111135, 'IoU-42': 9.333761921191735, 'IoU-43': 12.286431709277421, 'IoU-44': 9.315568731438074, 'IoU-45': 13.070863806619471, 'IoU-46': 10.420754302153743, 'IoU-47': 7.963802509835925, 'IoU-48': 10.929542921534253, 'IoU-49': 11.968068599986085, 'IoU-50': 11.30414528871215, 'IoU-51': 10.635431033131653, 'IoU-52': 11.73040319557455, 'IoU-53': 7.904039155743643, 'IoU-54': 12.625665809886058, 'IoU-55': 7.670771834794837, 'IoU-56': 10.65884909420383, 'IoU-57': 11.247549727324982, 'IoU-58': 8.86772483813189, 'IoU-59': 9.476069847709221, 'IoU-60': 10.178157171050547, 'IoU-61': 7.940020546037152, 'IoU-62': 8.659093268102657, 'IoU-63': 3.9313231504382173, 'IoU-64': 10.55898924400352, 'IoU-65': 6.438655819033796, 'IoU-66': 8.413766528539806, 'IoU-67': 7.008110887742301, 'IoU-68': 8.546894202965818, 'IoU-69': 7.614632946861994, 'IoU-70': 6.5536972148164425, 'IoU-71': 8.491287436525889, 'IoU-72': 6.192068170968723, 'IoU-73': 6.55934974145974, 'IoU-74': 7.496955173375226, 'IoU-75': 5.8798557346703015, 'IoU-76': 9.662522357073538, 'IoU-77': 2.085931936834847, 'IoU-78': 9.032619057261645, 'IoU-79': 7.406113909064321, 'IoU-80': 6.666834144315657, 'IoU-81': 8.776610825330112, 'IoU-82': 5.404387685799645, 'IoU-83': 9.415450147829887, 'IoU-84': 5.985048398624212, 'IoU-85': 5.458149110431993, 'IoU-86': 8.629365040568459, 'IoU-87': 4.978619086136819, 'IoU-88': 8.930442334242995, 'IoU-89': 4.108180354176325, 'IoU-90': 8.134477270006679, 'IoU-91': 4.933893323689769, 'IoU-92': 6.498335231125704, 'IoU-93': 7.173735330037048, 'IoU-94': 8.049512759163836, 'IoU-95': 3.4331204089934224, 'IoU-96': 6.457593545974697, 'IoU-97': 3.911287329843701, 'IoU-98': 6.882637986273715, 'IoU-99': 8.213433793460485, 'IoU-100': 2.064491244084377, 'IoU-101': 2.6458531949312927, 'IoU-102': 6.134977304000881, 'IoU-103': 6.20319039582366, 'IoU-104': 5.599201444703264, 'IoU-105': 5.4385545006006755, 'IoU-106': 4.232784383229927, 'IoU-107': 6.063530944488964, 'IoU-108': 5.303130230765896, 'IoU-109': 4.720089772939341, 'IoU-110': 2.24951571392567, 'IoU-111': 7.075803931847509, 'IoU-112': 2.035919428657107, 'IoU-113': 4.408193164086635, 'IoU-114': 4.488655607673808, 'IoU-115': 0.5407810810150072, 'IoU-116': 4.620808491851458, 'IoU-117': 1.318759805654724, 'IoU-118': 4.246252970874467, 'IoU-119': 3.814848886169389, 'IoU-120': 3.2221952925835033, 'IoU-121': 1.3191117897180618, 'IoU-122': 3.6521083854097065, 'IoU-123': 2.076776054310727, 'IoU-124': 3.1439798800443954, 'IoU-125': 2.8319306615540385, 'IoU-126': 2.6283233386050826, 'IoU-127': 2.7635505269679355, 'IoU-128': 0.6287031474596576, 'IoU-129': 0.9160783394981911, 'IoU-130': 2.2207522491790814, 'IoU-131': 0.40475168670864825, 'IoU-132': 0.6612145062437376, 'IoU-133': 0.9225955424844134, 'IoU-134': 1.1170051759650848, 'IoU-135': 1.5233491195772846, 'IoU-136': 0.7780092346618602, 'IoU-137': 3.1016160940066135, 'IoU-138': 1.9105510123187779, 'IoU-139': 2.053720824318574, 'IoU-140': 1.0408525556572807, 'IoU-141': 0.343083569912138, 'IoU-142': 0.15462919023535274, 'IoU-143': 2.755862142790017, 'IoU-144': 0.9961823727116919, 'IoU-145': 1.2351198187131711, 'IoU-146': 1.3204266926153947, 'IoU-147': 0.35318611049148635, 'IoU-148': 2.0840830030893422, 'IoU-149': 1.303411541761613, 'IoU-150': 1.3543545680169755, 'IoU-151': 0.1728525731914148, 'IoU-152': 0.22490322253439352, 'IoU-153': 0.14612664085950552, 'IoU-154': 0.27685761023068983, 'IoU-155': 0.0017208974726162188, 'IoU-156': 0.0015420041942514082, 'IoU-157': 0.2535162262336838, 'IoU-158': 0.42860792555060884, 'IoU-159': 0.21419352034415853, 'IoU-160': 1.0455259065166744, 'IoU-161': 0.0, 'IoU-162': 0.0, 'IoU-163': 0.0018294802446624913, 'IoU-164': 0.05223476695741922, 'IoU-165': 0.007413506462369357, 'IoU-166': 0.0, 'IoU-167': 0.0, 'IoU-168': 0.15498324937006536, 'IoU-169': 1.7895309961887533, 'IoU-170': 1.501743397415019, 'IoU-171': 0.0, 'IoU-172': 0.000370225708102945, 'IoU-173': 0.030779899613026202, 'IoU-174': 0.04157906439873961, 'IoU-175': 0.0, 'IoU-176': 0.0, 'IoU-177': 0.0, 'IoU-178': 0.00901351835976979, 'IoU-179': 0.0, 'IoU-180': 0.0, 'IoU-181': 0.0, 'IoU-182': 0.0, 'IoU-183': 0.0, 'IoU-184': 0.0, 'IoU-185': 0.13866753108763968, 'IoU-186': 0.0, 'IoU-187': 0.014942531505845662, 'IoU-188': 0.0, 'IoU-189': 0.0, 'IoU-190': 0.0, 'IoU-191': 0.0, 'mACC': 11.96499582609227, 'pACC': 26.833826314665803, 'ACC-0': nan, 'ACC-1': 97.91474615539617, 'ACC-2': 62.76715612316139, 'ACC-3': 45.73773370829877, 'ACC-4': 49.914380002601455, 'ACC-5': 50.85118907649891, 'ACC-6': 38.5430733119825, 'ACC-7': 30.349570284155753, 'ACC-8': 8.530126020607018, 'ACC-9': 3.3443198869283797, 'ACC-10': 19.677300503347126, 'ACC-11': 34.98328094534781, 'ACC-12': 29.231997802258945, 'ACC-13': 26.943110707148026, 'ACC-14': 11.88087574963277, 'ACC-15': 33.50551342504048, 'ACC-16': 14.75805489069743, 'ACC-17': 16.05104584922363, 'ACC-18': 15.908998856993792, 'ACC-19': 13.95489770183567, 'ACC-20': 34.18155732576334, 'ACC-21': 11.840083188976203, 'ACC-22': 23.502154959668005, 'ACC-23': 21.929736064174516, 'ACC-24': 19.524752708639287, 'ACC-25': 29.63461019468242, 'ACC-26': 20.868760420048154, 'ACC-27': 22.51192148749262, 'ACC-28': 13.766047579185168, 'ACC-29': 20.532250650877813, 'ACC-30': 24.535238398262354, 'ACC-31': 26.583097554165473, 'ACC-32': 17.44908609219466, 'ACC-33': 12.964770128271264, 'ACC-34': 20.541920424774716, 'ACC-35': 20.20074804349506, 'ACC-36': 14.220984925693884, 'ACC-37': 22.998509258895165, 'ACC-38': 26.90445353971938, 'ACC-39': 16.909955825104966, 'ACC-40': 14.153314112643548, 'ACC-41': 20.19176745110995, 'ACC-42': 15.486836112768787, 'ACC-43': 25.165038312917336, 'ACC-44': 14.83707408208347, 'ACC-45': 25.88453593282901, 'ACC-46': 16.060518393995075, 'ACC-47': 13.525489710101468, 'ACC-48': 22.441147027538012, 'ACC-49': 25.460070792729926, 'ACC-50': 22.067518624068608, 'ACC-51': 19.07179740577127, 'ACC-52': 24.348382099935822, 'ACC-53': 13.330046529497025, 'ACC-54': 30.22428124064524, 'ACC-55': 12.442090652737535, 'ACC-56': 20.670511035384457, 'ACC-57': 22.403340006279453, 'ACC-58': 14.057002432903968, 'ACC-59': 16.971722965260987, 'ACC-60': 23.682921769574182, 'ACC-61': 13.879616249626054, 'ACC-62': 16.1990903756897, 'ACC-63': 5.389114417553164, 'ACC-64': 22.641882087756983, 'ACC-65': 9.72778399584151, 'ACC-66': 15.505598661668637, 'ACC-67': 10.96076320697084, 'ACC-68': 17.22194828829892, 'ACC-69': 12.329765823443797, 'ACC-70': 11.17220869257271, 'ACC-71': 15.560323654961362, 'ACC-72': 9.039714124586729, 'ACC-73': 11.27798754424727, 'ACC-74': 12.869793722135267, 'ACC-75': 8.946359170025522, 'ACC-76': 20.458978148819835, 'ACC-77': 2.603584581839903, 'ACC-78': 22.066994056710488, 'ACC-79': 13.71360838606423, 'ACC-80': 11.767718860659427, 'ACC-81': 16.841278336031344, 'ACC-82': 11.248565196718534, 'ACC-83': 21.234437671610902, 'ACC-84': 9.055175488569656, 'ACC-85': 8.244235797975534, 'ACC-86': 22.31479312955691, 'ACC-87': 9.312129229148988, 'ACC-88': 20.51124333653493, 'ACC-89': 5.77594547439615, 'ACC-90': 15.63295284723655, 'ACC-91': 7.392742381683435, 'ACC-92': 13.675717205465732, 'ACC-93': 12.563423415539773, 'ACC-94': 17.034884357415503, 'ACC-95': 5.031967236337194, 'ACC-96': 13.019567953938713, 'ACC-97': 6.017981351619778, 'ACC-98': 16.016268778211433, 'ACC-99': 20.55336402842626, 'ACC-100': 3.1381092440722798, 'ACC-101': 3.451405761291398, 'ACC-102': 15.282306298142275, 'ACC-103': 12.672688202512516, 'ACC-104': 10.437569169474434, 'ACC-105': 10.828667536190947, 'ACC-106': 8.75743437129363, 'ACC-107': 13.857676374728578, 'ACC-108': 9.599889675817696, 'ACC-109': 10.956181793182825, 'ACC-110': 3.313652512112452, 'ACC-111': 20.228269246017007, 'ACC-112': 2.781776116604239, 'ACC-113': 7.9049694750329875, 'ACC-114': 13.832376638895742, 'ACC-115': 0.6983973057898456, 'ACC-116': 9.921890106918664, 'ACC-117': 1.9394017810430861, 'ACC-118': 9.148015049773333, 'ACC-119': 9.369741934183331, 'ACC-120': 7.614942528735632, 'ACC-121': 2.1064421902529356, 'ACC-122': 12.080962759583723, 'ACC-123': 3.595888045252423, 'ACC-124': 5.304456180035064, 'ACC-125': 6.85877069231631, 'ACC-126': 7.020157287406989, 'ACC-127': 8.972668186788027, 'ACC-128': 1.088803267751445, 'ACC-129': 1.2759489921615705, 'ACC-130': 4.76114620926987, 'ACC-131': 0.46068585836634623, 'ACC-132': 0.762031144755758, 'ACC-133': 1.2089788864166786, 'ACC-134': 2.345471779405429, 'ACC-135': 2.3174277407217376, 'ACC-136': 1.1299443253820352, 'ACC-137': 10.99214862881492, 'ACC-138': 3.421818285656268, 'ACC-139': 3.3790447738189613, 'ACC-140': 1.235408415288055, 'ACC-141': 0.4073120936346545, 'ACC-142': 0.15964215006886098, 'ACC-143': 7.958104080233418, 'ACC-144': 1.803519855595668, 'ACC-145': 3.2620586327959034, 'ACC-146': 4.062415754723447, 'ACC-147': 0.4675097007102248, 'ACC-148': 15.265704799482155, 'ACC-149': 4.0100642044319414, 'ACC-150': 3.0356188028254896, 'ACC-151': 0.203989627862809, 'ACC-152': 0.2710061366241823, 'ACC-153': 0.15012642225031606, 'ACC-154': 0.34896268142360803, 'ACC-155': 0.0017255702393365923, 'ACC-156': 0.0015513195912272876, 'ACC-157': 0.25982227557493676, 'ACC-158': 0.4664448995202886, 'ACC-159': 0.251497786989597, 'ACC-160': 1.20614283128987, 'ACC-161': 0.0, 'ACC-162': 0.0, 'ACC-163': 0.0018343939697355567, 'ACC-164': 0.05263842160838918, 'ACC-165': 0.007464416965508041, 'ACC-166': 0.0, 'ACC-167': 0.0, 'ACC-168': 0.16210703987622194, 'ACC-169': 16.748407157133006, 'ACC-170': 4.721240257251161, 'ACC-171': 0.0, 'ACC-172': 0.0003704115272067267, 'ACC-173': 0.031232008669704575, 'ACC-174': 0.042871974962766624, 'ACC-175': 0.0, 'ACC-176': 0.0, 'ACC-177': 0.0, 'ACC-178': 0.009078424062845872, 'ACC-179': 0.0, 'ACC-180': 0.0, 'ACC-181': 0.0, 'ACC-182': 0.0, 'ACC-183': 0.0, 'ACC-184': 0.0, 'ACC-185': 0.14227050797038188, 'ACC-186': 0.0, 'ACC-187': 0.014979644595851604, 'ACC-188': 0.0, 'ACC-189': 0.0, 'ACC-190': 0.0, 'ACC-191': 0.0})])
[01/17 20:26:21] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 20:26:21] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 20:26:21] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 20:26:21] d2.evaluation.testing INFO: copypaste: 4.3295,6.6685,18.8494,11.9650,26.8338
[01/17 20:26:21] d2.utils.events INFO:  eta: 14:45:45  iter: 3999  total_loss: 50.39  loss_ce: 0.41  loss_mask: 0.5566  loss_dice: 4.016  loss_ce_0: 0.7186  loss_mask_0: 0.5336  loss_dice_0: 4.112  loss_ce_1: 0.418  loss_mask_1: 0.5506  loss_dice_1: 4.03  loss_ce_2: 0.4231  loss_mask_2: 0.5563  loss_dice_2: 4.005  loss_ce_3: 0.4127  loss_mask_3: 0.5552  loss_dice_3: 4.001  loss_ce_4: 0.4216  loss_mask_4: 0.5555  loss_dice_4: 4.012  loss_ce_5: 0.419  loss_mask_5: 0.5567  loss_dice_5: 3.999  loss_ce_6: 0.4143  loss_mask_6: 0.5567  loss_dice_6: 4.002  loss_ce_7: 0.4103  loss_mask_7: 0.5566  loss_dice_7: 4.002  loss_ce_8: 0.3962  loss_mask_8: 0.5599  loss_dice_8: 4.011  time: 1.5192  data_time: 0.0869  lr: 9.0956e-06  max_mem: 21408M
[01/17 20:26:51] d2.utils.events INFO:  eta: 14:44:31  iter: 4019  total_loss: 50.64  loss_ce: 0.4055  loss_mask: 0.5555  loss_dice: 4.044  loss_ce_0: 0.6744  loss_mask_0: 0.5333  loss_dice_0: 4.14  loss_ce_1: 0.4617  loss_mask_1: 0.5508  loss_dice_1: 4.06  loss_ce_2: 0.439  loss_mask_2: 0.5552  loss_dice_2: 4.038  loss_ce_3: 0.4242  loss_mask_3: 0.5557  loss_dice_3: 4.039  loss_ce_4: 0.4196  loss_mask_4: 0.552  loss_dice_4: 4.041  loss_ce_5: 0.4248  loss_mask_5: 0.5559  loss_dice_5: 4.044  loss_ce_6: 0.4326  loss_mask_6: 0.5533  loss_dice_6: 4.046  loss_ce_7: 0.401  loss_mask_7: 0.5525  loss_dice_7: 4.048  loss_ce_8: 0.4067  loss_mask_8: 0.5559  loss_dice_8: 4.046  time: 1.5189  data_time: 0.0667  lr: 9.091e-06  max_mem: 21408M
[01/17 20:27:20] d2.utils.events INFO:  eta: 14:44:35  iter: 4039  total_loss: 50.2  loss_ce: 0.4043  loss_mask: 0.55  loss_dice: 4.038  loss_ce_0: 0.6663  loss_mask_0: 0.5261  loss_dice_0: 4.142  loss_ce_1: 0.4006  loss_mask_1: 0.5425  loss_dice_1: 4.07  loss_ce_2: 0.3952  loss_mask_2: 0.5429  loss_dice_2: 4.048  loss_ce_3: 0.3979  loss_mask_3: 0.5446  loss_dice_3: 4.043  loss_ce_4: 0.3926  loss_mask_4: 0.5437  loss_dice_4: 4.042  loss_ce_5: 0.3829  loss_mask_5: 0.5451  loss_dice_5: 4.042  loss_ce_6: 0.3819  loss_mask_6: 0.5463  loss_dice_6: 4.035  loss_ce_7: 0.3733  loss_mask_7: 0.5515  loss_dice_7: 4.045  loss_ce_8: 0.3812  loss_mask_8: 0.55  loss_dice_8: 4.038  time: 1.5187  data_time: 0.0667  lr: 9.0865e-06  max_mem: 21408M
[01/17 20:27:49] d2.utils.events INFO:  eta: 14:44:01  iter: 4059  total_loss: 50.44  loss_ce: 0.4288  loss_mask: 0.5449  loss_dice: 4.026  loss_ce_0: 0.7068  loss_mask_0: 0.5241  loss_dice_0: 4.134  loss_ce_1: 0.4378  loss_mask_1: 0.5377  loss_dice_1: 4.048  loss_ce_2: 0.4536  loss_mask_2: 0.5409  loss_dice_2: 4.033  loss_ce_3: 0.4437  loss_mask_3: 0.5433  loss_dice_3: 4.021  loss_ce_4: 0.4353  loss_mask_4: 0.5434  loss_dice_4: 4.023  loss_ce_5: 0.443  loss_mask_5: 0.5476  loss_dice_5: 4.023  loss_ce_6: 0.4285  loss_mask_6: 0.5467  loss_dice_6: 4.018  loss_ce_7: 0.4242  loss_mask_7: 0.5484  loss_dice_7: 4.034  loss_ce_8: 0.4195  loss_mask_8: 0.5478  loss_dice_8: 4.03  time: 1.5184  data_time: 0.0657  lr: 9.0819e-06  max_mem: 21408M
[01/17 20:28:19] d2.utils.events INFO:  eta: 14:43:40  iter: 4079  total_loss: 50.55  loss_ce: 0.4152  loss_mask: 0.5451  loss_dice: 4.044  loss_ce_0: 0.678  loss_mask_0: 0.5212  loss_dice_0: 4.15  loss_ce_1: 0.4186  loss_mask_1: 0.5378  loss_dice_1: 4.07  loss_ce_2: 0.4134  loss_mask_2: 0.5418  loss_dice_2: 4.044  loss_ce_3: 0.4081  loss_mask_3: 0.542  loss_dice_3: 4.043  loss_ce_4: 0.4208  loss_mask_4: 0.5405  loss_dice_4: 4.04  loss_ce_5: 0.4171  loss_mask_5: 0.5439  loss_dice_5: 4.036  loss_ce_6: 0.4199  loss_mask_6: 0.5445  loss_dice_6: 4.035  loss_ce_7: 0.4258  loss_mask_7: 0.5464  loss_dice_7: 4.047  loss_ce_8: 0.4174  loss_mask_8: 0.5445  loss_dice_8: 4.053  time: 1.5182  data_time: 0.0755  lr: 9.0774e-06  max_mem: 21408M
[01/17 20:28:49] d2.utils.events INFO:  eta: 14:43:24  iter: 4099  total_loss: 50.76  loss_ce: 0.4508  loss_mask: 0.5327  loss_dice: 4.05  loss_ce_0: 0.7247  loss_mask_0: 0.5151  loss_dice_0: 4.143  loss_ce_1: 0.4713  loss_mask_1: 0.5286  loss_dice_1: 4.072  loss_ce_2: 0.4577  loss_mask_2: 0.5299  loss_dice_2: 4.054  loss_ce_3: 0.4369  loss_mask_3: 0.5302  loss_dice_3: 4.044  loss_ce_4: 0.4348  loss_mask_4: 0.5321  loss_dice_4: 4.04  loss_ce_5: 0.4474  loss_mask_5: 0.5331  loss_dice_5: 4.043  loss_ce_6: 0.4531  loss_mask_6: 0.5308  loss_dice_6: 4.046  loss_ce_7: 0.4571  loss_mask_7: 0.5333  loss_dice_7: 4.045  loss_ce_8: 0.4297  loss_mask_8: 0.538  loss_dice_8: 4.042  time: 1.5180  data_time: 0.0709  lr: 9.0728e-06  max_mem: 21408M
[01/17 20:29:19] d2.utils.events INFO:  eta: 14:43:12  iter: 4119  total_loss: 50.51  loss_ce: 0.4088  loss_mask: 0.5573  loss_dice: 4.032  loss_ce_0: 0.6548  loss_mask_0: 0.5376  loss_dice_0: 4.137  loss_ce_1: 0.4286  loss_mask_1: 0.5542  loss_dice_1: 4.058  loss_ce_2: 0.4227  loss_mask_2: 0.5537  loss_dice_2: 4.041  loss_ce_3: 0.415  loss_mask_3: 0.5527  loss_dice_3: 4.035  loss_ce_4: 0.4094  loss_mask_4: 0.5528  loss_dice_4: 4.031  loss_ce_5: 0.4191  loss_mask_5: 0.5551  loss_dice_5: 4.035  loss_ce_6: 0.4153  loss_mask_6: 0.5536  loss_dice_6: 4.033  loss_ce_7: 0.4063  loss_mask_7: 0.5593  loss_dice_7: 4.04  loss_ce_8: 0.405  loss_mask_8: 0.5586  loss_dice_8: 4.039  time: 1.5179  data_time: 0.0752  lr: 9.0683e-06  max_mem: 21408M
[01/17 20:29:49] d2.utils.events INFO:  eta: 14:42:50  iter: 4139  total_loss: 50.29  loss_ce: 0.4204  loss_mask: 0.5316  loss_dice: 4.032  loss_ce_0: 0.6838  loss_mask_0: 0.5156  loss_dice_0: 4.133  loss_ce_1: 0.4281  loss_mask_1: 0.5268  loss_dice_1: 4.072  loss_ce_2: 0.4068  loss_mask_2: 0.5275  loss_dice_2: 4.046  loss_ce_3: 0.4171  loss_mask_3: 0.53  loss_dice_3: 4.031  loss_ce_4: 0.4094  loss_mask_4: 0.5313  loss_dice_4: 4.035  loss_ce_5: 0.408  loss_mask_5: 0.5336  loss_dice_5: 4.028  loss_ce_6: 0.4092  loss_mask_6: 0.5364  loss_dice_6: 4.034  loss_ce_7: 0.3913  loss_mask_7: 0.5351  loss_dice_7: 4.039  loss_ce_8: 0.4033  loss_mask_8: 0.5339  loss_dice_8: 4.032  time: 1.5178  data_time: 0.0742  lr: 9.0637e-06  max_mem: 21408M
[01/17 20:30:18] d2.utils.events INFO:  eta: 14:42:21  iter: 4159  total_loss: 50.61  loss_ce: 0.4546  loss_mask: 0.5555  loss_dice: 4.008  loss_ce_0: 0.6972  loss_mask_0: 0.542  loss_dice_0: 4.109  loss_ce_1: 0.4511  loss_mask_1: 0.5527  loss_dice_1: 4.032  loss_ce_2: 0.4487  loss_mask_2: 0.556  loss_dice_2: 4.015  loss_ce_3: 0.4601  loss_mask_3: 0.5518  loss_dice_3: 4.009  loss_ce_4: 0.4456  loss_mask_4: 0.5555  loss_dice_4: 4.006  loss_ce_5: 0.4392  loss_mask_5: 0.5563  loss_dice_5: 4.007  loss_ce_6: 0.4273  loss_mask_6: 0.5558  loss_dice_6: 4.008  loss_ce_7: 0.445  loss_mask_7: 0.5575  loss_dice_7: 4.012  loss_ce_8: 0.4308  loss_mask_8: 0.5585  loss_dice_8: 4.007  time: 1.5175  data_time: 0.0756  lr: 9.0592e-06  max_mem: 21408M
[01/17 20:30:48] d2.utils.events INFO:  eta: 14:41:48  iter: 4179  total_loss: 49.87  loss_ce: 0.4128  loss_mask: 0.5242  loss_dice: 4.007  loss_ce_0: 0.7148  loss_mask_0: 0.5038  loss_dice_0: 4.12  loss_ce_1: 0.4429  loss_mask_1: 0.5146  loss_dice_1: 4.041  loss_ce_2: 0.4299  loss_mask_2: 0.5182  loss_dice_2: 4.012  loss_ce_3: 0.4264  loss_mask_3: 0.5202  loss_dice_3: 4  loss_ce_4: 0.4206  loss_mask_4: 0.5184  loss_dice_4: 4.008  loss_ce_5: 0.4291  loss_mask_5: 0.5179  loss_dice_5: 4.001  loss_ce_6: 0.414  loss_mask_6: 0.5173  loss_dice_6: 4.001  loss_ce_7: 0.4176  loss_mask_7: 0.5185  loss_dice_7: 4.005  loss_ce_8: 0.4107  loss_mask_8: 0.5228  loss_dice_8: 4.009  time: 1.5173  data_time: 0.0825  lr: 9.0546e-06  max_mem: 21408M
[01/17 20:31:17] d2.utils.events INFO:  eta: 14:41:23  iter: 4199  total_loss: 50.14  loss_ce: 0.4135  loss_mask: 0.5577  loss_dice: 3.993  loss_ce_0: 0.7065  loss_mask_0: 0.5346  loss_dice_0: 4.105  loss_ce_1: 0.4278  loss_mask_1: 0.5528  loss_dice_1: 4.022  loss_ce_2: 0.4196  loss_mask_2: 0.5539  loss_dice_2: 4.006  loss_ce_3: 0.3991  loss_mask_3: 0.5511  loss_dice_3: 3.997  loss_ce_4: 0.4185  loss_mask_4: 0.5536  loss_dice_4: 3.989  loss_ce_5: 0.4135  loss_mask_5: 0.5556  loss_dice_5: 3.99  loss_ce_6: 0.4263  loss_mask_6: 0.5587  loss_dice_6: 3.984  loss_ce_7: 0.4082  loss_mask_7: 0.5591  loss_dice_7: 3.996  loss_ce_8: 0.4135  loss_mask_8: 0.5585  loss_dice_8: 3.992  time: 1.5171  data_time: 0.0670  lr: 9.0501e-06  max_mem: 21408M
[01/17 20:31:47] d2.utils.events INFO:  eta: 14:40:54  iter: 4219  total_loss: 49.75  loss_ce: 0.3945  loss_mask: 0.5373  loss_dice: 3.965  loss_ce_0: 0.6943  loss_mask_0: 0.5195  loss_dice_0: 4.09  loss_ce_1: 0.4397  loss_mask_1: 0.5363  loss_dice_1: 4.01  loss_ce_2: 0.4158  loss_mask_2: 0.5352  loss_dice_2: 3.982  loss_ce_3: 0.413  loss_mask_3: 0.5377  loss_dice_3: 3.974  loss_ce_4: 0.4042  loss_mask_4: 0.5372  loss_dice_4: 3.96  loss_ce_5: 0.4193  loss_mask_5: 0.5368  loss_dice_5: 3.966  loss_ce_6: 0.3944  loss_mask_6: 0.5366  loss_dice_6: 3.972  loss_ce_7: 0.4053  loss_mask_7: 0.5389  loss_dice_7: 3.969  loss_ce_8: 0.388  loss_mask_8: 0.5396  loss_dice_8: 3.977  time: 1.5170  data_time: 0.0771  lr: 9.0455e-06  max_mem: 21408M
[01/17 20:32:16] d2.utils.events INFO:  eta: 14:40:18  iter: 4239  total_loss: 49.45  loss_ce: 0.4067  loss_mask: 0.5244  loss_dice: 3.969  loss_ce_0: 0.6907  loss_mask_0: 0.5079  loss_dice_0: 4.095  loss_ce_1: 0.4355  loss_mask_1: 0.5225  loss_dice_1: 4.003  loss_ce_2: 0.4165  loss_mask_2: 0.5261  loss_dice_2: 3.983  loss_ce_3: 0.4193  loss_mask_3: 0.5289  loss_dice_3: 3.964  loss_ce_4: 0.4102  loss_mask_4: 0.5263  loss_dice_4: 3.974  loss_ce_5: 0.4094  loss_mask_5: 0.5275  loss_dice_5: 3.97  loss_ce_6: 0.4018  loss_mask_6: 0.5241  loss_dice_6: 3.967  loss_ce_7: 0.3897  loss_mask_7: 0.5283  loss_dice_7: 3.973  loss_ce_8: 0.3912  loss_mask_8: 0.5234  loss_dice_8: 3.973  time: 1.5168  data_time: 0.0760  lr: 9.041e-06  max_mem: 21408M
[01/17 20:32:46] d2.utils.events INFO:  eta: 14:39:59  iter: 4259  total_loss: 49.64  loss_ce: 0.4311  loss_mask: 0.5539  loss_dice: 3.966  loss_ce_0: 0.6719  loss_mask_0: 0.5291  loss_dice_0: 4.084  loss_ce_1: 0.4184  loss_mask_1: 0.546  loss_dice_1: 4.008  loss_ce_2: 0.4164  loss_mask_2: 0.5433  loss_dice_2: 3.984  loss_ce_3: 0.4256  loss_mask_3: 0.5437  loss_dice_3: 3.981  loss_ce_4: 0.4232  loss_mask_4: 0.5491  loss_dice_4: 3.971  loss_ce_5: 0.4211  loss_mask_5: 0.5513  loss_dice_5: 3.982  loss_ce_6: 0.4293  loss_mask_6: 0.5528  loss_dice_6: 3.968  loss_ce_7: 0.4155  loss_mask_7: 0.5536  loss_dice_7: 3.977  loss_ce_8: 0.4124  loss_mask_8: 0.5526  loss_dice_8: 3.978  time: 1.5165  data_time: 0.0766  lr: 9.0364e-06  max_mem: 21408M
[01/17 20:33:16] d2.utils.events INFO:  eta: 14:39:36  iter: 4279  total_loss: 49.56  loss_ce: 0.4  loss_mask: 0.5256  loss_dice: 3.976  loss_ce_0: 0.6821  loss_mask_0: 0.5076  loss_dice_0: 4.094  loss_ce_1: 0.4324  loss_mask_1: 0.5181  loss_dice_1: 4.004  loss_ce_2: 0.4139  loss_mask_2: 0.5257  loss_dice_2: 3.979  loss_ce_3: 0.4045  loss_mask_3: 0.5261  loss_dice_3: 3.973  loss_ce_4: 0.402  loss_mask_4: 0.5268  loss_dice_4: 3.974  loss_ce_5: 0.4087  loss_mask_5: 0.5281  loss_dice_5: 3.971  loss_ce_6: 0.4059  loss_mask_6: 0.5284  loss_dice_6: 3.968  loss_ce_7: 0.3824  loss_mask_7: 0.5299  loss_dice_7: 3.979  loss_ce_8: 0.392  loss_mask_8: 0.5277  loss_dice_8: 3.981  time: 1.5164  data_time: 0.0685  lr: 9.0319e-06  max_mem: 21408M
[01/17 20:33:45] d2.utils.events INFO:  eta: 14:39:30  iter: 4299  total_loss: 49.48  loss_ce: 0.4151  loss_mask: 0.5276  loss_dice: 3.973  loss_ce_0: 0.7172  loss_mask_0: 0.5007  loss_dice_0: 4.086  loss_ce_1: 0.423  loss_mask_1: 0.5203  loss_dice_1: 4.005  loss_ce_2: 0.4075  loss_mask_2: 0.5263  loss_dice_2: 3.985  loss_ce_3: 0.4135  loss_mask_3: 0.528  loss_dice_3: 3.977  loss_ce_4: 0.4079  loss_mask_4: 0.5261  loss_dice_4: 3.975  loss_ce_5: 0.4125  loss_mask_5: 0.5271  loss_dice_5: 3.972  loss_ce_6: 0.4165  loss_mask_6: 0.5284  loss_dice_6: 3.976  loss_ce_7: 0.4016  loss_mask_7: 0.5303  loss_dice_7: 3.98  loss_ce_8: 0.3962  loss_mask_8: 0.5287  loss_dice_8: 3.975  time: 1.5162  data_time: 0.0754  lr: 9.0273e-06  max_mem: 21408M
[01/17 20:34:15] d2.utils.events INFO:  eta: 14:38:37  iter: 4319  total_loss: 49.61  loss_ce: 0.391  loss_mask: 0.5399  loss_dice: 3.976  loss_ce_0: 0.6745  loss_mask_0: 0.5052  loss_dice_0: 4.093  loss_ce_1: 0.4149  loss_mask_1: 0.5295  loss_dice_1: 3.997  loss_ce_2: 0.4016  loss_mask_2: 0.5338  loss_dice_2: 3.971  loss_ce_3: 0.4068  loss_mask_3: 0.5327  loss_dice_3: 3.972  loss_ce_4: 0.4061  loss_mask_4: 0.5353  loss_dice_4: 3.971  loss_ce_5: 0.3959  loss_mask_5: 0.5372  loss_dice_5: 3.972  loss_ce_6: 0.3917  loss_mask_6: 0.5373  loss_dice_6: 3.971  loss_ce_7: 0.3771  loss_mask_7: 0.5402  loss_dice_7: 3.976  loss_ce_8: 0.3908  loss_mask_8: 0.5417  loss_dice_8: 3.973  time: 1.5160  data_time: 0.0662  lr: 9.0228e-06  max_mem: 21408M
[01/17 20:34:44] d2.utils.events INFO:  eta: 14:38:01  iter: 4339  total_loss: 49.55  loss_ce: 0.4152  loss_mask: 0.5376  loss_dice: 3.975  loss_ce_0: 0.7075  loss_mask_0: 0.5131  loss_dice_0: 4.095  loss_ce_1: 0.4314  loss_mask_1: 0.5289  loss_dice_1: 4.008  loss_ce_2: 0.4103  loss_mask_2: 0.5291  loss_dice_2: 3.99  loss_ce_3: 0.4267  loss_mask_3: 0.5326  loss_dice_3: 3.986  loss_ce_4: 0.4111  loss_mask_4: 0.5354  loss_dice_4: 3.979  loss_ce_5: 0.4026  loss_mask_5: 0.5395  loss_dice_5: 3.984  loss_ce_6: 0.416  loss_mask_6: 0.5361  loss_dice_6: 3.978  loss_ce_7: 0.3953  loss_mask_7: 0.5377  loss_dice_7: 3.982  loss_ce_8: 0.4001  loss_mask_8: 0.5378  loss_dice_8: 3.983  time: 1.5158  data_time: 0.0780  lr: 9.0182e-06  max_mem: 21408M
[01/17 20:35:14] d2.utils.events INFO:  eta: 14:38:26  iter: 4359  total_loss: 49.65  loss_ce: 0.4162  loss_mask: 0.5385  loss_dice: 3.981  loss_ce_0: 0.7279  loss_mask_0: 0.5167  loss_dice_0: 4.088  loss_ce_1: 0.4367  loss_mask_1: 0.5312  loss_dice_1: 4.015  loss_ce_2: 0.4176  loss_mask_2: 0.5322  loss_dice_2: 3.999  loss_ce_3: 0.4181  loss_mask_3: 0.5334  loss_dice_3: 3.983  loss_ce_4: 0.4076  loss_mask_4: 0.5312  loss_dice_4: 3.988  loss_ce_5: 0.4093  loss_mask_5: 0.5352  loss_dice_5: 3.988  loss_ce_6: 0.4181  loss_mask_6: 0.5364  loss_dice_6: 3.984  loss_ce_7: 0.3951  loss_mask_7: 0.5406  loss_dice_7: 3.981  loss_ce_8: 0.4034  loss_mask_8: 0.5361  loss_dice_8: 3.977  time: 1.5157  data_time: 0.0769  lr: 9.0137e-06  max_mem: 21408M
[01/17 20:35:44] d2.utils.events INFO:  eta: 14:37:53  iter: 4379  total_loss: 49.58  loss_ce: 0.4206  loss_mask: 0.5517  loss_dice: 3.961  loss_ce_0: 0.6867  loss_mask_0: 0.519  loss_dice_0: 4.096  loss_ce_1: 0.4389  loss_mask_1: 0.5362  loss_dice_1: 4.002  loss_ce_2: 0.4293  loss_mask_2: 0.5396  loss_dice_2: 3.985  loss_ce_3: 0.4245  loss_mask_3: 0.5421  loss_dice_3: 3.967  loss_ce_4: 0.4322  loss_mask_4: 0.5441  loss_dice_4: 3.969  loss_ce_5: 0.4315  loss_mask_5: 0.5481  loss_dice_5: 3.964  loss_ce_6: 0.4169  loss_mask_6: 0.5482  loss_dice_6: 3.964  loss_ce_7: 0.4017  loss_mask_7: 0.5509  loss_dice_7: 3.967  loss_ce_8: 0.4075  loss_mask_8: 0.5517  loss_dice_8: 3.973  time: 1.5156  data_time: 0.0749  lr: 9.0091e-06  max_mem: 21408M
[01/17 20:36:13] d2.utils.events INFO:  eta: 14:37:23  iter: 4399  total_loss: 49.43  loss_ce: 0.408  loss_mask: 0.5262  loss_dice: 3.957  loss_ce_0: 0.6912  loss_mask_0: 0.5124  loss_dice_0: 4.063  loss_ce_1: 0.4324  loss_mask_1: 0.526  loss_dice_1: 3.979  loss_ce_2: 0.423  loss_mask_2: 0.527  loss_dice_2: 3.962  loss_ce_3: 0.4128  loss_mask_3: 0.5274  loss_dice_3: 3.959  loss_ce_4: 0.4276  loss_mask_4: 0.5253  loss_dice_4: 3.955  loss_ce_5: 0.4223  loss_mask_5: 0.527  loss_dice_5: 3.951  loss_ce_6: 0.41  loss_mask_6: 0.5255  loss_dice_6: 3.959  loss_ce_7: 0.3849  loss_mask_7: 0.5285  loss_dice_7: 3.955  loss_ce_8: 0.4118  loss_mask_8: 0.5284  loss_dice_8: 3.953  time: 1.5154  data_time: 0.0676  lr: 9.0045e-06  max_mem: 21408M
[01/17 20:36:43] d2.utils.events INFO:  eta: 14:37:07  iter: 4419  total_loss: 49.55  loss_ce: 0.4197  loss_mask: 0.5298  loss_dice: 3.977  loss_ce_0: 0.706  loss_mask_0: 0.517  loss_dice_0: 4.071  loss_ce_1: 0.4477  loss_mask_1: 0.5245  loss_dice_1: 3.989  loss_ce_2: 0.4356  loss_mask_2: 0.5257  loss_dice_2: 3.962  loss_ce_3: 0.4298  loss_mask_3: 0.5296  loss_dice_3: 3.959  loss_ce_4: 0.4245  loss_mask_4: 0.5292  loss_dice_4: 3.957  loss_ce_5: 0.4265  loss_mask_5: 0.5283  loss_dice_5: 3.963  loss_ce_6: 0.4367  loss_mask_6: 0.5302  loss_dice_6: 3.97  loss_ce_7: 0.4177  loss_mask_7: 0.5294  loss_dice_7: 3.966  loss_ce_8: 0.408  loss_mask_8: 0.5266  loss_dice_8: 3.972  time: 1.5152  data_time: 0.0662  lr: 9e-06  max_mem: 21408M
[01/17 20:37:13] d2.utils.events INFO:  eta: 14:37:08  iter: 4439  total_loss: 49.39  loss_ce: 0.4199  loss_mask: 0.526  loss_dice: 3.959  loss_ce_0: 0.6955  loss_mask_0: 0.4993  loss_dice_0: 4.083  loss_ce_1: 0.4375  loss_mask_1: 0.5171  loss_dice_1: 4.006  loss_ce_2: 0.4236  loss_mask_2: 0.5186  loss_dice_2: 3.98  loss_ce_3: 0.4212  loss_mask_3: 0.5183  loss_dice_3: 3.968  loss_ce_4: 0.4301  loss_mask_4: 0.5208  loss_dice_4: 3.962  loss_ce_5: 0.4187  loss_mask_5: 0.5198  loss_dice_5: 3.964  loss_ce_6: 0.4158  loss_mask_6: 0.522  loss_dice_6: 3.96  loss_ce_7: 0.4046  loss_mask_7: 0.5302  loss_dice_7: 3.96  loss_ce_8: 0.4076  loss_mask_8: 0.5283  loss_dice_8: 3.965  time: 1.5150  data_time: 0.0682  lr: 8.9954e-06  max_mem: 21408M
[01/17 20:37:42] d2.utils.events INFO:  eta: 14:36:41  iter: 4459  total_loss: 49.67  loss_ce: 0.416  loss_mask: 0.5292  loss_dice: 3.964  loss_ce_0: 0.6759  loss_mask_0: 0.509  loss_dice_0: 4.074  loss_ce_1: 0.4035  loss_mask_1: 0.5242  loss_dice_1: 3.997  loss_ce_2: 0.4232  loss_mask_2: 0.5252  loss_dice_2: 3.973  loss_ce_3: 0.4129  loss_mask_3: 0.5279  loss_dice_3: 3.965  loss_ce_4: 0.4172  loss_mask_4: 0.5264  loss_dice_4: 3.97  loss_ce_5: 0.4069  loss_mask_5: 0.5264  loss_dice_5: 3.961  loss_ce_6: 0.4115  loss_mask_6: 0.529  loss_dice_6: 3.957  loss_ce_7: 0.4194  loss_mask_7: 0.5297  loss_dice_7: 3.959  loss_ce_8: 0.4051  loss_mask_8: 0.53  loss_dice_8: 3.967  time: 1.5149  data_time: 0.0677  lr: 8.9909e-06  max_mem: 21408M
[01/17 20:38:12] d2.utils.events INFO:  eta: 14:35:54  iter: 4479  total_loss: 49.38  loss_ce: 0.4327  loss_mask: 0.5403  loss_dice: 3.957  loss_ce_0: 0.6819  loss_mask_0: 0.5131  loss_dice_0: 4.064  loss_ce_1: 0.4319  loss_mask_1: 0.5334  loss_dice_1: 3.982  loss_ce_2: 0.4374  loss_mask_2: 0.5366  loss_dice_2: 3.97  loss_ce_3: 0.4272  loss_mask_3: 0.5393  loss_dice_3: 3.95  loss_ce_4: 0.4376  loss_mask_4: 0.5384  loss_dice_4: 3.948  loss_ce_5: 0.421  loss_mask_5: 0.5391  loss_dice_5: 3.955  loss_ce_6: 0.4112  loss_mask_6: 0.5351  loss_dice_6: 3.952  loss_ce_7: 0.4027  loss_mask_7: 0.5381  loss_dice_7: 3.964  loss_ce_8: 0.4197  loss_mask_8: 0.5394  loss_dice_8: 3.955  time: 1.5147  data_time: 0.0678  lr: 8.9863e-06  max_mem: 21408M
[01/17 20:38:41] d2.utils.events INFO:  eta: 14:35:37  iter: 4499  total_loss: 49.56  loss_ce: 0.3962  loss_mask: 0.5408  loss_dice: 3.92  loss_ce_0: 0.7094  loss_mask_0: 0.5236  loss_dice_0: 4.042  loss_ce_1: 0.4007  loss_mask_1: 0.5385  loss_dice_1: 3.955  loss_ce_2: 0.3995  loss_mask_2: 0.5392  loss_dice_2: 3.931  loss_ce_3: 0.401  loss_mask_3: 0.5349  loss_dice_3: 3.928  loss_ce_4: 0.4057  loss_mask_4: 0.5366  loss_dice_4: 3.929  loss_ce_5: 0.3868  loss_mask_5: 0.5384  loss_dice_5: 3.912  loss_ce_6: 0.3833  loss_mask_6: 0.5389  loss_dice_6: 3.924  loss_ce_7: 0.3724  loss_mask_7: 0.5379  loss_dice_7: 3.931  loss_ce_8: 0.3845  loss_mask_8: 0.5378  loss_dice_8: 3.932  time: 1.5145  data_time: 0.0687  lr: 8.9818e-06  max_mem: 21408M
[01/17 20:39:11] d2.utils.events INFO:  eta: 14:35:22  iter: 4519  total_loss: 49.27  loss_ce: 0.4086  loss_mask: 0.528  loss_dice: 3.94  loss_ce_0: 0.6904  loss_mask_0: 0.5013  loss_dice_0: 4.057  loss_ce_1: 0.4264  loss_mask_1: 0.5221  loss_dice_1: 3.972  loss_ce_2: 0.4218  loss_mask_2: 0.5202  loss_dice_2: 3.954  loss_ce_3: 0.4188  loss_mask_3: 0.5219  loss_dice_3: 3.941  loss_ce_4: 0.4262  loss_mask_4: 0.5208  loss_dice_4: 3.944  loss_ce_5: 0.4172  loss_mask_5: 0.5248  loss_dice_5: 3.941  loss_ce_6: 0.4068  loss_mask_6: 0.5272  loss_dice_6: 3.939  loss_ce_7: 0.4016  loss_mask_7: 0.5296  loss_dice_7: 3.939  loss_ce_8: 0.3996  loss_mask_8: 0.5302  loss_dice_8: 3.94  time: 1.5144  data_time: 0.0748  lr: 8.9772e-06  max_mem: 21408M
[01/17 20:39:41] d2.utils.events INFO:  eta: 14:34:37  iter: 4539  total_loss: 49.48  loss_ce: 0.406  loss_mask: 0.5389  loss_dice: 3.92  loss_ce_0: 0.6721  loss_mask_0: 0.5144  loss_dice_0: 4.048  loss_ce_1: 0.4304  loss_mask_1: 0.537  loss_dice_1: 3.953  loss_ce_2: 0.4245  loss_mask_2: 0.537  loss_dice_2: 3.929  loss_ce_3: 0.4078  loss_mask_3: 0.5388  loss_dice_3: 3.919  loss_ce_4: 0.4164  loss_mask_4: 0.5372  loss_dice_4: 3.922  loss_ce_5: 0.4096  loss_mask_5: 0.5364  loss_dice_5: 3.925  loss_ce_6: 0.4018  loss_mask_6: 0.5391  loss_dice_6: 3.922  loss_ce_7: 0.3911  loss_mask_7: 0.538  loss_dice_7: 3.928  loss_ce_8: 0.3909  loss_mask_8: 0.5358  loss_dice_8: 3.922  time: 1.5142  data_time: 0.0623  lr: 8.9727e-06  max_mem: 21408M
[01/17 20:40:10] d2.utils.events INFO:  eta: 14:34:13  iter: 4559  total_loss: 49.05  loss_ce: 0.4114  loss_mask: 0.5441  loss_dice: 3.916  loss_ce_0: 0.6667  loss_mask_0: 0.5174  loss_dice_0: 4.055  loss_ce_1: 0.4314  loss_mask_1: 0.5357  loss_dice_1: 3.97  loss_ce_2: 0.4173  loss_mask_2: 0.5413  loss_dice_2: 3.935  loss_ce_3: 0.4166  loss_mask_3: 0.5461  loss_dice_3: 3.92  loss_ce_4: 0.3899  loss_mask_4: 0.544  loss_dice_4: 3.92  loss_ce_5: 0.3992  loss_mask_5: 0.5424  loss_dice_5: 3.929  loss_ce_6: 0.402  loss_mask_6: 0.5428  loss_dice_6: 3.917  loss_ce_7: 0.3903  loss_mask_7: 0.5461  loss_dice_7: 3.92  loss_ce_8: 0.399  loss_mask_8: 0.5449  loss_dice_8: 3.923  time: 1.5140  data_time: 0.0628  lr: 8.9681e-06  max_mem: 21408M
[01/17 20:40:40] d2.utils.events INFO:  eta: 14:34:05  iter: 4579  total_loss: 49.48  loss_ce: 0.4319  loss_mask: 0.5358  loss_dice: 3.926  loss_ce_0: 0.6925  loss_mask_0: 0.5076  loss_dice_0: 4.027  loss_ce_1: 0.4492  loss_mask_1: 0.5332  loss_dice_1: 3.949  loss_ce_2: 0.4424  loss_mask_2: 0.5345  loss_dice_2: 3.93  loss_ce_3: 0.4575  loss_mask_3: 0.5345  loss_dice_3: 3.92  loss_ce_4: 0.4367  loss_mask_4: 0.5377  loss_dice_4: 3.925  loss_ce_5: 0.4444  loss_mask_5: 0.5361  loss_dice_5: 3.925  loss_ce_6: 0.4321  loss_mask_6: 0.5357  loss_dice_6: 3.92  loss_ce_7: 0.4151  loss_mask_7: 0.5363  loss_dice_7: 3.926  loss_ce_8: 0.4321  loss_mask_8: 0.5335  loss_dice_8: 3.924  time: 1.5138  data_time: 0.0719  lr: 8.9636e-06  max_mem: 21408M
[01/17 20:41:09] d2.utils.events INFO:  eta: 14:33:48  iter: 4599  total_loss: 49.51  loss_ce: 0.4296  loss_mask: 0.5262  loss_dice: 3.959  loss_ce_0: 0.7086  loss_mask_0: 0.5073  loss_dice_0: 4.072  loss_ce_1: 0.4269  loss_mask_1: 0.5259  loss_dice_1: 3.998  loss_ce_2: 0.4577  loss_mask_2: 0.5237  loss_dice_2: 3.972  loss_ce_3: 0.4456  loss_mask_3: 0.5261  loss_dice_3: 3.958  loss_ce_4: 0.4342  loss_mask_4: 0.522  loss_dice_4: 3.958  loss_ce_5: 0.4252  loss_mask_5: 0.5258  loss_dice_5: 3.953  loss_ce_6: 0.4343  loss_mask_6: 0.5254  loss_dice_6: 3.95  loss_ce_7: 0.4266  loss_mask_7: 0.5283  loss_dice_7: 3.958  loss_ce_8: 0.4385  loss_mask_8: 0.527  loss_dice_8: 3.962  time: 1.5137  data_time: 0.0747  lr: 8.959e-06  max_mem: 21408M
[01/17 20:41:39] d2.utils.events INFO:  eta: 14:33:18  iter: 4619  total_loss: 49.15  loss_ce: 0.394  loss_mask: 0.523  loss_dice: 3.93  loss_ce_0: 0.6719  loss_mask_0: 0.5078  loss_dice_0: 4.044  loss_ce_1: 0.4048  loss_mask_1: 0.5151  loss_dice_1: 3.968  loss_ce_2: 0.4187  loss_mask_2: 0.5197  loss_dice_2: 3.951  loss_ce_3: 0.4105  loss_mask_3: 0.5161  loss_dice_3: 3.939  loss_ce_4: 0.4192  loss_mask_4: 0.5183  loss_dice_4: 3.934  loss_ce_5: 0.4154  loss_mask_5: 0.5163  loss_dice_5: 3.927  loss_ce_6: 0.413  loss_mask_6: 0.5179  loss_dice_6: 3.929  loss_ce_7: 0.3964  loss_mask_7: 0.5204  loss_dice_7: 3.934  loss_ce_8: 0.397  loss_mask_8: 0.5207  loss_dice_8: 3.929  time: 1.5135  data_time: 0.0730  lr: 8.9545e-06  max_mem: 21408M
[01/17 20:42:08] d2.utils.events INFO:  eta: 14:32:27  iter: 4639  total_loss: 49.18  loss_ce: 0.4358  loss_mask: 0.5392  loss_dice: 3.931  loss_ce_0: 0.6807  loss_mask_0: 0.5167  loss_dice_0: 4.036  loss_ce_1: 0.4583  loss_mask_1: 0.5343  loss_dice_1: 3.958  loss_ce_2: 0.421  loss_mask_2: 0.5343  loss_dice_2: 3.938  loss_ce_3: 0.4298  loss_mask_3: 0.5355  loss_dice_3: 3.936  loss_ce_4: 0.412  loss_mask_4: 0.5365  loss_dice_4: 3.935  loss_ce_5: 0.4101  loss_mask_5: 0.5389  loss_dice_5: 3.93  loss_ce_6: 0.4222  loss_mask_6: 0.5369  loss_dice_6: 3.928  loss_ce_7: 0.413  loss_mask_7: 0.5406  loss_dice_7: 3.936  loss_ce_8: 0.4075  loss_mask_8: 0.5416  loss_dice_8: 3.94  time: 1.5132  data_time: 0.0670  lr: 8.9499e-06  max_mem: 21408M
[01/17 20:42:38] d2.utils.events INFO:  eta: 14:32:11  iter: 4659  total_loss: 49.23  loss_ce: 0.4192  loss_mask: 0.5458  loss_dice: 3.922  loss_ce_0: 0.6716  loss_mask_0: 0.519  loss_dice_0: 4.037  loss_ce_1: 0.4408  loss_mask_1: 0.5375  loss_dice_1: 3.951  loss_ce_2: 0.4454  loss_mask_2: 0.5436  loss_dice_2: 3.928  loss_ce_3: 0.4362  loss_mask_3: 0.5476  loss_dice_3: 3.918  loss_ce_4: 0.4396  loss_mask_4: 0.5444  loss_dice_4: 3.929  loss_ce_5: 0.4365  loss_mask_5: 0.5473  loss_dice_5: 3.933  loss_ce_6: 0.4163  loss_mask_6: 0.5472  loss_dice_6: 3.924  loss_ce_7: 0.4375  loss_mask_7: 0.5468  loss_dice_7: 3.926  loss_ce_8: 0.4137  loss_mask_8: 0.547  loss_dice_8: 3.926  time: 1.5131  data_time: 0.0676  lr: 8.9453e-06  max_mem: 21408M
[01/17 20:43:07] d2.utils.events INFO:  eta: 14:31:42  iter: 4679  total_loss: 48.99  loss_ce: 0.3991  loss_mask: 0.537  loss_dice: 3.922  loss_ce_0: 0.6641  loss_mask_0: 0.5163  loss_dice_0: 4.027  loss_ce_1: 0.4335  loss_mask_1: 0.5311  loss_dice_1: 3.949  loss_ce_2: 0.4292  loss_mask_2: 0.5344  loss_dice_2: 3.923  loss_ce_3: 0.414  loss_mask_3: 0.5349  loss_dice_3: 3.908  loss_ce_4: 0.4096  loss_mask_4: 0.537  loss_dice_4: 3.907  loss_ce_5: 0.4047  loss_mask_5: 0.5379  loss_dice_5: 3.912  loss_ce_6: 0.4101  loss_mask_6: 0.5362  loss_dice_6: 3.906  loss_ce_7: 0.4068  loss_mask_7: 0.5381  loss_dice_7: 3.92  loss_ce_8: 0.3954  loss_mask_8: 0.5371  loss_dice_8: 3.92  time: 1.5129  data_time: 0.0693  lr: 8.9408e-06  max_mem: 21408M
[01/17 20:43:37] d2.utils.events INFO:  eta: 14:30:58  iter: 4699  total_loss: 48.95  loss_ce: 0.3936  loss_mask: 0.5277  loss_dice: 3.923  loss_ce_0: 0.6553  loss_mask_0: 0.5118  loss_dice_0: 4.039  loss_ce_1: 0.4053  loss_mask_1: 0.5232  loss_dice_1: 3.961  loss_ce_2: 0.4188  loss_mask_2: 0.5259  loss_dice_2: 3.943  loss_ce_3: 0.3964  loss_mask_3: 0.5277  loss_dice_3: 3.931  loss_ce_4: 0.399  loss_mask_4: 0.5259  loss_dice_4: 3.929  loss_ce_5: 0.3941  loss_mask_5: 0.5272  loss_dice_5: 3.933  loss_ce_6: 0.3719  loss_mask_6: 0.528  loss_dice_6: 3.925  loss_ce_7: 0.3768  loss_mask_7: 0.5291  loss_dice_7: 3.936  loss_ce_8: 0.3853  loss_mask_8: 0.5285  loss_dice_8: 3.936  time: 1.5127  data_time: 0.0702  lr: 8.9362e-06  max_mem: 21408M
[01/17 20:44:06] d2.utils.events INFO:  eta: 14:30:11  iter: 4719  total_loss: 49  loss_ce: 0.4318  loss_mask: 0.5272  loss_dice: 3.886  loss_ce_0: 0.6967  loss_mask_0: 0.5018  loss_dice_0: 4.012  loss_ce_1: 0.4365  loss_mask_1: 0.5227  loss_dice_1: 3.923  loss_ce_2: 0.4426  loss_mask_2: 0.5246  loss_dice_2: 3.897  loss_ce_3: 0.463  loss_mask_3: 0.5282  loss_dice_3: 3.883  loss_ce_4: 0.4486  loss_mask_4: 0.528  loss_dice_4: 3.88  loss_ce_5: 0.4441  loss_mask_5: 0.5313  loss_dice_5: 3.882  loss_ce_6: 0.433  loss_mask_6: 0.5301  loss_dice_6: 3.886  loss_ce_7: 0.4238  loss_mask_7: 0.5332  loss_dice_7: 3.887  loss_ce_8: 0.4275  loss_mask_8: 0.5332  loss_dice_8: 3.89  time: 1.5126  data_time: 0.0741  lr: 8.9317e-06  max_mem: 21408M
[01/17 20:44:35] d2.utils.events INFO:  eta: 14:28:33  iter: 4739  total_loss: 48.51  loss_ce: 0.3878  loss_mask: 0.5327  loss_dice: 3.861  loss_ce_0: 0.6652  loss_mask_0: 0.5154  loss_dice_0: 3.986  loss_ce_1: 0.3997  loss_mask_1: 0.5281  loss_dice_1: 3.891  loss_ce_2: 0.396  loss_mask_2: 0.5308  loss_dice_2: 3.87  loss_ce_3: 0.4023  loss_mask_3: 0.5303  loss_dice_3: 3.863  loss_ce_4: 0.4059  loss_mask_4: 0.5315  loss_dice_4: 3.861  loss_ce_5: 0.3838  loss_mask_5: 0.5321  loss_dice_5: 3.863  loss_ce_6: 0.3853  loss_mask_6: 0.532  loss_dice_6: 3.858  loss_ce_7: 0.3861  loss_mask_7: 0.5293  loss_dice_7: 3.866  loss_ce_8: 0.3904  loss_mask_8: 0.5318  loss_dice_8: 3.859  time: 1.5123  data_time: 0.0728  lr: 8.9271e-06  max_mem: 21408M
[01/17 20:45:05] d2.utils.events INFO:  eta: 14:27:54  iter: 4759  total_loss: 48.62  loss_ce: 0.398  loss_mask: 0.5271  loss_dice: 3.887  loss_ce_0: 0.6429  loss_mask_0: 0.4976  loss_dice_0: 4.018  loss_ce_1: 0.4144  loss_mask_1: 0.521  loss_dice_1: 3.93  loss_ce_2: 0.4099  loss_mask_2: 0.5251  loss_dice_2: 3.904  loss_ce_3: 0.4025  loss_mask_3: 0.5299  loss_dice_3: 3.89  loss_ce_4: 0.4053  loss_mask_4: 0.5259  loss_dice_4: 3.893  loss_ce_5: 0.4027  loss_mask_5: 0.5256  loss_dice_5: 3.885  loss_ce_6: 0.3872  loss_mask_6: 0.528  loss_dice_6: 3.889  loss_ce_7: 0.3928  loss_mask_7: 0.5306  loss_dice_7: 3.884  loss_ce_8: 0.3916  loss_mask_8: 0.5282  loss_dice_8: 3.9  time: 1.5122  data_time: 0.0641  lr: 8.9226e-06  max_mem: 21408M
[01/17 20:45:34] d2.utils.events INFO:  eta: 14:27:05  iter: 4779  total_loss: 48.85  loss_ce: 0.4261  loss_mask: 0.5455  loss_dice: 3.883  loss_ce_0: 0.6826  loss_mask_0: 0.5197  loss_dice_0: 4.009  loss_ce_1: 0.416  loss_mask_1: 0.5361  loss_dice_1: 3.921  loss_ce_2: 0.4223  loss_mask_2: 0.5391  loss_dice_2: 3.9  loss_ce_3: 0.4329  loss_mask_3: 0.5395  loss_dice_3: 3.886  loss_ce_4: 0.428  loss_mask_4: 0.5415  loss_dice_4: 3.88  loss_ce_5: 0.4203  loss_mask_5: 0.5451  loss_dice_5: 3.886  loss_ce_6: 0.4203  loss_mask_6: 0.5437  loss_dice_6: 3.874  loss_ce_7: 0.4085  loss_mask_7: 0.5451  loss_dice_7: 3.889  loss_ce_8: 0.423  loss_mask_8: 0.5444  loss_dice_8: 3.884  time: 1.5119  data_time: 0.0637  lr: 8.918e-06  max_mem: 21408M
[01/17 20:46:03] d2.utils.events INFO:  eta: 14:25:11  iter: 4799  total_loss: 49.13  loss_ce: 0.419  loss_mask: 0.5255  loss_dice: 3.917  loss_ce_0: 0.6798  loss_mask_0: 0.5051  loss_dice_0: 4.034  loss_ce_1: 0.4225  loss_mask_1: 0.5175  loss_dice_1: 3.954  loss_ce_2: 0.4223  loss_mask_2: 0.5214  loss_dice_2: 3.925  loss_ce_3: 0.4359  loss_mask_3: 0.5228  loss_dice_3: 3.915  loss_ce_4: 0.4112  loss_mask_4: 0.5218  loss_dice_4: 3.925  loss_ce_5: 0.4158  loss_mask_5: 0.5232  loss_dice_5: 3.916  loss_ce_6: 0.4183  loss_mask_6: 0.5231  loss_dice_6: 3.914  loss_ce_7: 0.4181  loss_mask_7: 0.5244  loss_dice_7: 3.922  loss_ce_8: 0.4081  loss_mask_8: 0.5228  loss_dice_8: 3.92  time: 1.5117  data_time: 0.0640  lr: 8.9134e-06  max_mem: 21408M
[01/17 20:46:34] d2.utils.events INFO:  eta: 14:24:38  iter: 4819  total_loss: 49.77  loss_ce: 0.4513  loss_mask: 0.5362  loss_dice: 3.913  loss_ce_0: 0.6844  loss_mask_0: 0.5217  loss_dice_0: 4.013  loss_ce_1: 0.4653  loss_mask_1: 0.5315  loss_dice_1: 3.937  loss_ce_2: 0.4814  loss_mask_2: 0.5339  loss_dice_2: 3.917  loss_ce_3: 0.4439  loss_mask_3: 0.5359  loss_dice_3: 3.92  loss_ce_4: 0.4557  loss_mask_4: 0.5347  loss_dice_4: 3.914  loss_ce_5: 0.442  loss_mask_5: 0.5371  loss_dice_5: 3.915  loss_ce_6: 0.4557  loss_mask_6: 0.5363  loss_dice_6: 3.917  loss_ce_7: 0.4269  loss_mask_7: 0.534  loss_dice_7: 3.922  loss_ce_8: 0.4393  loss_mask_8: 0.5375  loss_dice_8: 3.923  time: 1.5119  data_time: 0.0826  lr: 8.9089e-06  max_mem: 21408M
[01/17 20:47:06] d2.utils.events INFO:  eta: 14:23:57  iter: 4839  total_loss: 49.18  loss_ce: 0.4435  loss_mask: 0.5308  loss_dice: 3.893  loss_ce_0: 0.6746  loss_mask_0: 0.5087  loss_dice_0: 4.001  loss_ce_1: 0.4396  loss_mask_1: 0.5251  loss_dice_1: 3.929  loss_ce_2: 0.4388  loss_mask_2: 0.5284  loss_dice_2: 3.897  loss_ce_3: 0.43  loss_mask_3: 0.5306  loss_dice_3: 3.888  loss_ce_4: 0.4311  loss_mask_4: 0.5324  loss_dice_4: 3.881  loss_ce_5: 0.4374  loss_mask_5: 0.5302  loss_dice_5: 3.891  loss_ce_6: 0.4458  loss_mask_6: 0.5297  loss_dice_6: 3.882  loss_ce_7: 0.4369  loss_mask_7: 0.5329  loss_dice_7: 3.887  loss_ce_8: 0.4293  loss_mask_8: 0.5317  loss_dice_8: 3.888  time: 1.5122  data_time: 0.0926  lr: 8.9043e-06  max_mem: 21408M
[01/17 20:47:35] d2.utils.events INFO:  eta: 14:22:46  iter: 4859  total_loss: 48.81  loss_ce: 0.4348  loss_mask: 0.5371  loss_dice: 3.859  loss_ce_0: 0.6441  loss_mask_0: 0.5151  loss_dice_0: 3.982  loss_ce_1: 0.4367  loss_mask_1: 0.5355  loss_dice_1: 3.894  loss_ce_2: 0.4549  loss_mask_2: 0.5353  loss_dice_2: 3.874  loss_ce_3: 0.4291  loss_mask_3: 0.5336  loss_dice_3: 3.866  loss_ce_4: 0.4428  loss_mask_4: 0.5307  loss_dice_4: 3.869  loss_ce_5: 0.4319  loss_mask_5: 0.5343  loss_dice_5: 3.874  loss_ce_6: 0.4185  loss_mask_6: 0.5367  loss_dice_6: 3.857  loss_ce_7: 0.4314  loss_mask_7: 0.5364  loss_dice_7: 3.857  loss_ce_8: 0.4312  loss_mask_8: 0.5371  loss_dice_8: 3.862  time: 1.5120  data_time: 0.0709  lr: 8.8998e-06  max_mem: 21408M
[01/17 20:48:04] d2.utils.events INFO:  eta: 14:21:45  iter: 4879  total_loss: 48.87  loss_ce: 0.4038  loss_mask: 0.5357  loss_dice: 3.908  loss_ce_0: 0.6572  loss_mask_0: 0.5176  loss_dice_0: 4.007  loss_ce_1: 0.4106  loss_mask_1: 0.5346  loss_dice_1: 3.943  loss_ce_2: 0.4204  loss_mask_2: 0.5354  loss_dice_2: 3.914  loss_ce_3: 0.422  loss_mask_3: 0.5342  loss_dice_3: 3.907  loss_ce_4: 0.4104  loss_mask_4: 0.5357  loss_dice_4: 3.907  loss_ce_5: 0.4225  loss_mask_5: 0.5349  loss_dice_5: 3.906  loss_ce_6: 0.3978  loss_mask_6: 0.5355  loss_dice_6: 3.912  loss_ce_7: 0.3936  loss_mask_7: 0.5387  loss_dice_7: 3.907  loss_ce_8: 0.3898  loss_mask_8: 0.5354  loss_dice_8: 3.911  time: 1.5118  data_time: 0.0669  lr: 8.8952e-06  max_mem: 21408M
[01/17 20:48:34] d2.utils.events INFO:  eta: 14:20:56  iter: 4899  total_loss: 48.75  loss_ce: 0.4274  loss_mask: 0.5212  loss_dice: 3.88  loss_ce_0: 0.6448  loss_mask_0: 0.4992  loss_dice_0: 4  loss_ce_1: 0.3976  loss_mask_1: 0.5171  loss_dice_1: 3.925  loss_ce_2: 0.3959  loss_mask_2: 0.5177  loss_dice_2: 3.902  loss_ce_3: 0.4238  loss_mask_3: 0.5196  loss_dice_3: 3.894  loss_ce_4: 0.415  loss_mask_4: 0.5182  loss_dice_4: 3.885  loss_ce_5: 0.419  loss_mask_5: 0.5181  loss_dice_5: 3.879  loss_ce_6: 0.424  loss_mask_6: 0.5181  loss_dice_6: 3.876  loss_ce_7: 0.4164  loss_mask_7: 0.5166  loss_dice_7: 3.88  loss_ce_8: 0.4278  loss_mask_8: 0.5175  loss_dice_8: 3.885  time: 1.5116  data_time: 0.0648  lr: 8.8907e-06  max_mem: 21408M
[01/17 20:49:03] d2.utils.events INFO:  eta: 14:20:01  iter: 4919  total_loss: 48.93  loss_ce: 0.4021  loss_mask: 0.5365  loss_dice: 3.907  loss_ce_0: 0.6737  loss_mask_0: 0.5145  loss_dice_0: 4.006  loss_ce_1: 0.3981  loss_mask_1: 0.536  loss_dice_1: 3.934  loss_ce_2: 0.3989  loss_mask_2: 0.5351  loss_dice_2: 3.916  loss_ce_3: 0.4006  loss_mask_3: 0.5351  loss_dice_3: 3.904  loss_ce_4: 0.4125  loss_mask_4: 0.5352  loss_dice_4: 3.901  loss_ce_5: 0.4135  loss_mask_5: 0.5366  loss_dice_5: 3.895  loss_ce_6: 0.4025  loss_mask_6: 0.5353  loss_dice_6: 3.903  loss_ce_7: 0.4066  loss_mask_7: 0.5378  loss_dice_7: 3.913  loss_ce_8: 0.3992  loss_mask_8: 0.5352  loss_dice_8: 3.909  time: 1.5114  data_time: 0.0598  lr: 8.8861e-06  max_mem: 21408M
[01/17 20:49:33] d2.utils.events INFO:  eta: 14:19:41  iter: 4939  total_loss: 48.59  loss_ce: 0.3922  loss_mask: 0.5387  loss_dice: 3.882  loss_ce_0: 0.6639  loss_mask_0: 0.5119  loss_dice_0: 3.989  loss_ce_1: 0.4124  loss_mask_1: 0.5265  loss_dice_1: 3.918  loss_ce_2: 0.3993  loss_mask_2: 0.5325  loss_dice_2: 3.893  loss_ce_3: 0.4051  loss_mask_3: 0.5319  loss_dice_3: 3.879  loss_ce_4: 0.4019  loss_mask_4: 0.531  loss_dice_4: 3.875  loss_ce_5: 0.4049  loss_mask_5: 0.5348  loss_dice_5: 3.879  loss_ce_6: 0.4067  loss_mask_6: 0.5362  loss_dice_6: 3.88  loss_ce_7: 0.4078  loss_mask_7: 0.5361  loss_dice_7: 3.884  loss_ce_8: 0.391  loss_mask_8: 0.5386  loss_dice_8: 3.886  time: 1.5112  data_time: 0.0836  lr: 8.8815e-06  max_mem: 21408M
[01/17 20:50:02] d2.utils.events INFO:  eta: 14:18:58  iter: 4959  total_loss: 48.74  loss_ce: 0.4247  loss_mask: 0.5327  loss_dice: 3.873  loss_ce_0: 0.6477  loss_mask_0: 0.5127  loss_dice_0: 3.981  loss_ce_1: 0.4569  loss_mask_1: 0.5306  loss_dice_1: 3.902  loss_ce_2: 0.4191  loss_mask_2: 0.5317  loss_dice_2: 3.884  loss_ce_3: 0.4502  loss_mask_3: 0.5339  loss_dice_3: 3.874  loss_ce_4: 0.4293  loss_mask_4: 0.5335  loss_dice_4: 3.876  loss_ce_5: 0.4357  loss_mask_5: 0.5331  loss_dice_5: 3.876  loss_ce_6: 0.4136  loss_mask_6: 0.5338  loss_dice_6: 3.876  loss_ce_7: 0.4254  loss_mask_7: 0.5319  loss_dice_7: 3.874  loss_ce_8: 0.431  loss_mask_8: 0.5332  loss_dice_8: 3.872  time: 1.5111  data_time: 0.0796  lr: 8.877e-06  max_mem: 21408M
[01/17 20:50:31] d2.utils.events INFO:  eta: 14:17:51  iter: 4979  total_loss: 48.58  loss_ce: 0.4147  loss_mask: 0.5392  loss_dice: 3.85  loss_ce_0: 0.6818  loss_mask_0: 0.5251  loss_dice_0: 3.967  loss_ce_1: 0.4409  loss_mask_1: 0.5365  loss_dice_1: 3.879  loss_ce_2: 0.4317  loss_mask_2: 0.5414  loss_dice_2: 3.861  loss_ce_3: 0.442  loss_mask_3: 0.5405  loss_dice_3: 3.851  loss_ce_4: 0.4401  loss_mask_4: 0.539  loss_dice_4: 3.851  loss_ce_5: 0.4336  loss_mask_5: 0.5375  loss_dice_5: 3.854  loss_ce_6: 0.4232  loss_mask_6: 0.5373  loss_dice_6: 3.853  loss_ce_7: 0.4248  loss_mask_7: 0.5401  loss_dice_7: 3.853  loss_ce_8: 0.4167  loss_mask_8: 0.5389  loss_dice_8: 3.852  time: 1.5108  data_time: 0.0694  lr: 8.8724e-06  max_mem: 21408M
[01/17 20:51:01] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0004999.pth
[01/17 20:51:02] d2.utils.events INFO:  eta: 14:17:12  iter: 4999  total_loss: 48.34  loss_ce: 0.3949  loss_mask: 0.5197  loss_dice: 3.873  loss_ce_0: 0.6554  loss_mask_0: 0.4969  loss_dice_0: 3.994  loss_ce_1: 0.412  loss_mask_1: 0.5148  loss_dice_1: 3.914  loss_ce_2: 0.4172  loss_mask_2: 0.5198  loss_dice_2: 3.881  loss_ce_3: 0.4035  loss_mask_3: 0.5215  loss_dice_3: 3.88  loss_ce_4: 0.4261  loss_mask_4: 0.5209  loss_dice_4: 3.877  loss_ce_5: 0.3909  loss_mask_5: 0.5235  loss_dice_5: 3.879  loss_ce_6: 0.4111  loss_mask_6: 0.5208  loss_dice_6: 3.864  loss_ce_7: 0.3986  loss_mask_7: 0.5259  loss_dice_7: 3.875  loss_ce_8: 0.3935  loss_mask_8: 0.5253  loss_dice_8: 3.874  time: 1.5107  data_time: 0.0727  lr: 8.8679e-06  max_mem: 21408M
[01/17 20:51:32] d2.utils.events INFO:  eta: 14:17:00  iter: 5019  total_loss: 49.32  loss_ce: 0.4551  loss_mask: 0.5229  loss_dice: 3.941  loss_ce_0: 0.661  loss_mask_0: 0.5027  loss_dice_0: 4.039  loss_ce_1: 0.4334  loss_mask_1: 0.5182  loss_dice_1: 3.975  loss_ce_2: 0.4423  loss_mask_2: 0.5178  loss_dice_2: 3.942  loss_ce_3: 0.4373  loss_mask_3: 0.5189  loss_dice_3: 3.945  loss_ce_4: 0.4421  loss_mask_4: 0.5178  loss_dice_4: 3.936  loss_ce_5: 0.4423  loss_mask_5: 0.5176  loss_dice_5: 3.937  loss_ce_6: 0.4449  loss_mask_6: 0.521  loss_dice_6: 3.932  loss_ce_7: 0.4218  loss_mask_7: 0.5229  loss_dice_7: 3.94  loss_ce_8: 0.453  loss_mask_8: 0.5227  loss_dice_8: 3.941  time: 1.5106  data_time: 0.0628  lr: 8.8633e-06  max_mem: 21408M
[01/17 20:52:02] d2.utils.events INFO:  eta: 14:16:37  iter: 5039  total_loss: 48.84  loss_ce: 0.4062  loss_mask: 0.5225  loss_dice: 3.911  loss_ce_0: 0.6537  loss_mask_0: 0.5032  loss_dice_0: 4.031  loss_ce_1: 0.4014  loss_mask_1: 0.5185  loss_dice_1: 3.949  loss_ce_2: 0.409  loss_mask_2: 0.5189  loss_dice_2: 3.921  loss_ce_3: 0.4322  loss_mask_3: 0.5226  loss_dice_3: 3.908  loss_ce_4: 0.4225  loss_mask_4: 0.5203  loss_dice_4: 3.905  loss_ce_5: 0.3996  loss_mask_5: 0.5228  loss_dice_5: 3.911  loss_ce_6: 0.4024  loss_mask_6: 0.5238  loss_dice_6: 3.915  loss_ce_7: 0.4019  loss_mask_7: 0.5214  loss_dice_7: 3.914  loss_ce_8: 0.4016  loss_mask_8: 0.5231  loss_dice_8: 3.913  time: 1.5105  data_time: 0.0834  lr: 8.8587e-06  max_mem: 21408M
[01/17 20:52:31] d2.utils.events INFO:  eta: 14:16:15  iter: 5059  total_loss: 48.87  loss_ce: 0.4429  loss_mask: 0.5318  loss_dice: 3.876  loss_ce_0: 0.6703  loss_mask_0: 0.5084  loss_dice_0: 3.987  loss_ce_1: 0.4218  loss_mask_1: 0.5309  loss_dice_1: 3.915  loss_ce_2: 0.4571  loss_mask_2: 0.5312  loss_dice_2: 3.9  loss_ce_3: 0.4436  loss_mask_3: 0.5305  loss_dice_3: 3.874  loss_ce_4: 0.4428  loss_mask_4: 0.5288  loss_dice_4: 3.875  loss_ce_5: 0.4416  loss_mask_5: 0.5309  loss_dice_5: 3.881  loss_ce_6: 0.4341  loss_mask_6: 0.5304  loss_dice_6: 3.88  loss_ce_7: 0.43  loss_mask_7: 0.5333  loss_dice_7: 3.87  loss_ce_8: 0.4517  loss_mask_8: 0.534  loss_dice_8: 3.88  time: 1.5102  data_time: 0.0641  lr: 8.8542e-06  max_mem: 21408M
[01/17 20:53:00] d2.utils.events INFO:  eta: 14:15:51  iter: 5079  total_loss: 49.2  loss_ce: 0.4597  loss_mask: 0.5352  loss_dice: 3.92  loss_ce_0: 0.6725  loss_mask_0: 0.5118  loss_dice_0: 4.031  loss_ce_1: 0.436  loss_mask_1: 0.5269  loss_dice_1: 3.963  loss_ce_2: 0.4652  loss_mask_2: 0.5274  loss_dice_2: 3.933  loss_ce_3: 0.4686  loss_mask_3: 0.5259  loss_dice_3: 3.924  loss_ce_4: 0.4516  loss_mask_4: 0.5269  loss_dice_4: 3.922  loss_ce_5: 0.4552  loss_mask_5: 0.5294  loss_dice_5: 3.922  loss_ce_6: 0.4573  loss_mask_6: 0.5316  loss_dice_6: 3.927  loss_ce_7: 0.4511  loss_mask_7: 0.5328  loss_dice_7: 3.926  loss_ce_8: 0.438  loss_mask_8: 0.5327  loss_dice_8: 3.93  time: 1.5101  data_time: 0.0731  lr: 8.8496e-06  max_mem: 21408M
[01/17 20:53:30] d2.utils.events INFO:  eta: 14:14:45  iter: 5099  total_loss: 48.31  loss_ce: 0.3988  loss_mask: 0.5384  loss_dice: 3.852  loss_ce_0: 0.6694  loss_mask_0: 0.5178  loss_dice_0: 3.96  loss_ce_1: 0.4021  loss_mask_1: 0.5425  loss_dice_1: 3.885  loss_ce_2: 0.4143  loss_mask_2: 0.5452  loss_dice_2: 3.862  loss_ce_3: 0.4003  loss_mask_3: 0.5453  loss_dice_3: 3.857  loss_ce_4: 0.4072  loss_mask_4: 0.5434  loss_dice_4: 3.861  loss_ce_5: 0.4001  loss_mask_5: 0.5428  loss_dice_5: 3.85  loss_ce_6: 0.417  loss_mask_6: 0.541  loss_dice_6: 3.85  loss_ce_7: 0.3918  loss_mask_7: 0.5442  loss_dice_7: 3.857  loss_ce_8: 0.3928  loss_mask_8: 0.5425  loss_dice_8: 3.852  time: 1.5100  data_time: 0.0779  lr: 8.845e-06  max_mem: 21408M
[01/17 20:53:59] d2.utils.events INFO:  eta: 14:14:01  iter: 5119  total_loss: 48.66  loss_ce: 0.4086  loss_mask: 0.5341  loss_dice: 3.85  loss_ce_0: 0.6455  loss_mask_0: 0.5134  loss_dice_0: 3.963  loss_ce_1: 0.4057  loss_mask_1: 0.5259  loss_dice_1: 3.879  loss_ce_2: 0.4236  loss_mask_2: 0.5249  loss_dice_2: 3.873  loss_ce_3: 0.4276  loss_mask_3: 0.5235  loss_dice_3: 3.86  loss_ce_4: 0.4261  loss_mask_4: 0.5251  loss_dice_4: 3.849  loss_ce_5: 0.3949  loss_mask_5: 0.53  loss_dice_5: 3.854  loss_ce_6: 0.4056  loss_mask_6: 0.529  loss_dice_6: 3.85  loss_ce_7: 0.402  loss_mask_7: 0.5277  loss_dice_7: 3.86  loss_ce_8: 0.385  loss_mask_8: 0.5301  loss_dice_8: 3.855  time: 1.5098  data_time: 0.0691  lr: 8.8405e-06  max_mem: 21408M
[01/17 20:54:29] d2.utils.events INFO:  eta: 14:13:02  iter: 5139  total_loss: 48.41  loss_ce: 0.4087  loss_mask: 0.5352  loss_dice: 3.858  loss_ce_0: 0.6558  loss_mask_0: 0.513  loss_dice_0: 3.977  loss_ce_1: 0.4214  loss_mask_1: 0.5296  loss_dice_1: 3.897  loss_ce_2: 0.4278  loss_mask_2: 0.5362  loss_dice_2: 3.868  loss_ce_3: 0.4203  loss_mask_3: 0.5381  loss_dice_3: 3.855  loss_ce_4: 0.4012  loss_mask_4: 0.5372  loss_dice_4: 3.858  loss_ce_5: 0.396  loss_mask_5: 0.54  loss_dice_5: 3.863  loss_ce_6: 0.4132  loss_mask_6: 0.5366  loss_dice_6: 3.862  loss_ce_7: 0.4007  loss_mask_7: 0.5387  loss_dice_7: 3.868  loss_ce_8: 0.3964  loss_mask_8: 0.5351  loss_dice_8: 3.872  time: 1.5096  data_time: 0.0688  lr: 8.8359e-06  max_mem: 21408M
[01/17 20:54:59] d2.utils.events INFO:  eta: 14:13:13  iter: 5159  total_loss: 48.42  loss_ce: 0.4037  loss_mask: 0.5215  loss_dice: 3.878  loss_ce_0: 0.6673  loss_mask_0: 0.4976  loss_dice_0: 3.991  loss_ce_1: 0.4187  loss_mask_1: 0.5166  loss_dice_1: 3.918  loss_ce_2: 0.4215  loss_mask_2: 0.5147  loss_dice_2: 3.893  loss_ce_3: 0.4104  loss_mask_3: 0.5163  loss_dice_3: 3.878  loss_ce_4: 0.4248  loss_mask_4: 0.5157  loss_dice_4: 3.878  loss_ce_5: 0.4176  loss_mask_5: 0.5225  loss_dice_5: 3.88  loss_ce_6: 0.4  loss_mask_6: 0.5206  loss_dice_6: 3.883  loss_ce_7: 0.4085  loss_mask_7: 0.5252  loss_dice_7: 3.881  loss_ce_8: 0.4152  loss_mask_8: 0.5237  loss_dice_8: 3.878  time: 1.5097  data_time: 0.0861  lr: 8.8314e-06  max_mem: 21408M
[01/17 20:55:29] d2.utils.events INFO:  eta: 14:13:02  iter: 5179  total_loss: 48.78  loss_ce: 0.4565  loss_mask: 0.5071  loss_dice: 3.851  loss_ce_0: 0.6892  loss_mask_0: 0.4868  loss_dice_0: 3.984  loss_ce_1: 0.466  loss_mask_1: 0.5013  loss_dice_1: 3.903  loss_ce_2: 0.4616  loss_mask_2: 0.5075  loss_dice_2: 3.881  loss_ce_3: 0.4669  loss_mask_3: 0.5078  loss_dice_3: 3.863  loss_ce_4: 0.4486  loss_mask_4: 0.5104  loss_dice_4: 3.863  loss_ce_5: 0.4495  loss_mask_5: 0.509  loss_dice_5: 3.867  loss_ce_6: 0.4485  loss_mask_6: 0.5115  loss_dice_6: 3.864  loss_ce_7: 0.4535  loss_mask_7: 0.5099  loss_dice_7: 3.869  loss_ce_8: 0.456  loss_mask_8: 0.5101  loss_dice_8: 3.864  time: 1.5096  data_time: 0.0633  lr: 8.8268e-06  max_mem: 21408M
[01/17 20:55:59] d2.utils.events INFO:  eta: 14:12:43  iter: 5199  total_loss: 48.68  loss_ce: 0.4342  loss_mask: 0.494  loss_dice: 3.87  loss_ce_0: 0.6638  loss_mask_0: 0.4854  loss_dice_0: 3.965  loss_ce_1: 0.4456  loss_mask_1: 0.489  loss_dice_1: 3.9  loss_ce_2: 0.4583  loss_mask_2: 0.4901  loss_dice_2: 3.869  loss_ce_3: 0.4506  loss_mask_3: 0.4962  loss_dice_3: 3.856  loss_ce_4: 0.4531  loss_mask_4: 0.4958  loss_dice_4: 3.862  loss_ce_5: 0.4496  loss_mask_5: 0.497  loss_dice_5: 3.862  loss_ce_6: 0.4507  loss_mask_6: 0.4981  loss_dice_6: 3.86  loss_ce_7: 0.4436  loss_mask_7: 0.4971  loss_dice_7: 3.865  loss_ce_8: 0.4391  loss_mask_8: 0.4958  loss_dice_8: 3.863  time: 1.5096  data_time: 0.0752  lr: 8.8222e-06  max_mem: 21408M
[01/17 20:56:29] d2.utils.events INFO:  eta: 14:12:14  iter: 5219  total_loss: 48.76  loss_ce: 0.4371  loss_mask: 0.5145  loss_dice: 3.861  loss_ce_0: 0.7029  loss_mask_0: 0.4894  loss_dice_0: 3.969  loss_ce_1: 0.4513  loss_mask_1: 0.508  loss_dice_1: 3.887  loss_ce_2: 0.4694  loss_mask_2: 0.5127  loss_dice_2: 3.865  loss_ce_3: 0.4447  loss_mask_3: 0.5126  loss_dice_3: 3.857  loss_ce_4: 0.4387  loss_mask_4: 0.5085  loss_dice_4: 3.857  loss_ce_5: 0.4416  loss_mask_5: 0.5135  loss_dice_5: 3.853  loss_ce_6: 0.4533  loss_mask_6: 0.5141  loss_dice_6: 3.852  loss_ce_7: 0.4381  loss_mask_7: 0.5116  loss_dice_7: 3.851  loss_ce_8: 0.4319  loss_mask_8: 0.5134  loss_dice_8: 3.848  time: 1.5095  data_time: 0.0672  lr: 8.8177e-06  max_mem: 21408M
[01/17 20:56:59] d2.utils.events INFO:  eta: 14:11:47  iter: 5239  total_loss: 48.96  loss_ce: 0.4301  loss_mask: 0.5268  loss_dice: 3.883  loss_ce_0: 0.6645  loss_mask_0: 0.5095  loss_dice_0: 3.987  loss_ce_1: 0.4153  loss_mask_1: 0.5236  loss_dice_1: 3.918  loss_ce_2: 0.4397  loss_mask_2: 0.5255  loss_dice_2: 3.895  loss_ce_3: 0.4468  loss_mask_3: 0.5279  loss_dice_3: 3.883  loss_ce_4: 0.4437  loss_mask_4: 0.5268  loss_dice_4: 3.883  loss_ce_5: 0.4319  loss_mask_5: 0.5248  loss_dice_5: 3.881  loss_ce_6: 0.4326  loss_mask_6: 0.5265  loss_dice_6: 3.884  loss_ce_7: 0.4247  loss_mask_7: 0.5292  loss_dice_7: 3.881  loss_ce_8: 0.411  loss_mask_8: 0.5251  loss_dice_8: 3.893  time: 1.5094  data_time: 0.0817  lr: 8.8131e-06  max_mem: 21408M
[01/17 20:57:29] d2.utils.events INFO:  eta: 14:11:42  iter: 5259  total_loss: 48.3  loss_ce: 0.4036  loss_mask: 0.5309  loss_dice: 3.857  loss_ce_0: 0.6606  loss_mask_0: 0.5117  loss_dice_0: 3.976  loss_ce_1: 0.4342  loss_mask_1: 0.5301  loss_dice_1: 3.9  loss_ce_2: 0.4241  loss_mask_2: 0.5312  loss_dice_2: 3.882  loss_ce_3: 0.4055  loss_mask_3: 0.5294  loss_dice_3: 3.861  loss_ce_4: 0.4141  loss_mask_4: 0.5305  loss_dice_4: 3.866  loss_ce_5: 0.4044  loss_mask_5: 0.53  loss_dice_5: 3.865  loss_ce_6: 0.4279  loss_mask_6: 0.5335  loss_dice_6: 3.856  loss_ce_7: 0.4072  loss_mask_7: 0.5346  loss_dice_7: 3.867  loss_ce_8: 0.4377  loss_mask_8: 0.5319  loss_dice_8: 3.864  time: 1.5094  data_time: 0.0818  lr: 8.8085e-06  max_mem: 21408M
[01/17 20:57:58] d2.utils.events INFO:  eta: 14:10:48  iter: 5279  total_loss: 48.27  loss_ce: 0.4145  loss_mask: 0.5351  loss_dice: 3.844  loss_ce_0: 0.6859  loss_mask_0: 0.5066  loss_dice_0: 3.978  loss_ce_1: 0.4242  loss_mask_1: 0.5256  loss_dice_1: 3.886  loss_ce_2: 0.4177  loss_mask_2: 0.5277  loss_dice_2: 3.857  loss_ce_3: 0.4221  loss_mask_3: 0.5328  loss_dice_3: 3.853  loss_ce_4: 0.4202  loss_mask_4: 0.5311  loss_dice_4: 3.85  loss_ce_5: 0.4113  loss_mask_5: 0.5292  loss_dice_5: 3.846  loss_ce_6: 0.4111  loss_mask_6: 0.5291  loss_dice_6: 3.846  loss_ce_7: 0.4022  loss_mask_7: 0.5297  loss_dice_7: 3.85  loss_ce_8: 0.4133  loss_mask_8: 0.5329  loss_dice_8: 3.841  time: 1.5092  data_time: 0.0664  lr: 8.804e-06  max_mem: 21408M
[01/17 20:58:28] d2.utils.events INFO:  eta: 14:10:01  iter: 5299  total_loss: 48.79  loss_ce: 0.445  loss_mask: 0.5245  loss_dice: 3.861  loss_ce_0: 0.6974  loss_mask_0: 0.5071  loss_dice_0: 3.982  loss_ce_1: 0.4437  loss_mask_1: 0.5261  loss_dice_1: 3.896  loss_ce_2: 0.4372  loss_mask_2: 0.524  loss_dice_2: 3.871  loss_ce_3: 0.4429  loss_mask_3: 0.5205  loss_dice_3: 3.876  loss_ce_4: 0.4246  loss_mask_4: 0.5213  loss_dice_4: 3.863  loss_ce_5: 0.4371  loss_mask_5: 0.5265  loss_dice_5: 3.869  loss_ce_6: 0.4233  loss_mask_6: 0.5262  loss_dice_6: 3.859  loss_ce_7: 0.4224  loss_mask_7: 0.5237  loss_dice_7: 3.872  loss_ce_8: 0.4208  loss_mask_8: 0.5241  loss_dice_8: 3.869  time: 1.5091  data_time: 0.0761  lr: 8.7994e-06  max_mem: 21408M
[01/17 20:58:57] d2.utils.events INFO:  eta: 14:09:37  iter: 5319  total_loss: 48.01  loss_ce: 0.3955  loss_mask: 0.5217  loss_dice: 3.821  loss_ce_0: 0.653  loss_mask_0: 0.5049  loss_dice_0: 3.943  loss_ce_1: 0.4235  loss_mask_1: 0.5201  loss_dice_1: 3.856  loss_ce_2: 0.4118  loss_mask_2: 0.5223  loss_dice_2: 3.823  loss_ce_3: 0.3928  loss_mask_3: 0.5253  loss_dice_3: 3.82  loss_ce_4: 0.4033  loss_mask_4: 0.5254  loss_dice_4: 3.816  loss_ce_5: 0.4007  loss_mask_5: 0.524  loss_dice_5: 3.821  loss_ce_6: 0.4041  loss_mask_6: 0.5259  loss_dice_6: 3.814  loss_ce_7: 0.3956  loss_mask_7: 0.5264  loss_dice_7: 3.822  loss_ce_8: 0.4003  loss_mask_8: 0.5244  loss_dice_8: 3.828  time: 1.5089  data_time: 0.0804  lr: 8.7949e-06  max_mem: 21460M
[01/17 20:59:27] d2.utils.events INFO:  eta: 14:09:20  iter: 5339  total_loss: 48.27  loss_ce: 0.4363  loss_mask: 0.5195  loss_dice: 3.823  loss_ce_0: 0.635  loss_mask_0: 0.5026  loss_dice_0: 3.955  loss_ce_1: 0.4184  loss_mask_1: 0.5136  loss_dice_1: 3.853  loss_ce_2: 0.4304  loss_mask_2: 0.516  loss_dice_2: 3.836  loss_ce_3: 0.4247  loss_mask_3: 0.5198  loss_dice_3: 3.831  loss_ce_4: 0.4289  loss_mask_4: 0.5185  loss_dice_4: 3.824  loss_ce_5: 0.4274  loss_mask_5: 0.5187  loss_dice_5: 3.826  loss_ce_6: 0.4135  loss_mask_6: 0.5199  loss_dice_6: 3.821  loss_ce_7: 0.4157  loss_mask_7: 0.5197  loss_dice_7: 3.831  loss_ce_8: 0.4142  loss_mask_8: 0.5207  loss_dice_8: 3.828  time: 1.5088  data_time: 0.0773  lr: 8.7903e-06  max_mem: 21460M
[01/17 20:59:56] d2.utils.events INFO:  eta: 14:08:27  iter: 5359  total_loss: 48.51  loss_ce: 0.4401  loss_mask: 0.5208  loss_dice: 3.84  loss_ce_0: 0.6544  loss_mask_0: 0.496  loss_dice_0: 3.968  loss_ce_1: 0.4235  loss_mask_1: 0.5182  loss_dice_1: 3.875  loss_ce_2: 0.4567  loss_mask_2: 0.5196  loss_dice_2: 3.848  loss_ce_3: 0.4288  loss_mask_3: 0.5198  loss_dice_3: 3.836  loss_ce_4: 0.4423  loss_mask_4: 0.5165  loss_dice_4: 3.842  loss_ce_5: 0.4156  loss_mask_5: 0.5151  loss_dice_5: 3.846  loss_ce_6: 0.4318  loss_mask_6: 0.5223  loss_dice_6: 3.841  loss_ce_7: 0.4263  loss_mask_7: 0.521  loss_dice_7: 3.835  loss_ce_8: 0.4233  loss_mask_8: 0.5214  loss_dice_8: 3.831  time: 1.5087  data_time: 0.0675  lr: 8.7857e-06  max_mem: 21460M
[01/17 21:00:26] d2.utils.events INFO:  eta: 14:07:58  iter: 5379  total_loss: 48.27  loss_ce: 0.4479  loss_mask: 0.5091  loss_dice: 3.83  loss_ce_0: 0.6642  loss_mask_0: 0.4983  loss_dice_0: 3.949  loss_ce_1: 0.4494  loss_mask_1: 0.5046  loss_dice_1: 3.876  loss_ce_2: 0.4838  loss_mask_2: 0.5103  loss_dice_2: 3.84  loss_ce_3: 0.4469  loss_mask_3: 0.509  loss_dice_3: 3.842  loss_ce_4: 0.457  loss_mask_4: 0.5092  loss_dice_4: 3.83  loss_ce_5: 0.4341  loss_mask_5: 0.5114  loss_dice_5: 3.837  loss_ce_6: 0.4311  loss_mask_6: 0.5098  loss_dice_6: 3.845  loss_ce_7: 0.4607  loss_mask_7: 0.5119  loss_dice_7: 3.832  loss_ce_8: 0.4587  loss_mask_8: 0.5102  loss_dice_8: 3.825  time: 1.5086  data_time: 0.0711  lr: 8.7812e-06  max_mem: 21460M
[01/17 21:00:55] d2.utils.events INFO:  eta: 14:07:33  iter: 5399  total_loss: 48.25  loss_ce: 0.4117  loss_mask: 0.524  loss_dice: 3.841  loss_ce_0: 0.6614  loss_mask_0: 0.5055  loss_dice_0: 3.96  loss_ce_1: 0.4132  loss_mask_1: 0.5231  loss_dice_1: 3.885  loss_ce_2: 0.433  loss_mask_2: 0.5237  loss_dice_2: 3.858  loss_ce_3: 0.4435  loss_mask_3: 0.5238  loss_dice_3: 3.849  loss_ce_4: 0.4382  loss_mask_4: 0.5217  loss_dice_4: 3.836  loss_ce_5: 0.4322  loss_mask_5: 0.5217  loss_dice_5: 3.839  loss_ce_6: 0.4232  loss_mask_6: 0.521  loss_dice_6: 3.838  loss_ce_7: 0.4203  loss_mask_7: 0.5246  loss_dice_7: 3.836  loss_ce_8: 0.4159  loss_mask_8: 0.5219  loss_dice_8: 3.845  time: 1.5084  data_time: 0.0782  lr: 8.7766e-06  max_mem: 21460M
[01/17 21:01:25] d2.utils.events INFO:  eta: 14:07:42  iter: 5419  total_loss: 48.56  loss_ce: 0.4428  loss_mask: 0.5255  loss_dice: 3.855  loss_ce_0: 0.7021  loss_mask_0: 0.5109  loss_dice_0: 3.968  loss_ce_1: 0.4362  loss_mask_1: 0.526  loss_dice_1: 3.889  loss_ce_2: 0.4446  loss_mask_2: 0.528  loss_dice_2: 3.864  loss_ce_3: 0.4434  loss_mask_3: 0.5258  loss_dice_3: 3.855  loss_ce_4: 0.4464  loss_mask_4: 0.5262  loss_dice_4: 3.855  loss_ce_5: 0.4507  loss_mask_5: 0.5271  loss_dice_5: 3.855  loss_ce_6: 0.4405  loss_mask_6: 0.527  loss_dice_6: 3.846  loss_ce_7: 0.4389  loss_mask_7: 0.5266  loss_dice_7: 3.859  loss_ce_8: 0.4343  loss_mask_8: 0.5256  loss_dice_8: 3.857  time: 1.5083  data_time: 0.0618  lr: 8.772e-06  max_mem: 21460M
[01/17 21:01:55] d2.utils.events INFO:  eta: 14:07:24  iter: 5439  total_loss: 48.04  loss_ce: 0.4056  loss_mask: 0.5123  loss_dice: 3.816  loss_ce_0: 0.6343  loss_mask_0: 0.4933  loss_dice_0: 3.945  loss_ce_1: 0.3986  loss_mask_1: 0.51  loss_dice_1: 3.848  loss_ce_2: 0.4347  loss_mask_2: 0.5097  loss_dice_2: 3.828  loss_ce_3: 0.403  loss_mask_3: 0.5104  loss_dice_3: 3.824  loss_ce_4: 0.4234  loss_mask_4: 0.5098  loss_dice_4: 3.819  loss_ce_5: 0.41  loss_mask_5: 0.5115  loss_dice_5: 3.817  loss_ce_6: 0.4012  loss_mask_6: 0.5135  loss_dice_6: 3.813  loss_ce_7: 0.3999  loss_mask_7: 0.5113  loss_dice_7: 3.817  loss_ce_8: 0.3939  loss_mask_8: 0.5123  loss_dice_8: 3.816  time: 1.5082  data_time: 0.0860  lr: 8.7675e-06  max_mem: 21460M
[01/17 21:02:25] d2.utils.events INFO:  eta: 14:07:25  iter: 5459  total_loss: 47.91  loss_ce: 0.4066  loss_mask: 0.5176  loss_dice: 3.849  loss_ce_0: 0.6396  loss_mask_0: 0.4929  loss_dice_0: 3.96  loss_ce_1: 0.4265  loss_mask_1: 0.5134  loss_dice_1: 3.882  loss_ce_2: 0.4356  loss_mask_2: 0.5175  loss_dice_2: 3.852  loss_ce_3: 0.4183  loss_mask_3: 0.5183  loss_dice_3: 3.846  loss_ce_4: 0.4164  loss_mask_4: 0.5186  loss_dice_4: 3.842  loss_ce_5: 0.4108  loss_mask_5: 0.5176  loss_dice_5: 3.85  loss_ce_6: 0.418  loss_mask_6: 0.5164  loss_dice_6: 3.843  loss_ce_7: 0.414  loss_mask_7: 0.5156  loss_dice_7: 3.85  loss_ce_8: 0.4156  loss_mask_8: 0.5156  loss_dice_8: 3.851  time: 1.5082  data_time: 0.0726  lr: 8.7629e-06  max_mem: 21460M
[01/17 21:02:54] d2.utils.events INFO:  eta: 14:06:51  iter: 5479  total_loss: 48.32  loss_ce: 0.45  loss_mask: 0.532  loss_dice: 3.829  loss_ce_0: 0.6596  loss_mask_0: 0.5161  loss_dice_0: 3.946  loss_ce_1: 0.4161  loss_mask_1: 0.5389  loss_dice_1: 3.866  loss_ce_2: 0.4696  loss_mask_2: 0.5373  loss_dice_2: 3.846  loss_ce_3: 0.4563  loss_mask_3: 0.5351  loss_dice_3: 3.83  loss_ce_4: 0.4452  loss_mask_4: 0.5346  loss_dice_4: 3.832  loss_ce_5: 0.4476  loss_mask_5: 0.5346  loss_dice_5: 3.839  loss_ce_6: 0.4643  loss_mask_6: 0.5343  loss_dice_6: 3.828  loss_ce_7: 0.4342  loss_mask_7: 0.5326  loss_dice_7: 3.838  loss_ce_8: 0.4496  loss_mask_8: 0.5334  loss_dice_8: 3.836  time: 1.5081  data_time: 0.0710  lr: 8.7583e-06  max_mem: 21460M
[01/17 21:03:25] d2.utils.events INFO:  eta: 14:06:35  iter: 5499  total_loss: 48.01  loss_ce: 0.4036  loss_mask: 0.5229  loss_dice: 3.836  loss_ce_0: 0.6404  loss_mask_0: 0.5012  loss_dice_0: 3.949  loss_ce_1: 0.4253  loss_mask_1: 0.5184  loss_dice_1: 3.87  loss_ce_2: 0.427  loss_mask_2: 0.5217  loss_dice_2: 3.844  loss_ce_3: 0.4266  loss_mask_3: 0.5233  loss_dice_3: 3.833  loss_ce_4: 0.3928  loss_mask_4: 0.5257  loss_dice_4: 3.822  loss_ce_5: 0.3973  loss_mask_5: 0.5219  loss_dice_5: 3.833  loss_ce_6: 0.3993  loss_mask_6: 0.5252  loss_dice_6: 3.833  loss_ce_7: 0.4016  loss_mask_7: 0.5221  loss_dice_7: 3.834  loss_ce_8: 0.396  loss_mask_8: 0.5221  loss_dice_8: 3.832  time: 1.5081  data_time: 0.0777  lr: 8.7538e-06  max_mem: 21460M
[01/17 21:03:55] d2.utils.events INFO:  eta: 14:06:38  iter: 5519  total_loss: 47.97  loss_ce: 0.4286  loss_mask: 0.5161  loss_dice: 3.841  loss_ce_0: 0.6614  loss_mask_0: 0.5014  loss_dice_0: 3.953  loss_ce_1: 0.4082  loss_mask_1: 0.511  loss_dice_1: 3.873  loss_ce_2: 0.4318  loss_mask_2: 0.5126  loss_dice_2: 3.851  loss_ce_3: 0.4256  loss_mask_3: 0.5126  loss_dice_3: 3.844  loss_ce_4: 0.4558  loss_mask_4: 0.5102  loss_dice_4: 3.849  loss_ce_5: 0.434  loss_mask_5: 0.5119  loss_dice_5: 3.853  loss_ce_6: 0.4225  loss_mask_6: 0.5136  loss_dice_6: 3.833  loss_ce_7: 0.428  loss_mask_7: 0.5149  loss_dice_7: 3.836  loss_ce_8: 0.4034  loss_mask_8: 0.5167  loss_dice_8: 3.839  time: 1.5081  data_time: 0.0707  lr: 8.7492e-06  max_mem: 21460M
[01/17 21:04:24] d2.utils.events INFO:  eta: 14:06:15  iter: 5539  total_loss: 48.3  loss_ce: 0.4236  loss_mask: 0.5258  loss_dice: 3.833  loss_ce_0: 0.6763  loss_mask_0: 0.5058  loss_dice_0: 3.924  loss_ce_1: 0.4475  loss_mask_1: 0.5254  loss_dice_1: 3.839  loss_ce_2: 0.4577  loss_mask_2: 0.5271  loss_dice_2: 3.817  loss_ce_3: 0.4426  loss_mask_3: 0.5256  loss_dice_3: 3.812  loss_ce_4: 0.4464  loss_mask_4: 0.5251  loss_dice_4: 3.816  loss_ce_5: 0.4442  loss_mask_5: 0.5266  loss_dice_5: 3.823  loss_ce_6: 0.4339  loss_mask_6: 0.5281  loss_dice_6: 3.816  loss_ce_7: 0.4433  loss_mask_7: 0.5273  loss_dice_7: 3.829  loss_ce_8: 0.4319  loss_mask_8: 0.5284  loss_dice_8: 3.83  time: 1.5080  data_time: 0.0644  lr: 8.7446e-06  max_mem: 21460M
[01/17 21:04:54] d2.utils.events INFO:  eta: 14:05:57  iter: 5559  total_loss: 47.53  loss_ce: 0.3964  loss_mask: 0.5273  loss_dice: 3.837  loss_ce_0: 0.6534  loss_mask_0: 0.5057  loss_dice_0: 3.942  loss_ce_1: 0.3972  loss_mask_1: 0.5237  loss_dice_1: 3.867  loss_ce_2: 0.4191  loss_mask_2: 0.5252  loss_dice_2: 3.84  loss_ce_3: 0.4098  loss_mask_3: 0.5256  loss_dice_3: 3.83  loss_ce_4: 0.4131  loss_mask_4: 0.5259  loss_dice_4: 3.831  loss_ce_5: 0.4038  loss_mask_5: 0.5263  loss_dice_5: 3.834  loss_ce_6: 0.3998  loss_mask_6: 0.5279  loss_dice_6: 3.831  loss_ce_7: 0.3948  loss_mask_7: 0.5272  loss_dice_7: 3.831  loss_ce_8: 0.3992  loss_mask_8: 0.5263  loss_dice_8: 3.837  time: 1.5079  data_time: 0.0811  lr: 8.7401e-06  max_mem: 21460M
[01/17 21:05:24] d2.utils.events INFO:  eta: 14:05:28  iter: 5579  total_loss: 48.48  loss_ce: 0.4545  loss_mask: 0.5307  loss_dice: 3.816  loss_ce_0: 0.6649  loss_mask_0: 0.5071  loss_dice_0: 3.942  loss_ce_1: 0.4402  loss_mask_1: 0.5271  loss_dice_1: 3.853  loss_ce_2: 0.4657  loss_mask_2: 0.5288  loss_dice_2: 3.828  loss_ce_3: 0.4587  loss_mask_3: 0.5277  loss_dice_3: 3.818  loss_ce_4: 0.4729  loss_mask_4: 0.5289  loss_dice_4: 3.817  loss_ce_5: 0.4671  loss_mask_5: 0.5274  loss_dice_5: 3.817  loss_ce_6: 0.4873  loss_mask_6: 0.5257  loss_dice_6: 3.808  loss_ce_7: 0.4634  loss_mask_7: 0.5275  loss_dice_7: 3.816  loss_ce_8: 0.478  loss_mask_8: 0.5282  loss_dice_8: 3.808  time: 1.5078  data_time: 0.0784  lr: 8.7355e-06  max_mem: 21460M
[01/17 21:05:53] d2.utils.events INFO:  eta: 14:04:44  iter: 5599  total_loss: 48.17  loss_ce: 0.4214  loss_mask: 0.5348  loss_dice: 3.76  loss_ce_0: 0.686  loss_mask_0: 0.5116  loss_dice_0: 3.883  loss_ce_1: 0.4302  loss_mask_1: 0.5292  loss_dice_1: 3.803  loss_ce_2: 0.4221  loss_mask_2: 0.535  loss_dice_2: 3.783  loss_ce_3: 0.4155  loss_mask_3: 0.5358  loss_dice_3: 3.765  loss_ce_4: 0.4256  loss_mask_4: 0.5386  loss_dice_4: 3.761  loss_ce_5: 0.4371  loss_mask_5: 0.5399  loss_dice_5: 3.761  loss_ce_6: 0.4335  loss_mask_6: 0.539  loss_dice_6: 3.758  loss_ce_7: 0.4148  loss_mask_7: 0.5389  loss_dice_7: 3.766  loss_ce_8: 0.4193  loss_mask_8: 0.5374  loss_dice_8: 3.769  time: 1.5077  data_time: 0.0654  lr: 8.7309e-06  max_mem: 21460M
[01/17 21:06:23] d2.utils.events INFO:  eta: 14:04:29  iter: 5619  total_loss: 47.42  loss_ce: 0.4023  loss_mask: 0.5106  loss_dice: 3.794  loss_ce_0: 0.6476  loss_mask_0: 0.49  loss_dice_0: 3.915  loss_ce_1: 0.4077  loss_mask_1: 0.5065  loss_dice_1: 3.826  loss_ce_2: 0.4169  loss_mask_2: 0.5114  loss_dice_2: 3.798  loss_ce_3: 0.4274  loss_mask_3: 0.5102  loss_dice_3: 3.781  loss_ce_4: 0.4029  loss_mask_4: 0.5098  loss_dice_4: 3.796  loss_ce_5: 0.4049  loss_mask_5: 0.5095  loss_dice_5: 3.794  loss_ce_6: 0.4161  loss_mask_6: 0.5102  loss_dice_6: 3.783  loss_ce_7: 0.403  loss_mask_7: 0.5111  loss_dice_7: 3.801  loss_ce_8: 0.3989  loss_mask_8: 0.5113  loss_dice_8: 3.8  time: 1.5076  data_time: 0.0748  lr: 8.7264e-06  max_mem: 21460M
[01/17 21:06:53] d2.utils.events INFO:  eta: 14:04:14  iter: 5639  total_loss: 47.46  loss_ce: 0.3838  loss_mask: 0.5219  loss_dice: 3.779  loss_ce_0: 0.6234  loss_mask_0: 0.498  loss_dice_0: 3.91  loss_ce_1: 0.3752  loss_mask_1: 0.5205  loss_dice_1: 3.822  loss_ce_2: 0.3996  loss_mask_2: 0.5195  loss_dice_2: 3.794  loss_ce_3: 0.3817  loss_mask_3: 0.5198  loss_dice_3: 3.79  loss_ce_4: 0.3919  loss_mask_4: 0.5193  loss_dice_4: 3.771  loss_ce_5: 0.3812  loss_mask_5: 0.518  loss_dice_5: 3.783  loss_ce_6: 0.3967  loss_mask_6: 0.5227  loss_dice_6: 3.774  loss_ce_7: 0.3863  loss_mask_7: 0.5243  loss_dice_7: 3.784  loss_ce_8: 0.3825  loss_mask_8: 0.5228  loss_dice_8: 3.784  time: 1.5075  data_time: 0.0753  lr: 8.7218e-06  max_mem: 21460M
[01/17 21:07:22] d2.utils.events INFO:  eta: 14:03:45  iter: 5659  total_loss: 47.67  loss_ce: 0.4076  loss_mask: 0.5136  loss_dice: 3.783  loss_ce_0: 0.6477  loss_mask_0: 0.494  loss_dice_0: 3.918  loss_ce_1: 0.4215  loss_mask_1: 0.509  loss_dice_1: 3.833  loss_ce_2: 0.4274  loss_mask_2: 0.5125  loss_dice_2: 3.797  loss_ce_3: 0.4128  loss_mask_3: 0.5138  loss_dice_3: 3.784  loss_ce_4: 0.4246  loss_mask_4: 0.514  loss_dice_4: 3.788  loss_ce_5: 0.4199  loss_mask_5: 0.5131  loss_dice_5: 3.786  loss_ce_6: 0.4201  loss_mask_6: 0.5098  loss_dice_6: 3.784  loss_ce_7: 0.4082  loss_mask_7: 0.5101  loss_dice_7: 3.788  loss_ce_8: 0.4085  loss_mask_8: 0.5112  loss_dice_8: 3.784  time: 1.5073  data_time: 0.0732  lr: 8.7172e-06  max_mem: 21460M
[01/17 21:07:52] d2.utils.events INFO:  eta: 14:03:40  iter: 5679  total_loss: 47.89  loss_ce: 0.4331  loss_mask: 0.529  loss_dice: 3.796  loss_ce_0: 0.6519  loss_mask_0: 0.5009  loss_dice_0: 3.925  loss_ce_1: 0.4172  loss_mask_1: 0.5255  loss_dice_1: 3.842  loss_ce_2: 0.4335  loss_mask_2: 0.526  loss_dice_2: 3.82  loss_ce_3: 0.4287  loss_mask_3: 0.5215  loss_dice_3: 3.806  loss_ce_4: 0.4157  loss_mask_4: 0.5223  loss_dice_4: 3.806  loss_ce_5: 0.4042  loss_mask_5: 0.5266  loss_dice_5: 3.804  loss_ce_6: 0.4164  loss_mask_6: 0.5289  loss_dice_6: 3.786  loss_ce_7: 0.4189  loss_mask_7: 0.5295  loss_dice_7: 3.803  loss_ce_8: 0.4262  loss_mask_8: 0.529  loss_dice_8: 3.793  time: 1.5073  data_time: 0.0692  lr: 8.7126e-06  max_mem: 21460M
[01/17 21:08:21] d2.utils.events INFO:  eta: 14:02:54  iter: 5699  total_loss: 47.38  loss_ce: 0.4105  loss_mask: 0.5173  loss_dice: 3.78  loss_ce_0: 0.6451  loss_mask_0: 0.4961  loss_dice_0: 3.901  loss_ce_1: 0.4049  loss_mask_1: 0.5113  loss_dice_1: 3.809  loss_ce_2: 0.4312  loss_mask_2: 0.5136  loss_dice_2: 3.786  loss_ce_3: 0.4207  loss_mask_3: 0.5125  loss_dice_3: 3.781  loss_ce_4: 0.403  loss_mask_4: 0.5155  loss_dice_4: 3.774  loss_ce_5: 0.4139  loss_mask_5: 0.5158  loss_dice_5: 3.772  loss_ce_6: 0.4194  loss_mask_6: 0.5145  loss_dice_6: 3.774  loss_ce_7: 0.404  loss_mask_7: 0.5157  loss_dice_7: 3.782  loss_ce_8: 0.4123  loss_mask_8: 0.5174  loss_dice_8: 3.776  time: 1.5071  data_time: 0.0665  lr: 8.7081e-06  max_mem: 21460M
[01/17 21:08:51] d2.utils.events INFO:  eta: 14:02:41  iter: 5719  total_loss: 47.4  loss_ce: 0.4006  loss_mask: 0.5141  loss_dice: 3.777  loss_ce_0: 0.6538  loss_mask_0: 0.4881  loss_dice_0: 3.896  loss_ce_1: 0.4067  loss_mask_1: 0.5134  loss_dice_1: 3.811  loss_ce_2: 0.4111  loss_mask_2: 0.5126  loss_dice_2: 3.793  loss_ce_3: 0.4052  loss_mask_3: 0.5182  loss_dice_3: 3.787  loss_ce_4: 0.4138  loss_mask_4: 0.5178  loss_dice_4: 3.781  loss_ce_5: 0.4018  loss_mask_5: 0.5179  loss_dice_5: 3.777  loss_ce_6: 0.3926  loss_mask_6: 0.5199  loss_dice_6: 3.78  loss_ce_7: 0.3906  loss_mask_7: 0.5162  loss_dice_7: 3.778  loss_ce_8: 0.3919  loss_mask_8: 0.5156  loss_dice_8: 3.787  time: 1.5071  data_time: 0.0678  lr: 8.7035e-06  max_mem: 21460M
[01/17 21:09:21] d2.utils.events INFO:  eta: 14:02:25  iter: 5739  total_loss: 47.49  loss_ce: 0.4299  loss_mask: 0.5044  loss_dice: 3.756  loss_ce_0: 0.6532  loss_mask_0: 0.4867  loss_dice_0: 3.884  loss_ce_1: 0.4122  loss_mask_1: 0.5074  loss_dice_1: 3.813  loss_ce_2: 0.4271  loss_mask_2: 0.5033  loss_dice_2: 3.781  loss_ce_3: 0.421  loss_mask_3: 0.5072  loss_dice_3: 3.771  loss_ce_4: 0.4057  loss_mask_4: 0.5078  loss_dice_4: 3.772  loss_ce_5: 0.4144  loss_mask_5: 0.5063  loss_dice_5: 3.777  loss_ce_6: 0.4161  loss_mask_6: 0.5027  loss_dice_6: 3.772  loss_ce_7: 0.412  loss_mask_7: 0.5076  loss_dice_7: 3.77  loss_ce_8: 0.4002  loss_mask_8: 0.5071  loss_dice_8: 3.764  time: 1.5070  data_time: 0.0727  lr: 8.6989e-06  max_mem: 21460M
[01/17 21:09:51] d2.utils.events INFO:  eta: 14:02:11  iter: 5759  total_loss: 47.3  loss_ce: 0.3898  loss_mask: 0.5036  loss_dice: 3.77  loss_ce_0: 0.623  loss_mask_0: 0.4799  loss_dice_0: 3.912  loss_ce_1: 0.3726  loss_mask_1: 0.5015  loss_dice_1: 3.818  loss_ce_2: 0.4055  loss_mask_2: 0.5029  loss_dice_2: 3.788  loss_ce_3: 0.3969  loss_mask_3: 0.4996  loss_dice_3: 3.777  loss_ce_4: 0.3989  loss_mask_4: 0.4992  loss_dice_4: 3.788  loss_ce_5: 0.3804  loss_mask_5: 0.5007  loss_dice_5: 3.785  loss_ce_6: 0.3876  loss_mask_6: 0.5045  loss_dice_6: 3.781  loss_ce_7: 0.3833  loss_mask_7: 0.5037  loss_dice_7: 3.78  loss_ce_8: 0.3918  loss_mask_8: 0.5047  loss_dice_8: 3.784  time: 1.5070  data_time: 0.0724  lr: 8.6944e-06  max_mem: 21460M
[01/17 21:10:21] d2.utils.events INFO:  eta: 14:01:58  iter: 5779  total_loss: 47.74  loss_ce: 0.396  loss_mask: 0.5096  loss_dice: 3.788  loss_ce_0: 0.6537  loss_mask_0: 0.4927  loss_dice_0: 3.918  loss_ce_1: 0.4444  loss_mask_1: 0.5104  loss_dice_1: 3.834  loss_ce_2: 0.4513  loss_mask_2: 0.509  loss_dice_2: 3.812  loss_ce_3: 0.4161  loss_mask_3: 0.5084  loss_dice_3: 3.804  loss_ce_4: 0.4231  loss_mask_4: 0.5128  loss_dice_4: 3.801  loss_ce_5: 0.4213  loss_mask_5: 0.5107  loss_dice_5: 3.792  loss_ce_6: 0.4129  loss_mask_6: 0.5109  loss_dice_6: 3.795  loss_ce_7: 0.3986  loss_mask_7: 0.5083  loss_dice_7: 3.801  loss_ce_8: 0.4035  loss_mask_8: 0.5102  loss_dice_8: 3.796  time: 1.5069  data_time: 0.0827  lr: 8.6898e-06  max_mem: 21460M
[01/17 21:10:51] d2.utils.events INFO:  eta: 14:01:34  iter: 5799  total_loss: 46.99  loss_ce: 0.4126  loss_mask: 0.5143  loss_dice: 3.742  loss_ce_0: 0.6443  loss_mask_0: 0.4948  loss_dice_0: 3.859  loss_ce_1: 0.4292  loss_mask_1: 0.5124  loss_dice_1: 3.785  loss_ce_2: 0.427  loss_mask_2: 0.5119  loss_dice_2: 3.753  loss_ce_3: 0.4065  loss_mask_3: 0.5142  loss_dice_3: 3.744  loss_ce_4: 0.4162  loss_mask_4: 0.5109  loss_dice_4: 3.745  loss_ce_5: 0.3947  loss_mask_5: 0.5141  loss_dice_5: 3.734  loss_ce_6: 0.4153  loss_mask_6: 0.5136  loss_dice_6: 3.74  loss_ce_7: 0.3859  loss_mask_7: 0.5137  loss_dice_7: 3.735  loss_ce_8: 0.408  loss_mask_8: 0.5143  loss_dice_8: 3.74  time: 1.5068  data_time: 0.0742  lr: 8.6852e-06  max_mem: 21460M
[01/17 21:11:21] d2.utils.events INFO:  eta: 14:01:00  iter: 5819  total_loss: 47.63  loss_ce: 0.4279  loss_mask: 0.5086  loss_dice: 3.779  loss_ce_0: 0.6709  loss_mask_0: 0.4857  loss_dice_0: 3.913  loss_ce_1: 0.4131  loss_mask_1: 0.5086  loss_dice_1: 3.821  loss_ce_2: 0.4282  loss_mask_2: 0.5092  loss_dice_2: 3.795  loss_ce_3: 0.4214  loss_mask_3: 0.5081  loss_dice_3: 3.778  loss_ce_4: 0.4311  loss_mask_4: 0.5088  loss_dice_4: 3.771  loss_ce_5: 0.4259  loss_mask_5: 0.5086  loss_dice_5: 3.772  loss_ce_6: 0.4229  loss_mask_6: 0.5066  loss_dice_6: 3.775  loss_ce_7: 0.4276  loss_mask_7: 0.509  loss_dice_7: 3.78  loss_ce_8: 0.4317  loss_mask_8: 0.5099  loss_dice_8: 3.774  time: 1.5068  data_time: 0.0783  lr: 8.6807e-06  max_mem: 21460M
[01/17 21:11:51] d2.utils.events INFO:  eta: 14:00:31  iter: 5839  total_loss: 47.16  loss_ce: 0.405  loss_mask: 0.523  loss_dice: 3.748  loss_ce_0: 0.6701  loss_mask_0: 0.5012  loss_dice_0: 3.865  loss_ce_1: 0.4086  loss_mask_1: 0.5225  loss_dice_1: 3.766  loss_ce_2: 0.4251  loss_mask_2: 0.5249  loss_dice_2: 3.755  loss_ce_3: 0.3827  loss_mask_3: 0.5246  loss_dice_3: 3.744  loss_ce_4: 0.3964  loss_mask_4: 0.5251  loss_dice_4: 3.747  loss_ce_5: 0.4013  loss_mask_5: 0.5241  loss_dice_5: 3.744  loss_ce_6: 0.4047  loss_mask_6: 0.5261  loss_dice_6: 3.744  loss_ce_7: 0.3956  loss_mask_7: 0.525  loss_dice_7: 3.745  loss_ce_8: 0.3939  loss_mask_8: 0.5249  loss_dice_8: 3.749  time: 1.5067  data_time: 0.0812  lr: 8.6761e-06  max_mem: 21460M
[01/17 21:12:20] d2.utils.events INFO:  eta: 14:00:13  iter: 5859  total_loss: 47.21  loss_ce: 0.4333  loss_mask: 0.5072  loss_dice: 3.729  loss_ce_0: 0.6626  loss_mask_0: 0.4855  loss_dice_0: 3.868  loss_ce_1: 0.4329  loss_mask_1: 0.5042  loss_dice_1: 3.769  loss_ce_2: 0.4228  loss_mask_2: 0.5047  loss_dice_2: 3.753  loss_ce_3: 0.4231  loss_mask_3: 0.5057  loss_dice_3: 3.73  loss_ce_4: 0.4398  loss_mask_4: 0.5053  loss_dice_4: 3.742  loss_ce_5: 0.4291  loss_mask_5: 0.507  loss_dice_5: 3.734  loss_ce_6: 0.4322  loss_mask_6: 0.5067  loss_dice_6: 3.727  loss_ce_7: 0.4145  loss_mask_7: 0.5068  loss_dice_7: 3.74  loss_ce_8: 0.4205  loss_mask_8: 0.5064  loss_dice_8: 3.737  time: 1.5066  data_time: 0.0819  lr: 8.6715e-06  max_mem: 21460M
[01/17 21:12:49] d2.utils.events INFO:  eta: 13:59:51  iter: 5879  total_loss: 46.86  loss_ce: 0.3877  loss_mask: 0.525  loss_dice: 3.738  loss_ce_0: 0.6478  loss_mask_0: 0.499  loss_dice_0: 3.872  loss_ce_1: 0.3751  loss_mask_1: 0.5248  loss_dice_1: 3.777  loss_ce_2: 0.3913  loss_mask_2: 0.5285  loss_dice_2: 3.744  loss_ce_3: 0.3924  loss_mask_3: 0.5255  loss_dice_3: 3.742  loss_ce_4: 0.3972  loss_mask_4: 0.529  loss_dice_4: 3.743  loss_ce_5: 0.3998  loss_mask_5: 0.5268  loss_dice_5: 3.735  loss_ce_6: 0.3994  loss_mask_6: 0.5234  loss_dice_6: 3.739  loss_ce_7: 0.3754  loss_mask_7: 0.5277  loss_dice_7: 3.742  loss_ce_8: 0.3964  loss_mask_8: 0.5256  loss_dice_8: 3.738  time: 1.5065  data_time: 0.0745  lr: 8.6669e-06  max_mem: 21460M
[01/17 21:13:19] d2.utils.events INFO:  eta: 13:59:32  iter: 5899  total_loss: 47.55  loss_ce: 0.4137  loss_mask: 0.5073  loss_dice: 3.749  loss_ce_0: 0.6205  loss_mask_0: 0.4879  loss_dice_0: 3.877  loss_ce_1: 0.4153  loss_mask_1: 0.5065  loss_dice_1: 3.794  loss_ce_2: 0.4281  loss_mask_2: 0.5073  loss_dice_2: 3.778  loss_ce_3: 0.4116  loss_mask_3: 0.507  loss_dice_3: 3.759  loss_ce_4: 0.4189  loss_mask_4: 0.5078  loss_dice_4: 3.757  loss_ce_5: 0.4181  loss_mask_5: 0.5097  loss_dice_5: 3.751  loss_ce_6: 0.4005  loss_mask_6: 0.509  loss_dice_6: 3.748  loss_ce_7: 0.4086  loss_mask_7: 0.5094  loss_dice_7: 3.747  loss_ce_8: 0.4167  loss_mask_8: 0.5095  loss_dice_8: 3.758  time: 1.5065  data_time: 0.0677  lr: 8.6624e-06  max_mem: 21460M
[01/17 21:13:49] d2.utils.events INFO:  eta: 13:59:15  iter: 5919  total_loss: 46.98  loss_ce: 0.4099  loss_mask: 0.5166  loss_dice: 3.757  loss_ce_0: 0.6125  loss_mask_0: 0.493  loss_dice_0: 3.874  loss_ce_1: 0.4362  loss_mask_1: 0.5196  loss_dice_1: 3.794  loss_ce_2: 0.4262  loss_mask_2: 0.5231  loss_dice_2: 3.765  loss_ce_3: 0.4076  loss_mask_3: 0.5203  loss_dice_3: 3.756  loss_ce_4: 0.3862  loss_mask_4: 0.5174  loss_dice_4: 3.757  loss_ce_5: 0.4251  loss_mask_5: 0.5151  loss_dice_5: 3.757  loss_ce_6: 0.4015  loss_mask_6: 0.5156  loss_dice_6: 3.753  loss_ce_7: 0.4167  loss_mask_7: 0.5154  loss_dice_7: 3.763  loss_ce_8: 0.4196  loss_mask_8: 0.5192  loss_dice_8: 3.756  time: 1.5064  data_time: 0.0777  lr: 8.6578e-06  max_mem: 21460M
[01/17 21:14:19] d2.utils.events INFO:  eta: 13:58:41  iter: 5939  total_loss: 47.39  loss_ce: 0.3962  loss_mask: 0.5151  loss_dice: 3.792  loss_ce_0: 0.6719  loss_mask_0: 0.5034  loss_dice_0: 3.92  loss_ce_1: 0.4331  loss_mask_1: 0.5187  loss_dice_1: 3.832  loss_ce_2: 0.4371  loss_mask_2: 0.517  loss_dice_2: 3.81  loss_ce_3: 0.4254  loss_mask_3: 0.5156  loss_dice_3: 3.797  loss_ce_4: 0.4077  loss_mask_4: 0.5155  loss_dice_4: 3.791  loss_ce_5: 0.4018  loss_mask_5: 0.515  loss_dice_5: 3.787  loss_ce_6: 0.3898  loss_mask_6: 0.5168  loss_dice_6: 3.788  loss_ce_7: 0.4009  loss_mask_7: 0.515  loss_dice_7: 3.796  loss_ce_8: 0.4009  loss_mask_8: 0.5146  loss_dice_8: 3.791  time: 1.5062  data_time: 0.0670  lr: 8.6532e-06  max_mem: 21460M
[01/17 21:14:48] d2.utils.events INFO:  eta: 13:58:11  iter: 5959  total_loss: 46.98  loss_ce: 0.4086  loss_mask: 0.5072  loss_dice: 3.732  loss_ce_0: 0.6671  loss_mask_0: 0.488  loss_dice_0: 3.866  loss_ce_1: 0.4087  loss_mask_1: 0.507  loss_dice_1: 3.764  loss_ce_2: 0.4171  loss_mask_2: 0.5036  loss_dice_2: 3.746  loss_ce_3: 0.4141  loss_mask_3: 0.5017  loss_dice_3: 3.733  loss_ce_4: 0.3985  loss_mask_4: 0.5009  loss_dice_4: 3.731  loss_ce_5: 0.3999  loss_mask_5: 0.5058  loss_dice_5: 3.737  loss_ce_6: 0.407  loss_mask_6: 0.5073  loss_dice_6: 3.729  loss_ce_7: 0.3987  loss_mask_7: 0.5064  loss_dice_7: 3.728  loss_ce_8: 0.4061  loss_mask_8: 0.5045  loss_dice_8: 3.735  time: 1.5062  data_time: 0.0749  lr: 8.6486e-06  max_mem: 21460M
[01/17 21:15:17] d2.utils.events INFO:  eta: 13:57:39  iter: 5979  total_loss: 46.92  loss_ce: 0.4117  loss_mask: 0.5279  loss_dice: 3.739  loss_ce_0: 0.6356  loss_mask_0: 0.4996  loss_dice_0: 3.854  loss_ce_1: 0.3917  loss_mask_1: 0.5223  loss_dice_1: 3.766  loss_ce_2: 0.4215  loss_mask_2: 0.5262  loss_dice_2: 3.749  loss_ce_3: 0.4215  loss_mask_3: 0.5247  loss_dice_3: 3.734  loss_ce_4: 0.3905  loss_mask_4: 0.5259  loss_dice_4: 3.737  loss_ce_5: 0.3902  loss_mask_5: 0.528  loss_dice_5: 3.732  loss_ce_6: 0.3927  loss_mask_6: 0.5263  loss_dice_6: 3.733  loss_ce_7: 0.394  loss_mask_7: 0.5262  loss_dice_7: 3.742  loss_ce_8: 0.3933  loss_mask_8: 0.5273  loss_dice_8: 3.736  time: 1.5060  data_time: 0.0733  lr: 8.6441e-06  max_mem: 21460M
[01/17 21:15:47] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 21:15:47] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 21:15:47] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 21:15:48] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 21:16:01] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0072 s/iter. Inference: 0.1657 s/iter. Eval: 0.1905 s/iter. Total: 0.3634 s/iter. ETA=0:06:33
[01/17 21:16:06] d2.evaluation.evaluator INFO: Inference done 25/1093. Dataloading: 0.0097 s/iter. Inference: 0.1725 s/iter. Eval: 0.1898 s/iter. Total: 0.3721 s/iter. ETA=0:06:37
[01/17 21:16:12] d2.evaluation.evaluator INFO: Inference done 41/1093. Dataloading: 0.0095 s/iter. Inference: 0.1638 s/iter. Eval: 0.1795 s/iter. Total: 0.3528 s/iter. ETA=0:06:11
[01/17 21:16:17] d2.evaluation.evaluator INFO: Inference done 57/1093. Dataloading: 0.0103 s/iter. Inference: 0.1591 s/iter. Eval: 0.1758 s/iter. Total: 0.3453 s/iter. ETA=0:05:57
[01/17 21:16:22] d2.evaluation.evaluator INFO: Inference done 73/1093. Dataloading: 0.0106 s/iter. Inference: 0.1551 s/iter. Eval: 0.1754 s/iter. Total: 0.3411 s/iter. ETA=0:05:47
[01/17 21:16:27] d2.evaluation.evaluator INFO: Inference done 87/1093. Dataloading: 0.0104 s/iter. Inference: 0.1555 s/iter. Eval: 0.1786 s/iter. Total: 0.3446 s/iter. ETA=0:05:46
[01/17 21:16:32] d2.evaluation.evaluator INFO: Inference done 103/1093. Dataloading: 0.0104 s/iter. Inference: 0.1547 s/iter. Eval: 0.1759 s/iter. Total: 0.3411 s/iter. ETA=0:05:37
[01/17 21:16:37] d2.evaluation.evaluator INFO: Inference done 117/1093. Dataloading: 0.0102 s/iter. Inference: 0.1590 s/iter. Eval: 0.1747 s/iter. Total: 0.3440 s/iter. ETA=0:05:35
[01/17 21:16:43] d2.evaluation.evaluator INFO: Inference done 133/1093. Dataloading: 0.0101 s/iter. Inference: 0.1584 s/iter. Eval: 0.1732 s/iter. Total: 0.3417 s/iter. ETA=0:05:28
[01/17 21:16:48] d2.evaluation.evaluator INFO: Inference done 150/1093. Dataloading: 0.0099 s/iter. Inference: 0.1570 s/iter. Eval: 0.1699 s/iter. Total: 0.3369 s/iter. ETA=0:05:17
[01/17 21:16:53] d2.evaluation.evaluator INFO: Inference done 166/1093. Dataloading: 0.0101 s/iter. Inference: 0.1568 s/iter. Eval: 0.1687 s/iter. Total: 0.3356 s/iter. ETA=0:05:11
[01/17 21:16:58] d2.evaluation.evaluator INFO: Inference done 181/1093. Dataloading: 0.0101 s/iter. Inference: 0.1573 s/iter. Eval: 0.1692 s/iter. Total: 0.3368 s/iter. ETA=0:05:07
[01/17 21:17:03] d2.evaluation.evaluator INFO: Inference done 197/1093. Dataloading: 0.0102 s/iter. Inference: 0.1571 s/iter. Eval: 0.1687 s/iter. Total: 0.3361 s/iter. ETA=0:05:01
[01/17 21:17:09] d2.evaluation.evaluator INFO: Inference done 212/1093. Dataloading: 0.0104 s/iter. Inference: 0.1573 s/iter. Eval: 0.1697 s/iter. Total: 0.3374 s/iter. ETA=0:04:57
[01/17 21:17:14] d2.evaluation.evaluator INFO: Inference done 226/1093. Dataloading: 0.0103 s/iter. Inference: 0.1585 s/iter. Eval: 0.1707 s/iter. Total: 0.3397 s/iter. ETA=0:04:54
[01/17 21:17:19] d2.evaluation.evaluator INFO: Inference done 240/1093. Dataloading: 0.0104 s/iter. Inference: 0.1587 s/iter. Eval: 0.1726 s/iter. Total: 0.3417 s/iter. ETA=0:04:51
[01/17 21:17:24] d2.evaluation.evaluator INFO: Inference done 254/1093. Dataloading: 0.0105 s/iter. Inference: 0.1580 s/iter. Eval: 0.1743 s/iter. Total: 0.3429 s/iter. ETA=0:04:47
[01/17 21:17:30] d2.evaluation.evaluator INFO: Inference done 268/1093. Dataloading: 0.0106 s/iter. Inference: 0.1587 s/iter. Eval: 0.1752 s/iter. Total: 0.3445 s/iter. ETA=0:04:44
[01/17 21:17:35] d2.evaluation.evaluator INFO: Inference done 282/1093. Dataloading: 0.0108 s/iter. Inference: 0.1590 s/iter. Eval: 0.1756 s/iter. Total: 0.3455 s/iter. ETA=0:04:40
[01/17 21:17:40] d2.evaluation.evaluator INFO: Inference done 298/1093. Dataloading: 0.0108 s/iter. Inference: 0.1583 s/iter. Eval: 0.1752 s/iter. Total: 0.3443 s/iter. ETA=0:04:33
[01/17 21:17:45] d2.evaluation.evaluator INFO: Inference done 313/1093. Dataloading: 0.0108 s/iter. Inference: 0.1580 s/iter. Eval: 0.1756 s/iter. Total: 0.3445 s/iter. ETA=0:04:28
[01/17 21:17:50] d2.evaluation.evaluator INFO: Inference done 328/1093. Dataloading: 0.0108 s/iter. Inference: 0.1576 s/iter. Eval: 0.1764 s/iter. Total: 0.3448 s/iter. ETA=0:04:23
[01/17 21:17:55] d2.evaluation.evaluator INFO: Inference done 345/1093. Dataloading: 0.0107 s/iter. Inference: 0.1582 s/iter. Eval: 0.1740 s/iter. Total: 0.3429 s/iter. ETA=0:04:16
[01/17 21:18:01] d2.evaluation.evaluator INFO: Inference done 362/1093. Dataloading: 0.0106 s/iter. Inference: 0.1575 s/iter. Eval: 0.1732 s/iter. Total: 0.3414 s/iter. ETA=0:04:09
[01/17 21:18:06] d2.evaluation.evaluator INFO: Inference done 377/1093. Dataloading: 0.0107 s/iter. Inference: 0.1579 s/iter. Eval: 0.1732 s/iter. Total: 0.3419 s/iter. ETA=0:04:04
[01/17 21:18:11] d2.evaluation.evaluator INFO: Inference done 392/1093. Dataloading: 0.0106 s/iter. Inference: 0.1576 s/iter. Eval: 0.1735 s/iter. Total: 0.3418 s/iter. ETA=0:03:59
[01/17 21:18:16] d2.evaluation.evaluator INFO: Inference done 407/1093. Dataloading: 0.0106 s/iter. Inference: 0.1580 s/iter. Eval: 0.1734 s/iter. Total: 0.3421 s/iter. ETA=0:03:54
[01/17 21:18:21] d2.evaluation.evaluator INFO: Inference done 422/1093. Dataloading: 0.0107 s/iter. Inference: 0.1575 s/iter. Eval: 0.1735 s/iter. Total: 0.3418 s/iter. ETA=0:03:49
[01/17 21:18:26] d2.evaluation.evaluator INFO: Inference done 438/1093. Dataloading: 0.0107 s/iter. Inference: 0.1573 s/iter. Eval: 0.1728 s/iter. Total: 0.3408 s/iter. ETA=0:03:43
[01/17 21:18:32] d2.evaluation.evaluator INFO: Inference done 455/1093. Dataloading: 0.0106 s/iter. Inference: 0.1567 s/iter. Eval: 0.1723 s/iter. Total: 0.3396 s/iter. ETA=0:03:36
[01/17 21:18:37] d2.evaluation.evaluator INFO: Inference done 471/1093. Dataloading: 0.0106 s/iter. Inference: 0.1566 s/iter. Eval: 0.1717 s/iter. Total: 0.3390 s/iter. ETA=0:03:30
[01/17 21:18:42] d2.evaluation.evaluator INFO: Inference done 487/1093. Dataloading: 0.0106 s/iter. Inference: 0.1565 s/iter. Eval: 0.1711 s/iter. Total: 0.3383 s/iter. ETA=0:03:24
[01/17 21:18:47] d2.evaluation.evaluator INFO: Inference done 504/1093. Dataloading: 0.0105 s/iter. Inference: 0.1566 s/iter. Eval: 0.1698 s/iter. Total: 0.3370 s/iter. ETA=0:03:18
[01/17 21:18:52] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0105 s/iter. Inference: 0.1563 s/iter. Eval: 0.1696 s/iter. Total: 0.3364 s/iter. ETA=0:03:12
[01/17 21:18:58] d2.evaluation.evaluator INFO: Inference done 536/1093. Dataloading: 0.0105 s/iter. Inference: 0.1562 s/iter. Eval: 0.1698 s/iter. Total: 0.3366 s/iter. ETA=0:03:07
[01/17 21:19:03] d2.evaluation.evaluator INFO: Inference done 552/1093. Dataloading: 0.0105 s/iter. Inference: 0.1557 s/iter. Eval: 0.1698 s/iter. Total: 0.3361 s/iter. ETA=0:03:01
[01/17 21:19:08] d2.evaluation.evaluator INFO: Inference done 568/1093. Dataloading: 0.0105 s/iter. Inference: 0.1556 s/iter. Eval: 0.1697 s/iter. Total: 0.3359 s/iter. ETA=0:02:56
[01/17 21:19:13] d2.evaluation.evaluator INFO: Inference done 585/1093. Dataloading: 0.0106 s/iter. Inference: 0.1557 s/iter. Eval: 0.1684 s/iter. Total: 0.3349 s/iter. ETA=0:02:50
[01/17 21:19:18] d2.evaluation.evaluator INFO: Inference done 599/1093. Dataloading: 0.0107 s/iter. Inference: 0.1559 s/iter. Eval: 0.1688 s/iter. Total: 0.3355 s/iter. ETA=0:02:45
[01/17 21:19:24] d2.evaluation.evaluator INFO: Inference done 614/1093. Dataloading: 0.0107 s/iter. Inference: 0.1562 s/iter. Eval: 0.1690 s/iter. Total: 0.3360 s/iter. ETA=0:02:40
[01/17 21:19:29] d2.evaluation.evaluator INFO: Inference done 629/1093. Dataloading: 0.0107 s/iter. Inference: 0.1564 s/iter. Eval: 0.1689 s/iter. Total: 0.3360 s/iter. ETA=0:02:35
[01/17 21:19:34] d2.evaluation.evaluator INFO: Inference done 646/1093. Dataloading: 0.0106 s/iter. Inference: 0.1563 s/iter. Eval: 0.1682 s/iter. Total: 0.3352 s/iter. ETA=0:02:29
[01/17 21:19:39] d2.evaluation.evaluator INFO: Inference done 663/1093. Dataloading: 0.0106 s/iter. Inference: 0.1559 s/iter. Eval: 0.1679 s/iter. Total: 0.3345 s/iter. ETA=0:02:23
[01/17 21:19:44] d2.evaluation.evaluator INFO: Inference done 679/1093. Dataloading: 0.0106 s/iter. Inference: 0.1557 s/iter. Eval: 0.1677 s/iter. Total: 0.3340 s/iter. ETA=0:02:18
[01/17 21:19:49] d2.evaluation.evaluator INFO: Inference done 696/1093. Dataloading: 0.0105 s/iter. Inference: 0.1555 s/iter. Eval: 0.1671 s/iter. Total: 0.3332 s/iter. ETA=0:02:12
[01/17 21:19:54] d2.evaluation.evaluator INFO: Inference done 711/1093. Dataloading: 0.0106 s/iter. Inference: 0.1553 s/iter. Eval: 0.1674 s/iter. Total: 0.3334 s/iter. ETA=0:02:07
[01/17 21:19:59] d2.evaluation.evaluator INFO: Inference done 727/1093. Dataloading: 0.0105 s/iter. Inference: 0.1554 s/iter. Eval: 0.1671 s/iter. Total: 0.3331 s/iter. ETA=0:02:01
[01/17 21:20:05] d2.evaluation.evaluator INFO: Inference done 743/1093. Dataloading: 0.0105 s/iter. Inference: 0.1556 s/iter. Eval: 0.1669 s/iter. Total: 0.3331 s/iter. ETA=0:01:56
[01/17 21:20:10] d2.evaluation.evaluator INFO: Inference done 760/1093. Dataloading: 0.0105 s/iter. Inference: 0.1553 s/iter. Eval: 0.1664 s/iter. Total: 0.3322 s/iter. ETA=0:01:50
[01/17 21:20:15] d2.evaluation.evaluator INFO: Inference done 775/1093. Dataloading: 0.0105 s/iter. Inference: 0.1550 s/iter. Eval: 0.1668 s/iter. Total: 0.3324 s/iter. ETA=0:01:45
[01/17 21:20:20] d2.evaluation.evaluator INFO: Inference done 792/1093. Dataloading: 0.0104 s/iter. Inference: 0.1549 s/iter. Eval: 0.1665 s/iter. Total: 0.3320 s/iter. ETA=0:01:39
[01/17 21:20:25] d2.evaluation.evaluator INFO: Inference done 808/1093. Dataloading: 0.0104 s/iter. Inference: 0.1550 s/iter. Eval: 0.1662 s/iter. Total: 0.3318 s/iter. ETA=0:01:34
[01/17 21:20:30] d2.evaluation.evaluator INFO: Inference done 824/1093. Dataloading: 0.0104 s/iter. Inference: 0.1551 s/iter. Eval: 0.1659 s/iter. Total: 0.3315 s/iter. ETA=0:01:29
[01/17 21:20:36] d2.evaluation.evaluator INFO: Inference done 840/1093. Dataloading: 0.0104 s/iter. Inference: 0.1553 s/iter. Eval: 0.1656 s/iter. Total: 0.3313 s/iter. ETA=0:01:23
[01/17 21:20:41] d2.evaluation.evaluator INFO: Inference done 855/1093. Dataloading: 0.0104 s/iter. Inference: 0.1553 s/iter. Eval: 0.1660 s/iter. Total: 0.3318 s/iter. ETA=0:01:18
[01/17 21:20:46] d2.evaluation.evaluator INFO: Inference done 869/1093. Dataloading: 0.0104 s/iter. Inference: 0.1558 s/iter. Eval: 0.1661 s/iter. Total: 0.3324 s/iter. ETA=0:01:14
[01/17 21:20:51] d2.evaluation.evaluator INFO: Inference done 884/1093. Dataloading: 0.0104 s/iter. Inference: 0.1557 s/iter. Eval: 0.1663 s/iter. Total: 0.3325 s/iter. ETA=0:01:09
[01/17 21:20:56] d2.evaluation.evaluator INFO: Inference done 899/1093. Dataloading: 0.0105 s/iter. Inference: 0.1558 s/iter. Eval: 0.1664 s/iter. Total: 0.3327 s/iter. ETA=0:01:04
[01/17 21:21:01] d2.evaluation.evaluator INFO: Inference done 915/1093. Dataloading: 0.0104 s/iter. Inference: 0.1557 s/iter. Eval: 0.1662 s/iter. Total: 0.3324 s/iter. ETA=0:00:59
[01/17 21:21:07] d2.evaluation.evaluator INFO: Inference done 930/1093. Dataloading: 0.0104 s/iter. Inference: 0.1560 s/iter. Eval: 0.1664 s/iter. Total: 0.3328 s/iter. ETA=0:00:54
[01/17 21:21:12] d2.evaluation.evaluator INFO: Inference done 945/1093. Dataloading: 0.0104 s/iter. Inference: 0.1560 s/iter. Eval: 0.1665 s/iter. Total: 0.3330 s/iter. ETA=0:00:49
[01/17 21:21:17] d2.evaluation.evaluator INFO: Inference done 960/1093. Dataloading: 0.0104 s/iter. Inference: 0.1562 s/iter. Eval: 0.1665 s/iter. Total: 0.3332 s/iter. ETA=0:00:44
[01/17 21:21:22] d2.evaluation.evaluator INFO: Inference done 975/1093. Dataloading: 0.0104 s/iter. Inference: 0.1562 s/iter. Eval: 0.1665 s/iter. Total: 0.3333 s/iter. ETA=0:00:39
[01/17 21:21:27] d2.evaluation.evaluator INFO: Inference done 992/1093. Dataloading: 0.0104 s/iter. Inference: 0.1562 s/iter. Eval: 0.1662 s/iter. Total: 0.3329 s/iter. ETA=0:00:33
[01/17 21:21:33] d2.evaluation.evaluator INFO: Inference done 1008/1093. Dataloading: 0.0104 s/iter. Inference: 0.1560 s/iter. Eval: 0.1662 s/iter. Total: 0.3327 s/iter. ETA=0:00:28
[01/17 21:21:38] d2.evaluation.evaluator INFO: Inference done 1023/1093. Dataloading: 0.0104 s/iter. Inference: 0.1559 s/iter. Eval: 0.1665 s/iter. Total: 0.3328 s/iter. ETA=0:00:23
[01/17 21:21:43] d2.evaluation.evaluator INFO: Inference done 1039/1093. Dataloading: 0.0104 s/iter. Inference: 0.1558 s/iter. Eval: 0.1665 s/iter. Total: 0.3328 s/iter. ETA=0:00:17
[01/17 21:21:48] d2.evaluation.evaluator INFO: Inference done 1055/1093. Dataloading: 0.0103 s/iter. Inference: 0.1557 s/iter. Eval: 0.1664 s/iter. Total: 0.3326 s/iter. ETA=0:00:12
[01/17 21:21:53] d2.evaluation.evaluator INFO: Inference done 1072/1093. Dataloading: 0.0103 s/iter. Inference: 0.1557 s/iter. Eval: 0.1659 s/iter. Total: 0.3320 s/iter. ETA=0:00:06
[01/17 21:21:58] d2.evaluation.evaluator INFO: Inference done 1088/1093. Dataloading: 0.0103 s/iter. Inference: 0.1555 s/iter. Eval: 0.1659 s/iter. Total: 0.3317 s/iter. ETA=0:00:01
[01/17 21:22:00] d2.evaluation.evaluator INFO: Total inference time: 0:06:01.278720 (0.332058 s / iter per device, on 4 devices)
[01/17 21:22:00] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:49 (0.155401 s / iter per device, on 4 devices)
[01/17 21:22:23] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 3.74053754021582, 'mIoU': 9.70229514939919, 'fwIoU': 23.568054600993797, 'IoU-0': nan, 'IoU-1': 94.10406267807019, 'IoU-2': 45.06679278572172, 'IoU-3': 45.881660873629414, 'IoU-4': 44.93953252951066, 'IoU-5': 34.7522062455775, 'IoU-6': 36.03281690266037, 'IoU-7': 25.420322392124035, 'IoU-8': 7.965645559472462, 'IoU-9': 9.766739394385747, 'IoU-10': 16.918851742204684, 'IoU-11': 21.4967520527858, 'IoU-12': 16.19057761424618, 'IoU-13': 21.381540767749822, 'IoU-14': 13.240456793590827, 'IoU-15': 18.61247498223177, 'IoU-16': 14.158652250044584, 'IoU-17': 15.062900047868913, 'IoU-18': 13.964266822322882, 'IoU-19': 17.266410576768145, 'IoU-20': 19.240996947561403, 'IoU-21': 14.333918841284484, 'IoU-22': 17.863494215987746, 'IoU-23': 20.34471388330944, 'IoU-24': 14.69897671559256, 'IoU-25': 17.102792846682366, 'IoU-26': 19.903874841679514, 'IoU-27': 19.694310259491214, 'IoU-28': 18.96098135070286, 'IoU-29': 18.506212580357836, 'IoU-30': 19.016740736065508, 'IoU-31': 20.433104909916057, 'IoU-32': 18.986808329718237, 'IoU-33': 17.521376202064793, 'IoU-34': 20.471902705390267, 'IoU-35': 18.173073714816844, 'IoU-36': 22.74268736157177, 'IoU-37': 16.142585836599082, 'IoU-38': 17.69023716321552, 'IoU-39': 17.53019186017754, 'IoU-40': 18.512390788960026, 'IoU-41': 16.309008191967692, 'IoU-42': 17.44172940234334, 'IoU-43': 15.157486085434002, 'IoU-44': 18.81059710647149, 'IoU-45': 19.7883835729772, 'IoU-46': 16.930415375810117, 'IoU-47': 17.423398447648076, 'IoU-48': 18.901817363167158, 'IoU-49': 14.285084087659703, 'IoU-50': 18.610536832783033, 'IoU-51': 16.4151805610109, 'IoU-52': 17.47570823549901, 'IoU-53': 15.016730576264884, 'IoU-54': 16.297527211775922, 'IoU-55': 14.684015767594602, 'IoU-56': 14.62458583404534, 'IoU-57': 14.027023161624747, 'IoU-58': 13.117230990145561, 'IoU-59': 11.5729721559741, 'IoU-60': 10.1604828701929, 'IoU-61': 13.672686569726277, 'IoU-62': 12.657020836831459, 'IoU-63': 10.405677760595717, 'IoU-64': 14.885966996371325, 'IoU-65': 7.880976985797525, 'IoU-66': 12.692316671029308, 'IoU-67': 13.773159252651201, 'IoU-68': 9.977182049879238, 'IoU-69': 10.8196284335424, 'IoU-70': 9.749350738875137, 'IoU-71': 12.048398147982441, 'IoU-72': 10.78450876163109, 'IoU-73': 10.746653100219678, 'IoU-74': 10.863581300798245, 'IoU-75': 9.574093422131119, 'IoU-76': 8.626316830546003, 'IoU-77': 12.428497696804632, 'IoU-78': 11.458739451934504, 'IoU-79': 7.874049707407801, 'IoU-80': 10.247974622922996, 'IoU-81': 10.123289851281553, 'IoU-82': 7.582743758815289, 'IoU-83': 11.717805313158188, 'IoU-84': 12.265928332306549, 'IoU-85': 7.255737777390954, 'IoU-86': 11.407612354873196, 'IoU-87': 7.622375806428868, 'IoU-88': 9.15793804318746, 'IoU-89': 8.693169344950354, 'IoU-90': 11.239952868793207, 'IoU-91': 9.997589445575942, 'IoU-92': 5.658918002344473, 'IoU-93': 10.844984453697885, 'IoU-94': 8.521425287014726, 'IoU-95': 6.802262265704931, 'IoU-96': 11.20061417132935, 'IoU-97': 6.650337423830837, 'IoU-98': 5.652080020883267, 'IoU-99': 9.595343699659399, 'IoU-100': 7.86802508890366, 'IoU-101': 5.151013568525768, 'IoU-102': 6.097116262572265, 'IoU-103': 8.504064784803726, 'IoU-104': 7.280298073360357, 'IoU-105': 3.596420571069688, 'IoU-106': 6.995528789460377, 'IoU-107': 7.713386759611794, 'IoU-108': 6.85508051588592, 'IoU-109': 4.039124360729822, 'IoU-110': 7.13984161099235, 'IoU-111': 7.151867246412555, 'IoU-112': 4.794099520630016, 'IoU-113': 4.993532566775103, 'IoU-114': 4.965723717093041, 'IoU-115': 5.037401010283876, 'IoU-116': 3.193481489973559, 'IoU-117': 5.529277787469128, 'IoU-118': 6.019146985148924, 'IoU-119': 4.432773295122034, 'IoU-120': 5.510106317567574, 'IoU-121': 3.2981450447515464, 'IoU-122': 4.677740263924836, 'IoU-123': 3.7578513955276227, 'IoU-124': 3.231050579274656, 'IoU-125': 4.137897629429567, 'IoU-126': 3.2504143802242385, 'IoU-127': 3.681163293600061, 'IoU-128': 1.9905295049678629, 'IoU-129': 2.8937145099083255, 'IoU-130': 3.5530663591496445, 'IoU-131': 2.775369103348938, 'IoU-132': 2.9987882087272877, 'IoU-133': 4.207599368605608, 'IoU-134': 2.3087365107609417, 'IoU-135': 1.9662363162740917, 'IoU-136': 2.7650995762904804, 'IoU-137': 3.3436807437142715, 'IoU-138': 1.6619423693696604, 'IoU-139': 2.515537402243385, 'IoU-140': 1.962286475877736, 'IoU-141': 3.871887454509538, 'IoU-142': 1.8684803270782135, 'IoU-143': 2.948974043086252, 'IoU-144': 3.2391057824469653, 'IoU-145': 1.5328385140234433, 'IoU-146': 2.4154137656671186, 'IoU-147': 1.1578860802979638, 'IoU-148': 2.593288218262935, 'IoU-149': 1.716853162545175, 'IoU-150': 1.113189120298078, 'IoU-151': 1.6110364962374117, 'IoU-152': 1.0672529508579087, 'IoU-153': 1.8553540063189933, 'IoU-154': 1.0342997872693873, 'IoU-155': 1.4430129512420071, 'IoU-156': 1.2210920015323508, 'IoU-157': 1.5521442079975465, 'IoU-158': 1.3390131964541756, 'IoU-159': 0.6345994103946446, 'IoU-160': 0.9825562448889245, 'IoU-161': 1.4726617577941408, 'IoU-162': 0.7899924076528407, 'IoU-163': 0.252876214642023, 'IoU-164': 1.4456543341855093, 'IoU-165': 0.0, 'IoU-166': 1.5566274515799425, 'IoU-167': 0.8119177242591912, 'IoU-168': 1.1023136264507019, 'IoU-169': 0.9347926033400804, 'IoU-170': 1.5237049423078524, 'IoU-171': 0.5147644820823429, 'IoU-172': 0.5536786643133665, 'IoU-173': 1.0274881880444136, 'IoU-174': 0.2288922609100539, 'IoU-175': 1.7599742510467449, 'IoU-176': 1.7001070636716384, 'IoU-177': 2.0852965202496354, 'IoU-178': 1.102708552427748, 'IoU-179': 0.2900581345328958, 'IoU-180': 1.43211915018276, 'IoU-181': 0.38534992896992276, 'IoU-182': 0.0, 'IoU-183': 0.5801428959845295, 'IoU-184': 0.0, 'IoU-185': 0.02002071108042803, 'IoU-186': 0.0, 'IoU-187': 1.4489166161050293, 'IoU-188': 0.0, 'IoU-189': 0.2673134755319226, 'IoU-190': 0.0007859863867157821, 'IoU-191': 0.5069420203362348, 'mACC': 16.461207702417756, 'pACC': 34.06031733162006, 'ACC-0': nan, 'ACC-1': 98.57526476594323, 'ACC-2': 62.105653969493645, 'ACC-3': 59.20644897412137, 'ACC-4': 66.67256687008958, 'ACC-5': 49.29542344821927, 'ACC-6': 55.93020107375004, 'ACC-7': 37.0337756288947, 'ACC-8': 9.025258531569555, 'ACC-9': 11.796599573634001, 'ACC-10': 25.19310350846788, 'ACC-11': 33.46992330091574, 'ACC-12': 23.54267587774374, 'ACC-13': 39.24259298418931, 'ACC-14': 20.0198103660047, 'ACC-15': 33.79386636873064, 'ACC-16': 22.430014054907012, 'ACC-17': 29.399065060564794, 'ACC-18': 24.12337064905113, 'ACC-19': 29.633084548909594, 'ACC-20': 35.08382793008856, 'ACC-21': 25.659035343581944, 'ACC-22': 31.760251899312564, 'ACC-23': 39.192876620940645, 'ACC-24': 25.711928190225137, 'ACC-25': 33.868725662366614, 'ACC-26': 33.13879789230887, 'ACC-27': 34.6011824356234, 'ACC-28': 32.0966310318132, 'ACC-29': 28.924925075728076, 'ACC-30': 34.860461683599894, 'ACC-31': 32.52754045614298, 'ACC-32': 30.897324077166388, 'ACC-33': 27.86737044580655, 'ACC-34': 34.7266473428082, 'ACC-35': 28.53971659494407, 'ACC-36': 38.00709147639547, 'ACC-37': 23.197772307187346, 'ACC-38': 32.46868993270308, 'ACC-39': 28.57328886730893, 'ACC-40': 32.48386218684039, 'ACC-41': 27.124119058473017, 'ACC-42': 30.677782640925194, 'ACC-43': 26.336424873576963, 'ACC-44': 32.98680164261012, 'ACC-45': 35.03726354419366, 'ACC-46': 28.818825475667747, 'ACC-47': 30.676392701197035, 'ACC-48': 33.22151219901507, 'ACC-49': 21.40673048070562, 'ACC-50': 35.98370299219905, 'ACC-51': 28.466766296183227, 'ACC-52': 30.62519989882245, 'ACC-53': 26.579811242773726, 'ACC-54': 33.53988870186836, 'ACC-55': 25.187715342294908, 'ACC-56': 26.936648528125883, 'ACC-57': 22.13253043740689, 'ACC-58': 25.987880641692623, 'ACC-59': 18.35094632773199, 'ACC-60': 15.430586091168896, 'ACC-61': 26.128128138488826, 'ACC-62': 22.193481983816596, 'ACC-63': 18.19369801096999, 'ACC-64': 29.36698354876314, 'ACC-65': 11.568233041708067, 'ACC-66': 26.117340704185676, 'ACC-67': 25.729657749056607, 'ACC-68': 18.71209729241217, 'ACC-69': 18.323926000093845, 'ACC-70': 15.412221756721323, 'ACC-71': 20.78440625677061, 'ACC-72': 17.924187066293555, 'ACC-73': 22.295043819658098, 'ACC-74': 17.920627100220287, 'ACC-75': 16.187985878072244, 'ACC-76': 12.599706256018173, 'ACC-77': 24.323210574588476, 'ACC-78': 22.68650173821974, 'ACC-79': 11.204701467443176, 'ACC-80': 19.46720149026382, 'ACC-81': 16.316545837917733, 'ACC-82': 11.89550649202777, 'ACC-83': 24.37877285219195, 'ACC-84': 23.613235003736527, 'ACC-85': 12.155905758207412, 'ACC-86': 24.664164967475735, 'ACC-87': 10.845484780593702, 'ACC-88': 16.800277410290974, 'ACC-89': 15.358427763428029, 'ACC-90': 21.824866449484244, 'ACC-91': 19.668198979915367, 'ACC-92': 9.851375042855219, 'ACC-93': 21.890490314324982, 'ACC-94': 15.695163130609096, 'ACC-95': 12.24678832369542, 'ACC-96': 23.465291249484377, 'ACC-97': 11.126160344704493, 'ACC-98': 8.599546613147496, 'ACC-99': 19.485836623838036, 'ACC-100': 19.462602593202593, 'ACC-101': 7.295390416647322, 'ACC-102': 9.284141368373358, 'ACC-103': 16.208486823462735, 'ACC-104': 13.50151250584098, 'ACC-105': 5.124489503520705, 'ACC-106': 15.881057401567103, 'ACC-107': 19.54862516263506, 'ACC-108': 12.892591172904691, 'ACC-109': 5.525207552839686, 'ACC-110': 14.899783780951829, 'ACC-111': 15.304017918562304, 'ACC-112': 8.004463013641953, 'ACC-113': 8.88787167896855, 'ACC-114': 10.15963209872314, 'ACC-115': 9.45454272097184, 'ACC-116': 4.291642233579786, 'ACC-117': 13.551542319663563, 'ACC-118': 11.87585994356125, 'ACC-119': 9.024990810782759, 'ACC-120': 11.868009361262505, 'ACC-121': 6.258280807784425, 'ACC-122': 8.848585203845754, 'ACC-123': 6.94136605949841, 'ACC-124': 5.195618136690567, 'ACC-125': 8.502245576556811, 'ACC-126': 4.886941137897431, 'ACC-127': 6.6932454279546585, 'ACC-128': 2.546619001794141, 'ACC-129': 5.1837428046278795, 'ACC-130': 6.406281036933467, 'ACC-131': 4.4817848985155315, 'ACC-132': 5.2128426040420495, 'ACC-133': 8.549946586648703, 'ACC-134': 7.291408875484705, 'ACC-135': 2.848683085342911, 'ACC-136': 5.083657519567648, 'ACC-137': 9.270907032957137, 'ACC-138': 2.6532001167601593, 'ACC-139': 3.740429414807752, 'ACC-140': 2.9339707497428305, 'ACC-141': 7.9999798027061, 'ACC-142': 3.1194844641464514, 'ACC-143': 7.6703751528728406, 'ACC-144': 8.85, 'ACC-145': 2.6724211185402176, 'ACC-146': 3.8478444632290785, 'ACC-147': 1.621102654359447, 'ACC-148': 8.530765143514905, 'ACC-149': 2.7234402460679727, 'ACC-150': 1.2854788729311482, 'ACC-151': 2.6629492381296473, 'ACC-152': 1.369255344257873, 'ACC-153': 3.186092636361971, 'ACC-154': 1.5059876688893663, 'ACC-155': 6.400509782750707, 'ACC-156': 1.8790358548740522, 'ACC-157': 4.091690450816827, 'ACC-158': 1.863662601562908, 'ACC-159': 0.8837864735587078, 'ACC-160': 1.1296543455639507, 'ACC-161': 3.32624685110243, 'ACC-162': 1.1501007137735382, 'ACC-163': 0.26812725190968056, 'ACC-164': 1.856772757397125, 'ACC-165': 0.0, 'ACC-166': 4.1023746967623875, 'ACC-167': 1.3323674971815302, 'ACC-168': 2.2492791335536957, 'ACC-169': 1.3735911829062648, 'ACC-170': 3.250422363246589, 'ACC-171': 0.682207678418248, 'ACC-172': 0.6698892469533652, 'ACC-173': 1.8129616598871885, 'ACC-174': 0.28314143464540215, 'ACC-175': 3.518162102799332, 'ACC-176': 5.971021012305549, 'ACC-177': 5.153828692912862, 'ACC-178': 2.928660971082322, 'ACC-179': 0.3351721815204887, 'ACC-180': 3.6421822615056385, 'ACC-181': 0.48509999265717785, 'ACC-182': 0.0, 'ACC-183': 0.8489679835950945, 'ACC-184': 0.0, 'ACC-185': 0.020093399664648088, 'ACC-186': 0.0, 'ACC-187': 1.8809118254629795, 'ACC-188': 0.0, 'ACC-189': 0.28547883169328486, 'ACC-190': 0.0007860234549398954, 'ACC-191': 0.587536704730832})])
[01/17 21:22:23] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 21:22:23] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 21:22:23] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 21:22:23] d2.evaluation.testing INFO: copypaste: 3.7405,9.7023,23.5681,16.4612,34.0603
[01/17 21:22:23] d2.utils.events INFO:  eta: 13:57:17  iter: 5999  total_loss: 47.17  loss_ce: 0.4227  loss_mask: 0.5083  loss_dice: 3.762  loss_ce_0: 0.6693  loss_mask_0: 0.4915  loss_dice_0: 3.89  loss_ce_1: 0.4467  loss_mask_1: 0.5047  loss_dice_1: 3.791  loss_ce_2: 0.4361  loss_mask_2: 0.5049  loss_dice_2: 3.774  loss_ce_3: 0.4265  loss_mask_3: 0.5032  loss_dice_3: 3.765  loss_ce_4: 0.4277  loss_mask_4: 0.5054  loss_dice_4: 3.762  loss_ce_5: 0.4096  loss_mask_5: 0.5093  loss_dice_5: 3.759  loss_ce_6: 0.4255  loss_mask_6: 0.509  loss_dice_6: 3.759  loss_ce_7: 0.4042  loss_mask_7: 0.5093  loss_dice_7: 3.766  loss_ce_8: 0.4133  loss_mask_8: 0.5104  loss_dice_8: 3.762  time: 1.5059  data_time: 0.0659  lr: 8.6395e-06  max_mem: 21460M
[01/17 21:22:53] d2.utils.events INFO:  eta: 13:56:45  iter: 6019  total_loss: 47.2  loss_ce: 0.4251  loss_mask: 0.5081  loss_dice: 3.724  loss_ce_0: 0.6167  loss_mask_0: 0.4863  loss_dice_0: 3.854  loss_ce_1: 0.3977  loss_mask_1: 0.5039  loss_dice_1: 3.759  loss_ce_2: 0.4283  loss_mask_2: 0.5059  loss_dice_2: 3.736  loss_ce_3: 0.419  loss_mask_3: 0.5029  loss_dice_3: 3.726  loss_ce_4: 0.4118  loss_mask_4: 0.506  loss_dice_4: 3.733  loss_ce_5: 0.4282  loss_mask_5: 0.5085  loss_dice_5: 3.736  loss_ce_6: 0.4158  loss_mask_6: 0.51  loss_dice_6: 3.735  loss_ce_7: 0.4072  loss_mask_7: 0.5083  loss_dice_7: 3.739  loss_ce_8: 0.4136  loss_mask_8: 0.5084  loss_dice_8: 3.728  time: 1.5058  data_time: 0.0767  lr: 8.6349e-06  max_mem: 21460M
[01/17 21:23:23] d2.utils.events INFO:  eta: 13:56:18  iter: 6039  total_loss: 47.31  loss_ce: 0.432  loss_mask: 0.5151  loss_dice: 3.745  loss_ce_0: 0.654  loss_mask_0: 0.4895  loss_dice_0: 3.87  loss_ce_1: 0.4535  loss_mask_1: 0.5029  loss_dice_1: 3.795  loss_ce_2: 0.4576  loss_mask_2: 0.508  loss_dice_2: 3.768  loss_ce_3: 0.4399  loss_mask_3: 0.5073  loss_dice_3: 3.749  loss_ce_4: 0.4264  loss_mask_4: 0.5096  loss_dice_4: 3.75  loss_ce_5: 0.4237  loss_mask_5: 0.511  loss_dice_5: 3.756  loss_ce_6: 0.4357  loss_mask_6: 0.5121  loss_dice_6: 3.741  loss_ce_7: 0.4386  loss_mask_7: 0.5106  loss_dice_7: 3.75  loss_ce_8: 0.4271  loss_mask_8: 0.5098  loss_dice_8: 3.75  time: 1.5058  data_time: 0.0726  lr: 8.6304e-06  max_mem: 21460M
[01/17 21:23:52] d2.utils.events INFO:  eta: 13:55:47  iter: 6059  total_loss: 47.44  loss_ce: 0.4237  loss_mask: 0.5183  loss_dice: 3.757  loss_ce_0: 0.6323  loss_mask_0: 0.5013  loss_dice_0: 3.877  loss_ce_1: 0.4069  loss_mask_1: 0.5301  loss_dice_1: 3.787  loss_ce_2: 0.4518  loss_mask_2: 0.5271  loss_dice_2: 3.767  loss_ce_3: 0.4247  loss_mask_3: 0.5248  loss_dice_3: 3.758  loss_ce_4: 0.4201  loss_mask_4: 0.5196  loss_dice_4: 3.764  loss_ce_5: 0.4056  loss_mask_5: 0.5206  loss_dice_5: 3.765  loss_ce_6: 0.4266  loss_mask_6: 0.5206  loss_dice_6: 3.757  loss_ce_7: 0.4177  loss_mask_7: 0.5208  loss_dice_7: 3.765  loss_ce_8: 0.4287  loss_mask_8: 0.5196  loss_dice_8: 3.764  time: 1.5057  data_time: 0.0737  lr: 8.6258e-06  max_mem: 21460M
[01/17 21:24:22] d2.utils.events INFO:  eta: 13:55:06  iter: 6079  total_loss: 46.9  loss_ce: 0.417  loss_mask: 0.5076  loss_dice: 3.743  loss_ce_0: 0.6395  loss_mask_0: 0.4866  loss_dice_0: 3.863  loss_ce_1: 0.4105  loss_mask_1: 0.501  loss_dice_1: 3.784  loss_ce_2: 0.4359  loss_mask_2: 0.5023  loss_dice_2: 3.762  loss_ce_3: 0.4144  loss_mask_3: 0.5032  loss_dice_3: 3.752  loss_ce_4: 0.4106  loss_mask_4: 0.5069  loss_dice_4: 3.745  loss_ce_5: 0.4276  loss_mask_5: 0.506  loss_dice_5: 3.753  loss_ce_6: 0.4252  loss_mask_6: 0.5091  loss_dice_6: 3.745  loss_ce_7: 0.4157  loss_mask_7: 0.5093  loss_dice_7: 3.752  loss_ce_8: 0.4055  loss_mask_8: 0.5097  loss_dice_8: 3.75  time: 1.5056  data_time: 0.0680  lr: 8.6212e-06  max_mem: 21460M
[01/17 21:24:51] d2.utils.events INFO:  eta: 13:54:44  iter: 6099  total_loss: 46.63  loss_ce: 0.4196  loss_mask: 0.5146  loss_dice: 3.718  loss_ce_0: 0.6512  loss_mask_0: 0.4906  loss_dice_0: 3.847  loss_ce_1: 0.4412  loss_mask_1: 0.5108  loss_dice_1: 3.752  loss_ce_2: 0.4419  loss_mask_2: 0.5115  loss_dice_2: 3.737  loss_ce_3: 0.4058  loss_mask_3: 0.5123  loss_dice_3: 3.721  loss_ce_4: 0.4251  loss_mask_4: 0.5157  loss_dice_4: 3.709  loss_ce_5: 0.4055  loss_mask_5: 0.5157  loss_dice_5: 3.72  loss_ce_6: 0.4019  loss_mask_6: 0.5186  loss_dice_6: 3.717  loss_ce_7: 0.4058  loss_mask_7: 0.517  loss_dice_7: 3.723  loss_ce_8: 0.4131  loss_mask_8: 0.5161  loss_dice_8: 3.717  time: 1.5055  data_time: 0.0747  lr: 8.6166e-06  max_mem: 21460M
[01/17 21:25:21] d2.utils.events INFO:  eta: 13:54:15  iter: 6119  total_loss: 46.92  loss_ce: 0.3934  loss_mask: 0.514  loss_dice: 3.742  loss_ce_0: 0.6221  loss_mask_0: 0.4897  loss_dice_0: 3.885  loss_ce_1: 0.3881  loss_mask_1: 0.5105  loss_dice_1: 3.794  loss_ce_2: 0.4031  loss_mask_2: 0.5126  loss_dice_2: 3.761  loss_ce_3: 0.4044  loss_mask_3: 0.5154  loss_dice_3: 3.754  loss_ce_4: 0.401  loss_mask_4: 0.513  loss_dice_4: 3.749  loss_ce_5: 0.3872  loss_mask_5: 0.5149  loss_dice_5: 3.756  loss_ce_6: 0.4097  loss_mask_6: 0.5128  loss_dice_6: 3.752  loss_ce_7: 0.3969  loss_mask_7: 0.5155  loss_dice_7: 3.749  loss_ce_8: 0.4075  loss_mask_8: 0.5132  loss_dice_8: 3.74  time: 1.5054  data_time: 0.0670  lr: 8.6121e-06  max_mem: 21460M
[01/17 21:25:50] d2.utils.events INFO:  eta: 13:53:45  iter: 6139  total_loss: 46.65  loss_ce: 0.4079  loss_mask: 0.5153  loss_dice: 3.716  loss_ce_0: 0.6719  loss_mask_0: 0.491  loss_dice_0: 3.842  loss_ce_1: 0.417  loss_mask_1: 0.509  loss_dice_1: 3.755  loss_ce_2: 0.4367  loss_mask_2: 0.5108  loss_dice_2: 3.736  loss_ce_3: 0.4085  loss_mask_3: 0.5119  loss_dice_3: 3.722  loss_ce_4: 0.4113  loss_mask_4: 0.5122  loss_dice_4: 3.726  loss_ce_5: 0.4186  loss_mask_5: 0.5127  loss_dice_5: 3.717  loss_ce_6: 0.4018  loss_mask_6: 0.5107  loss_dice_6: 3.719  loss_ce_7: 0.4253  loss_mask_7: 0.5113  loss_dice_7: 3.718  loss_ce_8: 0.404  loss_mask_8: 0.5154  loss_dice_8: 3.727  time: 1.5053  data_time: 0.0723  lr: 8.6075e-06  max_mem: 21460M
[01/17 21:26:20] d2.utils.events INFO:  eta: 13:53:02  iter: 6159  total_loss: 46.81  loss_ce: 0.4221  loss_mask: 0.5116  loss_dice: 3.733  loss_ce_0: 0.6767  loss_mask_0: 0.4891  loss_dice_0: 3.878  loss_ce_1: 0.4183  loss_mask_1: 0.5081  loss_dice_1: 3.774  loss_ce_2: 0.4265  loss_mask_2: 0.5067  loss_dice_2: 3.748  loss_ce_3: 0.4255  loss_mask_3: 0.5093  loss_dice_3: 3.732  loss_ce_4: 0.419  loss_mask_4: 0.5106  loss_dice_4: 3.729  loss_ce_5: 0.4193  loss_mask_5: 0.5123  loss_dice_5: 3.737  loss_ce_6: 0.4216  loss_mask_6: 0.5129  loss_dice_6: 3.728  loss_ce_7: 0.4036  loss_mask_7: 0.5103  loss_dice_7: 3.733  loss_ce_8: 0.4273  loss_mask_8: 0.5105  loss_dice_8: 3.73  time: 1.5052  data_time: 0.0750  lr: 8.6029e-06  max_mem: 21460M
[01/17 21:26:49] d2.utils.events INFO:  eta: 13:51:52  iter: 6179  total_loss: 46.77  loss_ce: 0.3994  loss_mask: 0.5145  loss_dice: 3.69  loss_ce_0: 0.6416  loss_mask_0: 0.4967  loss_dice_0: 3.817  loss_ce_1: 0.4189  loss_mask_1: 0.5163  loss_dice_1: 3.719  loss_ce_2: 0.4268  loss_mask_2: 0.5154  loss_dice_2: 3.698  loss_ce_3: 0.4286  loss_mask_3: 0.5141  loss_dice_3: 3.692  loss_ce_4: 0.4036  loss_mask_4: 0.5156  loss_dice_4: 3.682  loss_ce_5: 0.4022  loss_mask_5: 0.5117  loss_dice_5: 3.688  loss_ce_6: 0.4012  loss_mask_6: 0.5131  loss_dice_6: 3.698  loss_ce_7: 0.4019  loss_mask_7: 0.5158  loss_dice_7: 3.694  loss_ce_8: 0.4155  loss_mask_8: 0.5135  loss_dice_8: 3.695  time: 1.5050  data_time: 0.0603  lr: 8.5983e-06  max_mem: 21460M
[01/17 21:27:19] d2.utils.events INFO:  eta: 13:51:20  iter: 6199  total_loss: 46.58  loss_ce: 0.4066  loss_mask: 0.503  loss_dice: 3.717  loss_ce_0: 0.6416  loss_mask_0: 0.4788  loss_dice_0: 3.852  loss_ce_1: 0.4017  loss_mask_1: 0.5014  loss_dice_1: 3.756  loss_ce_2: 0.4252  loss_mask_2: 0.5013  loss_dice_2: 3.728  loss_ce_3: 0.4282  loss_mask_3: 0.5007  loss_dice_3: 3.712  loss_ce_4: 0.4183  loss_mask_4: 0.4997  loss_dice_4: 3.712  loss_ce_5: 0.406  loss_mask_5: 0.502  loss_dice_5: 3.717  loss_ce_6: 0.4121  loss_mask_6: 0.5017  loss_dice_6: 3.705  loss_ce_7: 0.397  loss_mask_7: 0.5012  loss_dice_7: 3.712  loss_ce_8: 0.3934  loss_mask_8: 0.5036  loss_dice_8: 3.706  time: 1.5050  data_time: 0.0856  lr: 8.5937e-06  max_mem: 21460M
[01/17 21:27:49] d2.utils.events INFO:  eta: 13:50:33  iter: 6219  total_loss: 46.99  loss_ce: 0.4449  loss_mask: 0.5034  loss_dice: 3.709  loss_ce_0: 0.6654  loss_mask_0: 0.4879  loss_dice_0: 3.835  loss_ce_1: 0.4552  loss_mask_1: 0.5065  loss_dice_1: 3.751  loss_ce_2: 0.4564  loss_mask_2: 0.5079  loss_dice_2: 3.727  loss_ce_3: 0.4355  loss_mask_3: 0.5066  loss_dice_3: 3.714  loss_ce_4: 0.4289  loss_mask_4: 0.5068  loss_dice_4: 3.707  loss_ce_5: 0.4177  loss_mask_5: 0.5043  loss_dice_5: 3.709  loss_ce_6: 0.4289  loss_mask_6: 0.5028  loss_dice_6: 3.715  loss_ce_7: 0.4099  loss_mask_7: 0.5034  loss_dice_7: 3.714  loss_ce_8: 0.4188  loss_mask_8: 0.5021  loss_dice_8: 3.721  time: 1.5049  data_time: 0.0727  lr: 8.5892e-06  max_mem: 21460M
[01/17 21:28:18] d2.utils.events INFO:  eta: 13:50:02  iter: 6239  total_loss: 47.23  loss_ce: 0.4573  loss_mask: 0.5107  loss_dice: 3.719  loss_ce_0: 0.6566  loss_mask_0: 0.4874  loss_dice_0: 3.855  loss_ce_1: 0.4623  loss_mask_1: 0.5022  loss_dice_1: 3.764  loss_ce_2: 0.4718  loss_mask_2: 0.5088  loss_dice_2: 3.717  loss_ce_3: 0.4579  loss_mask_3: 0.5113  loss_dice_3: 3.706  loss_ce_4: 0.4622  loss_mask_4: 0.5099  loss_dice_4: 3.71  loss_ce_5: 0.4315  loss_mask_5: 0.5101  loss_dice_5: 3.726  loss_ce_6: 0.4414  loss_mask_6: 0.5132  loss_dice_6: 3.719  loss_ce_7: 0.4227  loss_mask_7: 0.5092  loss_dice_7: 3.722  loss_ce_8: 0.4386  loss_mask_8: 0.5105  loss_dice_8: 3.721  time: 1.5048  data_time: 0.0671  lr: 8.5846e-06  max_mem: 21460M
[01/17 21:28:48] d2.utils.events INFO:  eta: 13:49:30  iter: 6259  total_loss: 46.82  loss_ce: 0.411  loss_mask: 0.5194  loss_dice: 3.701  loss_ce_0: 0.6622  loss_mask_0: 0.4955  loss_dice_0: 3.84  loss_ce_1: 0.4301  loss_mask_1: 0.5175  loss_dice_1: 3.735  loss_ce_2: 0.4253  loss_mask_2: 0.5171  loss_dice_2: 3.711  loss_ce_3: 0.4223  loss_mask_3: 0.5187  loss_dice_3: 3.7  loss_ce_4: 0.4168  loss_mask_4: 0.5209  loss_dice_4: 3.7  loss_ce_5: 0.4073  loss_mask_5: 0.5183  loss_dice_5: 3.707  loss_ce_6: 0.4206  loss_mask_6: 0.5181  loss_dice_6: 3.694  loss_ce_7: 0.4254  loss_mask_7: 0.5182  loss_dice_7: 3.704  loss_ce_8: 0.4016  loss_mask_8: 0.5178  loss_dice_8: 3.702  time: 1.5048  data_time: 0.0725  lr: 8.58e-06  max_mem: 21460M
[01/17 21:29:18] d2.utils.events INFO:  eta: 13:48:52  iter: 6279  total_loss: 47.3  loss_ce: 0.4348  loss_mask: 0.5204  loss_dice: 3.729  loss_ce_0: 0.6154  loss_mask_0: 0.4931  loss_dice_0: 3.843  loss_ce_1: 0.3991  loss_mask_1: 0.5238  loss_dice_1: 3.765  loss_ce_2: 0.4321  loss_mask_2: 0.5235  loss_dice_2: 3.742  loss_ce_3: 0.4141  loss_mask_3: 0.5178  loss_dice_3: 3.741  loss_ce_4: 0.4101  loss_mask_4: 0.5204  loss_dice_4: 3.73  loss_ce_5: 0.4048  loss_mask_5: 0.5241  loss_dice_5: 3.728  loss_ce_6: 0.4175  loss_mask_6: 0.5208  loss_dice_6: 3.736  loss_ce_7: 0.4089  loss_mask_7: 0.5211  loss_dice_7: 3.741  loss_ce_8: 0.4289  loss_mask_8: 0.5187  loss_dice_8: 3.736  time: 1.5047  data_time: 0.0716  lr: 8.5754e-06  max_mem: 21460M
[01/17 21:29:47] d2.utils.events INFO:  eta: 13:48:07  iter: 6299  total_loss: 47.2  loss_ce: 0.4144  loss_mask: 0.5184  loss_dice: 3.74  loss_ce_0: 0.6355  loss_mask_0: 0.4967  loss_dice_0: 3.866  loss_ce_1: 0.4238  loss_mask_1: 0.5201  loss_dice_1: 3.764  loss_ce_2: 0.4566  loss_mask_2: 0.5163  loss_dice_2: 3.752  loss_ce_3: 0.4207  loss_mask_3: 0.5175  loss_dice_3: 3.741  loss_ce_4: 0.4282  loss_mask_4: 0.5174  loss_dice_4: 3.742  loss_ce_5: 0.4178  loss_mask_5: 0.5168  loss_dice_5: 3.746  loss_ce_6: 0.4328  loss_mask_6: 0.5175  loss_dice_6: 3.741  loss_ce_7: 0.418  loss_mask_7: 0.5145  loss_dice_7: 3.748  loss_ce_8: 0.4241  loss_mask_8: 0.5161  loss_dice_8: 3.751  time: 1.5046  data_time: 0.0808  lr: 8.5709e-06  max_mem: 21460M
[01/17 21:30:16] d2.utils.events INFO:  eta: 13:47:32  iter: 6319  total_loss: 47.07  loss_ce: 0.4189  loss_mask: 0.5142  loss_dice: 3.731  loss_ce_0: 0.6568  loss_mask_0: 0.5055  loss_dice_0: 3.839  loss_ce_1: 0.4154  loss_mask_1: 0.5221  loss_dice_1: 3.76  loss_ce_2: 0.4341  loss_mask_2: 0.5182  loss_dice_2: 3.74  loss_ce_3: 0.4394  loss_mask_3: 0.515  loss_dice_3: 3.723  loss_ce_4: 0.4067  loss_mask_4: 0.518  loss_dice_4: 3.724  loss_ce_5: 0.4231  loss_mask_5: 0.5166  loss_dice_5: 3.727  loss_ce_6: 0.4238  loss_mask_6: 0.5192  loss_dice_6: 3.724  loss_ce_7: 0.4204  loss_mask_7: 0.5179  loss_dice_7: 3.721  loss_ce_8: 0.4172  loss_mask_8: 0.5153  loss_dice_8: 3.727  time: 1.5044  data_time: 0.0656  lr: 8.5663e-06  max_mem: 21460M
[01/17 21:30:46] d2.utils.events INFO:  eta: 13:47:09  iter: 6339  total_loss: 47.33  loss_ce: 0.4188  loss_mask: 0.4983  loss_dice: 3.733  loss_ce_0: 0.6367  loss_mask_0: 0.4824  loss_dice_0: 3.873  loss_ce_1: 0.4152  loss_mask_1: 0.5008  loss_dice_1: 3.772  loss_ce_2: 0.4298  loss_mask_2: 0.4941  loss_dice_2: 3.745  loss_ce_3: 0.4189  loss_mask_3: 0.4982  loss_dice_3: 3.738  loss_ce_4: 0.4137  loss_mask_4: 0.4991  loss_dice_4: 3.737  loss_ce_5: 0.413  loss_mask_5: 0.5002  loss_dice_5: 3.736  loss_ce_6: 0.4202  loss_mask_6: 0.4998  loss_dice_6: 3.74  loss_ce_7: 0.3934  loss_mask_7: 0.4977  loss_dice_7: 3.744  loss_ce_8: 0.404  loss_mask_8: 0.4982  loss_dice_8: 3.741  time: 1.5044  data_time: 0.0812  lr: 8.5617e-06  max_mem: 21460M
[01/17 21:31:15] d2.utils.events INFO:  eta: 13:46:44  iter: 6359  total_loss: 46.33  loss_ce: 0.3921  loss_mask: 0.5069  loss_dice: 3.684  loss_ce_0: 0.6593  loss_mask_0: 0.4851  loss_dice_0: 3.806  loss_ce_1: 0.395  loss_mask_1: 0.5057  loss_dice_1: 3.712  loss_ce_2: 0.4034  loss_mask_2: 0.5049  loss_dice_2: 3.691  loss_ce_3: 0.4003  loss_mask_3: 0.5064  loss_dice_3: 3.694  loss_ce_4: 0.3931  loss_mask_4: 0.506  loss_dice_4: 3.685  loss_ce_5: 0.3866  loss_mask_5: 0.5048  loss_dice_5: 3.699  loss_ce_6: 0.3841  loss_mask_6: 0.5066  loss_dice_6: 3.685  loss_ce_7: 0.3796  loss_mask_7: 0.5059  loss_dice_7: 3.677  loss_ce_8: 0.3865  loss_mask_8: 0.5054  loss_dice_8: 3.682  time: 1.5042  data_time: 0.0702  lr: 8.5571e-06  max_mem: 21460M
[01/17 21:31:45] d2.utils.events INFO:  eta: 13:46:07  iter: 6379  total_loss: 46.53  loss_ce: 0.414  loss_mask: 0.5185  loss_dice: 3.683  loss_ce_0: 0.6473  loss_mask_0: 0.4937  loss_dice_0: 3.805  loss_ce_1: 0.4176  loss_mask_1: 0.512  loss_dice_1: 3.702  loss_ce_2: 0.4181  loss_mask_2: 0.5127  loss_dice_2: 3.699  loss_ce_3: 0.4186  loss_mask_3: 0.5119  loss_dice_3: 3.677  loss_ce_4: 0.4161  loss_mask_4: 0.5156  loss_dice_4: 3.679  loss_ce_5: 0.4172  loss_mask_5: 0.5174  loss_dice_5: 3.679  loss_ce_6: 0.3985  loss_mask_6: 0.5191  loss_dice_6: 3.671  loss_ce_7: 0.4149  loss_mask_7: 0.5171  loss_dice_7: 3.689  loss_ce_8: 0.4049  loss_mask_8: 0.5198  loss_dice_8: 3.685  time: 1.5041  data_time: 0.0767  lr: 8.5525e-06  max_mem: 21460M
[01/17 21:32:15] d2.utils.events INFO:  eta: 13:46:03  iter: 6399  total_loss: 47.12  loss_ce: 0.4348  loss_mask: 0.4926  loss_dice: 3.727  loss_ce_0: 0.6841  loss_mask_0: 0.4721  loss_dice_0: 3.861  loss_ce_1: 0.4285  loss_mask_1: 0.4845  loss_dice_1: 3.78  loss_ce_2: 0.4573  loss_mask_2: 0.4865  loss_dice_2: 3.752  loss_ce_3: 0.4521  loss_mask_3: 0.4891  loss_dice_3: 3.735  loss_ce_4: 0.4513  loss_mask_4: 0.4892  loss_dice_4: 3.731  loss_ce_5: 0.4349  loss_mask_5: 0.4924  loss_dice_5: 3.738  loss_ce_6: 0.4358  loss_mask_6: 0.4934  loss_dice_6: 3.731  loss_ce_7: 0.4367  loss_mask_7: 0.4942  loss_dice_7: 3.732  loss_ce_8: 0.4307  loss_mask_8: 0.4949  loss_dice_8: 3.733  time: 1.5041  data_time: 0.0701  lr: 8.548e-06  max_mem: 21460M
[01/17 21:32:44] d2.utils.events INFO:  eta: 13:45:15  iter: 6419  total_loss: 46.62  loss_ce: 0.4214  loss_mask: 0.5126  loss_dice: 3.701  loss_ce_0: 0.633  loss_mask_0: 0.4844  loss_dice_0: 3.821  loss_ce_1: 0.3954  loss_mask_1: 0.5051  loss_dice_1: 3.738  loss_ce_2: 0.429  loss_mask_2: 0.5069  loss_dice_2: 3.717  loss_ce_3: 0.4245  loss_mask_3: 0.5102  loss_dice_3: 3.704  loss_ce_4: 0.4127  loss_mask_4: 0.5086  loss_dice_4: 3.705  loss_ce_5: 0.4096  loss_mask_5: 0.5122  loss_dice_5: 3.706  loss_ce_6: 0.4035  loss_mask_6: 0.5095  loss_dice_6: 3.716  loss_ce_7: 0.4054  loss_mask_7: 0.5103  loss_dice_7: 3.713  loss_ce_8: 0.4066  loss_mask_8: 0.5112  loss_dice_8: 3.708  time: 1.5040  data_time: 0.0713  lr: 8.5434e-06  max_mem: 21460M
[01/17 21:33:14] d2.utils.events INFO:  eta: 13:44:39  iter: 6439  total_loss: 47.09  loss_ce: 0.4549  loss_mask: 0.5132  loss_dice: 3.711  loss_ce_0: 0.6945  loss_mask_0: 0.4903  loss_dice_0: 3.84  loss_ce_1: 0.4509  loss_mask_1: 0.5149  loss_dice_1: 3.751  loss_ce_2: 0.4698  loss_mask_2: 0.5131  loss_dice_2: 3.718  loss_ce_3: 0.4707  loss_mask_3: 0.5122  loss_dice_3: 3.706  loss_ce_4: 0.4703  loss_mask_4: 0.5116  loss_dice_4: 3.713  loss_ce_5: 0.4639  loss_mask_5: 0.5127  loss_dice_5: 3.705  loss_ce_6: 0.4491  loss_mask_6: 0.513  loss_dice_6: 3.7  loss_ce_7: 0.4528  loss_mask_7: 0.5139  loss_dice_7: 3.708  loss_ce_8: 0.4552  loss_mask_8: 0.5137  loss_dice_8: 3.704  time: 1.5039  data_time: 0.0723  lr: 8.5388e-06  max_mem: 21460M
[01/17 21:33:43] d2.utils.events INFO:  eta: 13:43:55  iter: 6459  total_loss: 46.57  loss_ce: 0.4158  loss_mask: 0.5102  loss_dice: 3.681  loss_ce_0: 0.6118  loss_mask_0: 0.4848  loss_dice_0: 3.835  loss_ce_1: 0.3952  loss_mask_1: 0.5081  loss_dice_1: 3.719  loss_ce_2: 0.427  loss_mask_2: 0.5083  loss_dice_2: 3.701  loss_ce_3: 0.4  loss_mask_3: 0.5084  loss_dice_3: 3.685  loss_ce_4: 0.4068  loss_mask_4: 0.5071  loss_dice_4: 3.685  loss_ce_5: 0.4023  loss_mask_5: 0.5081  loss_dice_5: 3.689  loss_ce_6: 0.4071  loss_mask_6: 0.5092  loss_dice_6: 3.686  loss_ce_7: 0.4085  loss_mask_7: 0.5106  loss_dice_7: 3.68  loss_ce_8: 0.4215  loss_mask_8: 0.5089  loss_dice_8: 3.68  time: 1.5038  data_time: 0.0761  lr: 8.5342e-06  max_mem: 21460M
[01/17 21:34:13] d2.utils.events INFO:  eta: 13:43:42  iter: 6479  total_loss: 46.76  loss_ce: 0.423  loss_mask: 0.5069  loss_dice: 3.698  loss_ce_0: 0.6291  loss_mask_0: 0.4893  loss_dice_0: 3.826  loss_ce_1: 0.3977  loss_mask_1: 0.5031  loss_dice_1: 3.743  loss_ce_2: 0.4075  loss_mask_2: 0.5047  loss_dice_2: 3.726  loss_ce_3: 0.4203  loss_mask_3: 0.5056  loss_dice_3: 3.703  loss_ce_4: 0.4105  loss_mask_4: 0.5067  loss_dice_4: 3.7  loss_ce_5: 0.3974  loss_mask_5: 0.5056  loss_dice_5: 3.704  loss_ce_6: 0.4178  loss_mask_6: 0.5054  loss_dice_6: 3.702  loss_ce_7: 0.4054  loss_mask_7: 0.5062  loss_dice_7: 3.711  loss_ce_8: 0.402  loss_mask_8: 0.5099  loss_dice_8: 3.706  time: 1.5037  data_time: 0.0733  lr: 8.5297e-06  max_mem: 21460M
[01/17 21:34:43] d2.utils.events INFO:  eta: 13:43:02  iter: 6499  total_loss: 47.17  loss_ce: 0.4235  loss_mask: 0.4994  loss_dice: 3.717  loss_ce_0: 0.6569  loss_mask_0: 0.485  loss_dice_0: 3.85  loss_ce_1: 0.4009  loss_mask_1: 0.5054  loss_dice_1: 3.75  loss_ce_2: 0.429  loss_mask_2: 0.5083  loss_dice_2: 3.726  loss_ce_3: 0.4302  loss_mask_3: 0.5044  loss_dice_3: 3.716  loss_ce_4: 0.4318  loss_mask_4: 0.5038  loss_dice_4: 3.718  loss_ce_5: 0.4161  loss_mask_5: 0.503  loss_dice_5: 3.719  loss_ce_6: 0.4126  loss_mask_6: 0.5027  loss_dice_6: 3.712  loss_ce_7: 0.4246  loss_mask_7: 0.5034  loss_dice_7: 3.726  loss_ce_8: 0.415  loss_mask_8: 0.503  loss_dice_8: 3.723  time: 1.5037  data_time: 0.0842  lr: 8.5251e-06  max_mem: 21460M
[01/17 21:35:12] d2.utils.events INFO:  eta: 13:42:04  iter: 6519  total_loss: 46.93  loss_ce: 0.4039  loss_mask: 0.5082  loss_dice: 3.725  loss_ce_0: 0.6371  loss_mask_0: 0.4855  loss_dice_0: 3.85  loss_ce_1: 0.4021  loss_mask_1: 0.5066  loss_dice_1: 3.761  loss_ce_2: 0.4321  loss_mask_2: 0.5053  loss_dice_2: 3.727  loss_ce_3: 0.4329  loss_mask_3: 0.5081  loss_dice_3: 3.728  loss_ce_4: 0.4235  loss_mask_4: 0.5077  loss_dice_4: 3.718  loss_ce_5: 0.3935  loss_mask_5: 0.5089  loss_dice_5: 3.718  loss_ce_6: 0.3953  loss_mask_6: 0.5072  loss_dice_6: 3.711  loss_ce_7: 0.3995  loss_mask_7: 0.508  loss_dice_7: 3.724  loss_ce_8: 0.4035  loss_mask_8: 0.5085  loss_dice_8: 3.716  time: 1.5036  data_time: 0.0760  lr: 8.5205e-06  max_mem: 21460M
[01/17 21:35:42] d2.utils.events INFO:  eta: 13:41:24  iter: 6539  total_loss: 46.44  loss_ce: 0.3933  loss_mask: 0.5017  loss_dice: 3.705  loss_ce_0: 0.6417  loss_mask_0: 0.4848  loss_dice_0: 3.836  loss_ce_1: 0.3843  loss_mask_1: 0.501  loss_dice_1: 3.747  loss_ce_2: 0.4128  loss_mask_2: 0.5025  loss_dice_2: 3.727  loss_ce_3: 0.4073  loss_mask_3: 0.5031  loss_dice_3: 3.718  loss_ce_4: 0.3977  loss_mask_4: 0.5017  loss_dice_4: 3.711  loss_ce_5: 0.3776  loss_mask_5: 0.5011  loss_dice_5: 3.717  loss_ce_6: 0.3781  loss_mask_6: 0.5036  loss_dice_6: 3.712  loss_ce_7: 0.3889  loss_mask_7: 0.4996  loss_dice_7: 3.716  loss_ce_8: 0.3808  loss_mask_8: 0.503  loss_dice_8: 3.71  time: 1.5036  data_time: 0.0647  lr: 8.5159e-06  max_mem: 21460M
[01/17 21:36:12] d2.utils.events INFO:  eta: 13:40:55  iter: 6559  total_loss: 46.76  loss_ce: 0.419  loss_mask: 0.5012  loss_dice: 3.695  loss_ce_0: 0.6382  loss_mask_0: 0.4839  loss_dice_0: 3.824  loss_ce_1: 0.433  loss_mask_1: 0.4981  loss_dice_1: 3.733  loss_ce_2: 0.4358  loss_mask_2: 0.502  loss_dice_2: 3.71  loss_ce_3: 0.4348  loss_mask_3: 0.4991  loss_dice_3: 3.695  loss_ce_4: 0.4198  loss_mask_4: 0.5031  loss_dice_4: 3.701  loss_ce_5: 0.4254  loss_mask_5: 0.5038  loss_dice_5: 3.686  loss_ce_6: 0.4101  loss_mask_6: 0.5027  loss_dice_6: 3.683  loss_ce_7: 0.4192  loss_mask_7: 0.5018  loss_dice_7: 3.706  loss_ce_8: 0.4206  loss_mask_8: 0.5024  loss_dice_8: 3.698  time: 1.5034  data_time: 0.0669  lr: 8.5113e-06  max_mem: 21460M
[01/17 21:36:42] d2.utils.events INFO:  eta: 13:40:45  iter: 6579  total_loss: 46.7  loss_ce: 0.4178  loss_mask: 0.4942  loss_dice: 3.698  loss_ce_0: 0.6506  loss_mask_0: 0.4757  loss_dice_0: 3.818  loss_ce_1: 0.4413  loss_mask_1: 0.4934  loss_dice_1: 3.744  loss_ce_2: 0.4244  loss_mask_2: 0.4938  loss_dice_2: 3.716  loss_ce_3: 0.4308  loss_mask_3: 0.4958  loss_dice_3: 3.697  loss_ce_4: 0.4242  loss_mask_4: 0.4952  loss_dice_4: 3.702  loss_ce_5: 0.4356  loss_mask_5: 0.4951  loss_dice_5: 3.7  loss_ce_6: 0.4306  loss_mask_6: 0.4955  loss_dice_6: 3.695  loss_ce_7: 0.4339  loss_mask_7: 0.4959  loss_dice_7: 3.701  loss_ce_8: 0.4221  loss_mask_8: 0.4965  loss_dice_8: 3.706  time: 1.5034  data_time: 0.0719  lr: 8.5067e-06  max_mem: 21460M
[01/17 21:37:11] d2.utils.events INFO:  eta: 13:40:19  iter: 6599  total_loss: 46.51  loss_ce: 0.4139  loss_mask: 0.4811  loss_dice: 3.701  loss_ce_0: 0.6333  loss_mask_0: 0.4708  loss_dice_0: 3.814  loss_ce_1: 0.426  loss_mask_1: 0.4873  loss_dice_1: 3.736  loss_ce_2: 0.4297  loss_mask_2: 0.4805  loss_dice_2: 3.711  loss_ce_3: 0.4024  loss_mask_3: 0.4792  loss_dice_3: 3.703  loss_ce_4: 0.4178  loss_mask_4: 0.4784  loss_dice_4: 3.705  loss_ce_5: 0.4058  loss_mask_5: 0.4782  loss_dice_5: 3.701  loss_ce_6: 0.4102  loss_mask_6: 0.482  loss_dice_6: 3.697  loss_ce_7: 0.4064  loss_mask_7: 0.4823  loss_dice_7: 3.701  loss_ce_8: 0.4182  loss_mask_8: 0.4808  loss_dice_8: 3.703  time: 1.5033  data_time: 0.0780  lr: 8.5022e-06  max_mem: 21460M
[01/17 21:37:41] d2.utils.events INFO:  eta: 13:39:49  iter: 6619  total_loss: 46.52  loss_ce: 0.4299  loss_mask: 0.5055  loss_dice: 3.679  loss_ce_0: 0.6519  loss_mask_0: 0.4847  loss_dice_0: 3.815  loss_ce_1: 0.4457  loss_mask_1: 0.496  loss_dice_1: 3.729  loss_ce_2: 0.4605  loss_mask_2: 0.4971  loss_dice_2: 3.701  loss_ce_3: 0.4433  loss_mask_3: 0.4953  loss_dice_3: 3.687  loss_ce_4: 0.4268  loss_mask_4: 0.4974  loss_dice_4: 3.682  loss_ce_5: 0.4252  loss_mask_5: 0.4993  loss_dice_5: 3.687  loss_ce_6: 0.4154  loss_mask_6: 0.5004  loss_dice_6: 3.676  loss_ce_7: 0.4253  loss_mask_7: 0.5012  loss_dice_7: 3.688  loss_ce_8: 0.4217  loss_mask_8: 0.505  loss_dice_8: 3.681  time: 1.5033  data_time: 0.0746  lr: 8.4976e-06  max_mem: 21460M
[01/17 21:38:10] d2.utils.events INFO:  eta: 13:39:22  iter: 6639  total_loss: 46.21  loss_ce: 0.4207  loss_mask: 0.5132  loss_dice: 3.657  loss_ce_0: 0.6466  loss_mask_0: 0.4911  loss_dice_0: 3.784  loss_ce_1: 0.41  loss_mask_1: 0.5153  loss_dice_1: 3.696  loss_ce_2: 0.432  loss_mask_2: 0.5146  loss_dice_2: 3.667  loss_ce_3: 0.426  loss_mask_3: 0.5141  loss_dice_3: 3.659  loss_ce_4: 0.4275  loss_mask_4: 0.5143  loss_dice_4: 3.65  loss_ce_5: 0.4068  loss_mask_5: 0.5132  loss_dice_5: 3.657  loss_ce_6: 0.395  loss_mask_6: 0.5132  loss_dice_6: 3.657  loss_ce_7: 0.4205  loss_mask_7: 0.5137  loss_dice_7: 3.649  loss_ce_8: 0.4215  loss_mask_8: 0.5124  loss_dice_8: 3.654  time: 1.5032  data_time: 0.0693  lr: 8.493e-06  max_mem: 21460M
[01/17 21:38:40] d2.utils.events INFO:  eta: 13:38:38  iter: 6659  total_loss: 46.54  loss_ce: 0.4152  loss_mask: 0.5047  loss_dice: 3.699  loss_ce_0: 0.6404  loss_mask_0: 0.4817  loss_dice_0: 3.833  loss_ce_1: 0.422  loss_mask_1: 0.5055  loss_dice_1: 3.738  loss_ce_2: 0.4212  loss_mask_2: 0.5081  loss_dice_2: 3.716  loss_ce_3: 0.42  loss_mask_3: 0.5062  loss_dice_3: 3.704  loss_ce_4: 0.4341  loss_mask_4: 0.5032  loss_dice_4: 3.705  loss_ce_5: 0.4128  loss_mask_5: 0.5045  loss_dice_5: 3.702  loss_ce_6: 0.4128  loss_mask_6: 0.5046  loss_dice_6: 3.688  loss_ce_7: 0.4121  loss_mask_7: 0.5024  loss_dice_7: 3.698  loss_ce_8: 0.409  loss_mask_8: 0.5036  loss_dice_8: 3.71  time: 1.5030  data_time: 0.0691  lr: 8.4884e-06  max_mem: 21460M
[01/17 21:39:09] d2.utils.events INFO:  eta: 13:37:40  iter: 6679  total_loss: 46.77  loss_ce: 0.4348  loss_mask: 0.5071  loss_dice: 3.669  loss_ce_0: 0.6573  loss_mask_0: 0.4802  loss_dice_0: 3.792  loss_ce_1: 0.423  loss_mask_1: 0.5051  loss_dice_1: 3.702  loss_ce_2: 0.4607  loss_mask_2: 0.5035  loss_dice_2: 3.686  loss_ce_3: 0.4418  loss_mask_3: 0.4993  loss_dice_3: 3.687  loss_ce_4: 0.4743  loss_mask_4: 0.5029  loss_dice_4: 3.679  loss_ce_5: 0.4311  loss_mask_5: 0.5046  loss_dice_5: 3.68  loss_ce_6: 0.4315  loss_mask_6: 0.5058  loss_dice_6: 3.684  loss_ce_7: 0.4337  loss_mask_7: 0.5064  loss_dice_7: 3.679  loss_ce_8: 0.4317  loss_mask_8: 0.5067  loss_dice_8: 3.679  time: 1.5029  data_time: 0.0683  lr: 8.4838e-06  max_mem: 21460M
[01/17 21:39:39] d2.utils.events INFO:  eta: 13:37:48  iter: 6699  total_loss: 46.48  loss_ce: 0.4062  loss_mask: 0.4982  loss_dice: 3.697  loss_ce_0: 0.6286  loss_mask_0: 0.4823  loss_dice_0: 3.818  loss_ce_1: 0.4028  loss_mask_1: 0.503  loss_dice_1: 3.731  loss_ce_2: 0.4239  loss_mask_2: 0.5052  loss_dice_2: 3.703  loss_ce_3: 0.4355  loss_mask_3: 0.5104  loss_dice_3: 3.691  loss_ce_4: 0.4179  loss_mask_4: 0.5089  loss_dice_4: 3.681  loss_ce_5: 0.4205  loss_mask_5: 0.507  loss_dice_5: 3.686  loss_ce_6: 0.4123  loss_mask_6: 0.5033  loss_dice_6: 3.691  loss_ce_7: 0.4021  loss_mask_7: 0.5047  loss_dice_7: 3.695  loss_ce_8: 0.4133  loss_mask_8: 0.5011  loss_dice_8: 3.697  time: 1.5029  data_time: 0.0670  lr: 8.4793e-06  max_mem: 21460M
[01/17 21:40:09] d2.utils.events INFO:  eta: 13:36:47  iter: 6719  total_loss: 46.26  loss_ce: 0.4141  loss_mask: 0.5135  loss_dice: 3.701  loss_ce_0: 0.6187  loss_mask_0: 0.4867  loss_dice_0: 3.828  loss_ce_1: 0.3939  loss_mask_1: 0.511  loss_dice_1: 3.752  loss_ce_2: 0.3967  loss_mask_2: 0.5119  loss_dice_2: 3.725  loss_ce_3: 0.4153  loss_mask_3: 0.5097  loss_dice_3: 3.719  loss_ce_4: 0.3977  loss_mask_4: 0.5106  loss_dice_4: 3.713  loss_ce_5: 0.4065  loss_mask_5: 0.5124  loss_dice_5: 3.721  loss_ce_6: 0.3988  loss_mask_6: 0.5116  loss_dice_6: 3.698  loss_ce_7: 0.4134  loss_mask_7: 0.5109  loss_dice_7: 3.709  loss_ce_8: 0.4139  loss_mask_8: 0.5126  loss_dice_8: 3.705  time: 1.5028  data_time: 0.0713  lr: 8.4747e-06  max_mem: 21460M
[01/17 21:40:39] d2.utils.events INFO:  eta: 13:36:40  iter: 6739  total_loss: 46.35  loss_ce: 0.4108  loss_mask: 0.4972  loss_dice: 3.691  loss_ce_0: 0.6314  loss_mask_0: 0.4728  loss_dice_0: 3.81  loss_ce_1: 0.3882  loss_mask_1: 0.4939  loss_dice_1: 3.735  loss_ce_2: 0.4229  loss_mask_2: 0.4942  loss_dice_2: 3.707  loss_ce_3: 0.4021  loss_mask_3: 0.4922  loss_dice_3: 3.698  loss_ce_4: 0.4126  loss_mask_4: 0.4941  loss_dice_4: 3.688  loss_ce_5: 0.411  loss_mask_5: 0.4935  loss_dice_5: 3.691  loss_ce_6: 0.4045  loss_mask_6: 0.4972  loss_dice_6: 3.683  loss_ce_7: 0.3974  loss_mask_7: 0.4967  loss_dice_7: 3.69  loss_ce_8: 0.3957  loss_mask_8: 0.4972  loss_dice_8: 3.691  time: 1.5028  data_time: 0.0742  lr: 8.4701e-06  max_mem: 21460M
[01/17 21:41:08] d2.utils.events INFO:  eta: 13:35:48  iter: 6759  total_loss: 46.23  loss_ce: 0.4067  loss_mask: 0.5081  loss_dice: 3.667  loss_ce_0: 0.6149  loss_mask_0: 0.4872  loss_dice_0: 3.81  loss_ce_1: 0.3776  loss_mask_1: 0.5051  loss_dice_1: 3.707  loss_ce_2: 0.4177  loss_mask_2: 0.5049  loss_dice_2: 3.679  loss_ce_3: 0.4009  loss_mask_3: 0.5073  loss_dice_3: 3.67  loss_ce_4: 0.4168  loss_mask_4: 0.5106  loss_dice_4: 3.665  loss_ce_5: 0.412  loss_mask_5: 0.5064  loss_dice_5: 3.671  loss_ce_6: 0.4076  loss_mask_6: 0.5071  loss_dice_6: 3.663  loss_ce_7: 0.3981  loss_mask_7: 0.5072  loss_dice_7: 3.663  loss_ce_8: 0.4044  loss_mask_8: 0.5081  loss_dice_8: 3.667  time: 1.5027  data_time: 0.0849  lr: 8.4655e-06  max_mem: 21460M
[01/17 21:41:38] d2.utils.events INFO:  eta: 13:35:41  iter: 6779  total_loss: 46.27  loss_ce: 0.4195  loss_mask: 0.5029  loss_dice: 3.674  loss_ce_0: 0.6377  loss_mask_0: 0.4801  loss_dice_0: 3.816  loss_ce_1: 0.4001  loss_mask_1: 0.5007  loss_dice_1: 3.715  loss_ce_2: 0.4387  loss_mask_2: 0.4992  loss_dice_2: 3.693  loss_ce_3: 0.417  loss_mask_3: 0.5009  loss_dice_3: 3.691  loss_ce_4: 0.4207  loss_mask_4: 0.5025  loss_dice_4: 3.682  loss_ce_5: 0.4052  loss_mask_5: 0.5016  loss_dice_5: 3.681  loss_ce_6: 0.4208  loss_mask_6: 0.5007  loss_dice_6: 3.677  loss_ce_7: 0.4061  loss_mask_7: 0.4995  loss_dice_7: 3.679  loss_ce_8: 0.4004  loss_mask_8: 0.4999  loss_dice_8: 3.682  time: 1.5027  data_time: 0.0796  lr: 8.4609e-06  max_mem: 21460M
[01/17 21:42:08] d2.utils.events INFO:  eta: 13:35:30  iter: 6799  total_loss: 45.75  loss_ce: 0.3884  loss_mask: 0.5009  loss_dice: 3.685  loss_ce_0: 0.6348  loss_mask_0: 0.4793  loss_dice_0: 3.811  loss_ce_1: 0.3612  loss_mask_1: 0.4969  loss_dice_1: 3.718  loss_ce_2: 0.3788  loss_mask_2: 0.4975  loss_dice_2: 3.698  loss_ce_3: 0.3788  loss_mask_3: 0.4958  loss_dice_3: 3.69  loss_ce_4: 0.3739  loss_mask_4: 0.4981  loss_dice_4: 3.677  loss_ce_5: 0.3563  loss_mask_5: 0.501  loss_dice_5: 3.684  loss_ce_6: 0.3665  loss_mask_6: 0.5002  loss_dice_6: 3.686  loss_ce_7: 0.3703  loss_mask_7: 0.5034  loss_dice_7: 3.675  loss_ce_8: 0.393  loss_mask_8: 0.5027  loss_dice_8: 3.684  time: 1.5026  data_time: 0.0735  lr: 8.4563e-06  max_mem: 21460M
[01/17 21:42:37] d2.utils.events INFO:  eta: 13:34:53  iter: 6819  total_loss: 46.44  loss_ce: 0.4369  loss_mask: 0.5066  loss_dice: 3.665  loss_ce_0: 0.6344  loss_mask_0: 0.4836  loss_dice_0: 3.786  loss_ce_1: 0.418  loss_mask_1: 0.5078  loss_dice_1: 3.687  loss_ce_2: 0.4628  loss_mask_2: 0.5035  loss_dice_2: 3.678  loss_ce_3: 0.4194  loss_mask_3: 0.5013  loss_dice_3: 3.679  loss_ce_4: 0.4514  loss_mask_4: 0.5027  loss_dice_4: 3.663  loss_ce_5: 0.4427  loss_mask_5: 0.5045  loss_dice_5: 3.669  loss_ce_6: 0.4377  loss_mask_6: 0.5076  loss_dice_6: 3.669  loss_ce_7: 0.4277  loss_mask_7: 0.5039  loss_dice_7: 3.678  loss_ce_8: 0.445  loss_mask_8: 0.5061  loss_dice_8: 3.667  time: 1.5025  data_time: 0.0675  lr: 8.4517e-06  max_mem: 21460M
[01/17 21:43:07] d2.utils.events INFO:  eta: 13:34:03  iter: 6839  total_loss: 46.36  loss_ce: 0.4324  loss_mask: 0.5061  loss_dice: 3.665  loss_ce_0: 0.6306  loss_mask_0: 0.4756  loss_dice_0: 3.793  loss_ce_1: 0.3929  loss_mask_1: 0.4984  loss_dice_1: 3.71  loss_ce_2: 0.4226  loss_mask_2: 0.5039  loss_dice_2: 3.684  loss_ce_3: 0.4512  loss_mask_3: 0.5052  loss_dice_3: 3.666  loss_ce_4: 0.4144  loss_mask_4: 0.5044  loss_dice_4: 3.672  loss_ce_5: 0.4211  loss_mask_5: 0.503  loss_dice_5: 3.669  loss_ce_6: 0.4172  loss_mask_6: 0.5054  loss_dice_6: 3.668  loss_ce_7: 0.4318  loss_mask_7: 0.508  loss_dice_7: 3.661  loss_ce_8: 0.4368  loss_mask_8: 0.5061  loss_dice_8: 3.665  time: 1.5025  data_time: 0.0780  lr: 8.4472e-06  max_mem: 21460M
[01/17 21:43:37] d2.utils.events INFO:  eta: 13:33:43  iter: 6859  total_loss: 46.15  loss_ce: 0.4099  loss_mask: 0.4985  loss_dice: 3.668  loss_ce_0: 0.6258  loss_mask_0: 0.4752  loss_dice_0: 3.791  loss_ce_1: 0.4006  loss_mask_1: 0.4951  loss_dice_1: 3.702  loss_ce_2: 0.4215  loss_mask_2: 0.496  loss_dice_2: 3.68  loss_ce_3: 0.4131  loss_mask_3: 0.496  loss_dice_3: 3.674  loss_ce_4: 0.4142  loss_mask_4: 0.4987  loss_dice_4: 3.662  loss_ce_5: 0.4069  loss_mask_5: 0.4989  loss_dice_5: 3.67  loss_ce_6: 0.4031  loss_mask_6: 0.501  loss_dice_6: 3.663  loss_ce_7: 0.409  loss_mask_7: 0.498  loss_dice_7: 3.665  loss_ce_8: 0.4045  loss_mask_8: 0.4977  loss_dice_8: 3.669  time: 1.5024  data_time: 0.0768  lr: 8.4426e-06  max_mem: 21460M
[01/17 21:44:07] d2.utils.events INFO:  eta: 13:33:36  iter: 6879  total_loss: 46.81  loss_ce: 0.4052  loss_mask: 0.5218  loss_dice: 3.727  loss_ce_0: 0.6296  loss_mask_0: 0.4991  loss_dice_0: 3.817  loss_ce_1: 0.3977  loss_mask_1: 0.5164  loss_dice_1: 3.748  loss_ce_2: 0.4294  loss_mask_2: 0.5156  loss_dice_2: 3.725  loss_ce_3: 0.4133  loss_mask_3: 0.5187  loss_dice_3: 3.718  loss_ce_4: 0.4254  loss_mask_4: 0.5171  loss_dice_4: 3.726  loss_ce_5: 0.4063  loss_mask_5: 0.5189  loss_dice_5: 3.731  loss_ce_6: 0.4136  loss_mask_6: 0.5194  loss_dice_6: 3.72  loss_ce_7: 0.4076  loss_mask_7: 0.5214  loss_dice_7: 3.724  loss_ce_8: 0.4032  loss_mask_8: 0.5217  loss_dice_8: 3.723  time: 1.5025  data_time: 0.0809  lr: 8.438e-06  max_mem: 21460M
[01/17 21:44:36] d2.utils.events INFO:  eta: 13:32:58  iter: 6899  total_loss: 46.36  loss_ce: 0.42  loss_mask: 0.5072  loss_dice: 3.677  loss_ce_0: 0.6593  loss_mask_0: 0.4865  loss_dice_0: 3.803  loss_ce_1: 0.4379  loss_mask_1: 0.5054  loss_dice_1: 3.715  loss_ce_2: 0.4212  loss_mask_2: 0.5072  loss_dice_2: 3.69  loss_ce_3: 0.4246  loss_mask_3: 0.5104  loss_dice_3: 3.677  loss_ce_4: 0.4134  loss_mask_4: 0.5112  loss_dice_4: 3.681  loss_ce_5: 0.4269  loss_mask_5: 0.511  loss_dice_5: 3.677  loss_ce_6: 0.4054  loss_mask_6: 0.5093  loss_dice_6: 3.682  loss_ce_7: 0.4177  loss_mask_7: 0.5072  loss_dice_7: 3.683  loss_ce_8: 0.4193  loss_mask_8: 0.5073  loss_dice_8: 3.681  time: 1.5024  data_time: 0.0706  lr: 8.4334e-06  max_mem: 21460M
[01/17 21:45:06] d2.utils.events INFO:  eta: 13:32:05  iter: 6919  total_loss: 45.95  loss_ce: 0.3999  loss_mask: 0.5082  loss_dice: 3.643  loss_ce_0: 0.6363  loss_mask_0: 0.4888  loss_dice_0: 3.753  loss_ce_1: 0.4045  loss_mask_1: 0.5099  loss_dice_1: 3.671  loss_ce_2: 0.4316  loss_mask_2: 0.5092  loss_dice_2: 3.64  loss_ce_3: 0.4033  loss_mask_3: 0.5085  loss_dice_3: 3.635  loss_ce_4: 0.3987  loss_mask_4: 0.5076  loss_dice_4: 3.633  loss_ce_5: 0.3914  loss_mask_5: 0.5096  loss_dice_5: 3.638  loss_ce_6: 0.4182  loss_mask_6: 0.5086  loss_dice_6: 3.638  loss_ce_7: 0.3996  loss_mask_7: 0.5078  loss_dice_7: 3.627  loss_ce_8: 0.4099  loss_mask_8: 0.5071  loss_dice_8: 3.635  time: 1.5023  data_time: 0.0841  lr: 8.4288e-06  max_mem: 21460M
[01/17 21:45:36] d2.utils.events INFO:  eta: 13:32:00  iter: 6939  total_loss: 46.25  loss_ce: 0.4065  loss_mask: 0.5052  loss_dice: 3.657  loss_ce_0: 0.6637  loss_mask_0: 0.4873  loss_dice_0: 3.767  loss_ce_1: 0.4294  loss_mask_1: 0.5043  loss_dice_1: 3.69  loss_ce_2: 0.4496  loss_mask_2: 0.5034  loss_dice_2: 3.671  loss_ce_3: 0.4353  loss_mask_3: 0.5019  loss_dice_3: 3.655  loss_ce_4: 0.4215  loss_mask_4: 0.502  loss_dice_4: 3.655  loss_ce_5: 0.4214  loss_mask_5: 0.4989  loss_dice_5: 3.661  loss_ce_6: 0.4307  loss_mask_6: 0.5015  loss_dice_6: 3.656  loss_ce_7: 0.4068  loss_mask_7: 0.5038  loss_dice_7: 3.661  loss_ce_8: 0.4019  loss_mask_8: 0.5039  loss_dice_8: 3.653  time: 1.5022  data_time: 0.0737  lr: 8.4242e-06  max_mem: 21460M
[01/17 21:46:05] d2.utils.events INFO:  eta: 13:31:31  iter: 6959  total_loss: 46.2  loss_ce: 0.4003  loss_mask: 0.4997  loss_dice: 3.654  loss_ce_0: 0.65  loss_mask_0: 0.4718  loss_dice_0: 3.789  loss_ce_1: 0.4036  loss_mask_1: 0.4973  loss_dice_1: 3.704  loss_ce_2: 0.4126  loss_mask_2: 0.499  loss_dice_2: 3.675  loss_ce_3: 0.4155  loss_mask_3: 0.4985  loss_dice_3: 3.658  loss_ce_4: 0.4079  loss_mask_4: 0.5  loss_dice_4: 3.652  loss_ce_5: 0.4096  loss_mask_5: 0.5018  loss_dice_5: 3.651  loss_ce_6: 0.3879  loss_mask_6: 0.5  loss_dice_6: 3.653  loss_ce_7: 0.4086  loss_mask_7: 0.5002  loss_dice_7: 3.653  loss_ce_8: 0.398  loss_mask_8: 0.4999  loss_dice_8: 3.653  time: 1.5021  data_time: 0.0814  lr: 8.4196e-06  max_mem: 21460M
[01/17 21:46:35] d2.utils.events INFO:  eta: 13:31:03  iter: 6979  total_loss: 46.53  loss_ce: 0.419  loss_mask: 0.4989  loss_dice: 3.673  loss_ce_0: 0.638  loss_mask_0: 0.4841  loss_dice_0: 3.788  loss_ce_1: 0.4139  loss_mask_1: 0.5013  loss_dice_1: 3.711  loss_ce_2: 0.4351  loss_mask_2: 0.5033  loss_dice_2: 3.685  loss_ce_3: 0.4164  loss_mask_3: 0.4993  loss_dice_3: 3.682  loss_ce_4: 0.4136  loss_mask_4: 0.4992  loss_dice_4: 3.682  loss_ce_5: 0.4153  loss_mask_5: 0.499  loss_dice_5: 3.686  loss_ce_6: 0.4142  loss_mask_6: 0.5026  loss_dice_6: 3.672  loss_ce_7: 0.4102  loss_mask_7: 0.5001  loss_dice_7: 3.673  loss_ce_8: 0.4143  loss_mask_8: 0.4981  loss_dice_8: 3.677  time: 1.5021  data_time: 0.0767  lr: 8.4151e-06  max_mem: 21460M
[01/17 21:47:05] d2.utils.events INFO:  eta: 13:30:39  iter: 6999  total_loss: 46.61  loss_ce: 0.403  loss_mask: 0.4979  loss_dice: 3.678  loss_ce_0: 0.6054  loss_mask_0: 0.4743  loss_dice_0: 3.802  loss_ce_1: 0.4133  loss_mask_1: 0.5023  loss_dice_1: 3.714  loss_ce_2: 0.423  loss_mask_2: 0.4988  loss_dice_2: 3.691  loss_ce_3: 0.398  loss_mask_3: 0.5008  loss_dice_3: 3.679  loss_ce_4: 0.4194  loss_mask_4: 0.5013  loss_dice_4: 3.684  loss_ce_5: 0.4091  loss_mask_5: 0.4989  loss_dice_5: 3.681  loss_ce_6: 0.4143  loss_mask_6: 0.4998  loss_dice_6: 3.679  loss_ce_7: 0.4112  loss_mask_7: 0.4988  loss_dice_7: 3.68  loss_ce_8: 0.3977  loss_mask_8: 0.5001  loss_dice_8: 3.674  time: 1.5020  data_time: 0.0841  lr: 8.4105e-06  max_mem: 21460M
[01/17 21:47:34] d2.utils.events INFO:  eta: 13:30:14  iter: 7019  total_loss: 45.47  loss_ce: 0.4012  loss_mask: 0.5006  loss_dice: 3.62  loss_ce_0: 0.6406  loss_mask_0: 0.4848  loss_dice_0: 3.746  loss_ce_1: 0.4142  loss_mask_1: 0.4975  loss_dice_1: 3.658  loss_ce_2: 0.425  loss_mask_2: 0.4982  loss_dice_2: 3.635  loss_ce_3: 0.3935  loss_mask_3: 0.5  loss_dice_3: 3.624  loss_ce_4: 0.4004  loss_mask_4: 0.4977  loss_dice_4: 3.626  loss_ce_5: 0.3959  loss_mask_5: 0.4997  loss_dice_5: 3.634  loss_ce_6: 0.3947  loss_mask_6: 0.4966  loss_dice_6: 3.627  loss_ce_7: 0.3895  loss_mask_7: 0.4968  loss_dice_7: 3.622  loss_ce_8: 0.3857  loss_mask_8: 0.5015  loss_dice_8: 3.621  time: 1.5020  data_time: 0.0692  lr: 8.4059e-06  max_mem: 21460M
[01/17 21:48:04] d2.utils.events INFO:  eta: 13:29:44  iter: 7039  total_loss: 45.63  loss_ce: 0.3844  loss_mask: 0.4869  loss_dice: 3.645  loss_ce_0: 0.6156  loss_mask_0: 0.4623  loss_dice_0: 3.788  loss_ce_1: 0.3852  loss_mask_1: 0.4739  loss_dice_1: 3.706  loss_ce_2: 0.3994  loss_mask_2: 0.4815  loss_dice_2: 3.671  loss_ce_3: 0.3767  loss_mask_3: 0.484  loss_dice_3: 3.661  loss_ce_4: 0.3785  loss_mask_4: 0.4875  loss_dice_4: 3.667  loss_ce_5: 0.3926  loss_mask_5: 0.4884  loss_dice_5: 3.654  loss_ce_6: 0.3817  loss_mask_6: 0.4866  loss_dice_6: 3.652  loss_ce_7: 0.3939  loss_mask_7: 0.4873  loss_dice_7: 3.655  loss_ce_8: 0.3757  loss_mask_8: 0.4872  loss_dice_8: 3.654  time: 1.5019  data_time: 0.0649  lr: 8.4013e-06  max_mem: 21460M
[01/17 21:48:34] d2.utils.events INFO:  eta: 13:29:10  iter: 7059  total_loss: 46.07  loss_ce: 0.4325  loss_mask: 0.4919  loss_dice: 3.638  loss_ce_0: 0.673  loss_mask_0: 0.4709  loss_dice_0: 3.784  loss_ce_1: 0.4503  loss_mask_1: 0.4888  loss_dice_1: 3.677  loss_ce_2: 0.4594  loss_mask_2: 0.4934  loss_dice_2: 3.65  loss_ce_3: 0.4488  loss_mask_3: 0.4952  loss_dice_3: 3.643  loss_ce_4: 0.4312  loss_mask_4: 0.4917  loss_dice_4: 3.637  loss_ce_5: 0.4615  loss_mask_5: 0.4931  loss_dice_5: 3.638  loss_ce_6: 0.4453  loss_mask_6: 0.4966  loss_dice_6: 3.633  loss_ce_7: 0.4299  loss_mask_7: 0.4952  loss_dice_7: 3.633  loss_ce_8: 0.4446  loss_mask_8: 0.4927  loss_dice_8: 3.625  time: 1.5019  data_time: 0.0731  lr: 8.3967e-06  max_mem: 21460M
[01/17 21:49:03] d2.utils.events INFO:  eta: 13:28:45  iter: 7079  total_loss: 45.08  loss_ce: 0.4083  loss_mask: 0.4971  loss_dice: 3.552  loss_ce_0: 0.6316  loss_mask_0: 0.4743  loss_dice_0: 3.695  loss_ce_1: 0.3992  loss_mask_1: 0.4959  loss_dice_1: 3.591  loss_ce_2: 0.427  loss_mask_2: 0.4968  loss_dice_2: 3.571  loss_ce_3: 0.4119  loss_mask_3: 0.4979  loss_dice_3: 3.556  loss_ce_4: 0.4097  loss_mask_4: 0.4976  loss_dice_4: 3.553  loss_ce_5: 0.4128  loss_mask_5: 0.4945  loss_dice_5: 3.551  loss_ce_6: 0.4072  loss_mask_6: 0.4972  loss_dice_6: 3.549  loss_ce_7: 0.411  loss_mask_7: 0.4972  loss_dice_7: 3.556  loss_ce_8: 0.4021  loss_mask_8: 0.4957  loss_dice_8: 3.555  time: 1.5018  data_time: 0.0788  lr: 8.3921e-06  max_mem: 21460M
[01/17 21:49:33] d2.utils.events INFO:  eta: 13:28:05  iter: 7099  total_loss: 45.32  loss_ce: 0.4005  loss_mask: 0.4919  loss_dice: 3.596  loss_ce_0: 0.6189  loss_mask_0: 0.4703  loss_dice_0: 3.736  loss_ce_1: 0.3935  loss_mask_1: 0.4912  loss_dice_1: 3.638  loss_ce_2: 0.4102  loss_mask_2: 0.4924  loss_dice_2: 3.614  loss_ce_3: 0.3988  loss_mask_3: 0.4911  loss_dice_3: 3.598  loss_ce_4: 0.4088  loss_mask_4: 0.4914  loss_dice_4: 3.598  loss_ce_5: 0.3975  loss_mask_5: 0.4927  loss_dice_5: 3.604  loss_ce_6: 0.377  loss_mask_6: 0.4935  loss_dice_6: 3.595  loss_ce_7: 0.4002  loss_mask_7: 0.4908  loss_dice_7: 3.601  loss_ce_8: 0.3866  loss_mask_8: 0.4932  loss_dice_8: 3.6  time: 1.5017  data_time: 0.0745  lr: 8.3875e-06  max_mem: 21460M
[01/17 21:50:02] d2.utils.events INFO:  eta: 13:27:34  iter: 7119  total_loss: 45.48  loss_ce: 0.3571  loss_mask: 0.4991  loss_dice: 3.606  loss_ce_0: 0.6187  loss_mask_0: 0.4708  loss_dice_0: 3.746  loss_ce_1: 0.3936  loss_mask_1: 0.4957  loss_dice_1: 3.636  loss_ce_2: 0.4067  loss_mask_2: 0.4972  loss_dice_2: 3.616  loss_ce_3: 0.3688  loss_mask_3: 0.4979  loss_dice_3: 3.613  loss_ce_4: 0.3842  loss_mask_4: 0.4981  loss_dice_4: 3.619  loss_ce_5: 0.3813  loss_mask_5: 0.4984  loss_dice_5: 3.603  loss_ce_6: 0.3779  loss_mask_6: 0.498  loss_dice_6: 3.602  loss_ce_7: 0.363  loss_mask_7: 0.499  loss_dice_7: 3.601  loss_ce_8: 0.3595  loss_mask_8: 0.4989  loss_dice_8: 3.609  time: 1.5016  data_time: 0.0774  lr: 8.3829e-06  max_mem: 21460M
[01/17 21:50:33] d2.utils.events INFO:  eta: 13:27:28  iter: 7139  total_loss: 45.79  loss_ce: 0.3971  loss_mask: 0.4769  loss_dice: 3.646  loss_ce_0: 0.6692  loss_mask_0: 0.454  loss_dice_0: 3.782  loss_ce_1: 0.3987  loss_mask_1: 0.477  loss_dice_1: 3.682  loss_ce_2: 0.4106  loss_mask_2: 0.4773  loss_dice_2: 3.67  loss_ce_3: 0.4036  loss_mask_3: 0.4769  loss_dice_3: 3.647  loss_ce_4: 0.373  loss_mask_4: 0.4774  loss_dice_4: 3.653  loss_ce_5: 0.3758  loss_mask_5: 0.4784  loss_dice_5: 3.651  loss_ce_6: 0.3735  loss_mask_6: 0.4802  loss_dice_6: 3.649  loss_ce_7: 0.3808  loss_mask_7: 0.4789  loss_dice_7: 3.642  loss_ce_8: 0.3881  loss_mask_8: 0.4768  loss_dice_8: 3.648  time: 1.5017  data_time: 0.0784  lr: 8.3784e-06  max_mem: 21460M
[01/17 21:51:03] d2.utils.events INFO:  eta: 13:27:07  iter: 7159  total_loss: 45.6  loss_ce: 0.3783  loss_mask: 0.4927  loss_dice: 3.622  loss_ce_0: 0.6538  loss_mask_0: 0.4687  loss_dice_0: 3.765  loss_ce_1: 0.4095  loss_mask_1: 0.4894  loss_dice_1: 3.672  loss_ce_2: 0.4091  loss_mask_2: 0.4912  loss_dice_2: 3.634  loss_ce_3: 0.4001  loss_mask_3: 0.4892  loss_dice_3: 3.618  loss_ce_4: 0.391  loss_mask_4: 0.4897  loss_dice_4: 3.628  loss_ce_5: 0.3779  loss_mask_5: 0.4888  loss_dice_5: 3.625  loss_ce_6: 0.3814  loss_mask_6: 0.4885  loss_dice_6: 3.625  loss_ce_7: 0.3809  loss_mask_7: 0.4898  loss_dice_7: 3.624  loss_ce_8: 0.3742  loss_mask_8: 0.4895  loss_dice_8: 3.625  time: 1.5017  data_time: 0.0805  lr: 8.3738e-06  max_mem: 21460M
[01/17 21:51:33] d2.utils.events INFO:  eta: 13:27:18  iter: 7179  total_loss: 45.6  loss_ce: 0.4243  loss_mask: 0.496  loss_dice: 3.609  loss_ce_0: 0.6465  loss_mask_0: 0.4749  loss_dice_0: 3.734  loss_ce_1: 0.3984  loss_mask_1: 0.5001  loss_dice_1: 3.645  loss_ce_2: 0.4101  loss_mask_2: 0.495  loss_dice_2: 3.631  loss_ce_3: 0.416  loss_mask_3: 0.4979  loss_dice_3: 3.613  loss_ce_4: 0.4007  loss_mask_4: 0.4963  loss_dice_4: 3.612  loss_ce_5: 0.4098  loss_mask_5: 0.4962  loss_dice_5: 3.611  loss_ce_6: 0.4168  loss_mask_6: 0.494  loss_dice_6: 3.606  loss_ce_7: 0.4062  loss_mask_7: 0.4966  loss_dice_7: 3.607  loss_ce_8: 0.3925  loss_mask_8: 0.4987  loss_dice_8: 3.605  time: 1.5016  data_time: 0.0662  lr: 8.3692e-06  max_mem: 21460M
[01/17 21:52:02] d2.utils.events INFO:  eta: 13:26:09  iter: 7199  total_loss: 45.84  loss_ce: 0.4053  loss_mask: 0.5019  loss_dice: 3.637  loss_ce_0: 0.616  loss_mask_0: 0.4808  loss_dice_0: 3.769  loss_ce_1: 0.4116  loss_mask_1: 0.5034  loss_dice_1: 3.674  loss_ce_2: 0.4196  loss_mask_2: 0.5038  loss_dice_2: 3.643  loss_ce_3: 0.4143  loss_mask_3: 0.5029  loss_dice_3: 3.624  loss_ce_4: 0.4117  loss_mask_4: 0.5034  loss_dice_4: 3.626  loss_ce_5: 0.4114  loss_mask_5: 0.5  loss_dice_5: 3.632  loss_ce_6: 0.4181  loss_mask_6: 0.5012  loss_dice_6: 3.62  loss_ce_7: 0.426  loss_mask_7: 0.5004  loss_dice_7: 3.633  loss_ce_8: 0.4028  loss_mask_8: 0.5024  loss_dice_8: 3.639  time: 1.5015  data_time: 0.0730  lr: 8.3646e-06  max_mem: 21460M
[01/17 21:52:31] d2.utils.events INFO:  eta: 13:25:34  iter: 7219  total_loss: 45.24  loss_ce: 0.4143  loss_mask: 0.494  loss_dice: 3.583  loss_ce_0: 0.6439  loss_mask_0: 0.4753  loss_dice_0: 3.73  loss_ce_1: 0.3835  loss_mask_1: 0.4991  loss_dice_1: 3.624  loss_ce_2: 0.4155  loss_mask_2: 0.4962  loss_dice_2: 3.608  loss_ce_3: 0.4053  loss_mask_3: 0.4939  loss_dice_3: 3.594  loss_ce_4: 0.4019  loss_mask_4: 0.4958  loss_dice_4: 3.599  loss_ce_5: 0.4075  loss_mask_5: 0.4973  loss_dice_5: 3.592  loss_ce_6: 0.3892  loss_mask_6: 0.4978  loss_dice_6: 3.588  loss_ce_7: 0.3966  loss_mask_7: 0.4963  loss_dice_7: 3.592  loss_ce_8: 0.4038  loss_mask_8: 0.495  loss_dice_8: 3.586  time: 1.5014  data_time: 0.0824  lr: 8.36e-06  max_mem: 21460M
[01/17 21:53:01] d2.utils.events INFO:  eta: 13:25:04  iter: 7239  total_loss: 45.11  loss_ce: 0.3828  loss_mask: 0.4982  loss_dice: 3.576  loss_ce_0: 0.6323  loss_mask_0: 0.477  loss_dice_0: 3.717  loss_ce_1: 0.3882  loss_mask_1: 0.501  loss_dice_1: 3.617  loss_ce_2: 0.3881  loss_mask_2: 0.4991  loss_dice_2: 3.593  loss_ce_3: 0.3921  loss_mask_3: 0.499  loss_dice_3: 3.59  loss_ce_4: 0.3868  loss_mask_4: 0.5017  loss_dice_4: 3.58  loss_ce_5: 0.3775  loss_mask_5: 0.5003  loss_dice_5: 3.577  loss_ce_6: 0.4004  loss_mask_6: 0.4997  loss_dice_6: 3.573  loss_ce_7: 0.3771  loss_mask_7: 0.5007  loss_dice_7: 3.574  loss_ce_8: 0.3969  loss_mask_8: 0.4998  loss_dice_8: 3.579  time: 1.5013  data_time: 0.0716  lr: 8.3554e-06  max_mem: 21460M
[01/17 21:53:31] d2.utils.events INFO:  eta: 13:24:45  iter: 7259  total_loss: 45.51  loss_ce: 0.3878  loss_mask: 0.4989  loss_dice: 3.598  loss_ce_0: 0.6382  loss_mask_0: 0.4828  loss_dice_0: 3.74  loss_ce_1: 0.3919  loss_mask_1: 0.507  loss_dice_1: 3.635  loss_ce_2: 0.4121  loss_mask_2: 0.5052  loss_dice_2: 3.612  loss_ce_3: 0.3901  loss_mask_3: 0.5032  loss_dice_3: 3.595  loss_ce_4: 0.4016  loss_mask_4: 0.5015  loss_dice_4: 3.586  loss_ce_5: 0.3896  loss_mask_5: 0.5025  loss_dice_5: 3.593  loss_ce_6: 0.3988  loss_mask_6: 0.4996  loss_dice_6: 3.599  loss_ce_7: 0.3936  loss_mask_7: 0.4975  loss_dice_7: 3.601  loss_ce_8: 0.3929  loss_mask_8: 0.5006  loss_dice_8: 3.599  time: 1.5013  data_time: 0.0754  lr: 8.3508e-06  max_mem: 21460M
[01/17 21:54:01] d2.utils.events INFO:  eta: 13:25:05  iter: 7279  total_loss: 45.41  loss_ce: 0.3853  loss_mask: 0.4883  loss_dice: 3.621  loss_ce_0: 0.6224  loss_mask_0: 0.4682  loss_dice_0: 3.745  loss_ce_1: 0.3971  loss_mask_1: 0.4882  loss_dice_1: 3.651  loss_ce_2: 0.4159  loss_mask_2: 0.487  loss_dice_2: 3.628  loss_ce_3: 0.3998  loss_mask_3: 0.4867  loss_dice_3: 3.627  loss_ce_4: 0.4009  loss_mask_4: 0.4889  loss_dice_4: 3.624  loss_ce_5: 0.388  loss_mask_5: 0.488  loss_dice_5: 3.617  loss_ce_6: 0.3939  loss_mask_6: 0.4894  loss_dice_6: 3.617  loss_ce_7: 0.3836  loss_mask_7: 0.4933  loss_dice_7: 3.626  loss_ce_8: 0.3957  loss_mask_8: 0.4917  loss_dice_8: 3.617  time: 1.5013  data_time: 0.0678  lr: 8.3462e-06  max_mem: 21460M
[01/17 21:54:30] d2.utils.events INFO:  eta: 13:24:27  iter: 7299  total_loss: 44.77  loss_ce: 0.4047  loss_mask: 0.4951  loss_dice: 3.529  loss_ce_0: 0.6689  loss_mask_0: 0.4692  loss_dice_0: 3.686  loss_ce_1: 0.4044  loss_mask_1: 0.487  loss_dice_1: 3.59  loss_ce_2: 0.4326  loss_mask_2: 0.4893  loss_dice_2: 3.549  loss_ce_3: 0.4193  loss_mask_3: 0.4933  loss_dice_3: 3.538  loss_ce_4: 0.4132  loss_mask_4: 0.495  loss_dice_4: 3.533  loss_ce_5: 0.4078  loss_mask_5: 0.4948  loss_dice_5: 3.539  loss_ce_6: 0.4067  loss_mask_6: 0.4978  loss_dice_6: 3.534  loss_ce_7: 0.4129  loss_mask_7: 0.4975  loss_dice_7: 3.535  loss_ce_8: 0.4071  loss_mask_8: 0.4943  loss_dice_8: 3.53  time: 1.5012  data_time: 0.0636  lr: 8.3416e-06  max_mem: 21460M
[01/17 21:55:00] d2.utils.events INFO:  eta: 13:24:20  iter: 7319  total_loss: 46.22  loss_ce: 0.3843  loss_mask: 0.4958  loss_dice: 3.654  loss_ce_0: 0.6237  loss_mask_0: 0.4826  loss_dice_0: 3.78  loss_ce_1: 0.3882  loss_mask_1: 0.4957  loss_dice_1: 3.697  loss_ce_2: 0.4161  loss_mask_2: 0.4957  loss_dice_2: 3.664  loss_ce_3: 0.421  loss_mask_3: 0.4948  loss_dice_3: 3.659  loss_ce_4: 0.3972  loss_mask_4: 0.4948  loss_dice_4: 3.653  loss_ce_5: 0.3826  loss_mask_5: 0.4982  loss_dice_5: 3.657  loss_ce_6: 0.3949  loss_mask_6: 0.4982  loss_dice_6: 3.655  loss_ce_7: 0.3819  loss_mask_7: 0.4971  loss_dice_7: 3.662  loss_ce_8: 0.3867  loss_mask_8: 0.4948  loss_dice_8: 3.657  time: 1.5011  data_time: 0.0699  lr: 8.337e-06  max_mem: 21460M
[01/17 21:55:29] d2.utils.events INFO:  eta: 13:23:28  iter: 7339  total_loss: 45.46  loss_ce: 0.3893  loss_mask: 0.5099  loss_dice: 3.618  loss_ce_0: 0.6314  loss_mask_0: 0.4884  loss_dice_0: 3.733  loss_ce_1: 0.391  loss_mask_1: 0.5123  loss_dice_1: 3.641  loss_ce_2: 0.4084  loss_mask_2: 0.5106  loss_dice_2: 3.628  loss_ce_3: 0.3995  loss_mask_3: 0.5109  loss_dice_3: 3.621  loss_ce_4: 0.393  loss_mask_4: 0.5128  loss_dice_4: 3.604  loss_ce_5: 0.4043  loss_mask_5: 0.5114  loss_dice_5: 3.622  loss_ce_6: 0.3911  loss_mask_6: 0.51  loss_dice_6: 3.615  loss_ce_7: 0.395  loss_mask_7: 0.5097  loss_dice_7: 3.622  loss_ce_8: 0.4086  loss_mask_8: 0.5109  loss_dice_8: 3.618  time: 1.5010  data_time: 0.0672  lr: 8.3324e-06  max_mem: 21460M
[01/17 21:55:59] d2.utils.events INFO:  eta: 13:22:58  iter: 7359  total_loss: 45.09  loss_ce: 0.3781  loss_mask: 0.4881  loss_dice: 3.606  loss_ce_0: 0.6175  loss_mask_0: 0.4739  loss_dice_0: 3.736  loss_ce_1: 0.3677  loss_mask_1: 0.4961  loss_dice_1: 3.647  loss_ce_2: 0.3785  loss_mask_2: 0.4955  loss_dice_2: 3.618  loss_ce_3: 0.3696  loss_mask_3: 0.4904  loss_dice_3: 3.609  loss_ce_4: 0.3714  loss_mask_4: 0.4869  loss_dice_4: 3.606  loss_ce_5: 0.3783  loss_mask_5: 0.4847  loss_dice_5: 3.613  loss_ce_6: 0.374  loss_mask_6: 0.4904  loss_dice_6: 3.602  loss_ce_7: 0.3728  loss_mask_7: 0.4885  loss_dice_7: 3.613  loss_ce_8: 0.3789  loss_mask_8: 0.4898  loss_dice_8: 3.603  time: 1.5010  data_time: 0.0671  lr: 8.3279e-06  max_mem: 21460M
[01/17 21:56:28] d2.utils.events INFO:  eta: 13:22:49  iter: 7379  total_loss: 45.52  loss_ce: 0.432  loss_mask: 0.4903  loss_dice: 3.61  loss_ce_0: 0.6546  loss_mask_0: 0.4671  loss_dice_0: 3.753  loss_ce_1: 0.4133  loss_mask_1: 0.4836  loss_dice_1: 3.647  loss_ce_2: 0.4186  loss_mask_2: 0.4871  loss_dice_2: 3.629  loss_ce_3: 0.4131  loss_mask_3: 0.4884  loss_dice_3: 3.617  loss_ce_4: 0.4199  loss_mask_4: 0.4912  loss_dice_4: 3.615  loss_ce_5: 0.4053  loss_mask_5: 0.4924  loss_dice_5: 3.615  loss_ce_6: 0.4288  loss_mask_6: 0.4917  loss_dice_6: 3.611  loss_ce_7: 0.4117  loss_mask_7: 0.492  loss_dice_7: 3.606  loss_ce_8: 0.4082  loss_mask_8: 0.4915  loss_dice_8: 3.61  time: 1.5009  data_time: 0.0682  lr: 8.3233e-06  max_mem: 21460M
[01/17 21:56:58] d2.utils.events INFO:  eta: 13:21:31  iter: 7399  total_loss: 45.52  loss_ce: 0.4088  loss_mask: 0.4797  loss_dice: 3.625  loss_ce_0: 0.6679  loss_mask_0: 0.4602  loss_dice_0: 3.764  loss_ce_1: 0.408  loss_mask_1: 0.4792  loss_dice_1: 3.668  loss_ce_2: 0.42  loss_mask_2: 0.481  loss_dice_2: 3.645  loss_ce_3: 0.4234  loss_mask_3: 0.478  loss_dice_3: 3.625  loss_ce_4: 0.4199  loss_mask_4: 0.4779  loss_dice_4: 3.623  loss_ce_5: 0.3956  loss_mask_5: 0.4783  loss_dice_5: 3.623  loss_ce_6: 0.4028  loss_mask_6: 0.4788  loss_dice_6: 3.623  loss_ce_7: 0.411  loss_mask_7: 0.4793  loss_dice_7: 3.625  loss_ce_8: 0.4066  loss_mask_8: 0.479  loss_dice_8: 3.624  time: 1.5008  data_time: 0.0801  lr: 8.3187e-06  max_mem: 21460M
[01/17 21:57:28] d2.utils.events INFO:  eta: 13:21:50  iter: 7419  total_loss: 45.01  loss_ce: 0.4086  loss_mask: 0.4883  loss_dice: 3.584  loss_ce_0: 0.6112  loss_mask_0: 0.4616  loss_dice_0: 3.728  loss_ce_1: 0.3851  loss_mask_1: 0.4889  loss_dice_1: 3.623  loss_ce_2: 0.4075  loss_mask_2: 0.4901  loss_dice_2: 3.595  loss_ce_3: 0.401  loss_mask_3: 0.4896  loss_dice_3: 3.587  loss_ce_4: 0.406  loss_mask_4: 0.4883  loss_dice_4: 3.596  loss_ce_5: 0.3972  loss_mask_5: 0.4897  loss_dice_5: 3.595  loss_ce_6: 0.3878  loss_mask_6: 0.4893  loss_dice_6: 3.587  loss_ce_7: 0.39  loss_mask_7: 0.4881  loss_dice_7: 3.593  loss_ce_8: 0.3943  loss_mask_8: 0.4873  loss_dice_8: 3.59  time: 1.5008  data_time: 0.0704  lr: 8.3141e-06  max_mem: 21460M
[01/17 21:57:58] d2.utils.events INFO:  eta: 13:21:42  iter: 7439  total_loss: 45.25  loss_ce: 0.3902  loss_mask: 0.487  loss_dice: 3.599  loss_ce_0: 0.6261  loss_mask_0: 0.4599  loss_dice_0: 3.734  loss_ce_1: 0.396  loss_mask_1: 0.4849  loss_dice_1: 3.638  loss_ce_2: 0.402  loss_mask_2: 0.4843  loss_dice_2: 3.617  loss_ce_3: 0.3932  loss_mask_3: 0.4852  loss_dice_3: 3.592  loss_ce_4: 0.3876  loss_mask_4: 0.4849  loss_dice_4: 3.601  loss_ce_5: 0.3875  loss_mask_5: 0.485  loss_dice_5: 3.597  loss_ce_6: 0.3881  loss_mask_6: 0.4864  loss_dice_6: 3.592  loss_ce_7: 0.3931  loss_mask_7: 0.4855  loss_dice_7: 3.597  loss_ce_8: 0.3973  loss_mask_8: 0.4874  loss_dice_8: 3.597  time: 1.5008  data_time: 0.0771  lr: 8.3095e-06  max_mem: 21460M
[01/17 21:58:28] d2.utils.events INFO:  eta: 13:21:12  iter: 7459  total_loss: 45.47  loss_ce: 0.4112  loss_mask: 0.5077  loss_dice: 3.588  loss_ce_0: 0.6236  loss_mask_0: 0.482  loss_dice_0: 3.726  loss_ce_1: 0.4208  loss_mask_1: 0.5102  loss_dice_1: 3.638  loss_ce_2: 0.4393  loss_mask_2: 0.5055  loss_dice_2: 3.608  loss_ce_3: 0.4098  loss_mask_3: 0.5086  loss_dice_3: 3.601  loss_ce_4: 0.4191  loss_mask_4: 0.5092  loss_dice_4: 3.596  loss_ce_5: 0.4082  loss_mask_5: 0.5089  loss_dice_5: 3.601  loss_ce_6: 0.4082  loss_mask_6: 0.5059  loss_dice_6: 3.595  loss_ce_7: 0.3925  loss_mask_7: 0.51  loss_dice_7: 3.596  loss_ce_8: 0.3859  loss_mask_8: 0.5113  loss_dice_8: 3.602  time: 1.5008  data_time: 0.0902  lr: 8.3049e-06  max_mem: 21460M
[01/17 21:58:58] d2.utils.events INFO:  eta: 13:21:04  iter: 7479  total_loss: 44.98  loss_ce: 0.4108  loss_mask: 0.4936  loss_dice: 3.566  loss_ce_0: 0.6338  loss_mask_0: 0.4804  loss_dice_0: 3.693  loss_ce_1: 0.4061  loss_mask_1: 0.4947  loss_dice_1: 3.598  loss_ce_2: 0.4164  loss_mask_2: 0.4926  loss_dice_2: 3.575  loss_ce_3: 0.3988  loss_mask_3: 0.4931  loss_dice_3: 3.56  loss_ce_4: 0.4  loss_mask_4: 0.4932  loss_dice_4: 3.565  loss_ce_5: 0.3917  loss_mask_5: 0.4939  loss_dice_5: 3.571  loss_ce_6: 0.4092  loss_mask_6: 0.4956  loss_dice_6: 3.563  loss_ce_7: 0.3914  loss_mask_7: 0.4931  loss_dice_7: 3.568  loss_ce_8: 0.3982  loss_mask_8: 0.4939  loss_dice_8: 3.57  time: 1.5008  data_time: 0.0814  lr: 8.3003e-06  max_mem: 21460M
[01/17 21:59:28] d2.utils.events INFO:  eta: 13:21:10  iter: 7499  total_loss: 45.15  loss_ce: 0.3932  loss_mask: 0.501  loss_dice: 3.56  loss_ce_0: 0.62  loss_mask_0: 0.4738  loss_dice_0: 3.693  loss_ce_1: 0.4013  loss_mask_1: 0.4971  loss_dice_1: 3.599  loss_ce_2: 0.4048  loss_mask_2: 0.4966  loss_dice_2: 3.581  loss_ce_3: 0.4076  loss_mask_3: 0.4984  loss_dice_3: 3.556  loss_ce_4: 0.3923  loss_mask_4: 0.4997  loss_dice_4: 3.558  loss_ce_5: 0.3925  loss_mask_5: 0.4986  loss_dice_5: 3.563  loss_ce_6: 0.3892  loss_mask_6: 0.5001  loss_dice_6: 3.56  loss_ce_7: 0.3968  loss_mask_7: 0.4998  loss_dice_7: 3.56  loss_ce_8: 0.3781  loss_mask_8: 0.5009  loss_dice_8: 3.557  time: 1.5008  data_time: 0.0713  lr: 8.2957e-06  max_mem: 21460M
[01/17 21:59:58] d2.utils.events INFO:  eta: 13:20:54  iter: 7519  total_loss: 44.69  loss_ce: 0.3436  loss_mask: 0.4865  loss_dice: 3.589  loss_ce_0: 0.6129  loss_mask_0: 0.4618  loss_dice_0: 3.731  loss_ce_1: 0.3592  loss_mask_1: 0.4827  loss_dice_1: 3.631  loss_ce_2: 0.3736  loss_mask_2: 0.4861  loss_dice_2: 3.598  loss_ce_3: 0.3437  loss_mask_3: 0.4843  loss_dice_3: 3.578  loss_ce_4: 0.3439  loss_mask_4: 0.4827  loss_dice_4: 3.588  loss_ce_5: 0.3341  loss_mask_5: 0.4865  loss_dice_5: 3.589  loss_ce_6: 0.3428  loss_mask_6: 0.4859  loss_dice_6: 3.582  loss_ce_7: 0.3357  loss_mask_7: 0.4871  loss_dice_7: 3.586  loss_ce_8: 0.3426  loss_mask_8: 0.4866  loss_dice_8: 3.584  time: 1.5008  data_time: 0.0817  lr: 8.2911e-06  max_mem: 21460M
[01/17 22:00:28] d2.utils.events INFO:  eta: 13:20:53  iter: 7539  total_loss: 45.25  loss_ce: 0.4031  loss_mask: 0.4819  loss_dice: 3.58  loss_ce_0: 0.6605  loss_mask_0: 0.4647  loss_dice_0: 3.739  loss_ce_1: 0.4027  loss_mask_1: 0.4834  loss_dice_1: 3.633  loss_ce_2: 0.414  loss_mask_2: 0.479  loss_dice_2: 3.598  loss_ce_3: 0.4112  loss_mask_3: 0.4804  loss_dice_3: 3.59  loss_ce_4: 0.4128  loss_mask_4: 0.4793  loss_dice_4: 3.58  loss_ce_5: 0.4172  loss_mask_5: 0.4812  loss_dice_5: 3.586  loss_ce_6: 0.3965  loss_mask_6: 0.4806  loss_dice_6: 3.589  loss_ce_7: 0.4043  loss_mask_7: 0.4811  loss_dice_7: 3.58  loss_ce_8: 0.413  loss_mask_8: 0.4811  loss_dice_8: 3.591  time: 1.5008  data_time: 0.0689  lr: 8.2865e-06  max_mem: 21460M
[01/17 22:00:58] d2.utils.events INFO:  eta: 13:20:41  iter: 7559  total_loss: 44.99  loss_ce: 0.4109  loss_mask: 0.4897  loss_dice: 3.528  loss_ce_0: 0.6297  loss_mask_0: 0.4699  loss_dice_0: 3.676  loss_ce_1: 0.4015  loss_mask_1: 0.4952  loss_dice_1: 3.575  loss_ce_2: 0.4143  loss_mask_2: 0.4927  loss_dice_2: 3.554  loss_ce_3: 0.4  loss_mask_3: 0.4926  loss_dice_3: 3.55  loss_ce_4: 0.4083  loss_mask_4: 0.4934  loss_dice_4: 3.544  loss_ce_5: 0.3852  loss_mask_5: 0.4933  loss_dice_5: 3.536  loss_ce_6: 0.4002  loss_mask_6: 0.4923  loss_dice_6: 3.537  loss_ce_7: 0.3904  loss_mask_7: 0.4894  loss_dice_7: 3.533  loss_ce_8: 0.4027  loss_mask_8: 0.4893  loss_dice_8: 3.531  time: 1.5008  data_time: 0.0679  lr: 8.2819e-06  max_mem: 21460M
[01/17 22:01:28] d2.utils.events INFO:  eta: 13:20:17  iter: 7579  total_loss: 44.76  loss_ce: 0.4011  loss_mask: 0.4851  loss_dice: 3.537  loss_ce_0: 0.6433  loss_mask_0: 0.464  loss_dice_0: 3.685  loss_ce_1: 0.4257  loss_mask_1: 0.4808  loss_dice_1: 3.584  loss_ce_2: 0.4358  loss_mask_2: 0.4822  loss_dice_2: 3.543  loss_ce_3: 0.429  loss_mask_3: 0.4843  loss_dice_3: 3.548  loss_ce_4: 0.4137  loss_mask_4: 0.484  loss_dice_4: 3.539  loss_ce_5: 0.4001  loss_mask_5: 0.4856  loss_dice_5: 3.537  loss_ce_6: 0.4156  loss_mask_6: 0.4856  loss_dice_6: 3.53  loss_ce_7: 0.4015  loss_mask_7: 0.4865  loss_dice_7: 3.534  loss_ce_8: 0.4057  loss_mask_8: 0.4861  loss_dice_8: 3.527  time: 1.5007  data_time: 0.0727  lr: 8.2773e-06  max_mem: 21460M
[01/17 22:01:58] d2.utils.events INFO:  eta: 13:20:03  iter: 7599  total_loss: 45.44  loss_ce: 0.4243  loss_mask: 0.4916  loss_dice: 3.591  loss_ce_0: 0.6922  loss_mask_0: 0.4789  loss_dice_0: 3.717  loss_ce_1: 0.4242  loss_mask_1: 0.4983  loss_dice_1: 3.622  loss_ce_2: 0.4311  loss_mask_2: 0.4952  loss_dice_2: 3.607  loss_ce_3: 0.4382  loss_mask_3: 0.4958  loss_dice_3: 3.592  loss_ce_4: 0.4218  loss_mask_4: 0.4934  loss_dice_4: 3.586  loss_ce_5: 0.4244  loss_mask_5: 0.4967  loss_dice_5: 3.588  loss_ce_6: 0.4186  loss_mask_6: 0.4961  loss_dice_6: 3.592  loss_ce_7: 0.4057  loss_mask_7: 0.4945  loss_dice_7: 3.587  loss_ce_8: 0.4309  loss_mask_8: 0.4941  loss_dice_8: 3.591  time: 1.5008  data_time: 0.0812  lr: 8.2727e-06  max_mem: 21460M
[01/17 22:02:29] d2.utils.events INFO:  eta: 13:20:26  iter: 7619  total_loss: 45.66  loss_ce: 0.396  loss_mask: 0.4883  loss_dice: 3.624  loss_ce_0: 0.6572  loss_mask_0: 0.4705  loss_dice_0: 3.742  loss_ce_1: 0.412  loss_mask_1: 0.489  loss_dice_1: 3.65  loss_ce_2: 0.4409  loss_mask_2: 0.4897  loss_dice_2: 3.641  loss_ce_3: 0.4194  loss_mask_3: 0.4907  loss_dice_3: 3.625  loss_ce_4: 0.4058  loss_mask_4: 0.4898  loss_dice_4: 3.625  loss_ce_5: 0.3974  loss_mask_5: 0.4889  loss_dice_5: 3.628  loss_ce_6: 0.3899  loss_mask_6: 0.4901  loss_dice_6: 3.625  loss_ce_7: 0.3904  loss_mask_7: 0.4894  loss_dice_7: 3.622  loss_ce_8: 0.3902  loss_mask_8: 0.4881  loss_dice_8: 3.627  time: 1.5008  data_time: 0.0789  lr: 8.2681e-06  max_mem: 21460M
[01/17 22:02:58] d2.utils.events INFO:  eta: 13:19:59  iter: 7639  total_loss: 45.04  loss_ce: 0.3958  loss_mask: 0.5005  loss_dice: 3.59  loss_ce_0: 0.61  loss_mask_0: 0.4733  loss_dice_0: 3.725  loss_ce_1: 0.3866  loss_mask_1: 0.4969  loss_dice_1: 3.637  loss_ce_2: 0.4187  loss_mask_2: 0.4944  loss_dice_2: 3.615  loss_ce_3: 0.398  loss_mask_3: 0.4945  loss_dice_3: 3.593  loss_ce_4: 0.375  loss_mask_4: 0.4961  loss_dice_4: 3.6  loss_ce_5: 0.3959  loss_mask_5: 0.4971  loss_dice_5: 3.593  loss_ce_6: 0.4063  loss_mask_6: 0.4982  loss_dice_6: 3.592  loss_ce_7: 0.3735  loss_mask_7: 0.5001  loss_dice_7: 3.589  loss_ce_8: 0.3815  loss_mask_8: 0.4992  loss_dice_8: 3.599  time: 1.5008  data_time: 0.0746  lr: 8.2635e-06  max_mem: 21460M
[01/17 22:03:29] d2.utils.events INFO:  eta: 13:19:56  iter: 7659  total_loss: 45.4  loss_ce: 0.3935  loss_mask: 0.4891  loss_dice: 3.598  loss_ce_0: 0.6247  loss_mask_0: 0.4647  loss_dice_0: 3.732  loss_ce_1: 0.3991  loss_mask_1: 0.4881  loss_dice_1: 3.637  loss_ce_2: 0.4042  loss_mask_2: 0.4878  loss_dice_2: 3.616  loss_ce_3: 0.3767  loss_mask_3: 0.4888  loss_dice_3: 3.606  loss_ce_4: 0.3868  loss_mask_4: 0.4851  loss_dice_4: 3.608  loss_ce_5: 0.3891  loss_mask_5: 0.4869  loss_dice_5: 3.601  loss_ce_6: 0.3868  loss_mask_6: 0.4864  loss_dice_6: 3.596  loss_ce_7: 0.3809  loss_mask_7: 0.4874  loss_dice_7: 3.6  loss_ce_8: 0.396  loss_mask_8: 0.4871  loss_dice_8: 3.602  time: 1.5008  data_time: 0.0778  lr: 8.2589e-06  max_mem: 21460M
[01/17 22:03:59] d2.utils.events INFO:  eta: 13:19:49  iter: 7679  total_loss: 44.8  loss_ce: 0.3741  loss_mask: 0.4929  loss_dice: 3.574  loss_ce_0: 0.6031  loss_mask_0: 0.4699  loss_dice_0: 3.695  loss_ce_1: 0.3831  loss_mask_1: 0.4927  loss_dice_1: 3.604  loss_ce_2: 0.3847  loss_mask_2: 0.4903  loss_dice_2: 3.586  loss_ce_3: 0.3728  loss_mask_3: 0.4897  loss_dice_3: 3.58  loss_ce_4: 0.3775  loss_mask_4: 0.492  loss_dice_4: 3.581  loss_ce_5: 0.3803  loss_mask_5: 0.4939  loss_dice_5: 3.574  loss_ce_6: 0.3704  loss_mask_6: 0.4924  loss_dice_6: 3.568  loss_ce_7: 0.355  loss_mask_7: 0.4938  loss_dice_7: 3.576  loss_ce_8: 0.3698  loss_mask_8: 0.4938  loss_dice_8: 3.572  time: 1.5008  data_time: 0.0808  lr: 8.2543e-06  max_mem: 21460M
[01/17 22:04:29] d2.utils.events INFO:  eta: 13:19:38  iter: 7699  total_loss: 45.12  loss_ce: 0.4125  loss_mask: 0.492  loss_dice: 3.558  loss_ce_0: 0.6303  loss_mask_0: 0.4795  loss_dice_0: 3.692  loss_ce_1: 0.3973  loss_mask_1: 0.4952  loss_dice_1: 3.612  loss_ce_2: 0.4202  loss_mask_2: 0.4948  loss_dice_2: 3.573  loss_ce_3: 0.4105  loss_mask_3: 0.4944  loss_dice_3: 3.565  loss_ce_4: 0.4061  loss_mask_4: 0.4945  loss_dice_4: 3.559  loss_ce_5: 0.4087  loss_mask_5: 0.4958  loss_dice_5: 3.558  loss_ce_6: 0.3979  loss_mask_6: 0.4934  loss_dice_6: 3.554  loss_ce_7: 0.4184  loss_mask_7: 0.493  loss_dice_7: 3.558  loss_ce_8: 0.414  loss_mask_8: 0.4936  loss_dice_8: 3.554  time: 1.5008  data_time: 0.0864  lr: 8.2497e-06  max_mem: 21460M
[01/17 22:05:00] d2.utils.events INFO:  eta: 13:19:23  iter: 7719  total_loss: 45.46  loss_ce: 0.4471  loss_mask: 0.5014  loss_dice: 3.537  loss_ce_0: 0.6427  loss_mask_0: 0.4809  loss_dice_0: 3.665  loss_ce_1: 0.4162  loss_mask_1: 0.5046  loss_dice_1: 3.575  loss_ce_2: 0.4441  loss_mask_2: 0.502  loss_dice_2: 3.553  loss_ce_3: 0.4284  loss_mask_3: 0.4982  loss_dice_3: 3.544  loss_ce_4: 0.4487  loss_mask_4: 0.498  loss_dice_4: 3.537  loss_ce_5: 0.4252  loss_mask_5: 0.4977  loss_dice_5: 3.544  loss_ce_6: 0.4317  loss_mask_6: 0.4997  loss_dice_6: 3.541  loss_ce_7: 0.4299  loss_mask_7: 0.5003  loss_dice_7: 3.55  loss_ce_8: 0.4257  loss_mask_8: 0.5001  loss_dice_8: 3.536  time: 1.5009  data_time: 0.0858  lr: 8.2451e-06  max_mem: 21460M
[01/17 22:05:29] d2.utils.events INFO:  eta: 13:18:18  iter: 7739  total_loss: 45  loss_ce: 0.4257  loss_mask: 0.4898  loss_dice: 3.547  loss_ce_0: 0.6456  loss_mask_0: 0.4666  loss_dice_0: 3.69  loss_ce_1: 0.4225  loss_mask_1: 0.4842  loss_dice_1: 3.586  loss_ce_2: 0.4339  loss_mask_2: 0.4864  loss_dice_2: 3.559  loss_ce_3: 0.4265  loss_mask_3: 0.4854  loss_dice_3: 3.547  loss_ce_4: 0.4289  loss_mask_4: 0.4839  loss_dice_4: 3.545  loss_ce_5: 0.41  loss_mask_5: 0.4867  loss_dice_5: 3.552  loss_ce_6: 0.4335  loss_mask_6: 0.4856  loss_dice_6: 3.546  loss_ce_7: 0.4254  loss_mask_7: 0.4888  loss_dice_7: 3.543  loss_ce_8: 0.4231  loss_mask_8: 0.4868  loss_dice_8: 3.552  time: 1.5008  data_time: 0.0808  lr: 8.2405e-06  max_mem: 21460M
[01/17 22:05:59] d2.utils.events INFO:  eta: 13:17:50  iter: 7759  total_loss: 45.56  loss_ce: 0.4197  loss_mask: 0.4909  loss_dice: 3.623  loss_ce_0: 0.6495  loss_mask_0: 0.4621  loss_dice_0: 3.746  loss_ce_1: 0.3932  loss_mask_1: 0.4903  loss_dice_1: 3.657  loss_ce_2: 0.4256  loss_mask_2: 0.49  loss_dice_2: 3.635  loss_ce_3: 0.4024  loss_mask_3: 0.4926  loss_dice_3: 3.626  loss_ce_4: 0.4058  loss_mask_4: 0.4908  loss_dice_4: 3.629  loss_ce_5: 0.4165  loss_mask_5: 0.4874  loss_dice_5: 3.625  loss_ce_6: 0.4104  loss_mask_6: 0.4904  loss_dice_6: 3.622  loss_ce_7: 0.4255  loss_mask_7: 0.491  loss_dice_7: 3.629  loss_ce_8: 0.4132  loss_mask_8: 0.4892  loss_dice_8: 3.629  time: 1.5008  data_time: 0.0757  lr: 8.2359e-06  max_mem: 21460M
[01/17 22:06:28] d2.utils.events INFO:  eta: 13:17:16  iter: 7779  total_loss: 44.77  loss_ce: 0.4189  loss_mask: 0.5068  loss_dice: 3.539  loss_ce_0: 0.6137  loss_mask_0: 0.4843  loss_dice_0: 3.664  loss_ce_1: 0.3948  loss_mask_1: 0.5094  loss_dice_1: 3.575  loss_ce_2: 0.4294  loss_mask_2: 0.5019  loss_dice_2: 3.546  loss_ce_3: 0.3968  loss_mask_3: 0.5044  loss_dice_3: 3.541  loss_ce_4: 0.4133  loss_mask_4: 0.5035  loss_dice_4: 3.539  loss_ce_5: 0.3832  loss_mask_5: 0.5048  loss_dice_5: 3.543  loss_ce_6: 0.4078  loss_mask_6: 0.5049  loss_dice_6: 3.539  loss_ce_7: 0.3942  loss_mask_7: 0.5031  loss_dice_7: 3.543  loss_ce_8: 0.4137  loss_mask_8: 0.5056  loss_dice_8: 3.53  time: 1.5007  data_time: 0.0631  lr: 8.2314e-06  max_mem: 21460M
[01/17 22:06:57] d2.utils.events INFO:  eta: 13:16:29  iter: 7799  total_loss: 45.01  loss_ce: 0.4221  loss_mask: 0.4908  loss_dice: 3.573  loss_ce_0: 0.6341  loss_mask_0: 0.4721  loss_dice_0: 3.713  loss_ce_1: 0.4014  loss_mask_1: 0.4909  loss_dice_1: 3.619  loss_ce_2: 0.4269  loss_mask_2: 0.4919  loss_dice_2: 3.596  loss_ce_3: 0.4174  loss_mask_3: 0.4921  loss_dice_3: 3.578  loss_ce_4: 0.4199  loss_mask_4: 0.4921  loss_dice_4: 3.582  loss_ce_5: 0.4256  loss_mask_5: 0.4922  loss_dice_5: 3.589  loss_ce_6: 0.4206  loss_mask_6: 0.4937  loss_dice_6: 3.584  loss_ce_7: 0.4136  loss_mask_7: 0.4912  loss_dice_7: 3.58  loss_ce_8: 0.4202  loss_mask_8: 0.4927  loss_dice_8: 3.579  time: 1.5006  data_time: 0.0760  lr: 8.2268e-06  max_mem: 21460M
[01/17 22:07:27] d2.utils.events INFO:  eta: 13:15:59  iter: 7819  total_loss: 44.85  loss_ce: 0.4211  loss_mask: 0.4883  loss_dice: 3.57  loss_ce_0: 0.6013  loss_mask_0: 0.4655  loss_dice_0: 3.716  loss_ce_1: 0.3998  loss_mask_1: 0.4826  loss_dice_1: 3.614  loss_ce_2: 0.4134  loss_mask_2: 0.4837  loss_dice_2: 3.588  loss_ce_3: 0.398  loss_mask_3: 0.484  loss_dice_3: 3.568  loss_ce_4: 0.392  loss_mask_4: 0.4836  loss_dice_4: 3.569  loss_ce_5: 0.392  loss_mask_5: 0.4878  loss_dice_5: 3.568  loss_ce_6: 0.3973  loss_mask_6: 0.4879  loss_dice_6: 3.573  loss_ce_7: 0.3953  loss_mask_7: 0.4875  loss_dice_7: 3.572  loss_ce_8: 0.406  loss_mask_8: 0.4882  loss_dice_8: 3.57  time: 1.5005  data_time: 0.0777  lr: 8.2222e-06  max_mem: 21460M
[01/17 22:07:57] d2.utils.events INFO:  eta: 13:15:44  iter: 7839  total_loss: 45.09  loss_ce: 0.382  loss_mask: 0.4853  loss_dice: 3.574  loss_ce_0: 0.6326  loss_mask_0: 0.4666  loss_dice_0: 3.718  loss_ce_1: 0.396  loss_mask_1: 0.4894  loss_dice_1: 3.617  loss_ce_2: 0.4049  loss_mask_2: 0.4881  loss_dice_2: 3.59  loss_ce_3: 0.4096  loss_mask_3: 0.4902  loss_dice_3: 3.566  loss_ce_4: 0.3913  loss_mask_4: 0.4869  loss_dice_4: 3.563  loss_ce_5: 0.3991  loss_mask_5: 0.4866  loss_dice_5: 3.585  loss_ce_6: 0.3878  loss_mask_6: 0.4864  loss_dice_6: 3.578  loss_ce_7: 0.3861  loss_mask_7: 0.4873  loss_dice_7: 3.577  loss_ce_8: 0.3927  loss_mask_8: 0.4856  loss_dice_8: 3.577  time: 1.5005  data_time: 0.0769  lr: 8.2176e-06  max_mem: 21460M
[01/17 22:08:27] d2.utils.events INFO:  eta: 13:15:17  iter: 7859  total_loss: 45.04  loss_ce: 0.3987  loss_mask: 0.4895  loss_dice: 3.555  loss_ce_0: 0.6218  loss_mask_0: 0.4705  loss_dice_0: 3.679  loss_ce_1: 0.4062  loss_mask_1: 0.4887  loss_dice_1: 3.599  loss_ce_2: 0.4191  loss_mask_2: 0.4887  loss_dice_2: 3.573  loss_ce_3: 0.4023  loss_mask_3: 0.4919  loss_dice_3: 3.569  loss_ce_4: 0.4006  loss_mask_4: 0.4922  loss_dice_4: 3.566  loss_ce_5: 0.3998  loss_mask_5: 0.4928  loss_dice_5: 3.567  loss_ce_6: 0.3912  loss_mask_6: 0.492  loss_dice_6: 3.557  loss_ce_7: 0.3898  loss_mask_7: 0.4895  loss_dice_7: 3.563  loss_ce_8: 0.389  loss_mask_8: 0.4885  loss_dice_8: 3.557  time: 1.5005  data_time: 0.0769  lr: 8.213e-06  max_mem: 21460M
[01/17 22:08:56] d2.utils.events INFO:  eta: 13:14:26  iter: 7879  total_loss: 45.54  loss_ce: 0.4193  loss_mask: 0.4959  loss_dice: 3.595  loss_ce_0: 0.6306  loss_mask_0: 0.47  loss_dice_0: 3.713  loss_ce_1: 0.4276  loss_mask_1: 0.4961  loss_dice_1: 3.621  loss_ce_2: 0.4423  loss_mask_2: 0.4967  loss_dice_2: 3.596  loss_ce_3: 0.4151  loss_mask_3: 0.4943  loss_dice_3: 3.583  loss_ce_4: 0.4055  loss_mask_4: 0.4959  loss_dice_4: 3.59  loss_ce_5: 0.3973  loss_mask_5: 0.4987  loss_dice_5: 3.591  loss_ce_6: 0.4119  loss_mask_6: 0.4991  loss_dice_6: 3.591  loss_ce_7: 0.4006  loss_mask_7: 0.4972  loss_dice_7: 3.585  loss_ce_8: 0.3928  loss_mask_8: 0.4995  loss_dice_8: 3.582  time: 1.5004  data_time: 0.0693  lr: 8.2084e-06  max_mem: 21460M
[01/17 22:09:26] d2.utils.events INFO:  eta: 13:13:57  iter: 7899  total_loss: 44.99  loss_ce: 0.3966  loss_mask: 0.4921  loss_dice: 3.575  loss_ce_0: 0.6044  loss_mask_0: 0.472  loss_dice_0: 3.716  loss_ce_1: 0.3769  loss_mask_1: 0.4937  loss_dice_1: 3.612  loss_ce_2: 0.4195  loss_mask_2: 0.4904  loss_dice_2: 3.583  loss_ce_3: 0.4009  loss_mask_3: 0.4926  loss_dice_3: 3.576  loss_ce_4: 0.3895  loss_mask_4: 0.4929  loss_dice_4: 3.569  loss_ce_5: 0.3879  loss_mask_5: 0.4916  loss_dice_5: 3.577  loss_ce_6: 0.3857  loss_mask_6: 0.4906  loss_dice_6: 3.562  loss_ce_7: 0.3905  loss_mask_7: 0.492  loss_dice_7: 3.575  loss_ce_8: 0.3866  loss_mask_8: 0.4914  loss_dice_8: 3.575  time: 1.5003  data_time: 0.0739  lr: 8.2038e-06  max_mem: 21460M
[01/17 22:09:55] d2.utils.events INFO:  eta: 13:13:27  iter: 7919  total_loss: 45.26  loss_ce: 0.4352  loss_mask: 0.4929  loss_dice: 3.564  loss_ce_0: 0.6485  loss_mask_0: 0.4739  loss_dice_0: 3.693  loss_ce_1: 0.42  loss_mask_1: 0.4942  loss_dice_1: 3.601  loss_ce_2: 0.4462  loss_mask_2: 0.4929  loss_dice_2: 3.576  loss_ce_3: 0.419  loss_mask_3: 0.4934  loss_dice_3: 3.569  loss_ce_4: 0.4289  loss_mask_4: 0.494  loss_dice_4: 3.56  loss_ce_5: 0.4293  loss_mask_5: 0.4932  loss_dice_5: 3.566  loss_ce_6: 0.4286  loss_mask_6: 0.4952  loss_dice_6: 3.556  loss_ce_7: 0.4215  loss_mask_7: 0.4943  loss_dice_7: 3.558  loss_ce_8: 0.4157  loss_mask_8: 0.493  loss_dice_8: 3.559  time: 1.5002  data_time: 0.0644  lr: 8.1992e-06  max_mem: 21460M
[01/17 22:10:24] d2.utils.events INFO:  eta: 13:12:45  iter: 7939  total_loss: 44.83  loss_ce: 0.3879  loss_mask: 0.4854  loss_dice: 3.566  loss_ce_0: 0.6164  loss_mask_0: 0.4667  loss_dice_0: 3.717  loss_ce_1: 0.3621  loss_mask_1: 0.4815  loss_dice_1: 3.621  loss_ce_2: 0.3849  loss_mask_2: 0.4782  loss_dice_2: 3.589  loss_ce_3: 0.3852  loss_mask_3: 0.4855  loss_dice_3: 3.576  loss_ce_4: 0.3826  loss_mask_4: 0.485  loss_dice_4: 3.566  loss_ce_5: 0.3638  loss_mask_5: 0.4859  loss_dice_5: 3.57  loss_ce_6: 0.379  loss_mask_6: 0.4845  loss_dice_6: 3.559  loss_ce_7: 0.3827  loss_mask_7: 0.4849  loss_dice_7: 3.558  loss_ce_8: 0.3729  loss_mask_8: 0.4855  loss_dice_8: 3.563  time: 1.5001  data_time: 0.0683  lr: 8.1946e-06  max_mem: 21460M
[01/17 22:10:54] d2.utils.events INFO:  eta: 13:12:26  iter: 7959  total_loss: 44.64  loss_ce: 0.405  loss_mask: 0.4871  loss_dice: 3.533  loss_ce_0: 0.6291  loss_mask_0: 0.4656  loss_dice_0: 3.657  loss_ce_1: 0.3903  loss_mask_1: 0.489  loss_dice_1: 3.56  loss_ce_2: 0.4296  loss_mask_2: 0.4866  loss_dice_2: 3.539  loss_ce_3: 0.3898  loss_mask_3: 0.4867  loss_dice_3: 3.528  loss_ce_4: 0.3729  loss_mask_4: 0.4873  loss_dice_4: 3.541  loss_ce_5: 0.3866  loss_mask_5: 0.4867  loss_dice_5: 3.522  loss_ce_6: 0.3858  loss_mask_6: 0.4858  loss_dice_6: 3.539  loss_ce_7: 0.3872  loss_mask_7: 0.4888  loss_dice_7: 3.538  loss_ce_8: 0.3763  loss_mask_8: 0.4891  loss_dice_8: 3.532  time: 1.5001  data_time: 0.0705  lr: 8.19e-06  max_mem: 21492M
[01/17 22:11:24] d2.utils.events INFO:  eta: 13:11:59  iter: 7979  total_loss: 45.43  loss_ce: 0.366  loss_mask: 0.483  loss_dice: 3.605  loss_ce_0: 0.5999  loss_mask_0: 0.4687  loss_dice_0: 3.725  loss_ce_1: 0.3871  loss_mask_1: 0.4842  loss_dice_1: 3.631  loss_ce_2: 0.4058  loss_mask_2: 0.4823  loss_dice_2: 3.604  loss_ce_3: 0.3915  loss_mask_3: 0.4787  loss_dice_3: 3.605  loss_ce_4: 0.3882  loss_mask_4: 0.4818  loss_dice_4: 3.6  loss_ce_5: 0.3927  loss_mask_5: 0.4818  loss_dice_5: 3.598  loss_ce_6: 0.3803  loss_mask_6: 0.4825  loss_dice_6: 3.591  loss_ce_7: 0.3702  loss_mask_7: 0.482  loss_dice_7: 3.601  loss_ce_8: 0.374  loss_mask_8: 0.482  loss_dice_8: 3.598  time: 1.5001  data_time: 0.0787  lr: 8.1854e-06  max_mem: 21492M
[01/17 22:11:54] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 22:11:54] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 22:11:54] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 22:11:55] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 22:12:07] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0065 s/iter. Inference: 0.1645 s/iter. Eval: 0.1731 s/iter. Total: 0.3441 s/iter. ETA=0:06:12
[01/17 22:12:13] d2.evaluation.evaluator INFO: Inference done 25/1093. Dataloading: 0.0088 s/iter. Inference: 0.1714 s/iter. Eval: 0.1826 s/iter. Total: 0.3629 s/iter. ETA=0:06:27
[01/17 22:12:18] d2.evaluation.evaluator INFO: Inference done 41/1093. Dataloading: 0.0088 s/iter. Inference: 0.1601 s/iter. Eval: 0.1765 s/iter. Total: 0.3456 s/iter. ETA=0:06:03
[01/17 22:12:23] d2.evaluation.evaluator INFO: Inference done 56/1093. Dataloading: 0.0102 s/iter. Inference: 0.1567 s/iter. Eval: 0.1821 s/iter. Total: 0.3492 s/iter. ETA=0:06:02
[01/17 22:12:28] d2.evaluation.evaluator INFO: Inference done 70/1093. Dataloading: 0.0106 s/iter. Inference: 0.1582 s/iter. Eval: 0.1822 s/iter. Total: 0.3512 s/iter. ETA=0:05:59
[01/17 22:12:33] d2.evaluation.evaluator INFO: Inference done 85/1093. Dataloading: 0.0105 s/iter. Inference: 0.1565 s/iter. Eval: 0.1844 s/iter. Total: 0.3516 s/iter. ETA=0:05:54
[01/17 22:12:39] d2.evaluation.evaluator INFO: Inference done 101/1093. Dataloading: 0.0107 s/iter. Inference: 0.1540 s/iter. Eval: 0.1819 s/iter. Total: 0.3468 s/iter. ETA=0:05:44
[01/17 22:12:44] d2.evaluation.evaluator INFO: Inference done 116/1093. Dataloading: 0.0106 s/iter. Inference: 0.1552 s/iter. Eval: 0.1813 s/iter. Total: 0.3473 s/iter. ETA=0:05:39
[01/17 22:12:49] d2.evaluation.evaluator INFO: Inference done 131/1093. Dataloading: 0.0106 s/iter. Inference: 0.1557 s/iter. Eval: 0.1813 s/iter. Total: 0.3477 s/iter. ETA=0:05:34
[01/17 22:12:54] d2.evaluation.evaluator INFO: Inference done 147/1093. Dataloading: 0.0104 s/iter. Inference: 0.1557 s/iter. Eval: 0.1782 s/iter. Total: 0.3445 s/iter. ETA=0:05:25
[01/17 22:13:00] d2.evaluation.evaluator INFO: Inference done 164/1093. Dataloading: 0.0103 s/iter. Inference: 0.1545 s/iter. Eval: 0.1767 s/iter. Total: 0.3416 s/iter. ETA=0:05:17
[01/17 22:13:05] d2.evaluation.evaluator INFO: Inference done 178/1093. Dataloading: 0.0104 s/iter. Inference: 0.1559 s/iter. Eval: 0.1773 s/iter. Total: 0.3437 s/iter. ETA=0:05:14
[01/17 22:13:10] d2.evaluation.evaluator INFO: Inference done 193/1093. Dataloading: 0.0103 s/iter. Inference: 0.1565 s/iter. Eval: 0.1760 s/iter. Total: 0.3429 s/iter. ETA=0:05:08
[01/17 22:13:15] d2.evaluation.evaluator INFO: Inference done 208/1093. Dataloading: 0.0104 s/iter. Inference: 0.1572 s/iter. Eval: 0.1765 s/iter. Total: 0.3442 s/iter. ETA=0:05:04
[01/17 22:13:20] d2.evaluation.evaluator INFO: Inference done 224/1093. Dataloading: 0.0104 s/iter. Inference: 0.1570 s/iter. Eval: 0.1755 s/iter. Total: 0.3430 s/iter. ETA=0:04:58
[01/17 22:13:26] d2.evaluation.evaluator INFO: Inference done 239/1093. Dataloading: 0.0107 s/iter. Inference: 0.1561 s/iter. Eval: 0.1762 s/iter. Total: 0.3432 s/iter. ETA=0:04:53
[01/17 22:13:31] d2.evaluation.evaluator INFO: Inference done 254/1093. Dataloading: 0.0107 s/iter. Inference: 0.1554 s/iter. Eval: 0.1766 s/iter. Total: 0.3428 s/iter. ETA=0:04:47
[01/17 22:13:36] d2.evaluation.evaluator INFO: Inference done 270/1093. Dataloading: 0.0107 s/iter. Inference: 0.1545 s/iter. Eval: 0.1759 s/iter. Total: 0.3412 s/iter. ETA=0:04:40
[01/17 22:13:41] d2.evaluation.evaluator INFO: Inference done 285/1093. Dataloading: 0.0106 s/iter. Inference: 0.1544 s/iter. Eval: 0.1759 s/iter. Total: 0.3410 s/iter. ETA=0:04:35
[01/17 22:13:46] d2.evaluation.evaluator INFO: Inference done 300/1093. Dataloading: 0.0105 s/iter. Inference: 0.1543 s/iter. Eval: 0.1757 s/iter. Total: 0.3406 s/iter. ETA=0:04:30
[01/17 22:13:51] d2.evaluation.evaluator INFO: Inference done 314/1093. Dataloading: 0.0106 s/iter. Inference: 0.1548 s/iter. Eval: 0.1768 s/iter. Total: 0.3423 s/iter. ETA=0:04:26
[01/17 22:13:56] d2.evaluation.evaluator INFO: Inference done 329/1093. Dataloading: 0.0105 s/iter. Inference: 0.1549 s/iter. Eval: 0.1771 s/iter. Total: 0.3426 s/iter. ETA=0:04:21
[01/17 22:14:01] d2.evaluation.evaluator INFO: Inference done 347/1093. Dataloading: 0.0103 s/iter. Inference: 0.1547 s/iter. Eval: 0.1744 s/iter. Total: 0.3395 s/iter. ETA=0:04:13
[01/17 22:14:07] d2.evaluation.evaluator INFO: Inference done 362/1093. Dataloading: 0.0103 s/iter. Inference: 0.1549 s/iter. Eval: 0.1741 s/iter. Total: 0.3395 s/iter. ETA=0:04:08
[01/17 22:14:12] d2.evaluation.evaluator INFO: Inference done 377/1093. Dataloading: 0.0104 s/iter. Inference: 0.1553 s/iter. Eval: 0.1740 s/iter. Total: 0.3398 s/iter. ETA=0:04:03
[01/17 22:14:17] d2.evaluation.evaluator INFO: Inference done 391/1093. Dataloading: 0.0104 s/iter. Inference: 0.1560 s/iter. Eval: 0.1744 s/iter. Total: 0.3408 s/iter. ETA=0:03:59
[01/17 22:14:22] d2.evaluation.evaluator INFO: Inference done 406/1093. Dataloading: 0.0104 s/iter. Inference: 0.1563 s/iter. Eval: 0.1741 s/iter. Total: 0.3409 s/iter. ETA=0:03:54
[01/17 22:14:27] d2.evaluation.evaluator INFO: Inference done 421/1093. Dataloading: 0.0103 s/iter. Inference: 0.1559 s/iter. Eval: 0.1749 s/iter. Total: 0.3413 s/iter. ETA=0:03:49
[01/17 22:14:32] d2.evaluation.evaluator INFO: Inference done 437/1093. Dataloading: 0.0103 s/iter. Inference: 0.1556 s/iter. Eval: 0.1744 s/iter. Total: 0.3405 s/iter. ETA=0:03:43
[01/17 22:14:38] d2.evaluation.evaluator INFO: Inference done 453/1093. Dataloading: 0.0103 s/iter. Inference: 0.1555 s/iter. Eval: 0.1738 s/iter. Total: 0.3397 s/iter. ETA=0:03:37
[01/17 22:14:43] d2.evaluation.evaluator INFO: Inference done 470/1093. Dataloading: 0.0102 s/iter. Inference: 0.1552 s/iter. Eval: 0.1729 s/iter. Total: 0.3385 s/iter. ETA=0:03:30
[01/17 22:14:48] d2.evaluation.evaluator INFO: Inference done 486/1093. Dataloading: 0.0102 s/iter. Inference: 0.1550 s/iter. Eval: 0.1726 s/iter. Total: 0.3379 s/iter. ETA=0:03:25
[01/17 22:14:53] d2.evaluation.evaluator INFO: Inference done 503/1093. Dataloading: 0.0102 s/iter. Inference: 0.1549 s/iter. Eval: 0.1717 s/iter. Total: 0.3369 s/iter. ETA=0:03:18
[01/17 22:14:58] d2.evaluation.evaluator INFO: Inference done 518/1093. Dataloading: 0.0101 s/iter. Inference: 0.1552 s/iter. Eval: 0.1717 s/iter. Total: 0.3372 s/iter. ETA=0:03:13
[01/17 22:15:04] d2.evaluation.evaluator INFO: Inference done 533/1093. Dataloading: 0.0102 s/iter. Inference: 0.1548 s/iter. Eval: 0.1724 s/iter. Total: 0.3375 s/iter. ETA=0:03:09
[01/17 22:15:09] d2.evaluation.evaluator INFO: Inference done 547/1093. Dataloading: 0.0102 s/iter. Inference: 0.1552 s/iter. Eval: 0.1726 s/iter. Total: 0.3381 s/iter. ETA=0:03:04
[01/17 22:15:14] d2.evaluation.evaluator INFO: Inference done 562/1093. Dataloading: 0.0102 s/iter. Inference: 0.1549 s/iter. Eval: 0.1729 s/iter. Total: 0.3381 s/iter. ETA=0:02:59
[01/17 22:15:19] d2.evaluation.evaluator INFO: Inference done 579/1093. Dataloading: 0.0103 s/iter. Inference: 0.1552 s/iter. Eval: 0.1716 s/iter. Total: 0.3371 s/iter. ETA=0:02:53
[01/17 22:15:24] d2.evaluation.evaluator INFO: Inference done 595/1093. Dataloading: 0.0103 s/iter. Inference: 0.1547 s/iter. Eval: 0.1715 s/iter. Total: 0.3366 s/iter. ETA=0:02:47
[01/17 22:15:29] d2.evaluation.evaluator INFO: Inference done 609/1093. Dataloading: 0.0104 s/iter. Inference: 0.1546 s/iter. Eval: 0.1721 s/iter. Total: 0.3372 s/iter. ETA=0:02:43
[01/17 22:15:34] d2.evaluation.evaluator INFO: Inference done 625/1093. Dataloading: 0.0103 s/iter. Inference: 0.1546 s/iter. Eval: 0.1719 s/iter. Total: 0.3369 s/iter. ETA=0:02:37
[01/17 22:15:40] d2.evaluation.evaluator INFO: Inference done 641/1093. Dataloading: 0.0103 s/iter. Inference: 0.1545 s/iter. Eval: 0.1719 s/iter. Total: 0.3368 s/iter. ETA=0:02:32
[01/17 22:15:45] d2.evaluation.evaluator INFO: Inference done 658/1093. Dataloading: 0.0103 s/iter. Inference: 0.1542 s/iter. Eval: 0.1715 s/iter. Total: 0.3361 s/iter. ETA=0:02:26
[01/17 22:15:50] d2.evaluation.evaluator INFO: Inference done 672/1093. Dataloading: 0.0104 s/iter. Inference: 0.1542 s/iter. Eval: 0.1721 s/iter. Total: 0.3368 s/iter. ETA=0:02:21
[01/17 22:15:55] d2.evaluation.evaluator INFO: Inference done 691/1093. Dataloading: 0.0103 s/iter. Inference: 0.1540 s/iter. Eval: 0.1707 s/iter. Total: 0.3351 s/iter. ETA=0:02:14
[01/17 22:16:00] d2.evaluation.evaluator INFO: Inference done 706/1093. Dataloading: 0.0104 s/iter. Inference: 0.1539 s/iter. Eval: 0.1710 s/iter. Total: 0.3354 s/iter. ETA=0:02:09
[01/17 22:16:06] d2.evaluation.evaluator INFO: Inference done 719/1093. Dataloading: 0.0104 s/iter. Inference: 0.1539 s/iter. Eval: 0.1720 s/iter. Total: 0.3364 s/iter. ETA=0:02:05
[01/17 22:16:11] d2.evaluation.evaluator INFO: Inference done 736/1093. Dataloading: 0.0104 s/iter. Inference: 0.1539 s/iter. Eval: 0.1712 s/iter. Total: 0.3356 s/iter. ETA=0:01:59
[01/17 22:16:16] d2.evaluation.evaluator INFO: Inference done 753/1093. Dataloading: 0.0103 s/iter. Inference: 0.1537 s/iter. Eval: 0.1709 s/iter. Total: 0.3351 s/iter. ETA=0:01:53
[01/17 22:16:21] d2.evaluation.evaluator INFO: Inference done 768/1093. Dataloading: 0.0103 s/iter. Inference: 0.1536 s/iter. Eval: 0.1714 s/iter. Total: 0.3354 s/iter. ETA=0:01:48
[01/17 22:16:26] d2.evaluation.evaluator INFO: Inference done 785/1093. Dataloading: 0.0104 s/iter. Inference: 0.1533 s/iter. Eval: 0.1710 s/iter. Total: 0.3347 s/iter. ETA=0:01:43
[01/17 22:16:32] d2.evaluation.evaluator INFO: Inference done 802/1093. Dataloading: 0.0103 s/iter. Inference: 0.1531 s/iter. Eval: 0.1707 s/iter. Total: 0.3342 s/iter. ETA=0:01:37
[01/17 22:16:37] d2.evaluation.evaluator INFO: Inference done 818/1093. Dataloading: 0.0104 s/iter. Inference: 0.1531 s/iter. Eval: 0.1703 s/iter. Total: 0.3340 s/iter. ETA=0:01:31
[01/17 22:16:42] d2.evaluation.evaluator INFO: Inference done 834/1093. Dataloading: 0.0104 s/iter. Inference: 0.1531 s/iter. Eval: 0.1700 s/iter. Total: 0.3336 s/iter. ETA=0:01:26
[01/17 22:16:47] d2.evaluation.evaluator INFO: Inference done 850/1093. Dataloading: 0.0104 s/iter. Inference: 0.1532 s/iter. Eval: 0.1697 s/iter. Total: 0.3334 s/iter. ETA=0:01:21
[01/17 22:16:52] d2.evaluation.evaluator INFO: Inference done 864/1093. Dataloading: 0.0104 s/iter. Inference: 0.1531 s/iter. Eval: 0.1702 s/iter. Total: 0.3338 s/iter. ETA=0:01:16
[01/17 22:16:57] d2.evaluation.evaluator INFO: Inference done 880/1093. Dataloading: 0.0103 s/iter. Inference: 0.1529 s/iter. Eval: 0.1701 s/iter. Total: 0.3335 s/iter. ETA=0:01:11
[01/17 22:17:02] d2.evaluation.evaluator INFO: Inference done 893/1093. Dataloading: 0.0104 s/iter. Inference: 0.1531 s/iter. Eval: 0.1707 s/iter. Total: 0.3342 s/iter. ETA=0:01:06
[01/17 22:17:07] d2.evaluation.evaluator INFO: Inference done 910/1093. Dataloading: 0.0103 s/iter. Inference: 0.1531 s/iter. Eval: 0.1702 s/iter. Total: 0.3337 s/iter. ETA=0:01:01
[01/17 22:17:12] d2.evaluation.evaluator INFO: Inference done 924/1093. Dataloading: 0.0103 s/iter. Inference: 0.1530 s/iter. Eval: 0.1707 s/iter. Total: 0.3341 s/iter. ETA=0:00:56
[01/17 22:17:17] d2.evaluation.evaluator INFO: Inference done 940/1093. Dataloading: 0.0103 s/iter. Inference: 0.1528 s/iter. Eval: 0.1706 s/iter. Total: 0.3337 s/iter. ETA=0:00:51
[01/17 22:17:23] d2.evaluation.evaluator INFO: Inference done 955/1093. Dataloading: 0.0103 s/iter. Inference: 0.1528 s/iter. Eval: 0.1707 s/iter. Total: 0.3339 s/iter. ETA=0:00:46
[01/17 22:17:28] d2.evaluation.evaluator INFO: Inference done 970/1093. Dataloading: 0.0103 s/iter. Inference: 0.1527 s/iter. Eval: 0.1709 s/iter. Total: 0.3340 s/iter. ETA=0:00:41
[01/17 22:17:33] d2.evaluation.evaluator INFO: Inference done 985/1093. Dataloading: 0.0103 s/iter. Inference: 0.1530 s/iter. Eval: 0.1709 s/iter. Total: 0.3343 s/iter. ETA=0:00:36
[01/17 22:17:38] d2.evaluation.evaluator INFO: Inference done 1000/1093. Dataloading: 0.0103 s/iter. Inference: 0.1531 s/iter. Eval: 0.1710 s/iter. Total: 0.3345 s/iter. ETA=0:00:31
[01/17 22:17:43] d2.evaluation.evaluator INFO: Inference done 1016/1093. Dataloading: 0.0103 s/iter. Inference: 0.1532 s/iter. Eval: 0.1708 s/iter. Total: 0.3344 s/iter. ETA=0:00:25
[01/17 22:17:49] d2.evaluation.evaluator INFO: Inference done 1032/1093. Dataloading: 0.0102 s/iter. Inference: 0.1531 s/iter. Eval: 0.1708 s/iter. Total: 0.3342 s/iter. ETA=0:00:20
[01/17 22:17:54] d2.evaluation.evaluator INFO: Inference done 1048/1093. Dataloading: 0.0102 s/iter. Inference: 0.1530 s/iter. Eval: 0.1707 s/iter. Total: 0.3340 s/iter. ETA=0:00:15
[01/17 22:17:59] d2.evaluation.evaluator INFO: Inference done 1064/1093. Dataloading: 0.0102 s/iter. Inference: 0.1529 s/iter. Eval: 0.1706 s/iter. Total: 0.3338 s/iter. ETA=0:00:09
[01/17 22:18:04] d2.evaluation.evaluator INFO: Inference done 1081/1093. Dataloading: 0.0102 s/iter. Inference: 0.1528 s/iter. Eval: 0.1702 s/iter. Total: 0.3333 s/iter. ETA=0:00:03
[01/17 22:18:08] d2.evaluation.evaluator INFO: Total inference time: 0:06:02.623782 (0.333294 s / iter per device, on 4 devices)
[01/17 22:18:08] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:46 (0.152626 s / iter per device, on 4 devices)
[01/17 22:18:32] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 3.2885769328136782, 'mIoU': 11.147415387036695, 'fwIoU': 25.638453581900272, 'IoU-0': nan, 'IoU-1': 94.97043119069791, 'IoU-2': 42.72242013626412, 'IoU-3': 50.80270355492795, 'IoU-4': 44.065584500334076, 'IoU-5': 37.38527042186491, 'IoU-6': 35.84292702074666, 'IoU-7': 29.878994157244115, 'IoU-8': 8.872059740476528, 'IoU-9': 19.61208969991826, 'IoU-10': 20.73601116732159, 'IoU-11': 26.383221729103358, 'IoU-12': 26.84079089846153, 'IoU-13': 24.849169624151777, 'IoU-14': 19.63012740750031, 'IoU-15': 18.249572028874482, 'IoU-16': 19.997646050130665, 'IoU-17': 16.58128370888831, 'IoU-18': 16.007731243051023, 'IoU-19': 17.266989130702594, 'IoU-20': 18.1775348982733, 'IoU-21': 16.566906671701172, 'IoU-22': 18.44506479153427, 'IoU-23': 14.96083690895924, 'IoU-24': 17.10185487862114, 'IoU-25': 15.809892390609615, 'IoU-26': 15.554005501511611, 'IoU-27': 19.369125433442374, 'IoU-28': 16.253713329468283, 'IoU-29': 15.563808732647644, 'IoU-30': 20.396994996592372, 'IoU-31': 19.617562632602834, 'IoU-32': 19.056057583918268, 'IoU-33': 19.982885862893827, 'IoU-34': 20.567235882658355, 'IoU-35': 21.871650475840227, 'IoU-36': 23.310486276695993, 'IoU-37': 22.684831899555924, 'IoU-38': 22.385872827133657, 'IoU-39': 22.813060034418932, 'IoU-40': 23.198794296668233, 'IoU-41': 22.579755284846687, 'IoU-42': 23.129169224606656, 'IoU-43': 21.380841545263003, 'IoU-44': 20.384295466794487, 'IoU-45': 20.41468658769394, 'IoU-46': 18.357290778891784, 'IoU-47': 18.505056843358837, 'IoU-48': 20.29863253326034, 'IoU-49': 18.383536448296596, 'IoU-50': 20.268918207430023, 'IoU-51': 20.606540089349267, 'IoU-52': 20.59080407580809, 'IoU-53': 19.50290399209377, 'IoU-54': 18.0992495173531, 'IoU-55': 19.8250773378484, 'IoU-56': 16.94066027904688, 'IoU-57': 18.126744027042264, 'IoU-58': 17.604348773204073, 'IoU-59': 16.372302472087558, 'IoU-60': 14.516437890019008, 'IoU-61': 17.45951989705445, 'IoU-62': 14.267917916397058, 'IoU-63': 16.749691187901917, 'IoU-64': 17.222366804126064, 'IoU-65': 14.318226449059518, 'IoU-66': 15.782542147950027, 'IoU-67': 14.077507677678804, 'IoU-68': 13.841546473405108, 'IoU-69': 14.0028653070691, 'IoU-70': 13.155836535651552, 'IoU-71': 12.426309651139402, 'IoU-72': 12.93141107871006, 'IoU-73': 14.008932812880944, 'IoU-74': 13.88974457414092, 'IoU-75': 9.805993626617878, 'IoU-76': 16.151230886584045, 'IoU-77': 10.276040234679213, 'IoU-78': 12.089699439245646, 'IoU-79': 12.124519571157217, 'IoU-80': 12.489175772782685, 'IoU-81': 12.817956532139496, 'IoU-82': 8.487918449150463, 'IoU-83': 11.248793518187245, 'IoU-84': 12.302476831558714, 'IoU-85': 12.913258076841505, 'IoU-86': 8.674002361018406, 'IoU-87': 10.13415251685072, 'IoU-88': 11.524491779116214, 'IoU-89': 10.84291805710518, 'IoU-90': 9.684041834512737, 'IoU-91': 11.607647039170242, 'IoU-92': 8.164274343402836, 'IoU-93': 11.23296338731467, 'IoU-94': 10.733863888461313, 'IoU-95': 10.452957681758884, 'IoU-96': 8.960944180982244, 'IoU-97': 10.318147314214993, 'IoU-98': 9.779756927392345, 'IoU-99': 8.901974357427907, 'IoU-100': 9.551359478178671, 'IoU-101': 7.822191464738963, 'IoU-102': 7.944142004845594, 'IoU-103': 10.56621761821442, 'IoU-104': 5.807926299584695, 'IoU-105': 8.552496034628916, 'IoU-106': 8.319838256996032, 'IoU-107': 5.5901098618872815, 'IoU-108': 9.59433181422818, 'IoU-109': 5.414148820277724, 'IoU-110': 7.062635391450072, 'IoU-111': 6.927647157740467, 'IoU-112': 6.708890828282117, 'IoU-113': 7.553902252662657, 'IoU-114': 6.838417352274151, 'IoU-115': 6.809845601240025, 'IoU-116': 3.4671325199004843, 'IoU-117': 3.295911653069097, 'IoU-118': 8.467499320636897, 'IoU-119': 5.343472195577601, 'IoU-120': 4.837284470356103, 'IoU-121': 4.883264741607515, 'IoU-122': 5.766008674739478, 'IoU-123': 4.510865825802193, 'IoU-124': 4.546742622821019, 'IoU-125': 5.424224624927368, 'IoU-126': 4.471107641718051, 'IoU-127': 3.4864500536314056, 'IoU-128': 4.847803568142479, 'IoU-129': 3.8270346078545012, 'IoU-130': 3.8445993735475392, 'IoU-131': 2.990777429135111, 'IoU-132': 4.523444576429475, 'IoU-133': 2.03869890123403, 'IoU-134': 2.9488347320062407, 'IoU-135': 2.953603695401128, 'IoU-136': 3.4996305651741304, 'IoU-137': 4.641069270794802, 'IoU-138': 3.0513551375477546, 'IoU-139': 2.3465614168919258, 'IoU-140': 4.20405469603045, 'IoU-141': 3.012402176773091, 'IoU-142': 2.2967510288800335, 'IoU-143': 2.782699214264692, 'IoU-144': 3.2818719748931886, 'IoU-145': 2.148274393161189, 'IoU-146': 1.6099646070767915, 'IoU-147': 2.198204349832183, 'IoU-148': 1.6427671418362357, 'IoU-149': 2.194000653498715, 'IoU-150': 1.1885912701300139, 'IoU-151': 2.33202842114807, 'IoU-152': 2.6136487159609296, 'IoU-153': 1.2660895492046282, 'IoU-154': 1.2472100538355493, 'IoU-155': 1.4576213304857553, 'IoU-156': 0.8305866448127965, 'IoU-157': 1.011229018223549, 'IoU-158': 1.838544898006619, 'IoU-159': 1.4660374045354716, 'IoU-160': 0.3634526118388026, 'IoU-161': 2.6804187199410694, 'IoU-162': 1.6344527147450156, 'IoU-163': 1.2715537683387634, 'IoU-164': 1.2692424999788194, 'IoU-165': 0.8407595175866738, 'IoU-166': 0.6823737377880221, 'IoU-167': 0.4183124745841758, 'IoU-168': 0.8460623249888835, 'IoU-169': 2.261456078113942, 'IoU-170': 1.389176539284427, 'IoU-171': 1.0540855040902635, 'IoU-172': 1.6591233900626345, 'IoU-173': 1.1913487998837324, 'IoU-174': 0.41663393143126504, 'IoU-175': 1.389728484428444, 'IoU-176': 1.272604037619779, 'IoU-177': 1.56047265987025, 'IoU-178': 0.6609619100627436, 'IoU-179': 1.8562453577099323, 'IoU-180': 0.8707433150122803, 'IoU-181': 0.260157261174986, 'IoU-182': 0.0, 'IoU-183': 0.5274097432622896, 'IoU-184': 0.047785698456083325, 'IoU-185': 0.0, 'IoU-186': 0.40815975062644777, 'IoU-187': 2.358573850704682, 'IoU-188': 1.0863028265778774, 'IoU-189': 0.040164726218923176, 'IoU-190': 0.6533653148787764, 'IoU-191': 0.3610877644425148, 'mACC': 18.62726092406192, 'pACC': 36.841145861480385, 'ACC-0': nan, 'ACC-1': 98.74631834647234, 'ACC-2': 51.49571559160779, 'ACC-3': 64.22168868471708, 'ACC-4': 60.66803712300717, 'ACC-5': 55.539791333605834, 'ACC-6': 57.00757281787813, 'ACC-7': 44.62135051467451, 'ACC-8': 10.032957014858416, 'ACC-9': 30.97826657697043, 'ACC-10': 33.623843716764064, 'ACC-11': 39.920783987346475, 'ACC-12': 45.49460737567768, 'ACC-13': 47.01949148651382, 'ACC-14': 34.3130311872496, 'ACC-15': 29.822290568493287, 'ACC-16': 34.28182836086014, 'ACC-17': 31.084121890599743, 'ACC-18': 24.531548777144533, 'ACC-19': 29.41056879964796, 'ACC-20': 30.905773467706656, 'ACC-21': 28.62864677597222, 'ACC-22': 33.34443274634257, 'ACC-23': 24.946133741936215, 'ACC-24': 29.199692296918194, 'ACC-25': 29.727545908915975, 'ACC-26': 25.247709965158172, 'ACC-27': 31.466425431529228, 'ACC-28': 29.88713735675017, 'ACC-29': 23.458551491138312, 'ACC-30': 37.01483605911765, 'ACC-31': 32.09103686431954, 'ACC-32': 30.011881405094847, 'ACC-33': 33.67349407615677, 'ACC-34': 34.8902913737448, 'ACC-35': 34.48872701774332, 'ACC-36': 36.897648699371686, 'ACC-37': 34.0429387232363, 'ACC-38': 34.30161229281677, 'ACC-39': 34.676464977805274, 'ACC-40': 35.06012974745791, 'ACC-41': 37.47700561163473, 'ACC-42': 38.832075378384275, 'ACC-43': 35.87093100121538, 'ACC-44': 32.87770324260048, 'ACC-45': 30.507019644135813, 'ACC-46': 28.88761545758136, 'ACC-47': 31.241715050254875, 'ACC-48': 34.434211427415626, 'ACC-49': 33.65103236787157, 'ACC-50': 36.943452511862766, 'ACC-51': 36.347669204263035, 'ACC-52': 36.73999251620618, 'ACC-53': 34.34490163123966, 'ACC-54': 30.27254508658812, 'ACC-55': 36.80249748243384, 'ACC-56': 26.67708815834327, 'ACC-57': 30.412145237067545, 'ACC-58': 31.106406092982315, 'ACC-59': 26.61087460507943, 'ACC-60': 23.026486335780078, 'ACC-61': 33.41852048965768, 'ACC-62': 22.382376589792102, 'ACC-63': 27.325469220133193, 'ACC-64': 29.960530078187663, 'ACC-65': 22.300454971024593, 'ACC-66': 26.82353576058521, 'ACC-67': 24.990250210390197, 'ACC-68': 23.51748560804144, 'ACC-69': 23.29059333405995, 'ACC-70': 20.79569730316831, 'ACC-71': 21.98700374385236, 'ACC-72': 24.687680130999503, 'ACC-73': 24.633856470233614, 'ACC-74': 22.495048357420632, 'ACC-75': 15.434932879346627, 'ACC-76': 31.79565729981754, 'ACC-77': 17.980459524939093, 'ACC-78': 19.177052615559834, 'ACC-79': 21.507801232171545, 'ACC-80': 21.804961080834936, 'ACC-81': 21.700732506977708, 'ACC-82': 14.088108678936875, 'ACC-83': 21.7653289414595, 'ACC-84': 23.75485893011613, 'ACC-85': 27.162990510921535, 'ACC-86': 13.625505236314522, 'ACC-87': 16.360898473018697, 'ACC-88': 21.022426344626734, 'ACC-89': 21.930260088022145, 'ACC-90': 17.127541003082655, 'ACC-91': 21.08523266785525, 'ACC-92': 15.260098296470378, 'ACC-93': 19.354113511505968, 'ACC-94': 20.877123519952512, 'ACC-95': 19.105279394589967, 'ACC-96': 14.689562753159144, 'ACC-97': 18.738692743786885, 'ACC-98': 17.55845926459764, 'ACC-99': 17.10639482816739, 'ACC-100': 17.883127477908147, 'ACC-101': 13.201462074278119, 'ACC-102': 13.897122791119166, 'ACC-103': 22.78457069991499, 'ACC-104': 9.45476008952067, 'ACC-105': 14.99078929121342, 'ACC-106': 16.97599892173591, 'ACC-107': 8.93864578845911, 'ACC-108': 26.195111853852517, 'ACC-109': 7.51102436203282, 'ACC-110': 12.331712117028673, 'ACC-111': 11.839269468212697, 'ACC-112': 11.185628135136957, 'ACC-113': 15.537095507342116, 'ACC-114': 12.645932193074556, 'ACC-115': 12.832378632748483, 'ACC-116': 5.2690967172935945, 'ACC-117': 4.903011828884208, 'ACC-118': 20.710359042313105, 'ACC-119': 9.760256860603112, 'ACC-120': 9.78430266676506, 'ACC-121': 10.404850594583007, 'ACC-122': 11.858010888127788, 'ACC-123': 7.828335658980517, 'ACC-124': 8.1373884516603, 'ACC-125': 11.189704175608545, 'ACC-126': 8.561610306104791, 'ACC-127': 6.477434141053556, 'ACC-128': 11.39035554726177, 'ACC-129': 7.541421434662915, 'ACC-130': 7.165484737245352, 'ACC-131': 4.917416145138641, 'ACC-132': 13.799733925800819, 'ACC-133': 3.2488926085447756, 'ACC-134': 4.320413614821198, 'ACC-135': 5.422296334302492, 'ACC-136': 7.082134620760343, 'ACC-137': 14.076752171586843, 'ACC-138': 6.128242632687768, 'ACC-139': 3.7122269836598067, 'ACC-140': 8.399634247548812, 'ACC-141': 5.453185197403301, 'ACC-142': 6.911422186241606, 'ACC-143': 8.181583731184622, 'ACC-144': 6.699097472924189, 'ACC-145': 5.128916120664594, 'ACC-146': 2.9539192616115693, 'ACC-147': 3.7423989050894955, 'ACC-148': 3.3212268226540345, 'ACC-149': 3.1187472681931996, 'ACC-150': 1.70261066969353, 'ACC-151': 4.3312092407088265, 'ACC-152': 4.61100293344123, 'ACC-153': 2.1395591711097675, 'ACC-154': 1.7229738853331173, 'ACC-155': 2.072779622494534, 'ACC-156': 1.0770036262095444, 'ACC-157': 1.5159248325582217, 'ACC-158': 2.902543359617021, 'ACC-159': 3.266777624352471, 'ACC-160': 0.4647276387820658, 'ACC-161': 6.356209471546885, 'ACC-162': 5.803612604544468, 'ACC-163': 1.7623940064234362, 'ACC-164': 1.900215310170314, 'ACC-165': 0.9752975443656356, 'ACC-166': 1.1172574728660047, 'ACC-167': 0.5110593450727591, 'ACC-168': 1.6091145650186371, 'ACC-169': 13.260910381422478, 'ACC-170': 2.702059996876642, 'ACC-171': 1.9933542612263944, 'ACC-172': 3.607808274993518, 'ACC-173': 2.8377854624406873, 'ACC-174': 0.6425204247680719, 'ACC-175': 2.0019264937259535, 'ACC-176': 2.5508982470397026, 'ACC-177': 3.977345077869374, 'ACC-178': 1.1767182849118523, 'ACC-179': 2.2461811268845824, 'ACC-180': 1.6500728097802162, 'ACC-181': 0.28518573787072365, 'ACC-182': 0.0, 'ACC-183': 0.8536766246577407, 'ACC-184': 0.04881879735953983, 'ACC-185': 0.0, 'ACC-186': 0.4475783321097322, 'ACC-187': 4.712064654078933, 'ACC-188': 1.3758020257455308, 'ACC-189': 0.04056263975498112, 'ACC-190': 0.8143202993177316, 'ACC-191': 0.47321370309951055})])
[01/17 22:18:32] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 22:18:32] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 22:18:32] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 22:18:32] d2.evaluation.testing INFO: copypaste: 3.2886,11.1474,25.6385,18.6273,36.8411
[01/17 22:18:32] d2.utils.events INFO:  eta: 13:11:28  iter: 7999  total_loss: 44.76  loss_ce: 0.3842  loss_mask: 0.4985  loss_dice: 3.539  loss_ce_0: 0.6089  loss_mask_0: 0.4752  loss_dice_0: 3.68  loss_ce_1: 0.3672  loss_mask_1: 0.5042  loss_dice_1: 3.596  loss_ce_2: 0.409  loss_mask_2: 0.5041  loss_dice_2: 3.561  loss_ce_3: 0.3996  loss_mask_3: 0.5016  loss_dice_3: 3.548  loss_ce_4: 0.4033  loss_mask_4: 0.4999  loss_dice_4: 3.547  loss_ce_5: 0.3885  loss_mask_5: 0.5001  loss_dice_5: 3.547  loss_ce_6: 0.3779  loss_mask_6: 0.4993  loss_dice_6: 3.542  loss_ce_7: 0.3904  loss_mask_7: 0.5005  loss_dice_7: 3.537  loss_ce_8: 0.3807  loss_mask_8: 0.4976  loss_dice_8: 3.542  time: 1.5000  data_time: 0.0746  lr: 8.1808e-06  max_mem: 21492M
[01/17 22:19:01] d2.utils.events INFO:  eta: 13:10:35  iter: 8019  total_loss: 45.09  loss_ce: 0.4243  loss_mask: 0.4907  loss_dice: 3.546  loss_ce_0: 0.647  loss_mask_0: 0.4725  loss_dice_0: 3.667  loss_ce_1: 0.4221  loss_mask_1: 0.4949  loss_dice_1: 3.574  loss_ce_2: 0.4427  loss_mask_2: 0.4965  loss_dice_2: 3.552  loss_ce_3: 0.4245  loss_mask_3: 0.4948  loss_dice_3: 3.542  loss_ce_4: 0.4378  loss_mask_4: 0.4927  loss_dice_4: 3.544  loss_ce_5: 0.4291  loss_mask_5: 0.493  loss_dice_5: 3.553  loss_ce_6: 0.4396  loss_mask_6: 0.4919  loss_dice_6: 3.544  loss_ce_7: 0.403  loss_mask_7: 0.4954  loss_dice_7: 3.549  loss_ce_8: 0.4198  loss_mask_8: 0.4928  loss_dice_8: 3.548  time: 1.4999  data_time: 0.0783  lr: 8.1761e-06  max_mem: 21492M
[01/17 22:19:31] d2.utils.events INFO:  eta: 13:10:03  iter: 8039  total_loss: 44.62  loss_ce: 0.3818  loss_mask: 0.4841  loss_dice: 3.516  loss_ce_0: 0.632  loss_mask_0: 0.4698  loss_dice_0: 3.654  loss_ce_1: 0.392  loss_mask_1: 0.4823  loss_dice_1: 3.56  loss_ce_2: 0.3729  loss_mask_2: 0.4838  loss_dice_2: 3.534  loss_ce_3: 0.4009  loss_mask_3: 0.4839  loss_dice_3: 3.523  loss_ce_4: 0.3975  loss_mask_4: 0.485  loss_dice_4: 3.52  loss_ce_5: 0.3871  loss_mask_5: 0.4841  loss_dice_5: 3.525  loss_ce_6: 0.3733  loss_mask_6: 0.4837  loss_dice_6: 3.525  loss_ce_7: 0.3685  loss_mask_7: 0.4854  loss_dice_7: 3.533  loss_ce_8: 0.3719  loss_mask_8: 0.4835  loss_dice_8: 3.522  time: 1.4999  data_time: 0.0687  lr: 8.1715e-06  max_mem: 21492M
[01/17 22:20:01] d2.utils.events INFO:  eta: 13:09:36  iter: 8059  total_loss: 45.04  loss_ce: 0.4125  loss_mask: 0.4712  loss_dice: 3.604  loss_ce_0: 0.6168  loss_mask_0: 0.4575  loss_dice_0: 3.726  loss_ce_1: 0.3989  loss_mask_1: 0.4739  loss_dice_1: 3.651  loss_ce_2: 0.432  loss_mask_2: 0.4714  loss_dice_2: 3.614  loss_ce_3: 0.4155  loss_mask_3: 0.4735  loss_dice_3: 3.613  loss_ce_4: 0.4175  loss_mask_4: 0.4751  loss_dice_4: 3.609  loss_ce_5: 0.4045  loss_mask_5: 0.4763  loss_dice_5: 3.612  loss_ce_6: 0.3812  loss_mask_6: 0.4731  loss_dice_6: 3.62  loss_ce_7: 0.4011  loss_mask_7: 0.4719  loss_dice_7: 3.605  loss_ce_8: 0.4041  loss_mask_8: 0.4729  loss_dice_8: 3.609  time: 1.4999  data_time: 0.0947  lr: 8.1669e-06  max_mem: 21492M
[01/17 22:20:31] d2.utils.events INFO:  eta: 13:09:30  iter: 8079  total_loss: 45.42  loss_ce: 0.3978  loss_mask: 0.4972  loss_dice: 3.585  loss_ce_0: 0.6245  loss_mask_0: 0.4813  loss_dice_0: 3.718  loss_ce_1: 0.4105  loss_mask_1: 0.4962  loss_dice_1: 3.619  loss_ce_2: 0.4156  loss_mask_2: 0.4968  loss_dice_2: 3.598  loss_ce_3: 0.3903  loss_mask_3: 0.4977  loss_dice_3: 3.579  loss_ce_4: 0.4044  loss_mask_4: 0.4956  loss_dice_4: 3.576  loss_ce_5: 0.3802  loss_mask_5: 0.4968  loss_dice_5: 3.584  loss_ce_6: 0.3879  loss_mask_6: 0.4973  loss_dice_6: 3.587  loss_ce_7: 0.3791  loss_mask_7: 0.4975  loss_dice_7: 3.579  loss_ce_8: 0.3862  loss_mask_8: 0.4959  loss_dice_8: 3.589  time: 1.4999  data_time: 0.0749  lr: 8.1623e-06  max_mem: 21492M
[01/17 22:21:00] d2.utils.events INFO:  eta: 13:09:00  iter: 8099  total_loss: 44.57  loss_ce: 0.3852  loss_mask: 0.4803  loss_dice: 3.544  loss_ce_0: 0.589  loss_mask_0: 0.4593  loss_dice_0: 3.695  loss_ce_1: 0.4148  loss_mask_1: 0.4743  loss_dice_1: 3.59  loss_ce_2: 0.4055  loss_mask_2: 0.4749  loss_dice_2: 3.565  loss_ce_3: 0.4149  loss_mask_3: 0.4788  loss_dice_3: 3.548  loss_ce_4: 0.404  loss_mask_4: 0.4771  loss_dice_4: 3.537  loss_ce_5: 0.3947  loss_mask_5: 0.4811  loss_dice_5: 3.552  loss_ce_6: 0.3948  loss_mask_6: 0.478  loss_dice_6: 3.553  loss_ce_7: 0.3893  loss_mask_7: 0.4783  loss_dice_7: 3.544  loss_ce_8: 0.4072  loss_mask_8: 0.4792  loss_dice_8: 3.544  time: 1.4998  data_time: 0.0765  lr: 8.1577e-06  max_mem: 21492M
[01/17 22:21:29] d2.utils.events INFO:  eta: 13:08:30  iter: 8119  total_loss: 43.97  loss_ce: 0.3742  loss_mask: 0.4942  loss_dice: 3.489  loss_ce_0: 0.579  loss_mask_0: 0.4683  loss_dice_0: 3.632  loss_ce_1: 0.3909  loss_mask_1: 0.4942  loss_dice_1: 3.522  loss_ce_2: 0.3796  loss_mask_2: 0.4917  loss_dice_2: 3.507  loss_ce_3: 0.3909  loss_mask_3: 0.4923  loss_dice_3: 3.496  loss_ce_4: 0.3683  loss_mask_4: 0.4902  loss_dice_4: 3.494  loss_ce_5: 0.3614  loss_mask_5: 0.4907  loss_dice_5: 3.493  loss_ce_6: 0.3662  loss_mask_6: 0.4919  loss_dice_6: 3.486  loss_ce_7: 0.3705  loss_mask_7: 0.4906  loss_dice_7: 3.499  loss_ce_8: 0.3571  loss_mask_8: 0.4936  loss_dice_8: 3.5  time: 1.4997  data_time: 0.0636  lr: 8.1531e-06  max_mem: 21492M
[01/17 22:21:59] d2.utils.events INFO:  eta: 13:07:59  iter: 8139  total_loss: 44.99  loss_ce: 0.3925  loss_mask: 0.488  loss_dice: 3.582  loss_ce_0: 0.6132  loss_mask_0: 0.4726  loss_dice_0: 3.712  loss_ce_1: 0.3793  loss_mask_1: 0.4888  loss_dice_1: 3.628  loss_ce_2: 0.3979  loss_mask_2: 0.4879  loss_dice_2: 3.592  loss_ce_3: 0.3646  loss_mask_3: 0.4864  loss_dice_3: 3.589  loss_ce_4: 0.3881  loss_mask_4: 0.487  loss_dice_4: 3.573  loss_ce_5: 0.383  loss_mask_5: 0.4881  loss_dice_5: 3.587  loss_ce_6: 0.3933  loss_mask_6: 0.4864  loss_dice_6: 3.587  loss_ce_7: 0.4018  loss_mask_7: 0.4849  loss_dice_7: 3.58  loss_ce_8: 0.3882  loss_mask_8: 0.4875  loss_dice_8: 3.581  time: 1.4997  data_time: 0.0749  lr: 8.1485e-06  max_mem: 21492M
[01/17 22:22:29] d2.utils.events INFO:  eta: 13:07:10  iter: 8159  total_loss: 45.28  loss_ce: 0.4046  loss_mask: 0.4826  loss_dice: 3.599  loss_ce_0: 0.6281  loss_mask_0: 0.463  loss_dice_0: 3.733  loss_ce_1: 0.3941  loss_mask_1: 0.4866  loss_dice_1: 3.64  loss_ce_2: 0.4085  loss_mask_2: 0.4797  loss_dice_2: 3.618  loss_ce_3: 0.4057  loss_mask_3: 0.4811  loss_dice_3: 3.608  loss_ce_4: 0.3814  loss_mask_4: 0.4831  loss_dice_4: 3.606  loss_ce_5: 0.3989  loss_mask_5: 0.4851  loss_dice_5: 3.613  loss_ce_6: 0.3995  loss_mask_6: 0.4826  loss_dice_6: 3.606  loss_ce_7: 0.3932  loss_mask_7: 0.4843  loss_dice_7: 3.605  loss_ce_8: 0.3924  loss_mask_8: 0.4851  loss_dice_8: 3.607  time: 1.4997  data_time: 0.0832  lr: 8.1439e-06  max_mem: 21492M
[01/17 22:22:59] d2.utils.events INFO:  eta: 13:07:01  iter: 8179  total_loss: 44.79  loss_ce: 0.4211  loss_mask: 0.476  loss_dice: 3.544  loss_ce_0: 0.6562  loss_mask_0: 0.4613  loss_dice_0: 3.698  loss_ce_1: 0.4098  loss_mask_1: 0.4779  loss_dice_1: 3.591  loss_ce_2: 0.4172  loss_mask_2: 0.4765  loss_dice_2: 3.569  loss_ce_3: 0.4072  loss_mask_3: 0.4773  loss_dice_3: 3.557  loss_ce_4: 0.413  loss_mask_4: 0.4754  loss_dice_4: 3.55  loss_ce_5: 0.4092  loss_mask_5: 0.4766  loss_dice_5: 3.552  loss_ce_6: 0.4124  loss_mask_6: 0.4752  loss_dice_6: 3.556  loss_ce_7: 0.4018  loss_mask_7: 0.4772  loss_dice_7: 3.559  loss_ce_8: 0.4131  loss_mask_8: 0.4766  loss_dice_8: 3.55  time: 1.4996  data_time: 0.0699  lr: 8.1393e-06  max_mem: 21492M
[01/17 22:23:29] d2.utils.events INFO:  eta: 13:06:19  iter: 8199  total_loss: 44.67  loss_ce: 0.396  loss_mask: 0.4922  loss_dice: 3.554  loss_ce_0: 0.596  loss_mask_0: 0.4663  loss_dice_0: 3.683  loss_ce_1: 0.3867  loss_mask_1: 0.4886  loss_dice_1: 3.591  loss_ce_2: 0.4061  loss_mask_2: 0.4919  loss_dice_2: 3.574  loss_ce_3: 0.4103  loss_mask_3: 0.4906  loss_dice_3: 3.562  loss_ce_4: 0.3993  loss_mask_4: 0.492  loss_dice_4: 3.557  loss_ce_5: 0.3796  loss_mask_5: 0.492  loss_dice_5: 3.566  loss_ce_6: 0.39  loss_mask_6: 0.4946  loss_dice_6: 3.553  loss_ce_7: 0.3891  loss_mask_7: 0.4915  loss_dice_7: 3.563  loss_ce_8: 0.3922  loss_mask_8: 0.4929  loss_dice_8: 3.563  time: 1.4996  data_time: 0.0728  lr: 8.1347e-06  max_mem: 21492M
[01/17 22:23:58] d2.utils.events INFO:  eta: 13:06:02  iter: 8219  total_loss: 45.34  loss_ce: 0.4324  loss_mask: 0.481  loss_dice: 3.56  loss_ce_0: 0.6538  loss_mask_0: 0.4609  loss_dice_0: 3.693  loss_ce_1: 0.4076  loss_mask_1: 0.4759  loss_dice_1: 3.598  loss_ce_2: 0.4535  loss_mask_2: 0.4754  loss_dice_2: 3.576  loss_ce_3: 0.4219  loss_mask_3: 0.4759  loss_dice_3: 3.559  loss_ce_4: 0.4423  loss_mask_4: 0.4781  loss_dice_4: 3.555  loss_ce_5: 0.4299  loss_mask_5: 0.4779  loss_dice_5: 3.554  loss_ce_6: 0.4334  loss_mask_6: 0.4807  loss_dice_6: 3.554  loss_ce_7: 0.4143  loss_mask_7: 0.4814  loss_dice_7: 3.557  loss_ce_8: 0.4324  loss_mask_8: 0.4843  loss_dice_8: 3.554  time: 1.4995  data_time: 0.0631  lr: 8.1301e-06  max_mem: 21492M
[01/17 22:24:28] d2.utils.events INFO:  eta: 13:05:52  iter: 8239  total_loss: 44.73  loss_ce: 0.3913  loss_mask: 0.4676  loss_dice: 3.537  loss_ce_0: 0.6047  loss_mask_0: 0.4606  loss_dice_0: 3.67  loss_ce_1: 0.3749  loss_mask_1: 0.469  loss_dice_1: 3.584  loss_ce_2: 0.4298  loss_mask_2: 0.4693  loss_dice_2: 3.546  loss_ce_3: 0.4082  loss_mask_3: 0.4686  loss_dice_3: 3.549  loss_ce_4: 0.4014  loss_mask_4: 0.4704  loss_dice_4: 3.541  loss_ce_5: 0.4086  loss_mask_5: 0.4694  loss_dice_5: 3.544  loss_ce_6: 0.4102  loss_mask_6: 0.4677  loss_dice_6: 3.545  loss_ce_7: 0.4094  loss_mask_7: 0.4689  loss_dice_7: 3.54  loss_ce_8: 0.3952  loss_mask_8: 0.4681  loss_dice_8: 3.54  time: 1.4995  data_time: 0.0737  lr: 8.1255e-06  max_mem: 21492M
[01/17 22:24:58] d2.utils.events INFO:  eta: 13:05:03  iter: 8259  total_loss: 45.08  loss_ce: 0.4022  loss_mask: 0.4914  loss_dice: 3.564  loss_ce_0: 0.6185  loss_mask_0: 0.4769  loss_dice_0: 3.681  loss_ce_1: 0.3849  loss_mask_1: 0.4924  loss_dice_1: 3.605  loss_ce_2: 0.4059  loss_mask_2: 0.4901  loss_dice_2: 3.584  loss_ce_3: 0.4146  loss_mask_3: 0.4902  loss_dice_3: 3.571  loss_ce_4: 0.4062  loss_mask_4: 0.4902  loss_dice_4: 3.566  loss_ce_5: 0.4106  loss_mask_5: 0.491  loss_dice_5: 3.566  loss_ce_6: 0.4136  loss_mask_6: 0.4921  loss_dice_6: 3.57  loss_ce_7: 0.3971  loss_mask_7: 0.4927  loss_dice_7: 3.582  loss_ce_8: 0.4063  loss_mask_8: 0.4924  loss_dice_8: 3.574  time: 1.4995  data_time: 0.0750  lr: 8.1209e-06  max_mem: 21492M
[01/17 22:25:28] d2.utils.events INFO:  eta: 13:04:31  iter: 8279  total_loss: 44.97  loss_ce: 0.3905  loss_mask: 0.488  loss_dice: 3.565  loss_ce_0: 0.6219  loss_mask_0: 0.4632  loss_dice_0: 3.694  loss_ce_1: 0.3957  loss_mask_1: 0.4882  loss_dice_1: 3.601  loss_ce_2: 0.4127  loss_mask_2: 0.4869  loss_dice_2: 3.579  loss_ce_3: 0.4033  loss_mask_3: 0.4899  loss_dice_3: 3.565  loss_ce_4: 0.3886  loss_mask_4: 0.4905  loss_dice_4: 3.567  loss_ce_5: 0.381  loss_mask_5: 0.4908  loss_dice_5: 3.568  loss_ce_6: 0.3902  loss_mask_6: 0.4894  loss_dice_6: 3.567  loss_ce_7: 0.3983  loss_mask_7: 0.4892  loss_dice_7: 3.562  loss_ce_8: 0.3796  loss_mask_8: 0.4894  loss_dice_8: 3.571  time: 1.4995  data_time: 0.0786  lr: 8.1163e-06  max_mem: 21492M
[01/17 22:25:58] d2.utils.events INFO:  eta: 13:04:07  iter: 8299  total_loss: 44.8  loss_ce: 0.4169  loss_mask: 0.4727  loss_dice: 3.567  loss_ce_0: 0.6277  loss_mask_0: 0.4515  loss_dice_0: 3.703  loss_ce_1: 0.4013  loss_mask_1: 0.4735  loss_dice_1: 3.606  loss_ce_2: 0.4125  loss_mask_2: 0.4699  loss_dice_2: 3.585  loss_ce_3: 0.4017  loss_mask_3: 0.4714  loss_dice_3: 3.574  loss_ce_4: 0.4135  loss_mask_4: 0.4718  loss_dice_4: 3.568  loss_ce_5: 0.4094  loss_mask_5: 0.4716  loss_dice_5: 3.571  loss_ce_6: 0.4029  loss_mask_6: 0.4737  loss_dice_6: 3.57  loss_ce_7: 0.3983  loss_mask_7: 0.4711  loss_dice_7: 3.574  loss_ce_8: 0.4067  loss_mask_8: 0.4707  loss_dice_8: 3.569  time: 1.4995  data_time: 0.0720  lr: 8.1117e-06  max_mem: 21492M
[01/17 22:26:28] d2.utils.events INFO:  eta: 13:03:33  iter: 8319  total_loss: 45.19  loss_ce: 0.3954  loss_mask: 0.4747  loss_dice: 3.593  loss_ce_0: 0.6443  loss_mask_0: 0.4641  loss_dice_0: 3.709  loss_ce_1: 0.3983  loss_mask_1: 0.481  loss_dice_1: 3.634  loss_ce_2: 0.4138  loss_mask_2: 0.4766  loss_dice_2: 3.611  loss_ce_3: 0.4066  loss_mask_3: 0.4745  loss_dice_3: 3.606  loss_ce_4: 0.4037  loss_mask_4: 0.4779  loss_dice_4: 3.603  loss_ce_5: 0.4114  loss_mask_5: 0.4795  loss_dice_5: 3.601  loss_ce_6: 0.4084  loss_mask_6: 0.4766  loss_dice_6: 3.599  loss_ce_7: 0.389  loss_mask_7: 0.4771  loss_dice_7: 3.597  loss_ce_8: 0.3904  loss_mask_8: 0.4785  loss_dice_8: 3.6  time: 1.4994  data_time: 0.0744  lr: 8.1071e-06  max_mem: 21492M
[01/17 22:26:58] d2.utils.events INFO:  eta: 13:03:24  iter: 8339  total_loss: 45.34  loss_ce: 0.389  loss_mask: 0.478  loss_dice: 3.596  loss_ce_0: 0.6127  loss_mask_0: 0.4562  loss_dice_0: 3.714  loss_ce_1: 0.362  loss_mask_1: 0.4784  loss_dice_1: 3.626  loss_ce_2: 0.386  loss_mask_2: 0.4773  loss_dice_2: 3.601  loss_ce_3: 0.3837  loss_mask_3: 0.4772  loss_dice_3: 3.593  loss_ce_4: 0.3821  loss_mask_4: 0.4745  loss_dice_4: 3.592  loss_ce_5: 0.3887  loss_mask_5: 0.4758  loss_dice_5: 3.593  loss_ce_6: 0.3705  loss_mask_6: 0.476  loss_dice_6: 3.59  loss_ce_7: 0.3898  loss_mask_7: 0.4775  loss_dice_7: 3.593  loss_ce_8: 0.3801  loss_mask_8: 0.4794  loss_dice_8: 3.593  time: 1.4994  data_time: 0.0761  lr: 8.1025e-06  max_mem: 21492M
[01/17 22:27:28] d2.utils.events INFO:  eta: 13:02:54  iter: 8359  total_loss: 44.7  loss_ce: 0.4062  loss_mask: 0.4762  loss_dice: 3.522  loss_ce_0: 0.6429  loss_mask_0: 0.4603  loss_dice_0: 3.654  loss_ce_1: 0.401  loss_mask_1: 0.48  loss_dice_1: 3.556  loss_ce_2: 0.4075  loss_mask_2: 0.4803  loss_dice_2: 3.538  loss_ce_3: 0.4026  loss_mask_3: 0.4804  loss_dice_3: 3.526  loss_ce_4: 0.4064  loss_mask_4: 0.4778  loss_dice_4: 3.523  loss_ce_5: 0.4185  loss_mask_5: 0.4758  loss_dice_5: 3.526  loss_ce_6: 0.4031  loss_mask_6: 0.4751  loss_dice_6: 3.523  loss_ce_7: 0.387  loss_mask_7: 0.4769  loss_dice_7: 3.528  loss_ce_8: 0.3923  loss_mask_8: 0.475  loss_dice_8: 3.523  time: 1.4994  data_time: 0.0764  lr: 8.0979e-06  max_mem: 21492M
[01/17 22:27:58] d2.utils.events INFO:  eta: 13:02:32  iter: 8379  total_loss: 44.83  loss_ce: 0.3742  loss_mask: 0.495  loss_dice: 3.561  loss_ce_0: 0.627  loss_mask_0: 0.4766  loss_dice_0: 3.679  loss_ce_1: 0.3848  loss_mask_1: 0.4962  loss_dice_1: 3.587  loss_ce_2: 0.4025  loss_mask_2: 0.4951  loss_dice_2: 3.576  loss_ce_3: 0.3801  loss_mask_3: 0.4976  loss_dice_3: 3.577  loss_ce_4: 0.3801  loss_mask_4: 0.496  loss_dice_4: 3.568  loss_ce_5: 0.366  loss_mask_5: 0.4957  loss_dice_5: 3.563  loss_ce_6: 0.3662  loss_mask_6: 0.4963  loss_dice_6: 3.565  loss_ce_7: 0.3738  loss_mask_7: 0.4969  loss_dice_7: 3.569  loss_ce_8: 0.3714  loss_mask_8: 0.4941  loss_dice_8: 3.569  time: 1.4994  data_time: 0.0716  lr: 8.0933e-06  max_mem: 21492M
[01/17 22:28:28] d2.utils.events INFO:  eta: 13:02:24  iter: 8399  total_loss: 44.83  loss_ce: 0.3831  loss_mask: 0.4742  loss_dice: 3.571  loss_ce_0: 0.6643  loss_mask_0: 0.4641  loss_dice_0: 3.682  loss_ce_1: 0.4186  loss_mask_1: 0.4765  loss_dice_1: 3.605  loss_ce_2: 0.4117  loss_mask_2: 0.4759  loss_dice_2: 3.589  loss_ce_3: 0.389  loss_mask_3: 0.4751  loss_dice_3: 3.563  loss_ce_4: 0.3872  loss_mask_4: 0.4734  loss_dice_4: 3.575  loss_ce_5: 0.3736  loss_mask_5: 0.4753  loss_dice_5: 3.57  loss_ce_6: 0.3909  loss_mask_6: 0.4745  loss_dice_6: 3.563  loss_ce_7: 0.3805  loss_mask_7: 0.4736  loss_dice_7: 3.571  loss_ce_8: 0.37  loss_mask_8: 0.4733  loss_dice_8: 3.567  time: 1.4994  data_time: 0.0735  lr: 8.0887e-06  max_mem: 21492M
[01/17 22:28:58] d2.utils.events INFO:  eta: 13:01:56  iter: 8419  total_loss: 44.07  loss_ce: 0.3625  loss_mask: 0.4778  loss_dice: 3.493  loss_ce_0: 0.6018  loss_mask_0: 0.4542  loss_dice_0: 3.627  loss_ce_1: 0.3543  loss_mask_1: 0.4754  loss_dice_1: 3.55  loss_ce_2: 0.3611  loss_mask_2: 0.4746  loss_dice_2: 3.513  loss_ce_3: 0.3554  loss_mask_3: 0.4748  loss_dice_3: 3.51  loss_ce_4: 0.3552  loss_mask_4: 0.4729  loss_dice_4: 3.501  loss_ce_5: 0.3587  loss_mask_5: 0.4742  loss_dice_5: 3.501  loss_ce_6: 0.3625  loss_mask_6: 0.4764  loss_dice_6: 3.497  loss_ce_7: 0.3482  loss_mask_7: 0.4736  loss_dice_7: 3.499  loss_ce_8: 0.3645  loss_mask_8: 0.474  loss_dice_8: 3.507  time: 1.4995  data_time: 0.0799  lr: 8.0841e-06  max_mem: 21492M
[01/17 22:29:28] d2.utils.events INFO:  eta: 13:01:24  iter: 8439  total_loss: 43.8  loss_ce: 0.3759  loss_mask: 0.4801  loss_dice: 3.489  loss_ce_0: 0.6087  loss_mask_0: 0.4582  loss_dice_0: 3.628  loss_ce_1: 0.4008  loss_mask_1: 0.4826  loss_dice_1: 3.541  loss_ce_2: 0.3909  loss_mask_2: 0.478  loss_dice_2: 3.503  loss_ce_3: 0.3757  loss_mask_3: 0.4779  loss_dice_3: 3.497  loss_ce_4: 0.3676  loss_mask_4: 0.478  loss_dice_4: 3.493  loss_ce_5: 0.3923  loss_mask_5: 0.4781  loss_dice_5: 3.492  loss_ce_6: 0.3807  loss_mask_6: 0.4798  loss_dice_6: 3.496  loss_ce_7: 0.3718  loss_mask_7: 0.4801  loss_dice_7: 3.492  loss_ce_8: 0.3757  loss_mask_8: 0.4805  loss_dice_8: 3.499  time: 1.4994  data_time: 0.0774  lr: 8.0794e-06  max_mem: 21492M
[01/17 22:29:58] d2.utils.events INFO:  eta: 13:00:49  iter: 8459  total_loss: 43.86  loss_ce: 0.3534  loss_mask: 0.478  loss_dice: 3.497  loss_ce_0: 0.6044  loss_mask_0: 0.4576  loss_dice_0: 3.627  loss_ce_1: 0.3849  loss_mask_1: 0.4759  loss_dice_1: 3.536  loss_ce_2: 0.3877  loss_mask_2: 0.4751  loss_dice_2: 3.518  loss_ce_3: 0.382  loss_mask_3: 0.4773  loss_dice_3: 3.498  loss_ce_4: 0.3745  loss_mask_4: 0.4797  loss_dice_4: 3.502  loss_ce_5: 0.3528  loss_mask_5: 0.4777  loss_dice_5: 3.5  loss_ce_6: 0.358  loss_mask_6: 0.4798  loss_dice_6: 3.493  loss_ce_7: 0.3682  loss_mask_7: 0.4772  loss_dice_7: 3.499  loss_ce_8: 0.3526  loss_mask_8: 0.4769  loss_dice_8: 3.496  time: 1.4994  data_time: 0.0865  lr: 8.0748e-06  max_mem: 21492M
[01/17 22:30:28] d2.utils.events INFO:  eta: 13:00:26  iter: 8479  total_loss: 44.9  loss_ce: 0.3974  loss_mask: 0.4981  loss_dice: 3.534  loss_ce_0: 0.6297  loss_mask_0: 0.4818  loss_dice_0: 3.66  loss_ce_1: 0.4093  loss_mask_1: 0.5039  loss_dice_1: 3.574  loss_ce_2: 0.4215  loss_mask_2: 0.5037  loss_dice_2: 3.548  loss_ce_3: 0.4197  loss_mask_3: 0.5023  loss_dice_3: 3.543  loss_ce_4: 0.399  loss_mask_4: 0.5  loss_dice_4: 3.538  loss_ce_5: 0.4191  loss_mask_5: 0.5001  loss_dice_5: 3.539  loss_ce_6: 0.398  loss_mask_6: 0.4997  loss_dice_6: 3.532  loss_ce_7: 0.4098  loss_mask_7: 0.499  loss_dice_7: 3.536  loss_ce_8: 0.4004  loss_mask_8: 0.497  loss_dice_8: 3.532  time: 1.4994  data_time: 0.0874  lr: 8.0702e-06  max_mem: 21492M
[01/17 22:30:58] d2.utils.events INFO:  eta: 12:59:43  iter: 8499  total_loss: 44.45  loss_ce: 0.3858  loss_mask: 0.4897  loss_dice: 3.509  loss_ce_0: 0.6179  loss_mask_0: 0.4678  loss_dice_0: 3.649  loss_ce_1: 0.3813  loss_mask_1: 0.4831  loss_dice_1: 3.548  loss_ce_2: 0.4062  loss_mask_2: 0.4861  loss_dice_2: 3.542  loss_ce_3: 0.3928  loss_mask_3: 0.487  loss_dice_3: 3.522  loss_ce_4: 0.3752  loss_mask_4: 0.4901  loss_dice_4: 3.522  loss_ce_5: 0.379  loss_mask_5: 0.4931  loss_dice_5: 3.517  loss_ce_6: 0.364  loss_mask_6: 0.49  loss_dice_6: 3.519  loss_ce_7: 0.3781  loss_mask_7: 0.4898  loss_dice_7: 3.512  loss_ce_8: 0.3866  loss_mask_8: 0.4909  loss_dice_8: 3.511  time: 1.4994  data_time: 0.0775  lr: 8.0656e-06  max_mem: 21492M
[01/17 22:31:28] d2.utils.events INFO:  eta: 12:59:11  iter: 8519  total_loss: 44.07  loss_ce: 0.3787  loss_mask: 0.472  loss_dice: 3.488  loss_ce_0: 0.5832  loss_mask_0: 0.4624  loss_dice_0: 3.622  loss_ce_1: 0.3717  loss_mask_1: 0.4754  loss_dice_1: 3.537  loss_ce_2: 0.3914  loss_mask_2: 0.4743  loss_dice_2: 3.52  loss_ce_3: 0.3663  loss_mask_3: 0.4715  loss_dice_3: 3.49  loss_ce_4: 0.3757  loss_mask_4: 0.472  loss_dice_4: 3.488  loss_ce_5: 0.3705  loss_mask_5: 0.4695  loss_dice_5: 3.491  loss_ce_6: 0.382  loss_mask_6: 0.4725  loss_dice_6: 3.481  loss_ce_7: 0.3763  loss_mask_7: 0.4727  loss_dice_7: 3.484  loss_ce_8: 0.3679  loss_mask_8: 0.4725  loss_dice_8: 3.492  time: 1.4994  data_time: 0.0734  lr: 8.061e-06  max_mem: 21492M
[01/17 22:31:58] d2.utils.events INFO:  eta: 12:58:18  iter: 8539  total_loss: 43.87  loss_ce: 0.3865  loss_mask: 0.479  loss_dice: 3.433  loss_ce_0: 0.6029  loss_mask_0: 0.4615  loss_dice_0: 3.578  loss_ce_1: 0.3861  loss_mask_1: 0.4756  loss_dice_1: 3.48  loss_ce_2: 0.4136  loss_mask_2: 0.4738  loss_dice_2: 3.453  loss_ce_3: 0.3965  loss_mask_3: 0.4769  loss_dice_3: 3.434  loss_ce_4: 0.3796  loss_mask_4: 0.4782  loss_dice_4: 3.446  loss_ce_5: 0.3819  loss_mask_5: 0.4785  loss_dice_5: 3.434  loss_ce_6: 0.3756  loss_mask_6: 0.4813  loss_dice_6: 3.44  loss_ce_7: 0.3698  loss_mask_7: 0.4797  loss_dice_7: 3.439  loss_ce_8: 0.3751  loss_mask_8: 0.4772  loss_dice_8: 3.444  time: 1.4994  data_time: 0.0746  lr: 8.0564e-06  max_mem: 21492M
[01/17 22:32:28] d2.utils.events INFO:  eta: 12:58:04  iter: 8559  total_loss: 44.25  loss_ce: 0.3617  loss_mask: 0.482  loss_dice: 3.524  loss_ce_0: 0.6006  loss_mask_0: 0.4534  loss_dice_0: 3.665  loss_ce_1: 0.3759  loss_mask_1: 0.4722  loss_dice_1: 3.568  loss_ce_2: 0.3906  loss_mask_2: 0.4787  loss_dice_2: 3.538  loss_ce_3: 0.3699  loss_mask_3: 0.4746  loss_dice_3: 3.531  loss_ce_4: 0.3751  loss_mask_4: 0.4793  loss_dice_4: 3.523  loss_ce_5: 0.3563  loss_mask_5: 0.4787  loss_dice_5: 3.526  loss_ce_6: 0.3703  loss_mask_6: 0.4811  loss_dice_6: 3.531  loss_ce_7: 0.3743  loss_mask_7: 0.4803  loss_dice_7: 3.523  loss_ce_8: 0.3676  loss_mask_8: 0.4804  loss_dice_8: 3.528  time: 1.4994  data_time: 0.0747  lr: 8.0518e-06  max_mem: 21492M
[01/17 22:32:58] d2.utils.events INFO:  eta: 12:57:44  iter: 8579  total_loss: 43.36  loss_ce: 0.3643  loss_mask: 0.4738  loss_dice: 3.471  loss_ce_0: 0.5968  loss_mask_0: 0.4487  loss_dice_0: 3.613  loss_ce_1: 0.3815  loss_mask_1: 0.4659  loss_dice_1: 3.517  loss_ce_2: 0.3778  loss_mask_2: 0.4688  loss_dice_2: 3.488  loss_ce_3: 0.3707  loss_mask_3: 0.4696  loss_dice_3: 3.477  loss_ce_4: 0.3719  loss_mask_4: 0.4708  loss_dice_4: 3.466  loss_ce_5: 0.365  loss_mask_5: 0.472  loss_dice_5: 3.468  loss_ce_6: 0.3669  loss_mask_6: 0.4706  loss_dice_6: 3.461  loss_ce_7: 0.3611  loss_mask_7: 0.4719  loss_dice_7: 3.47  loss_ce_8: 0.3654  loss_mask_8: 0.4729  loss_dice_8: 3.465  time: 1.4993  data_time: 0.0768  lr: 8.0472e-06  max_mem: 21492M
[01/17 22:33:27] d2.utils.events INFO:  eta: 12:56:43  iter: 8599  total_loss: 43.41  loss_ce: 0.366  loss_mask: 0.4885  loss_dice: 3.456  loss_ce_0: 0.5785  loss_mask_0: 0.4638  loss_dice_0: 3.588  loss_ce_1: 0.3674  loss_mask_1: 0.4886  loss_dice_1: 3.485  loss_ce_2: 0.3979  loss_mask_2: 0.4878  loss_dice_2: 3.471  loss_ce_3: 0.3656  loss_mask_3: 0.4918  loss_dice_3: 3.439  loss_ce_4: 0.3552  loss_mask_4: 0.4889  loss_dice_4: 3.454  loss_ce_5: 0.3632  loss_mask_5: 0.4914  loss_dice_5: 3.464  loss_ce_6: 0.3567  loss_mask_6: 0.491  loss_dice_6: 3.457  loss_ce_7: 0.343  loss_mask_7: 0.4891  loss_dice_7: 3.461  loss_ce_8: 0.3448  loss_mask_8: 0.4889  loss_dice_8: 3.454  time: 1.4993  data_time: 0.0808  lr: 8.0426e-06  max_mem: 21492M
[01/17 22:33:58] d2.utils.events INFO:  eta: 12:56:09  iter: 8619  total_loss: 43.75  loss_ce: 0.3765  loss_mask: 0.4807  loss_dice: 3.462  loss_ce_0: 0.59  loss_mask_0: 0.468  loss_dice_0: 3.59  loss_ce_1: 0.3551  loss_mask_1: 0.4844  loss_dice_1: 3.497  loss_ce_2: 0.374  loss_mask_2: 0.4813  loss_dice_2: 3.479  loss_ce_3: 0.3808  loss_mask_3: 0.4799  loss_dice_3: 3.473  loss_ce_4: 0.3713  loss_mask_4: 0.4797  loss_dice_4: 3.468  loss_ce_5: 0.37  loss_mask_5: 0.4801  loss_dice_5: 3.471  loss_ce_6: 0.3721  loss_mask_6: 0.4799  loss_dice_6: 3.464  loss_ce_7: 0.3798  loss_mask_7: 0.4823  loss_dice_7: 3.47  loss_ce_8: 0.3632  loss_mask_8: 0.4808  loss_dice_8: 3.471  time: 1.4993  data_time: 0.0911  lr: 8.038e-06  max_mem: 21492M
[01/17 22:34:28] d2.utils.events INFO:  eta: 12:56:05  iter: 8639  total_loss: 44.26  loss_ce: 0.3816  loss_mask: 0.4725  loss_dice: 3.523  loss_ce_0: 0.6046  loss_mask_0: 0.4556  loss_dice_0: 3.655  loss_ce_1: 0.3707  loss_mask_1: 0.4696  loss_dice_1: 3.562  loss_ce_2: 0.4034  loss_mask_2: 0.4677  loss_dice_2: 3.537  loss_ce_3: 0.3854  loss_mask_3: 0.4671  loss_dice_3: 3.52  loss_ce_4: 0.3785  loss_mask_4: 0.4676  loss_dice_4: 3.516  loss_ce_5: 0.3768  loss_mask_5: 0.469  loss_dice_5: 3.521  loss_ce_6: 0.3727  loss_mask_6: 0.4723  loss_dice_6: 3.51  loss_ce_7: 0.3814  loss_mask_7: 0.4703  loss_dice_7: 3.518  loss_ce_8: 0.367  loss_mask_8: 0.4726  loss_dice_8: 3.52  time: 1.4994  data_time: 0.0844  lr: 8.0334e-06  max_mem: 21492M
[01/17 22:34:58] d2.utils.events INFO:  eta: 12:55:11  iter: 8659  total_loss: 44.14  loss_ce: 0.3957  loss_mask: 0.4775  loss_dice: 3.495  loss_ce_0: 0.6012  loss_mask_0: 0.4492  loss_dice_0: 3.618  loss_ce_1: 0.3722  loss_mask_1: 0.4838  loss_dice_1: 3.523  loss_ce_2: 0.408  loss_mask_2: 0.48  loss_dice_2: 3.501  loss_ce_3: 0.4171  loss_mask_3: 0.4713  loss_dice_3: 3.493  loss_ce_4: 0.4082  loss_mask_4: 0.4744  loss_dice_4: 3.48  loss_ce_5: 0.3969  loss_mask_5: 0.4752  loss_dice_5: 3.485  loss_ce_6: 0.4011  loss_mask_6: 0.4723  loss_dice_6: 3.487  loss_ce_7: 0.4131  loss_mask_7: 0.4766  loss_dice_7: 3.48  loss_ce_8: 0.4044  loss_mask_8: 0.4777  loss_dice_8: 3.492  time: 1.4993  data_time: 0.0819  lr: 8.0287e-06  max_mem: 21492M
[01/17 22:35:27] d2.utils.events INFO:  eta: 12:54:37  iter: 8679  total_loss: 44.38  loss_ce: 0.4344  loss_mask: 0.5014  loss_dice: 3.47  loss_ce_0: 0.6465  loss_mask_0: 0.4785  loss_dice_0: 3.606  loss_ce_1: 0.4148  loss_mask_1: 0.4967  loss_dice_1: 3.513  loss_ce_2: 0.439  loss_mask_2: 0.4989  loss_dice_2: 3.488  loss_ce_3: 0.4269  loss_mask_3: 0.4997  loss_dice_3: 3.477  loss_ce_4: 0.4166  loss_mask_4: 0.5001  loss_dice_4: 3.478  loss_ce_5: 0.4329  loss_mask_5: 0.5005  loss_dice_5: 3.488  loss_ce_6: 0.4138  loss_mask_6: 0.5012  loss_dice_6: 3.489  loss_ce_7: 0.4237  loss_mask_7: 0.5012  loss_dice_7: 3.477  loss_ce_8: 0.4189  loss_mask_8: 0.5012  loss_dice_8: 3.488  time: 1.4993  data_time: 0.0789  lr: 8.0241e-06  max_mem: 21492M
[01/17 22:35:57] d2.utils.events INFO:  eta: 12:53:29  iter: 8699  total_loss: 43.78  loss_ce: 0.3911  loss_mask: 0.4803  loss_dice: 3.504  loss_ce_0: 0.6108  loss_mask_0: 0.4608  loss_dice_0: 3.627  loss_ce_1: 0.3875  loss_mask_1: 0.481  loss_dice_1: 3.539  loss_ce_2: 0.398  loss_mask_2: 0.4818  loss_dice_2: 3.52  loss_ce_3: 0.3821  loss_mask_3: 0.4822  loss_dice_3: 3.508  loss_ce_4: 0.3754  loss_mask_4: 0.479  loss_dice_4: 3.508  loss_ce_5: 0.3777  loss_mask_5: 0.4828  loss_dice_5: 3.509  loss_ce_6: 0.3697  loss_mask_6: 0.4801  loss_dice_6: 3.499  loss_ce_7: 0.358  loss_mask_7: 0.4839  loss_dice_7: 3.503  loss_ce_8: 0.3628  loss_mask_8: 0.4815  loss_dice_8: 3.51  time: 1.4992  data_time: 0.0773  lr: 8.0195e-06  max_mem: 21492M
[01/17 22:36:27] d2.utils.events INFO:  eta: 12:52:28  iter: 8719  total_loss: 43.85  loss_ce: 0.362  loss_mask: 0.483  loss_dice: 3.503  loss_ce_0: 0.6124  loss_mask_0: 0.4548  loss_dice_0: 3.643  loss_ce_1: 0.3674  loss_mask_1: 0.4788  loss_dice_1: 3.548  loss_ce_2: 0.3893  loss_mask_2: 0.4777  loss_dice_2: 3.53  loss_ce_3: 0.3742  loss_mask_3: 0.4781  loss_dice_3: 3.517  loss_ce_4: 0.3719  loss_mask_4: 0.4781  loss_dice_4: 3.509  loss_ce_5: 0.3591  loss_mask_5: 0.4764  loss_dice_5: 3.512  loss_ce_6: 0.3701  loss_mask_6: 0.4804  loss_dice_6: 3.51  loss_ce_7: 0.3548  loss_mask_7: 0.4797  loss_dice_7: 3.517  loss_ce_8: 0.3471  loss_mask_8: 0.4831  loss_dice_8: 3.513  time: 1.4992  data_time: 0.0789  lr: 8.0149e-06  max_mem: 21492M
[01/17 22:36:56] d2.utils.events INFO:  eta: 12:52:12  iter: 8739  total_loss: 44.14  loss_ce: 0.382  loss_mask: 0.4738  loss_dice: 3.495  loss_ce_0: 0.6574  loss_mask_0: 0.4609  loss_dice_0: 3.626  loss_ce_1: 0.4036  loss_mask_1: 0.4774  loss_dice_1: 3.535  loss_ce_2: 0.4106  loss_mask_2: 0.4756  loss_dice_2: 3.516  loss_ce_3: 0.3881  loss_mask_3: 0.476  loss_dice_3: 3.503  loss_ce_4: 0.3984  loss_mask_4: 0.4772  loss_dice_4: 3.499  loss_ce_5: 0.3609  loss_mask_5: 0.4783  loss_dice_5: 3.5  loss_ce_6: 0.3884  loss_mask_6: 0.4799  loss_dice_6: 3.497  loss_ce_7: 0.3947  loss_mask_7: 0.4765  loss_dice_7: 3.5  loss_ce_8: 0.3998  loss_mask_8: 0.4749  loss_dice_8: 3.499  time: 1.4992  data_time: 0.0679  lr: 8.0103e-06  max_mem: 21492M
[01/17 22:37:26] d2.utils.events INFO:  eta: 12:51:38  iter: 8759  total_loss: 43.96  loss_ce: 0.3657  loss_mask: 0.4727  loss_dice: 3.499  loss_ce_0: 0.6164  loss_mask_0: 0.4522  loss_dice_0: 3.631  loss_ce_1: 0.3936  loss_mask_1: 0.4763  loss_dice_1: 3.527  loss_ce_2: 0.4017  loss_mask_2: 0.4734  loss_dice_2: 3.514  loss_ce_3: 0.387  loss_mask_3: 0.4761  loss_dice_3: 3.499  loss_ce_4: 0.3887  loss_mask_4: 0.4752  loss_dice_4: 3.5  loss_ce_5: 0.3785  loss_mask_5: 0.4773  loss_dice_5: 3.5  loss_ce_6: 0.3699  loss_mask_6: 0.4764  loss_dice_6: 3.495  loss_ce_7: 0.3637  loss_mask_7: 0.4723  loss_dice_7: 3.503  loss_ce_8: 0.379  loss_mask_8: 0.4743  loss_dice_8: 3.503  time: 1.4991  data_time: 0.0707  lr: 8.0057e-06  max_mem: 21492M
[01/17 22:37:56] d2.utils.events INFO:  eta: 12:51:33  iter: 8779  total_loss: 44.13  loss_ce: 0.4016  loss_mask: 0.4623  loss_dice: 3.482  loss_ce_0: 0.6376  loss_mask_0: 0.4405  loss_dice_0: 3.619  loss_ce_1: 0.3714  loss_mask_1: 0.4594  loss_dice_1: 3.523  loss_ce_2: 0.4182  loss_mask_2: 0.4603  loss_dice_2: 3.487  loss_ce_3: 0.4226  loss_mask_3: 0.4631  loss_dice_3: 3.485  loss_ce_4: 0.4135  loss_mask_4: 0.4628  loss_dice_4: 3.471  loss_ce_5: 0.3981  loss_mask_5: 0.463  loss_dice_5: 3.475  loss_ce_6: 0.3872  loss_mask_6: 0.4622  loss_dice_6: 3.481  loss_ce_7: 0.4092  loss_mask_7: 0.4642  loss_dice_7: 3.48  loss_ce_8: 0.4084  loss_mask_8: 0.4637  loss_dice_8: 3.482  time: 1.4991  data_time: 0.0756  lr: 8.0011e-06  max_mem: 21492M
[01/17 22:38:26] d2.utils.events INFO:  eta: 12:51:08  iter: 8799  total_loss: 44.12  loss_ce: 0.3866  loss_mask: 0.4844  loss_dice: 3.483  loss_ce_0: 0.5929  loss_mask_0: 0.4641  loss_dice_0: 3.631  loss_ce_1: 0.3807  loss_mask_1: 0.4836  loss_dice_1: 3.542  loss_ce_2: 0.3888  loss_mask_2: 0.4817  loss_dice_2: 3.511  loss_ce_3: 0.3806  loss_mask_3: 0.4811  loss_dice_3: 3.498  loss_ce_4: 0.3895  loss_mask_4: 0.4809  loss_dice_4: 3.49  loss_ce_5: 0.3982  loss_mask_5: 0.4793  loss_dice_5: 3.491  loss_ce_6: 0.3676  loss_mask_6: 0.4797  loss_dice_6: 3.489  loss_ce_7: 0.3848  loss_mask_7: 0.4799  loss_dice_7: 3.494  loss_ce_8: 0.3724  loss_mask_8: 0.4807  loss_dice_8: 3.488  time: 1.4991  data_time: 0.0729  lr: 7.9965e-06  max_mem: 21492M
[01/17 22:38:55] d2.utils.events INFO:  eta: 12:50:38  iter: 8819  total_loss: 43.32  loss_ce: 0.3595  loss_mask: 0.4738  loss_dice: 3.476  loss_ce_0: 0.5963  loss_mask_0: 0.4586  loss_dice_0: 3.602  loss_ce_1: 0.3756  loss_mask_1: 0.4753  loss_dice_1: 3.513  loss_ce_2: 0.3874  loss_mask_2: 0.4769  loss_dice_2: 3.49  loss_ce_3: 0.3819  loss_mask_3: 0.4724  loss_dice_3: 3.484  loss_ce_4: 0.367  loss_mask_4: 0.4739  loss_dice_4: 3.485  loss_ce_5: 0.3714  loss_mask_5: 0.4779  loss_dice_5: 3.48  loss_ce_6: 0.366  loss_mask_6: 0.4794  loss_dice_6: 3.479  loss_ce_7: 0.3671  loss_mask_7: 0.4806  loss_dice_7: 3.477  loss_ce_8: 0.3734  loss_mask_8: 0.4768  loss_dice_8: 3.476  time: 1.4990  data_time: 0.0811  lr: 7.9918e-06  max_mem: 21492M
[01/17 22:39:25] d2.utils.events INFO:  eta: 12:50:06  iter: 8839  total_loss: 43.58  loss_ce: 0.4011  loss_mask: 0.479  loss_dice: 3.448  loss_ce_0: 0.6023  loss_mask_0: 0.4623  loss_dice_0: 3.587  loss_ce_1: 0.4051  loss_mask_1: 0.4823  loss_dice_1: 3.495  loss_ce_2: 0.4263  loss_mask_2: 0.4786  loss_dice_2: 3.465  loss_ce_3: 0.3974  loss_mask_3: 0.4751  loss_dice_3: 3.457  loss_ce_4: 0.3935  loss_mask_4: 0.475  loss_dice_4: 3.455  loss_ce_5: 0.4019  loss_mask_5: 0.4774  loss_dice_5: 3.451  loss_ce_6: 0.3928  loss_mask_6: 0.4772  loss_dice_6: 3.446  loss_ce_7: 0.4028  loss_mask_7: 0.4788  loss_dice_7: 3.447  loss_ce_8: 0.4008  loss_mask_8: 0.477  loss_dice_8: 3.453  time: 1.4989  data_time: 0.0753  lr: 7.9872e-06  max_mem: 21492M
[01/17 22:39:55] d2.utils.events INFO:  eta: 12:49:46  iter: 8859  total_loss: 43.82  loss_ce: 0.3724  loss_mask: 0.4748  loss_dice: 3.491  loss_ce_0: 0.5959  loss_mask_0: 0.4591  loss_dice_0: 3.608  loss_ce_1: 0.3751  loss_mask_1: 0.4739  loss_dice_1: 3.524  loss_ce_2: 0.3877  loss_mask_2: 0.4723  loss_dice_2: 3.517  loss_ce_3: 0.3887  loss_mask_3: 0.4756  loss_dice_3: 3.492  loss_ce_4: 0.3717  loss_mask_4: 0.4757  loss_dice_4: 3.501  loss_ce_5: 0.3851  loss_mask_5: 0.474  loss_dice_5: 3.498  loss_ce_6: 0.3839  loss_mask_6: 0.4743  loss_dice_6: 3.499  loss_ce_7: 0.3642  loss_mask_7: 0.4761  loss_dice_7: 3.507  loss_ce_8: 0.3671  loss_mask_8: 0.476  loss_dice_8: 3.498  time: 1.4989  data_time: 0.0778  lr: 7.9826e-06  max_mem: 21492M
[01/17 22:40:24] d2.utils.events INFO:  eta: 12:49:22  iter: 8879  total_loss: 44.02  loss_ce: 0.3817  loss_mask: 0.481  loss_dice: 3.459  loss_ce_0: 0.6215  loss_mask_0: 0.4639  loss_dice_0: 3.595  loss_ce_1: 0.3705  loss_mask_1: 0.4855  loss_dice_1: 3.496  loss_ce_2: 0.3966  loss_mask_2: 0.4843  loss_dice_2: 3.482  loss_ce_3: 0.3726  loss_mask_3: 0.4832  loss_dice_3: 3.474  loss_ce_4: 0.3849  loss_mask_4: 0.4849  loss_dice_4: 3.461  loss_ce_5: 0.38  loss_mask_5: 0.4851  loss_dice_5: 3.465  loss_ce_6: 0.3891  loss_mask_6: 0.4827  loss_dice_6: 3.462  loss_ce_7: 0.3837  loss_mask_7: 0.4816  loss_dice_7: 3.467  loss_ce_8: 0.382  loss_mask_8: 0.482  loss_dice_8: 3.464  time: 1.4989  data_time: 0.0695  lr: 7.978e-06  max_mem: 21492M
[01/17 22:40:54] d2.utils.events INFO:  eta: 12:49:10  iter: 8899  total_loss: 44.28  loss_ce: 0.4186  loss_mask: 0.4663  loss_dice: 3.505  loss_ce_0: 0.6349  loss_mask_0: 0.4498  loss_dice_0: 3.63  loss_ce_1: 0.4181  loss_mask_1: 0.4656  loss_dice_1: 3.544  loss_ce_2: 0.4203  loss_mask_2: 0.4649  loss_dice_2: 3.519  loss_ce_3: 0.4017  loss_mask_3: 0.4627  loss_dice_3: 3.515  loss_ce_4: 0.4035  loss_mask_4: 0.4634  loss_dice_4: 3.512  loss_ce_5: 0.401  loss_mask_5: 0.4655  loss_dice_5: 3.51  loss_ce_6: 0.3988  loss_mask_6: 0.4661  loss_dice_6: 3.501  loss_ce_7: 0.3921  loss_mask_7: 0.4646  loss_dice_7: 3.511  loss_ce_8: 0.4025  loss_mask_8: 0.4669  loss_dice_8: 3.506  time: 1.4988  data_time: 0.0752  lr: 7.9734e-06  max_mem: 21492M
[01/17 22:41:24] d2.utils.events INFO:  eta: 12:49:00  iter: 8919  total_loss: 43.77  loss_ce: 0.4003  loss_mask: 0.4724  loss_dice: 3.487  loss_ce_0: 0.5998  loss_mask_0: 0.4587  loss_dice_0: 3.631  loss_ce_1: 0.3917  loss_mask_1: 0.4731  loss_dice_1: 3.532  loss_ce_2: 0.4406  loss_mask_2: 0.4716  loss_dice_2: 3.515  loss_ce_3: 0.4145  loss_mask_3: 0.4732  loss_dice_3: 3.499  loss_ce_4: 0.4117  loss_mask_4: 0.4728  loss_dice_4: 3.493  loss_ce_5: 0.4097  loss_mask_5: 0.4734  loss_dice_5: 3.493  loss_ce_6: 0.4074  loss_mask_6: 0.4692  loss_dice_6: 3.494  loss_ce_7: 0.3906  loss_mask_7: 0.4717  loss_dice_7: 3.49  loss_ce_8: 0.4066  loss_mask_8: 0.4707  loss_dice_8: 3.48  time: 1.4988  data_time: 0.0811  lr: 7.9688e-06  max_mem: 21492M
[01/17 22:41:53] d2.utils.events INFO:  eta: 12:48:33  iter: 8939  total_loss: 43.54  loss_ce: 0.356  loss_mask: 0.458  loss_dice: 3.49  loss_ce_0: 0.6332  loss_mask_0: 0.4508  loss_dice_0: 3.651  loss_ce_1: 0.3878  loss_mask_1: 0.459  loss_dice_1: 3.55  loss_ce_2: 0.3969  loss_mask_2: 0.455  loss_dice_2: 3.524  loss_ce_3: 0.3782  loss_mask_3: 0.4567  loss_dice_3: 3.508  loss_ce_4: 0.3779  loss_mask_4: 0.4558  loss_dice_4: 3.499  loss_ce_5: 0.3616  loss_mask_5: 0.4565  loss_dice_5: 3.497  loss_ce_6: 0.3645  loss_mask_6: 0.4583  loss_dice_6: 3.492  loss_ce_7: 0.3517  loss_mask_7: 0.4579  loss_dice_7: 3.506  loss_ce_8: 0.3637  loss_mask_8: 0.4576  loss_dice_8: 3.499  time: 1.4988  data_time: 0.0781  lr: 7.9642e-06  max_mem: 21492M
[01/17 22:42:23] d2.utils.events INFO:  eta: 12:48:04  iter: 8959  total_loss: 43.97  loss_ce: 0.3858  loss_mask: 0.4754  loss_dice: 3.491  loss_ce_0: 0.6291  loss_mask_0: 0.4537  loss_dice_0: 3.639  loss_ce_1: 0.3999  loss_mask_1: 0.4787  loss_dice_1: 3.539  loss_ce_2: 0.4091  loss_mask_2: 0.4764  loss_dice_2: 3.514  loss_ce_3: 0.3779  loss_mask_3: 0.4771  loss_dice_3: 3.501  loss_ce_4: 0.3757  loss_mask_4: 0.4776  loss_dice_4: 3.499  loss_ce_5: 0.3563  loss_mask_5: 0.4766  loss_dice_5: 3.5  loss_ce_6: 0.3792  loss_mask_6: 0.4751  loss_dice_6: 3.496  loss_ce_7: 0.3851  loss_mask_7: 0.4767  loss_dice_7: 3.495  loss_ce_8: 0.372  loss_mask_8: 0.4765  loss_dice_8: 3.498  time: 1.4987  data_time: 0.0870  lr: 7.9595e-06  max_mem: 21492M
[01/17 22:42:53] d2.utils.events INFO:  eta: 12:47:34  iter: 8979  total_loss: 43.49  loss_ce: 0.3783  loss_mask: 0.4666  loss_dice: 3.447  loss_ce_0: 0.6191  loss_mask_0: 0.4489  loss_dice_0: 3.593  loss_ce_1: 0.3807  loss_mask_1: 0.4677  loss_dice_1: 3.494  loss_ce_2: 0.3895  loss_mask_2: 0.4647  loss_dice_2: 3.468  loss_ce_3: 0.3835  loss_mask_3: 0.4673  loss_dice_3: 3.445  loss_ce_4: 0.3629  loss_mask_4: 0.465  loss_dice_4: 3.453  loss_ce_5: 0.3655  loss_mask_5: 0.4654  loss_dice_5: 3.452  loss_ce_6: 0.3771  loss_mask_6: 0.4667  loss_dice_6: 3.442  loss_ce_7: 0.3738  loss_mask_7: 0.4649  loss_dice_7: 3.447  loss_ce_8: 0.3584  loss_mask_8: 0.4655  loss_dice_8: 3.449  time: 1.4987  data_time: 0.0755  lr: 7.9549e-06  max_mem: 21492M
[01/17 22:43:22] d2.utils.events INFO:  eta: 12:47:07  iter: 8999  total_loss: 43.74  loss_ce: 0.3778  loss_mask: 0.4734  loss_dice: 3.46  loss_ce_0: 0.5866  loss_mask_0: 0.4576  loss_dice_0: 3.602  loss_ce_1: 0.3797  loss_mask_1: 0.4737  loss_dice_1: 3.496  loss_ce_2: 0.4063  loss_mask_2: 0.4724  loss_dice_2: 3.476  loss_ce_3: 0.3985  loss_mask_3: 0.4731  loss_dice_3: 3.464  loss_ce_4: 0.3813  loss_mask_4: 0.4705  loss_dice_4: 3.468  loss_ce_5: 0.3873  loss_mask_5: 0.4737  loss_dice_5: 3.466  loss_ce_6: 0.3844  loss_mask_6: 0.4741  loss_dice_6: 3.463  loss_ce_7: 0.3824  loss_mask_7: 0.4737  loss_dice_7: 3.461  loss_ce_8: 0.4027  loss_mask_8: 0.473  loss_dice_8: 3.469  time: 1.4986  data_time: 0.0717  lr: 7.9503e-06  max_mem: 21492M
[01/17 22:43:52] d2.utils.events INFO:  eta: 12:46:41  iter: 9019  total_loss: 44.58  loss_ce: 0.3891  loss_mask: 0.4758  loss_dice: 3.511  loss_ce_0: 0.6319  loss_mask_0: 0.4562  loss_dice_0: 3.632  loss_ce_1: 0.3996  loss_mask_1: 0.4764  loss_dice_1: 3.546  loss_ce_2: 0.4093  loss_mask_2: 0.4753  loss_dice_2: 3.516  loss_ce_3: 0.3757  loss_mask_3: 0.4736  loss_dice_3: 3.509  loss_ce_4: 0.3818  loss_mask_4: 0.4758  loss_dice_4: 3.51  loss_ce_5: 0.3598  loss_mask_5: 0.4758  loss_dice_5: 3.512  loss_ce_6: 0.3766  loss_mask_6: 0.4745  loss_dice_6: 3.513  loss_ce_7: 0.387  loss_mask_7: 0.4751  loss_dice_7: 3.512  loss_ce_8: 0.3821  loss_mask_8: 0.4757  loss_dice_8: 3.519  time: 1.4986  data_time: 0.0709  lr: 7.9457e-06  max_mem: 21492M
[01/17 22:44:22] d2.utils.events INFO:  eta: 12:46:17  iter: 9039  total_loss: 43.66  loss_ce: 0.4085  loss_mask: 0.456  loss_dice: 3.434  loss_ce_0: 0.6096  loss_mask_0: 0.4452  loss_dice_0: 3.57  loss_ce_1: 0.4006  loss_mask_1: 0.4618  loss_dice_1: 3.477  loss_ce_2: 0.4049  loss_mask_2: 0.4593  loss_dice_2: 3.452  loss_ce_3: 0.3869  loss_mask_3: 0.458  loss_dice_3: 3.44  loss_ce_4: 0.3985  loss_mask_4: 0.4574  loss_dice_4: 3.437  loss_ce_5: 0.3913  loss_mask_5: 0.4562  loss_dice_5: 3.437  loss_ce_6: 0.3928  loss_mask_6: 0.4582  loss_dice_6: 3.424  loss_ce_7: 0.3759  loss_mask_7: 0.4553  loss_dice_7: 3.445  loss_ce_8: 0.39  loss_mask_8: 0.4563  loss_dice_8: 3.441  time: 1.4986  data_time: 0.0760  lr: 7.9411e-06  max_mem: 21492M
[01/17 22:44:52] d2.utils.events INFO:  eta: 12:45:51  iter: 9059  total_loss: 43.95  loss_ce: 0.39  loss_mask: 0.4717  loss_dice: 3.492  loss_ce_0: 0.5727  loss_mask_0: 0.4592  loss_dice_0: 3.62  loss_ce_1: 0.3718  loss_mask_1: 0.4715  loss_dice_1: 3.532  loss_ce_2: 0.3926  loss_mask_2: 0.4697  loss_dice_2: 3.502  loss_ce_3: 0.3964  loss_mask_3: 0.4708  loss_dice_3: 3.492  loss_ce_4: 0.3945  loss_mask_4: 0.4723  loss_dice_4: 3.498  loss_ce_5: 0.3789  loss_mask_5: 0.4714  loss_dice_5: 3.492  loss_ce_6: 0.3894  loss_mask_6: 0.4714  loss_dice_6: 3.495  loss_ce_7: 0.3813  loss_mask_7: 0.471  loss_dice_7: 3.486  loss_ce_8: 0.3788  loss_mask_8: 0.4716  loss_dice_8: 3.5  time: 1.4986  data_time: 0.0922  lr: 7.9365e-06  max_mem: 21492M
[01/17 22:45:22] d2.utils.events INFO:  eta: 12:45:12  iter: 9079  total_loss: 44.1  loss_ce: 0.3938  loss_mask: 0.4885  loss_dice: 3.484  loss_ce_0: 0.6021  loss_mask_0: 0.4648  loss_dice_0: 3.597  loss_ce_1: 0.3807  loss_mask_1: 0.4902  loss_dice_1: 3.506  loss_ce_2: 0.3957  loss_mask_2: 0.4901  loss_dice_2: 3.497  loss_ce_3: 0.3747  loss_mask_3: 0.4903  loss_dice_3: 3.489  loss_ce_4: 0.372  loss_mask_4: 0.4886  loss_dice_4: 3.487  loss_ce_5: 0.385  loss_mask_5: 0.4884  loss_dice_5: 3.489  loss_ce_6: 0.3727  loss_mask_6: 0.4901  loss_dice_6: 3.481  loss_ce_7: 0.3599  loss_mask_7: 0.4899  loss_dice_7: 3.485  loss_ce_8: 0.3672  loss_mask_8: 0.4882  loss_dice_8: 3.491  time: 1.4986  data_time: 0.0723  lr: 7.9318e-06  max_mem: 21492M
[01/17 22:45:51] d2.utils.events INFO:  eta: 12:44:53  iter: 9099  total_loss: 44.76  loss_ce: 0.3809  loss_mask: 0.49  loss_dice: 3.512  loss_ce_0: 0.6217  loss_mask_0: 0.482  loss_dice_0: 3.642  loss_ce_1: 0.3884  loss_mask_1: 0.5039  loss_dice_1: 3.541  loss_ce_2: 0.4246  loss_mask_2: 0.4978  loss_dice_2: 3.521  loss_ce_3: 0.3992  loss_mask_3: 0.4938  loss_dice_3: 3.518  loss_ce_4: 0.4033  loss_mask_4: 0.4925  loss_dice_4: 3.505  loss_ce_5: 0.4115  loss_mask_5: 0.4912  loss_dice_5: 3.512  loss_ce_6: 0.3957  loss_mask_6: 0.4902  loss_dice_6: 3.527  loss_ce_7: 0.4006  loss_mask_7: 0.4898  loss_dice_7: 3.508  loss_ce_8: 0.4077  loss_mask_8: 0.4881  loss_dice_8: 3.518  time: 1.4985  data_time: 0.0691  lr: 7.9272e-06  max_mem: 21492M
[01/17 22:46:21] d2.utils.events INFO:  eta: 12:44:36  iter: 9119  total_loss: 43.34  loss_ce: 0.3627  loss_mask: 0.4683  loss_dice: 3.466  loss_ce_0: 0.6072  loss_mask_0: 0.451  loss_dice_0: 3.584  loss_ce_1: 0.3851  loss_mask_1: 0.4669  loss_dice_1: 3.496  loss_ce_2: 0.3907  loss_mask_2: 0.4679  loss_dice_2: 3.471  loss_ce_3: 0.3898  loss_mask_3: 0.4666  loss_dice_3: 3.466  loss_ce_4: 0.3832  loss_mask_4: 0.4651  loss_dice_4: 3.464  loss_ce_5: 0.3744  loss_mask_5: 0.4677  loss_dice_5: 3.46  loss_ce_6: 0.3862  loss_mask_6: 0.4667  loss_dice_6: 3.455  loss_ce_7: 0.3594  loss_mask_7: 0.4688  loss_dice_7: 3.465  loss_ce_8: 0.364  loss_mask_8: 0.4712  loss_dice_8: 3.457  time: 1.4985  data_time: 0.0690  lr: 7.9226e-06  max_mem: 21492M
[01/17 22:46:51] d2.utils.events INFO:  eta: 12:43:33  iter: 9139  total_loss: 43.88  loss_ce: 0.3596  loss_mask: 0.4798  loss_dice: 3.492  loss_ce_0: 0.6354  loss_mask_0: 0.4575  loss_dice_0: 3.636  loss_ce_1: 0.387  loss_mask_1: 0.4791  loss_dice_1: 3.536  loss_ce_2: 0.3899  loss_mask_2: 0.4743  loss_dice_2: 3.512  loss_ce_3: 0.3864  loss_mask_3: 0.4792  loss_dice_3: 3.496  loss_ce_4: 0.3859  loss_mask_4: 0.4773  loss_dice_4: 3.496  loss_ce_5: 0.3978  loss_mask_5: 0.4765  loss_dice_5: 3.497  loss_ce_6: 0.3661  loss_mask_6: 0.4768  loss_dice_6: 3.495  loss_ce_7: 0.3622  loss_mask_7: 0.4775  loss_dice_7: 3.498  loss_ce_8: 0.3679  loss_mask_8: 0.4795  loss_dice_8: 3.497  time: 1.4984  data_time: 0.0652  lr: 7.918e-06  max_mem: 21492M
[01/17 22:47:21] d2.utils.events INFO:  eta: 12:43:04  iter: 9159  total_loss: 43.64  loss_ce: 0.3708  loss_mask: 0.4715  loss_dice: 3.464  loss_ce_0: 0.6514  loss_mask_0: 0.4519  loss_dice_0: 3.595  loss_ce_1: 0.3912  loss_mask_1: 0.474  loss_dice_1: 3.506  loss_ce_2: 0.4201  loss_mask_2: 0.4704  loss_dice_2: 3.479  loss_ce_3: 0.3937  loss_mask_3: 0.4735  loss_dice_3: 3.469  loss_ce_4: 0.3655  loss_mask_4: 0.4741  loss_dice_4: 3.466  loss_ce_5: 0.3785  loss_mask_5: 0.4749  loss_dice_5: 3.471  loss_ce_6: 0.376  loss_mask_6: 0.4788  loss_dice_6: 3.467  loss_ce_7: 0.3769  loss_mask_7: 0.4782  loss_dice_7: 3.466  loss_ce_8: 0.3681  loss_mask_8: 0.4755  loss_dice_8: 3.462  time: 1.4984  data_time: 0.0704  lr: 7.9134e-06  max_mem: 21492M
[01/17 22:47:50] d2.utils.events INFO:  eta: 12:42:13  iter: 9179  total_loss: 43.25  loss_ce: 0.383  loss_mask: 0.475  loss_dice: 3.426  loss_ce_0: 0.6278  loss_mask_0: 0.4547  loss_dice_0: 3.558  loss_ce_1: 0.3988  loss_mask_1: 0.4749  loss_dice_1: 3.466  loss_ce_2: 0.413  loss_mask_2: 0.4739  loss_dice_2: 3.444  loss_ce_3: 0.3805  loss_mask_3: 0.4726  loss_dice_3: 3.434  loss_ce_4: 0.387  loss_mask_4: 0.4728  loss_dice_4: 3.432  loss_ce_5: 0.3746  loss_mask_5: 0.4769  loss_dice_5: 3.43  loss_ce_6: 0.3815  loss_mask_6: 0.4746  loss_dice_6: 3.426  loss_ce_7: 0.3747  loss_mask_7: 0.4776  loss_dice_7: 3.431  loss_ce_8: 0.38  loss_mask_8: 0.4762  loss_dice_8: 3.436  time: 1.4983  data_time: 0.0594  lr: 7.9088e-06  max_mem: 21492M
[01/17 22:48:20] d2.utils.events INFO:  eta: 12:41:51  iter: 9199  total_loss: 43.04  loss_ce: 0.3706  loss_mask: 0.4732  loss_dice: 3.428  loss_ce_0: 0.6035  loss_mask_0: 0.4514  loss_dice_0: 3.56  loss_ce_1: 0.3803  loss_mask_1: 0.4757  loss_dice_1: 3.472  loss_ce_2: 0.3844  loss_mask_2: 0.4731  loss_dice_2: 3.435  loss_ce_3: 0.3735  loss_mask_3: 0.4716  loss_dice_3: 3.433  loss_ce_4: 0.3708  loss_mask_4: 0.4761  loss_dice_4: 3.432  loss_ce_5: 0.3643  loss_mask_5: 0.4766  loss_dice_5: 3.437  loss_ce_6: 0.3638  loss_mask_6: 0.4748  loss_dice_6: 3.436  loss_ce_7: 0.3515  loss_mask_7: 0.473  loss_dice_7: 3.438  loss_ce_8: 0.3623  loss_mask_8: 0.4736  loss_dice_8: 3.435  time: 1.4983  data_time: 0.0789  lr: 7.9041e-06  max_mem: 21492M
[01/17 22:48:50] d2.utils.events INFO:  eta: 12:41:36  iter: 9219  total_loss: 44.67  loss_ce: 0.4173  loss_mask: 0.4648  loss_dice: 3.554  loss_ce_0: 0.6507  loss_mask_0: 0.452  loss_dice_0: 3.673  loss_ce_1: 0.4154  loss_mask_1: 0.4663  loss_dice_1: 3.584  loss_ce_2: 0.4442  loss_mask_2: 0.4611  loss_dice_2: 3.576  loss_ce_3: 0.4166  loss_mask_3: 0.4639  loss_dice_3: 3.559  loss_ce_4: 0.4136  loss_mask_4: 0.4635  loss_dice_4: 3.558  loss_ce_5: 0.4094  loss_mask_5: 0.4624  loss_dice_5: 3.554  loss_ce_6: 0.4184  loss_mask_6: 0.4634  loss_dice_6: 3.556  loss_ce_7: 0.3998  loss_mask_7: 0.4641  loss_dice_7: 3.552  loss_ce_8: 0.4151  loss_mask_8: 0.4632  loss_dice_8: 3.557  time: 1.4984  data_time: 0.0729  lr: 7.8995e-06  max_mem: 21492M
[01/17 22:49:21] d2.utils.events INFO:  eta: 12:41:16  iter: 9239  total_loss: 43.86  loss_ce: 0.4078  loss_mask: 0.4708  loss_dice: 3.468  loss_ce_0: 0.6178  loss_mask_0: 0.4499  loss_dice_0: 3.601  loss_ce_1: 0.4078  loss_mask_1: 0.4679  loss_dice_1: 3.508  loss_ce_2: 0.4116  loss_mask_2: 0.4702  loss_dice_2: 3.479  loss_ce_3: 0.4127  loss_mask_3: 0.4691  loss_dice_3: 3.477  loss_ce_4: 0.405  loss_mask_4: 0.4701  loss_dice_4: 3.479  loss_ce_5: 0.4005  loss_mask_5: 0.4713  loss_dice_5: 3.475  loss_ce_6: 0.3998  loss_mask_6: 0.469  loss_dice_6: 3.475  loss_ce_7: 0.3914  loss_mask_7: 0.471  loss_dice_7: 3.474  loss_ce_8: 0.3861  loss_mask_8: 0.4714  loss_dice_8: 3.479  time: 1.4984  data_time: 0.0749  lr: 7.8949e-06  max_mem: 21492M
[01/17 22:49:50] d2.utils.events INFO:  eta: 12:40:45  iter: 9259  total_loss: 43.24  loss_ce: 0.3806  loss_mask: 0.4818  loss_dice: 3.421  loss_ce_0: 0.6315  loss_mask_0: 0.4706  loss_dice_0: 3.546  loss_ce_1: 0.3785  loss_mask_1: 0.4854  loss_dice_1: 3.459  loss_ce_2: 0.4009  loss_mask_2: 0.4858  loss_dice_2: 3.429  loss_ce_3: 0.3864  loss_mask_3: 0.4834  loss_dice_3: 3.419  loss_ce_4: 0.382  loss_mask_4: 0.4852  loss_dice_4: 3.423  loss_ce_5: 0.3741  loss_mask_5: 0.4861  loss_dice_5: 3.433  loss_ce_6: 0.3773  loss_mask_6: 0.4865  loss_dice_6: 3.424  loss_ce_7: 0.368  loss_mask_7: 0.4844  loss_dice_7: 3.431  loss_ce_8: 0.3879  loss_mask_8: 0.4842  loss_dice_8: 3.423  time: 1.4984  data_time: 0.0701  lr: 7.8903e-06  max_mem: 21492M
[01/17 22:50:21] d2.utils.events INFO:  eta: 12:40:24  iter: 9279  total_loss: 43.46  loss_ce: 0.3802  loss_mask: 0.4729  loss_dice: 3.452  loss_ce_0: 0.6626  loss_mask_0: 0.4539  loss_dice_0: 3.589  loss_ce_1: 0.3791  loss_mask_1: 0.4726  loss_dice_1: 3.489  loss_ce_2: 0.3876  loss_mask_2: 0.4762  loss_dice_2: 3.46  loss_ce_3: 0.3777  loss_mask_3: 0.4764  loss_dice_3: 3.458  loss_ce_4: 0.3678  loss_mask_4: 0.4736  loss_dice_4: 3.455  loss_ce_5: 0.3691  loss_mask_5: 0.4745  loss_dice_5: 3.46  loss_ce_6: 0.3757  loss_mask_6: 0.475  loss_dice_6: 3.452  loss_ce_7: 0.376  loss_mask_7: 0.4735  loss_dice_7: 3.457  loss_ce_8: 0.3683  loss_mask_8: 0.4733  loss_dice_8: 3.453  time: 1.4984  data_time: 0.0695  lr: 7.8857e-06  max_mem: 21492M
[01/17 22:50:50] d2.utils.events INFO:  eta: 12:39:47  iter: 9299  total_loss: 43.47  loss_ce: 0.3592  loss_mask: 0.4684  loss_dice: 3.466  loss_ce_0: 0.5897  loss_mask_0: 0.4572  loss_dice_0: 3.595  loss_ce_1: 0.3633  loss_mask_1: 0.4725  loss_dice_1: 3.497  loss_ce_2: 0.3687  loss_mask_2: 0.4672  loss_dice_2: 3.475  loss_ce_3: 0.3673  loss_mask_3: 0.4672  loss_dice_3: 3.465  loss_ce_4: 0.3567  loss_mask_4: 0.4678  loss_dice_4: 3.46  loss_ce_5: 0.3735  loss_mask_5: 0.4665  loss_dice_5: 3.458  loss_ce_6: 0.365  loss_mask_6: 0.4671  loss_dice_6: 3.46  loss_ce_7: 0.3661  loss_mask_7: 0.4669  loss_dice_7: 3.461  loss_ce_8: 0.3505  loss_mask_8: 0.467  loss_dice_8: 3.459  time: 1.4984  data_time: 0.0791  lr: 7.881e-06  max_mem: 21492M
[01/17 22:51:21] d2.utils.events INFO:  eta: 12:39:26  iter: 9319  total_loss: 43.98  loss_ce: 0.3953  loss_mask: 0.4812  loss_dice: 3.478  loss_ce_0: 0.5943  loss_mask_0: 0.462  loss_dice_0: 3.616  loss_ce_1: 0.3901  loss_mask_1: 0.4791  loss_dice_1: 3.51  loss_ce_2: 0.3941  loss_mask_2: 0.4756  loss_dice_2: 3.501  loss_ce_3: 0.39  loss_mask_3: 0.478  loss_dice_3: 3.492  loss_ce_4: 0.4008  loss_mask_4: 0.4795  loss_dice_4: 3.485  loss_ce_5: 0.3858  loss_mask_5: 0.4827  loss_dice_5: 3.482  loss_ce_6: 0.398  loss_mask_6: 0.4798  loss_dice_6: 3.483  loss_ce_7: 0.3926  loss_mask_7: 0.4802  loss_dice_7: 3.487  loss_ce_8: 0.3899  loss_mask_8: 0.4816  loss_dice_8: 3.473  time: 1.4984  data_time: 0.0844  lr: 7.8764e-06  max_mem: 21492M
[01/17 22:51:51] d2.utils.events INFO:  eta: 12:38:52  iter: 9339  total_loss: 43.46  loss_ce: 0.3712  loss_mask: 0.4777  loss_dice: 3.438  loss_ce_0: 0.6233  loss_mask_0: 0.4481  loss_dice_0: 3.568  loss_ce_1: 0.3874  loss_mask_1: 0.4724  loss_dice_1: 3.463  loss_ce_2: 0.4047  loss_mask_2: 0.4772  loss_dice_2: 3.438  loss_ce_3: 0.3937  loss_mask_3: 0.4776  loss_dice_3: 3.414  loss_ce_4: 0.3944  loss_mask_4: 0.4793  loss_dice_4: 3.415  loss_ce_5: 0.3758  loss_mask_5: 0.4804  loss_dice_5: 3.429  loss_ce_6: 0.3783  loss_mask_6: 0.4816  loss_dice_6: 3.432  loss_ce_7: 0.3836  loss_mask_7: 0.4787  loss_dice_7: 3.433  loss_ce_8: 0.3902  loss_mask_8: 0.4834  loss_dice_8: 3.438  time: 1.4985  data_time: 0.0810  lr: 7.8718e-06  max_mem: 21492M
[01/17 22:52:21] d2.utils.events INFO:  eta: 12:38:33  iter: 9359  total_loss: 43.67  loss_ce: 0.3807  loss_mask: 0.4596  loss_dice: 3.448  loss_ce_0: 0.6451  loss_mask_0: 0.4401  loss_dice_0: 3.594  loss_ce_1: 0.3902  loss_mask_1: 0.4611  loss_dice_1: 3.497  loss_ce_2: 0.3994  loss_mask_2: 0.4586  loss_dice_2: 3.467  loss_ce_3: 0.3971  loss_mask_3: 0.4572  loss_dice_3: 3.457  loss_ce_4: 0.3725  loss_mask_4: 0.4587  loss_dice_4: 3.458  loss_ce_5: 0.3767  loss_mask_5: 0.4614  loss_dice_5: 3.454  loss_ce_6: 0.3825  loss_mask_6: 0.4627  loss_dice_6: 3.454  loss_ce_7: 0.3722  loss_mask_7: 0.4625  loss_dice_7: 3.457  loss_ce_8: 0.3752  loss_mask_8: 0.4618  loss_dice_8: 3.46  time: 1.4985  data_time: 0.0737  lr: 7.8672e-06  max_mem: 21492M
[01/17 22:52:52] d2.utils.events INFO:  eta: 12:38:09  iter: 9379  total_loss: 43.46  loss_ce: 0.3622  loss_mask: 0.485  loss_dice: 3.41  loss_ce_0: 0.6137  loss_mask_0: 0.4627  loss_dice_0: 3.552  loss_ce_1: 0.3867  loss_mask_1: 0.4883  loss_dice_1: 3.444  loss_ce_2: 0.4033  loss_mask_2: 0.4888  loss_dice_2: 3.428  loss_ce_3: 0.3796  loss_mask_3: 0.4846  loss_dice_3: 3.41  loss_ce_4: 0.3923  loss_mask_4: 0.4863  loss_dice_4: 3.409  loss_ce_5: 0.386  loss_mask_5: 0.4828  loss_dice_5: 3.41  loss_ce_6: 0.3699  loss_mask_6: 0.484  loss_dice_6: 3.405  loss_ce_7: 0.3852  loss_mask_7: 0.4852  loss_dice_7: 3.407  loss_ce_8: 0.3695  loss_mask_8: 0.4836  loss_dice_8: 3.417  time: 1.4985  data_time: 0.0853  lr: 7.8626e-06  max_mem: 21492M
[01/17 22:53:22] d2.utils.events INFO:  eta: 12:37:39  iter: 9399  total_loss: 43.58  loss_ce: 0.3687  loss_mask: 0.4693  loss_dice: 3.458  loss_ce_0: 0.6049  loss_mask_0: 0.4505  loss_dice_0: 3.578  loss_ce_1: 0.3691  loss_mask_1: 0.4685  loss_dice_1: 3.501  loss_ce_2: 0.3974  loss_mask_2: 0.4685  loss_dice_2: 3.47  loss_ce_3: 0.3774  loss_mask_3: 0.4676  loss_dice_3: 3.462  loss_ce_4: 0.382  loss_mask_4: 0.4682  loss_dice_4: 3.451  loss_ce_5: 0.3989  loss_mask_5: 0.467  loss_dice_5: 3.458  loss_ce_6: 0.3712  loss_mask_6: 0.4684  loss_dice_6: 3.464  loss_ce_7: 0.3637  loss_mask_7: 0.4675  loss_dice_7: 3.461  loss_ce_8: 0.3802  loss_mask_8: 0.4713  loss_dice_8: 3.459  time: 1.4985  data_time: 0.0933  lr: 7.8579e-06  max_mem: 21492M
[01/17 22:53:52] d2.utils.events INFO:  eta: 12:36:43  iter: 9419  total_loss: 43.33  loss_ce: 0.3571  loss_mask: 0.479  loss_dice: 3.458  loss_ce_0: 0.5735  loss_mask_0: 0.4509  loss_dice_0: 3.599  loss_ce_1: 0.3557  loss_mask_1: 0.4768  loss_dice_1: 3.507  loss_ce_2: 0.384  loss_mask_2: 0.4742  loss_dice_2: 3.478  loss_ce_3: 0.36  loss_mask_3: 0.478  loss_dice_3: 3.458  loss_ce_4: 0.355  loss_mask_4: 0.4777  loss_dice_4: 3.452  loss_ce_5: 0.3561  loss_mask_5: 0.4793  loss_dice_5: 3.462  loss_ce_6: 0.3655  loss_mask_6: 0.4782  loss_dice_6: 3.461  loss_ce_7: 0.352  loss_mask_7: 0.4776  loss_dice_7: 3.466  loss_ce_8: 0.3646  loss_mask_8: 0.4791  loss_dice_8: 3.458  time: 1.4985  data_time: 0.0697  lr: 7.8533e-06  max_mem: 21492M
[01/17 22:54:22] d2.utils.events INFO:  eta: 12:36:10  iter: 9439  total_loss: 43.65  loss_ce: 0.3798  loss_mask: 0.4686  loss_dice: 3.467  loss_ce_0: 0.6165  loss_mask_0: 0.4571  loss_dice_0: 3.604  loss_ce_1: 0.3831  loss_mask_1: 0.4748  loss_dice_1: 3.509  loss_ce_2: 0.4049  loss_mask_2: 0.4743  loss_dice_2: 3.465  loss_ce_3: 0.3846  loss_mask_3: 0.4727  loss_dice_3: 3.462  loss_ce_4: 0.3919  loss_mask_4: 0.471  loss_dice_4: 3.458  loss_ce_5: 0.3669  loss_mask_5: 0.4705  loss_dice_5: 3.47  loss_ce_6: 0.3877  loss_mask_6: 0.4711  loss_dice_6: 3.446  loss_ce_7: 0.3885  loss_mask_7: 0.4698  loss_dice_7: 3.462  loss_ce_8: 0.3812  loss_mask_8: 0.4707  loss_dice_8: 3.456  time: 1.4985  data_time: 0.0880  lr: 7.8487e-06  max_mem: 21492M
[01/17 22:54:51] d2.utils.events INFO:  eta: 12:35:39  iter: 9459  total_loss: 43.5  loss_ce: 0.4134  loss_mask: 0.4712  loss_dice: 3.444  loss_ce_0: 0.6302  loss_mask_0: 0.4496  loss_dice_0: 3.575  loss_ce_1: 0.4045  loss_mask_1: 0.467  loss_dice_1: 3.49  loss_ce_2: 0.4311  loss_mask_2: 0.4619  loss_dice_2: 3.466  loss_ce_3: 0.4192  loss_mask_3: 0.4666  loss_dice_3: 3.441  loss_ce_4: 0.4084  loss_mask_4: 0.4667  loss_dice_4: 3.449  loss_ce_5: 0.3965  loss_mask_5: 0.4684  loss_dice_5: 3.443  loss_ce_6: 0.4044  loss_mask_6: 0.4697  loss_dice_6: 3.441  loss_ce_7: 0.4049  loss_mask_7: 0.4688  loss_dice_7: 3.444  loss_ce_8: 0.4085  loss_mask_8: 0.4695  loss_dice_8: 3.444  time: 1.4984  data_time: 0.0721  lr: 7.8441e-06  max_mem: 21492M
[01/17 22:55:21] d2.utils.events INFO:  eta: 12:35:14  iter: 9479  total_loss: 43.87  loss_ce: 0.4166  loss_mask: 0.477  loss_dice: 3.441  loss_ce_0: 0.614  loss_mask_0: 0.4646  loss_dice_0: 3.569  loss_ce_1: 0.3973  loss_mask_1: 0.4783  loss_dice_1: 3.48  loss_ce_2: 0.4317  loss_mask_2: 0.4788  loss_dice_2: 3.457  loss_ce_3: 0.4274  loss_mask_3: 0.4784  loss_dice_3: 3.446  loss_ce_4: 0.4372  loss_mask_4: 0.4786  loss_dice_4: 3.436  loss_ce_5: 0.4118  loss_mask_5: 0.4811  loss_dice_5: 3.445  loss_ce_6: 0.42  loss_mask_6: 0.4787  loss_dice_6: 3.448  loss_ce_7: 0.4132  loss_mask_7: 0.4782  loss_dice_7: 3.442  loss_ce_8: 0.4036  loss_mask_8: 0.48  loss_dice_8: 3.445  time: 1.4985  data_time: 0.0687  lr: 7.8394e-06  max_mem: 21492M
[01/17 22:55:51] d2.utils.events INFO:  eta: 12:34:41  iter: 9499  total_loss: 43.23  loss_ce: 0.3825  loss_mask: 0.4646  loss_dice: 3.448  loss_ce_0: 0.6148  loss_mask_0: 0.4462  loss_dice_0: 3.578  loss_ce_1: 0.3741  loss_mask_1: 0.4676  loss_dice_1: 3.489  loss_ce_2: 0.4107  loss_mask_2: 0.4667  loss_dice_2: 3.462  loss_ce_3: 0.3836  loss_mask_3: 0.465  loss_dice_3: 3.454  loss_ce_4: 0.3885  loss_mask_4: 0.4659  loss_dice_4: 3.439  loss_ce_5: 0.376  loss_mask_5: 0.4664  loss_dice_5: 3.45  loss_ce_6: 0.3832  loss_mask_6: 0.466  loss_dice_6: 3.443  loss_ce_7: 0.3839  loss_mask_7: 0.4624  loss_dice_7: 3.451  loss_ce_8: 0.3739  loss_mask_8: 0.4655  loss_dice_8: 3.452  time: 1.4984  data_time: 0.0643  lr: 7.8348e-06  max_mem: 21492M
[01/17 22:56:21] d2.utils.events INFO:  eta: 12:34:00  iter: 9519  total_loss: 43.76  loss_ce: 0.3942  loss_mask: 0.4616  loss_dice: 3.463  loss_ce_0: 0.589  loss_mask_0: 0.443  loss_dice_0: 3.587  loss_ce_1: 0.3862  loss_mask_1: 0.462  loss_dice_1: 3.504  loss_ce_2: 0.403  loss_mask_2: 0.4605  loss_dice_2: 3.483  loss_ce_3: 0.389  loss_mask_3: 0.4615  loss_dice_3: 3.472  loss_ce_4: 0.388  loss_mask_4: 0.4608  loss_dice_4: 3.463  loss_ce_5: 0.3647  loss_mask_5: 0.4622  loss_dice_5: 3.462  loss_ce_6: 0.3728  loss_mask_6: 0.4645  loss_dice_6: 3.466  loss_ce_7: 0.3879  loss_mask_7: 0.4633  loss_dice_7: 3.465  loss_ce_8: 0.3758  loss_mask_8: 0.462  loss_dice_8: 3.463  time: 1.4984  data_time: 0.0651  lr: 7.8302e-06  max_mem: 21492M
[01/17 22:56:51] d2.utils.events INFO:  eta: 12:33:36  iter: 9539  total_loss: 43.67  loss_ce: 0.401  loss_mask: 0.4681  loss_dice: 3.459  loss_ce_0: 0.618  loss_mask_0: 0.4507  loss_dice_0: 3.581  loss_ce_1: 0.4302  loss_mask_1: 0.465  loss_dice_1: 3.496  loss_ce_2: 0.4303  loss_mask_2: 0.4657  loss_dice_2: 3.474  loss_ce_3: 0.4202  loss_mask_3: 0.4653  loss_dice_3: 3.463  loss_ce_4: 0.4086  loss_mask_4: 0.465  loss_dice_4: 3.46  loss_ce_5: 0.4021  loss_mask_5: 0.4674  loss_dice_5: 3.458  loss_ce_6: 0.3991  loss_mask_6: 0.4653  loss_dice_6: 3.461  loss_ce_7: 0.4209  loss_mask_7: 0.4703  loss_dice_7: 3.46  loss_ce_8: 0.3943  loss_mask_8: 0.4705  loss_dice_8: 3.454  time: 1.4984  data_time: 0.0820  lr: 7.8256e-06  max_mem: 21492M
[01/17 22:57:20] d2.utils.events INFO:  eta: 12:32:53  iter: 9559  total_loss: 43.58  loss_ce: 0.3944  loss_mask: 0.4648  loss_dice: 3.455  loss_ce_0: 0.6083  loss_mask_0: 0.4488  loss_dice_0: 3.579  loss_ce_1: 0.4237  loss_mask_1: 0.4684  loss_dice_1: 3.49  loss_ce_2: 0.4346  loss_mask_2: 0.4641  loss_dice_2: 3.47  loss_ce_3: 0.4086  loss_mask_3: 0.4648  loss_dice_3: 3.45  loss_ce_4: 0.4175  loss_mask_4: 0.4664  loss_dice_4: 3.452  loss_ce_5: 0.4087  loss_mask_5: 0.4653  loss_dice_5: 3.462  loss_ce_6: 0.3904  loss_mask_6: 0.465  loss_dice_6: 3.449  loss_ce_7: 0.3981  loss_mask_7: 0.4661  loss_dice_7: 3.465  loss_ce_8: 0.4057  loss_mask_8: 0.4649  loss_dice_8: 3.451  time: 1.4983  data_time: 0.0652  lr: 7.8209e-06  max_mem: 21492M
[01/17 22:57:50] d2.utils.events INFO:  eta: 12:31:30  iter: 9579  total_loss: 43.38  loss_ce: 0.4061  loss_mask: 0.4817  loss_dice: 3.411  loss_ce_0: 0.6137  loss_mask_0: 0.4642  loss_dice_0: 3.533  loss_ce_1: 0.406  loss_mask_1: 0.486  loss_dice_1: 3.445  loss_ce_2: 0.4232  loss_mask_2: 0.4814  loss_dice_2: 3.42  loss_ce_3: 0.4179  loss_mask_3: 0.4829  loss_dice_3: 3.414  loss_ce_4: 0.4063  loss_mask_4: 0.4798  loss_dice_4: 3.408  loss_ce_5: 0.4137  loss_mask_5: 0.4822  loss_dice_5: 3.412  loss_ce_6: 0.4025  loss_mask_6: 0.4835  loss_dice_6: 3.405  loss_ce_7: 0.4071  loss_mask_7: 0.4842  loss_dice_7: 3.415  loss_ce_8: 0.3968  loss_mask_8: 0.4841  loss_dice_8: 3.418  time: 1.4983  data_time: 0.0766  lr: 7.8163e-06  max_mem: 21492M
[01/17 22:58:20] d2.utils.events INFO:  eta: 12:31:53  iter: 9599  total_loss: 43.06  loss_ce: 0.3819  loss_mask: 0.4712  loss_dice: 3.417  loss_ce_0: 0.5876  loss_mask_0: 0.448  loss_dice_0: 3.56  loss_ce_1: 0.366  loss_mask_1: 0.4706  loss_dice_1: 3.449  loss_ce_2: 0.3805  loss_mask_2: 0.4686  loss_dice_2: 3.432  loss_ce_3: 0.3751  loss_mask_3: 0.4719  loss_dice_3: 3.43  loss_ce_4: 0.3781  loss_mask_4: 0.4692  loss_dice_4: 3.435  loss_ce_5: 0.362  loss_mask_5: 0.468  loss_dice_5: 3.431  loss_ce_6: 0.373  loss_mask_6: 0.4682  loss_dice_6: 3.42  loss_ce_7: 0.3557  loss_mask_7: 0.4712  loss_dice_7: 3.44  loss_ce_8: 0.366  loss_mask_8: 0.4718  loss_dice_8: 3.425  time: 1.4983  data_time: 0.0863  lr: 7.8117e-06  max_mem: 21492M
[01/17 22:58:50] d2.utils.events INFO:  eta: 12:31:20  iter: 9619  total_loss: 43.49  loss_ce: 0.3874  loss_mask: 0.4715  loss_dice: 3.442  loss_ce_0: 0.6313  loss_mask_0: 0.4527  loss_dice_0: 3.561  loss_ce_1: 0.3826  loss_mask_1: 0.4714  loss_dice_1: 3.459  loss_ce_2: 0.4327  loss_mask_2: 0.4699  loss_dice_2: 3.442  loss_ce_3: 0.3938  loss_mask_3: 0.473  loss_dice_3: 3.437  loss_ce_4: 0.3869  loss_mask_4: 0.4722  loss_dice_4: 3.445  loss_ce_5: 0.3795  loss_mask_5: 0.4726  loss_dice_5: 3.44  loss_ce_6: 0.3783  loss_mask_6: 0.472  loss_dice_6: 3.435  loss_ce_7: 0.3811  loss_mask_7: 0.4702  loss_dice_7: 3.437  loss_ce_8: 0.3645  loss_mask_8: 0.4723  loss_dice_8: 3.432  time: 1.4983  data_time: 0.0850  lr: 7.8071e-06  max_mem: 21492M
[01/17 22:59:20] d2.utils.events INFO:  eta: 12:30:41  iter: 9639  total_loss: 43.65  loss_ce: 0.3829  loss_mask: 0.4727  loss_dice: 3.424  loss_ce_0: 0.5976  loss_mask_0: 0.4485  loss_dice_0: 3.552  loss_ce_1: 0.3991  loss_mask_1: 0.4643  loss_dice_1: 3.462  loss_ce_2: 0.4139  loss_mask_2: 0.4658  loss_dice_2: 3.44  loss_ce_3: 0.3962  loss_mask_3: 0.4679  loss_dice_3: 3.426  loss_ce_4: 0.392  loss_mask_4: 0.4718  loss_dice_4: 3.424  loss_ce_5: 0.3812  loss_mask_5: 0.4747  loss_dice_5: 3.42  loss_ce_6: 0.3948  loss_mask_6: 0.4723  loss_dice_6: 3.419  loss_ce_7: 0.3866  loss_mask_7: 0.4723  loss_dice_7: 3.425  loss_ce_8: 0.3926  loss_mask_8: 0.4701  loss_dice_8: 3.424  time: 1.4983  data_time: 0.0721  lr: 7.8024e-06  max_mem: 21492M
[01/17 22:59:50] d2.utils.events INFO:  eta: 12:30:26  iter: 9659  total_loss: 43.66  loss_ce: 0.3809  loss_mask: 0.4718  loss_dice: 3.468  loss_ce_0: 0.6162  loss_mask_0: 0.459  loss_dice_0: 3.599  loss_ce_1: 0.3874  loss_mask_1: 0.4749  loss_dice_1: 3.498  loss_ce_2: 0.4002  loss_mask_2: 0.4729  loss_dice_2: 3.472  loss_ce_3: 0.3896  loss_mask_3: 0.4694  loss_dice_3: 3.457  loss_ce_4: 0.3783  loss_mask_4: 0.47  loss_dice_4: 3.465  loss_ce_5: 0.3833  loss_mask_5: 0.471  loss_dice_5: 3.475  loss_ce_6: 0.3887  loss_mask_6: 0.4726  loss_dice_6: 3.463  loss_ce_7: 0.3724  loss_mask_7: 0.4723  loss_dice_7: 3.469  loss_ce_8: 0.3685  loss_mask_8: 0.4722  loss_dice_8: 3.467  time: 1.4983  data_time: 0.0889  lr: 7.7978e-06  max_mem: 21492M
[01/17 23:00:20] d2.utils.events INFO:  eta: 12:29:59  iter: 9679  total_loss: 43.69  loss_ce: 0.3659  loss_mask: 0.4569  loss_dice: 3.449  loss_ce_0: 0.6486  loss_mask_0: 0.4428  loss_dice_0: 3.57  loss_ce_1: 0.3881  loss_mask_1: 0.4599  loss_dice_1: 3.48  loss_ce_2: 0.3905  loss_mask_2: 0.4577  loss_dice_2: 3.455  loss_ce_3: 0.3757  loss_mask_3: 0.4557  loss_dice_3: 3.454  loss_ce_4: 0.3734  loss_mask_4: 0.4586  loss_dice_4: 3.457  loss_ce_5: 0.3757  loss_mask_5: 0.4593  loss_dice_5: 3.456  loss_ce_6: 0.3755  loss_mask_6: 0.4583  loss_dice_6: 3.45  loss_ce_7: 0.3812  loss_mask_7: 0.4563  loss_dice_7: 3.449  loss_ce_8: 0.3764  loss_mask_8: 0.4543  loss_dice_8: 3.453  time: 1.4983  data_time: 0.0762  lr: 7.7932e-06  max_mem: 21492M
[01/17 23:00:50] d2.utils.events INFO:  eta: 12:29:41  iter: 9699  total_loss: 43.15  loss_ce: 0.3597  loss_mask: 0.4663  loss_dice: 3.437  loss_ce_0: 0.5935  loss_mask_0: 0.4548  loss_dice_0: 3.564  loss_ce_1: 0.3825  loss_mask_1: 0.4701  loss_dice_1: 3.467  loss_ce_2: 0.3799  loss_mask_2: 0.4654  loss_dice_2: 3.45  loss_ce_3: 0.3704  loss_mask_3: 0.4692  loss_dice_3: 3.45  loss_ce_4: 0.3835  loss_mask_4: 0.4669  loss_dice_4: 3.447  loss_ce_5: 0.3645  loss_mask_5: 0.4707  loss_dice_5: 3.448  loss_ce_6: 0.3657  loss_mask_6: 0.4679  loss_dice_6: 3.443  loss_ce_7: 0.3535  loss_mask_7: 0.4687  loss_dice_7: 3.443  loss_ce_8: 0.3516  loss_mask_8: 0.4669  loss_dice_8: 3.434  time: 1.4983  data_time: 0.0696  lr: 7.7886e-06  max_mem: 21492M
[01/17 23:01:21] d2.utils.events INFO:  eta: 12:30:08  iter: 9719  total_loss: 43.79  loss_ce: 0.402  loss_mask: 0.4781  loss_dice: 3.474  loss_ce_0: 0.6105  loss_mask_0: 0.4608  loss_dice_0: 3.604  loss_ce_1: 0.404  loss_mask_1: 0.4778  loss_dice_1: 3.512  loss_ce_2: 0.4237  loss_mask_2: 0.4752  loss_dice_2: 3.49  loss_ce_3: 0.3987  loss_mask_3: 0.4742  loss_dice_3: 3.487  loss_ce_4: 0.3935  loss_mask_4: 0.4718  loss_dice_4: 3.479  loss_ce_5: 0.3926  loss_mask_5: 0.4729  loss_dice_5: 3.485  loss_ce_6: 0.3964  loss_mask_6: 0.4731  loss_dice_6: 3.482  loss_ce_7: 0.3913  loss_mask_7: 0.4739  loss_dice_7: 3.482  loss_ce_8: 0.3903  loss_mask_8: 0.4793  loss_dice_8: 3.483  time: 1.4984  data_time: 0.0763  lr: 7.7839e-06  max_mem: 21492M
[01/17 23:01:51] d2.utils.events INFO:  eta: 12:30:22  iter: 9739  total_loss: 43.31  loss_ce: 0.3685  loss_mask: 0.4681  loss_dice: 3.434  loss_ce_0: 0.6105  loss_mask_0: 0.4553  loss_dice_0: 3.564  loss_ce_1: 0.3801  loss_mask_1: 0.4717  loss_dice_1: 3.47  loss_ce_2: 0.4032  loss_mask_2: 0.4688  loss_dice_2: 3.448  loss_ce_3: 0.3629  loss_mask_3: 0.4681  loss_dice_3: 3.432  loss_ce_4: 0.371  loss_mask_4: 0.4655  loss_dice_4: 3.439  loss_ce_5: 0.3683  loss_mask_5: 0.4671  loss_dice_5: 3.441  loss_ce_6: 0.3782  loss_mask_6: 0.4673  loss_dice_6: 3.433  loss_ce_7: 0.3537  loss_mask_7: 0.4672  loss_dice_7: 3.438  loss_ce_8: 0.3639  loss_mask_8: 0.4654  loss_dice_8: 3.433  time: 1.4984  data_time: 0.0778  lr: 7.7793e-06  max_mem: 21492M
[01/17 23:02:22] d2.utils.events INFO:  eta: 12:29:59  iter: 9759  total_loss: 43.47  loss_ce: 0.406  loss_mask: 0.4682  loss_dice: 3.434  loss_ce_0: 0.6018  loss_mask_0: 0.4487  loss_dice_0: 3.558  loss_ce_1: 0.3956  loss_mask_1: 0.4605  loss_dice_1: 3.468  loss_ce_2: 0.4079  loss_mask_2: 0.4642  loss_dice_2: 3.456  loss_ce_3: 0.3906  loss_mask_3: 0.4672  loss_dice_3: 3.442  loss_ce_4: 0.3923  loss_mask_4: 0.4729  loss_dice_4: 3.427  loss_ce_5: 0.3872  loss_mask_5: 0.4694  loss_dice_5: 3.434  loss_ce_6: 0.3827  loss_mask_6: 0.4698  loss_dice_6: 3.426  loss_ce_7: 0.3834  loss_mask_7: 0.4691  loss_dice_7: 3.431  loss_ce_8: 0.386  loss_mask_8: 0.4701  loss_dice_8: 3.431  time: 1.4984  data_time: 0.0745  lr: 7.7747e-06  max_mem: 21492M
[01/17 23:02:52] d2.utils.events INFO:  eta: 12:29:13  iter: 9779  total_loss: 43.75  loss_ce: 0.3904  loss_mask: 0.4775  loss_dice: 3.483  loss_ce_0: 0.6427  loss_mask_0: 0.4608  loss_dice_0: 3.601  loss_ce_1: 0.3974  loss_mask_1: 0.4801  loss_dice_1: 3.515  loss_ce_2: 0.4155  loss_mask_2: 0.478  loss_dice_2: 3.495  loss_ce_3: 0.393  loss_mask_3: 0.4777  loss_dice_3: 3.484  loss_ce_4: 0.3905  loss_mask_4: 0.4776  loss_dice_4: 3.495  loss_ce_5: 0.3813  loss_mask_5: 0.4779  loss_dice_5: 3.495  loss_ce_6: 0.3812  loss_mask_6: 0.4802  loss_dice_6: 3.488  loss_ce_7: 0.3863  loss_mask_7: 0.4805  loss_dice_7: 3.488  loss_ce_8: 0.3868  loss_mask_8: 0.4776  loss_dice_8: 3.478  time: 1.4984  data_time: 0.0768  lr: 7.7701e-06  max_mem: 21492M
[01/17 23:03:22] d2.utils.events INFO:  eta: 12:29:04  iter: 9799  total_loss: 43.72  loss_ce: 0.3987  loss_mask: 0.4622  loss_dice: 3.462  loss_ce_0: 0.628  loss_mask_0: 0.4413  loss_dice_0: 3.608  loss_ce_1: 0.4114  loss_mask_1: 0.4607  loss_dice_1: 3.512  loss_ce_2: 0.4278  loss_mask_2: 0.4625  loss_dice_2: 3.483  loss_ce_3: 0.4138  loss_mask_3: 0.4659  loss_dice_3: 3.47  loss_ce_4: 0.3941  loss_mask_4: 0.4644  loss_dice_4: 3.47  loss_ce_5: 0.4144  loss_mask_5: 0.4655  loss_dice_5: 3.473  loss_ce_6: 0.4141  loss_mask_6: 0.4638  loss_dice_6: 3.462  loss_ce_7: 0.4081  loss_mask_7: 0.4627  loss_dice_7: 3.462  loss_ce_8: 0.412  loss_mask_8: 0.4622  loss_dice_8: 3.474  time: 1.4984  data_time: 0.0856  lr: 7.7654e-06  max_mem: 21492M
[01/17 23:03:52] d2.utils.events INFO:  eta: 12:29:05  iter: 9819  total_loss: 42.84  loss_ce: 0.3964  loss_mask: 0.4678  loss_dice: 3.37  loss_ce_0: 0.613  loss_mask_0: 0.4473  loss_dice_0: 3.519  loss_ce_1: 0.3917  loss_mask_1: 0.4636  loss_dice_1: 3.418  loss_ce_2: 0.4105  loss_mask_2: 0.4642  loss_dice_2: 3.397  loss_ce_3: 0.3885  loss_mask_3: 0.4663  loss_dice_3: 3.378  loss_ce_4: 0.3826  loss_mask_4: 0.4643  loss_dice_4: 3.373  loss_ce_5: 0.3852  loss_mask_5: 0.4629  loss_dice_5: 3.372  loss_ce_6: 0.373  loss_mask_6: 0.4687  loss_dice_6: 3.365  loss_ce_7: 0.3709  loss_mask_7: 0.4674  loss_dice_7: 3.375  loss_ce_8: 0.3745  loss_mask_8: 0.4697  loss_dice_8: 3.367  time: 1.4985  data_time: 0.0821  lr: 7.7608e-06  max_mem: 21492M
[01/17 23:04:22] d2.utils.events INFO:  eta: 12:28:44  iter: 9839  total_loss: 42.68  loss_ce: 0.3895  loss_mask: 0.4634  loss_dice: 3.374  loss_ce_0: 0.5978  loss_mask_0: 0.4466  loss_dice_0: 3.523  loss_ce_1: 0.3996  loss_mask_1: 0.4629  loss_dice_1: 3.423  loss_ce_2: 0.4032  loss_mask_2: 0.4641  loss_dice_2: 3.399  loss_ce_3: 0.3923  loss_mask_3: 0.4631  loss_dice_3: 3.393  loss_ce_4: 0.395  loss_mask_4: 0.4638  loss_dice_4: 3.382  loss_ce_5: 0.3807  loss_mask_5: 0.465  loss_dice_5: 3.383  loss_ce_6: 0.388  loss_mask_6: 0.4629  loss_dice_6: 3.38  loss_ce_7: 0.3853  loss_mask_7: 0.4631  loss_dice_7: 3.38  loss_ce_8: 0.3848  loss_mask_8: 0.4613  loss_dice_8: 3.378  time: 1.4984  data_time: 0.0800  lr: 7.7562e-06  max_mem: 21492M
[01/17 23:04:51] d2.utils.events INFO:  eta: 12:27:45  iter: 9859  total_loss: 42.41  loss_ce: 0.3558  loss_mask: 0.4652  loss_dice: 3.374  loss_ce_0: 0.5713  loss_mask_0: 0.4424  loss_dice_0: 3.514  loss_ce_1: 0.3697  loss_mask_1: 0.4678  loss_dice_1: 3.404  loss_ce_2: 0.3775  loss_mask_2: 0.4626  loss_dice_2: 3.386  loss_ce_3: 0.3658  loss_mask_3: 0.4639  loss_dice_3: 3.361  loss_ce_4: 0.3614  loss_mask_4: 0.464  loss_dice_4: 3.365  loss_ce_5: 0.357  loss_mask_5: 0.4643  loss_dice_5: 3.373  loss_ce_6: 0.3615  loss_mask_6: 0.4635  loss_dice_6: 3.365  loss_ce_7: 0.349  loss_mask_7: 0.4623  loss_dice_7: 3.363  loss_ce_8: 0.3591  loss_mask_8: 0.4638  loss_dice_8: 3.367  time: 1.4984  data_time: 0.0774  lr: 7.7515e-06  max_mem: 21492M
[01/17 23:05:21] d2.utils.events INFO:  eta: 12:27:32  iter: 9879  total_loss: 42.63  loss_ce: 0.3743  loss_mask: 0.4683  loss_dice: 3.366  loss_ce_0: 0.6126  loss_mask_0: 0.4454  loss_dice_0: 3.502  loss_ce_1: 0.378  loss_mask_1: 0.4671  loss_dice_1: 3.416  loss_ce_2: 0.3986  loss_mask_2: 0.4684  loss_dice_2: 3.381  loss_ce_3: 0.3842  loss_mask_3: 0.463  loss_dice_3: 3.373  loss_ce_4: 0.3757  loss_mask_4: 0.4659  loss_dice_4: 3.374  loss_ce_5: 0.3786  loss_mask_5: 0.4654  loss_dice_5: 3.369  loss_ce_6: 0.3625  loss_mask_6: 0.4687  loss_dice_6: 3.369  loss_ce_7: 0.381  loss_mask_7: 0.4678  loss_dice_7: 3.364  loss_ce_8: 0.3669  loss_mask_8: 0.469  loss_dice_8: 3.37  time: 1.4984  data_time: 0.0792  lr: 7.7469e-06  max_mem: 21492M
[01/17 23:05:51] d2.utils.events INFO:  eta: 12:27:03  iter: 9899  total_loss: 42.47  loss_ce: 0.3676  loss_mask: 0.4631  loss_dice: 3.376  loss_ce_0: 0.5977  loss_mask_0: 0.445  loss_dice_0: 3.517  loss_ce_1: 0.3689  loss_mask_1: 0.4711  loss_dice_1: 3.413  loss_ce_2: 0.3862  loss_mask_2: 0.4731  loss_dice_2: 3.388  loss_ce_3: 0.3782  loss_mask_3: 0.4665  loss_dice_3: 3.381  loss_ce_4: 0.3749  loss_mask_4: 0.464  loss_dice_4: 3.377  loss_ce_5: 0.3668  loss_mask_5: 0.4626  loss_dice_5: 3.374  loss_ce_6: 0.3716  loss_mask_6: 0.4632  loss_dice_6: 3.369  loss_ce_7: 0.3698  loss_mask_7: 0.463  loss_dice_7: 3.371  loss_ce_8: 0.371  loss_mask_8: 0.4629  loss_dice_8: 3.366  time: 1.4983  data_time: 0.0639  lr: 7.7423e-06  max_mem: 21492M
[01/17 23:06:21] d2.utils.events INFO:  eta: 12:27:10  iter: 9919  total_loss: 43.03  loss_ce: 0.3802  loss_mask: 0.4707  loss_dice: 3.397  loss_ce_0: 0.6165  loss_mask_0: 0.451  loss_dice_0: 3.535  loss_ce_1: 0.3939  loss_mask_1: 0.4708  loss_dice_1: 3.434  loss_ce_2: 0.4226  loss_mask_2: 0.4711  loss_dice_2: 3.411  loss_ce_3: 0.4089  loss_mask_3: 0.4697  loss_dice_3: 3.402  loss_ce_4: 0.4192  loss_mask_4: 0.4707  loss_dice_4: 3.394  loss_ce_5: 0.3896  loss_mask_5: 0.4694  loss_dice_5: 3.394  loss_ce_6: 0.3842  loss_mask_6: 0.4705  loss_dice_6: 3.391  loss_ce_7: 0.3778  loss_mask_7: 0.4722  loss_dice_7: 3.39  loss_ce_8: 0.3765  loss_mask_8: 0.4698  loss_dice_8: 3.392  time: 1.4984  data_time: 0.0861  lr: 7.7376e-06  max_mem: 21492M
[01/17 23:06:51] d2.utils.events INFO:  eta: 12:26:40  iter: 9939  total_loss: 42.33  loss_ce: 0.37  loss_mask: 0.4651  loss_dice: 3.368  loss_ce_0: 0.5923  loss_mask_0: 0.4444  loss_dice_0: 3.524  loss_ce_1: 0.3752  loss_mask_1: 0.4631  loss_dice_1: 3.416  loss_ce_2: 0.3931  loss_mask_2: 0.4612  loss_dice_2: 3.386  loss_ce_3: 0.3711  loss_mask_3: 0.464  loss_dice_3: 3.375  loss_ce_4: 0.3712  loss_mask_4: 0.4637  loss_dice_4: 3.375  loss_ce_5: 0.3701  loss_mask_5: 0.4663  loss_dice_5: 3.369  loss_ce_6: 0.3592  loss_mask_6: 0.4658  loss_dice_6: 3.376  loss_ce_7: 0.3599  loss_mask_7: 0.4649  loss_dice_7: 3.368  loss_ce_8: 0.3651  loss_mask_8: 0.4663  loss_dice_8: 3.371  time: 1.4984  data_time: 0.0685  lr: 7.733e-06  max_mem: 21492M
[01/17 23:07:22] d2.utils.events INFO:  eta: 12:27:11  iter: 9959  total_loss: 43.16  loss_ce: 0.3585  loss_mask: 0.4732  loss_dice: 3.446  loss_ce_0: 0.626  loss_mask_0: 0.4572  loss_dice_0: 3.561  loss_ce_1: 0.373  loss_mask_1: 0.478  loss_dice_1: 3.472  loss_ce_2: 0.3948  loss_mask_2: 0.4798  loss_dice_2: 3.447  loss_ce_3: 0.3788  loss_mask_3: 0.4767  loss_dice_3: 3.45  loss_ce_4: 0.3769  loss_mask_4: 0.477  loss_dice_4: 3.441  loss_ce_5: 0.363  loss_mask_5: 0.4772  loss_dice_5: 3.445  loss_ce_6: 0.3636  loss_mask_6: 0.4743  loss_dice_6: 3.45  loss_ce_7: 0.3573  loss_mask_7: 0.4742  loss_dice_7: 3.46  loss_ce_8: 0.3557  loss_mask_8: 0.4734  loss_dice_8: 3.446  time: 1.4985  data_time: 0.0901  lr: 7.7284e-06  max_mem: 21492M
[01/17 23:07:53] d2.utils.events INFO:  eta: 12:26:46  iter: 9979  total_loss: 42.99  loss_ce: 0.3661  loss_mask: 0.4618  loss_dice: 3.38  loss_ce_0: 0.6088  loss_mask_0: 0.4409  loss_dice_0: 3.529  loss_ce_1: 0.373  loss_mask_1: 0.4602  loss_dice_1: 3.434  loss_ce_2: 0.3985  loss_mask_2: 0.4597  loss_dice_2: 3.411  loss_ce_3: 0.3808  loss_mask_3: 0.4604  loss_dice_3: 3.397  loss_ce_4: 0.3902  loss_mask_4: 0.4603  loss_dice_4: 3.39  loss_ce_5: 0.3643  loss_mask_5: 0.4628  loss_dice_5: 3.392  loss_ce_6: 0.3679  loss_mask_6: 0.4618  loss_dice_6: 3.385  loss_ce_7: 0.3708  loss_mask_7: 0.4603  loss_dice_7: 3.394  loss_ce_8: 0.363  loss_mask_8: 0.4617  loss_dice_8: 3.381  time: 1.4985  data_time: 0.0787  lr: 7.7238e-06  max_mem: 21492M
[01/17 23:08:23] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0009999.pth
[01/17 23:08:24] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/17 23:08:25] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/17 23:08:25] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/17 23:08:26] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/17 23:08:39] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0082 s/iter. Inference: 0.1845 s/iter. Eval: 0.2018 s/iter. Total: 0.3945 s/iter. ETA=0:07:06
[01/17 23:08:45] d2.evaluation.evaluator INFO: Inference done 25/1093. Dataloading: 0.0100 s/iter. Inference: 0.1692 s/iter. Eval: 0.2012 s/iter. Total: 0.3804 s/iter. ETA=0:06:46
[01/17 23:08:50] d2.evaluation.evaluator INFO: Inference done 39/1093. Dataloading: 0.0103 s/iter. Inference: 0.1665 s/iter. Eval: 0.1940 s/iter. Total: 0.3709 s/iter. ETA=0:06:30
[01/17 23:08:55] d2.evaluation.evaluator INFO: Inference done 53/1093. Dataloading: 0.0113 s/iter. Inference: 0.1685 s/iter. Eval: 0.1935 s/iter. Total: 0.3734 s/iter. ETA=0:06:28
[01/17 23:09:00] d2.evaluation.evaluator INFO: Inference done 67/1093. Dataloading: 0.0118 s/iter. Inference: 0.1670 s/iter. Eval: 0.1947 s/iter. Total: 0.3736 s/iter. ETA=0:06:23
[01/17 23:09:05] d2.evaluation.evaluator INFO: Inference done 81/1093. Dataloading: 0.0116 s/iter. Inference: 0.1697 s/iter. Eval: 0.1899 s/iter. Total: 0.3713 s/iter. ETA=0:06:15
[01/17 23:09:10] d2.evaluation.evaluator INFO: Inference done 93/1093. Dataloading: 0.0122 s/iter. Inference: 0.1705 s/iter. Eval: 0.1963 s/iter. Total: 0.3790 s/iter. ETA=0:06:19
[01/17 23:09:16] d2.evaluation.evaluator INFO: Inference done 107/1093. Dataloading: 0.0120 s/iter. Inference: 0.1705 s/iter. Eval: 0.1953 s/iter. Total: 0.3779 s/iter. ETA=0:06:12
[01/17 23:09:21] d2.evaluation.evaluator INFO: Inference done 121/1093. Dataloading: 0.0119 s/iter. Inference: 0.1702 s/iter. Eval: 0.1946 s/iter. Total: 0.3767 s/iter. ETA=0:06:06
[01/17 23:09:26] d2.evaluation.evaluator INFO: Inference done 136/1093. Dataloading: 0.0116 s/iter. Inference: 0.1684 s/iter. Eval: 0.1927 s/iter. Total: 0.3728 s/iter. ETA=0:05:56
[01/17 23:09:31] d2.evaluation.evaluator INFO: Inference done 151/1093. Dataloading: 0.0114 s/iter. Inference: 0.1692 s/iter. Eval: 0.1882 s/iter. Total: 0.3689 s/iter. ETA=0:05:47
[01/17 23:09:36] d2.evaluation.evaluator INFO: Inference done 166/1093. Dataloading: 0.0113 s/iter. Inference: 0.1679 s/iter. Eval: 0.1884 s/iter. Total: 0.3678 s/iter. ETA=0:05:40
[01/17 23:09:41] d2.evaluation.evaluator INFO: Inference done 181/1093. Dataloading: 0.0113 s/iter. Inference: 0.1660 s/iter. Eval: 0.1888 s/iter. Total: 0.3661 s/iter. ETA=0:05:33
[01/17 23:09:47] d2.evaluation.evaluator INFO: Inference done 196/1093. Dataloading: 0.0113 s/iter. Inference: 0.1663 s/iter. Eval: 0.1865 s/iter. Total: 0.3641 s/iter. ETA=0:05:26
[01/17 23:09:52] d2.evaluation.evaluator INFO: Inference done 211/1093. Dataloading: 0.0114 s/iter. Inference: 0.1655 s/iter. Eval: 0.1861 s/iter. Total: 0.3631 s/iter. ETA=0:05:20
[01/17 23:09:57] d2.evaluation.evaluator INFO: Inference done 225/1093. Dataloading: 0.0114 s/iter. Inference: 0.1663 s/iter. Eval: 0.1856 s/iter. Total: 0.3634 s/iter. ETA=0:05:15
[01/17 23:10:02] d2.evaluation.evaluator INFO: Inference done 237/1093. Dataloading: 0.0114 s/iter. Inference: 0.1674 s/iter. Eval: 0.1877 s/iter. Total: 0.3665 s/iter. ETA=0:05:13
[01/17 23:10:07] d2.evaluation.evaluator INFO: Inference done 250/1093. Dataloading: 0.0116 s/iter. Inference: 0.1683 s/iter. Eval: 0.1888 s/iter. Total: 0.3688 s/iter. ETA=0:05:10
[01/17 23:10:13] d2.evaluation.evaluator INFO: Inference done 264/1093. Dataloading: 0.0116 s/iter. Inference: 0.1683 s/iter. Eval: 0.1889 s/iter. Total: 0.3689 s/iter. ETA=0:05:05
[01/17 23:10:18] d2.evaluation.evaluator INFO: Inference done 278/1093. Dataloading: 0.0115 s/iter. Inference: 0.1675 s/iter. Eval: 0.1893 s/iter. Total: 0.3684 s/iter. ETA=0:05:00
[01/17 23:10:23] d2.evaluation.evaluator INFO: Inference done 292/1093. Dataloading: 0.0115 s/iter. Inference: 0.1686 s/iter. Eval: 0.1881 s/iter. Total: 0.3683 s/iter. ETA=0:04:55
[01/17 23:10:28] d2.evaluation.evaluator INFO: Inference done 306/1093. Dataloading: 0.0116 s/iter. Inference: 0.1675 s/iter. Eval: 0.1890 s/iter. Total: 0.3683 s/iter. ETA=0:04:49
[01/17 23:10:33] d2.evaluation.evaluator INFO: Inference done 320/1093. Dataloading: 0.0116 s/iter. Inference: 0.1666 s/iter. Eval: 0.1903 s/iter. Total: 0.3685 s/iter. ETA=0:04:44
[01/17 23:10:38] d2.evaluation.evaluator INFO: Inference done 335/1093. Dataloading: 0.0115 s/iter. Inference: 0.1665 s/iter. Eval: 0.1895 s/iter. Total: 0.3675 s/iter. ETA=0:04:38
[01/17 23:10:43] d2.evaluation.evaluator INFO: Inference done 351/1093. Dataloading: 0.0113 s/iter. Inference: 0.1667 s/iter. Eval: 0.1873 s/iter. Total: 0.3654 s/iter. ETA=0:04:31
[01/17 23:10:49] d2.evaluation.evaluator INFO: Inference done 365/1093. Dataloading: 0.0114 s/iter. Inference: 0.1671 s/iter. Eval: 0.1871 s/iter. Total: 0.3657 s/iter. ETA=0:04:26
[01/17 23:10:54] d2.evaluation.evaluator INFO: Inference done 380/1093. Dataloading: 0.0113 s/iter. Inference: 0.1673 s/iter. Eval: 0.1863 s/iter. Total: 0.3649 s/iter. ETA=0:04:20
[01/17 23:10:59] d2.evaluation.evaluator INFO: Inference done 393/1093. Dataloading: 0.0113 s/iter. Inference: 0.1674 s/iter. Eval: 0.1875 s/iter. Total: 0.3664 s/iter. ETA=0:04:16
[01/17 23:11:04] d2.evaluation.evaluator INFO: Inference done 407/1093. Dataloading: 0.0113 s/iter. Inference: 0.1677 s/iter. Eval: 0.1871 s/iter. Total: 0.3661 s/iter. ETA=0:04:11
[01/17 23:11:09] d2.evaluation.evaluator INFO: Inference done 421/1093. Dataloading: 0.0112 s/iter. Inference: 0.1675 s/iter. Eval: 0.1876 s/iter. Total: 0.3664 s/iter. ETA=0:04:06
[01/17 23:11:15] d2.evaluation.evaluator INFO: Inference done 434/1093. Dataloading: 0.0113 s/iter. Inference: 0.1677 s/iter. Eval: 0.1883 s/iter. Total: 0.3674 s/iter. ETA=0:04:02
[01/17 23:11:20] d2.evaluation.evaluator INFO: Inference done 448/1093. Dataloading: 0.0112 s/iter. Inference: 0.1680 s/iter. Eval: 0.1879 s/iter. Total: 0.3672 s/iter. ETA=0:03:56
[01/17 23:11:25] d2.evaluation.evaluator INFO: Inference done 462/1093. Dataloading: 0.0113 s/iter. Inference: 0.1680 s/iter. Eval: 0.1881 s/iter. Total: 0.3674 s/iter. ETA=0:03:51
[01/17 23:11:30] d2.evaluation.evaluator INFO: Inference done 478/1093. Dataloading: 0.0113 s/iter. Inference: 0.1675 s/iter. Eval: 0.1870 s/iter. Total: 0.3659 s/iter. ETA=0:03:45
[01/17 23:11:35] d2.evaluation.evaluator INFO: Inference done 493/1093. Dataloading: 0.0113 s/iter. Inference: 0.1679 s/iter. Eval: 0.1863 s/iter. Total: 0.3656 s/iter. ETA=0:03:39
[01/17 23:11:40] d2.evaluation.evaluator INFO: Inference done 509/1093. Dataloading: 0.0112 s/iter. Inference: 0.1677 s/iter. Eval: 0.1849 s/iter. Total: 0.3640 s/iter. ETA=0:03:32
[01/17 23:11:46] d2.evaluation.evaluator INFO: Inference done 523/1093. Dataloading: 0.0112 s/iter. Inference: 0.1676 s/iter. Eval: 0.1853 s/iter. Total: 0.3642 s/iter. ETA=0:03:27
[01/17 23:11:51] d2.evaluation.evaluator INFO: Inference done 537/1093. Dataloading: 0.0113 s/iter. Inference: 0.1676 s/iter. Eval: 0.1854 s/iter. Total: 0.3644 s/iter. ETA=0:03:22
[01/17 23:11:56] d2.evaluation.evaluator INFO: Inference done 550/1093. Dataloading: 0.0113 s/iter. Inference: 0.1682 s/iter. Eval: 0.1856 s/iter. Total: 0.3652 s/iter. ETA=0:03:18
[01/17 23:12:01] d2.evaluation.evaluator INFO: Inference done 564/1093. Dataloading: 0.0113 s/iter. Inference: 0.1682 s/iter. Eval: 0.1858 s/iter. Total: 0.3653 s/iter. ETA=0:03:13
[01/17 23:12:06] d2.evaluation.evaluator INFO: Inference done 581/1093. Dataloading: 0.0113 s/iter. Inference: 0.1679 s/iter. Eval: 0.1843 s/iter. Total: 0.3635 s/iter. ETA=0:03:06
[01/17 23:12:11] d2.evaluation.evaluator INFO: Inference done 595/1093. Dataloading: 0.0113 s/iter. Inference: 0.1678 s/iter. Eval: 0.1843 s/iter. Total: 0.3635 s/iter. ETA=0:03:01
[01/17 23:12:17] d2.evaluation.evaluator INFO: Inference done 608/1093. Dataloading: 0.0114 s/iter. Inference: 0.1677 s/iter. Eval: 0.1850 s/iter. Total: 0.3642 s/iter. ETA=0:02:56
[01/17 23:12:22] d2.evaluation.evaluator INFO: Inference done 623/1093. Dataloading: 0.0113 s/iter. Inference: 0.1675 s/iter. Eval: 0.1850 s/iter. Total: 0.3640 s/iter. ETA=0:02:51
[01/17 23:12:27] d2.evaluation.evaluator INFO: Inference done 638/1093. Dataloading: 0.0113 s/iter. Inference: 0.1674 s/iter. Eval: 0.1851 s/iter. Total: 0.3638 s/iter. ETA=0:02:45
[01/17 23:12:32] d2.evaluation.evaluator INFO: Inference done 653/1093. Dataloading: 0.0113 s/iter. Inference: 0.1677 s/iter. Eval: 0.1843 s/iter. Total: 0.3633 s/iter. ETA=0:02:39
[01/17 23:12:38] d2.evaluation.evaluator INFO: Inference done 668/1093. Dataloading: 0.0112 s/iter. Inference: 0.1675 s/iter. Eval: 0.1842 s/iter. Total: 0.3631 s/iter. ETA=0:02:34
[01/17 23:12:43] d2.evaluation.evaluator INFO: Inference done 684/1093. Dataloading: 0.0112 s/iter. Inference: 0.1677 s/iter. Eval: 0.1832 s/iter. Total: 0.3622 s/iter. ETA=0:02:28
[01/17 23:12:48] d2.evaluation.evaluator INFO: Inference done 699/1093. Dataloading: 0.0112 s/iter. Inference: 0.1679 s/iter. Eval: 0.1827 s/iter. Total: 0.3619 s/iter. ETA=0:02:22
[01/17 23:12:54] d2.evaluation.evaluator INFO: Inference done 713/1093. Dataloading: 0.0112 s/iter. Inference: 0.1677 s/iter. Eval: 0.1834 s/iter. Total: 0.3624 s/iter. ETA=0:02:17
[01/17 23:12:59] d2.evaluation.evaluator INFO: Inference done 728/1093. Dataloading: 0.0112 s/iter. Inference: 0.1676 s/iter. Eval: 0.1832 s/iter. Total: 0.3621 s/iter. ETA=0:02:12
[01/17 23:13:04] d2.evaluation.evaluator INFO: Inference done 743/1093. Dataloading: 0.0112 s/iter. Inference: 0.1677 s/iter. Eval: 0.1827 s/iter. Total: 0.3617 s/iter. ETA=0:02:06
[01/17 23:13:09] d2.evaluation.evaluator INFO: Inference done 758/1093. Dataloading: 0.0112 s/iter. Inference: 0.1677 s/iter. Eval: 0.1823 s/iter. Total: 0.3612 s/iter. ETA=0:02:01
[01/17 23:13:14] d2.evaluation.evaluator INFO: Inference done 770/1093. Dataloading: 0.0113 s/iter. Inference: 0.1678 s/iter. Eval: 0.1830 s/iter. Total: 0.3621 s/iter. ETA=0:01:56
[01/17 23:13:19] d2.evaluation.evaluator INFO: Inference done 785/1093. Dataloading: 0.0113 s/iter. Inference: 0.1680 s/iter. Eval: 0.1826 s/iter. Total: 0.3619 s/iter. ETA=0:01:51
[01/17 23:13:25] d2.evaluation.evaluator INFO: Inference done 801/1093. Dataloading: 0.0112 s/iter. Inference: 0.1678 s/iter. Eval: 0.1821 s/iter. Total: 0.3612 s/iter. ETA=0:01:45
[01/17 23:13:30] d2.evaluation.evaluator INFO: Inference done 815/1093. Dataloading: 0.0112 s/iter. Inference: 0.1678 s/iter. Eval: 0.1822 s/iter. Total: 0.3613 s/iter. ETA=0:01:40
[01/17 23:13:35] d2.evaluation.evaluator INFO: Inference done 831/1093. Dataloading: 0.0111 s/iter. Inference: 0.1678 s/iter. Eval: 0.1815 s/iter. Total: 0.3606 s/iter. ETA=0:01:34
[01/17 23:13:40] d2.evaluation.evaluator INFO: Inference done 846/1093. Dataloading: 0.0111 s/iter. Inference: 0.1676 s/iter. Eval: 0.1815 s/iter. Total: 0.3602 s/iter. ETA=0:01:28
[01/17 23:13:45] d2.evaluation.evaluator INFO: Inference done 859/1093. Dataloading: 0.0111 s/iter. Inference: 0.1677 s/iter. Eval: 0.1820 s/iter. Total: 0.3609 s/iter. ETA=0:01:24
[01/17 23:13:50] d2.evaluation.evaluator INFO: Inference done 873/1093. Dataloading: 0.0111 s/iter. Inference: 0.1678 s/iter. Eval: 0.1819 s/iter. Total: 0.3609 s/iter. ETA=0:01:19
[01/17 23:13:56] d2.evaluation.evaluator INFO: Inference done 887/1093. Dataloading: 0.0111 s/iter. Inference: 0.1678 s/iter. Eval: 0.1822 s/iter. Total: 0.3612 s/iter. ETA=0:01:14
[01/17 23:14:01] d2.evaluation.evaluator INFO: Inference done 903/1093. Dataloading: 0.0111 s/iter. Inference: 0.1675 s/iter. Eval: 0.1819 s/iter. Total: 0.3606 s/iter. ETA=0:01:08
[01/17 23:14:06] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0111 s/iter. Inference: 0.1676 s/iter. Eval: 0.1819 s/iter. Total: 0.3607 s/iter. ETA=0:01:03
[01/17 23:14:11] d2.evaluation.evaluator INFO: Inference done 932/1093. Dataloading: 0.0111 s/iter. Inference: 0.1674 s/iter. Eval: 0.1819 s/iter. Total: 0.3605 s/iter. ETA=0:00:58
[01/17 23:14:16] d2.evaluation.evaluator INFO: Inference done 945/1093. Dataloading: 0.0110 s/iter. Inference: 0.1678 s/iter. Eval: 0.1820 s/iter. Total: 0.3609 s/iter. ETA=0:00:53
[01/17 23:14:22] d2.evaluation.evaluator INFO: Inference done 960/1093. Dataloading: 0.0110 s/iter. Inference: 0.1675 s/iter. Eval: 0.1822 s/iter. Total: 0.3608 s/iter. ETA=0:00:47
[01/17 23:14:27] d2.evaluation.evaluator INFO: Inference done 974/1093. Dataloading: 0.0110 s/iter. Inference: 0.1677 s/iter. Eval: 0.1821 s/iter. Total: 0.3609 s/iter. ETA=0:00:42
[01/17 23:14:32] d2.evaluation.evaluator INFO: Inference done 989/1093. Dataloading: 0.0110 s/iter. Inference: 0.1680 s/iter. Eval: 0.1816 s/iter. Total: 0.3606 s/iter. ETA=0:00:37
[01/17 23:14:37] d2.evaluation.evaluator INFO: Inference done 1003/1093. Dataloading: 0.0110 s/iter. Inference: 0.1682 s/iter. Eval: 0.1816 s/iter. Total: 0.3608 s/iter. ETA=0:00:32
[01/17 23:14:42] d2.evaluation.evaluator INFO: Inference done 1016/1093. Dataloading: 0.0109 s/iter. Inference: 0.1682 s/iter. Eval: 0.1819 s/iter. Total: 0.3611 s/iter. ETA=0:00:27
[01/17 23:14:47] d2.evaluation.evaluator INFO: Inference done 1025/1093. Dataloading: 0.0110 s/iter. Inference: 0.1692 s/iter. Eval: 0.1828 s/iter. Total: 0.3631 s/iter. ETA=0:00:24
[01/17 23:14:53] d2.evaluation.evaluator INFO: Inference done 1034/1093. Dataloading: 0.0111 s/iter. Inference: 0.1704 s/iter. Eval: 0.1834 s/iter. Total: 0.3649 s/iter. ETA=0:00:21
[01/17 23:14:58] d2.evaluation.evaluator INFO: Inference done 1043/1093. Dataloading: 0.0112 s/iter. Inference: 0.1717 s/iter. Eval: 0.1840 s/iter. Total: 0.3670 s/iter. ETA=0:00:18
[01/17 23:15:03] d2.evaluation.evaluator INFO: Inference done 1052/1093. Dataloading: 0.0112 s/iter. Inference: 0.1728 s/iter. Eval: 0.1848 s/iter. Total: 0.3689 s/iter. ETA=0:00:15
[01/17 23:15:09] d2.evaluation.evaluator INFO: Inference done 1069/1093. Dataloading: 0.0112 s/iter. Inference: 0.1725 s/iter. Eval: 0.1842 s/iter. Total: 0.3680 s/iter. ETA=0:00:08
[01/17 23:15:14] d2.evaluation.evaluator INFO: Inference done 1088/1093. Dataloading: 0.0111 s/iter. Inference: 0.1719 s/iter. Eval: 0.1833 s/iter. Total: 0.3663 s/iter. ETA=0:00:01
[01/17 23:15:16] d2.evaluation.evaluator INFO: Total inference time: 0:06:38.766107 (0.366513 s / iter per device, on 4 devices)
[01/17 23:15:16] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:06 (0.171709 s / iter per device, on 4 devices)
[01/17 23:15:39] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 3.078599050934352, 'mIoU': 12.955354869475203, 'fwIoU': 29.636898519011513, 'IoU-0': nan, 'IoU-1': 94.87627177602369, 'IoU-2': 46.955056953998756, 'IoU-3': 49.74815373743232, 'IoU-4': 46.114343062734655, 'IoU-5': 39.982109455825665, 'IoU-6': 38.15755087241812, 'IoU-7': 29.732947089683787, 'IoU-8': 8.151034021652729, 'IoU-9': 17.033502366723184, 'IoU-10': 16.905552116554446, 'IoU-11': 24.480322513989933, 'IoU-12': 23.5224417533169, 'IoU-13': 27.237587787370583, 'IoU-14': 26.92714416069235, 'IoU-15': 28.0944121115976, 'IoU-16': 24.909994313849566, 'IoU-17': 22.847962485178712, 'IoU-18': 26.42830173090806, 'IoU-19': 25.540778710325363, 'IoU-20': 27.629541248267493, 'IoU-21': 26.401859648185777, 'IoU-22': 24.882891712633693, 'IoU-23': 24.36303520608086, 'IoU-24': 27.37205727307344, 'IoU-25': 24.004566810731813, 'IoU-26': 25.105925945219713, 'IoU-27': 27.803779065478416, 'IoU-28': 27.081201402785144, 'IoU-29': 26.69791842841014, 'IoU-30': 28.175488537781014, 'IoU-31': 28.63900724493386, 'IoU-32': 27.981592914195332, 'IoU-33': 26.485522374311355, 'IoU-34': 26.1113279312453, 'IoU-35': 27.430058469534558, 'IoU-36': 26.245638637942125, 'IoU-37': 27.063902218244973, 'IoU-38': 27.8478985005871, 'IoU-39': 25.07679981716013, 'IoU-40': 28.60283191496705, 'IoU-41': 24.27916376572018, 'IoU-42': 26.746634170270777, 'IoU-43': 24.740243610017508, 'IoU-44': 25.24998593406762, 'IoU-45': 25.688408284088617, 'IoU-46': 25.480162071267916, 'IoU-47': 23.98736728941674, 'IoU-48': 25.552429445848368, 'IoU-49': 23.873709124876335, 'IoU-50': 23.894135750019206, 'IoU-51': 21.21032966385562, 'IoU-52': 19.810364733412726, 'IoU-53': 20.983700882745445, 'IoU-54': 22.865191728161843, 'IoU-55': 21.867639690431258, 'IoU-56': 19.080468820645557, 'IoU-57': 20.722828850890682, 'IoU-58': 18.881642301718912, 'IoU-59': 17.540434237487183, 'IoU-60': 18.52514958434374, 'IoU-61': 17.07325989086677, 'IoU-62': 18.721770986073487, 'IoU-63': 17.72923438384457, 'IoU-64': 17.76213995732983, 'IoU-65': 16.545542697426228, 'IoU-66': 14.787537173770987, 'IoU-67': 14.703770212964058, 'IoU-68': 15.390760051460928, 'IoU-69': 14.416294064886017, 'IoU-70': 15.04923204441256, 'IoU-71': 12.432440261053113, 'IoU-72': 13.035663424714233, 'IoU-73': 13.044382262027021, 'IoU-74': 11.339705192827548, 'IoU-75': 13.568841918972865, 'IoU-76': 14.767218879685437, 'IoU-77': 13.888497772223921, 'IoU-78': 15.756728491581093, 'IoU-79': 11.064221960441117, 'IoU-80': 12.933611742763171, 'IoU-81': 14.719579821783876, 'IoU-82': 12.641523598557097, 'IoU-83': 15.377146115743626, 'IoU-84': 10.720825021646151, 'IoU-85': 15.712774851274606, 'IoU-86': 9.93252246938596, 'IoU-87': 11.71629866455311, 'IoU-88': 12.560036237729832, 'IoU-89': 11.948482570186378, 'IoU-90': 10.030450250594697, 'IoU-91': 13.908735315721273, 'IoU-92': 11.293202592860519, 'IoU-93': 11.768140415186437, 'IoU-94': 14.846849397282671, 'IoU-95': 8.334568509525992, 'IoU-96': 9.556380620997377, 'IoU-97': 12.047485250718443, 'IoU-98': 12.161115451812261, 'IoU-99': 13.25641402989003, 'IoU-100': 9.082865031518544, 'IoU-101': 7.748706688955183, 'IoU-102': 11.052888493091725, 'IoU-103': 11.075577353007954, 'IoU-104': 7.312808569349411, 'IoU-105': 9.720002253680379, 'IoU-106': 8.507904640408682, 'IoU-107': 10.125935047299008, 'IoU-108': 7.858586536284269, 'IoU-109': 8.395235891727843, 'IoU-110': 9.52696202184129, 'IoU-111': 7.138285144813771, 'IoU-112': 5.872649198608605, 'IoU-113': 6.70297769318235, 'IoU-114': 6.4009436515982845, 'IoU-115': 7.035285270829174, 'IoU-116': 8.380836477132744, 'IoU-117': 3.970478495110662, 'IoU-118': 6.044982138117104, 'IoU-119': 6.630871506191665, 'IoU-120': 5.424400151080424, 'IoU-121': 6.777971960695791, 'IoU-122': 3.7316365796556186, 'IoU-123': 6.199236304840222, 'IoU-124': 5.358301583643475, 'IoU-125': 4.605965883219708, 'IoU-126': 2.544374200304654, 'IoU-127': 4.986017295178613, 'IoU-128': 3.331377248096181, 'IoU-129': 3.145197547106841, 'IoU-130': 6.766880527674441, 'IoU-131': 2.447853319757699, 'IoU-132': 1.83194531341344, 'IoU-133': 5.599804659345314, 'IoU-134': 4.120705359678284, 'IoU-135': 3.306111696522655, 'IoU-136': 4.0098052320346955, 'IoU-137': 3.8537079821842943, 'IoU-138': 2.7110733163901317, 'IoU-139': 3.289054866313327, 'IoU-140': 4.6525135321956945, 'IoU-141': 3.03618390714651, 'IoU-142': 2.693665608790121, 'IoU-143': 3.2049942786346444, 'IoU-144': 4.0493684281384486, 'IoU-145': 2.6409282798901863, 'IoU-146': 0.8322226152199356, 'IoU-147': 2.685201936251707, 'IoU-148': 1.8663794739320954, 'IoU-149': 2.47604802741453, 'IoU-150': 1.5029036113373462, 'IoU-151': 1.0592828660932807, 'IoU-152': 2.262532727552749, 'IoU-153': 0.491153046053347, 'IoU-154': 1.3126737575900083, 'IoU-155': 1.9299936730624447, 'IoU-156': 0.5067007720476852, 'IoU-157': 1.4762695478071124, 'IoU-158': 1.8555408883021989, 'IoU-159': 1.4141034396218926, 'IoU-160': 1.7902517415351542, 'IoU-161': 1.2931483313375332, 'IoU-162': 1.967177088489574, 'IoU-163': 1.0804127652608557, 'IoU-164': 0.567392359716084, 'IoU-165': 0.700270330827899, 'IoU-166': 0.15869274412333895, 'IoU-167': 0.22433332431757327, 'IoU-168': 1.6684133395596663, 'IoU-169': 1.165879228075124, 'IoU-170': 0.488426146297969, 'IoU-171': 1.1247947700079952, 'IoU-172': 0.5848723000729774, 'IoU-173': 1.2978870804669063, 'IoU-174': 0.851105724758698, 'IoU-175': 0.20656980004956368, 'IoU-176': 0.7505878398279996, 'IoU-177': 0.6786568759343149, 'IoU-178': 1.2769992269848267, 'IoU-179': 2.699720524753786, 'IoU-180': 0.13914426278387915, 'IoU-181': 1.671119965137272, 'IoU-182': 0.15412996255908135, 'IoU-183': 1.2398572772669894, 'IoU-184': 0.8102953692904202, 'IoU-185': 1.2088880630046444, 'IoU-186': 1.1580360021292389, 'IoU-187': 3.555019935856227, 'IoU-188': 1.1554016799682119, 'IoU-189': 0.23085138656282855, 'IoU-190': 0.8616302467057617, 'IoU-191': 0.9662640130273376, 'mACC': 21.22582692169544, 'pACC': 42.24533865384352, 'ACC-0': nan, 'ACC-1': 98.42043205276175, 'ACC-2': 63.50148878199439, 'ACC-3': 61.24561237293501, 'ACC-4': 66.35999018420704, 'ACC-5': 58.12389626335334, 'ACC-6': 55.90191726295494, 'ACC-7': 41.39255645375951, 'ACC-8': 9.388029985553546, 'ACC-9': 22.63097225902939, 'ACC-10': 21.94604697324667, 'ACC-11': 34.42495193473009, 'ACC-12': 34.688814648466405, 'ACC-13': 47.02833802417887, 'ACC-14': 44.39544770506555, 'ACC-15': 44.526975467753346, 'ACC-16': 38.6557955047443, 'ACC-17': 40.7128948353308, 'ACC-18': 41.39868501306794, 'ACC-19': 40.37306597337684, 'ACC-20': 44.77523724386236, 'ACC-21': 40.86501617649893, 'ACC-22': 36.775830479606306, 'ACC-23': 40.74613774083457, 'ACC-24': 48.796674970800105, 'ACC-25': 38.84078326647533, 'ACC-26': 44.05497452053524, 'ACC-27': 47.021851290774336, 'ACC-28': 43.17159869450145, 'ACC-29': 38.061446801253375, 'ACC-30': 46.510449199081144, 'ACC-31': 43.63092147964372, 'ACC-32': 43.36008269072492, 'ACC-33': 45.49359809137191, 'ACC-34': 40.051150170431455, 'ACC-35': 45.746618876061696, 'ACC-36': 41.440232194691205, 'ACC-37': 40.58473069520692, 'ACC-38': 42.608556844622235, 'ACC-39': 37.731781054110805, 'ACC-40': 45.43675974820046, 'ACC-41': 36.481029923763614, 'ACC-42': 44.82541259540097, 'ACC-43': 42.84135202152888, 'ACC-44': 38.646287865186196, 'ACC-45': 38.7034933179681, 'ACC-46': 44.14144415340709, 'ACC-47': 37.161799282875926, 'ACC-48': 42.10134721550511, 'ACC-49': 39.25637168394321, 'ACC-50': 38.67058627728762, 'ACC-51': 38.59128911423849, 'ACC-52': 30.81980989491332, 'ACC-53': 31.768569829480725, 'ACC-54': 38.64258045318721, 'ACC-55': 35.13848951237977, 'ACC-56': 31.242370312315437, 'ACC-57': 33.63950748655833, 'ACC-58': 31.82229937546876, 'ACC-59': 25.986074792226034, 'ACC-60': 30.45228440067735, 'ACC-61': 28.42018489037993, 'ACC-62': 33.63905629281827, 'ACC-63': 29.14084277381074, 'ACC-64': 29.041730168449188, 'ACC-65': 25.67531222071263, 'ACC-66': 23.79969836849983, 'ACC-67': 25.263879140274874, 'ACC-68': 26.22503028764716, 'ACC-69': 24.49128745257805, 'ACC-70': 24.684992622822694, 'ACC-71': 20.254809094402304, 'ACC-72': 22.854408715957188, 'ACC-73': 21.004265505272986, 'ACC-74': 18.290833391156507, 'ACC-75': 26.356756674117843, 'ACC-76': 26.217931854276888, 'ACC-77': 24.808545830341377, 'ACC-78': 27.225028855712125, 'ACC-79': 19.393085399826447, 'ACC-80': 20.45155099985835, 'ACC-81': 29.007376038346095, 'ACC-82': 20.67003316164824, 'ACC-83': 29.300020623079664, 'ACC-84': 16.687533521840106, 'ACC-85': 30.97042596005227, 'ACC-86': 15.959520617673443, 'ACC-87': 22.187598183008724, 'ACC-88': 21.6522050171647, 'ACC-89': 23.299037865614846, 'ACC-90': 16.415523436734677, 'ACC-91': 28.297428614895324, 'ACC-92': 18.622791454239156, 'ACC-93': 19.962651248668408, 'ACC-94': 35.75582774400257, 'ACC-95': 14.851150096484075, 'ACC-96': 14.054128408130136, 'ACC-97': 19.19530600878745, 'ACC-98': 22.732257858943143, 'ACC-99': 29.56498229336929, 'ACC-100': 15.666056033871062, 'ACC-101': 13.043678243928944, 'ACC-102': 24.33072043497961, 'ACC-103': 19.974071975063758, 'ACC-104': 15.893829468040629, 'ACC-105': 18.020401215563275, 'ACC-106': 15.672851738425509, 'ACC-107': 23.929736215726972, 'ACC-108': 13.466238230037527, 'ACC-109': 14.272661424867787, 'ACC-110': 19.225549798881055, 'ACC-111': 14.420437490461099, 'ACC-112': 9.129525491161724, 'ACC-113': 10.822292848956463, 'ACC-114': 11.978329422371818, 'ACC-115': 13.046171425134936, 'ACC-116': 17.960873522813557, 'ACC-117': 5.54805931238332, 'ACC-118': 9.724326814032178, 'ACC-119': 14.144017444286753, 'ACC-120': 8.629782097639655, 'ACC-121': 13.12025152307816, 'ACC-122': 5.831988828818629, 'ACC-123': 12.48768920143131, 'ACC-124': 8.732348445821508, 'ACC-125': 7.619460430754772, 'ACC-126': 3.410493264965983, 'ACC-127': 10.701845171901429, 'ACC-128': 5.736555225023814, 'ACC-129': 5.009039514696349, 'ACC-130': 18.028314712248125, 'ACC-131': 4.1062315537913205, 'ACC-132': 2.7512876449752524, 'ACC-133': 12.951751073370213, 'ACC-134': 7.6125807841447655, 'ACC-135': 5.634299640841458, 'ACC-136': 7.927081392098398, 'ACC-137': 6.98151527571387, 'ACC-138': 4.450687869634744, 'ACC-139': 5.828393125493441, 'ACC-140': 8.724305807479704, 'ACC-141': 4.571236697136529, 'ACC-142': 4.771361525717844, 'ACC-143': 8.670597104918619, 'ACC-144': 8.740974729241877, 'ACC-145': 6.4775151501598875, 'ACC-146': 1.141760526375911, 'ACC-147': 8.503291138300233, 'ACC-148': 2.8513826801172186, 'ACC-149': 4.6368966066269195, 'ACC-150': 2.0080070126504963, 'ACC-151': 1.3312614637588018, 'ACC-152': 3.9626744891766132, 'ACC-153': 0.5534408838564283, 'ACC-154': 2.228687973116253, 'ACC-155': 4.711053263423088, 'ACC-156': 0.6443147368897335, 'ACC-157': 3.104937439859105, 'ACC-158': 2.9460123547916806, 'ACC-159': 2.005035626490344, 'ACC-160': 4.200645839931231, 'ACC-161': 1.6740761127508936, 'ACC-162': 5.433188238059393, 'ACC-163': 1.6243558602008354, 'ACC-164': 0.8598137360912484, 'ACC-165': 0.7643880607444723, 'ACC-166': 0.17745171282034064, 'ACC-167': 0.24493326871264115, 'ACC-168': 4.089422603558619, 'ACC-169': 2.0017792813738717, 'ACC-170': 0.7465891505884692, 'ACC-171': 2.0750942765457587, 'ACC-172': 1.0286328110530798, 'ACC-173': 5.600877506556841, 'ACC-174': 4.318512677988596, 'ACC-175': 0.23424832541104387, 'ACC-176': 1.6363260390062688, 'ACC-177': 1.1050641391184954, 'ACC-178': 5.421364429189258, 'ACC-179': 7.4064530789379175, 'ACC-180': 0.17186494632395272, 'ACC-181': 7.9477759539155, 'ACC-182': 0.1672633022893276, 'ACC-183': 5.810933935411571, 'ACC-184': 1.212451387562291, 'ACC-185': 2.1636203225106123, 'ACC-186': 1.823126296879463, 'ACC-187': 19.293057417944166, 'ACC-188': 1.892074573261773, 'ACC-189': 0.2852221061252153, 'ACC-190': 1.7300376243227098, 'ACC-191': 2.408352365415987})])
[01/17 23:15:39] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/17 23:15:39] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/17 23:15:39] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/17 23:15:39] d2.evaluation.testing INFO: copypaste: 3.0786,12.9554,29.6369,21.2258,42.2453
[01/17 23:15:39] d2.utils.events INFO:  eta: 12:26:16  iter: 9999  total_loss: 43.27  loss_ce: 0.3713  loss_mask: 0.4654  loss_dice: 3.436  loss_ce_0: 0.6269  loss_mask_0: 0.4475  loss_dice_0: 3.575  loss_ce_1: 0.3803  loss_mask_1: 0.4705  loss_dice_1: 3.481  loss_ce_2: 0.377  loss_mask_2: 0.4651  loss_dice_2: 3.445  loss_ce_3: 0.3721  loss_mask_3: 0.4662  loss_dice_3: 3.44  loss_ce_4: 0.3613  loss_mask_4: 0.4654  loss_dice_4: 3.443  loss_ce_5: 0.3475  loss_mask_5: 0.4643  loss_dice_5: 3.443  loss_ce_6: 0.3587  loss_mask_6: 0.4669  loss_dice_6: 3.436  loss_ce_7: 0.3736  loss_mask_7: 0.4667  loss_dice_7: 3.438  loss_ce_8: 0.3688  loss_mask_8: 0.4661  loss_dice_8: 3.433  time: 1.4986  data_time: 0.0847  lr: 7.7191e-06  max_mem: 21492M
[01/17 23:16:09] d2.utils.events INFO:  eta: 12:25:53  iter: 10019  total_loss: 43  loss_ce: 0.385  loss_mask: 0.4486  loss_dice: 3.401  loss_ce_0: 0.5954  loss_mask_0: 0.4355  loss_dice_0: 3.536  loss_ce_1: 0.3743  loss_mask_1: 0.4496  loss_dice_1: 3.439  loss_ce_2: 0.3931  loss_mask_2: 0.45  loss_dice_2: 3.414  loss_ce_3: 0.387  loss_mask_3: 0.4505  loss_dice_3: 3.406  loss_ce_4: 0.3812  loss_mask_4: 0.4488  loss_dice_4: 3.4  loss_ce_5: 0.3733  loss_mask_5: 0.4483  loss_dice_5: 3.399  loss_ce_6: 0.3744  loss_mask_6: 0.4488  loss_dice_6: 3.412  loss_ce_7: 0.38  loss_mask_7: 0.4509  loss_dice_7: 3.41  loss_ce_8: 0.3671  loss_mask_8: 0.4492  loss_dice_8: 3.405  time: 1.4986  data_time: 0.0803  lr: 7.7145e-06  max_mem: 21492M
[01/17 23:16:39] d2.utils.events INFO:  eta: 12:25:24  iter: 10039  total_loss: 42.33  loss_ce: 0.3587  loss_mask: 0.459  loss_dice: 3.39  loss_ce_0: 0.5921  loss_mask_0: 0.435  loss_dice_0: 3.535  loss_ce_1: 0.3662  loss_mask_1: 0.4617  loss_dice_1: 3.424  loss_ce_2: 0.3906  loss_mask_2: 0.4583  loss_dice_2: 3.408  loss_ce_3: 0.3683  loss_mask_3: 0.4587  loss_dice_3: 3.398  loss_ce_4: 0.3668  loss_mask_4: 0.4594  loss_dice_4: 3.391  loss_ce_5: 0.3515  loss_mask_5: 0.4586  loss_dice_5: 3.397  loss_ce_6: 0.3567  loss_mask_6: 0.4597  loss_dice_6: 3.387  loss_ce_7: 0.3589  loss_mask_7: 0.4586  loss_dice_7: 3.389  loss_ce_8: 0.3539  loss_mask_8: 0.4574  loss_dice_8: 3.394  time: 1.4986  data_time: 0.0719  lr: 7.7099e-06  max_mem: 21492M
[01/17 23:17:09] d2.utils.events INFO:  eta: 12:25:27  iter: 10059  total_loss: 43.28  loss_ce: 0.3888  loss_mask: 0.4715  loss_dice: 3.413  loss_ce_0: 0.6572  loss_mask_0: 0.453  loss_dice_0: 3.557  loss_ce_1: 0.3836  loss_mask_1: 0.4753  loss_dice_1: 3.459  loss_ce_2: 0.4247  loss_mask_2: 0.4715  loss_dice_2: 3.423  loss_ce_3: 0.3902  loss_mask_3: 0.4722  loss_dice_3: 3.419  loss_ce_4: 0.3751  loss_mask_4: 0.4731  loss_dice_4: 3.426  loss_ce_5: 0.3848  loss_mask_5: 0.4701  loss_dice_5: 3.417  loss_ce_6: 0.3857  loss_mask_6: 0.4699  loss_dice_6: 3.413  loss_ce_7: 0.3862  loss_mask_7: 0.471  loss_dice_7: 3.418  loss_ce_8: 0.3877  loss_mask_8: 0.4717  loss_dice_8: 3.415  time: 1.4986  data_time: 0.0863  lr: 7.7052e-06  max_mem: 21492M
[01/17 23:17:40] d2.utils.events INFO:  eta: 12:25:04  iter: 10079  total_loss: 42.6  loss_ce: 0.3703  loss_mask: 0.4723  loss_dice: 3.358  loss_ce_0: 0.6025  loss_mask_0: 0.4543  loss_dice_0: 3.5  loss_ce_1: 0.3583  loss_mask_1: 0.4752  loss_dice_1: 3.397  loss_ce_2: 0.3885  loss_mask_2: 0.4692  loss_dice_2: 3.371  loss_ce_3: 0.3691  loss_mask_3: 0.4706  loss_dice_3: 3.358  loss_ce_4: 0.3581  loss_mask_4: 0.468  loss_dice_4: 3.361  loss_ce_5: 0.3533  loss_mask_5: 0.471  loss_dice_5: 3.363  loss_ce_6: 0.358  loss_mask_6: 0.4725  loss_dice_6: 3.36  loss_ce_7: 0.3681  loss_mask_7: 0.473  loss_dice_7: 3.361  loss_ce_8: 0.3645  loss_mask_8: 0.4726  loss_dice_8: 3.357  time: 1.4986  data_time: 0.0655  lr: 7.7006e-06  max_mem: 21492M
[01/17 23:18:10] d2.utils.events INFO:  eta: 12:24:34  iter: 10099  total_loss: 42.78  loss_ce: 0.3719  loss_mask: 0.4495  loss_dice: 3.401  loss_ce_0: 0.6065  loss_mask_0: 0.4318  loss_dice_0: 3.542  loss_ce_1: 0.385  loss_mask_1: 0.4471  loss_dice_1: 3.44  loss_ce_2: 0.3909  loss_mask_2: 0.4476  loss_dice_2: 3.427  loss_ce_3: 0.3783  loss_mask_3: 0.4502  loss_dice_3: 3.404  loss_ce_4: 0.3607  loss_mask_4: 0.4506  loss_dice_4: 3.398  loss_ce_5: 0.3532  loss_mask_5: 0.4502  loss_dice_5: 3.409  loss_ce_6: 0.3603  loss_mask_6: 0.4511  loss_dice_6: 3.401  loss_ce_7: 0.3552  loss_mask_7: 0.451  loss_dice_7: 3.404  loss_ce_8: 0.3708  loss_mask_8: 0.4509  loss_dice_8: 3.407  time: 1.4986  data_time: 0.0780  lr: 7.696e-06  max_mem: 21492M
[01/17 23:18:40] d2.utils.events INFO:  eta: 12:24:06  iter: 10119  total_loss: 42.4  loss_ce: 0.3584  loss_mask: 0.4605  loss_dice: 3.34  loss_ce_0: 0.6278  loss_mask_0: 0.4393  loss_dice_0: 3.48  loss_ce_1: 0.3912  loss_mask_1: 0.4588  loss_dice_1: 3.385  loss_ce_2: 0.4043  loss_mask_2: 0.4573  loss_dice_2: 3.365  loss_ce_3: 0.383  loss_mask_3: 0.4574  loss_dice_3: 3.351  loss_ce_4: 0.3984  loss_mask_4: 0.4555  loss_dice_4: 3.345  loss_ce_5: 0.3713  loss_mask_5: 0.4573  loss_dice_5: 3.348  loss_ce_6: 0.3791  loss_mask_6: 0.4594  loss_dice_6: 3.347  loss_ce_7: 0.3546  loss_mask_7: 0.4582  loss_dice_7: 3.344  loss_ce_8: 0.3743  loss_mask_8: 0.4608  loss_dice_8: 3.349  time: 1.4986  data_time: 0.0849  lr: 7.6913e-06  max_mem: 21492M
[01/17 23:19:10] d2.utils.events INFO:  eta: 12:24:01  iter: 10139  total_loss: 42.92  loss_ce: 0.3828  loss_mask: 0.4689  loss_dice: 3.354  loss_ce_0: 0.6352  loss_mask_0: 0.4552  loss_dice_0: 3.487  loss_ce_1: 0.389  loss_mask_1: 0.4709  loss_dice_1: 3.389  loss_ce_2: 0.3886  loss_mask_2: 0.4703  loss_dice_2: 3.353  loss_ce_3: 0.4142  loss_mask_3: 0.4731  loss_dice_3: 3.349  loss_ce_4: 0.3971  loss_mask_4: 0.4732  loss_dice_4: 3.347  loss_ce_5: 0.3738  loss_mask_5: 0.472  loss_dice_5: 3.361  loss_ce_6: 0.3787  loss_mask_6: 0.4736  loss_dice_6: 3.356  loss_ce_7: 0.3852  loss_mask_7: 0.4727  loss_dice_7: 3.355  loss_ce_8: 0.3819  loss_mask_8: 0.4718  loss_dice_8: 3.343  time: 1.4986  data_time: 0.0741  lr: 7.6867e-06  max_mem: 21492M
[01/17 23:19:40] d2.utils.events INFO:  eta: 12:23:33  iter: 10159  total_loss: 43.03  loss_ce: 0.3757  loss_mask: 0.4702  loss_dice: 3.443  loss_ce_0: 0.597  loss_mask_0: 0.456  loss_dice_0: 3.566  loss_ce_1: 0.3723  loss_mask_1: 0.4741  loss_dice_1: 3.489  loss_ce_2: 0.3876  loss_mask_2: 0.4719  loss_dice_2: 3.455  loss_ce_3: 0.3692  loss_mask_3: 0.4729  loss_dice_3: 3.441  loss_ce_4: 0.3621  loss_mask_4: 0.4731  loss_dice_4: 3.437  loss_ce_5: 0.372  loss_mask_5: 0.4734  loss_dice_5: 3.442  loss_ce_6: 0.3628  loss_mask_6: 0.4705  loss_dice_6: 3.436  loss_ce_7: 0.3567  loss_mask_7: 0.4705  loss_dice_7: 3.441  loss_ce_8: 0.3638  loss_mask_8: 0.4697  loss_dice_8: 3.444  time: 1.4986  data_time: 0.0710  lr: 7.6821e-06  max_mem: 21492M
[01/17 23:20:10] d2.utils.events INFO:  eta: 12:24:03  iter: 10179  total_loss: 42.77  loss_ce: 0.3652  loss_mask: 0.4627  loss_dice: 3.404  loss_ce_0: 0.6435  loss_mask_0: 0.4463  loss_dice_0: 3.551  loss_ce_1: 0.3818  loss_mask_1: 0.4604  loss_dice_1: 3.443  loss_ce_2: 0.3819  loss_mask_2: 0.4574  loss_dice_2: 3.411  loss_ce_3: 0.3717  loss_mask_3: 0.4601  loss_dice_3: 3.404  loss_ce_4: 0.381  loss_mask_4: 0.4593  loss_dice_4: 3.389  loss_ce_5: 0.3634  loss_mask_5: 0.4607  loss_dice_5: 3.412  loss_ce_6: 0.3831  loss_mask_6: 0.4582  loss_dice_6: 3.395  loss_ce_7: 0.3729  loss_mask_7: 0.4622  loss_dice_7: 3.404  loss_ce_8: 0.3622  loss_mask_8: 0.4619  loss_dice_8: 3.406  time: 1.4987  data_time: 0.0896  lr: 7.6774e-06  max_mem: 21492M
[01/17 23:20:41] d2.utils.events INFO:  eta: 12:23:54  iter: 10199  total_loss: 43.43  loss_ce: 0.4165  loss_mask: 0.4597  loss_dice: 3.441  loss_ce_0: 0.643  loss_mask_0: 0.4391  loss_dice_0: 3.584  loss_ce_1: 0.4085  loss_mask_1: 0.456  loss_dice_1: 3.493  loss_ce_2: 0.4448  loss_mask_2: 0.4567  loss_dice_2: 3.467  loss_ce_3: 0.4139  loss_mask_3: 0.4559  loss_dice_3: 3.445  loss_ce_4: 0.4182  loss_mask_4: 0.4534  loss_dice_4: 3.455  loss_ce_5: 0.4187  loss_mask_5: 0.4555  loss_dice_5: 3.444  loss_ce_6: 0.4184  loss_mask_6: 0.4572  loss_dice_6: 3.445  loss_ce_7: 0.4141  loss_mask_7: 0.4596  loss_dice_7: 3.45  loss_ce_8: 0.4055  loss_mask_8: 0.4571  loss_dice_8: 3.45  time: 1.4987  data_time: 0.0798  lr: 7.6728e-06  max_mem: 21492M
[01/17 23:21:11] d2.utils.events INFO:  eta: 12:23:21  iter: 10219  total_loss: 43.14  loss_ce: 0.3809  loss_mask: 0.4625  loss_dice: 3.405  loss_ce_0: 0.6208  loss_mask_0: 0.444  loss_dice_0: 3.537  loss_ce_1: 0.3759  loss_mask_1: 0.4616  loss_dice_1: 3.432  loss_ce_2: 0.4088  loss_mask_2: 0.46  loss_dice_2: 3.42  loss_ce_3: 0.3899  loss_mask_3: 0.4636  loss_dice_3: 3.402  loss_ce_4: 0.3851  loss_mask_4: 0.4643  loss_dice_4: 3.401  loss_ce_5: 0.3754  loss_mask_5: 0.4664  loss_dice_5: 3.401  loss_ce_6: 0.3773  loss_mask_6: 0.4655  loss_dice_6: 3.397  loss_ce_7: 0.3716  loss_mask_7: 0.4631  loss_dice_7: 3.396  loss_ce_8: 0.3833  loss_mask_8: 0.4647  loss_dice_8: 3.401  time: 1.4987  data_time: 0.0893  lr: 7.6682e-06  max_mem: 21492M
[01/17 23:21:42] d2.utils.events INFO:  eta: 12:23:04  iter: 10239  total_loss: 42.51  loss_ce: 0.3623  loss_mask: 0.4586  loss_dice: 3.369  loss_ce_0: 0.5812  loss_mask_0: 0.4452  loss_dice_0: 3.495  loss_ce_1: 0.3595  loss_mask_1: 0.4615  loss_dice_1: 3.41  loss_ce_2: 0.382  loss_mask_2: 0.4541  loss_dice_2: 3.374  loss_ce_3: 0.3693  loss_mask_3: 0.4565  loss_dice_3: 3.369  loss_ce_4: 0.3678  loss_mask_4: 0.457  loss_dice_4: 3.366  loss_ce_5: 0.3658  loss_mask_5: 0.4601  loss_dice_5: 3.372  loss_ce_6: 0.3591  loss_mask_6: 0.4609  loss_dice_6: 3.358  loss_ce_7: 0.3587  loss_mask_7: 0.459  loss_dice_7: 3.372  loss_ce_8: 0.3484  loss_mask_8: 0.4572  loss_dice_8: 3.369  time: 1.4988  data_time: 0.0730  lr: 7.6635e-06  max_mem: 21492M
[01/17 23:22:12] d2.utils.events INFO:  eta: 12:22:45  iter: 10259  total_loss: 42.59  loss_ce: 0.3785  loss_mask: 0.4598  loss_dice: 3.412  loss_ce_0: 0.6014  loss_mask_0: 0.4399  loss_dice_0: 3.54  loss_ce_1: 0.3658  loss_mask_1: 0.4626  loss_dice_1: 3.446  loss_ce_2: 0.3911  loss_mask_2: 0.4622  loss_dice_2: 3.43  loss_ce_3: 0.3651  loss_mask_3: 0.4603  loss_dice_3: 3.419  loss_ce_4: 0.3687  loss_mask_4: 0.4622  loss_dice_4: 3.409  loss_ce_5: 0.3659  loss_mask_5: 0.4599  loss_dice_5: 3.409  loss_ce_6: 0.3708  loss_mask_6: 0.4585  loss_dice_6: 3.409  loss_ce_7: 0.3495  loss_mask_7: 0.4597  loss_dice_7: 3.417  loss_ce_8: 0.3706  loss_mask_8: 0.4607  loss_dice_8: 3.414  time: 1.4988  data_time: 0.0838  lr: 7.6589e-06  max_mem: 21492M
[01/17 23:22:43] d2.utils.events INFO:  eta: 12:22:09  iter: 10279  total_loss: 42.44  loss_ce: 0.3516  loss_mask: 0.459  loss_dice: 3.354  loss_ce_0: 0.5974  loss_mask_0: 0.4408  loss_dice_0: 3.499  loss_ce_1: 0.3761  loss_mask_1: 0.4614  loss_dice_1: 3.399  loss_ce_2: 0.3939  loss_mask_2: 0.4601  loss_dice_2: 3.378  loss_ce_3: 0.375  loss_mask_3: 0.4588  loss_dice_3: 3.351  loss_ce_4: 0.3736  loss_mask_4: 0.46  loss_dice_4: 3.359  loss_ce_5: 0.3771  loss_mask_5: 0.4606  loss_dice_5: 3.353  loss_ce_6: 0.3787  loss_mask_6: 0.4613  loss_dice_6: 3.36  loss_ce_7: 0.3596  loss_mask_7: 0.4592  loss_dice_7: 3.369  loss_ce_8: 0.3714  loss_mask_8: 0.4587  loss_dice_8: 3.358  time: 1.4990  data_time: 0.0875  lr: 7.6543e-06  max_mem: 21492M
[01/17 23:23:15] d2.utils.events INFO:  eta: 12:22:09  iter: 10299  total_loss: 41.48  loss_ce: 0.3408  loss_mask: 0.4662  loss_dice: 3.297  loss_ce_0: 0.5934  loss_mask_0: 0.4472  loss_dice_0: 3.437  loss_ce_1: 0.379  loss_mask_1: 0.4685  loss_dice_1: 3.324  loss_ce_2: 0.3813  loss_mask_2: 0.4682  loss_dice_2: 3.307  loss_ce_3: 0.3527  loss_mask_3: 0.4639  loss_dice_3: 3.298  loss_ce_4: 0.3539  loss_mask_4: 0.4637  loss_dice_4: 3.295  loss_ce_5: 0.353  loss_mask_5: 0.4652  loss_dice_5: 3.297  loss_ce_6: 0.3472  loss_mask_6: 0.4648  loss_dice_6: 3.287  loss_ce_7: 0.3463  loss_mask_7: 0.4639  loss_dice_7: 3.298  loss_ce_8: 0.3516  loss_mask_8: 0.4672  loss_dice_8: 3.293  time: 1.4991  data_time: 0.0875  lr: 7.6496e-06  max_mem: 21492M
[01/17 23:23:45] d2.utils.events INFO:  eta: 12:21:50  iter: 10319  total_loss: 43.15  loss_ce: 0.3969  loss_mask: 0.4632  loss_dice: 3.422  loss_ce_0: 0.5964  loss_mask_0: 0.4515  loss_dice_0: 3.542  loss_ce_1: 0.4065  loss_mask_1: 0.4693  loss_dice_1: 3.46  loss_ce_2: 0.4001  loss_mask_2: 0.4652  loss_dice_2: 3.447  loss_ce_3: 0.4041  loss_mask_3: 0.4626  loss_dice_3: 3.439  loss_ce_4: 0.3959  loss_mask_4: 0.4646  loss_dice_4: 3.434  loss_ce_5: 0.3977  loss_mask_5: 0.464  loss_dice_5: 3.427  loss_ce_6: 0.3821  loss_mask_6: 0.4653  loss_dice_6: 3.429  loss_ce_7: 0.3839  loss_mask_7: 0.4658  loss_dice_7: 3.43  loss_ce_8: 0.3754  loss_mask_8: 0.4633  loss_dice_8: 3.432  time: 1.4991  data_time: 0.0822  lr: 7.645e-06  max_mem: 21492M
[01/17 23:24:15] d2.utils.events INFO:  eta: 12:21:21  iter: 10339  total_loss: 42.41  loss_ce: 0.336  loss_mask: 0.4623  loss_dice: 3.367  loss_ce_0: 0.5949  loss_mask_0: 0.4427  loss_dice_0: 3.51  loss_ce_1: 0.343  loss_mask_1: 0.4636  loss_dice_1: 3.421  loss_ce_2: 0.388  loss_mask_2: 0.4643  loss_dice_2: 3.38  loss_ce_3: 0.3742  loss_mask_3: 0.4651  loss_dice_3: 3.371  loss_ce_4: 0.3622  loss_mask_4: 0.4647  loss_dice_4: 3.361  loss_ce_5: 0.3543  loss_mask_5: 0.464  loss_dice_5: 3.37  loss_ce_6: 0.3567  loss_mask_6: 0.4619  loss_dice_6: 3.367  loss_ce_7: 0.3537  loss_mask_7: 0.4628  loss_dice_7: 3.368  loss_ce_8: 0.3451  loss_mask_8: 0.4619  loss_dice_8: 3.37  time: 1.4991  data_time: 0.0861  lr: 7.6403e-06  max_mem: 21492M
[01/17 23:24:46] d2.utils.events INFO:  eta: 12:20:51  iter: 10359  total_loss: 42.59  loss_ce: 0.391  loss_mask: 0.4613  loss_dice: 3.344  loss_ce_0: 0.6238  loss_mask_0: 0.4437  loss_dice_0: 3.495  loss_ce_1: 0.4055  loss_mask_1: 0.4657  loss_dice_1: 3.381  loss_ce_2: 0.4168  loss_mask_2: 0.463  loss_dice_2: 3.361  loss_ce_3: 0.3986  loss_mask_3: 0.4649  loss_dice_3: 3.349  loss_ce_4: 0.3904  loss_mask_4: 0.4609  loss_dice_4: 3.353  loss_ce_5: 0.3956  loss_mask_5: 0.4617  loss_dice_5: 3.343  loss_ce_6: 0.3911  loss_mask_6: 0.463  loss_dice_6: 3.341  loss_ce_7: 0.3769  loss_mask_7: 0.4619  loss_dice_7: 3.339  loss_ce_8: 0.3904  loss_mask_8: 0.4637  loss_dice_8: 3.342  time: 1.4992  data_time: 0.0673  lr: 7.6357e-06  max_mem: 21492M
[01/17 23:25:16] d2.utils.events INFO:  eta: 12:20:21  iter: 10379  total_loss: 42.5  loss_ce: 0.3857  loss_mask: 0.4472  loss_dice: 3.344  loss_ce_0: 0.6275  loss_mask_0: 0.4387  loss_dice_0: 3.469  loss_ce_1: 0.3827  loss_mask_1: 0.4539  loss_dice_1: 3.385  loss_ce_2: 0.3889  loss_mask_2: 0.4528  loss_dice_2: 3.353  loss_ce_3: 0.3904  loss_mask_3: 0.4467  loss_dice_3: 3.352  loss_ce_4: 0.3873  loss_mask_4: 0.4488  loss_dice_4: 3.342  loss_ce_5: 0.3948  loss_mask_5: 0.4492  loss_dice_5: 3.342  loss_ce_6: 0.3819  loss_mask_6: 0.4497  loss_dice_6: 3.343  loss_ce_7: 0.3793  loss_mask_7: 0.4489  loss_dice_7: 3.352  loss_ce_8: 0.3725  loss_mask_8: 0.4471  loss_dice_8: 3.346  time: 1.4992  data_time: 0.0756  lr: 7.6311e-06  max_mem: 21492M
[01/17 23:25:46] d2.utils.events INFO:  eta: 12:19:51  iter: 10399  total_loss: 42.15  loss_ce: 0.3384  loss_mask: 0.4516  loss_dice: 3.358  loss_ce_0: 0.5828  loss_mask_0: 0.4372  loss_dice_0: 3.516  loss_ce_1: 0.365  loss_mask_1: 0.4569  loss_dice_1: 3.4  loss_ce_2: 0.3769  loss_mask_2: 0.4512  loss_dice_2: 3.384  loss_ce_3: 0.3774  loss_mask_3: 0.4504  loss_dice_3: 3.361  loss_ce_4: 0.363  loss_mask_4: 0.4479  loss_dice_4: 3.367  loss_ce_5: 0.348  loss_mask_5: 0.4489  loss_dice_5: 3.367  loss_ce_6: 0.3549  loss_mask_6: 0.4498  loss_dice_6: 3.363  loss_ce_7: 0.3541  loss_mask_7: 0.4498  loss_dice_7: 3.365  loss_ce_8: 0.354  loss_mask_8: 0.4495  loss_dice_8: 3.367  time: 1.4992  data_time: 0.0825  lr: 7.6264e-06  max_mem: 21492M
[01/17 23:26:16] d2.utils.events INFO:  eta: 12:19:21  iter: 10419  total_loss: 42.33  loss_ce: 0.3756  loss_mask: 0.4617  loss_dice: 3.362  loss_ce_0: 0.5973  loss_mask_0: 0.4449  loss_dice_0: 3.51  loss_ce_1: 0.3712  loss_mask_1: 0.4657  loss_dice_1: 3.41  loss_ce_2: 0.3657  loss_mask_2: 0.464  loss_dice_2: 3.376  loss_ce_3: 0.3588  loss_mask_3: 0.4618  loss_dice_3: 3.37  loss_ce_4: 0.3513  loss_mask_4: 0.4622  loss_dice_4: 3.368  loss_ce_5: 0.3491  loss_mask_5: 0.4619  loss_dice_5: 3.375  loss_ce_6: 0.3554  loss_mask_6: 0.4635  loss_dice_6: 3.372  loss_ce_7: 0.3454  loss_mask_7: 0.4653  loss_dice_7: 3.365  loss_ce_8: 0.3551  loss_mask_8: 0.4617  loss_dice_8: 3.376  time: 1.4992  data_time: 0.0725  lr: 7.6218e-06  max_mem: 21492M
[01/17 23:26:48] d2.utils.events INFO:  eta: 12:19:25  iter: 10439  total_loss: 42.38  loss_ce: 0.3817  loss_mask: 0.4571  loss_dice: 3.354  loss_ce_0: 0.6066  loss_mask_0: 0.4323  loss_dice_0: 3.502  loss_ce_1: 0.4004  loss_mask_1: 0.4496  loss_dice_1: 3.399  loss_ce_2: 0.3941  loss_mask_2: 0.449  loss_dice_2: 3.371  loss_ce_3: 0.3855  loss_mask_3: 0.4535  loss_dice_3: 3.356  loss_ce_4: 0.3696  loss_mask_4: 0.4577  loss_dice_4: 3.359  loss_ce_5: 0.3772  loss_mask_5: 0.4568  loss_dice_5: 3.36  loss_ce_6: 0.3659  loss_mask_6: 0.4559  loss_dice_6: 3.357  loss_ce_7: 0.3641  loss_mask_7: 0.4572  loss_dice_7: 3.346  loss_ce_8: 0.37  loss_mask_8: 0.4585  loss_dice_8: 3.351  time: 1.4993  data_time: 0.1019  lr: 7.6172e-06  max_mem: 21492M
[01/17 23:27:18] d2.utils.events INFO:  eta: 12:19:25  iter: 10459  total_loss: 42.57  loss_ce: 0.3581  loss_mask: 0.4606  loss_dice: 3.343  loss_ce_0: 0.5998  loss_mask_0: 0.4444  loss_dice_0: 3.468  loss_ce_1: 0.3728  loss_mask_1: 0.4653  loss_dice_1: 3.378  loss_ce_2: 0.3828  loss_mask_2: 0.4643  loss_dice_2: 3.368  loss_ce_3: 0.3606  loss_mask_3: 0.4602  loss_dice_3: 3.346  loss_ce_4: 0.3723  loss_mask_4: 0.4634  loss_dice_4: 3.349  loss_ce_5: 0.3605  loss_mask_5: 0.4628  loss_dice_5: 3.339  loss_ce_6: 0.367  loss_mask_6: 0.4585  loss_dice_6: 3.341  loss_ce_7: 0.375  loss_mask_7: 0.4583  loss_dice_7: 3.342  loss_ce_8: 0.3623  loss_mask_8: 0.4626  loss_dice_8: 3.338  time: 1.4994  data_time: 0.0783  lr: 7.6125e-06  max_mem: 21492M
[01/17 23:27:49] d2.utils.events INFO:  eta: 12:18:53  iter: 10479  total_loss: 42.61  loss_ce: 0.3442  loss_mask: 0.4487  loss_dice: 3.416  loss_ce_0: 0.6124  loss_mask_0: 0.4352  loss_dice_0: 3.544  loss_ce_1: 0.3477  loss_mask_1: 0.452  loss_dice_1: 3.449  loss_ce_2: 0.3558  loss_mask_2: 0.4532  loss_dice_2: 3.42  loss_ce_3: 0.3491  loss_mask_3: 0.4504  loss_dice_3: 3.413  loss_ce_4: 0.3505  loss_mask_4: 0.4507  loss_dice_4: 3.408  loss_ce_5: 0.3363  loss_mask_5: 0.4488  loss_dice_5: 3.41  loss_ce_6: 0.3293  loss_mask_6: 0.4515  loss_dice_6: 3.407  loss_ce_7: 0.3565  loss_mask_7: 0.4508  loss_dice_7: 3.406  loss_ce_8: 0.3578  loss_mask_8: 0.4476  loss_dice_8: 3.415  time: 1.4994  data_time: 0.0738  lr: 7.6079e-06  max_mem: 21492M
[01/17 23:28:19] d2.utils.events INFO:  eta: 12:18:28  iter: 10499  total_loss: 42.7  loss_ce: 0.3901  loss_mask: 0.4509  loss_dice: 3.355  loss_ce_0: 0.6307  loss_mask_0: 0.4332  loss_dice_0: 3.494  loss_ce_1: 0.4116  loss_mask_1: 0.454  loss_dice_1: 3.398  loss_ce_2: 0.4336  loss_mask_2: 0.453  loss_dice_2: 3.367  loss_ce_3: 0.432  loss_mask_3: 0.4539  loss_dice_3: 3.355  loss_ce_4: 0.4092  loss_mask_4: 0.4525  loss_dice_4: 3.353  loss_ce_5: 0.4094  loss_mask_5: 0.4537  loss_dice_5: 3.362  loss_ce_6: 0.4179  loss_mask_6: 0.4502  loss_dice_6: 3.351  loss_ce_7: 0.3993  loss_mask_7: 0.452  loss_dice_7: 3.356  loss_ce_8: 0.3959  loss_mask_8: 0.4517  loss_dice_8: 3.359  time: 1.4995  data_time: 0.0812  lr: 7.6032e-06  max_mem: 21492M
[01/17 23:28:50] d2.utils.events INFO:  eta: 12:18:25  iter: 10519  total_loss: 42.36  loss_ce: 0.3763  loss_mask: 0.4526  loss_dice: 3.361  loss_ce_0: 0.6176  loss_mask_0: 0.4398  loss_dice_0: 3.487  loss_ce_1: 0.3787  loss_mask_1: 0.4574  loss_dice_1: 3.386  loss_ce_2: 0.3971  loss_mask_2: 0.4547  loss_dice_2: 3.377  loss_ce_3: 0.3918  loss_mask_3: 0.4564  loss_dice_3: 3.367  loss_ce_4: 0.3813  loss_mask_4: 0.4537  loss_dice_4: 3.366  loss_ce_5: 0.3659  loss_mask_5: 0.4541  loss_dice_5: 3.366  loss_ce_6: 0.375  loss_mask_6: 0.4558  loss_dice_6: 3.367  loss_ce_7: 0.3764  loss_mask_7: 0.4544  loss_dice_7: 3.367  loss_ce_8: 0.3778  loss_mask_8: 0.4534  loss_dice_8: 3.368  time: 1.4995  data_time: 0.0838  lr: 7.5986e-06  max_mem: 21492M
[01/17 23:29:20] d2.utils.events INFO:  eta: 12:17:50  iter: 10539  total_loss: 42.55  loss_ce: 0.3656  loss_mask: 0.4678  loss_dice: 3.347  loss_ce_0: 0.594  loss_mask_0: 0.446  loss_dice_0: 3.481  loss_ce_1: 0.3838  loss_mask_1: 0.4688  loss_dice_1: 3.387  loss_ce_2: 0.4007  loss_mask_2: 0.4665  loss_dice_2: 3.361  loss_ce_3: 0.3839  loss_mask_3: 0.4702  loss_dice_3: 3.349  loss_ce_4: 0.3844  loss_mask_4: 0.4687  loss_dice_4: 3.356  loss_ce_5: 0.3645  loss_mask_5: 0.4675  loss_dice_5: 3.351  loss_ce_6: 0.3589  loss_mask_6: 0.467  loss_dice_6: 3.349  loss_ce_7: 0.3769  loss_mask_7: 0.4675  loss_dice_7: 3.354  loss_ce_8: 0.3541  loss_mask_8: 0.4678  loss_dice_8: 3.357  time: 1.4995  data_time: 0.0800  lr: 7.594e-06  max_mem: 21492M
[01/17 23:29:52] d2.utils.events INFO:  eta: 12:17:43  iter: 10559  total_loss: 42.43  loss_ce: 0.3756  loss_mask: 0.4634  loss_dice: 3.354  loss_ce_0: 0.6395  loss_mask_0: 0.4459  loss_dice_0: 3.489  loss_ce_1: 0.3838  loss_mask_1: 0.4596  loss_dice_1: 3.4  loss_ce_2: 0.4006  loss_mask_2: 0.4585  loss_dice_2: 3.368  loss_ce_3: 0.3741  loss_mask_3: 0.4611  loss_dice_3: 3.361  loss_ce_4: 0.3761  loss_mask_4: 0.4629  loss_dice_4: 3.358  loss_ce_5: 0.3806  loss_mask_5: 0.4624  loss_dice_5: 3.36  loss_ce_6: 0.3817  loss_mask_6: 0.4607  loss_dice_6: 3.362  loss_ce_7: 0.3763  loss_mask_7: 0.4624  loss_dice_7: 3.361  loss_ce_8: 0.378  loss_mask_8: 0.4639  loss_dice_8: 3.361  time: 1.4997  data_time: 0.0818  lr: 7.5893e-06  max_mem: 21492M
[01/17 23:30:25] d2.utils.events INFO:  eta: 12:17:52  iter: 10579  total_loss: 42.89  loss_ce: 0.361  loss_mask: 0.4582  loss_dice: 3.411  loss_ce_0: 0.5958  loss_mask_0: 0.4454  loss_dice_0: 3.558  loss_ce_1: 0.3687  loss_mask_1: 0.4584  loss_dice_1: 3.456  loss_ce_2: 0.3744  loss_mask_2: 0.4585  loss_dice_2: 3.434  loss_ce_3: 0.3709  loss_mask_3: 0.456  loss_dice_3: 3.423  loss_ce_4: 0.3604  loss_mask_4: 0.4545  loss_dice_4: 3.421  loss_ce_5: 0.3691  loss_mask_5: 0.4525  loss_dice_5: 3.423  loss_ce_6: 0.3609  loss_mask_6: 0.4553  loss_dice_6: 3.417  loss_ce_7: 0.3551  loss_mask_7: 0.4589  loss_dice_7: 3.421  loss_ce_8: 0.3569  loss_mask_8: 0.4559  loss_dice_8: 3.423  time: 1.5000  data_time: 0.1028  lr: 7.5847e-06  max_mem: 21492M
[01/17 23:30:57] d2.utils.events INFO:  eta: 12:17:22  iter: 10599  total_loss: 42.51  loss_ce: 0.3695  loss_mask: 0.4601  loss_dice: 3.36  loss_ce_0: 0.5931  loss_mask_0: 0.4475  loss_dice_0: 3.487  loss_ce_1: 0.3686  loss_mask_1: 0.4656  loss_dice_1: 3.394  loss_ce_2: 0.3878  loss_mask_2: 0.462  loss_dice_2: 3.381  loss_ce_3: 0.385  loss_mask_3: 0.4583  loss_dice_3: 3.358  loss_ce_4: 0.377  loss_mask_4: 0.4585  loss_dice_4: 3.359  loss_ce_5: 0.381  loss_mask_5: 0.4581  loss_dice_5: 3.358  loss_ce_6: 0.3752  loss_mask_6: 0.4604  loss_dice_6: 3.363  loss_ce_7: 0.3798  loss_mask_7: 0.4617  loss_dice_7: 3.363  loss_ce_8: 0.3752  loss_mask_8: 0.4614  loss_dice_8: 3.358  time: 1.5001  data_time: 0.1125  lr: 7.58e-06  max_mem: 21492M
[01/17 23:31:30] d2.utils.events INFO:  eta: 12:17:40  iter: 10619  total_loss: 42.21  loss_ce: 0.3552  loss_mask: 0.4618  loss_dice: 3.352  loss_ce_0: 0.5988  loss_mask_0: 0.4371  loss_dice_0: 3.493  loss_ce_1: 0.3491  loss_mask_1: 0.4619  loss_dice_1: 3.384  loss_ce_2: 0.3651  loss_mask_2: 0.4608  loss_dice_2: 3.364  loss_ce_3: 0.3602  loss_mask_3: 0.4628  loss_dice_3: 3.349  loss_ce_4: 0.376  loss_mask_4: 0.4641  loss_dice_4: 3.357  loss_ce_5: 0.3532  loss_mask_5: 0.4623  loss_dice_5: 3.358  loss_ce_6: 0.3618  loss_mask_6: 0.4612  loss_dice_6: 3.353  loss_ce_7: 0.3456  loss_mask_7: 0.4602  loss_dice_7: 3.355  loss_ce_8: 0.3667  loss_mask_8: 0.4612  loss_dice_8: 3.349  time: 1.5004  data_time: 0.1115  lr: 7.5754e-06  max_mem: 21492M
[01/17 23:32:03] d2.utils.events INFO:  eta: 12:17:53  iter: 10639  total_loss: 42.12  loss_ce: 0.3762  loss_mask: 0.4458  loss_dice: 3.322  loss_ce_0: 0.5953  loss_mask_0: 0.4297  loss_dice_0: 3.466  loss_ce_1: 0.3926  loss_mask_1: 0.4472  loss_dice_1: 3.364  loss_ce_2: 0.3959  loss_mask_2: 0.4459  loss_dice_2: 3.339  loss_ce_3: 0.3598  loss_mask_3: 0.4461  loss_dice_3: 3.342  loss_ce_4: 0.3796  loss_mask_4: 0.4452  loss_dice_4: 3.331  loss_ce_5: 0.3618  loss_mask_5: 0.446  loss_dice_5: 3.33  loss_ce_6: 0.362  loss_mask_6: 0.4462  loss_dice_6: 3.322  loss_ce_7: 0.3552  loss_mask_7: 0.4458  loss_dice_7: 3.332  loss_ce_8: 0.3574  loss_mask_8: 0.4468  loss_dice_8: 3.327  time: 1.5007  data_time: 0.0953  lr: 7.5708e-06  max_mem: 21492M
[01/17 23:32:33] d2.utils.events INFO:  eta: 12:17:27  iter: 10659  total_loss: 42.95  loss_ce: 0.3596  loss_mask: 0.4538  loss_dice: 3.414  loss_ce_0: 0.6175  loss_mask_0: 0.4376  loss_dice_0: 3.549  loss_ce_1: 0.4013  loss_mask_1: 0.4551  loss_dice_1: 3.456  loss_ce_2: 0.3939  loss_mask_2: 0.4548  loss_dice_2: 3.432  loss_ce_3: 0.398  loss_mask_3: 0.4545  loss_dice_3: 3.423  loss_ce_4: 0.3742  loss_mask_4: 0.4569  loss_dice_4: 3.42  loss_ce_5: 0.3849  loss_mask_5: 0.4545  loss_dice_5: 3.415  loss_ce_6: 0.382  loss_mask_6: 0.4522  loss_dice_6: 3.417  loss_ce_7: 0.3765  loss_mask_7: 0.4529  loss_dice_7: 3.412  loss_ce_8: 0.3732  loss_mask_8: 0.455  loss_dice_8: 3.408  time: 1.5007  data_time: 0.0783  lr: 7.5661e-06  max_mem: 21492M
[01/17 23:33:03] d2.utils.events INFO:  eta: 12:17:08  iter: 10679  total_loss: 42.25  loss_ce: 0.3733  loss_mask: 0.4565  loss_dice: 3.356  loss_ce_0: 0.583  loss_mask_0: 0.4365  loss_dice_0: 3.505  loss_ce_1: 0.3759  loss_mask_1: 0.4584  loss_dice_1: 3.397  loss_ce_2: 0.3691  loss_mask_2: 0.4558  loss_dice_2: 3.37  loss_ce_3: 0.3727  loss_mask_3: 0.452  loss_dice_3: 3.356  loss_ce_4: 0.3532  loss_mask_4: 0.452  loss_dice_4: 3.363  loss_ce_5: 0.3546  loss_mask_5: 0.4529  loss_dice_5: 3.366  loss_ce_6: 0.3548  loss_mask_6: 0.455  loss_dice_6: 3.363  loss_ce_7: 0.3477  loss_mask_7: 0.4566  loss_dice_7: 3.359  loss_ce_8: 0.34  loss_mask_8: 0.456  loss_dice_8: 3.355  time: 1.5007  data_time: 0.0689  lr: 7.5615e-06  max_mem: 21492M
[01/17 23:33:33] d2.utils.events INFO:  eta: 12:16:23  iter: 10699  total_loss: 43.02  loss_ce: 0.3595  loss_mask: 0.4643  loss_dice: 3.407  loss_ce_0: 0.6237  loss_mask_0: 0.4536  loss_dice_0: 3.519  loss_ce_1: 0.3862  loss_mask_1: 0.4721  loss_dice_1: 3.439  loss_ce_2: 0.3963  loss_mask_2: 0.4697  loss_dice_2: 3.423  loss_ce_3: 0.3761  loss_mask_3: 0.4633  loss_dice_3: 3.416  loss_ce_4: 0.376  loss_mask_4: 0.4645  loss_dice_4: 3.411  loss_ce_5: 0.3735  loss_mask_5: 0.4646  loss_dice_5: 3.41  loss_ce_6: 0.3681  loss_mask_6: 0.4642  loss_dice_6: 3.403  loss_ce_7: 0.3748  loss_mask_7: 0.4634  loss_dice_7: 3.407  loss_ce_8: 0.3798  loss_mask_8: 0.4655  loss_dice_8: 3.4  time: 1.5007  data_time: 0.0835  lr: 7.5568e-06  max_mem: 21492M
[01/17 23:34:03] d2.utils.events INFO:  eta: 12:15:05  iter: 10719  total_loss: 42.48  loss_ce: 0.3666  loss_mask: 0.4567  loss_dice: 3.359  loss_ce_0: 0.6205  loss_mask_0: 0.4545  loss_dice_0: 3.48  loss_ce_1: 0.3651  loss_mask_1: 0.4675  loss_dice_1: 3.396  loss_ce_2: 0.3916  loss_mask_2: 0.4647  loss_dice_2: 3.372  loss_ce_3: 0.3658  loss_mask_3: 0.4642  loss_dice_3: 3.363  loss_ce_4: 0.3574  loss_mask_4: 0.4629  loss_dice_4: 3.365  loss_ce_5: 0.3572  loss_mask_5: 0.461  loss_dice_5: 3.36  loss_ce_6: 0.3683  loss_mask_6: 0.4597  loss_dice_6: 3.374  loss_ce_7: 0.3593  loss_mask_7: 0.4594  loss_dice_7: 3.369  loss_ce_8: 0.357  loss_mask_8: 0.4578  loss_dice_8: 3.365  time: 1.5007  data_time: 0.0718  lr: 7.5522e-06  max_mem: 21492M
[01/17 23:34:33] d2.utils.events INFO:  eta: 12:14:09  iter: 10739  total_loss: 42.36  loss_ce: 0.3771  loss_mask: 0.4868  loss_dice: 3.378  loss_ce_0: 0.6184  loss_mask_0: 0.4673  loss_dice_0: 3.494  loss_ce_1: 0.3593  loss_mask_1: 0.4836  loss_dice_1: 3.407  loss_ce_2: 0.3877  loss_mask_2: 0.4808  loss_dice_2: 3.386  loss_ce_3: 0.3776  loss_mask_3: 0.4766  loss_dice_3: 3.371  loss_ce_4: 0.3793  loss_mask_4: 0.4772  loss_dice_4: 3.371  loss_ce_5: 0.3674  loss_mask_5: 0.482  loss_dice_5: 3.376  loss_ce_6: 0.361  loss_mask_6: 0.4865  loss_dice_6: 3.371  loss_ce_7: 0.3825  loss_mask_7: 0.4831  loss_dice_7: 3.372  loss_ce_8: 0.3782  loss_mask_8: 0.4854  loss_dice_8: 3.376  time: 1.5007  data_time: 0.0758  lr: 7.5476e-06  max_mem: 21492M
[01/17 23:35:03] d2.utils.events INFO:  eta: 12:13:39  iter: 10759  total_loss: 42.62  loss_ce: 0.3554  loss_mask: 0.4571  loss_dice: 3.398  loss_ce_0: 0.5729  loss_mask_0: 0.4425  loss_dice_0: 3.503  loss_ce_1: 0.382  loss_mask_1: 0.4608  loss_dice_1: 3.437  loss_ce_2: 0.3726  loss_mask_2: 0.4562  loss_dice_2: 3.402  loss_ce_3: 0.3976  loss_mask_3: 0.4548  loss_dice_3: 3.398  loss_ce_4: 0.3735  loss_mask_4: 0.4565  loss_dice_4: 3.391  loss_ce_5: 0.3644  loss_mask_5: 0.4528  loss_dice_5: 3.396  loss_ce_6: 0.3639  loss_mask_6: 0.4554  loss_dice_6: 3.394  loss_ce_7: 0.3597  loss_mask_7: 0.457  loss_dice_7: 3.397  loss_ce_8: 0.3799  loss_mask_8: 0.4577  loss_dice_8: 3.392  time: 1.5007  data_time: 0.0726  lr: 7.5429e-06  max_mem: 21492M
[01/17 23:35:36] d2.utils.events INFO:  eta: 12:13:55  iter: 10779  total_loss: 41.72  loss_ce: 0.3742  loss_mask: 0.4668  loss_dice: 3.296  loss_ce_0: 0.5923  loss_mask_0: 0.4505  loss_dice_0: 3.421  loss_ce_1: 0.3711  loss_mask_1: 0.4749  loss_dice_1: 3.342  loss_ce_2: 0.3879  loss_mask_2: 0.4729  loss_dice_2: 3.327  loss_ce_3: 0.3853  loss_mask_3: 0.4709  loss_dice_3: 3.314  loss_ce_4: 0.3704  loss_mask_4: 0.4701  loss_dice_4: 3.314  loss_ce_5: 0.3649  loss_mask_5: 0.4708  loss_dice_5: 3.304  loss_ce_6: 0.3831  loss_mask_6: 0.4688  loss_dice_6: 3.301  loss_ce_7: 0.3766  loss_mask_7: 0.4685  loss_dice_7: 3.305  loss_ce_8: 0.3721  loss_mask_8: 0.4689  loss_dice_8: 3.312  time: 1.5010  data_time: 0.0926  lr: 7.5383e-06  max_mem: 21492M
[01/17 23:36:07] d2.utils.events INFO:  eta: 12:13:26  iter: 10799  total_loss: 41.99  loss_ce: 0.384  loss_mask: 0.4592  loss_dice: 3.34  loss_ce_0: 0.6168  loss_mask_0: 0.4377  loss_dice_0: 3.479  loss_ce_1: 0.3704  loss_mask_1: 0.4576  loss_dice_1: 3.379  loss_ce_2: 0.3985  loss_mask_2: 0.4522  loss_dice_2: 3.356  loss_ce_3: 0.3684  loss_mask_3: 0.4572  loss_dice_3: 3.344  loss_ce_4: 0.3552  loss_mask_4: 0.4542  loss_dice_4: 3.354  loss_ce_5: 0.3671  loss_mask_5: 0.4545  loss_dice_5: 3.344  loss_ce_6: 0.3718  loss_mask_6: 0.4561  loss_dice_6: 3.343  loss_ce_7: 0.3556  loss_mask_7: 0.4587  loss_dice_7: 3.357  loss_ce_8: 0.3553  loss_mask_8: 0.4572  loss_dice_8: 3.347  time: 1.5010  data_time: 0.0811  lr: 7.5336e-06  max_mem: 21492M
[01/17 23:36:36] d2.utils.events INFO:  eta: 12:12:45  iter: 10819  total_loss: 42.98  loss_ce: 0.3918  loss_mask: 0.4692  loss_dice: 3.398  loss_ce_0: 0.6155  loss_mask_0: 0.4579  loss_dice_0: 3.517  loss_ce_1: 0.4078  loss_mask_1: 0.4749  loss_dice_1: 3.432  loss_ce_2: 0.407  loss_mask_2: 0.4711  loss_dice_2: 3.411  loss_ce_3: 0.404  loss_mask_3: 0.469  loss_dice_3: 3.407  loss_ce_4: 0.393  loss_mask_4: 0.4664  loss_dice_4: 3.403  loss_ce_5: 0.4012  loss_mask_5: 0.469  loss_dice_5: 3.399  loss_ce_6: 0.3817  loss_mask_6: 0.4709  loss_dice_6: 3.4  loss_ce_7: 0.3704  loss_mask_7: 0.4722  loss_dice_7: 3.404  loss_ce_8: 0.387  loss_mask_8: 0.4727  loss_dice_8: 3.401  time: 1.5010  data_time: 0.0723  lr: 7.529e-06  max_mem: 21492M
[01/17 23:37:07] d2.utils.events INFO:  eta: 12:12:22  iter: 10839  total_loss: 42.94  loss_ce: 0.3859  loss_mask: 0.4588  loss_dice: 3.39  loss_ce_0: 0.6029  loss_mask_0: 0.4448  loss_dice_0: 3.507  loss_ce_1: 0.3903  loss_mask_1: 0.4605  loss_dice_1: 3.438  loss_ce_2: 0.4028  loss_mask_2: 0.4581  loss_dice_2: 3.405  loss_ce_3: 0.3849  loss_mask_3: 0.4591  loss_dice_3: 3.384  loss_ce_4: 0.3762  loss_mask_4: 0.4592  loss_dice_4: 3.389  loss_ce_5: 0.3822  loss_mask_5: 0.458  loss_dice_5: 3.39  loss_ce_6: 0.3857  loss_mask_6: 0.4585  loss_dice_6: 3.385  loss_ce_7: 0.3634  loss_mask_7: 0.4615  loss_dice_7: 3.389  loss_ce_8: 0.3787  loss_mask_8: 0.4611  loss_dice_8: 3.386  time: 1.5010  data_time: 0.0883  lr: 7.5243e-06  max_mem: 21492M
[01/17 23:37:36] d2.utils.events INFO:  eta: 12:11:55  iter: 10859  total_loss: 41.93  loss_ce: 0.3622  loss_mask: 0.4519  loss_dice: 3.349  loss_ce_0: 0.6165  loss_mask_0: 0.4428  loss_dice_0: 3.478  loss_ce_1: 0.3551  loss_mask_1: 0.4659  loss_dice_1: 3.386  loss_ce_2: 0.3747  loss_mask_2: 0.4603  loss_dice_2: 3.364  loss_ce_3: 0.3626  loss_mask_3: 0.4537  loss_dice_3: 3.353  loss_ce_4: 0.358  loss_mask_4: 0.4511  loss_dice_4: 3.353  loss_ce_5: 0.3505  loss_mask_5: 0.4533  loss_dice_5: 3.348  loss_ce_6: 0.3505  loss_mask_6: 0.4515  loss_dice_6: 3.353  loss_ce_7: 0.3432  loss_mask_7: 0.4514  loss_dice_7: 3.352  loss_ce_8: 0.353  loss_mask_8: 0.4504  loss_dice_8: 3.349  time: 1.5009  data_time: 0.0715  lr: 7.5197e-06  max_mem: 21492M
[01/17 23:38:06] d2.utils.events INFO:  eta: 12:11:21  iter: 10879  total_loss: 42.52  loss_ce: 0.3813  loss_mask: 0.4489  loss_dice: 3.369  loss_ce_0: 0.6134  loss_mask_0: 0.4368  loss_dice_0: 3.5  loss_ce_1: 0.3712  loss_mask_1: 0.4512  loss_dice_1: 3.409  loss_ce_2: 0.3995  loss_mask_2: 0.4527  loss_dice_2: 3.382  loss_ce_3: 0.4004  loss_mask_3: 0.4519  loss_dice_3: 3.357  loss_ce_4: 0.391  loss_mask_4: 0.4513  loss_dice_4: 3.363  loss_ce_5: 0.392  loss_mask_5: 0.4508  loss_dice_5: 3.366  loss_ce_6: 0.3825  loss_mask_6: 0.4492  loss_dice_6: 3.366  loss_ce_7: 0.3783  loss_mask_7: 0.4496  loss_dice_7: 3.369  loss_ce_8: 0.3875  loss_mask_8: 0.4497  loss_dice_8: 3.375  time: 1.5009  data_time: 0.0798  lr: 7.515e-06  max_mem: 21492M
[01/17 23:38:36] d2.utils.events INFO:  eta: 12:11:26  iter: 10899  total_loss: 42.83  loss_ce: 0.3408  loss_mask: 0.46  loss_dice: 3.433  loss_ce_0: 0.5851  loss_mask_0: 0.4443  loss_dice_0: 3.545  loss_ce_1: 0.3464  loss_mask_1: 0.4645  loss_dice_1: 3.473  loss_ce_2: 0.3728  loss_mask_2: 0.4598  loss_dice_2: 3.445  loss_ce_3: 0.3542  loss_mask_3: 0.4578  loss_dice_3: 3.439  loss_ce_4: 0.3516  loss_mask_4: 0.459  loss_dice_4: 3.423  loss_ce_5: 0.3375  loss_mask_5: 0.4624  loss_dice_5: 3.439  loss_ce_6: 0.3426  loss_mask_6: 0.4618  loss_dice_6: 3.421  loss_ce_7: 0.3463  loss_mask_7: 0.4601  loss_dice_7: 3.421  loss_ce_8: 0.3556  loss_mask_8: 0.4615  loss_dice_8: 3.433  time: 1.5009  data_time: 0.0904  lr: 7.5104e-06  max_mem: 21492M
[01/17 23:39:07] d2.utils.events INFO:  eta: 12:10:25  iter: 10919  total_loss: 42.84  loss_ce: 0.3782  loss_mask: 0.4623  loss_dice: 3.361  loss_ce_0: 0.6172  loss_mask_0: 0.449  loss_dice_0: 3.478  loss_ce_1: 0.4241  loss_mask_1: 0.4628  loss_dice_1: 3.389  loss_ce_2: 0.4106  loss_mask_2: 0.462  loss_dice_2: 3.371  loss_ce_3: 0.4046  loss_mask_3: 0.4629  loss_dice_3: 3.359  loss_ce_4: 0.3816  loss_mask_4: 0.463  loss_dice_4: 3.354  loss_ce_5: 0.3889  loss_mask_5: 0.4631  loss_dice_5: 3.358  loss_ce_6: 0.385  loss_mask_6: 0.4634  loss_dice_6: 3.35  loss_ce_7: 0.384  loss_mask_7: 0.4641  loss_dice_7: 3.356  loss_ce_8: 0.3853  loss_mask_8: 0.4616  loss_dice_8: 3.355  time: 1.5010  data_time: 0.0747  lr: 7.5058e-06  max_mem: 21492M
[01/17 23:39:40] d2.utils.events INFO:  eta: 12:10:46  iter: 10939  total_loss: 41.7  loss_ce: 0.3619  loss_mask: 0.4511  loss_dice: 3.323  loss_ce_0: 0.594  loss_mask_0: 0.4304  loss_dice_0: 3.458  loss_ce_1: 0.3477  loss_mask_1: 0.4453  loss_dice_1: 3.37  loss_ce_2: 0.3711  loss_mask_2: 0.4455  loss_dice_2: 3.337  loss_ce_3: 0.3428  loss_mask_3: 0.4468  loss_dice_3: 3.325  loss_ce_4: 0.3686  loss_mask_4: 0.4469  loss_dice_4: 3.328  loss_ce_5: 0.3455  loss_mask_5: 0.4491  loss_dice_5: 3.326  loss_ce_6: 0.368  loss_mask_6: 0.4503  loss_dice_6: 3.322  loss_ce_7: 0.354  loss_mask_7: 0.4513  loss_dice_7: 3.322  loss_ce_8: 0.3371  loss_mask_8: 0.4525  loss_dice_8: 3.321  time: 1.5012  data_time: 0.0841  lr: 7.5011e-06  max_mem: 21589M
[01/17 23:40:12] d2.utils.events INFO:  eta: 12:10:03  iter: 10959  total_loss: 42.15  loss_ce: 0.371  loss_mask: 0.4544  loss_dice: 3.337  loss_ce_0: 0.6192  loss_mask_0: 0.4459  loss_dice_0: 3.464  loss_ce_1: 0.3659  loss_mask_1: 0.4653  loss_dice_1: 3.367  loss_ce_2: 0.4097  loss_mask_2: 0.4591  loss_dice_2: 3.359  loss_ce_3: 0.3628  loss_mask_3: 0.4574  loss_dice_3: 3.334  loss_ce_4: 0.3567  loss_mask_4: 0.4589  loss_dice_4: 3.338  loss_ce_5: 0.3666  loss_mask_5: 0.4577  loss_dice_5: 3.341  loss_ce_6: 0.3603  loss_mask_6: 0.4576  loss_dice_6: 3.341  loss_ce_7: 0.3556  loss_mask_7: 0.4569  loss_dice_7: 3.344  loss_ce_8: 0.3672  loss_mask_8: 0.456  loss_dice_8: 3.333  time: 1.5014  data_time: 0.0958  lr: 7.4965e-06  max_mem: 21589M
[01/17 23:40:42] d2.utils.events INFO:  eta: 12:08:46  iter: 10979  total_loss: 42.17  loss_ce: 0.3834  loss_mask: 0.4549  loss_dice: 3.343  loss_ce_0: 0.605  loss_mask_0: 0.435  loss_dice_0: 3.469  loss_ce_1: 0.4087  loss_mask_1: 0.4566  loss_dice_1: 3.378  loss_ce_2: 0.4097  loss_mask_2: 0.4548  loss_dice_2: 3.351  loss_ce_3: 0.3856  loss_mask_3: 0.4527  loss_dice_3: 3.337  loss_ce_4: 0.3846  loss_mask_4: 0.4534  loss_dice_4: 3.342  loss_ce_5: 0.3909  loss_mask_5: 0.4538  loss_dice_5: 3.333  loss_ce_6: 0.3799  loss_mask_6: 0.4546  loss_dice_6: 3.341  loss_ce_7: 0.3996  loss_mask_7: 0.4544  loss_dice_7: 3.339  loss_ce_8: 0.3894  loss_mask_8: 0.4534  loss_dice_8: 3.344  time: 1.5014  data_time: 0.0778  lr: 7.4918e-06  max_mem: 21589M
[01/17 23:41:12] d2.utils.events INFO:  eta: 12:08:08  iter: 10999  total_loss: 42.93  loss_ce: 0.3883  loss_mask: 0.4491  loss_dice: 3.395  loss_ce_0: 0.605  loss_mask_0: 0.4335  loss_dice_0: 3.516  loss_ce_1: 0.3848  loss_mask_1: 0.4513  loss_dice_1: 3.435  loss_ce_2: 0.4066  loss_mask_2: 0.4483  loss_dice_2: 3.412  loss_ce_3: 0.3972  loss_mask_3: 0.45  loss_dice_3: 3.398  loss_ce_4: 0.3947  loss_mask_4: 0.4507  loss_dice_4: 3.4  loss_ce_5: 0.381  loss_mask_5: 0.4502  loss_dice_5: 3.394  loss_ce_6: 0.3726  loss_mask_6: 0.4494  loss_dice_6: 3.392  loss_ce_7: 0.3744  loss_mask_7: 0.4493  loss_dice_7: 3.39  loss_ce_8: 0.3746  loss_mask_8: 0.4495  loss_dice_8: 3.4  time: 1.5014  data_time: 0.0786  lr: 7.4872e-06  max_mem: 21589M
[01/17 23:41:42] d2.utils.events INFO:  eta: 12:07:50  iter: 11019  total_loss: 42.22  loss_ce: 0.3722  loss_mask: 0.4657  loss_dice: 3.331  loss_ce_0: 0.5803  loss_mask_0: 0.4477  loss_dice_0: 3.452  loss_ce_1: 0.3745  loss_mask_1: 0.4653  loss_dice_1: 3.365  loss_ce_2: 0.3825  loss_mask_2: 0.4641  loss_dice_2: 3.339  loss_ce_3: 0.3706  loss_mask_3: 0.4626  loss_dice_3: 3.332  loss_ce_4: 0.3578  loss_mask_4: 0.4632  loss_dice_4: 3.329  loss_ce_5: 0.3616  loss_mask_5: 0.4606  loss_dice_5: 3.331  loss_ce_6: 0.3543  loss_mask_6: 0.4595  loss_dice_6: 3.327  loss_ce_7: 0.358  loss_mask_7: 0.4636  loss_dice_7: 3.324  loss_ce_8: 0.3567  loss_mask_8: 0.4655  loss_dice_8: 3.331  time: 1.5014  data_time: 0.0661  lr: 7.4825e-06  max_mem: 21589M
[01/17 23:42:12] d2.utils.events INFO:  eta: 12:06:56  iter: 11039  total_loss: 42.24  loss_ce: 0.3835  loss_mask: 0.456  loss_dice: 3.353  loss_ce_0: 0.6455  loss_mask_0: 0.4412  loss_dice_0: 3.478  loss_ce_1: 0.3857  loss_mask_1: 0.4546  loss_dice_1: 3.391  loss_ce_2: 0.3959  loss_mask_2: 0.455  loss_dice_2: 3.361  loss_ce_3: 0.3874  loss_mask_3: 0.4534  loss_dice_3: 3.355  loss_ce_4: 0.3992  loss_mask_4: 0.454  loss_dice_4: 3.348  loss_ce_5: 0.3726  loss_mask_5: 0.4565  loss_dice_5: 3.35  loss_ce_6: 0.3765  loss_mask_6: 0.4558  loss_dice_6: 3.355  loss_ce_7: 0.3895  loss_mask_7: 0.4568  loss_dice_7: 3.348  loss_ce_8: 0.3802  loss_mask_8: 0.4564  loss_dice_8: 3.356  time: 1.5014  data_time: 0.0703  lr: 7.4779e-06  max_mem: 21589M
[01/17 23:42:43] d2.utils.events INFO:  eta: 12:06:05  iter: 11059  total_loss: 42.5  loss_ce: 0.3462  loss_mask: 0.4628  loss_dice: 3.386  loss_ce_0: 0.5897  loss_mask_0: 0.4484  loss_dice_0: 3.508  loss_ce_1: 0.352  loss_mask_1: 0.4672  loss_dice_1: 3.423  loss_ce_2: 0.3508  loss_mask_2: 0.4638  loss_dice_2: 3.396  loss_ce_3: 0.3459  loss_mask_3: 0.4629  loss_dice_3: 3.39  loss_ce_4: 0.3546  loss_mask_4: 0.4615  loss_dice_4: 3.385  loss_ce_5: 0.3407  loss_mask_5: 0.4624  loss_dice_5: 3.384  loss_ce_6: 0.3358  loss_mask_6: 0.4623  loss_dice_6: 3.386  loss_ce_7: 0.3484  loss_mask_7: 0.4646  loss_dice_7: 3.382  loss_ce_8: 0.3475  loss_mask_8: 0.4637  loss_dice_8: 3.39  time: 1.5014  data_time: 0.0718  lr: 7.4732e-06  max_mem: 21589M
[01/17 23:43:15] d2.utils.events INFO:  eta: 12:06:07  iter: 11079  total_loss: 42.17  loss_ce: 0.3776  loss_mask: 0.466  loss_dice: 3.322  loss_ce_0: 0.6224  loss_mask_0: 0.4478  loss_dice_0: 3.464  loss_ce_1: 0.3729  loss_mask_1: 0.4679  loss_dice_1: 3.365  loss_ce_2: 0.3723  loss_mask_2: 0.4662  loss_dice_2: 3.344  loss_ce_3: 0.3706  loss_mask_3: 0.466  loss_dice_3: 3.33  loss_ce_4: 0.3592  loss_mask_4: 0.4642  loss_dice_4: 3.329  loss_ce_5: 0.3733  loss_mask_5: 0.4659  loss_dice_5: 3.335  loss_ce_6: 0.369  loss_mask_6: 0.4698  loss_dice_6: 3.326  loss_ce_7: 0.3614  loss_mask_7: 0.4654  loss_dice_7: 3.336  loss_ce_8: 0.3728  loss_mask_8: 0.4648  loss_dice_8: 3.331  time: 1.5017  data_time: 0.1014  lr: 7.4686e-06  max_mem: 21589M
[01/17 23:43:49] d2.utils.events INFO:  eta: 12:06:46  iter: 11099  total_loss: 42.55  loss_ce: 0.4147  loss_mask: 0.4658  loss_dice: 3.307  loss_ce_0: 0.6063  loss_mask_0: 0.4475  loss_dice_0: 3.463  loss_ce_1: 0.3862  loss_mask_1: 0.4685  loss_dice_1: 3.362  loss_ce_2: 0.4106  loss_mask_2: 0.4668  loss_dice_2: 3.33  loss_ce_3: 0.393  loss_mask_3: 0.4656  loss_dice_3: 3.315  loss_ce_4: 0.3923  loss_mask_4: 0.4646  loss_dice_4: 3.31  loss_ce_5: 0.4026  loss_mask_5: 0.4634  loss_dice_5: 3.302  loss_ce_6: 0.3895  loss_mask_6: 0.4646  loss_dice_6: 3.312  loss_ce_7: 0.4061  loss_mask_7: 0.4647  loss_dice_7: 3.308  loss_ce_8: 0.4188  loss_mask_8: 0.4667  loss_dice_8: 3.306  time: 1.5020  data_time: 0.1136  lr: 7.4639e-06  max_mem: 21589M
[01/17 23:44:20] d2.utils.events INFO:  eta: 12:06:15  iter: 11119  total_loss: 42.49  loss_ce: 0.3831  loss_mask: 0.4479  loss_dice: 3.377  loss_ce_0: 0.5786  loss_mask_0: 0.4337  loss_dice_0: 3.508  loss_ce_1: 0.3889  loss_mask_1: 0.4514  loss_dice_1: 3.414  loss_ce_2: 0.4151  loss_mask_2: 0.4459  loss_dice_2: 3.376  loss_ce_3: 0.4014  loss_mask_3: 0.4447  loss_dice_3: 3.381  loss_ce_4: 0.4003  loss_mask_4: 0.4491  loss_dice_4: 3.366  loss_ce_5: 0.3784  loss_mask_5: 0.4498  loss_dice_5: 3.37  loss_ce_6: 0.3799  loss_mask_6: 0.4488  loss_dice_6: 3.374  loss_ce_7: 0.3642  loss_mask_7: 0.4512  loss_dice_7: 3.385  loss_ce_8: 0.3709  loss_mask_8: 0.4485  loss_dice_8: 3.387  time: 1.5020  data_time: 0.0739  lr: 7.4593e-06  max_mem: 21589M
[01/17 23:44:53] d2.utils.events INFO:  eta: 12:07:19  iter: 11139  total_loss: 42.78  loss_ce: 0.3701  loss_mask: 0.4563  loss_dice: 3.422  loss_ce_0: 0.6322  loss_mask_0: 0.4431  loss_dice_0: 3.535  loss_ce_1: 0.3776  loss_mask_1: 0.4623  loss_dice_1: 3.461  loss_ce_2: 0.3825  loss_mask_2: 0.458  loss_dice_2: 3.433  loss_ce_3: 0.3859  loss_mask_3: 0.459  loss_dice_3: 3.415  loss_ce_4: 0.3754  loss_mask_4: 0.4573  loss_dice_4: 3.421  loss_ce_5: 0.3758  loss_mask_5: 0.4537  loss_dice_5: 3.428  loss_ce_6: 0.3816  loss_mask_6: 0.4546  loss_dice_6: 3.418  loss_ce_7: 0.3562  loss_mask_7: 0.4553  loss_dice_7: 3.423  loss_ce_8: 0.3671  loss_mask_8: 0.4563  loss_dice_8: 3.426  time: 1.5024  data_time: 0.0838  lr: 7.4546e-06  max_mem: 21589M
[01/17 23:45:26] d2.utils.events INFO:  eta: 12:08:23  iter: 11159  total_loss: 42.26  loss_ce: 0.3615  loss_mask: 0.4539  loss_dice: 3.321  loss_ce_0: 0.6204  loss_mask_0: 0.4347  loss_dice_0: 3.456  loss_ce_1: 0.3896  loss_mask_1: 0.4509  loss_dice_1: 3.371  loss_ce_2: 0.3973  loss_mask_2: 0.4488  loss_dice_2: 3.344  loss_ce_3: 0.3752  loss_mask_3: 0.4507  loss_dice_3: 3.331  loss_ce_4: 0.3752  loss_mask_4: 0.4511  loss_dice_4: 3.327  loss_ce_5: 0.3827  loss_mask_5: 0.4525  loss_dice_5: 3.331  loss_ce_6: 0.3691  loss_mask_6: 0.4532  loss_dice_6: 3.329  loss_ce_7: 0.3721  loss_mask_7: 0.4531  loss_dice_7: 3.327  loss_ce_8: 0.364  loss_mask_8: 0.4526  loss_dice_8: 3.33  time: 1.5026  data_time: 0.0889  lr: 7.45e-06  max_mem: 21589M
[01/17 23:45:57] d2.utils.events INFO:  eta: 12:07:44  iter: 11179  total_loss: 41.77  loss_ce: 0.3488  loss_mask: 0.4522  loss_dice: 3.32  loss_ce_0: 0.5953  loss_mask_0: 0.4317  loss_dice_0: 3.444  loss_ce_1: 0.3704  loss_mask_1: 0.451  loss_dice_1: 3.337  loss_ce_2: 0.382  loss_mask_2: 0.4522  loss_dice_2: 3.327  loss_ce_3: 0.3712  loss_mask_3: 0.4524  loss_dice_3: 3.318  loss_ce_4: 0.3629  loss_mask_4: 0.4509  loss_dice_4: 3.323  loss_ce_5: 0.3648  loss_mask_5: 0.4521  loss_dice_5: 3.318  loss_ce_6: 0.3459  loss_mask_6: 0.453  loss_dice_6: 3.317  loss_ce_7: 0.3486  loss_mask_7: 0.4539  loss_dice_7: 3.316  loss_ce_8: 0.3536  loss_mask_8: 0.4539  loss_dice_8: 3.323  time: 1.5027  data_time: 0.0721  lr: 7.4453e-06  max_mem: 21589M
[01/17 23:46:29] d2.utils.events INFO:  eta: 12:07:39  iter: 11199  total_loss: 42.1  loss_ce: 0.3484  loss_mask: 0.4577  loss_dice: 3.342  loss_ce_0: 0.5707  loss_mask_0: 0.4447  loss_dice_0: 3.478  loss_ce_1: 0.3706  loss_mask_1: 0.4627  loss_dice_1: 3.386  loss_ce_2: 0.3638  loss_mask_2: 0.4596  loss_dice_2: 3.368  loss_ce_3: 0.3562  loss_mask_3: 0.4594  loss_dice_3: 3.352  loss_ce_4: 0.3427  loss_mask_4: 0.4564  loss_dice_4: 3.358  loss_ce_5: 0.3457  loss_mask_5: 0.4585  loss_dice_5: 3.35  loss_ce_6: 0.3376  loss_mask_6: 0.459  loss_dice_6: 3.354  loss_ce_7: 0.3304  loss_mask_7: 0.4583  loss_dice_7: 3.346  loss_ce_8: 0.3305  loss_mask_8: 0.4562  loss_dice_8: 3.349  time: 1.5029  data_time: 0.0815  lr: 7.4407e-06  max_mem: 21589M
[01/17 23:47:00] d2.utils.events INFO:  eta: 12:07:09  iter: 11219  total_loss: 42.53  loss_ce: 0.3356  loss_mask: 0.4515  loss_dice: 3.342  loss_ce_0: 0.6126  loss_mask_0: 0.4333  loss_dice_0: 3.475  loss_ce_1: 0.3588  loss_mask_1: 0.4466  loss_dice_1: 3.392  loss_ce_2: 0.376  loss_mask_2: 0.4483  loss_dice_2: 3.354  loss_ce_3: 0.3513  loss_mask_3: 0.4481  loss_dice_3: 3.356  loss_ce_4: 0.338  loss_mask_4: 0.4464  loss_dice_4: 3.353  loss_ce_5: 0.3401  loss_mask_5: 0.4478  loss_dice_5: 3.343  loss_ce_6: 0.3434  loss_mask_6: 0.4491  loss_dice_6: 3.338  loss_ce_7: 0.3326  loss_mask_7: 0.4497  loss_dice_7: 3.343  loss_ce_8: 0.3522  loss_mask_8: 0.4477  loss_dice_8: 3.349  time: 1.5030  data_time: 0.0717  lr: 7.436e-06  max_mem: 21589M
[01/17 23:47:30] d2.utils.events INFO:  eta: 12:05:21  iter: 11239  total_loss: 41.25  loss_ce: 0.3493  loss_mask: 0.4491  loss_dice: 3.249  loss_ce_0: 0.6106  loss_mask_0: 0.4362  loss_dice_0: 3.408  loss_ce_1: 0.358  loss_mask_1: 0.4517  loss_dice_1: 3.305  loss_ce_2: 0.3663  loss_mask_2: 0.4507  loss_dice_2: 3.271  loss_ce_3: 0.3464  loss_mask_3: 0.453  loss_dice_3: 3.263  loss_ce_4: 0.3435  loss_mask_4: 0.451  loss_dice_4: 3.264  loss_ce_5: 0.3475  loss_mask_5: 0.4497  loss_dice_5: 3.26  loss_ce_6: 0.3503  loss_mask_6: 0.4525  loss_dice_6: 3.258  loss_ce_7: 0.3516  loss_mask_7: 0.4532  loss_dice_7: 3.252  loss_ce_8: 0.3309  loss_mask_8: 0.4505  loss_dice_8: 3.259  time: 1.5029  data_time: 0.0688  lr: 7.4314e-06  max_mem: 21589M
[01/17 23:48:00] d2.utils.events INFO:  eta: 12:04:44  iter: 11259  total_loss: 41.94  loss_ce: 0.366  loss_mask: 0.469  loss_dice: 3.337  loss_ce_0: 0.5995  loss_mask_0: 0.4447  loss_dice_0: 3.479  loss_ce_1: 0.3853  loss_mask_1: 0.4694  loss_dice_1: 3.373  loss_ce_2: 0.3813  loss_mask_2: 0.4659  loss_dice_2: 3.359  loss_ce_3: 0.3795  loss_mask_3: 0.4669  loss_dice_3: 3.342  loss_ce_4: 0.3493  loss_mask_4: 0.4689  loss_dice_4: 3.352  loss_ce_5: 0.3652  loss_mask_5: 0.467  loss_dice_5: 3.345  loss_ce_6: 0.3783  loss_mask_6: 0.4686  loss_dice_6: 3.34  loss_ce_7: 0.3553  loss_mask_7: 0.4682  loss_dice_7: 3.342  loss_ce_8: 0.3815  loss_mask_8: 0.468  loss_dice_8: 3.337  time: 1.5029  data_time: 0.0678  lr: 7.4267e-06  max_mem: 21589M
[01/17 23:48:30] d2.utils.events INFO:  eta: 12:04:04  iter: 11279  total_loss: 41.26  loss_ce: 0.3159  loss_mask: 0.4555  loss_dice: 3.304  loss_ce_0: 0.6013  loss_mask_0: 0.4392  loss_dice_0: 3.436  loss_ce_1: 0.3413  loss_mask_1: 0.4582  loss_dice_1: 3.339  loss_ce_2: 0.3392  loss_mask_2: 0.4558  loss_dice_2: 3.316  loss_ce_3: 0.3222  loss_mask_3: 0.4536  loss_dice_3: 3.301  loss_ce_4: 0.321  loss_mask_4: 0.4557  loss_dice_4: 3.309  loss_ce_5: 0.3305  loss_mask_5: 0.4555  loss_dice_5: 3.301  loss_ce_6: 0.3186  loss_mask_6: 0.4559  loss_dice_6: 3.302  loss_ce_7: 0.3207  loss_mask_7: 0.4554  loss_dice_7: 3.305  loss_ce_8: 0.3232  loss_mask_8: 0.4558  loss_dice_8: 3.297  time: 1.5029  data_time: 0.0779  lr: 7.4221e-06  max_mem: 21589M
[01/17 23:49:01] d2.utils.events INFO:  eta: 12:03:39  iter: 11299  total_loss: 41.2  loss_ce: 0.3899  loss_mask: 0.4478  loss_dice: 3.267  loss_ce_0: 0.6199  loss_mask_0: 0.4283  loss_dice_0: 3.416  loss_ce_1: 0.3768  loss_mask_1: 0.4439  loss_dice_1: 3.331  loss_ce_2: 0.383  loss_mask_2: 0.4457  loss_dice_2: 3.298  loss_ce_3: 0.3805  loss_mask_3: 0.4486  loss_dice_3: 3.277  loss_ce_4: 0.3798  loss_mask_4: 0.4477  loss_dice_4: 3.287  loss_ce_5: 0.3727  loss_mask_5: 0.4482  loss_dice_5: 3.278  loss_ce_6: 0.38  loss_mask_6: 0.4496  loss_dice_6: 3.274  loss_ce_7: 0.3749  loss_mask_7: 0.4467  loss_dice_7: 3.273  loss_ce_8: 0.378  loss_mask_8: 0.4485  loss_dice_8: 3.272  time: 1.5030  data_time: 0.0725  lr: 7.4174e-06  max_mem: 21589M
[01/17 23:49:31] d2.utils.events INFO:  eta: 12:03:02  iter: 11319  total_loss: 41.67  loss_ce: 0.3576  loss_mask: 0.4382  loss_dice: 3.339  loss_ce_0: 0.6189  loss_mask_0: 0.4238  loss_dice_0: 3.492  loss_ce_1: 0.3646  loss_mask_1: 0.4381  loss_dice_1: 3.384  loss_ce_2: 0.3806  loss_mask_2: 0.4396  loss_dice_2: 3.365  loss_ce_3: 0.3587  loss_mask_3: 0.4392  loss_dice_3: 3.345  loss_ce_4: 0.3513  loss_mask_4: 0.4403  loss_dice_4: 3.338  loss_ce_5: 0.3383  loss_mask_5: 0.4387  loss_dice_5: 3.35  loss_ce_6: 0.3495  loss_mask_6: 0.4392  loss_dice_6: 3.347  loss_ce_7: 0.3312  loss_mask_7: 0.439  loss_dice_7: 3.344  loss_ce_8: 0.3433  loss_mask_8: 0.4368  loss_dice_8: 3.343  time: 1.5030  data_time: 0.0714  lr: 7.4128e-06  max_mem: 21589M
[01/17 23:50:01] d2.utils.events INFO:  eta: 12:02:17  iter: 11339  total_loss: 41.53  loss_ce: 0.3625  loss_mask: 0.4479  loss_dice: 3.339  loss_ce_0: 0.5906  loss_mask_0: 0.4316  loss_dice_0: 3.471  loss_ce_1: 0.3501  loss_mask_1: 0.4511  loss_dice_1: 3.378  loss_ce_2: 0.3772  loss_mask_2: 0.4451  loss_dice_2: 3.358  loss_ce_3: 0.3545  loss_mask_3: 0.4479  loss_dice_3: 3.348  loss_ce_4: 0.3512  loss_mask_4: 0.446  loss_dice_4: 3.344  loss_ce_5: 0.345  loss_mask_5: 0.4461  loss_dice_5: 3.343  loss_ce_6: 0.3345  loss_mask_6: 0.4497  loss_dice_6: 3.345  loss_ce_7: 0.3279  loss_mask_7: 0.4503  loss_dice_7: 3.341  loss_ce_8: 0.3396  loss_mask_8: 0.4494  loss_dice_8: 3.343  time: 1.5030  data_time: 0.0779  lr: 7.4081e-06  max_mem: 21589M
[01/17 23:50:31] d2.utils.events INFO:  eta: 12:01:36  iter: 11359  total_loss: 42.13  loss_ce: 0.3626  loss_mask: 0.4554  loss_dice: 3.351  loss_ce_0: 0.6177  loss_mask_0: 0.4423  loss_dice_0: 3.485  loss_ce_1: 0.3897  loss_mask_1: 0.4576  loss_dice_1: 3.394  loss_ce_2: 0.4041  loss_mask_2: 0.4548  loss_dice_2: 3.37  loss_ce_3: 0.3807  loss_mask_3: 0.4546  loss_dice_3: 3.362  loss_ce_4: 0.3684  loss_mask_4: 0.4522  loss_dice_4: 3.356  loss_ce_5: 0.3659  loss_mask_5: 0.4513  loss_dice_5: 3.355  loss_ce_6: 0.3633  loss_mask_6: 0.4531  loss_dice_6: 3.355  loss_ce_7: 0.3627  loss_mask_7: 0.452  loss_dice_7: 3.353  loss_ce_8: 0.3617  loss_mask_8: 0.4537  loss_dice_8: 3.353  time: 1.5030  data_time: 0.0752  lr: 7.4035e-06  max_mem: 21589M
[01/17 23:51:02] d2.utils.events INFO:  eta: 12:00:59  iter: 11379  total_loss: 41.61  loss_ce: 0.3787  loss_mask: 0.4393  loss_dice: 3.307  loss_ce_0: 0.6009  loss_mask_0: 0.4275  loss_dice_0: 3.455  loss_ce_1: 0.3753  loss_mask_1: 0.4393  loss_dice_1: 3.356  loss_ce_2: 0.3803  loss_mask_2: 0.4383  loss_dice_2: 3.322  loss_ce_3: 0.3611  loss_mask_3: 0.4397  loss_dice_3: 3.316  loss_ce_4: 0.3733  loss_mask_4: 0.4393  loss_dice_4: 3.317  loss_ce_5: 0.356  loss_mask_5: 0.44  loss_dice_5: 3.315  loss_ce_6: 0.3738  loss_mask_6: 0.4409  loss_dice_6: 3.313  loss_ce_7: 0.3485  loss_mask_7: 0.4404  loss_dice_7: 3.313  loss_ce_8: 0.3518  loss_mask_8: 0.4394  loss_dice_8: 3.31  time: 1.5030  data_time: 0.0750  lr: 7.3988e-06  max_mem: 21589M
[01/17 23:51:32] d2.utils.events INFO:  eta: 12:00:19  iter: 11399  total_loss: 41.38  loss_ce: 0.3309  loss_mask: 0.4416  loss_dice: 3.309  loss_ce_0: 0.592  loss_mask_0: 0.4221  loss_dice_0: 3.445  loss_ce_1: 0.3659  loss_mask_1: 0.4422  loss_dice_1: 3.348  loss_ce_2: 0.3675  loss_mask_2: 0.4421  loss_dice_2: 3.326  loss_ce_3: 0.3598  loss_mask_3: 0.4403  loss_dice_3: 3.32  loss_ce_4: 0.3421  loss_mask_4: 0.4406  loss_dice_4: 3.314  loss_ce_5: 0.3566  loss_mask_5: 0.4414  loss_dice_5: 3.316  loss_ce_6: 0.3531  loss_mask_6: 0.4416  loss_dice_6: 3.309  loss_ce_7: 0.3282  loss_mask_7: 0.4399  loss_dice_7: 3.314  loss_ce_8: 0.3493  loss_mask_8: 0.4408  loss_dice_8: 3.303  time: 1.5030  data_time: 0.0700  lr: 7.3942e-06  max_mem: 21589M
[01/17 23:52:02] d2.utils.events INFO:  eta: 12:00:05  iter: 11419  total_loss: 41.57  loss_ce: 0.3572  loss_mask: 0.4438  loss_dice: 3.313  loss_ce_0: 0.6214  loss_mask_0: 0.4258  loss_dice_0: 3.444  loss_ce_1: 0.3757  loss_mask_1: 0.443  loss_dice_1: 3.365  loss_ce_2: 0.3806  loss_mask_2: 0.4424  loss_dice_2: 3.333  loss_ce_3: 0.3746  loss_mask_3: 0.4411  loss_dice_3: 3.315  loss_ce_4: 0.3544  loss_mask_4: 0.4427  loss_dice_4: 3.315  loss_ce_5: 0.3535  loss_mask_5: 0.443  loss_dice_5: 3.32  loss_ce_6: 0.3656  loss_mask_6: 0.4419  loss_dice_6: 3.308  loss_ce_7: 0.3602  loss_mask_7: 0.4435  loss_dice_7: 3.32  loss_ce_8: 0.3583  loss_mask_8: 0.4434  loss_dice_8: 3.322  time: 1.5030  data_time: 0.0755  lr: 7.3895e-06  max_mem: 21589M
[01/17 23:52:32] d2.utils.events INFO:  eta: 11:59:28  iter: 11439  total_loss: 42.24  loss_ce: 0.3685  loss_mask: 0.4507  loss_dice: 3.294  loss_ce_0: 0.6204  loss_mask_0: 0.438  loss_dice_0: 3.441  loss_ce_1: 0.3848  loss_mask_1: 0.4537  loss_dice_1: 3.342  loss_ce_2: 0.3884  loss_mask_2: 0.4545  loss_dice_2: 3.315  loss_ce_3: 0.3676  loss_mask_3: 0.4548  loss_dice_3: 3.298  loss_ce_4: 0.3517  loss_mask_4: 0.4539  loss_dice_4: 3.299  loss_ce_5: 0.3573  loss_mask_5: 0.4546  loss_dice_5: 3.299  loss_ce_6: 0.3835  loss_mask_6: 0.4521  loss_dice_6: 3.294  loss_ce_7: 0.3638  loss_mask_7: 0.4497  loss_dice_7: 3.3  loss_ce_8: 0.3569  loss_mask_8: 0.4538  loss_dice_8: 3.295  time: 1.5030  data_time: 0.0817  lr: 7.3849e-06  max_mem: 21589M
[01/17 23:53:05] d2.utils.events INFO:  eta: 11:59:30  iter: 11459  total_loss: 41.15  loss_ce: 0.3492  loss_mask: 0.46  loss_dice: 3.239  loss_ce_0: 0.6088  loss_mask_0: 0.4442  loss_dice_0: 3.37  loss_ce_1: 0.3627  loss_mask_1: 0.4622  loss_dice_1: 3.278  loss_ce_2: 0.3694  loss_mask_2: 0.4591  loss_dice_2: 3.239  loss_ce_3: 0.3484  loss_mask_3: 0.46  loss_dice_3: 3.237  loss_ce_4: 0.3431  loss_mask_4: 0.4582  loss_dice_4: 3.236  loss_ce_5: 0.337  loss_mask_5: 0.4566  loss_dice_5: 3.224  loss_ce_6: 0.3372  loss_mask_6: 0.4589  loss_dice_6: 3.239  loss_ce_7: 0.3438  loss_mask_7: 0.4586  loss_dice_7: 3.23  loss_ce_8: 0.3337  loss_mask_8: 0.4585  loss_dice_8: 3.233  time: 1.5032  data_time: 0.1051  lr: 7.3802e-06  max_mem: 21589M
[01/17 23:53:39] d2.utils.events INFO:  eta: 12:00:24  iter: 11479  total_loss: 41.43  loss_ce: 0.3365  loss_mask: 0.4503  loss_dice: 3.278  loss_ce_0: 0.5838  loss_mask_0: 0.4337  loss_dice_0: 3.424  loss_ce_1: 0.3618  loss_mask_1: 0.45  loss_dice_1: 3.322  loss_ce_2: 0.3685  loss_mask_2: 0.4516  loss_dice_2: 3.293  loss_ce_3: 0.3441  loss_mask_3: 0.4493  loss_dice_3: 3.286  loss_ce_4: 0.3723  loss_mask_4: 0.4503  loss_dice_4: 3.28  loss_ce_5: 0.3404  loss_mask_5: 0.4485  loss_dice_5: 3.271  loss_ce_6: 0.3328  loss_mask_6: 0.4483  loss_dice_6: 3.28  loss_ce_7: 0.3235  loss_mask_7: 0.4507  loss_dice_7: 3.288  loss_ce_8: 0.3302  loss_mask_8: 0.4503  loss_dice_8: 3.284  time: 1.5036  data_time: 0.1176  lr: 7.3755e-06  max_mem: 21589M
[01/17 23:54:11] d2.utils.events INFO:  eta: 12:00:28  iter: 11499  total_loss: 41.07  loss_ce: 0.3403  loss_mask: 0.4409  loss_dice: 3.222  loss_ce_0: 0.5887  loss_mask_0: 0.4213  loss_dice_0: 3.358  loss_ce_1: 0.3893  loss_mask_1: 0.4419  loss_dice_1: 3.266  loss_ce_2: 0.4022  loss_mask_2: 0.4385  loss_dice_2: 3.231  loss_ce_3: 0.3696  loss_mask_3: 0.4413  loss_dice_3: 3.22  loss_ce_4: 0.3695  loss_mask_4: 0.4406  loss_dice_4: 3.219  loss_ce_5: 0.3508  loss_mask_5: 0.4402  loss_dice_5: 3.217  loss_ce_6: 0.3474  loss_mask_6: 0.4389  loss_dice_6: 3.215  loss_ce_7: 0.3506  loss_mask_7: 0.4413  loss_dice_7: 3.216  loss_ce_8: 0.3568  loss_mask_8: 0.44  loss_dice_8: 3.22  time: 1.5038  data_time: 0.0990  lr: 7.3709e-06  max_mem: 21589M
[01/17 23:54:42] d2.utils.events INFO:  eta: 11:59:39  iter: 11519  total_loss: 41.61  loss_ce: 0.365  loss_mask: 0.4553  loss_dice: 3.287  loss_ce_0: 0.6253  loss_mask_0: 0.4382  loss_dice_0: 3.413  loss_ce_1: 0.361  loss_mask_1: 0.4556  loss_dice_1: 3.32  loss_ce_2: 0.3803  loss_mask_2: 0.4544  loss_dice_2: 3.298  loss_ce_3: 0.3471  loss_mask_3: 0.4539  loss_dice_3: 3.289  loss_ce_4: 0.3486  loss_mask_4: 0.4551  loss_dice_4: 3.293  loss_ce_5: 0.3448  loss_mask_5: 0.4589  loss_dice_5: 3.3  loss_ce_6: 0.3549  loss_mask_6: 0.4569  loss_dice_6: 3.291  loss_ce_7: 0.3512  loss_mask_7: 0.4548  loss_dice_7: 3.303  loss_ce_8: 0.364  loss_mask_8: 0.4548  loss_dice_8: 3.292  time: 1.5038  data_time: 0.0825  lr: 7.3662e-06  max_mem: 21589M
[01/17 23:55:11] d2.utils.events INFO:  eta: 11:58:57  iter: 11539  total_loss: 41.76  loss_ce: 0.3653  loss_mask: 0.451  loss_dice: 3.319  loss_ce_0: 0.6101  loss_mask_0: 0.4339  loss_dice_0: 3.473  loss_ce_1: 0.3725  loss_mask_1: 0.4521  loss_dice_1: 3.361  loss_ce_2: 0.3858  loss_mask_2: 0.4495  loss_dice_2: 3.335  loss_ce_3: 0.3813  loss_mask_3: 0.4498  loss_dice_3: 3.333  loss_ce_4: 0.3779  loss_mask_4: 0.4478  loss_dice_4: 3.318  loss_ce_5: 0.375  loss_mask_5: 0.4481  loss_dice_5: 3.326  loss_ce_6: 0.3644  loss_mask_6: 0.4473  loss_dice_6: 3.319  loss_ce_7: 0.3711  loss_mask_7: 0.4483  loss_dice_7: 3.316  loss_ce_8: 0.3628  loss_mask_8: 0.449  loss_dice_8: 3.312  time: 1.5037  data_time: 0.0752  lr: 7.3616e-06  max_mem: 21589M
[01/17 23:55:41] d2.utils.events INFO:  eta: 11:58:00  iter: 11559  total_loss: 41.8  loss_ce: 0.3971  loss_mask: 0.4391  loss_dice: 3.3  loss_ce_0: 0.607  loss_mask_0: 0.4261  loss_dice_0: 3.442  loss_ce_1: 0.3854  loss_mask_1: 0.4436  loss_dice_1: 3.337  loss_ce_2: 0.4066  loss_mask_2: 0.4405  loss_dice_2: 3.322  loss_ce_3: 0.4136  loss_mask_3: 0.4366  loss_dice_3: 3.306  loss_ce_4: 0.3938  loss_mask_4: 0.4378  loss_dice_4: 3.302  loss_ce_5: 0.3897  loss_mask_5: 0.4395  loss_dice_5: 3.31  loss_ce_6: 0.3947  loss_mask_6: 0.44  loss_dice_6: 3.296  loss_ce_7: 0.3819  loss_mask_7: 0.438  loss_dice_7: 3.3  loss_ce_8: 0.386  loss_mask_8: 0.4402  loss_dice_8: 3.307  time: 1.5037  data_time: 0.0717  lr: 7.3569e-06  max_mem: 21589M
[01/17 23:56:11] d2.utils.events INFO:  eta: 11:56:30  iter: 11579  total_loss: 41.36  loss_ce: 0.3379  loss_mask: 0.4411  loss_dice: 3.281  loss_ce_0: 0.6089  loss_mask_0: 0.4314  loss_dice_0: 3.42  loss_ce_1: 0.361  loss_mask_1: 0.4412  loss_dice_1: 3.323  loss_ce_2: 0.3618  loss_mask_2: 0.4412  loss_dice_2: 3.309  loss_ce_3: 0.3522  loss_mask_3: 0.4387  loss_dice_3: 3.294  loss_ce_4: 0.341  loss_mask_4: 0.4395  loss_dice_4: 3.295  loss_ce_5: 0.3351  loss_mask_5: 0.4413  loss_dice_5: 3.29  loss_ce_6: 0.3415  loss_mask_6: 0.4435  loss_dice_6: 3.293  loss_ce_7: 0.3364  loss_mask_7: 0.4428  loss_dice_7: 3.293  loss_ce_8: 0.3362  loss_mask_8: 0.4407  loss_dice_8: 3.284  time: 1.5037  data_time: 0.0712  lr: 7.3523e-06  max_mem: 21589M
[01/17 23:56:42] d2.utils.events INFO:  eta: 11:56:05  iter: 11599  total_loss: 41.53  loss_ce: 0.3444  loss_mask: 0.44  loss_dice: 3.317  loss_ce_0: 0.5876  loss_mask_0: 0.4275  loss_dice_0: 3.456  loss_ce_1: 0.3784  loss_mask_1: 0.4449  loss_dice_1: 3.353  loss_ce_2: 0.3772  loss_mask_2: 0.4414  loss_dice_2: 3.327  loss_ce_3: 0.3609  loss_mask_3: 0.4411  loss_dice_3: 3.314  loss_ce_4: 0.3685  loss_mask_4: 0.4411  loss_dice_4: 3.313  loss_ce_5: 0.3551  loss_mask_5: 0.4409  loss_dice_5: 3.319  loss_ce_6: 0.3486  loss_mask_6: 0.4387  loss_dice_6: 3.318  loss_ce_7: 0.344  loss_mask_7: 0.4391  loss_dice_7: 3.316  loss_ce_8: 0.3556  loss_mask_8: 0.4409  loss_dice_8: 3.315  time: 1.5038  data_time: 0.0839  lr: 7.3476e-06  max_mem: 21589M
[01/17 23:57:13] d2.utils.events INFO:  eta: 11:55:07  iter: 11619  total_loss: 41.2  loss_ce: 0.3648  loss_mask: 0.4567  loss_dice: 3.296  loss_ce_0: 0.579  loss_mask_0: 0.4362  loss_dice_0: 3.429  loss_ce_1: 0.3736  loss_mask_1: 0.4609  loss_dice_1: 3.336  loss_ce_2: 0.3775  loss_mask_2: 0.4583  loss_dice_2: 3.318  loss_ce_3: 0.3682  loss_mask_3: 0.4549  loss_dice_3: 3.308  loss_ce_4: 0.3723  loss_mask_4: 0.4535  loss_dice_4: 3.306  loss_ce_5: 0.3613  loss_mask_5: 0.4551  loss_dice_5: 3.301  loss_ce_6: 0.3712  loss_mask_6: 0.4564  loss_dice_6: 3.296  loss_ce_7: 0.3754  loss_mask_7: 0.4537  loss_dice_7: 3.295  loss_ce_8: 0.366  loss_mask_8: 0.4556  loss_dice_8: 3.299  time: 1.5038  data_time: 0.0718  lr: 7.343e-06  max_mem: 21589M
[01/17 23:57:43] d2.utils.events INFO:  eta: 11:54:15  iter: 11639  total_loss: 41.54  loss_ce: 0.3693  loss_mask: 0.4532  loss_dice: 3.305  loss_ce_0: 0.6129  loss_mask_0: 0.4364  loss_dice_0: 3.438  loss_ce_1: 0.383  loss_mask_1: 0.459  loss_dice_1: 3.34  loss_ce_2: 0.3952  loss_mask_2: 0.4558  loss_dice_2: 3.323  loss_ce_3: 0.3882  loss_mask_3: 0.4545  loss_dice_3: 3.3  loss_ce_4: 0.3777  loss_mask_4: 0.4526  loss_dice_4: 3.303  loss_ce_5: 0.3809  loss_mask_5: 0.4517  loss_dice_5: 3.307  loss_ce_6: 0.3647  loss_mask_6: 0.4545  loss_dice_6: 3.3  loss_ce_7: 0.3777  loss_mask_7: 0.453  loss_dice_7: 3.305  loss_ce_8: 0.3835  loss_mask_8: 0.4541  loss_dice_8: 3.299  time: 1.5038  data_time: 0.0786  lr: 7.3383e-06  max_mem: 21589M
[01/17 23:58:13] d2.utils.events INFO:  eta: 11:53:47  iter: 11659  total_loss: 41.87  loss_ce: 0.379  loss_mask: 0.4454  loss_dice: 3.328  loss_ce_0: 0.6245  loss_mask_0: 0.4296  loss_dice_0: 3.459  loss_ce_1: 0.4041  loss_mask_1: 0.4491  loss_dice_1: 3.365  loss_ce_2: 0.385  loss_mask_2: 0.4468  loss_dice_2: 3.333  loss_ce_3: 0.3961  loss_mask_3: 0.448  loss_dice_3: 3.321  loss_ce_4: 0.3752  loss_mask_4: 0.4465  loss_dice_4: 3.32  loss_ce_5: 0.3729  loss_mask_5: 0.4449  loss_dice_5: 3.325  loss_ce_6: 0.3698  loss_mask_6: 0.4444  loss_dice_6: 3.318  loss_ce_7: 0.3738  loss_mask_7: 0.443  loss_dice_7: 3.324  loss_ce_8: 0.3742  loss_mask_8: 0.4446  loss_dice_8: 3.318  time: 1.5038  data_time: 0.0844  lr: 7.3336e-06  max_mem: 21589M
[01/17 23:58:43] d2.utils.events INFO:  eta: 11:53:10  iter: 11679  total_loss: 41.81  loss_ce: 0.3685  loss_mask: 0.4492  loss_dice: 3.314  loss_ce_0: 0.6002  loss_mask_0: 0.4385  loss_dice_0: 3.449  loss_ce_1: 0.3789  loss_mask_1: 0.4611  loss_dice_1: 3.35  loss_ce_2: 0.3765  loss_mask_2: 0.4563  loss_dice_2: 3.324  loss_ce_3: 0.3743  loss_mask_3: 0.4534  loss_dice_3: 3.311  loss_ce_4: 0.3625  loss_mask_4: 0.4504  loss_dice_4: 3.306  loss_ce_5: 0.358  loss_mask_5: 0.45  loss_dice_5: 3.317  loss_ce_6: 0.3619  loss_mask_6: 0.451  loss_dice_6: 3.306  loss_ce_7: 0.3644  loss_mask_7: 0.4491  loss_dice_7: 3.312  loss_ce_8: 0.3511  loss_mask_8: 0.4511  loss_dice_8: 3.311  time: 1.5038  data_time: 0.0894  lr: 7.329e-06  max_mem: 21589M
[01/17 23:59:13] d2.utils.events INFO:  eta: 11:52:55  iter: 11699  total_loss: 41.55  loss_ce: 0.3636  loss_mask: 0.4552  loss_dice: 3.271  loss_ce_0: 0.6005  loss_mask_0: 0.4387  loss_dice_0: 3.415  loss_ce_1: 0.3772  loss_mask_1: 0.4613  loss_dice_1: 3.314  loss_ce_2: 0.3823  loss_mask_2: 0.4574  loss_dice_2: 3.292  loss_ce_3: 0.3795  loss_mask_3: 0.4553  loss_dice_3: 3.281  loss_ce_4: 0.3742  loss_mask_4: 0.4552  loss_dice_4: 3.272  loss_ce_5: 0.3617  loss_mask_5: 0.456  loss_dice_5: 3.28  loss_ce_6: 0.3702  loss_mask_6: 0.4552  loss_dice_6: 3.275  loss_ce_7: 0.3611  loss_mask_7: 0.4565  loss_dice_7: 3.284  loss_ce_8: 0.3648  loss_mask_8: 0.4551  loss_dice_8: 3.272  time: 1.5038  data_time: 0.0648  lr: 7.3243e-06  max_mem: 21589M
[01/17 23:59:44] d2.utils.events INFO:  eta: 11:52:52  iter: 11719  total_loss: 41.43  loss_ce: 0.3657  loss_mask: 0.4463  loss_dice: 3.265  loss_ce_0: 0.5882  loss_mask_0: 0.4303  loss_dice_0: 3.409  loss_ce_1: 0.3742  loss_mask_1: 0.4482  loss_dice_1: 3.32  loss_ce_2: 0.3753  loss_mask_2: 0.4505  loss_dice_2: 3.287  loss_ce_3: 0.3672  loss_mask_3: 0.4476  loss_dice_3: 3.273  loss_ce_4: 0.3618  loss_mask_4: 0.45  loss_dice_4: 3.279  loss_ce_5: 0.3615  loss_mask_5: 0.4478  loss_dice_5: 3.271  loss_ce_6: 0.3568  loss_mask_6: 0.4472  loss_dice_6: 3.27  loss_ce_7: 0.3496  loss_mask_7: 0.4454  loss_dice_7: 3.271  loss_ce_8: 0.3779  loss_mask_8: 0.447  loss_dice_8: 3.278  time: 1.5039  data_time: 0.0907  lr: 7.3197e-06  max_mem: 21589M
[01/18 00:00:15] d2.utils.events INFO:  eta: 11:53:04  iter: 11739  total_loss: 41.55  loss_ce: 0.3473  loss_mask: 0.4346  loss_dice: 3.299  loss_ce_0: 0.5998  loss_mask_0: 0.4224  loss_dice_0: 3.433  loss_ce_1: 0.37  loss_mask_1: 0.435  loss_dice_1: 3.333  loss_ce_2: 0.3662  loss_mask_2: 0.4327  loss_dice_2: 3.301  loss_ce_3: 0.3681  loss_mask_3: 0.4324  loss_dice_3: 3.299  loss_ce_4: 0.3571  loss_mask_4: 0.4329  loss_dice_4: 3.292  loss_ce_5: 0.3507  loss_mask_5: 0.4346  loss_dice_5: 3.294  loss_ce_6: 0.3469  loss_mask_6: 0.4374  loss_dice_6: 3.29  loss_ce_7: 0.3611  loss_mask_7: 0.4363  loss_dice_7: 3.294  loss_ce_8: 0.3553  loss_mask_8: 0.4367  loss_dice_8: 3.292  time: 1.5040  data_time: 0.0825  lr: 7.315e-06  max_mem: 21589M
[01/18 00:00:46] d2.utils.events INFO:  eta: 11:53:23  iter: 11759  total_loss: 41.27  loss_ce: 0.3492  loss_mask: 0.4479  loss_dice: 3.28  loss_ce_0: 0.6033  loss_mask_0: 0.4351  loss_dice_0: 3.418  loss_ce_1: 0.3592  loss_mask_1: 0.4506  loss_dice_1: 3.32  loss_ce_2: 0.3791  loss_mask_2: 0.4499  loss_dice_2: 3.298  loss_ce_3: 0.3436  loss_mask_3: 0.4511  loss_dice_3: 3.28  loss_ce_4: 0.3399  loss_mask_4: 0.4484  loss_dice_4: 3.275  loss_ce_5: 0.3531  loss_mask_5: 0.4481  loss_dice_5: 3.286  loss_ce_6: 0.3516  loss_mask_6: 0.4454  loss_dice_6: 3.281  loss_ce_7: 0.347  loss_mask_7: 0.4458  loss_dice_7: 3.277  loss_ce_8: 0.3459  loss_mask_8: 0.4468  loss_dice_8: 3.279  time: 1.5040  data_time: 0.0824  lr: 7.3103e-06  max_mem: 21589M
[01/18 00:01:17] d2.utils.events INFO:  eta: 11:52:21  iter: 11779  total_loss: 40.94  loss_ce: 0.3543  loss_mask: 0.4501  loss_dice: 3.259  loss_ce_0: 0.5968  loss_mask_0: 0.431  loss_dice_0: 3.4  loss_ce_1: 0.3749  loss_mask_1: 0.448  loss_dice_1: 3.306  loss_ce_2: 0.3573  loss_mask_2: 0.4487  loss_dice_2: 3.281  loss_ce_3: 0.3424  loss_mask_3: 0.4472  loss_dice_3: 3.278  loss_ce_4: 0.3355  loss_mask_4: 0.4468  loss_dice_4: 3.273  loss_ce_5: 0.3397  loss_mask_5: 0.4474  loss_dice_5: 3.273  loss_ce_6: 0.3448  loss_mask_6: 0.4475  loss_dice_6: 3.265  loss_ce_7: 0.3214  loss_mask_7: 0.4485  loss_dice_7: 3.258  loss_ce_8: 0.3507  loss_mask_8: 0.4481  loss_dice_8: 3.273  time: 1.5041  data_time: 0.0945  lr: 7.3057e-06  max_mem: 21589M
[01/18 00:01:48] d2.utils.events INFO:  eta: 11:51:56  iter: 11799  total_loss: 41.89  loss_ce: 0.3373  loss_mask: 0.4334  loss_dice: 3.328  loss_ce_0: 0.5631  loss_mask_0: 0.4144  loss_dice_0: 3.47  loss_ce_1: 0.3524  loss_mask_1: 0.4354  loss_dice_1: 3.367  loss_ce_2: 0.3591  loss_mask_2: 0.4318  loss_dice_2: 3.343  loss_ce_3: 0.351  loss_mask_3: 0.4325  loss_dice_3: 3.32  loss_ce_4: 0.356  loss_mask_4: 0.4339  loss_dice_4: 3.321  loss_ce_5: 0.3346  loss_mask_5: 0.435  loss_dice_5: 3.324  loss_ce_6: 0.3402  loss_mask_6: 0.4342  loss_dice_6: 3.323  loss_ce_7: 0.3444  loss_mask_7: 0.4336  loss_dice_7: 3.325  loss_ce_8: 0.3316  loss_mask_8: 0.433  loss_dice_8: 3.331  time: 1.5042  data_time: 0.0875  lr: 7.301e-06  max_mem: 21589M
[01/18 00:02:18] d2.utils.events INFO:  eta: 11:51:40  iter: 11819  total_loss: 41.3  loss_ce: 0.3745  loss_mask: 0.4504  loss_dice: 3.247  loss_ce_0: 0.6116  loss_mask_0: 0.4378  loss_dice_0: 3.385  loss_ce_1: 0.3862  loss_mask_1: 0.4568  loss_dice_1: 3.304  loss_ce_2: 0.3808  loss_mask_2: 0.4534  loss_dice_2: 3.272  loss_ce_3: 0.3538  loss_mask_3: 0.4499  loss_dice_3: 3.26  loss_ce_4: 0.3663  loss_mask_4: 0.448  loss_dice_4: 3.242  loss_ce_5: 0.3461  loss_mask_5: 0.4466  loss_dice_5: 3.261  loss_ce_6: 0.3583  loss_mask_6: 0.4503  loss_dice_6: 3.257  loss_ce_7: 0.3632  loss_mask_7: 0.4488  loss_dice_7: 3.251  loss_ce_8: 0.3532  loss_mask_8: 0.4501  loss_dice_8: 3.247  time: 1.5042  data_time: 0.0701  lr: 7.2964e-06  max_mem: 21589M
[01/18 00:02:49] d2.utils.events INFO:  eta: 11:51:32  iter: 11839  total_loss: 41.3  loss_ce: 0.3603  loss_mask: 0.4456  loss_dice: 3.287  loss_ce_0: 0.6195  loss_mask_0: 0.4341  loss_dice_0: 3.413  loss_ce_1: 0.3734  loss_mask_1: 0.4495  loss_dice_1: 3.334  loss_ce_2: 0.3924  loss_mask_2: 0.4482  loss_dice_2: 3.286  loss_ce_3: 0.3808  loss_mask_3: 0.4456  loss_dice_3: 3.281  loss_ce_4: 0.3819  loss_mask_4: 0.4456  loss_dice_4: 3.285  loss_ce_5: 0.3518  loss_mask_5: 0.4475  loss_dice_5: 3.289  loss_ce_6: 0.3509  loss_mask_6: 0.4473  loss_dice_6: 3.283  loss_ce_7: 0.3647  loss_mask_7: 0.4489  loss_dice_7: 3.279  loss_ce_8: 0.3717  loss_mask_8: 0.4483  loss_dice_8: 3.286  time: 1.5042  data_time: 0.0696  lr: 7.2917e-06  max_mem: 21589M
[01/18 00:03:20] d2.utils.events INFO:  eta: 11:51:44  iter: 11859  total_loss: 41.74  loss_ce: 0.3488  loss_mask: 0.4487  loss_dice: 3.328  loss_ce_0: 0.587  loss_mask_0: 0.4329  loss_dice_0: 3.468  loss_ce_1: 0.3553  loss_mask_1: 0.4559  loss_dice_1: 3.354  loss_ce_2: 0.3674  loss_mask_2: 0.4517  loss_dice_2: 3.337  loss_ce_3: 0.3434  loss_mask_3: 0.4508  loss_dice_3: 3.331  loss_ce_4: 0.3406  loss_mask_4: 0.447  loss_dice_4: 3.33  loss_ce_5: 0.3443  loss_mask_5: 0.4452  loss_dice_5: 3.335  loss_ce_6: 0.3377  loss_mask_6: 0.4474  loss_dice_6: 3.326  loss_ce_7: 0.3141  loss_mask_7: 0.4472  loss_dice_7: 3.337  loss_ce_8: 0.3329  loss_mask_8: 0.4482  loss_dice_8: 3.335  time: 1.5043  data_time: 0.0821  lr: 7.287e-06  max_mem: 21589M
[01/18 00:03:51] d2.utils.events INFO:  eta: 11:51:26  iter: 11879  total_loss: 41.97  loss_ce: 0.3749  loss_mask: 0.4488  loss_dice: 3.297  loss_ce_0: 0.6299  loss_mask_0: 0.4455  loss_dice_0: 3.438  loss_ce_1: 0.3876  loss_mask_1: 0.459  loss_dice_1: 3.33  loss_ce_2: 0.4096  loss_mask_2: 0.4536  loss_dice_2: 3.309  loss_ce_3: 0.3996  loss_mask_3: 0.454  loss_dice_3: 3.299  loss_ce_4: 0.3873  loss_mask_4: 0.455  loss_dice_4: 3.298  loss_ce_5: 0.3815  loss_mask_5: 0.4548  loss_dice_5: 3.291  loss_ce_6: 0.3779  loss_mask_6: 0.4524  loss_dice_6: 3.286  loss_ce_7: 0.3519  loss_mask_7: 0.4534  loss_dice_7: 3.297  loss_ce_8: 0.3694  loss_mask_8: 0.4523  loss_dice_8: 3.303  time: 1.5043  data_time: 0.0805  lr: 7.2824e-06  max_mem: 21589M
[01/18 00:04:21] d2.utils.events INFO:  eta: 11:50:55  iter: 11899  total_loss: 41.7  loss_ce: 0.3593  loss_mask: 0.4409  loss_dice: 3.314  loss_ce_0: 0.6079  loss_mask_0: 0.4316  loss_dice_0: 3.449  loss_ce_1: 0.3825  loss_mask_1: 0.4443  loss_dice_1: 3.356  loss_ce_2: 0.3916  loss_mask_2: 0.4439  loss_dice_2: 3.327  loss_ce_3: 0.3559  loss_mask_3: 0.4433  loss_dice_3: 3.317  loss_ce_4: 0.3778  loss_mask_4: 0.4425  loss_dice_4: 3.322  loss_ce_5: 0.3689  loss_mask_5: 0.4401  loss_dice_5: 3.321  loss_ce_6: 0.3564  loss_mask_6: 0.439  loss_dice_6: 3.313  loss_ce_7: 0.3668  loss_mask_7: 0.4394  loss_dice_7: 3.319  loss_ce_8: 0.3686  loss_mask_8: 0.4403  loss_dice_8: 3.317  time: 1.5044  data_time: 0.0765  lr: 7.2777e-06  max_mem: 21589M
[01/18 00:04:52] d2.utils.events INFO:  eta: 11:50:33  iter: 11919  total_loss: 41.02  loss_ce: 0.3802  loss_mask: 0.4645  loss_dice: 3.223  loss_ce_0: 0.612  loss_mask_0: 0.4443  loss_dice_0: 3.359  loss_ce_1: 0.3882  loss_mask_1: 0.4625  loss_dice_1: 3.257  loss_ce_2: 0.409  loss_mask_2: 0.4609  loss_dice_2: 3.24  loss_ce_3: 0.3844  loss_mask_3: 0.463  loss_dice_3: 3.226  loss_ce_4: 0.3737  loss_mask_4: 0.4638  loss_dice_4: 3.227  loss_ce_5: 0.3777  loss_mask_5: 0.4644  loss_dice_5: 3.236  loss_ce_6: 0.3553  loss_mask_6: 0.4633  loss_dice_6: 3.225  loss_ce_7: 0.3598  loss_mask_7: 0.4634  loss_dice_7: 3.227  loss_ce_8: 0.3703  loss_mask_8: 0.4644  loss_dice_8: 3.227  time: 1.5044  data_time: 0.0812  lr: 7.2731e-06  max_mem: 21589M
[01/18 00:05:23] d2.utils.events INFO:  eta: 11:49:47  iter: 11939  total_loss: 41.04  loss_ce: 0.3324  loss_mask: 0.4508  loss_dice: 3.255  loss_ce_0: 0.5968  loss_mask_0: 0.4449  loss_dice_0: 3.403  loss_ce_1: 0.3509  loss_mask_1: 0.4569  loss_dice_1: 3.306  loss_ce_2: 0.3514  loss_mask_2: 0.4534  loss_dice_2: 3.264  loss_ce_3: 0.3357  loss_mask_3: 0.4529  loss_dice_3: 3.259  loss_ce_4: 0.3403  loss_mask_4: 0.4531  loss_dice_4: 3.248  loss_ce_5: 0.324  loss_mask_5: 0.4531  loss_dice_5: 3.263  loss_ce_6: 0.3441  loss_mask_6: 0.4531  loss_dice_6: 3.251  loss_ce_7: 0.3186  loss_mask_7: 0.4523  loss_dice_7: 3.259  loss_ce_8: 0.3269  loss_mask_8: 0.4516  loss_dice_8: 3.263  time: 1.5045  data_time: 0.0850  lr: 7.2684e-06  max_mem: 21589M
[01/18 00:05:53] d2.utils.events INFO:  eta: 11:49:14  iter: 11959  total_loss: 40.92  loss_ce: 0.3544  loss_mask: 0.4435  loss_dice: 3.278  loss_ce_0: 0.5939  loss_mask_0: 0.4255  loss_dice_0: 3.404  loss_ce_1: 0.3746  loss_mask_1: 0.4457  loss_dice_1: 3.312  loss_ce_2: 0.3768  loss_mask_2: 0.4451  loss_dice_2: 3.303  loss_ce_3: 0.3531  loss_mask_3: 0.4419  loss_dice_3: 3.279  loss_ce_4: 0.3361  loss_mask_4: 0.4396  loss_dice_4: 3.288  loss_ce_5: 0.3409  loss_mask_5: 0.4396  loss_dice_5: 3.283  loss_ce_6: 0.3427  loss_mask_6: 0.4409  loss_dice_6: 3.283  loss_ce_7: 0.3547  loss_mask_7: 0.4406  loss_dice_7: 3.282  loss_ce_8: 0.346  loss_mask_8: 0.442  loss_dice_8: 3.279  time: 1.5045  data_time: 0.0873  lr: 7.2637e-06  max_mem: 21589M
[01/18 00:06:24] d2.utils.events INFO:  eta: 11:49:01  iter: 11979  total_loss: 40.86  loss_ce: 0.3471  loss_mask: 0.4499  loss_dice: 3.228  loss_ce_0: 0.5659  loss_mask_0: 0.434  loss_dice_0: 3.37  loss_ce_1: 0.3431  loss_mask_1: 0.4537  loss_dice_1: 3.269  loss_ce_2: 0.3502  loss_mask_2: 0.4486  loss_dice_2: 3.247  loss_ce_3: 0.3433  loss_mask_3: 0.4487  loss_dice_3: 3.241  loss_ce_4: 0.3402  loss_mask_4: 0.4489  loss_dice_4: 3.232  loss_ce_5: 0.3424  loss_mask_5: 0.4485  loss_dice_5: 3.231  loss_ce_6: 0.3389  loss_mask_6: 0.4509  loss_dice_6: 3.232  loss_ce_7: 0.3388  loss_mask_7: 0.449  loss_dice_7: 3.234  loss_ce_8: 0.3476  loss_mask_8: 0.4511  loss_dice_8: 3.241  time: 1.5046  data_time: 0.0780  lr: 7.2591e-06  max_mem: 21589M
[01/18 00:06:55] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 00:06:55] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 00:06:55] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 00:06:56] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 00:07:10] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0100 s/iter. Inference: 0.1736 s/iter. Eval: 0.1736 s/iter. Total: 0.3572 s/iter. ETA=0:06:26
[01/18 00:07:15] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0104 s/iter. Inference: 0.1693 s/iter. Eval: 0.2052 s/iter. Total: 0.3850 s/iter. ETA=0:06:51
[01/18 00:07:20] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0100 s/iter. Inference: 0.1730 s/iter. Eval: 0.1984 s/iter. Total: 0.3815 s/iter. ETA=0:06:42
[01/18 00:07:25] d2.evaluation.evaluator INFO: Inference done 53/1093. Dataloading: 0.0104 s/iter. Inference: 0.1642 s/iter. Eval: 0.1929 s/iter. Total: 0.3676 s/iter. ETA=0:06:22
[01/18 00:07:30] d2.evaluation.evaluator INFO: Inference done 66/1093. Dataloading: 0.0109 s/iter. Inference: 0.1657 s/iter. Eval: 0.1953 s/iter. Total: 0.3720 s/iter. ETA=0:06:22
[01/18 00:07:35] d2.evaluation.evaluator INFO: Inference done 81/1093. Dataloading: 0.0110 s/iter. Inference: 0.1635 s/iter. Eval: 0.1922 s/iter. Total: 0.3667 s/iter. ETA=0:06:11
[01/18 00:07:40] d2.evaluation.evaluator INFO: Inference done 93/1093. Dataloading: 0.0115 s/iter. Inference: 0.1650 s/iter. Eval: 0.1972 s/iter. Total: 0.3738 s/iter. ETA=0:06:13
[01/18 00:07:46] d2.evaluation.evaluator INFO: Inference done 107/1093. Dataloading: 0.0114 s/iter. Inference: 0.1655 s/iter. Eval: 0.1952 s/iter. Total: 0.3722 s/iter. ETA=0:06:07
[01/18 00:07:51] d2.evaluation.evaluator INFO: Inference done 121/1093. Dataloading: 0.0114 s/iter. Inference: 0.1675 s/iter. Eval: 0.1954 s/iter. Total: 0.3746 s/iter. ETA=0:06:04
[01/18 00:07:56] d2.evaluation.evaluator INFO: Inference done 136/1093. Dataloading: 0.0112 s/iter. Inference: 0.1660 s/iter. Eval: 0.1930 s/iter. Total: 0.3704 s/iter. ETA=0:05:54
[01/18 00:08:01] d2.evaluation.evaluator INFO: Inference done 151/1093. Dataloading: 0.0110 s/iter. Inference: 0.1682 s/iter. Eval: 0.1885 s/iter. Total: 0.3679 s/iter. ETA=0:05:46
[01/18 00:08:07] d2.evaluation.evaluator INFO: Inference done 166/1093. Dataloading: 0.0111 s/iter. Inference: 0.1672 s/iter. Eval: 0.1880 s/iter. Total: 0.3665 s/iter. ETA=0:05:39
[01/18 00:08:12] d2.evaluation.evaluator INFO: Inference done 180/1093. Dataloading: 0.0110 s/iter. Inference: 0.1676 s/iter. Eval: 0.1889 s/iter. Total: 0.3676 s/iter. ETA=0:05:35
[01/18 00:08:17] d2.evaluation.evaluator INFO: Inference done 195/1093. Dataloading: 0.0110 s/iter. Inference: 0.1673 s/iter. Eval: 0.1876 s/iter. Total: 0.3661 s/iter. ETA=0:05:28
[01/18 00:08:22] d2.evaluation.evaluator INFO: Inference done 208/1093. Dataloading: 0.0111 s/iter. Inference: 0.1677 s/iter. Eval: 0.1889 s/iter. Total: 0.3678 s/iter. ETA=0:05:25
[01/18 00:08:27] d2.evaluation.evaluator INFO: Inference done 220/1093. Dataloading: 0.0112 s/iter. Inference: 0.1694 s/iter. Eval: 0.1905 s/iter. Total: 0.3712 s/iter. ETA=0:05:24
[01/18 00:08:33] d2.evaluation.evaluator INFO: Inference done 228/1093. Dataloading: 0.0112 s/iter. Inference: 0.1757 s/iter. Eval: 0.1940 s/iter. Total: 0.3811 s/iter. ETA=0:05:29
[01/18 00:08:38] d2.evaluation.evaluator INFO: Inference done 236/1093. Dataloading: 0.0113 s/iter. Inference: 0.1810 s/iter. Eval: 0.1988 s/iter. Total: 0.3912 s/iter. ETA=0:05:35
[01/18 00:08:43] d2.evaluation.evaluator INFO: Inference done 243/1093. Dataloading: 0.0118 s/iter. Inference: 0.1869 s/iter. Eval: 0.2020 s/iter. Total: 0.4009 s/iter. ETA=0:05:40
[01/18 00:08:48] d2.evaluation.evaluator INFO: Inference done 252/1093. Dataloading: 0.0122 s/iter. Inference: 0.1920 s/iter. Eval: 0.2036 s/iter. Total: 0.4079 s/iter. ETA=0:05:43
[01/18 00:08:54] d2.evaluation.evaluator INFO: Inference done 260/1093. Dataloading: 0.0123 s/iter. Inference: 0.1978 s/iter. Eval: 0.2055 s/iter. Total: 0.4157 s/iter. ETA=0:05:46
[01/18 00:08:59] d2.evaluation.evaluator INFO: Inference done 267/1093. Dataloading: 0.0125 s/iter. Inference: 0.2041 s/iter. Eval: 0.2081 s/iter. Total: 0.4249 s/iter. ETA=0:05:50
[01/18 00:09:04] d2.evaluation.evaluator INFO: Inference done 281/1093. Dataloading: 0.0125 s/iter. Inference: 0.2023 s/iter. Eval: 0.2077 s/iter. Total: 0.4227 s/iter. ETA=0:05:43
[01/18 00:09:09] d2.evaluation.evaluator INFO: Inference done 296/1093. Dataloading: 0.0123 s/iter. Inference: 0.2005 s/iter. Eval: 0.2055 s/iter. Total: 0.4185 s/iter. ETA=0:05:33
[01/18 00:09:14] d2.evaluation.evaluator INFO: Inference done 308/1093. Dataloading: 0.0124 s/iter. Inference: 0.1994 s/iter. Eval: 0.2066 s/iter. Total: 0.4186 s/iter. ETA=0:05:28
[01/18 00:09:20] d2.evaluation.evaluator INFO: Inference done 323/1093. Dataloading: 0.0123 s/iter. Inference: 0.1971 s/iter. Eval: 0.2059 s/iter. Total: 0.4155 s/iter. ETA=0:05:19
[01/18 00:09:25] d2.evaluation.evaluator INFO: Inference done 336/1093. Dataloading: 0.0123 s/iter. Inference: 0.1976 s/iter. Eval: 0.2048 s/iter. Total: 0.4149 s/iter. ETA=0:05:14
[01/18 00:09:30] d2.evaluation.evaluator INFO: Inference done 353/1093. Dataloading: 0.0122 s/iter. Inference: 0.1954 s/iter. Eval: 0.2020 s/iter. Total: 0.4097 s/iter. ETA=0:05:03
[01/18 00:09:35] d2.evaluation.evaluator INFO: Inference done 366/1093. Dataloading: 0.0122 s/iter. Inference: 0.1947 s/iter. Eval: 0.2025 s/iter. Total: 0.4096 s/iter. ETA=0:04:57
[01/18 00:09:40] d2.evaluation.evaluator INFO: Inference done 376/1093. Dataloading: 0.0124 s/iter. Inference: 0.1967 s/iter. Eval: 0.2028 s/iter. Total: 0.4120 s/iter. ETA=0:04:55
[01/18 00:09:46] d2.evaluation.evaluator INFO: Inference done 391/1093. Dataloading: 0.0123 s/iter. Inference: 0.1948 s/iter. Eval: 0.2024 s/iter. Total: 0.4096 s/iter. ETA=0:04:47
[01/18 00:09:51] d2.evaluation.evaluator INFO: Inference done 404/1093. Dataloading: 0.0123 s/iter. Inference: 0.1947 s/iter. Eval: 0.2019 s/iter. Total: 0.4090 s/iter. ETA=0:04:41
[01/18 00:09:56] d2.evaluation.evaluator INFO: Inference done 418/1093. Dataloading: 0.0124 s/iter. Inference: 0.1947 s/iter. Eval: 0.2014 s/iter. Total: 0.4087 s/iter. ETA=0:04:35
[01/18 00:10:01] d2.evaluation.evaluator INFO: Inference done 429/1093. Dataloading: 0.0124 s/iter. Inference: 0.1955 s/iter. Eval: 0.2019 s/iter. Total: 0.4099 s/iter. ETA=0:04:32
[01/18 00:10:07] d2.evaluation.evaluator INFO: Inference done 445/1093. Dataloading: 0.0123 s/iter. Inference: 0.1947 s/iter. Eval: 0.1997 s/iter. Total: 0.4068 s/iter. ETA=0:04:23
[01/18 00:10:12] d2.evaluation.evaluator INFO: Inference done 460/1093. Dataloading: 0.0122 s/iter. Inference: 0.1937 s/iter. Eval: 0.1989 s/iter. Total: 0.4049 s/iter. ETA=0:04:16
[01/18 00:10:17] d2.evaluation.evaluator INFO: Inference done 477/1093. Dataloading: 0.0121 s/iter. Inference: 0.1919 s/iter. Eval: 0.1974 s/iter. Total: 0.4016 s/iter. ETA=0:04:07
[01/18 00:10:22] d2.evaluation.evaluator INFO: Inference done 492/1093. Dataloading: 0.0120 s/iter. Inference: 0.1909 s/iter. Eval: 0.1967 s/iter. Total: 0.3997 s/iter. ETA=0:04:00
[01/18 00:10:27] d2.evaluation.evaluator INFO: Inference done 508/1093. Dataloading: 0.0120 s/iter. Inference: 0.1902 s/iter. Eval: 0.1948 s/iter. Total: 0.3972 s/iter. ETA=0:03:52
[01/18 00:10:32] d2.evaluation.evaluator INFO: Inference done 523/1093. Dataloading: 0.0119 s/iter. Inference: 0.1891 s/iter. Eval: 0.1943 s/iter. Total: 0.3955 s/iter. ETA=0:03:45
[01/18 00:10:38] d2.evaluation.evaluator INFO: Inference done 537/1093. Dataloading: 0.0119 s/iter. Inference: 0.1883 s/iter. Eval: 0.1944 s/iter. Total: 0.3947 s/iter. ETA=0:03:39
[01/18 00:10:43] d2.evaluation.evaluator INFO: Inference done 547/1093. Dataloading: 0.0119 s/iter. Inference: 0.1892 s/iter. Eval: 0.1955 s/iter. Total: 0.3968 s/iter. ETA=0:03:36
[01/18 00:10:48] d2.evaluation.evaluator INFO: Inference done 556/1093. Dataloading: 0.0120 s/iter. Inference: 0.1912 s/iter. Eval: 0.1966 s/iter. Total: 0.4000 s/iter. ETA=0:03:34
[01/18 00:10:53] d2.evaluation.evaluator INFO: Inference done 565/1093. Dataloading: 0.0121 s/iter. Inference: 0.1935 s/iter. Eval: 0.1972 s/iter. Total: 0.4030 s/iter. ETA=0:03:32
[01/18 00:10:58] d2.evaluation.evaluator INFO: Inference done 576/1093. Dataloading: 0.0123 s/iter. Inference: 0.1951 s/iter. Eval: 0.1966 s/iter. Total: 0.4042 s/iter. ETA=0:03:28
[01/18 00:11:04] d2.evaluation.evaluator INFO: Inference done 588/1093. Dataloading: 0.0124 s/iter. Inference: 0.1964 s/iter. Eval: 0.1962 s/iter. Total: 0.4051 s/iter. ETA=0:03:24
[01/18 00:11:09] d2.evaluation.evaluator INFO: Inference done 602/1093. Dataloading: 0.0124 s/iter. Inference: 0.1959 s/iter. Eval: 0.1963 s/iter. Total: 0.4047 s/iter. ETA=0:03:18
[01/18 00:11:14] d2.evaluation.evaluator INFO: Inference done 616/1093. Dataloading: 0.0124 s/iter. Inference: 0.1952 s/iter. Eval: 0.1962 s/iter. Total: 0.4041 s/iter. ETA=0:03:12
[01/18 00:11:20] d2.evaluation.evaluator INFO: Inference done 631/1093. Dataloading: 0.0124 s/iter. Inference: 0.1941 s/iter. Eval: 0.1959 s/iter. Total: 0.4026 s/iter. ETA=0:03:05
[01/18 00:11:25] d2.evaluation.evaluator INFO: Inference done 647/1093. Dataloading: 0.0123 s/iter. Inference: 0.1935 s/iter. Eval: 0.1947 s/iter. Total: 0.4007 s/iter. ETA=0:02:58
[01/18 00:11:30] d2.evaluation.evaluator INFO: Inference done 661/1093. Dataloading: 0.0123 s/iter. Inference: 0.1928 s/iter. Eval: 0.1947 s/iter. Total: 0.4000 s/iter. ETA=0:02:52
[01/18 00:11:35] d2.evaluation.evaluator INFO: Inference done 673/1093. Dataloading: 0.0122 s/iter. Inference: 0.1932 s/iter. Eval: 0.1949 s/iter. Total: 0.4005 s/iter. ETA=0:02:48
[01/18 00:11:40] d2.evaluation.evaluator INFO: Inference done 684/1093. Dataloading: 0.0122 s/iter. Inference: 0.1946 s/iter. Eval: 0.1945 s/iter. Total: 0.4016 s/iter. ETA=0:02:44
[01/18 00:11:45] d2.evaluation.evaluator INFO: Inference done 693/1093. Dataloading: 0.0123 s/iter. Inference: 0.1966 s/iter. Eval: 0.1947 s/iter. Total: 0.4038 s/iter. ETA=0:02:41
[01/18 00:11:50] d2.evaluation.evaluator INFO: Inference done 702/1093. Dataloading: 0.0124 s/iter. Inference: 0.1986 s/iter. Eval: 0.1946 s/iter. Total: 0.4058 s/iter. ETA=0:02:38
[01/18 00:11:56] d2.evaluation.evaluator INFO: Inference done 711/1093. Dataloading: 0.0125 s/iter. Inference: 0.1998 s/iter. Eval: 0.1957 s/iter. Total: 0.4081 s/iter. ETA=0:02:35
[01/18 00:12:01] d2.evaluation.evaluator INFO: Inference done 720/1093. Dataloading: 0.0126 s/iter. Inference: 0.2007 s/iter. Eval: 0.1966 s/iter. Total: 0.4101 s/iter. ETA=0:02:32
[01/18 00:12:06] d2.evaluation.evaluator INFO: Inference done 730/1093. Dataloading: 0.0127 s/iter. Inference: 0.2022 s/iter. Eval: 0.1967 s/iter. Total: 0.4117 s/iter. ETA=0:02:29
[01/18 00:12:11] d2.evaluation.evaluator INFO: Inference done 740/1093. Dataloading: 0.0128 s/iter. Inference: 0.2032 s/iter. Eval: 0.1970 s/iter. Total: 0.4132 s/iter. ETA=0:02:25
[01/18 00:12:16] d2.evaluation.evaluator INFO: Inference done 749/1093. Dataloading: 0.0128 s/iter. Inference: 0.2051 s/iter. Eval: 0.1969 s/iter. Total: 0.4150 s/iter. ETA=0:02:22
[01/18 00:12:22] d2.evaluation.evaluator INFO: Inference done 758/1093. Dataloading: 0.0128 s/iter. Inference: 0.2070 s/iter. Eval: 0.1973 s/iter. Total: 0.4173 s/iter. ETA=0:02:19
[01/18 00:12:27] d2.evaluation.evaluator INFO: Inference done 767/1093. Dataloading: 0.0128 s/iter. Inference: 0.2078 s/iter. Eval: 0.1984 s/iter. Total: 0.4192 s/iter. ETA=0:02:16
[01/18 00:12:32] d2.evaluation.evaluator INFO: Inference done 775/1093. Dataloading: 0.0129 s/iter. Inference: 0.2095 s/iter. Eval: 0.1990 s/iter. Total: 0.4216 s/iter. ETA=0:02:14
[01/18 00:12:38] d2.evaluation.evaluator INFO: Inference done 784/1093. Dataloading: 0.0129 s/iter. Inference: 0.2112 s/iter. Eval: 0.1993 s/iter. Total: 0.4236 s/iter. ETA=0:02:10
[01/18 00:12:43] d2.evaluation.evaluator INFO: Inference done 794/1093. Dataloading: 0.0129 s/iter. Inference: 0.2124 s/iter. Eval: 0.1994 s/iter. Total: 0.4249 s/iter. ETA=0:02:07
[01/18 00:12:48] d2.evaluation.evaluator INFO: Inference done 803/1093. Dataloading: 0.0130 s/iter. Inference: 0.2137 s/iter. Eval: 0.2000 s/iter. Total: 0.4269 s/iter. ETA=0:02:03
[01/18 00:12:54] d2.evaluation.evaluator INFO: Inference done 813/1093. Dataloading: 0.0131 s/iter. Inference: 0.2147 s/iter. Eval: 0.2003 s/iter. Total: 0.4282 s/iter. ETA=0:01:59
[01/18 00:12:59] d2.evaluation.evaluator INFO: Inference done 823/1093. Dataloading: 0.0130 s/iter. Inference: 0.2157 s/iter. Eval: 0.2007 s/iter. Total: 0.4296 s/iter. ETA=0:01:55
[01/18 00:13:04] d2.evaluation.evaluator INFO: Inference done 833/1093. Dataloading: 0.0130 s/iter. Inference: 0.2165 s/iter. Eval: 0.2009 s/iter. Total: 0.4306 s/iter. ETA=0:01:51
[01/18 00:13:09] d2.evaluation.evaluator INFO: Inference done 842/1093. Dataloading: 0.0130 s/iter. Inference: 0.2178 s/iter. Eval: 0.2010 s/iter. Total: 0.4320 s/iter. ETA=0:01:48
[01/18 00:13:14] d2.evaluation.evaluator INFO: Inference done 851/1093. Dataloading: 0.0130 s/iter. Inference: 0.2193 s/iter. Eval: 0.2012 s/iter. Total: 0.4337 s/iter. ETA=0:01:44
[01/18 00:13:20] d2.evaluation.evaluator INFO: Inference done 859/1093. Dataloading: 0.0132 s/iter. Inference: 0.2205 s/iter. Eval: 0.2024 s/iter. Total: 0.4362 s/iter. ETA=0:01:42
[01/18 00:13:25] d2.evaluation.evaluator INFO: Inference done 868/1093. Dataloading: 0.0132 s/iter. Inference: 0.2217 s/iter. Eval: 0.2027 s/iter. Total: 0.4379 s/iter. ETA=0:01:38
[01/18 00:13:31] d2.evaluation.evaluator INFO: Inference done 877/1093. Dataloading: 0.0132 s/iter. Inference: 0.2226 s/iter. Eval: 0.2033 s/iter. Total: 0.4393 s/iter. ETA=0:01:34
[01/18 00:13:36] d2.evaluation.evaluator INFO: Inference done 885/1093. Dataloading: 0.0134 s/iter. Inference: 0.2232 s/iter. Eval: 0.2044 s/iter. Total: 0.4412 s/iter. ETA=0:01:31
[01/18 00:13:41] d2.evaluation.evaluator INFO: Inference done 894/1093. Dataloading: 0.0134 s/iter. Inference: 0.2238 s/iter. Eval: 0.2053 s/iter. Total: 0.4427 s/iter. ETA=0:01:28
[01/18 00:13:47] d2.evaluation.evaluator INFO: Inference done 904/1093. Dataloading: 0.0135 s/iter. Inference: 0.2247 s/iter. Eval: 0.2054 s/iter. Total: 0.4437 s/iter. ETA=0:01:23
[01/18 00:13:52] d2.evaluation.evaluator INFO: Inference done 914/1093. Dataloading: 0.0135 s/iter. Inference: 0.2250 s/iter. Eval: 0.2060 s/iter. Total: 0.4446 s/iter. ETA=0:01:19
[01/18 00:13:57] d2.evaluation.evaluator INFO: Inference done 923/1093. Dataloading: 0.0135 s/iter. Inference: 0.2258 s/iter. Eval: 0.2066 s/iter. Total: 0.4461 s/iter. ETA=0:01:15
[01/18 00:14:02] d2.evaluation.evaluator INFO: Inference done 933/1093. Dataloading: 0.0135 s/iter. Inference: 0.2267 s/iter. Eval: 0.2067 s/iter. Total: 0.4471 s/iter. ETA=0:01:11
[01/18 00:14:07] d2.evaluation.evaluator INFO: Inference done 942/1093. Dataloading: 0.0136 s/iter. Inference: 0.2272 s/iter. Eval: 0.2072 s/iter. Total: 0.4481 s/iter. ETA=0:01:07
[01/18 00:14:13] d2.evaluation.evaluator INFO: Inference done 952/1093. Dataloading: 0.0136 s/iter. Inference: 0.2277 s/iter. Eval: 0.2076 s/iter. Total: 0.4492 s/iter. ETA=0:01:03
[01/18 00:14:18] d2.evaluation.evaluator INFO: Inference done 961/1093. Dataloading: 0.0137 s/iter. Inference: 0.2281 s/iter. Eval: 0.2082 s/iter. Total: 0.4502 s/iter. ETA=0:00:59
[01/18 00:14:23] d2.evaluation.evaluator INFO: Inference done 971/1093. Dataloading: 0.0137 s/iter. Inference: 0.2283 s/iter. Eval: 0.2086 s/iter. Total: 0.4507 s/iter. ETA=0:00:54
[01/18 00:14:28] d2.evaluation.evaluator INFO: Inference done 981/1093. Dataloading: 0.0137 s/iter. Inference: 0.2287 s/iter. Eval: 0.2088 s/iter. Total: 0.4514 s/iter. ETA=0:00:50
[01/18 00:14:34] d2.evaluation.evaluator INFO: Inference done 992/1093. Dataloading: 0.0137 s/iter. Inference: 0.2295 s/iter. Eval: 0.2086 s/iter. Total: 0.4520 s/iter. ETA=0:00:45
[01/18 00:14:39] d2.evaluation.evaluator INFO: Inference done 1002/1093. Dataloading: 0.0138 s/iter. Inference: 0.2301 s/iter. Eval: 0.2089 s/iter. Total: 0.4529 s/iter. ETA=0:00:41
[01/18 00:14:44] d2.evaluation.evaluator INFO: Inference done 1013/1093. Dataloading: 0.0137 s/iter. Inference: 0.2307 s/iter. Eval: 0.2086 s/iter. Total: 0.4532 s/iter. ETA=0:00:36
[01/18 00:14:49] d2.evaluation.evaluator INFO: Inference done 1023/1093. Dataloading: 0.0138 s/iter. Inference: 0.2309 s/iter. Eval: 0.2088 s/iter. Total: 0.4536 s/iter. ETA=0:00:31
[01/18 00:14:55] d2.evaluation.evaluator INFO: Inference done 1033/1093. Dataloading: 0.0138 s/iter. Inference: 0.2315 s/iter. Eval: 0.2090 s/iter. Total: 0.4546 s/iter. ETA=0:00:27
[01/18 00:15:00] d2.evaluation.evaluator INFO: Inference done 1042/1093. Dataloading: 0.0138 s/iter. Inference: 0.2323 s/iter. Eval: 0.2093 s/iter. Total: 0.4556 s/iter. ETA=0:00:23
[01/18 00:15:05] d2.evaluation.evaluator INFO: Inference done 1053/1093. Dataloading: 0.0138 s/iter. Inference: 0.2324 s/iter. Eval: 0.2094 s/iter. Total: 0.4558 s/iter. ETA=0:00:18
[01/18 00:15:10] d2.evaluation.evaluator INFO: Inference done 1064/1093. Dataloading: 0.0138 s/iter. Inference: 0.2325 s/iter. Eval: 0.2093 s/iter. Total: 0.4558 s/iter. ETA=0:00:13
[01/18 00:15:15] d2.evaluation.evaluator INFO: Inference done 1079/1093. Dataloading: 0.0137 s/iter. Inference: 0.2319 s/iter. Eval: 0.2084 s/iter. Total: 0.4542 s/iter. ETA=0:00:06
[01/18 00:15:20] d2.evaluation.evaluator INFO: Total inference time: 0:08:12.145721 (0.452340 s / iter per device, on 4 devices)
[01/18 00:15:20] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:04:10 (0.230524 s / iter per device, on 4 devices)
[01/18 00:15:42] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.9669901330957575, 'mIoU': 13.781140336431678, 'fwIoU': 31.51198588701144, 'IoU-0': nan, 'IoU-1': 94.89396288610934, 'IoU-2': 44.97065305213859, 'IoU-3': 53.222445104515046, 'IoU-4': 46.34566083176579, 'IoU-5': 37.312359276514734, 'IoU-6': 36.77206407487555, 'IoU-7': 26.953118490192583, 'IoU-8': 9.195869248070448, 'IoU-9': 22.399423697381895, 'IoU-10': 20.885579360456052, 'IoU-11': 28.368587807519457, 'IoU-12': 27.259410320323173, 'IoU-13': 31.38627573400364, 'IoU-14': 33.36293150701307, 'IoU-15': 34.23276079251538, 'IoU-16': 32.837031977858, 'IoU-17': 30.554595375934134, 'IoU-18': 26.706181471635194, 'IoU-19': 31.102764123929138, 'IoU-20': 30.526869438620402, 'IoU-21': 31.147107968230443, 'IoU-22': 33.31008272777442, 'IoU-23': 28.512379306692363, 'IoU-24': 29.443839098701822, 'IoU-25': 29.646536274011453, 'IoU-26': 28.21389103314358, 'IoU-27': 31.430912547396666, 'IoU-28': 27.9067761132767, 'IoU-29': 32.79414403570884, 'IoU-30': 29.739189057386305, 'IoU-31': 31.264578636305913, 'IoU-32': 28.53855599812038, 'IoU-33': 26.41040776067077, 'IoU-34': 26.19271782273606, 'IoU-35': 25.57646460947224, 'IoU-36': 25.667241692092873, 'IoU-37': 27.567910189445357, 'IoU-38': 28.957513184515072, 'IoU-39': 26.092493258302476, 'IoU-40': 27.565976918979306, 'IoU-41': 25.200452367733845, 'IoU-42': 25.453969039345303, 'IoU-43': 25.420103172578706, 'IoU-44': 26.024619030247425, 'IoU-45': 25.846709672317992, 'IoU-46': 26.3778329698932, 'IoU-47': 25.407146285646416, 'IoU-48': 27.441519174587103, 'IoU-49': 25.693866565396746, 'IoU-50': 25.753812241241743, 'IoU-51': 26.479081551050022, 'IoU-52': 23.83877319791167, 'IoU-53': 24.129602415691366, 'IoU-54': 23.643530388299826, 'IoU-55': 23.76322403736268, 'IoU-56': 20.278746807895974, 'IoU-57': 22.47331823855452, 'IoU-58': 19.312998248867956, 'IoU-59': 19.255807964909476, 'IoU-60': 19.25115083176671, 'IoU-61': 18.5355303532975, 'IoU-62': 16.608428657092002, 'IoU-63': 18.030687955469666, 'IoU-64': 16.36146592464833, 'IoU-65': 17.96797343288043, 'IoU-66': 15.174722101691302, 'IoU-67': 16.038498840203726, 'IoU-68': 15.887676843879921, 'IoU-69': 16.386976448379247, 'IoU-70': 15.640434289711996, 'IoU-71': 15.579867629989147, 'IoU-72': 12.601670593102728, 'IoU-73': 15.753445676302963, 'IoU-74': 13.959659741173153, 'IoU-75': 15.743853118984902, 'IoU-76': 13.067263953497864, 'IoU-77': 14.416030291212447, 'IoU-78': 13.553645530269712, 'IoU-79': 11.601717632917948, 'IoU-80': 15.21242123986145, 'IoU-81': 9.291292814189186, 'IoU-82': 12.048679238946077, 'IoU-83': 12.905312183792836, 'IoU-84': 13.409404180130288, 'IoU-85': 11.916128897438956, 'IoU-86': 12.817610148739897, 'IoU-87': 11.526606337826495, 'IoU-88': 12.991702001536915, 'IoU-89': 13.12606599145101, 'IoU-90': 13.059563095620334, 'IoU-91': 13.102845200739313, 'IoU-92': 12.083011266841186, 'IoU-93': 12.340245403055212, 'IoU-94': 10.21732259513359, 'IoU-95': 12.017180627676662, 'IoU-96': 9.229798835175382, 'IoU-97': 12.149634116339488, 'IoU-98': 11.723019672299003, 'IoU-99': 9.263951292978456, 'IoU-100': 11.233709000398518, 'IoU-101': 9.939035503601602, 'IoU-102': 9.238659039493228, 'IoU-103': 10.75059937806948, 'IoU-104': 8.374283745430988, 'IoU-105': 10.498536571832823, 'IoU-106': 8.500171158678116, 'IoU-107': 11.936462652466666, 'IoU-108': 8.640840976290907, 'IoU-109': 8.774435039405963, 'IoU-110': 11.164706700786855, 'IoU-111': 8.023880071350911, 'IoU-112': 7.095121635522614, 'IoU-113': 5.836907993576018, 'IoU-114': 9.914126216318063, 'IoU-115': 6.8596762899806585, 'IoU-116': 8.731010695545368, 'IoU-117': 3.6711246022516075, 'IoU-118': 5.863055701493082, 'IoU-119': 7.1064345469330075, 'IoU-120': 5.544660025030473, 'IoU-121': 4.5576521956126985, 'IoU-122': 6.927518065326094, 'IoU-123': 5.565696287682223, 'IoU-124': 4.140026879843951, 'IoU-125': 5.8519493529714035, 'IoU-126': 5.652383723838702, 'IoU-127': 3.1065146468198197, 'IoU-128': 6.411061397489505, 'IoU-129': 4.584266914096647, 'IoU-130': 2.912594321297432, 'IoU-131': 4.316023081030278, 'IoU-132': 5.045515054226538, 'IoU-133': 3.234401005502399, 'IoU-134': 4.53778948211554, 'IoU-135': 3.036292742753665, 'IoU-136': 4.144026520421121, 'IoU-137': 4.775557652308542, 'IoU-138': 3.0920972527507735, 'IoU-139': 2.816097659090236, 'IoU-140': 3.678496315259653, 'IoU-141': 1.8420401521154577, 'IoU-142': 2.798781843981087, 'IoU-143': 2.3576564233235775, 'IoU-144': 4.026779587223802, 'IoU-145': 2.1472276000203507, 'IoU-146': 3.018092623483909, 'IoU-147': 2.509838551106419, 'IoU-148': 4.200401788113104, 'IoU-149': 2.0528127891680135, 'IoU-150': 1.383120335186284, 'IoU-151': 0.519987221229073, 'IoU-152': 2.176370041495868, 'IoU-153': 0.6225701425892523, 'IoU-154': 2.569423644687861, 'IoU-155': 2.699112658896592, 'IoU-156': 0.8605295337756749, 'IoU-157': 1.0971306389150555, 'IoU-158': 0.9190531199938731, 'IoU-159': 1.3845029621316909, 'IoU-160': 1.4037433745485506, 'IoU-161': 0.7442086971920621, 'IoU-162': 1.8059455252096264, 'IoU-163': 2.623847296681474, 'IoU-164': 1.5373935556850344, 'IoU-165': 2.8524934632003855, 'IoU-166': 1.594686329588015, 'IoU-167': 1.5866832788679124, 'IoU-168': 1.6259915930764677, 'IoU-169': 0.10697357015068518, 'IoU-170': 1.4456115881387206, 'IoU-171': 0.5646736685404427, 'IoU-172': 0.9413248676850233, 'IoU-173': 0.19617006028761702, 'IoU-174': 0.02186606479396793, 'IoU-175': 0.2888408792034567, 'IoU-176': 0.2917400658678048, 'IoU-177': 2.117100696491066, 'IoU-178': 1.1275499793083852, 'IoU-179': 0.5794564096384688, 'IoU-180': 1.4422611239622205, 'IoU-181': 1.5702407843514201, 'IoU-182': 0.04200134026748718, 'IoU-183': 1.6564468565057782, 'IoU-184': 1.3962382105185938, 'IoU-185': 0.44653062035810476, 'IoU-186': 1.8907653183415867, 'IoU-187': 0.47578133390613253, 'IoU-188': 0.13579729434921475, 'IoU-189': 1.243876944312261, 'IoU-190': 3.0625155919255116, 'IoU-191': 0.4553469630552861, 'mACC': 22.241459005678678, 'pACC': 44.640007264604634, 'ACC-0': nan, 'ACC-1': 98.22782264354062, 'ACC-2': 63.83581759499213, 'ACC-3': 67.71804109060406, 'ACC-4': 64.821914428277, 'ACC-5': 52.14908079154982, 'ACC-6': 59.13937882542852, 'ACC-7': 38.33240352815487, 'ACC-8': 11.034492714034602, 'ACC-9': 30.60326588200991, 'ACC-10': 28.47994653573241, 'ACC-11': 39.80798890874096, 'ACC-12': 40.32485722620023, 'ACC-13': 52.50350838737036, 'ACC-14': 56.28592997282899, 'ACC-15': 51.57813049056132, 'ACC-16': 50.80031775482493, 'ACC-17': 48.522084682104186, 'ACC-18': 38.25111841941299, 'ACC-19': 47.929034909822676, 'ACC-20': 47.70938723046181, 'ACC-21': 48.12741595409976, 'ACC-22': 48.91896701722823, 'ACC-23': 41.84334157212056, 'ACC-24': 48.99011345919733, 'ACC-25': 47.856200098286635, 'ACC-26': 42.65463256429925, 'ACC-27': 48.04552453276199, 'ACC-28': 48.420285251935056, 'ACC-29': 49.60178746158486, 'ACC-30': 48.807630721577844, 'ACC-31': 49.17102035235649, 'ACC-32': 47.43567207408557, 'ACC-33': 42.855743167594, 'ACC-34': 42.9150546173148, 'ACC-35': 40.72648720893975, 'ACC-36': 38.27810092197853, 'ACC-37': 40.307150499172664, 'ACC-38': 43.91596605814277, 'ACC-39': 43.9207298802537, 'ACC-40': 41.34399989524109, 'ACC-41': 41.49879850331576, 'ACC-42': 39.81313848032029, 'ACC-43': 39.27817006720074, 'ACC-44': 41.130785144233165, 'ACC-45': 41.3627592054677, 'ACC-46': 39.88009901214195, 'ACC-47': 37.853712532920945, 'ACC-48': 44.4525153030735, 'ACC-49': 41.60503747921182, 'ACC-50': 39.79054992817679, 'ACC-51': 44.314983724229364, 'ACC-52': 36.74441380780865, 'ACC-53': 39.534465593721116, 'ACC-54': 39.53078422251739, 'ACC-55': 40.0940384483153, 'ACC-56': 32.82221869315283, 'ACC-57': 37.03113850032479, 'ACC-58': 31.118562300877155, 'ACC-59': 33.09284320620478, 'ACC-60': 32.57691580253907, 'ACC-61': 32.84015375924184, 'ACC-62': 26.034219216935163, 'ACC-63': 30.55988413188188, 'ACC-64': 25.124435283050033, 'ACC-65': 30.813720054582678, 'ACC-66': 24.7667139132742, 'ACC-67': 28.971370216281333, 'ACC-68': 26.07961937723355, 'ACC-69': 27.51015010106409, 'ACC-70': 25.730950552366945, 'ACC-71': 27.103649746100544, 'ACC-72': 20.962437424662365, 'ACC-73': 30.856915302914516, 'ACC-74': 21.481888311334927, 'ACC-75': 30.262356413781653, 'ACC-76': 22.5435267663295, 'ACC-77': 23.073292173352232, 'ACC-78': 22.790133842143337, 'ACC-79': 18.085007205841126, 'ACC-80': 31.44994305685656, 'ACC-81': 13.414539559919646, 'ACC-82': 20.18138433735997, 'ACC-83': 21.563104787392522, 'ACC-84': 22.545946115765055, 'ACC-85': 21.051066230980993, 'ACC-86': 22.493568372102953, 'ACC-87': 16.974465811024686, 'ACC-88': 22.45536512156307, 'ACC-89': 24.76027361717607, 'ACC-90': 23.218093255047613, 'ACC-91': 20.473241761038057, 'ACC-92': 20.279281650543435, 'ACC-93': 24.22231228429483, 'ACC-94': 19.611982454731894, 'ACC-95': 22.247103116766194, 'ACC-96': 18.990272727015363, 'ACC-97': 19.074980623469717, 'ACC-98': 21.785351997644426, 'ACC-99': 16.097741681431835, 'ACC-100': 23.487791804221235, 'ACC-101': 17.475918675449773, 'ACC-102': 16.174943362029907, 'ACC-103': 18.950434495135543, 'ACC-104': 14.770345048080472, 'ACC-105': 18.169575622945533, 'ACC-106': 16.851363969983773, 'ACC-107': 28.19708431784656, 'ACC-108': 14.146506202833443, 'ACC-109': 13.099677809387329, 'ACC-110': 22.470190466067823, 'ACC-111': 15.993325981131857, 'ACC-112': 11.815174646772624, 'ACC-113': 9.53069055260507, 'ACC-114': 22.26285232819132, 'ACC-115': 14.710394960383685, 'ACC-116': 19.774371905906545, 'ACC-117': 5.488468867106476, 'ACC-118': 9.441545251138159, 'ACC-119': 16.635035172077476, 'ACC-120': 8.373700263900457, 'ACC-121': 6.594986344190001, 'ACC-122': 16.73818487172955, 'ACC-123': 10.173456438029561, 'ACC-124': 6.729898923925777, 'ACC-125': 11.780058584041342, 'ACC-126': 10.547853350212662, 'ACC-127': 5.3484403181040845, 'ACC-128': 12.47214568765848, 'ACC-129': 7.71380287770958, 'ACC-130': 4.0469581547970686, 'ACC-131': 7.348976183498116, 'ACC-132': 10.364226759558063, 'ACC-133': 5.8762851524491895, 'ACC-134': 10.271296854803964, 'ACC-135': 5.483153754061912, 'ACC-136': 7.158061172195304, 'ACC-137': 12.585548806037473, 'ACC-138': 6.12372134934132, 'ACC-139': 4.442210841047327, 'ACC-140': 6.001620044824553, 'ACC-141': 2.8444522241428354, 'ACC-142': 4.252961195953753, 'ACC-143': 6.042846629339185, 'ACC-144': 6.7947653429602894, 'ACC-145': 3.5863055971734608, 'ACC-146': 4.639418793264947, 'ACC-147': 4.0747161282954325, 'ACC-148': 9.213621773088947, 'ACC-149': 3.1587039040878477, 'ACC-150': 2.0555362728513074, 'ACC-151': 0.5707233318732196, 'ACC-152': 3.3660816643064266, 'ACC-153': 0.7580754502647539, 'ACC-154': 6.063754965262245, 'ACC-155': 10.799481342888061, 'ACC-156': 1.2727284479693872, 'ACC-157': 1.5978593584336078, 'ACC-158': 1.2193899945099225, 'ACC-159': 1.9554165591586874, 'ACC-160': 2.2470083246618104, 'ACC-161': 0.9441778042389708, 'ACC-162': 4.069461263132213, 'ACC-163': 6.495589046833607, 'ACC-164': 3.999410195998846, 'ACC-165': 8.028853942006245, 'ACC-166': 4.20881214797156, 'ACC-167': 2.7807743018099003, 'ACC-168': 6.5957521625993385, 'ACC-169': 0.11195083989450651, 'ACC-170': 2.825219699873646, 'ACC-171': 0.6939635457900218, 'ACC-172': 1.4816461088269066, 'ACC-173': 0.22351582108198215, 'ACC-174': 0.021995187154810702, 'ACC-175': 0.3183706522161149, 'ACC-176': 0.3040109124680752, 'ACC-177': 6.462816658638896, 'ACC-178': 2.284092862620265, 'ACC-179': 0.6634461462300231, 'ACC-180': 3.7188018558027705, 'ACC-181': 3.8694304101795205, 'ACC-182': 0.042171200860482025, 'ACC-183': 5.400811298855094, 'ACC-184': 2.064775704747687, 'ACC-185': 0.7263648499461867, 'ACC-186': 3.2241692299339255, 'ACC-187': 0.5351598835453436, 'ACC-188': 0.1397239820830475, 'ACC-189': 3.1741549236113076, 'ACC-190': 5.371422283240932, 'ACC-191': 1.0069820554649267})])
[01/18 00:15:42] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 00:15:42] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 00:15:42] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 00:15:42] d2.evaluation.testing INFO: copypaste: 2.9670,13.7811,31.5120,22.2415,44.6400
[01/18 00:15:42] d2.utils.events INFO:  eta: 11:48:35  iter: 11999  total_loss: 40.99  loss_ce: 0.3513  loss_mask: 0.4504  loss_dice: 3.253  loss_ce_0: 0.5945  loss_mask_0: 0.4256  loss_dice_0: 3.405  loss_ce_1: 0.3748  loss_mask_1: 0.4453  loss_dice_1: 3.297  loss_ce_2: 0.3903  loss_mask_2: 0.4477  loss_dice_2: 3.268  loss_ce_3: 0.3887  loss_mask_3: 0.449  loss_dice_3: 3.266  loss_ce_4: 0.3686  loss_mask_4: 0.4473  loss_dice_4: 3.265  loss_ce_5: 0.3704  loss_mask_5: 0.448  loss_dice_5: 3.257  loss_ce_6: 0.3723  loss_mask_6: 0.4479  loss_dice_6: 3.259  loss_ce_7: 0.3701  loss_mask_7: 0.449  loss_dice_7: 3.264  loss_ce_8: 0.367  loss_mask_8: 0.4489  loss_dice_8: 3.255  time: 1.5046  data_time: 0.0781  lr: 7.2544e-06  max_mem: 21589M
[01/18 00:16:13] d2.utils.events INFO:  eta: 11:48:32  iter: 12019  total_loss: 41.62  loss_ce: 0.3602  loss_mask: 0.4604  loss_dice: 3.304  loss_ce_0: 0.605  loss_mask_0: 0.4462  loss_dice_0: 3.444  loss_ce_1: 0.372  loss_mask_1: 0.466  loss_dice_1: 3.343  loss_ce_2: 0.3855  loss_mask_2: 0.4642  loss_dice_2: 3.323  loss_ce_3: 0.3757  loss_mask_3: 0.4601  loss_dice_3: 3.306  loss_ce_4: 0.3552  loss_mask_4: 0.4612  loss_dice_4: 3.307  loss_ce_5: 0.3661  loss_mask_5: 0.4628  loss_dice_5: 3.31  loss_ce_6: 0.3671  loss_mask_6: 0.4633  loss_dice_6: 3.303  loss_ce_7: 0.3578  loss_mask_7: 0.4593  loss_dice_7: 3.313  loss_ce_8: 0.3479  loss_mask_8: 0.4605  loss_dice_8: 3.306  time: 1.5046  data_time: 0.0792  lr: 7.2497e-06  max_mem: 21589M
[01/18 00:16:43] d2.utils.events INFO:  eta: 11:48:20  iter: 12039  total_loss: 40.98  loss_ce: 0.3658  loss_mask: 0.4388  loss_dice: 3.244  loss_ce_0: 0.5844  loss_mask_0: 0.4308  loss_dice_0: 3.381  loss_ce_1: 0.3766  loss_mask_1: 0.4479  loss_dice_1: 3.288  loss_ce_2: 0.3858  loss_mask_2: 0.4436  loss_dice_2: 3.258  loss_ce_3: 0.3696  loss_mask_3: 0.4436  loss_dice_3: 3.249  loss_ce_4: 0.3782  loss_mask_4: 0.4431  loss_dice_4: 3.248  loss_ce_5: 0.3561  loss_mask_5: 0.4423  loss_dice_5: 3.246  loss_ce_6: 0.3528  loss_mask_6: 0.44  loss_dice_6: 3.249  loss_ce_7: 0.3729  loss_mask_7: 0.4407  loss_dice_7: 3.25  loss_ce_8: 0.3604  loss_mask_8: 0.4391  loss_dice_8: 3.247  time: 1.5047  data_time: 0.0820  lr: 7.2451e-06  max_mem: 21589M
[01/18 00:17:14] d2.utils.events INFO:  eta: 11:47:46  iter: 12059  total_loss: 41.41  loss_ce: 0.3526  loss_mask: 0.4396  loss_dice: 3.28  loss_ce_0: 0.5866  loss_mask_0: 0.4255  loss_dice_0: 3.429  loss_ce_1: 0.3641  loss_mask_1: 0.4436  loss_dice_1: 3.32  loss_ce_2: 0.3683  loss_mask_2: 0.4414  loss_dice_2: 3.3  loss_ce_3: 0.3518  loss_mask_3: 0.4416  loss_dice_3: 3.274  loss_ce_4: 0.3687  loss_mask_4: 0.4384  loss_dice_4: 3.283  loss_ce_5: 0.3607  loss_mask_5: 0.4396  loss_dice_5: 3.282  loss_ce_6: 0.3545  loss_mask_6: 0.439  loss_dice_6: 3.27  loss_ce_7: 0.3639  loss_mask_7: 0.4384  loss_dice_7: 3.278  loss_ce_8: 0.3518  loss_mask_8: 0.4399  loss_dice_8: 3.279  time: 1.5047  data_time: 0.0800  lr: 7.2404e-06  max_mem: 21589M
[01/18 00:17:43] d2.utils.events INFO:  eta: 11:46:12  iter: 12079  total_loss: 40.53  loss_ce: 0.3242  loss_mask: 0.4522  loss_dice: 3.224  loss_ce_0: 0.5989  loss_mask_0: 0.4366  loss_dice_0: 3.366  loss_ce_1: 0.3578  loss_mask_1: 0.4527  loss_dice_1: 3.268  loss_ce_2: 0.3673  loss_mask_2: 0.4526  loss_dice_2: 3.242  loss_ce_3: 0.3368  loss_mask_3: 0.4502  loss_dice_3: 3.228  loss_ce_4: 0.3298  loss_mask_4: 0.4508  loss_dice_4: 3.227  loss_ce_5: 0.3444  loss_mask_5: 0.4518  loss_dice_5: 3.237  loss_ce_6: 0.3312  loss_mask_6: 0.4511  loss_dice_6: 3.224  loss_ce_7: 0.3424  loss_mask_7: 0.4507  loss_dice_7: 3.228  loss_ce_8: 0.3443  loss_mask_8: 0.4535  loss_dice_8: 3.222  time: 1.5046  data_time: 0.0862  lr: 7.2358e-06  max_mem: 21589M
[01/18 00:18:13] d2.utils.events INFO:  eta: 11:44:22  iter: 12099  total_loss: 41.57  loss_ce: 0.357  loss_mask: 0.4365  loss_dice: 3.276  loss_ce_0: 0.5924  loss_mask_0: 0.4221  loss_dice_0: 3.407  loss_ce_1: 0.3904  loss_mask_1: 0.4369  loss_dice_1: 3.328  loss_ce_2: 0.4056  loss_mask_2: 0.4366  loss_dice_2: 3.297  loss_ce_3: 0.3834  loss_mask_3: 0.4363  loss_dice_3: 3.278  loss_ce_4: 0.3772  loss_mask_4: 0.4373  loss_dice_4: 3.274  loss_ce_5: 0.3751  loss_mask_5: 0.4376  loss_dice_5: 3.281  loss_ce_6: 0.3592  loss_mask_6: 0.4387  loss_dice_6: 3.278  loss_ce_7: 0.3529  loss_mask_7: 0.4393  loss_dice_7: 3.281  loss_ce_8: 0.353  loss_mask_8: 0.4377  loss_dice_8: 3.279  time: 1.5046  data_time: 0.0759  lr: 7.2311e-06  max_mem: 21589M
[01/18 00:18:44] d2.utils.events INFO:  eta: 11:43:52  iter: 12119  total_loss: 41.55  loss_ce: 0.3516  loss_mask: 0.4326  loss_dice: 3.31  loss_ce_0: 0.6018  loss_mask_0: 0.4258  loss_dice_0: 3.445  loss_ce_1: 0.3854  loss_mask_1: 0.4379  loss_dice_1: 3.353  loss_ce_2: 0.3714  loss_mask_2: 0.4326  loss_dice_2: 3.325  loss_ce_3: 0.3617  loss_mask_3: 0.4324  loss_dice_3: 3.313  loss_ce_4: 0.3656  loss_mask_4: 0.4323  loss_dice_4: 3.305  loss_ce_5: 0.3597  loss_mask_5: 0.4329  loss_dice_5: 3.309  loss_ce_6: 0.3624  loss_mask_6: 0.432  loss_dice_6: 3.307  loss_ce_7: 0.3524  loss_mask_7: 0.4313  loss_dice_7: 3.313  loss_ce_8: 0.3459  loss_mask_8: 0.4322  loss_dice_8: 3.313  time: 1.5046  data_time: 0.0808  lr: 7.2264e-06  max_mem: 21589M
[01/18 00:19:13] d2.utils.events INFO:  eta: 11:42:11  iter: 12139  total_loss: 41.05  loss_ce: 0.357  loss_mask: 0.4472  loss_dice: 3.245  loss_ce_0: 0.5958  loss_mask_0: 0.4258  loss_dice_0: 3.393  loss_ce_1: 0.3586  loss_mask_1: 0.4449  loss_dice_1: 3.29  loss_ce_2: 0.3656  loss_mask_2: 0.4433  loss_dice_2: 3.266  loss_ce_3: 0.3528  loss_mask_3: 0.4463  loss_dice_3: 3.252  loss_ce_4: 0.3674  loss_mask_4: 0.4458  loss_dice_4: 3.245  loss_ce_5: 0.3672  loss_mask_5: 0.4452  loss_dice_5: 3.249  loss_ce_6: 0.3469  loss_mask_6: 0.4459  loss_dice_6: 3.246  loss_ce_7: 0.3443  loss_mask_7: 0.4476  loss_dice_7: 3.243  loss_ce_8: 0.3542  loss_mask_8: 0.4468  loss_dice_8: 3.247  time: 1.5046  data_time: 0.0844  lr: 7.2218e-06  max_mem: 21589M
[01/18 00:19:43] d2.utils.events INFO:  eta: 11:41:11  iter: 12159  total_loss: 41.11  loss_ce: 0.3457  loss_mask: 0.4409  loss_dice: 3.221  loss_ce_0: 0.6  loss_mask_0: 0.4209  loss_dice_0: 3.365  loss_ce_1: 0.3708  loss_mask_1: 0.4441  loss_dice_1: 3.249  loss_ce_2: 0.3637  loss_mask_2: 0.4379  loss_dice_2: 3.235  loss_ce_3: 0.3376  loss_mask_3: 0.4365  loss_dice_3: 3.227  loss_ce_4: 0.3518  loss_mask_4: 0.4374  loss_dice_4: 3.228  loss_ce_5: 0.3704  loss_mask_5: 0.4366  loss_dice_5: 3.221  loss_ce_6: 0.3493  loss_mask_6: 0.4395  loss_dice_6: 3.223  loss_ce_7: 0.3488  loss_mask_7: 0.4374  loss_dice_7: 3.22  loss_ce_8: 0.3513  loss_mask_8: 0.4387  loss_dice_8: 3.23  time: 1.5046  data_time: 0.0807  lr: 7.2171e-06  max_mem: 21589M
[01/18 00:20:13] d2.utils.events INFO:  eta: 11:40:40  iter: 12179  total_loss: 40.96  loss_ce: 0.3698  loss_mask: 0.4356  loss_dice: 3.245  loss_ce_0: 0.6115  loss_mask_0: 0.426  loss_dice_0: 3.395  loss_ce_1: 0.366  loss_mask_1: 0.4398  loss_dice_1: 3.293  loss_ce_2: 0.3704  loss_mask_2: 0.4365  loss_dice_2: 3.262  loss_ce_3: 0.3397  loss_mask_3: 0.4353  loss_dice_3: 3.248  loss_ce_4: 0.3526  loss_mask_4: 0.4379  loss_dice_4: 3.25  loss_ce_5: 0.3587  loss_mask_5: 0.435  loss_dice_5: 3.246  loss_ce_6: 0.3456  loss_mask_6: 0.4367  loss_dice_6: 3.247  loss_ce_7: 0.3465  loss_mask_7: 0.4369  loss_dice_7: 3.255  loss_ce_8: 0.3519  loss_mask_8: 0.4362  loss_dice_8: 3.245  time: 1.5046  data_time: 0.0751  lr: 7.2124e-06  max_mem: 21589M
[01/18 00:20:48] d2.utils.events INFO:  eta: 11:40:31  iter: 12199  total_loss: 40.86  loss_ce: 0.3499  loss_mask: 0.4456  loss_dice: 3.254  loss_ce_0: 0.5858  loss_mask_0: 0.4314  loss_dice_0: 3.384  loss_ce_1: 0.3607  loss_mask_1: 0.449  loss_dice_1: 3.284  loss_ce_2: 0.3679  loss_mask_2: 0.4455  loss_dice_2: 3.263  loss_ce_3: 0.3532  loss_mask_3: 0.4453  loss_dice_3: 3.256  loss_ce_4: 0.3585  loss_mask_4: 0.445  loss_dice_4: 3.251  loss_ce_5: 0.3436  loss_mask_5: 0.4467  loss_dice_5: 3.257  loss_ce_6: 0.3509  loss_mask_6: 0.445  loss_dice_6: 3.26  loss_ce_7: 0.3512  loss_mask_7: 0.4471  loss_dice_7: 3.254  loss_ce_8: 0.35  loss_mask_8: 0.4454  loss_dice_8: 3.258  time: 1.5049  data_time: 0.1043  lr: 7.2078e-06  max_mem: 21589M
[01/18 00:21:22] d2.utils.events INFO:  eta: 11:41:10  iter: 12219  total_loss: 40.98  loss_ce: 0.3618  loss_mask: 0.4417  loss_dice: 3.218  loss_ce_0: 0.5882  loss_mask_0: 0.425  loss_dice_0: 3.366  loss_ce_1: 0.3623  loss_mask_1: 0.445  loss_dice_1: 3.268  loss_ce_2: 0.3872  loss_mask_2: 0.4442  loss_dice_2: 3.25  loss_ce_3: 0.3667  loss_mask_3: 0.4417  loss_dice_3: 3.243  loss_ce_4: 0.362  loss_mask_4: 0.4407  loss_dice_4: 3.235  loss_ce_5: 0.3603  loss_mask_5: 0.4406  loss_dice_5: 3.236  loss_ce_6: 0.3848  loss_mask_6: 0.4415  loss_dice_6: 3.226  loss_ce_7: 0.3514  loss_mask_7: 0.4395  loss_dice_7: 3.238  loss_ce_8: 0.3682  loss_mask_8: 0.4398  loss_dice_8: 3.228  time: 1.5052  data_time: 0.1037  lr: 7.2031e-06  max_mem: 21589M
[01/18 00:21:54] d2.utils.events INFO:  eta: 11:41:16  iter: 12239  total_loss: 41.06  loss_ce: 0.3388  loss_mask: 0.4486  loss_dice: 3.24  loss_ce_0: 0.5741  loss_mask_0: 0.4296  loss_dice_0: 3.387  loss_ce_1: 0.3666  loss_mask_1: 0.4527  loss_dice_1: 3.295  loss_ce_2: 0.3702  loss_mask_2: 0.4487  loss_dice_2: 3.253  loss_ce_3: 0.3546  loss_mask_3: 0.45  loss_dice_3: 3.25  loss_ce_4: 0.3582  loss_mask_4: 0.4473  loss_dice_4: 3.256  loss_ce_5: 0.3386  loss_mask_5: 0.4481  loss_dice_5: 3.243  loss_ce_6: 0.3609  loss_mask_6: 0.4471  loss_dice_6: 3.244  loss_ce_7: 0.3442  loss_mask_7: 0.4485  loss_dice_7: 3.247  loss_ce_8: 0.3382  loss_mask_8: 0.4508  loss_dice_8: 3.242  time: 1.5054  data_time: 0.0954  lr: 7.1984e-06  max_mem: 21589M
[01/18 00:22:23] d2.utils.events INFO:  eta: 11:40:28  iter: 12259  total_loss: 40.43  loss_ce: 0.3689  loss_mask: 0.4406  loss_dice: 3.235  loss_ce_0: 0.5718  loss_mask_0: 0.4451  loss_dice_0: 3.379  loss_ce_1: 0.3844  loss_mask_1: 0.4505  loss_dice_1: 3.285  loss_ce_2: 0.3668  loss_mask_2: 0.4452  loss_dice_2: 3.253  loss_ce_3: 0.3599  loss_mask_3: 0.4417  loss_dice_3: 3.23  loss_ce_4: 0.3554  loss_mask_4: 0.4412  loss_dice_4: 3.24  loss_ce_5: 0.3585  loss_mask_5: 0.4402  loss_dice_5: 3.233  loss_ce_6: 0.3524  loss_mask_6: 0.4424  loss_dice_6: 3.237  loss_ce_7: 0.3573  loss_mask_7: 0.4416  loss_dice_7: 3.229  loss_ce_8: 0.3564  loss_mask_8: 0.441  loss_dice_8: 3.241  time: 1.5054  data_time: 0.0755  lr: 7.1938e-06  max_mem: 21589M
[01/18 00:22:54] d2.utils.events INFO:  eta: 11:40:03  iter: 12279  total_loss: 42.29  loss_ce: 0.3976  loss_mask: 0.4488  loss_dice: 3.333  loss_ce_0: 0.6195  loss_mask_0: 0.4314  loss_dice_0: 3.457  loss_ce_1: 0.4157  loss_mask_1: 0.4502  loss_dice_1: 3.379  loss_ce_2: 0.4336  loss_mask_2: 0.4488  loss_dice_2: 3.351  loss_ce_3: 0.419  loss_mask_3: 0.4489  loss_dice_3: 3.332  loss_ce_4: 0.405  loss_mask_4: 0.45  loss_dice_4: 3.334  loss_ce_5: 0.4088  loss_mask_5: 0.4515  loss_dice_5: 3.334  loss_ce_6: 0.3958  loss_mask_6: 0.4483  loss_dice_6: 3.345  loss_ce_7: 0.3995  loss_mask_7: 0.4487  loss_dice_7: 3.336  loss_ce_8: 0.3965  loss_mask_8: 0.4473  loss_dice_8: 3.339  time: 1.5053  data_time: 0.0806  lr: 7.1891e-06  max_mem: 21589M
[01/18 00:23:23] d2.utils.events INFO:  eta: 11:38:37  iter: 12299  total_loss: 41.33  loss_ce: 0.3553  loss_mask: 0.4345  loss_dice: 3.269  loss_ce_0: 0.6317  loss_mask_0: 0.4189  loss_dice_0: 3.412  loss_ce_1: 0.3752  loss_mask_1: 0.4347  loss_dice_1: 3.319  loss_ce_2: 0.3796  loss_mask_2: 0.4329  loss_dice_2: 3.303  loss_ce_3: 0.3892  loss_mask_3: 0.4303  loss_dice_3: 3.275  loss_ce_4: 0.371  loss_mask_4: 0.4307  loss_dice_4: 3.268  loss_ce_5: 0.3637  loss_mask_5: 0.4344  loss_dice_5: 3.277  loss_ce_6: 0.3692  loss_mask_6: 0.4344  loss_dice_6: 3.27  loss_ce_7: 0.3576  loss_mask_7: 0.4351  loss_dice_7: 3.275  loss_ce_8: 0.3644  loss_mask_8: 0.4355  loss_dice_8: 3.277  time: 1.5053  data_time: 0.0801  lr: 7.1844e-06  max_mem: 21589M
[01/18 00:23:55] d2.utils.events INFO:  eta: 11:38:22  iter: 12319  total_loss: 41.21  loss_ce: 0.3547  loss_mask: 0.4577  loss_dice: 3.278  loss_ce_0: 0.5814  loss_mask_0: 0.4377  loss_dice_0: 3.422  loss_ce_1: 0.3683  loss_mask_1: 0.4588  loss_dice_1: 3.318  loss_ce_2: 0.3556  loss_mask_2: 0.4549  loss_dice_2: 3.289  loss_ce_3: 0.3598  loss_mask_3: 0.4565  loss_dice_3: 3.28  loss_ce_4: 0.3699  loss_mask_4: 0.458  loss_dice_4: 3.28  loss_ce_5: 0.3521  loss_mask_5: 0.4576  loss_dice_5: 3.271  loss_ce_6: 0.3522  loss_mask_6: 0.4573  loss_dice_6: 3.283  loss_ce_7: 0.3624  loss_mask_7: 0.4569  loss_dice_7: 3.275  loss_ce_8: 0.3413  loss_mask_8: 0.4581  loss_dice_8: 3.268  time: 1.5054  data_time: 0.0993  lr: 7.1798e-06  max_mem: 21589M
[01/18 00:24:25] d2.utils.events INFO:  eta: 11:37:52  iter: 12339  total_loss: 41.46  loss_ce: 0.3788  loss_mask: 0.4422  loss_dice: 3.273  loss_ce_0: 0.5836  loss_mask_0: 0.4325  loss_dice_0: 3.409  loss_ce_1: 0.3828  loss_mask_1: 0.4426  loss_dice_1: 3.309  loss_ce_2: 0.3851  loss_mask_2: 0.4419  loss_dice_2: 3.298  loss_ce_3: 0.3921  loss_mask_3: 0.4422  loss_dice_3: 3.28  loss_ce_4: 0.3852  loss_mask_4: 0.4423  loss_dice_4: 3.273  loss_ce_5: 0.3786  loss_mask_5: 0.4413  loss_dice_5: 3.281  loss_ce_6: 0.3761  loss_mask_6: 0.4425  loss_dice_6: 3.273  loss_ce_7: 0.3761  loss_mask_7: 0.4418  loss_dice_7: 3.277  loss_ce_8: 0.3853  loss_mask_8: 0.4424  loss_dice_8: 3.269  time: 1.5054  data_time: 0.0773  lr: 7.1751e-06  max_mem: 21589M
[01/18 00:24:55] d2.utils.events INFO:  eta: 11:37:22  iter: 12359  total_loss: 40.61  loss_ce: 0.3685  loss_mask: 0.4501  loss_dice: 3.234  loss_ce_0: 0.5749  loss_mask_0: 0.4395  loss_dice_0: 3.36  loss_ce_1: 0.3522  loss_mask_1: 0.447  loss_dice_1: 3.279  loss_ce_2: 0.3803  loss_mask_2: 0.4457  loss_dice_2: 3.253  loss_ce_3: 0.3549  loss_mask_3: 0.449  loss_dice_3: 3.239  loss_ce_4: 0.3578  loss_mask_4: 0.4462  loss_dice_4: 3.244  loss_ce_5: 0.3599  loss_mask_5: 0.4484  loss_dice_5: 3.233  loss_ce_6: 0.3478  loss_mask_6: 0.4478  loss_dice_6: 3.237  loss_ce_7: 0.3497  loss_mask_7: 0.4507  loss_dice_7: 3.227  loss_ce_8: 0.3456  loss_mask_8: 0.4498  loss_dice_8: 3.241  time: 1.5054  data_time: 0.0770  lr: 7.1704e-06  max_mem: 21589M
[01/18 00:25:25] d2.utils.events INFO:  eta: 11:36:45  iter: 12379  total_loss: 41.24  loss_ce: 0.35  loss_mask: 0.4446  loss_dice: 3.3  loss_ce_0: 0.5927  loss_mask_0: 0.4364  loss_dice_0: 3.423  loss_ce_1: 0.3405  loss_mask_1: 0.4458  loss_dice_1: 3.328  loss_ce_2: 0.387  loss_mask_2: 0.4419  loss_dice_2: 3.315  loss_ce_3: 0.3518  loss_mask_3: 0.4443  loss_dice_3: 3.301  loss_ce_4: 0.3577  loss_mask_4: 0.4439  loss_dice_4: 3.297  loss_ce_5: 0.36  loss_mask_5: 0.4437  loss_dice_5: 3.291  loss_ce_6: 0.3579  loss_mask_6: 0.4432  loss_dice_6: 3.299  loss_ce_7: 0.3458  loss_mask_7: 0.444  loss_dice_7: 3.298  loss_ce_8: 0.3521  loss_mask_8: 0.4433  loss_dice_8: 3.294  time: 1.5054  data_time: 0.0709  lr: 7.1657e-06  max_mem: 21589M
[01/18 00:25:56] d2.utils.events INFO:  eta: 11:36:32  iter: 12399  total_loss: 41.1  loss_ce: 0.3677  loss_mask: 0.4431  loss_dice: 3.257  loss_ce_0: 0.6079  loss_mask_0: 0.4238  loss_dice_0: 3.388  loss_ce_1: 0.3839  loss_mask_1: 0.448  loss_dice_1: 3.281  loss_ce_2: 0.3786  loss_mask_2: 0.4493  loss_dice_2: 3.271  loss_ce_3: 0.3705  loss_mask_3: 0.4455  loss_dice_3: 3.253  loss_ce_4: 0.3744  loss_mask_4: 0.4464  loss_dice_4: 3.26  loss_ce_5: 0.3784  loss_mask_5: 0.4466  loss_dice_5: 3.256  loss_ce_6: 0.3773  loss_mask_6: 0.4472  loss_dice_6: 3.248  loss_ce_7: 0.3519  loss_mask_7: 0.4447  loss_dice_7: 3.253  loss_ce_8: 0.3634  loss_mask_8: 0.4453  loss_dice_8: 3.255  time: 1.5055  data_time: 0.0860  lr: 7.1611e-06  max_mem: 21589M
[01/18 00:26:30] d2.utils.events INFO:  eta: 11:36:39  iter: 12419  total_loss: 41.36  loss_ce: 0.377  loss_mask: 0.4445  loss_dice: 3.25  loss_ce_0: 0.614  loss_mask_0: 0.4353  loss_dice_0: 3.398  loss_ce_1: 0.3876  loss_mask_1: 0.449  loss_dice_1: 3.305  loss_ce_2: 0.3876  loss_mask_2: 0.4456  loss_dice_2: 3.277  loss_ce_3: 0.3609  loss_mask_3: 0.4452  loss_dice_3: 3.258  loss_ce_4: 0.3644  loss_mask_4: 0.4421  loss_dice_4: 3.26  loss_ce_5: 0.3646  loss_mask_5: 0.4411  loss_dice_5: 3.258  loss_ce_6: 0.3637  loss_mask_6: 0.4427  loss_dice_6: 3.251  loss_ce_7: 0.36  loss_mask_7: 0.4413  loss_dice_7: 3.267  loss_ce_8: 0.3685  loss_mask_8: 0.4432  loss_dice_8: 3.251  time: 1.5058  data_time: 0.1312  lr: 7.1564e-06  max_mem: 21589M
[01/18 00:27:05] d2.utils.events INFO:  eta: 11:37:24  iter: 12439  total_loss: 40.79  loss_ce: 0.3751  loss_mask: 0.4424  loss_dice: 3.257  loss_ce_0: 0.6099  loss_mask_0: 0.4303  loss_dice_0: 3.401  loss_ce_1: 0.3886  loss_mask_1: 0.4481  loss_dice_1: 3.306  loss_ce_2: 0.388  loss_mask_2: 0.4425  loss_dice_2: 3.274  loss_ce_3: 0.3769  loss_mask_3: 0.4428  loss_dice_3: 3.246  loss_ce_4: 0.3837  loss_mask_4: 0.4452  loss_dice_4: 3.259  loss_ce_5: 0.3708  loss_mask_5: 0.4439  loss_dice_5: 3.251  loss_ce_6: 0.3576  loss_mask_6: 0.4438  loss_dice_6: 3.252  loss_ce_7: 0.3672  loss_mask_7: 0.4438  loss_dice_7: 3.257  loss_ce_8: 0.3585  loss_mask_8: 0.4434  loss_dice_8: 3.259  time: 1.5061  data_time: 0.1208  lr: 7.1517e-06  max_mem: 21589M
[01/18 00:27:35] d2.utils.events INFO:  eta: 11:35:56  iter: 12459  total_loss: 41.26  loss_ce: 0.3542  loss_mask: 0.4506  loss_dice: 3.253  loss_ce_0: 0.5971  loss_mask_0: 0.4321  loss_dice_0: 3.373  loss_ce_1: 0.3773  loss_mask_1: 0.4491  loss_dice_1: 3.287  loss_ce_2: 0.3794  loss_mask_2: 0.4498  loss_dice_2: 3.269  loss_ce_3: 0.3678  loss_mask_3: 0.4469  loss_dice_3: 3.251  loss_ce_4: 0.3588  loss_mask_4: 0.4481  loss_dice_4: 3.253  loss_ce_5: 0.3659  loss_mask_5: 0.4479  loss_dice_5: 3.263  loss_ce_6: 0.3466  loss_mask_6: 0.4475  loss_dice_6: 3.262  loss_ce_7: 0.3556  loss_mask_7: 0.4498  loss_dice_7: 3.25  loss_ce_8: 0.3569  loss_mask_8: 0.4497  loss_dice_8: 3.255  time: 1.5061  data_time: 0.0780  lr: 7.1471e-06  max_mem: 21589M
[01/18 00:28:05] d2.utils.events INFO:  eta: 11:34:41  iter: 12479  total_loss: 40.91  loss_ce: 0.3663  loss_mask: 0.44  loss_dice: 3.251  loss_ce_0: 0.5959  loss_mask_0: 0.4315  loss_dice_0: 3.382  loss_ce_1: 0.3743  loss_mask_1: 0.449  loss_dice_1: 3.29  loss_ce_2: 0.3586  loss_mask_2: 0.4422  loss_dice_2: 3.271  loss_ce_3: 0.3607  loss_mask_3: 0.4406  loss_dice_3: 3.259  loss_ce_4: 0.3519  loss_mask_4: 0.4399  loss_dice_4: 3.253  loss_ce_5: 0.3481  loss_mask_5: 0.4409  loss_dice_5: 3.259  loss_ce_6: 0.3628  loss_mask_6: 0.4405  loss_dice_6: 3.248  loss_ce_7: 0.3612  loss_mask_7: 0.4404  loss_dice_7: 3.249  loss_ce_8: 0.35  loss_mask_8: 0.4398  loss_dice_8: 3.252  time: 1.5061  data_time: 0.0743  lr: 7.1424e-06  max_mem: 21589M
[01/18 00:28:34] d2.utils.events INFO:  eta: 11:33:24  iter: 12499  total_loss: 40.87  loss_ce: 0.3822  loss_mask: 0.4524  loss_dice: 3.187  loss_ce_0: 0.5796  loss_mask_0: 0.4372  loss_dice_0: 3.332  loss_ce_1: 0.3807  loss_mask_1: 0.4509  loss_dice_1: 3.232  loss_ce_2: 0.3823  loss_mask_2: 0.4516  loss_dice_2: 3.194  loss_ce_3: 0.3588  loss_mask_3: 0.4512  loss_dice_3: 3.191  loss_ce_4: 0.3848  loss_mask_4: 0.4506  loss_dice_4: 3.183  loss_ce_5: 0.3717  loss_mask_5: 0.4485  loss_dice_5: 3.188  loss_ce_6: 0.361  loss_mask_6: 0.4514  loss_dice_6: 3.186  loss_ce_7: 0.3678  loss_mask_7: 0.4484  loss_dice_7: 3.19  loss_ce_8: 0.3707  loss_mask_8: 0.4517  loss_dice_8: 3.189  time: 1.5060  data_time: 0.0713  lr: 7.1377e-06  max_mem: 21589M
[01/18 00:29:06] d2.utils.events INFO:  eta: 11:33:10  iter: 12519  total_loss: 41.41  loss_ce: 0.3639  loss_mask: 0.4564  loss_dice: 3.274  loss_ce_0: 0.6314  loss_mask_0: 0.4363  loss_dice_0: 3.402  loss_ce_1: 0.3758  loss_mask_1: 0.4617  loss_dice_1: 3.323  loss_ce_2: 0.374  loss_mask_2: 0.4609  loss_dice_2: 3.295  loss_ce_3: 0.3698  loss_mask_3: 0.4575  loss_dice_3: 3.285  loss_ce_4: 0.3677  loss_mask_4: 0.4541  loss_dice_4: 3.287  loss_ce_5: 0.3664  loss_mask_5: 0.458  loss_dice_5: 3.29  loss_ce_6: 0.3662  loss_mask_6: 0.4567  loss_dice_6: 3.28  loss_ce_7: 0.3555  loss_mask_7: 0.4586  loss_dice_7: 3.289  loss_ce_8: 0.3651  loss_mask_8: 0.4583  loss_dice_8: 3.291  time: 1.5062  data_time: 0.1061  lr: 7.1331e-06  max_mem: 21589M
[01/18 00:29:41] d2.utils.events INFO:  eta: 11:34:01  iter: 12539  total_loss: 41.1  loss_ce: 0.384  loss_mask: 0.4384  loss_dice: 3.25  loss_ce_0: 0.6093  loss_mask_0: 0.4312  loss_dice_0: 3.395  loss_ce_1: 0.3948  loss_mask_1: 0.4455  loss_dice_1: 3.294  loss_ce_2: 0.3872  loss_mask_2: 0.4411  loss_dice_2: 3.277  loss_ce_3: 0.3713  loss_mask_3: 0.4409  loss_dice_3: 3.265  loss_ce_4: 0.3822  loss_mask_4: 0.4388  loss_dice_4: 3.249  loss_ce_5: 0.3987  loss_mask_5: 0.4382  loss_dice_5: 3.254  loss_ce_6: 0.3883  loss_mask_6: 0.4389  loss_dice_6: 3.249  loss_ce_7: 0.3921  loss_mask_7: 0.4431  loss_dice_7: 3.254  loss_ce_8: 0.3805  loss_mask_8: 0.4412  loss_dice_8: 3.254  time: 1.5066  data_time: 0.1375  lr: 7.1284e-06  max_mem: 21589M
[01/18 00:30:11] d2.utils.events INFO:  eta: 11:33:12  iter: 12559  total_loss: 41.8  loss_ce: 0.3612  loss_mask: 0.4265  loss_dice: 3.343  loss_ce_0: 0.6271  loss_mask_0: 0.4212  loss_dice_0: 3.461  loss_ce_1: 0.3706  loss_mask_1: 0.4331  loss_dice_1: 3.37  loss_ce_2: 0.3681  loss_mask_2: 0.4263  loss_dice_2: 3.355  loss_ce_3: 0.3745  loss_mask_3: 0.4228  loss_dice_3: 3.339  loss_ce_4: 0.3493  loss_mask_4: 0.4232  loss_dice_4: 3.341  loss_ce_5: 0.351  loss_mask_5: 0.4272  loss_dice_5: 3.342  loss_ce_6: 0.3514  loss_mask_6: 0.4268  loss_dice_6: 3.338  loss_ce_7: 0.3596  loss_mask_7: 0.4264  loss_dice_7: 3.346  loss_ce_8: 0.366  loss_mask_8: 0.427  loss_dice_8: 3.342  time: 1.5065  data_time: 0.0812  lr: 7.1237e-06  max_mem: 21589M
[01/18 00:30:40] d2.utils.events INFO:  eta: 11:32:04  iter: 12579  total_loss: 41.27  loss_ce: 0.3447  loss_mask: 0.4411  loss_dice: 3.261  loss_ce_0: 0.5929  loss_mask_0: 0.425  loss_dice_0: 3.378  loss_ce_1: 0.3635  loss_mask_1: 0.4381  loss_dice_1: 3.302  loss_ce_2: 0.3874  loss_mask_2: 0.4363  loss_dice_2: 3.282  loss_ce_3: 0.3521  loss_mask_3: 0.4377  loss_dice_3: 3.267  loss_ce_4: 0.3497  loss_mask_4: 0.4399  loss_dice_4: 3.266  loss_ce_5: 0.3516  loss_mask_5: 0.438  loss_dice_5: 3.264  loss_ce_6: 0.3544  loss_mask_6: 0.4417  loss_dice_6: 3.262  loss_ce_7: 0.3641  loss_mask_7: 0.4413  loss_dice_7: 3.257  loss_ce_8: 0.3427  loss_mask_8: 0.4431  loss_dice_8: 3.266  time: 1.5065  data_time: 0.0679  lr: 7.119e-06  max_mem: 21589M
[01/18 00:31:10] d2.utils.events INFO:  eta: 11:30:43  iter: 12599  total_loss: 41.8  loss_ce: 0.367  loss_mask: 0.4498  loss_dice: 3.288  loss_ce_0: 0.5939  loss_mask_0: 0.4343  loss_dice_0: 3.432  loss_ce_1: 0.3708  loss_mask_1: 0.4504  loss_dice_1: 3.329  loss_ce_2: 0.3729  loss_mask_2: 0.4489  loss_dice_2: 3.31  loss_ce_3: 0.3608  loss_mask_3: 0.4458  loss_dice_3: 3.296  loss_ce_4: 0.3592  loss_mask_4: 0.4474  loss_dice_4: 3.289  loss_ce_5: 0.3593  loss_mask_5: 0.4475  loss_dice_5: 3.299  loss_ce_6: 0.3507  loss_mask_6: 0.4459  loss_dice_6: 3.298  loss_ce_7: 0.3638  loss_mask_7: 0.4474  loss_dice_7: 3.291  loss_ce_8: 0.3664  loss_mask_8: 0.4476  loss_dice_8: 3.291  time: 1.5064  data_time: 0.0675  lr: 7.1144e-06  max_mem: 21589M
[01/18 00:31:39] d2.utils.events INFO:  eta: 11:29:51  iter: 12619  total_loss: 40.52  loss_ce: 0.3442  loss_mask: 0.4333  loss_dice: 3.234  loss_ce_0: 0.5927  loss_mask_0: 0.4226  loss_dice_0: 3.376  loss_ce_1: 0.3542  loss_mask_1: 0.4378  loss_dice_1: 3.279  loss_ce_2: 0.3612  loss_mask_2: 0.4349  loss_dice_2: 3.257  loss_ce_3: 0.3608  loss_mask_3: 0.4349  loss_dice_3: 3.236  loss_ce_4: 0.3561  loss_mask_4: 0.4319  loss_dice_4: 3.238  loss_ce_5: 0.3501  loss_mask_5: 0.4309  loss_dice_5: 3.239  loss_ce_6: 0.3472  loss_mask_6: 0.4345  loss_dice_6: 3.232  loss_ce_7: 0.3516  loss_mask_7: 0.4338  loss_dice_7: 3.239  loss_ce_8: 0.3362  loss_mask_8: 0.4335  loss_dice_8: 3.241  time: 1.5064  data_time: 0.0743  lr: 7.1097e-06  max_mem: 21589M
[01/18 00:32:09] d2.utils.events INFO:  eta: 11:29:15  iter: 12639  total_loss: 41.17  loss_ce: 0.3529  loss_mask: 0.4377  loss_dice: 3.269  loss_ce_0: 0.5916  loss_mask_0: 0.421  loss_dice_0: 3.406  loss_ce_1: 0.3877  loss_mask_1: 0.4366  loss_dice_1: 3.303  loss_ce_2: 0.3821  loss_mask_2: 0.4371  loss_dice_2: 3.279  loss_ce_3: 0.3734  loss_mask_3: 0.4381  loss_dice_3: 3.264  loss_ce_4: 0.3454  loss_mask_4: 0.4392  loss_dice_4: 3.268  loss_ce_5: 0.3494  loss_mask_5: 0.436  loss_dice_5: 3.273  loss_ce_6: 0.3479  loss_mask_6: 0.4378  loss_dice_6: 3.267  loss_ce_7: 0.3466  loss_mask_7: 0.4367  loss_dice_7: 3.274  loss_ce_8: 0.3567  loss_mask_8: 0.4369  loss_dice_8: 3.266  time: 1.5063  data_time: 0.0809  lr: 7.105e-06  max_mem: 21589M
[01/18 00:32:39] d2.utils.events INFO:  eta: 11:28:17  iter: 12659  total_loss: 41.1  loss_ce: 0.369  loss_mask: 0.4359  loss_dice: 3.232  loss_ce_0: 0.6175  loss_mask_0: 0.4165  loss_dice_0: 3.36  loss_ce_1: 0.398  loss_mask_1: 0.434  loss_dice_1: 3.26  loss_ce_2: 0.3964  loss_mask_2: 0.4352  loss_dice_2: 3.241  loss_ce_3: 0.3925  loss_mask_3: 0.434  loss_dice_3: 3.234  loss_ce_4: 0.3897  loss_mask_4: 0.4336  loss_dice_4: 3.231  loss_ce_5: 0.3595  loss_mask_5: 0.4327  loss_dice_5: 3.231  loss_ce_6: 0.3705  loss_mask_6: 0.434  loss_dice_6: 3.231  loss_ce_7: 0.3773  loss_mask_7: 0.4334  loss_dice_7: 3.227  loss_ce_8: 0.3689  loss_mask_8: 0.4359  loss_dice_8: 3.229  time: 1.5063  data_time: 0.0767  lr: 7.1003e-06  max_mem: 21589M
[01/18 00:33:08] d2.utils.events INFO:  eta: 11:27:34  iter: 12679  total_loss: 40.23  loss_ce: 0.3426  loss_mask: 0.4456  loss_dice: 3.176  loss_ce_0: 0.6086  loss_mask_0: 0.4199  loss_dice_0: 3.333  loss_ce_1: 0.3426  loss_mask_1: 0.4385  loss_dice_1: 3.231  loss_ce_2: 0.3441  loss_mask_2: 0.4393  loss_dice_2: 3.197  loss_ce_3: 0.3533  loss_mask_3: 0.4432  loss_dice_3: 3.176  loss_ce_4: 0.3348  loss_mask_4: 0.4403  loss_dice_4: 3.175  loss_ce_5: 0.339  loss_mask_5: 0.4416  loss_dice_5: 3.181  loss_ce_6: 0.3465  loss_mask_6: 0.4444  loss_dice_6: 3.17  loss_ce_7: 0.3354  loss_mask_7: 0.4435  loss_dice_7: 3.174  loss_ce_8: 0.3425  loss_mask_8: 0.4457  loss_dice_8: 3.168  time: 1.5062  data_time: 0.0748  lr: 7.0957e-06  max_mem: 21589M
[01/18 00:33:38] d2.utils.events INFO:  eta: 11:26:58  iter: 12699  total_loss: 41.61  loss_ce: 0.367  loss_mask: 0.4422  loss_dice: 3.313  loss_ce_0: 0.6109  loss_mask_0: 0.432  loss_dice_0: 3.426  loss_ce_1: 0.3845  loss_mask_1: 0.445  loss_dice_1: 3.331  loss_ce_2: 0.3764  loss_mask_2: 0.4445  loss_dice_2: 3.318  loss_ce_3: 0.38  loss_mask_3: 0.4399  loss_dice_3: 3.322  loss_ce_4: 0.3865  loss_mask_4: 0.4396  loss_dice_4: 3.31  loss_ce_5: 0.3677  loss_mask_5: 0.4445  loss_dice_5: 3.311  loss_ce_6: 0.3663  loss_mask_6: 0.4423  loss_dice_6: 3.301  loss_ce_7: 0.3745  loss_mask_7: 0.4434  loss_dice_7: 3.307  loss_ce_8: 0.3681  loss_mask_8: 0.4438  loss_dice_8: 3.31  time: 1.5062  data_time: 0.0735  lr: 7.091e-06  max_mem: 21589M
[01/18 00:34:09] d2.utils.events INFO:  eta: 11:26:06  iter: 12719  total_loss: 40.81  loss_ce: 0.3254  loss_mask: 0.4397  loss_dice: 3.264  loss_ce_0: 0.6093  loss_mask_0: 0.4279  loss_dice_0: 3.385  loss_ce_1: 0.3468  loss_mask_1: 0.4453  loss_dice_1: 3.299  loss_ce_2: 0.3581  loss_mask_2: 0.4414  loss_dice_2: 3.282  loss_ce_3: 0.3376  loss_mask_3: 0.4391  loss_dice_3: 3.264  loss_ce_4: 0.3434  loss_mask_4: 0.4377  loss_dice_4: 3.271  loss_ce_5: 0.3313  loss_mask_5: 0.4375  loss_dice_5: 3.273  loss_ce_6: 0.3479  loss_mask_6: 0.4389  loss_dice_6: 3.26  loss_ce_7: 0.3468  loss_mask_7: 0.4398  loss_dice_7: 3.263  loss_ce_8: 0.3341  loss_mask_8: 0.4366  loss_dice_8: 3.26  time: 1.5063  data_time: 0.0787  lr: 7.0863e-06  max_mem: 21589M
[01/18 00:34:39] d2.utils.events INFO:  eta: 11:24:57  iter: 12739  total_loss: 40.64  loss_ce: 0.3238  loss_mask: 0.427  loss_dice: 3.244  loss_ce_0: 0.5852  loss_mask_0: 0.4105  loss_dice_0: 3.37  loss_ce_1: 0.3727  loss_mask_1: 0.4259  loss_dice_1: 3.278  loss_ce_2: 0.3603  loss_mask_2: 0.4252  loss_dice_2: 3.272  loss_ce_3: 0.3741  loss_mask_3: 0.4274  loss_dice_3: 3.232  loss_ce_4: 0.3499  loss_mask_4: 0.4274  loss_dice_4: 3.241  loss_ce_5: 0.337  loss_mask_5: 0.4273  loss_dice_5: 3.247  loss_ce_6: 0.3405  loss_mask_6: 0.4269  loss_dice_6: 3.238  loss_ce_7: 0.3498  loss_mask_7: 0.4269  loss_dice_7: 3.239  loss_ce_8: 0.3362  loss_mask_8: 0.4277  loss_dice_8: 3.245  time: 1.5062  data_time: 0.0809  lr: 7.0816e-06  max_mem: 21589M
[01/18 00:35:08] d2.utils.events INFO:  eta: 11:23:56  iter: 12759  total_loss: 40.4  loss_ce: 0.3554  loss_mask: 0.4378  loss_dice: 3.166  loss_ce_0: 0.5825  loss_mask_0: 0.42  loss_dice_0: 3.302  loss_ce_1: 0.3624  loss_mask_1: 0.4351  loss_dice_1: 3.194  loss_ce_2: 0.3746  loss_mask_2: 0.4354  loss_dice_2: 3.188  loss_ce_3: 0.3651  loss_mask_3: 0.4338  loss_dice_3: 3.165  loss_ce_4: 0.3654  loss_mask_4: 0.4348  loss_dice_4: 3.17  loss_ce_5: 0.3666  loss_mask_5: 0.4354  loss_dice_5: 3.164  loss_ce_6: 0.3633  loss_mask_6: 0.4373  loss_dice_6: 3.167  loss_ce_7: 0.3558  loss_mask_7: 0.4372  loss_dice_7: 3.166  loss_ce_8: 0.3563  loss_mask_8: 0.438  loss_dice_8: 3.163  time: 1.5062  data_time: 0.0717  lr: 7.077e-06  max_mem: 21589M
[01/18 00:35:38] d2.utils.events INFO:  eta: 11:22:45  iter: 12779  total_loss: 40.26  loss_ce: 0.3442  loss_mask: 0.4368  loss_dice: 3.197  loss_ce_0: 0.6006  loss_mask_0: 0.4212  loss_dice_0: 3.328  loss_ce_1: 0.3728  loss_mask_1: 0.4368  loss_dice_1: 3.236  loss_ce_2: 0.3539  loss_mask_2: 0.432  loss_dice_2: 3.218  loss_ce_3: 0.3521  loss_mask_3: 0.4358  loss_dice_3: 3.196  loss_ce_4: 0.3426  loss_mask_4: 0.4339  loss_dice_4: 3.2  loss_ce_5: 0.3432  loss_mask_5: 0.435  loss_dice_5: 3.206  loss_ce_6: 0.347  loss_mask_6: 0.4352  loss_dice_6: 3.195  loss_ce_7: 0.3398  loss_mask_7: 0.4355  loss_dice_7: 3.19  loss_ce_8: 0.3375  loss_mask_8: 0.4373  loss_dice_8: 3.196  time: 1.5061  data_time: 0.0669  lr: 7.0723e-06  max_mem: 21589M
[01/18 00:36:08] d2.utils.events INFO:  eta: 11:21:59  iter: 12799  total_loss: 40.56  loss_ce: 0.3431  loss_mask: 0.4367  loss_dice: 3.205  loss_ce_0: 0.604  loss_mask_0: 0.421  loss_dice_0: 3.342  loss_ce_1: 0.3567  loss_mask_1: 0.4359  loss_dice_1: 3.25  loss_ce_2: 0.3359  loss_mask_2: 0.4351  loss_dice_2: 3.235  loss_ce_3: 0.3242  loss_mask_3: 0.4371  loss_dice_3: 3.211  loss_ce_4: 0.3432  loss_mask_4: 0.4362  loss_dice_4: 3.209  loss_ce_5: 0.3115  loss_mask_5: 0.4359  loss_dice_5: 3.213  loss_ce_6: 0.3216  loss_mask_6: 0.4379  loss_dice_6: 3.209  loss_ce_7: 0.3257  loss_mask_7: 0.4366  loss_dice_7: 3.21  loss_ce_8: 0.3171  loss_mask_8: 0.4354  loss_dice_8: 3.216  time: 1.5061  data_time: 0.0764  lr: 7.0676e-06  max_mem: 21589M
[01/18 00:36:37] d2.utils.events INFO:  eta: 11:21:08  iter: 12819  total_loss: 40.56  loss_ce: 0.3549  loss_mask: 0.4496  loss_dice: 3.18  loss_ce_0: 0.5948  loss_mask_0: 0.4301  loss_dice_0: 3.324  loss_ce_1: 0.3675  loss_mask_1: 0.4504  loss_dice_1: 3.226  loss_ce_2: 0.3868  loss_mask_2: 0.4453  loss_dice_2: 3.204  loss_ce_3: 0.371  loss_mask_3: 0.4482  loss_dice_3: 3.183  loss_ce_4: 0.368  loss_mask_4: 0.4482  loss_dice_4: 3.188  loss_ce_5: 0.3583  loss_mask_5: 0.4487  loss_dice_5: 3.187  loss_ce_6: 0.3601  loss_mask_6: 0.4491  loss_dice_6: 3.188  loss_ce_7: 0.3571  loss_mask_7: 0.4492  loss_dice_7: 3.194  loss_ce_8: 0.349  loss_mask_8: 0.4486  loss_dice_8: 3.191  time: 1.5061  data_time: 0.0731  lr: 7.0629e-06  max_mem: 21589M
[01/18 00:37:08] d2.utils.events INFO:  eta: 11:20:33  iter: 12839  total_loss: 41.63  loss_ce: 0.354  loss_mask: 0.4316  loss_dice: 3.286  loss_ce_0: 0.6061  loss_mask_0: 0.426  loss_dice_0: 3.412  loss_ce_1: 0.3941  loss_mask_1: 0.4414  loss_dice_1: 3.326  loss_ce_2: 0.391  loss_mask_2: 0.4389  loss_dice_2: 3.304  loss_ce_3: 0.367  loss_mask_3: 0.4378  loss_dice_3: 3.288  loss_ce_4: 0.3681  loss_mask_4: 0.4338  loss_dice_4: 3.294  loss_ce_5: 0.3806  loss_mask_5: 0.4311  loss_dice_5: 3.303  loss_ce_6: 0.367  loss_mask_6: 0.4335  loss_dice_6: 3.292  loss_ce_7: 0.362  loss_mask_7: 0.4341  loss_dice_7: 3.291  loss_ce_8: 0.3582  loss_mask_8: 0.4322  loss_dice_8: 3.287  time: 1.5061  data_time: 0.0833  lr: 7.0583e-06  max_mem: 21589M
[01/18 00:37:37] d2.utils.events INFO:  eta: 11:19:19  iter: 12859  total_loss: 40.58  loss_ce: 0.3481  loss_mask: 0.437  loss_dice: 3.197  loss_ce_0: 0.6192  loss_mask_0: 0.4289  loss_dice_0: 3.347  loss_ce_1: 0.4054  loss_mask_1: 0.4464  loss_dice_1: 3.245  loss_ce_2: 0.3829  loss_mask_2: 0.4421  loss_dice_2: 3.218  loss_ce_3: 0.3759  loss_mask_3: 0.4423  loss_dice_3: 3.206  loss_ce_4: 0.3709  loss_mask_4: 0.4392  loss_dice_4: 3.195  loss_ce_5: 0.3582  loss_mask_5: 0.4364  loss_dice_5: 3.197  loss_ce_6: 0.3713  loss_mask_6: 0.4368  loss_dice_6: 3.196  loss_ce_7: 0.3636  loss_mask_7: 0.4362  loss_dice_7: 3.198  loss_ce_8: 0.3589  loss_mask_8: 0.4358  loss_dice_8: 3.189  time: 1.5060  data_time: 0.0678  lr: 7.0536e-06  max_mem: 21589M
[01/18 00:38:07] d2.utils.events INFO:  eta: 11:18:40  iter: 12879  total_loss: 40.54  loss_ce: 0.3281  loss_mask: 0.4397  loss_dice: 3.219  loss_ce_0: 0.5914  loss_mask_0: 0.4229  loss_dice_0: 3.355  loss_ce_1: 0.3404  loss_mask_1: 0.442  loss_dice_1: 3.27  loss_ce_2: 0.348  loss_mask_2: 0.4413  loss_dice_2: 3.241  loss_ce_3: 0.3312  loss_mask_3: 0.4366  loss_dice_3: 3.23  loss_ce_4: 0.3255  loss_mask_4: 0.4387  loss_dice_4: 3.224  loss_ce_5: 0.3281  loss_mask_5: 0.4384  loss_dice_5: 3.234  loss_ce_6: 0.3178  loss_mask_6: 0.4384  loss_dice_6: 3.233  loss_ce_7: 0.3232  loss_mask_7: 0.4372  loss_dice_7: 3.239  loss_ce_8: 0.3144  loss_mask_8: 0.4383  loss_dice_8: 3.235  time: 1.5060  data_time: 0.0664  lr: 7.0489e-06  max_mem: 21589M
[01/18 00:38:36] d2.utils.events INFO:  eta: 11:17:24  iter: 12899  total_loss: 40.61  loss_ce: 0.3577  loss_mask: 0.4442  loss_dice: 3.237  loss_ce_0: 0.5554  loss_mask_0: 0.4312  loss_dice_0: 3.375  loss_ce_1: 0.3352  loss_mask_1: 0.4451  loss_dice_1: 3.286  loss_ce_2: 0.3426  loss_mask_2: 0.4435  loss_dice_2: 3.251  loss_ce_3: 0.3304  loss_mask_3: 0.4469  loss_dice_3: 3.238  loss_ce_4: 0.333  loss_mask_4: 0.4415  loss_dice_4: 3.245  loss_ce_5: 0.3451  loss_mask_5: 0.4425  loss_dice_5: 3.239  loss_ce_6: 0.3341  loss_mask_6: 0.4414  loss_dice_6: 3.219  loss_ce_7: 0.3477  loss_mask_7: 0.4435  loss_dice_7: 3.223  loss_ce_8: 0.3494  loss_mask_8: 0.4421  loss_dice_8: 3.232  time: 1.5059  data_time: 0.0736  lr: 7.0442e-06  max_mem: 21589M
[01/18 00:39:06] d2.utils.events INFO:  eta: 11:16:40  iter: 12919  total_loss: 40.99  loss_ce: 0.3672  loss_mask: 0.4221  loss_dice: 3.2  loss_ce_0: 0.6246  loss_mask_0: 0.4146  loss_dice_0: 3.329  loss_ce_1: 0.3726  loss_mask_1: 0.4272  loss_dice_1: 3.244  loss_ce_2: 0.3768  loss_mask_2: 0.4226  loss_dice_2: 3.212  loss_ce_3: 0.3619  loss_mask_3: 0.4214  loss_dice_3: 3.207  loss_ce_4: 0.3542  loss_mask_4: 0.4213  loss_dice_4: 3.202  loss_ce_5: 0.347  loss_mask_5: 0.4212  loss_dice_5: 3.202  loss_ce_6: 0.3461  loss_mask_6: 0.4223  loss_dice_6: 3.192  loss_ce_7: 0.3617  loss_mask_7: 0.4227  loss_dice_7: 3.2  loss_ce_8: 0.3736  loss_mask_8: 0.4228  loss_dice_8: 3.202  time: 1.5059  data_time: 0.0760  lr: 7.0395e-06  max_mem: 21589M
[01/18 00:39:36] d2.utils.events INFO:  eta: 11:15:36  iter: 12939  total_loss: 40.55  loss_ce: 0.3614  loss_mask: 0.4422  loss_dice: 3.229  loss_ce_0: 0.5823  loss_mask_0: 0.4282  loss_dice_0: 3.373  loss_ce_1: 0.3646  loss_mask_1: 0.4502  loss_dice_1: 3.277  loss_ce_2: 0.3887  loss_mask_2: 0.4472  loss_dice_2: 3.245  loss_ce_3: 0.359  loss_mask_3: 0.4463  loss_dice_3: 3.236  loss_ce_4: 0.3443  loss_mask_4: 0.4442  loss_dice_4: 3.237  loss_ce_5: 0.3517  loss_mask_5: 0.4417  loss_dice_5: 3.24  loss_ce_6: 0.3333  loss_mask_6: 0.4431  loss_dice_6: 3.237  loss_ce_7: 0.3448  loss_mask_7: 0.4443  loss_dice_7: 3.231  loss_ce_8: 0.3468  loss_mask_8: 0.4422  loss_dice_8: 3.227  time: 1.5059  data_time: 0.0744  lr: 7.0349e-06  max_mem: 21589M
[01/18 00:40:06] d2.utils.events INFO:  eta: 11:14:44  iter: 12959  total_loss: 41.2  loss_ce: 0.3357  loss_mask: 0.4371  loss_dice: 3.29  loss_ce_0: 0.5684  loss_mask_0: 0.4262  loss_dice_0: 3.429  loss_ce_1: 0.3814  loss_mask_1: 0.4405  loss_dice_1: 3.326  loss_ce_2: 0.3819  loss_mask_2: 0.4391  loss_dice_2: 3.298  loss_ce_3: 0.3672  loss_mask_3: 0.4373  loss_dice_3: 3.294  loss_ce_4: 0.3564  loss_mask_4: 0.4351  loss_dice_4: 3.295  loss_ce_5: 0.341  loss_mask_5: 0.4371  loss_dice_5: 3.299  loss_ce_6: 0.3483  loss_mask_6: 0.4363  loss_dice_6: 3.288  loss_ce_7: 0.3545  loss_mask_7: 0.4356  loss_dice_7: 3.281  loss_ce_8: 0.35  loss_mask_8: 0.4365  loss_dice_8: 3.277  time: 1.5058  data_time: 0.0732  lr: 7.0302e-06  max_mem: 21589M
[01/18 00:40:36] d2.utils.events INFO:  eta: 11:14:25  iter: 12979  total_loss: 40.48  loss_ce: 0.3401  loss_mask: 0.4243  loss_dice: 3.225  loss_ce_0: 0.5888  loss_mask_0: 0.4122  loss_dice_0: 3.366  loss_ce_1: 0.3457  loss_mask_1: 0.425  loss_dice_1: 3.27  loss_ce_2: 0.3541  loss_mask_2: 0.426  loss_dice_2: 3.246  loss_ce_3: 0.3479  loss_mask_3: 0.4253  loss_dice_3: 3.239  loss_ce_4: 0.3451  loss_mask_4: 0.4235  loss_dice_4: 3.237  loss_ce_5: 0.3453  loss_mask_5: 0.4233  loss_dice_5: 3.239  loss_ce_6: 0.345  loss_mask_6: 0.4261  loss_dice_6: 3.233  loss_ce_7: 0.3371  loss_mask_7: 0.4254  loss_dice_7: 3.233  loss_ce_8: 0.3427  loss_mask_8: 0.4274  loss_dice_8: 3.236  time: 1.5059  data_time: 0.0848  lr: 7.0255e-06  max_mem: 21589M
[01/18 00:41:06] d2.utils.events INFO:  eta: 11:13:22  iter: 12999  total_loss: 40.74  loss_ce: 0.354  loss_mask: 0.4433  loss_dice: 3.234  loss_ce_0: 0.5832  loss_mask_0: 0.4187  loss_dice_0: 3.362  loss_ce_1: 0.361  loss_mask_1: 0.4444  loss_dice_1: 3.269  loss_ce_2: 0.3683  loss_mask_2: 0.4419  loss_dice_2: 3.25  loss_ce_3: 0.3653  loss_mask_3: 0.4457  loss_dice_3: 3.24  loss_ce_4: 0.3543  loss_mask_4: 0.4476  loss_dice_4: 3.231  loss_ce_5: 0.3571  loss_mask_5: 0.445  loss_dice_5: 3.236  loss_ce_6: 0.3642  loss_mask_6: 0.445  loss_dice_6: 3.237  loss_ce_7: 0.3615  loss_mask_7: 0.4443  loss_dice_7: 3.236  loss_ce_8: 0.372  loss_mask_8: 0.4443  loss_dice_8: 3.237  time: 1.5058  data_time: 0.0712  lr: 7.0208e-06  max_mem: 21589M
[01/18 00:41:36] d2.utils.events INFO:  eta: 11:12:52  iter: 13019  total_loss: 40.85  loss_ce: 0.349  loss_mask: 0.4267  loss_dice: 3.214  loss_ce_0: 0.6015  loss_mask_0: 0.4245  loss_dice_0: 3.343  loss_ce_1: 0.349  loss_mask_1: 0.4339  loss_dice_1: 3.25  loss_ce_2: 0.3641  loss_mask_2: 0.43  loss_dice_2: 3.232  loss_ce_3: 0.3406  loss_mask_3: 0.4269  loss_dice_3: 3.22  loss_ce_4: 0.3408  loss_mask_4: 0.4277  loss_dice_4: 3.223  loss_ce_5: 0.3469  loss_mask_5: 0.4284  loss_dice_5: 3.219  loss_ce_6: 0.3336  loss_mask_6: 0.4281  loss_dice_6: 3.222  loss_ce_7: 0.328  loss_mask_7: 0.4282  loss_dice_7: 3.22  loss_ce_8: 0.3322  loss_mask_8: 0.4277  loss_dice_8: 3.225  time: 1.5058  data_time: 0.0737  lr: 7.0161e-06  max_mem: 21589M
[01/18 00:42:05] d2.utils.events INFO:  eta: 11:11:46  iter: 13039  total_loss: 40.62  loss_ce: 0.3466  loss_mask: 0.4594  loss_dice: 3.213  loss_ce_0: 0.5631  loss_mask_0: 0.4328  loss_dice_0: 3.347  loss_ce_1: 0.3751  loss_mask_1: 0.4529  loss_dice_1: 3.248  loss_ce_2: 0.3698  loss_mask_2: 0.451  loss_dice_2: 3.235  loss_ce_3: 0.3443  loss_mask_3: 0.453  loss_dice_3: 3.22  loss_ce_4: 0.3452  loss_mask_4: 0.4551  loss_dice_4: 3.222  loss_ce_5: 0.3383  loss_mask_5: 0.4546  loss_dice_5: 3.217  loss_ce_6: 0.3496  loss_mask_6: 0.4554  loss_dice_6: 3.221  loss_ce_7: 0.3437  loss_mask_7: 0.4566  loss_dice_7: 3.217  loss_ce_8: 0.3497  loss_mask_8: 0.4579  loss_dice_8: 3.227  time: 1.5058  data_time: 0.0716  lr: 7.0115e-06  max_mem: 21589M
[01/18 00:42:35] d2.utils.events INFO:  eta: 11:10:54  iter: 13059  total_loss: 40.37  loss_ce: 0.3503  loss_mask: 0.4353  loss_dice: 3.191  loss_ce_0: 0.6256  loss_mask_0: 0.4225  loss_dice_0: 3.326  loss_ce_1: 0.4009  loss_mask_1: 0.4385  loss_dice_1: 3.228  loss_ce_2: 0.3876  loss_mask_2: 0.4354  loss_dice_2: 3.206  loss_ce_3: 0.365  loss_mask_3: 0.4346  loss_dice_3: 3.195  loss_ce_4: 0.3485  loss_mask_4: 0.4352  loss_dice_4: 3.187  loss_ce_5: 0.3709  loss_mask_5: 0.4322  loss_dice_5: 3.191  loss_ce_6: 0.3492  loss_mask_6: 0.4352  loss_dice_6: 3.189  loss_ce_7: 0.3487  loss_mask_7: 0.4352  loss_dice_7: 3.185  loss_ce_8: 0.3442  loss_mask_8: 0.4343  loss_dice_8: 3.187  time: 1.5057  data_time: 0.0766  lr: 7.0068e-06  max_mem: 21589M
[01/18 00:43:05] d2.utils.events INFO:  eta: 11:10:30  iter: 13079  total_loss: 40.6  loss_ce: 0.3482  loss_mask: 0.4302  loss_dice: 3.23  loss_ce_0: 0.611  loss_mask_0: 0.4193  loss_dice_0: 3.368  loss_ce_1: 0.3586  loss_mask_1: 0.4337  loss_dice_1: 3.276  loss_ce_2: 0.3646  loss_mask_2: 0.4315  loss_dice_2: 3.249  loss_ce_3: 0.3596  loss_mask_3: 0.4273  loss_dice_3: 3.237  loss_ce_4: 0.3423  loss_mask_4: 0.4274  loss_dice_4: 3.236  loss_ce_5: 0.3434  loss_mask_5: 0.428  loss_dice_5: 3.23  loss_ce_6: 0.3421  loss_mask_6: 0.4279  loss_dice_6: 3.23  loss_ce_7: 0.3449  loss_mask_7: 0.429  loss_dice_7: 3.23  loss_ce_8: 0.3353  loss_mask_8: 0.4293  loss_dice_8: 3.233  time: 1.5057  data_time: 0.0760  lr: 7.0021e-06  max_mem: 21589M
[01/18 00:43:34] d2.utils.events INFO:  eta: 11:09:55  iter: 13099  total_loss: 40.41  loss_ce: 0.3418  loss_mask: 0.4424  loss_dice: 3.215  loss_ce_0: 0.5641  loss_mask_0: 0.4299  loss_dice_0: 3.338  loss_ce_1: 0.3512  loss_mask_1: 0.4478  loss_dice_1: 3.255  loss_ce_2: 0.3805  loss_mask_2: 0.4426  loss_dice_2: 3.235  loss_ce_3: 0.3645  loss_mask_3: 0.4443  loss_dice_3: 3.216  loss_ce_4: 0.351  loss_mask_4: 0.4434  loss_dice_4: 3.22  loss_ce_5: 0.3512  loss_mask_5: 0.4409  loss_dice_5: 3.22  loss_ce_6: 0.3461  loss_mask_6: 0.4423  loss_dice_6: 3.217  loss_ce_7: 0.3398  loss_mask_7: 0.4416  loss_dice_7: 3.22  loss_ce_8: 0.3414  loss_mask_8: 0.4418  loss_dice_8: 3.217  time: 1.5056  data_time: 0.0769  lr: 6.9974e-06  max_mem: 21589M
[01/18 00:44:04] d2.utils.events INFO:  eta: 11:09:27  iter: 13119  total_loss: 41.33  loss_ce: 0.3623  loss_mask: 0.4381  loss_dice: 3.246  loss_ce_0: 0.6231  loss_mask_0: 0.4318  loss_dice_0: 3.385  loss_ce_1: 0.3771  loss_mask_1: 0.4492  loss_dice_1: 3.275  loss_ce_2: 0.382  loss_mask_2: 0.4425  loss_dice_2: 3.256  loss_ce_3: 0.3738  loss_mask_3: 0.4414  loss_dice_3: 3.236  loss_ce_4: 0.3561  loss_mask_4: 0.4391  loss_dice_4: 3.244  loss_ce_5: 0.3743  loss_mask_5: 0.4412  loss_dice_5: 3.24  loss_ce_6: 0.377  loss_mask_6: 0.4376  loss_dice_6: 3.238  loss_ce_7: 0.36  loss_mask_7: 0.4396  loss_dice_7: 3.247  loss_ce_8: 0.359  loss_mask_8: 0.4385  loss_dice_8: 3.244  time: 1.5056  data_time: 0.0752  lr: 6.9927e-06  max_mem: 21589M
[01/18 00:44:34] d2.utils.events INFO:  eta: 11:09:00  iter: 13139  total_loss: 40.9  loss_ce: 0.3202  loss_mask: 0.4337  loss_dice: 3.236  loss_ce_0: 0.6188  loss_mask_0: 0.4198  loss_dice_0: 3.363  loss_ce_1: 0.3538  loss_mask_1: 0.4357  loss_dice_1: 3.286  loss_ce_2: 0.3805  loss_mask_2: 0.4325  loss_dice_2: 3.257  loss_ce_3: 0.3413  loss_mask_3: 0.4321  loss_dice_3: 3.245  loss_ce_4: 0.3292  loss_mask_4: 0.4345  loss_dice_4: 3.242  loss_ce_5: 0.3395  loss_mask_5: 0.4349  loss_dice_5: 3.239  loss_ce_6: 0.3279  loss_mask_6: 0.4337  loss_dice_6: 3.246  loss_ce_7: 0.3322  loss_mask_7: 0.4362  loss_dice_7: 3.245  loss_ce_8: 0.3288  loss_mask_8: 0.435  loss_dice_8: 3.242  time: 1.5056  data_time: 0.0855  lr: 6.988e-06  max_mem: 21589M
[01/18 00:45:04] d2.utils.events INFO:  eta: 11:08:27  iter: 13159  total_loss: 40.37  loss_ce: 0.3149  loss_mask: 0.4461  loss_dice: 3.203  loss_ce_0: 0.5976  loss_mask_0: 0.4331  loss_dice_0: 3.345  loss_ce_1: 0.3434  loss_mask_1: 0.4525  loss_dice_1: 3.249  loss_ce_2: 0.3426  loss_mask_2: 0.4464  loss_dice_2: 3.226  loss_ce_3: 0.3206  loss_mask_3: 0.4431  loss_dice_3: 3.211  loss_ce_4: 0.3272  loss_mask_4: 0.4437  loss_dice_4: 3.215  loss_ce_5: 0.3179  loss_mask_5: 0.4416  loss_dice_5: 3.208  loss_ce_6: 0.3178  loss_mask_6: 0.4425  loss_dice_6: 3.201  loss_ce_7: 0.3117  loss_mask_7: 0.4442  loss_dice_7: 3.205  loss_ce_8: 0.3164  loss_mask_8: 0.4443  loss_dice_8: 3.209  time: 1.5056  data_time: 0.0725  lr: 6.9834e-06  max_mem: 21589M
[01/18 00:45:33] d2.utils.events INFO:  eta: 11:07:16  iter: 13179  total_loss: 40.77  loss_ce: 0.3472  loss_mask: 0.4483  loss_dice: 3.221  loss_ce_0: 0.5665  loss_mask_0: 0.441  loss_dice_0: 3.372  loss_ce_1: 0.3625  loss_mask_1: 0.4632  loss_dice_1: 3.262  loss_ce_2: 0.376  loss_mask_2: 0.4558  loss_dice_2: 3.232  loss_ce_3: 0.3472  loss_mask_3: 0.4499  loss_dice_3: 3.226  loss_ce_4: 0.3488  loss_mask_4: 0.4498  loss_dice_4: 3.217  loss_ce_5: 0.3395  loss_mask_5: 0.449  loss_dice_5: 3.22  loss_ce_6: 0.3389  loss_mask_6: 0.4499  loss_dice_6: 3.216  loss_ce_7: 0.3284  loss_mask_7: 0.45  loss_dice_7: 3.227  loss_ce_8: 0.3356  loss_mask_8: 0.4492  loss_dice_8: 3.226  time: 1.5055  data_time: 0.0689  lr: 6.9787e-06  max_mem: 21589M
[01/18 00:46:03] d2.utils.events INFO:  eta: 11:05:31  iter: 13199  total_loss: 39.98  loss_ce: 0.3216  loss_mask: 0.4271  loss_dice: 3.177  loss_ce_0: 0.6016  loss_mask_0: 0.4191  loss_dice_0: 3.321  loss_ce_1: 0.3464  loss_mask_1: 0.4322  loss_dice_1: 3.219  loss_ce_2: 0.3608  loss_mask_2: 0.4292  loss_dice_2: 3.192  loss_ce_3: 0.3491  loss_mask_3: 0.43  loss_dice_3: 3.188  loss_ce_4: 0.3274  loss_mask_4: 0.4309  loss_dice_4: 3.182  loss_ce_5: 0.3287  loss_mask_5: 0.429  loss_dice_5: 3.182  loss_ce_6: 0.3379  loss_mask_6: 0.4293  loss_dice_6: 3.178  loss_ce_7: 0.3154  loss_mask_7: 0.4295  loss_dice_7: 3.177  loss_ce_8: 0.3286  loss_mask_8: 0.4289  loss_dice_8: 3.185  time: 1.5055  data_time: 0.0725  lr: 6.974e-06  max_mem: 21589M
[01/18 00:46:33] d2.utils.events INFO:  eta: 11:04:14  iter: 13219  total_loss: 40.27  loss_ce: 0.3315  loss_mask: 0.4309  loss_dice: 3.233  loss_ce_0: 0.574  loss_mask_0: 0.412  loss_dice_0: 3.369  loss_ce_1: 0.359  loss_mask_1: 0.4335  loss_dice_1: 3.277  loss_ce_2: 0.3718  loss_mask_2: 0.4311  loss_dice_2: 3.242  loss_ce_3: 0.3426  loss_mask_3: 0.4312  loss_dice_3: 3.238  loss_ce_4: 0.3309  loss_mask_4: 0.431  loss_dice_4: 3.236  loss_ce_5: 0.3387  loss_mask_5: 0.4314  loss_dice_5: 3.239  loss_ce_6: 0.325  loss_mask_6: 0.4307  loss_dice_6: 3.243  loss_ce_7: 0.3441  loss_mask_7: 0.43  loss_dice_7: 3.245  loss_ce_8: 0.3352  loss_mask_8: 0.4307  loss_dice_8: 3.245  time: 1.5055  data_time: 0.0754  lr: 6.9693e-06  max_mem: 21589M
[01/18 00:47:03] d2.utils.events INFO:  eta: 11:03:25  iter: 13239  total_loss: 40.66  loss_ce: 0.3281  loss_mask: 0.4355  loss_dice: 3.2  loss_ce_0: 0.5823  loss_mask_0: 0.4219  loss_dice_0: 3.341  loss_ce_1: 0.3625  loss_mask_1: 0.4355  loss_dice_1: 3.245  loss_ce_2: 0.3678  loss_mask_2: 0.4345  loss_dice_2: 3.219  loss_ce_3: 0.3505  loss_mask_3: 0.433  loss_dice_3: 3.2  loss_ce_4: 0.3454  loss_mask_4: 0.4354  loss_dice_4: 3.203  loss_ce_5: 0.3254  loss_mask_5: 0.4355  loss_dice_5: 3.208  loss_ce_6: 0.3292  loss_mask_6: 0.4363  loss_dice_6: 3.204  loss_ce_7: 0.3275  loss_mask_7: 0.4366  loss_dice_7: 3.201  loss_ce_8: 0.324  loss_mask_8: 0.4348  loss_dice_8: 3.203  time: 1.5054  data_time: 0.0830  lr: 6.9646e-06  max_mem: 21589M
[01/18 00:47:33] d2.utils.events INFO:  eta: 11:03:49  iter: 13259  total_loss: 40.04  loss_ce: 0.3285  loss_mask: 0.4294  loss_dice: 3.192  loss_ce_0: 0.5859  loss_mask_0: 0.4173  loss_dice_0: 3.32  loss_ce_1: 0.3424  loss_mask_1: 0.4324  loss_dice_1: 3.226  loss_ce_2: 0.363  loss_mask_2: 0.4306  loss_dice_2: 3.206  loss_ce_3: 0.3379  loss_mask_3: 0.4288  loss_dice_3: 3.187  loss_ce_4: 0.3508  loss_mask_4: 0.4274  loss_dice_4: 3.199  loss_ce_5: 0.3417  loss_mask_5: 0.4278  loss_dice_5: 3.193  loss_ce_6: 0.3268  loss_mask_6: 0.4294  loss_dice_6: 3.196  loss_ce_7: 0.3279  loss_mask_7: 0.4279  loss_dice_7: 3.192  loss_ce_8: 0.3245  loss_mask_8: 0.429  loss_dice_8: 3.191  time: 1.5055  data_time: 0.0780  lr: 6.9599e-06  max_mem: 21589M
[01/18 00:48:03] d2.utils.events INFO:  eta: 11:03:04  iter: 13279  total_loss: 40.45  loss_ce: 0.3696  loss_mask: 0.4445  loss_dice: 3.172  loss_ce_0: 0.6134  loss_mask_0: 0.434  loss_dice_0: 3.285  loss_ce_1: 0.39  loss_mask_1: 0.4519  loss_dice_1: 3.2  loss_ce_2: 0.3929  loss_mask_2: 0.4508  loss_dice_2: 3.176  loss_ce_3: 0.3876  loss_mask_3: 0.4496  loss_dice_3: 3.164  loss_ce_4: 0.3595  loss_mask_4: 0.4468  loss_dice_4: 3.173  loss_ce_5: 0.3786  loss_mask_5: 0.4464  loss_dice_5: 3.159  loss_ce_6: 0.3717  loss_mask_6: 0.4473  loss_dice_6: 3.163  loss_ce_7: 0.3783  loss_mask_7: 0.4484  loss_dice_7: 3.173  loss_ce_8: 0.3632  loss_mask_8: 0.4457  loss_dice_8: 3.174  time: 1.5055  data_time: 0.0811  lr: 6.9553e-06  max_mem: 21589M
[01/18 00:48:33] d2.utils.events INFO:  eta: 11:02:54  iter: 13299  total_loss: 40.82  loss_ce: 0.3523  loss_mask: 0.4269  loss_dice: 3.214  loss_ce_0: 0.5784  loss_mask_0: 0.42  loss_dice_0: 3.348  loss_ce_1: 0.3709  loss_mask_1: 0.4364  loss_dice_1: 3.242  loss_ce_2: 0.3907  loss_mask_2: 0.4301  loss_dice_2: 3.235  loss_ce_3: 0.3569  loss_mask_3: 0.4308  loss_dice_3: 3.221  loss_ce_4: 0.3536  loss_mask_4: 0.4281  loss_dice_4: 3.215  loss_ce_5: 0.3386  loss_mask_5: 0.4297  loss_dice_5: 3.219  loss_ce_6: 0.3412  loss_mask_6: 0.4295  loss_dice_6: 3.214  loss_ce_7: 0.3487  loss_mask_7: 0.4265  loss_dice_7: 3.217  loss_ce_8: 0.3563  loss_mask_8: 0.4269  loss_dice_8: 3.219  time: 1.5055  data_time: 0.0773  lr: 6.9506e-06  max_mem: 21589M
[01/18 00:49:03] d2.utils.events INFO:  eta: 11:02:11  iter: 13319  total_loss: 40.67  loss_ce: 0.3518  loss_mask: 0.4442  loss_dice: 3.208  loss_ce_0: 0.5753  loss_mask_0: 0.425  loss_dice_0: 3.345  loss_ce_1: 0.359  loss_mask_1: 0.4453  loss_dice_1: 3.246  loss_ce_2: 0.3603  loss_mask_2: 0.4435  loss_dice_2: 3.23  loss_ce_3: 0.3537  loss_mask_3: 0.4416  loss_dice_3: 3.216  loss_ce_4: 0.3551  loss_mask_4: 0.4428  loss_dice_4: 3.206  loss_ce_5: 0.3422  loss_mask_5: 0.4417  loss_dice_5: 3.212  loss_ce_6: 0.3458  loss_mask_6: 0.4411  loss_dice_6: 3.207  loss_ce_7: 0.3447  loss_mask_7: 0.4433  loss_dice_7: 3.203  loss_ce_8: 0.3423  loss_mask_8: 0.4437  loss_dice_8: 3.211  time: 1.5054  data_time: 0.0916  lr: 6.9459e-06  max_mem: 21589M
[01/18 00:49:34] d2.utils.events INFO:  eta: 11:01:50  iter: 13339  total_loss: 40.69  loss_ce: 0.3278  loss_mask: 0.4448  loss_dice: 3.244  loss_ce_0: 0.5897  loss_mask_0: 0.4383  loss_dice_0: 3.35  loss_ce_1: 0.3408  loss_mask_1: 0.4521  loss_dice_1: 3.261  loss_ce_2: 0.3489  loss_mask_2: 0.4499  loss_dice_2: 3.242  loss_ce_3: 0.3406  loss_mask_3: 0.4472  loss_dice_3: 3.229  loss_ce_4: 0.3354  loss_mask_4: 0.4471  loss_dice_4: 3.232  loss_ce_5: 0.3364  loss_mask_5: 0.4465  loss_dice_5: 3.239  loss_ce_6: 0.3448  loss_mask_6: 0.4467  loss_dice_6: 3.237  loss_ce_7: 0.3304  loss_mask_7: 0.4481  loss_dice_7: 3.235  loss_ce_8: 0.3295  loss_mask_8: 0.4462  loss_dice_8: 3.233  time: 1.5054  data_time: 0.0805  lr: 6.9412e-06  max_mem: 21589M
[01/18 00:50:03] d2.utils.events INFO:  eta: 11:01:05  iter: 13359  total_loss: 40.5  loss_ce: 0.3386  loss_mask: 0.4293  loss_dice: 3.207  loss_ce_0: 0.6101  loss_mask_0: 0.4192  loss_dice_0: 3.337  loss_ce_1: 0.3454  loss_mask_1: 0.4377  loss_dice_1: 3.255  loss_ce_2: 0.3757  loss_mask_2: 0.4322  loss_dice_2: 3.236  loss_ce_3: 0.3533  loss_mask_3: 0.43  loss_dice_3: 3.218  loss_ce_4: 0.3562  loss_mask_4: 0.4276  loss_dice_4: 3.216  loss_ce_5: 0.3556  loss_mask_5: 0.4292  loss_dice_5: 3.217  loss_ce_6: 0.3487  loss_mask_6: 0.4297  loss_dice_6: 3.223  loss_ce_7: 0.3336  loss_mask_7: 0.4278  loss_dice_7: 3.215  loss_ce_8: 0.342  loss_mask_8: 0.4279  loss_dice_8: 3.219  time: 1.5054  data_time: 0.0672  lr: 6.9365e-06  max_mem: 21589M
[01/18 00:50:33] d2.utils.events INFO:  eta: 11:00:45  iter: 13379  total_loss: 40.2  loss_ce: 0.3427  loss_mask: 0.4273  loss_dice: 3.201  loss_ce_0: 0.5993  loss_mask_0: 0.4184  loss_dice_0: 3.333  loss_ce_1: 0.3566  loss_mask_1: 0.4287  loss_dice_1: 3.239  loss_ce_2: 0.3701  loss_mask_2: 0.427  loss_dice_2: 3.207  loss_ce_3: 0.3591  loss_mask_3: 0.4262  loss_dice_3: 3.212  loss_ce_4: 0.3542  loss_mask_4: 0.4257  loss_dice_4: 3.198  loss_ce_5: 0.3499  loss_mask_5: 0.4263  loss_dice_5: 3.204  loss_ce_6: 0.3382  loss_mask_6: 0.425  loss_dice_6: 3.203  loss_ce_7: 0.3331  loss_mask_7: 0.4267  loss_dice_7: 3.202  loss_ce_8: 0.35  loss_mask_8: 0.4286  loss_dice_8: 3.203  time: 1.5054  data_time: 0.0762  lr: 6.9318e-06  max_mem: 21589M
[01/18 00:51:04] d2.utils.events INFO:  eta: 11:00:25  iter: 13399  total_loss: 40.58  loss_ce: 0.3616  loss_mask: 0.4413  loss_dice: 3.189  loss_ce_0: 0.5819  loss_mask_0: 0.4269  loss_dice_0: 3.325  loss_ce_1: 0.3624  loss_mask_1: 0.4485  loss_dice_1: 3.23  loss_ce_2: 0.3651  loss_mask_2: 0.4449  loss_dice_2: 3.212  loss_ce_3: 0.3518  loss_mask_3: 0.4435  loss_dice_3: 3.202  loss_ce_4: 0.3666  loss_mask_4: 0.44  loss_dice_4: 3.198  loss_ce_5: 0.3468  loss_mask_5: 0.441  loss_dice_5: 3.197  loss_ce_6: 0.3496  loss_mask_6: 0.4401  loss_dice_6: 3.194  loss_ce_7: 0.3534  loss_mask_7: 0.4432  loss_dice_7: 3.195  loss_ce_8: 0.3442  loss_mask_8: 0.4411  loss_dice_8: 3.195  time: 1.5054  data_time: 0.0777  lr: 6.9271e-06  max_mem: 21589M
[01/18 00:51:33] d2.utils.events INFO:  eta: 10:59:17  iter: 13419  total_loss: 40.66  loss_ce: 0.3676  loss_mask: 0.4371  loss_dice: 3.217  loss_ce_0: 0.6089  loss_mask_0: 0.4355  loss_dice_0: 3.346  loss_ce_1: 0.3788  loss_mask_1: 0.4426  loss_dice_1: 3.252  loss_ce_2: 0.3859  loss_mask_2: 0.4366  loss_dice_2: 3.235  loss_ce_3: 0.3668  loss_mask_3: 0.4388  loss_dice_3: 3.222  loss_ce_4: 0.3552  loss_mask_4: 0.4384  loss_dice_4: 3.217  loss_ce_5: 0.3633  loss_mask_5: 0.4431  loss_dice_5: 3.221  loss_ce_6: 0.3575  loss_mask_6: 0.4404  loss_dice_6: 3.222  loss_ce_7: 0.3626  loss_mask_7: 0.438  loss_dice_7: 3.209  loss_ce_8: 0.3613  loss_mask_8: 0.4393  loss_dice_8: 3.217  time: 1.5054  data_time: 0.0846  lr: 6.9225e-06  max_mem: 21589M
[01/18 00:52:03] d2.utils.events INFO:  eta: 10:57:52  iter: 13439  total_loss: 40.1  loss_ce: 0.3375  loss_mask: 0.4489  loss_dice: 3.14  loss_ce_0: 0.5795  loss_mask_0: 0.4294  loss_dice_0: 3.286  loss_ce_1: 0.3621  loss_mask_1: 0.4464  loss_dice_1: 3.187  loss_ce_2: 0.3611  loss_mask_2: 0.4438  loss_dice_2: 3.163  loss_ce_3: 0.3586  loss_mask_3: 0.4417  loss_dice_3: 3.159  loss_ce_4: 0.3306  loss_mask_4: 0.4411  loss_dice_4: 3.157  loss_ce_5: 0.3213  loss_mask_5: 0.4416  loss_dice_5: 3.155  loss_ce_6: 0.3325  loss_mask_6: 0.4467  loss_dice_6: 3.145  loss_ce_7: 0.3195  loss_mask_7: 0.4451  loss_dice_7: 3.156  loss_ce_8: 0.332  loss_mask_8: 0.4433  loss_dice_8: 3.145  time: 1.5054  data_time: 0.0661  lr: 6.9178e-06  max_mem: 21589M
[01/18 00:52:33] d2.utils.events INFO:  eta: 10:57:23  iter: 13459  total_loss: 40.55  loss_ce: 0.3484  loss_mask: 0.4307  loss_dice: 3.214  loss_ce_0: 0.5778  loss_mask_0: 0.422  loss_dice_0: 3.351  loss_ce_1: 0.354  loss_mask_1: 0.4325  loss_dice_1: 3.255  loss_ce_2: 0.3439  loss_mask_2: 0.432  loss_dice_2: 3.233  loss_ce_3: 0.3386  loss_mask_3: 0.4315  loss_dice_3: 3.226  loss_ce_4: 0.3454  loss_mask_4: 0.4322  loss_dice_4: 3.222  loss_ce_5: 0.3368  loss_mask_5: 0.4316  loss_dice_5: 3.218  loss_ce_6: 0.3313  loss_mask_6: 0.4276  loss_dice_6: 3.217  loss_ce_7: 0.3243  loss_mask_7: 0.4305  loss_dice_7: 3.22  loss_ce_8: 0.3333  loss_mask_8: 0.4305  loss_dice_8: 3.216  time: 1.5053  data_time: 0.0757  lr: 6.9131e-06  max_mem: 21589M
[01/18 00:53:03] d2.utils.events INFO:  eta: 10:57:10  iter: 13479  total_loss: 39.85  loss_ce: 0.3391  loss_mask: 0.4308  loss_dice: 3.141  loss_ce_0: 0.6232  loss_mask_0: 0.4209  loss_dice_0: 3.284  loss_ce_1: 0.3655  loss_mask_1: 0.4376  loss_dice_1: 3.194  loss_ce_2: 0.3698  loss_mask_2: 0.4358  loss_dice_2: 3.17  loss_ce_3: 0.3541  loss_mask_3: 0.4338  loss_dice_3: 3.157  loss_ce_4: 0.3513  loss_mask_4: 0.4323  loss_dice_4: 3.15  loss_ce_5: 0.3416  loss_mask_5: 0.4322  loss_dice_5: 3.157  loss_ce_6: 0.3569  loss_mask_6: 0.4314  loss_dice_6: 3.145  loss_ce_7: 0.3439  loss_mask_7: 0.4315  loss_dice_7: 3.156  loss_ce_8: 0.3495  loss_mask_8: 0.4322  loss_dice_8: 3.155  time: 1.5053  data_time: 0.0741  lr: 6.9084e-06  max_mem: 21589M
[01/18 00:53:33] d2.utils.events INFO:  eta: 10:56:47  iter: 13499  total_loss: 40.18  loss_ce: 0.3466  loss_mask: 0.4393  loss_dice: 3.207  loss_ce_0: 0.5461  loss_mask_0: 0.4296  loss_dice_0: 3.332  loss_ce_1: 0.3454  loss_mask_1: 0.4417  loss_dice_1: 3.248  loss_ce_2: 0.3535  loss_mask_2: 0.4392  loss_dice_2: 3.231  loss_ce_3: 0.3428  loss_mask_3: 0.4396  loss_dice_3: 3.222  loss_ce_4: 0.3363  loss_mask_4: 0.4392  loss_dice_4: 3.221  loss_ce_5: 0.3217  loss_mask_5: 0.4392  loss_dice_5: 3.221  loss_ce_6: 0.3448  loss_mask_6: 0.4379  loss_dice_6: 3.224  loss_ce_7: 0.331  loss_mask_7: 0.4405  loss_dice_7: 3.221  loss_ce_8: 0.3377  loss_mask_8: 0.4407  loss_dice_8: 3.213  time: 1.5053  data_time: 0.0735  lr: 6.9037e-06  max_mem: 21589M
[01/18 00:54:04] d2.utils.events INFO:  eta: 10:56:17  iter: 13519  total_loss: 40.43  loss_ce: 0.3548  loss_mask: 0.4327  loss_dice: 3.201  loss_ce_0: 0.5955  loss_mask_0: 0.4241  loss_dice_0: 3.339  loss_ce_1: 0.3751  loss_mask_1: 0.4371  loss_dice_1: 3.242  loss_ce_2: 0.3669  loss_mask_2: 0.4346  loss_dice_2: 3.209  loss_ce_3: 0.3685  loss_mask_3: 0.4353  loss_dice_3: 3.201  loss_ce_4: 0.3553  loss_mask_4: 0.4332  loss_dice_4: 3.21  loss_ce_5: 0.3528  loss_mask_5: 0.4343  loss_dice_5: 3.202  loss_ce_6: 0.3556  loss_mask_6: 0.4335  loss_dice_6: 3.209  loss_ce_7: 0.3494  loss_mask_7: 0.4329  loss_dice_7: 3.207  loss_ce_8: 0.3469  loss_mask_8: 0.4317  loss_dice_8: 3.197  time: 1.5054  data_time: 0.0783  lr: 6.899e-06  max_mem: 21589M
[01/18 00:54:34] d2.utils.events INFO:  eta: 10:54:57  iter: 13539  total_loss: 39.47  loss_ce: 0.3234  loss_mask: 0.4331  loss_dice: 3.116  loss_ce_0: 0.6003  loss_mask_0: 0.4137  loss_dice_0: 3.255  loss_ce_1: 0.3535  loss_mask_1: 0.4352  loss_dice_1: 3.155  loss_ce_2: 0.346  loss_mask_2: 0.4323  loss_dice_2: 3.133  loss_ce_3: 0.3369  loss_mask_3: 0.4325  loss_dice_3: 3.126  loss_ce_4: 0.3199  loss_mask_4: 0.4323  loss_dice_4: 3.124  loss_ce_5: 0.3083  loss_mask_5: 0.4334  loss_dice_5: 3.119  loss_ce_6: 0.33  loss_mask_6: 0.4338  loss_dice_6: 3.123  loss_ce_7: 0.3211  loss_mask_7: 0.4326  loss_dice_7: 3.118  loss_ce_8: 0.3129  loss_mask_8: 0.4329  loss_dice_8: 3.122  time: 1.5053  data_time: 0.0647  lr: 6.8943e-06  max_mem: 21589M
[01/18 00:55:03] d2.utils.events INFO:  eta: 10:54:07  iter: 13559  total_loss: 39.55  loss_ce: 0.3376  loss_mask: 0.4289  loss_dice: 3.125  loss_ce_0: 0.5976  loss_mask_0: 0.4084  loss_dice_0: 3.263  loss_ce_1: 0.351  loss_mask_1: 0.4236  loss_dice_1: 3.171  loss_ce_2: 0.3541  loss_mask_2: 0.4255  loss_dice_2: 3.144  loss_ce_3: 0.331  loss_mask_3: 0.4288  loss_dice_3: 3.134  loss_ce_4: 0.3409  loss_mask_4: 0.4292  loss_dice_4: 3.13  loss_ce_5: 0.3333  loss_mask_5: 0.4278  loss_dice_5: 3.121  loss_ce_6: 0.3246  loss_mask_6: 0.4276  loss_dice_6: 3.133  loss_ce_7: 0.3186  loss_mask_7: 0.4291  loss_dice_7: 3.133  loss_ce_8: 0.3306  loss_mask_8: 0.4304  loss_dice_8: 3.133  time: 1.5053  data_time: 0.0797  lr: 6.8896e-06  max_mem: 21589M
[01/18 00:55:32] d2.utils.events INFO:  eta: 10:53:47  iter: 13579  total_loss: 40.09  loss_ce: 0.3166  loss_mask: 0.4242  loss_dice: 3.153  loss_ce_0: 0.564  loss_mask_0: 0.4129  loss_dice_0: 3.296  loss_ce_1: 0.3394  loss_mask_1: 0.431  loss_dice_1: 3.196  loss_ce_2: 0.3252  loss_mask_2: 0.4254  loss_dice_2: 3.163  loss_ce_3: 0.3138  loss_mask_3: 0.4271  loss_dice_3: 3.157  loss_ce_4: 0.307  loss_mask_4: 0.4278  loss_dice_4: 3.154  loss_ce_5: 0.3021  loss_mask_5: 0.4258  loss_dice_5: 3.154  loss_ce_6: 0.3053  loss_mask_6: 0.4263  loss_dice_6: 3.145  loss_ce_7: 0.3274  loss_mask_7: 0.4254  loss_dice_7: 3.155  loss_ce_8: 0.3136  loss_mask_8: 0.4274  loss_dice_8: 3.15  time: 1.5052  data_time: 0.0641  lr: 6.8849e-06  max_mem: 21589M
[01/18 00:56:02] d2.utils.events INFO:  eta: 10:53:26  iter: 13599  total_loss: 40.64  loss_ce: 0.3573  loss_mask: 0.4168  loss_dice: 3.258  loss_ce_0: 0.6575  loss_mask_0: 0.4087  loss_dice_0: 3.368  loss_ce_1: 0.3796  loss_mask_1: 0.4215  loss_dice_1: 3.289  loss_ce_2: 0.3695  loss_mask_2: 0.4217  loss_dice_2: 3.271  loss_ce_3: 0.3592  loss_mask_3: 0.4194  loss_dice_3: 3.255  loss_ce_4: 0.3466  loss_mask_4: 0.4195  loss_dice_4: 3.258  loss_ce_5: 0.3443  loss_mask_5: 0.4204  loss_dice_5: 3.256  loss_ce_6: 0.3509  loss_mask_6: 0.4203  loss_dice_6: 3.257  loss_ce_7: 0.3419  loss_mask_7: 0.4196  loss_dice_7: 3.251  loss_ce_8: 0.3519  loss_mask_8: 0.4178  loss_dice_8: 3.255  time: 1.5052  data_time: 0.0691  lr: 6.8803e-06  max_mem: 21589M
[01/18 00:56:32] d2.utils.events INFO:  eta: 10:52:48  iter: 13619  total_loss: 39.58  loss_ce: 0.3421  loss_mask: 0.4234  loss_dice: 3.148  loss_ce_0: 0.5954  loss_mask_0: 0.4114  loss_dice_0: 3.311  loss_ce_1: 0.3491  loss_mask_1: 0.4241  loss_dice_1: 3.212  loss_ce_2: 0.3687  loss_mask_2: 0.4229  loss_dice_2: 3.178  loss_ce_3: 0.352  loss_mask_3: 0.4214  loss_dice_3: 3.166  loss_ce_4: 0.3457  loss_mask_4: 0.4219  loss_dice_4: 3.159  loss_ce_5: 0.3402  loss_mask_5: 0.4227  loss_dice_5: 3.162  loss_ce_6: 0.3271  loss_mask_6: 0.4224  loss_dice_6: 3.15  loss_ce_7: 0.3413  loss_mask_7: 0.4237  loss_dice_7: 3.149  loss_ce_8: 0.3256  loss_mask_8: 0.4227  loss_dice_8: 3.155  time: 1.5051  data_time: 0.0661  lr: 6.8756e-06  max_mem: 21589M
[01/18 00:57:01] d2.utils.events INFO:  eta: 10:52:13  iter: 13639  total_loss: 40.97  loss_ce: 0.3729  loss_mask: 0.427  loss_dice: 3.267  loss_ce_0: 0.6284  loss_mask_0: 0.4195  loss_dice_0: 3.378  loss_ce_1: 0.3912  loss_mask_1: 0.435  loss_dice_1: 3.301  loss_ce_2: 0.3719  loss_mask_2: 0.4313  loss_dice_2: 3.278  loss_ce_3: 0.3658  loss_mask_3: 0.4315  loss_dice_3: 3.266  loss_ce_4: 0.3565  loss_mask_4: 0.431  loss_dice_4: 3.266  loss_ce_5: 0.3502  loss_mask_5: 0.4319  loss_dice_5: 3.27  loss_ce_6: 0.3756  loss_mask_6: 0.4307  loss_dice_6: 3.256  loss_ce_7: 0.3576  loss_mask_7: 0.4276  loss_dice_7: 3.257  loss_ce_8: 0.3652  loss_mask_8: 0.4265  loss_dice_8: 3.254  time: 1.5051  data_time: 0.0789  lr: 6.8709e-06  max_mem: 21589M
[01/18 00:57:31] d2.utils.events INFO:  eta: 10:51:35  iter: 13659  total_loss: 39.86  loss_ce: 0.3578  loss_mask: 0.4334  loss_dice: 3.162  loss_ce_0: 0.5869  loss_mask_0: 0.4235  loss_dice_0: 3.302  loss_ce_1: 0.373  loss_mask_1: 0.4394  loss_dice_1: 3.214  loss_ce_2: 0.3825  loss_mask_2: 0.4354  loss_dice_2: 3.187  loss_ce_3: 0.353  loss_mask_3: 0.4371  loss_dice_3: 3.171  loss_ce_4: 0.35  loss_mask_4: 0.4344  loss_dice_4: 3.175  loss_ce_5: 0.3474  loss_mask_5: 0.4346  loss_dice_5: 3.167  loss_ce_6: 0.3354  loss_mask_6: 0.4355  loss_dice_6: 3.17  loss_ce_7: 0.3471  loss_mask_7: 0.4335  loss_dice_7: 3.167  loss_ce_8: 0.3451  loss_mask_8: 0.4353  loss_dice_8: 3.162  time: 1.5050  data_time: 0.0826  lr: 6.8662e-06  max_mem: 21589M
[01/18 00:58:01] d2.utils.events INFO:  eta: 10:51:05  iter: 13679  total_loss: 40.21  loss_ce: 0.3359  loss_mask: 0.4326  loss_dice: 3.192  loss_ce_0: 0.5709  loss_mask_0: 0.4199  loss_dice_0: 3.325  loss_ce_1: 0.3481  loss_mask_1: 0.436  loss_dice_1: 3.227  loss_ce_2: 0.3489  loss_mask_2: 0.4377  loss_dice_2: 3.201  loss_ce_3: 0.3398  loss_mask_3: 0.4356  loss_dice_3: 3.199  loss_ce_4: 0.3373  loss_mask_4: 0.4328  loss_dice_4: 3.194  loss_ce_5: 0.3268  loss_mask_5: 0.4313  loss_dice_5: 3.202  loss_ce_6: 0.3343  loss_mask_6: 0.4331  loss_dice_6: 3.184  loss_ce_7: 0.3429  loss_mask_7: 0.4322  loss_dice_7: 3.196  loss_ce_8: 0.3301  loss_mask_8: 0.4325  loss_dice_8: 3.189  time: 1.5050  data_time: 0.0668  lr: 6.8615e-06  max_mem: 21589M
[01/18 00:58:30] d2.utils.events INFO:  eta: 10:50:22  iter: 13699  total_loss: 40.18  loss_ce: 0.3416  loss_mask: 0.4283  loss_dice: 3.164  loss_ce_0: 0.6064  loss_mask_0: 0.4192  loss_dice_0: 3.306  loss_ce_1: 0.35  loss_mask_1: 0.4391  loss_dice_1: 3.211  loss_ce_2: 0.3548  loss_mask_2: 0.438  loss_dice_2: 3.186  loss_ce_3: 0.3497  loss_mask_3: 0.4337  loss_dice_3: 3.17  loss_ce_4: 0.3391  loss_mask_4: 0.4323  loss_dice_4: 3.166  loss_ce_5: 0.3232  loss_mask_5: 0.435  loss_dice_5: 3.18  loss_ce_6: 0.3276  loss_mask_6: 0.4339  loss_dice_6: 3.177  loss_ce_7: 0.3222  loss_mask_7: 0.4311  loss_dice_7: 3.164  loss_ce_8: 0.3315  loss_mask_8: 0.4316  loss_dice_8: 3.167  time: 1.5050  data_time: 0.0770  lr: 6.8568e-06  max_mem: 21589M
[01/18 00:59:00] d2.utils.events INFO:  eta: 10:49:50  iter: 13719  total_loss: 39.49  loss_ce: 0.3591  loss_mask: 0.4265  loss_dice: 3.124  loss_ce_0: 0.6184  loss_mask_0: 0.42  loss_dice_0: 3.262  loss_ce_1: 0.3679  loss_mask_1: 0.4314  loss_dice_1: 3.168  loss_ce_2: 0.3496  loss_mask_2: 0.4285  loss_dice_2: 3.143  loss_ce_3: 0.3532  loss_mask_3: 0.4278  loss_dice_3: 3.135  loss_ce_4: 0.3474  loss_mask_4: 0.4273  loss_dice_4: 3.132  loss_ce_5: 0.3484  loss_mask_5: 0.4264  loss_dice_5: 3.133  loss_ce_6: 0.3436  loss_mask_6: 0.4281  loss_dice_6: 3.132  loss_ce_7: 0.3438  loss_mask_7: 0.4256  loss_dice_7: 3.142  loss_ce_8: 0.3427  loss_mask_8: 0.4256  loss_dice_8: 3.138  time: 1.5049  data_time: 0.0691  lr: 6.8521e-06  max_mem: 21589M
[01/18 00:59:30] d2.utils.events INFO:  eta: 10:49:20  iter: 13739  total_loss: 40.32  loss_ce: 0.3468  loss_mask: 0.4286  loss_dice: 3.179  loss_ce_0: 0.634  loss_mask_0: 0.4212  loss_dice_0: 3.313  loss_ce_1: 0.3908  loss_mask_1: 0.4355  loss_dice_1: 3.22  loss_ce_2: 0.3862  loss_mask_2: 0.4307  loss_dice_2: 3.199  loss_ce_3: 0.355  loss_mask_3: 0.4337  loss_dice_3: 3.188  loss_ce_4: 0.3601  loss_mask_4: 0.4294  loss_dice_4: 3.193  loss_ce_5: 0.3478  loss_mask_5: 0.429  loss_dice_5: 3.194  loss_ce_6: 0.3473  loss_mask_6: 0.4316  loss_dice_6: 3.181  loss_ce_7: 0.3497  loss_mask_7: 0.4298  loss_dice_7: 3.177  loss_ce_8: 0.3581  loss_mask_8: 0.4281  loss_dice_8: 3.182  time: 1.5049  data_time: 0.0704  lr: 6.8474e-06  max_mem: 21589M
[01/18 00:59:59] d2.utils.events INFO:  eta: 10:48:50  iter: 13759  total_loss: 40.21  loss_ce: 0.3556  loss_mask: 0.4367  loss_dice: 3.201  loss_ce_0: 0.6059  loss_mask_0: 0.4194  loss_dice_0: 3.335  loss_ce_1: 0.3745  loss_mask_1: 0.4335  loss_dice_1: 3.239  loss_ce_2: 0.3771  loss_mask_2: 0.4313  loss_dice_2: 3.216  loss_ce_3: 0.3627  loss_mask_3: 0.4367  loss_dice_3: 3.198  loss_ce_4: 0.3617  loss_mask_4: 0.4391  loss_dice_4: 3.207  loss_ce_5: 0.3732  loss_mask_5: 0.4386  loss_dice_5: 3.206  loss_ce_6: 0.3486  loss_mask_6: 0.4376  loss_dice_6: 3.205  loss_ce_7: 0.3521  loss_mask_7: 0.4363  loss_dice_7: 3.201  loss_ce_8: 0.3567  loss_mask_8: 0.4337  loss_dice_8: 3.209  time: 1.5049  data_time: 0.0746  lr: 6.8427e-06  max_mem: 21589M
[01/18 01:00:30] d2.utils.events INFO:  eta: 10:48:31  iter: 13779  total_loss: 39.93  loss_ce: 0.3273  loss_mask: 0.4261  loss_dice: 3.18  loss_ce_0: 0.6167  loss_mask_0: 0.4137  loss_dice_0: 3.293  loss_ce_1: 0.3597  loss_mask_1: 0.4329  loss_dice_1: 3.201  loss_ce_2: 0.3526  loss_mask_2: 0.4291  loss_dice_2: 3.184  loss_ce_3: 0.336  loss_mask_3: 0.4274  loss_dice_3: 3.172  loss_ce_4: 0.3363  loss_mask_4: 0.4251  loss_dice_4: 3.173  loss_ce_5: 0.3258  loss_mask_5: 0.4247  loss_dice_5: 3.18  loss_ce_6: 0.338  loss_mask_6: 0.4259  loss_dice_6: 3.169  loss_ce_7: 0.3339  loss_mask_7: 0.4249  loss_dice_7: 3.178  loss_ce_8: 0.3358  loss_mask_8: 0.4259  loss_dice_8: 3.181  time: 1.5049  data_time: 0.0838  lr: 6.838e-06  max_mem: 21589M
[01/18 01:00:59] d2.utils.events INFO:  eta: 10:47:56  iter: 13799  total_loss: 40.61  loss_ce: 0.3438  loss_mask: 0.4312  loss_dice: 3.226  loss_ce_0: 0.6026  loss_mask_0: 0.4326  loss_dice_0: 3.352  loss_ce_1: 0.3544  loss_mask_1: 0.4387  loss_dice_1: 3.255  loss_ce_2: 0.3624  loss_mask_2: 0.4371  loss_dice_2: 3.245  loss_ce_3: 0.3444  loss_mask_3: 0.4332  loss_dice_3: 3.227  loss_ce_4: 0.3573  loss_mask_4: 0.4325  loss_dice_4: 3.224  loss_ce_5: 0.3539  loss_mask_5: 0.4303  loss_dice_5: 3.229  loss_ce_6: 0.3335  loss_mask_6: 0.4325  loss_dice_6: 3.222  loss_ce_7: 0.3418  loss_mask_7: 0.4304  loss_dice_7: 3.224  loss_ce_8: 0.3441  loss_mask_8: 0.433  loss_dice_8: 3.227  time: 1.5049  data_time: 0.0701  lr: 6.8333e-06  max_mem: 21589M
[01/18 01:01:29] d2.utils.events INFO:  eta: 10:47:31  iter: 13819  total_loss: 39.63  loss_ce: 0.3401  loss_mask: 0.4271  loss_dice: 3.149  loss_ce_0: 0.5892  loss_mask_0: 0.4086  loss_dice_0: 3.271  loss_ce_1: 0.3486  loss_mask_1: 0.4265  loss_dice_1: 3.172  loss_ce_2: 0.361  loss_mask_2: 0.4241  loss_dice_2: 3.165  loss_ce_3: 0.3422  loss_mask_3: 0.4253  loss_dice_3: 3.147  loss_ce_4: 0.3352  loss_mask_4: 0.4213  loss_dice_4: 3.144  loss_ce_5: 0.333  loss_mask_5: 0.4245  loss_dice_5: 3.151  loss_ce_6: 0.3245  loss_mask_6: 0.4248  loss_dice_6: 3.145  loss_ce_7: 0.3436  loss_mask_7: 0.4255  loss_dice_7: 3.146  loss_ce_8: 0.3439  loss_mask_8: 0.4255  loss_dice_8: 3.154  time: 1.5048  data_time: 0.0800  lr: 6.8286e-06  max_mem: 21589M
[01/18 01:01:59] d2.utils.events INFO:  eta: 10:46:26  iter: 13839  total_loss: 40.25  loss_ce: 0.3409  loss_mask: 0.4308  loss_dice: 3.184  loss_ce_0: 0.6038  loss_mask_0: 0.4126  loss_dice_0: 3.314  loss_ce_1: 0.382  loss_mask_1: 0.4316  loss_dice_1: 3.221  loss_ce_2: 0.3754  loss_mask_2: 0.4307  loss_dice_2: 3.185  loss_ce_3: 0.349  loss_mask_3: 0.4296  loss_dice_3: 3.191  loss_ce_4: 0.3433  loss_mask_4: 0.4294  loss_dice_4: 3.193  loss_ce_5: 0.3499  loss_mask_5: 0.4289  loss_dice_5: 3.192  loss_ce_6: 0.3601  loss_mask_6: 0.4308  loss_dice_6: 3.186  loss_ce_7: 0.3479  loss_mask_7: 0.4304  loss_dice_7: 3.19  loss_ce_8: 0.3432  loss_mask_8: 0.4313  loss_dice_8: 3.183  time: 1.5048  data_time: 0.0806  lr: 6.8239e-06  max_mem: 21589M
[01/18 01:02:28] d2.utils.events INFO:  eta: 10:46:10  iter: 13859  total_loss: 40.04  loss_ce: 0.3777  loss_mask: 0.4203  loss_dice: 3.172  loss_ce_0: 0.6424  loss_mask_0: 0.4105  loss_dice_0: 3.297  loss_ce_1: 0.3916  loss_mask_1: 0.424  loss_dice_1: 3.2  loss_ce_2: 0.3901  loss_mask_2: 0.4224  loss_dice_2: 3.184  loss_ce_3: 0.3747  loss_mask_3: 0.4151  loss_dice_3: 3.173  loss_ce_4: 0.3748  loss_mask_4: 0.4169  loss_dice_4: 3.17  loss_ce_5: 0.3918  loss_mask_5: 0.4192  loss_dice_5: 3.168  loss_ce_6: 0.3826  loss_mask_6: 0.4193  loss_dice_6: 3.167  loss_ce_7: 0.3723  loss_mask_7: 0.4179  loss_dice_7: 3.168  loss_ce_8: 0.378  loss_mask_8: 0.4198  loss_dice_8: 3.172  time: 1.5048  data_time: 0.0874  lr: 6.8192e-06  max_mem: 21589M
[01/18 01:02:58] d2.utils.events INFO:  eta: 10:45:30  iter: 13879  total_loss: 40.69  loss_ce: 0.3364  loss_mask: 0.4537  loss_dice: 3.216  loss_ce_0: 0.5829  loss_mask_0: 0.4506  loss_dice_0: 3.337  loss_ce_1: 0.3667  loss_mask_1: 0.4657  loss_dice_1: 3.238  loss_ce_2: 0.3771  loss_mask_2: 0.4601  loss_dice_2: 3.23  loss_ce_3: 0.3544  loss_mask_3: 0.4588  loss_dice_3: 3.209  loss_ce_4: 0.341  loss_mask_4: 0.4581  loss_dice_4: 3.214  loss_ce_5: 0.3493  loss_mask_5: 0.4583  loss_dice_5: 3.206  loss_ce_6: 0.3312  loss_mask_6: 0.4538  loss_dice_6: 3.223  loss_ce_7: 0.3369  loss_mask_7: 0.4547  loss_dice_7: 3.218  loss_ce_8: 0.332  loss_mask_8: 0.4538  loss_dice_8: 3.221  time: 1.5047  data_time: 0.0700  lr: 6.8145e-06  max_mem: 21589M
[01/18 01:03:28] d2.utils.events INFO:  eta: 10:45:36  iter: 13899  total_loss: 40.45  loss_ce: 0.3414  loss_mask: 0.4301  loss_dice: 3.228  loss_ce_0: 0.5665  loss_mask_0: 0.4211  loss_dice_0: 3.359  loss_ce_1: 0.3703  loss_mask_1: 0.4305  loss_dice_1: 3.268  loss_ce_2: 0.347  loss_mask_2: 0.4281  loss_dice_2: 3.263  loss_ce_3: 0.3485  loss_mask_3: 0.4315  loss_dice_3: 3.233  loss_ce_4: 0.3488  loss_mask_4: 0.4304  loss_dice_4: 3.237  loss_ce_5: 0.3526  loss_mask_5: 0.4304  loss_dice_5: 3.243  loss_ce_6: 0.3437  loss_mask_6: 0.4268  loss_dice_6: 3.241  loss_ce_7: 0.3451  loss_mask_7: 0.4306  loss_dice_7: 3.236  loss_ce_8: 0.3395  loss_mask_8: 0.4284  loss_dice_8: 3.237  time: 1.5047  data_time: 0.0752  lr: 6.8098e-06  max_mem: 21589M
[01/18 01:03:58] d2.utils.events INFO:  eta: 10:44:58  iter: 13919  total_loss: 39.73  loss_ce: 0.3493  loss_mask: 0.4211  loss_dice: 3.158  loss_ce_0: 0.6037  loss_mask_0: 0.4123  loss_dice_0: 3.302  loss_ce_1: 0.3736  loss_mask_1: 0.423  loss_dice_1: 3.195  loss_ce_2: 0.3542  loss_mask_2: 0.4206  loss_dice_2: 3.174  loss_ce_3: 0.3411  loss_mask_3: 0.4218  loss_dice_3: 3.161  loss_ce_4: 0.3562  loss_mask_4: 0.4215  loss_dice_4: 3.157  loss_ce_5: 0.3305  loss_mask_5: 0.4216  loss_dice_5: 3.166  loss_ce_6: 0.3285  loss_mask_6: 0.4242  loss_dice_6: 3.157  loss_ce_7: 0.3394  loss_mask_7: 0.4225  loss_dice_7: 3.154  loss_ce_8: 0.3406  loss_mask_8: 0.421  loss_dice_8: 3.152  time: 1.5047  data_time: 0.0819  lr: 6.8052e-06  max_mem: 21589M
[01/18 01:04:28] d2.utils.events INFO:  eta: 10:44:33  iter: 13939  total_loss: 40.44  loss_ce: 0.374  loss_mask: 0.4381  loss_dice: 3.202  loss_ce_0: 0.6152  loss_mask_0: 0.429  loss_dice_0: 3.322  loss_ce_1: 0.407  loss_mask_1: 0.4443  loss_dice_1: 3.225  loss_ce_2: 0.4048  loss_mask_2: 0.4389  loss_dice_2: 3.209  loss_ce_3: 0.3794  loss_mask_3: 0.4398  loss_dice_3: 3.2  loss_ce_4: 0.3875  loss_mask_4: 0.4399  loss_dice_4: 3.2  loss_ce_5: 0.3821  loss_mask_5: 0.4414  loss_dice_5: 3.197  loss_ce_6: 0.3963  loss_mask_6: 0.4395  loss_dice_6: 3.198  loss_ce_7: 0.3669  loss_mask_7: 0.4389  loss_dice_7: 3.2  loss_ce_8: 0.3757  loss_mask_8: 0.4358  loss_dice_8: 3.198  time: 1.5047  data_time: 0.0754  lr: 6.8005e-06  max_mem: 21589M
[01/18 01:04:57] d2.utils.events INFO:  eta: 10:43:56  iter: 13959  total_loss: 39.95  loss_ce: 0.3516  loss_mask: 0.4365  loss_dice: 3.187  loss_ce_0: 0.5884  loss_mask_0: 0.4201  loss_dice_0: 3.319  loss_ce_1: 0.3693  loss_mask_1: 0.4389  loss_dice_1: 3.229  loss_ce_2: 0.3641  loss_mask_2: 0.4367  loss_dice_2: 3.205  loss_ce_3: 0.3561  loss_mask_3: 0.4364  loss_dice_3: 3.19  loss_ce_4: 0.3594  loss_mask_4: 0.4359  loss_dice_4: 3.193  loss_ce_5: 0.3488  loss_mask_5: 0.4348  loss_dice_5: 3.185  loss_ce_6: 0.3493  loss_mask_6: 0.4354  loss_dice_6: 3.193  loss_ce_7: 0.3476  loss_mask_7: 0.437  loss_dice_7: 3.181  loss_ce_8: 0.3494  loss_mask_8: 0.437  loss_dice_8: 3.186  time: 1.5046  data_time: 0.0698  lr: 6.7958e-06  max_mem: 21589M
[01/18 01:05:27] d2.utils.events INFO:  eta: 10:42:43  iter: 13979  total_loss: 39.91  loss_ce: 0.3417  loss_mask: 0.4304  loss_dice: 3.133  loss_ce_0: 0.5942  loss_mask_0: 0.4214  loss_dice_0: 3.271  loss_ce_1: 0.3628  loss_mask_1: 0.4331  loss_dice_1: 3.163  loss_ce_2: 0.3584  loss_mask_2: 0.4316  loss_dice_2: 3.139  loss_ce_3: 0.3548  loss_mask_3: 0.4303  loss_dice_3: 3.129  loss_ce_4: 0.3447  loss_mask_4: 0.4305  loss_dice_4: 3.131  loss_ce_5: 0.3456  loss_mask_5: 0.4305  loss_dice_5: 3.123  loss_ce_6: 0.339  loss_mask_6: 0.4313  loss_dice_6: 3.129  loss_ce_7: 0.343  loss_mask_7: 0.4313  loss_dice_7: 3.134  loss_ce_8: 0.3387  loss_mask_8: 0.4292  loss_dice_8: 3.128  time: 1.5046  data_time: 0.0817  lr: 6.7911e-06  max_mem: 21589M
[01/18 01:05:57] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 01:05:57] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 01:05:57] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 01:05:58] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 01:06:11] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0068 s/iter. Inference: 0.1351 s/iter. Eval: 0.1677 s/iter. Total: 0.3096 s/iter. ETA=0:05:34
[01/18 01:06:16] d2.evaluation.evaluator INFO: Inference done 25/1093. Dataloading: 0.0094 s/iter. Inference: 0.1573 s/iter. Eval: 0.1879 s/iter. Total: 0.3547 s/iter. ETA=0:06:18
[01/18 01:06:21] d2.evaluation.evaluator INFO: Inference done 40/1093. Dataloading: 0.0089 s/iter. Inference: 0.1569 s/iter. Eval: 0.1890 s/iter. Total: 0.3551 s/iter. ETA=0:06:13
[01/18 01:06:27] d2.evaluation.evaluator INFO: Inference done 55/1093. Dataloading: 0.0096 s/iter. Inference: 0.1585 s/iter. Eval: 0.1885 s/iter. Total: 0.3567 s/iter. ETA=0:06:10
[01/18 01:06:32] d2.evaluation.evaluator INFO: Inference done 69/1093. Dataloading: 0.0097 s/iter. Inference: 0.1612 s/iter. Eval: 0.1894 s/iter. Total: 0.3604 s/iter. ETA=0:06:09
[01/18 01:06:37] d2.evaluation.evaluator INFO: Inference done 83/1093. Dataloading: 0.0096 s/iter. Inference: 0.1627 s/iter. Eval: 0.1897 s/iter. Total: 0.3621 s/iter. ETA=0:06:05
[01/18 01:06:42] d2.evaluation.evaluator INFO: Inference done 96/1093. Dataloading: 0.0098 s/iter. Inference: 0.1627 s/iter. Eval: 0.1931 s/iter. Total: 0.3658 s/iter. ETA=0:06:04
[01/18 01:06:47] d2.evaluation.evaluator INFO: Inference done 112/1093. Dataloading: 0.0095 s/iter. Inference: 0.1600 s/iter. Eval: 0.1889 s/iter. Total: 0.3586 s/iter. ETA=0:05:51
[01/18 01:06:52] d2.evaluation.evaluator INFO: Inference done 127/1093. Dataloading: 0.0093 s/iter. Inference: 0.1583 s/iter. Eval: 0.1891 s/iter. Total: 0.3568 s/iter. ETA=0:05:44
[01/18 01:06:58] d2.evaluation.evaluator INFO: Inference done 143/1093. Dataloading: 0.0093 s/iter. Inference: 0.1571 s/iter. Eval: 0.1871 s/iter. Total: 0.3536 s/iter. ETA=0:05:35
[01/18 01:07:03] d2.evaluation.evaluator INFO: Inference done 158/1093. Dataloading: 0.0092 s/iter. Inference: 0.1582 s/iter. Eval: 0.1842 s/iter. Total: 0.3516 s/iter. ETA=0:05:28
[01/18 01:07:08] d2.evaluation.evaluator INFO: Inference done 172/1093. Dataloading: 0.0092 s/iter. Inference: 0.1576 s/iter. Eval: 0.1853 s/iter. Total: 0.3522 s/iter. ETA=0:05:24
[01/18 01:07:13] d2.evaluation.evaluator INFO: Inference done 187/1093. Dataloading: 0.0091 s/iter. Inference: 0.1585 s/iter. Eval: 0.1841 s/iter. Total: 0.3518 s/iter. ETA=0:05:18
[01/18 01:07:18] d2.evaluation.evaluator INFO: Inference done 202/1093. Dataloading: 0.0091 s/iter. Inference: 0.1591 s/iter. Eval: 0.1829 s/iter. Total: 0.3513 s/iter. ETA=0:05:12
[01/18 01:07:23] d2.evaluation.evaluator INFO: Inference done 215/1093. Dataloading: 0.0093 s/iter. Inference: 0.1609 s/iter. Eval: 0.1839 s/iter. Total: 0.3541 s/iter. ETA=0:05:10
[01/18 01:07:28] d2.evaluation.evaluator INFO: Inference done 229/1093. Dataloading: 0.0092 s/iter. Inference: 0.1612 s/iter. Eval: 0.1842 s/iter. Total: 0.3547 s/iter. ETA=0:05:06
[01/18 01:07:33] d2.evaluation.evaluator INFO: Inference done 243/1093. Dataloading: 0.0093 s/iter. Inference: 0.1601 s/iter. Eval: 0.1861 s/iter. Total: 0.3556 s/iter. ETA=0:05:02
[01/18 01:07:39] d2.evaluation.evaluator INFO: Inference done 257/1093. Dataloading: 0.0094 s/iter. Inference: 0.1598 s/iter. Eval: 0.1873 s/iter. Total: 0.3566 s/iter. ETA=0:04:58
[01/18 01:07:44] d2.evaluation.evaluator INFO: Inference done 272/1093. Dataloading: 0.0093 s/iter. Inference: 0.1598 s/iter. Eval: 0.1867 s/iter. Total: 0.3558 s/iter. ETA=0:04:52
[01/18 01:07:49] d2.evaluation.evaluator INFO: Inference done 287/1093. Dataloading: 0.0093 s/iter. Inference: 0.1597 s/iter. Eval: 0.1860 s/iter. Total: 0.3550 s/iter. ETA=0:04:46
[01/18 01:07:54] d2.evaluation.evaluator INFO: Inference done 302/1093. Dataloading: 0.0093 s/iter. Inference: 0.1592 s/iter. Eval: 0.1862 s/iter. Total: 0.3548 s/iter. ETA=0:04:40
[01/18 01:07:59] d2.evaluation.evaluator INFO: Inference done 315/1093. Dataloading: 0.0095 s/iter. Inference: 0.1594 s/iter. Eval: 0.1873 s/iter. Total: 0.3563 s/iter. ETA=0:04:37
[01/18 01:08:05] d2.evaluation.evaluator INFO: Inference done 330/1093. Dataloading: 0.0096 s/iter. Inference: 0.1594 s/iter. Eval: 0.1871 s/iter. Total: 0.3562 s/iter. ETA=0:04:31
[01/18 01:08:10] d2.evaluation.evaluator INFO: Inference done 348/1093. Dataloading: 0.0095 s/iter. Inference: 0.1590 s/iter. Eval: 0.1838 s/iter. Total: 0.3524 s/iter. ETA=0:04:22
[01/18 01:08:15] d2.evaluation.evaluator INFO: Inference done 364/1093. Dataloading: 0.0094 s/iter. Inference: 0.1583 s/iter. Eval: 0.1838 s/iter. Total: 0.3516 s/iter. ETA=0:04:16
[01/18 01:08:20] d2.evaluation.evaluator INFO: Inference done 379/1093. Dataloading: 0.0094 s/iter. Inference: 0.1584 s/iter. Eval: 0.1834 s/iter. Total: 0.3512 s/iter. ETA=0:04:10
[01/18 01:08:25] d2.evaluation.evaluator INFO: Inference done 393/1093. Dataloading: 0.0093 s/iter. Inference: 0.1580 s/iter. Eval: 0.1841 s/iter. Total: 0.3515 s/iter. ETA=0:04:06
[01/18 01:08:30] d2.evaluation.evaluator INFO: Inference done 408/1093. Dataloading: 0.0093 s/iter. Inference: 0.1582 s/iter. Eval: 0.1834 s/iter. Total: 0.3510 s/iter. ETA=0:04:00
[01/18 01:08:35] d2.evaluation.evaluator INFO: Inference done 421/1093. Dataloading: 0.0097 s/iter. Inference: 0.1580 s/iter. Eval: 0.1844 s/iter. Total: 0.3522 s/iter. ETA=0:03:56
[01/18 01:08:41] d2.evaluation.evaluator INFO: Inference done 436/1093. Dataloading: 0.0098 s/iter. Inference: 0.1579 s/iter. Eval: 0.1845 s/iter. Total: 0.3523 s/iter. ETA=0:03:51
[01/18 01:08:46] d2.evaluation.evaluator INFO: Inference done 451/1093. Dataloading: 0.0098 s/iter. Inference: 0.1575 s/iter. Eval: 0.1843 s/iter. Total: 0.3517 s/iter. ETA=0:03:45
[01/18 01:08:51] d2.evaluation.evaluator INFO: Inference done 466/1093. Dataloading: 0.0097 s/iter. Inference: 0.1577 s/iter. Eval: 0.1840 s/iter. Total: 0.3515 s/iter. ETA=0:03:40
[01/18 01:08:56] d2.evaluation.evaluator INFO: Inference done 482/1093. Dataloading: 0.0097 s/iter. Inference: 0.1583 s/iter. Eval: 0.1827 s/iter. Total: 0.3508 s/iter. ETA=0:03:34
[01/18 01:09:01] d2.evaluation.evaluator INFO: Inference done 497/1093. Dataloading: 0.0097 s/iter. Inference: 0.1584 s/iter. Eval: 0.1825 s/iter. Total: 0.3507 s/iter. ETA=0:03:29
[01/18 01:09:06] d2.evaluation.evaluator INFO: Inference done 513/1093. Dataloading: 0.0097 s/iter. Inference: 0.1582 s/iter. Eval: 0.1817 s/iter. Total: 0.3496 s/iter. ETA=0:03:22
[01/18 01:09:12] d2.evaluation.evaluator INFO: Inference done 527/1093. Dataloading: 0.0097 s/iter. Inference: 0.1582 s/iter. Eval: 0.1823 s/iter. Total: 0.3503 s/iter. ETA=0:03:18
[01/18 01:09:17] d2.evaluation.evaluator INFO: Inference done 542/1093. Dataloading: 0.0097 s/iter. Inference: 0.1583 s/iter. Eval: 0.1820 s/iter. Total: 0.3501 s/iter. ETA=0:03:12
[01/18 01:09:22] d2.evaluation.evaluator INFO: Inference done 556/1093. Dataloading: 0.0097 s/iter. Inference: 0.1582 s/iter. Eval: 0.1823 s/iter. Total: 0.3503 s/iter. ETA=0:03:08
[01/18 01:09:27] d2.evaluation.evaluator INFO: Inference done 573/1093. Dataloading: 0.0096 s/iter. Inference: 0.1577 s/iter. Eval: 0.1816 s/iter. Total: 0.3490 s/iter. ETA=0:03:01
[01/18 01:09:32] d2.evaluation.evaluator INFO: Inference done 591/1093. Dataloading: 0.0096 s/iter. Inference: 0.1575 s/iter. Eval: 0.1802 s/iter. Total: 0.3473 s/iter. ETA=0:02:54
[01/18 01:09:37] d2.evaluation.evaluator INFO: Inference done 605/1093. Dataloading: 0.0096 s/iter. Inference: 0.1571 s/iter. Eval: 0.1810 s/iter. Total: 0.3478 s/iter. ETA=0:02:49
[01/18 01:09:43] d2.evaluation.evaluator INFO: Inference done 619/1093. Dataloading: 0.0096 s/iter. Inference: 0.1577 s/iter. Eval: 0.1809 s/iter. Total: 0.3482 s/iter. ETA=0:02:45
[01/18 01:09:48] d2.evaluation.evaluator INFO: Inference done 634/1093. Dataloading: 0.0096 s/iter. Inference: 0.1574 s/iter. Eval: 0.1808 s/iter. Total: 0.3480 s/iter. ETA=0:02:39
[01/18 01:09:53] d2.evaluation.evaluator INFO: Inference done 650/1093. Dataloading: 0.0096 s/iter. Inference: 0.1574 s/iter. Eval: 0.1802 s/iter. Total: 0.3474 s/iter. ETA=0:02:33
[01/18 01:09:58] d2.evaluation.evaluator INFO: Inference done 665/1093. Dataloading: 0.0096 s/iter. Inference: 0.1575 s/iter. Eval: 0.1803 s/iter. Total: 0.3475 s/iter. ETA=0:02:28
[01/18 01:10:03] d2.evaluation.evaluator INFO: Inference done 682/1093. Dataloading: 0.0096 s/iter. Inference: 0.1572 s/iter. Eval: 0.1795 s/iter. Total: 0.3464 s/iter. ETA=0:02:22
[01/18 01:10:08] d2.evaluation.evaluator INFO: Inference done 699/1093. Dataloading: 0.0095 s/iter. Inference: 0.1571 s/iter. Eval: 0.1786 s/iter. Total: 0.3453 s/iter. ETA=0:02:16
[01/18 01:10:14] d2.evaluation.evaluator INFO: Inference done 713/1093. Dataloading: 0.0095 s/iter. Inference: 0.1568 s/iter. Eval: 0.1794 s/iter. Total: 0.3458 s/iter. ETA=0:02:11
[01/18 01:10:19] d2.evaluation.evaluator INFO: Inference done 729/1093. Dataloading: 0.0095 s/iter. Inference: 0.1566 s/iter. Eval: 0.1791 s/iter. Total: 0.3453 s/iter. ETA=0:02:05
[01/18 01:10:24] d2.evaluation.evaluator INFO: Inference done 744/1093. Dataloading: 0.0095 s/iter. Inference: 0.1566 s/iter. Eval: 0.1790 s/iter. Total: 0.3452 s/iter. ETA=0:02:00
[01/18 01:10:29] d2.evaluation.evaluator INFO: Inference done 760/1093. Dataloading: 0.0095 s/iter. Inference: 0.1564 s/iter. Eval: 0.1786 s/iter. Total: 0.3445 s/iter. ETA=0:01:54
[01/18 01:10:34] d2.evaluation.evaluator INFO: Inference done 774/1093. Dataloading: 0.0094 s/iter. Inference: 0.1565 s/iter. Eval: 0.1789 s/iter. Total: 0.3449 s/iter. ETA=0:01:50
[01/18 01:10:39] d2.evaluation.evaluator INFO: Inference done 791/1093. Dataloading: 0.0094 s/iter. Inference: 0.1564 s/iter. Eval: 0.1781 s/iter. Total: 0.3440 s/iter. ETA=0:01:43
[01/18 01:10:44] d2.evaluation.evaluator INFO: Inference done 806/1093. Dataloading: 0.0094 s/iter. Inference: 0.1563 s/iter. Eval: 0.1780 s/iter. Total: 0.3439 s/iter. ETA=0:01:38
[01/18 01:10:49] d2.evaluation.evaluator INFO: Inference done 822/1093. Dataloading: 0.0094 s/iter. Inference: 0.1565 s/iter. Eval: 0.1774 s/iter. Total: 0.3434 s/iter. ETA=0:01:33
[01/18 01:10:55] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0093 s/iter. Inference: 0.1567 s/iter. Eval: 0.1769 s/iter. Total: 0.3429 s/iter. ETA=0:01:27
[01/18 01:11:00] d2.evaluation.evaluator INFO: Inference done 854/1093. Dataloading: 0.0093 s/iter. Inference: 0.1568 s/iter. Eval: 0.1769 s/iter. Total: 0.3431 s/iter. ETA=0:01:22
[01/18 01:11:05] d2.evaluation.evaluator INFO: Inference done 867/1093. Dataloading: 0.0093 s/iter. Inference: 0.1571 s/iter. Eval: 0.1774 s/iter. Total: 0.3439 s/iter. ETA=0:01:17
[01/18 01:11:11] d2.evaluation.evaluator INFO: Inference done 882/1093. Dataloading: 0.0093 s/iter. Inference: 0.1571 s/iter. Eval: 0.1776 s/iter. Total: 0.3441 s/iter. ETA=0:01:12
[01/18 01:11:16] d2.evaluation.evaluator INFO: Inference done 896/1093. Dataloading: 0.0093 s/iter. Inference: 0.1569 s/iter. Eval: 0.1782 s/iter. Total: 0.3445 s/iter. ETA=0:01:07
[01/18 01:11:21] d2.evaluation.evaluator INFO: Inference done 912/1093. Dataloading: 0.0093 s/iter. Inference: 0.1569 s/iter. Eval: 0.1781 s/iter. Total: 0.3444 s/iter. ETA=0:01:02
[01/18 01:11:26] d2.evaluation.evaluator INFO: Inference done 928/1093. Dataloading: 0.0093 s/iter. Inference: 0.1566 s/iter. Eval: 0.1781 s/iter. Total: 0.3440 s/iter. ETA=0:00:56
[01/18 01:11:32] d2.evaluation.evaluator INFO: Inference done 943/1093. Dataloading: 0.0093 s/iter. Inference: 0.1565 s/iter. Eval: 0.1784 s/iter. Total: 0.3442 s/iter. ETA=0:00:51
[01/18 01:11:37] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0093 s/iter. Inference: 0.1565 s/iter. Eval: 0.1785 s/iter. Total: 0.3444 s/iter. ETA=0:00:46
[01/18 01:11:42] d2.evaluation.evaluator INFO: Inference done 973/1093. Dataloading: 0.0093 s/iter. Inference: 0.1566 s/iter. Eval: 0.1785 s/iter. Total: 0.3445 s/iter. ETA=0:00:41
[01/18 01:11:47] d2.evaluation.evaluator INFO: Inference done 990/1093. Dataloading: 0.0092 s/iter. Inference: 0.1566 s/iter. Eval: 0.1778 s/iter. Total: 0.3438 s/iter. ETA=0:00:35
[01/18 01:11:53] d2.evaluation.evaluator INFO: Inference done 1005/1093. Dataloading: 0.0092 s/iter. Inference: 0.1565 s/iter. Eval: 0.1779 s/iter. Total: 0.3437 s/iter. ETA=0:00:30
[01/18 01:11:58] d2.evaluation.evaluator INFO: Inference done 1019/1093. Dataloading: 0.0092 s/iter. Inference: 0.1569 s/iter. Eval: 0.1777 s/iter. Total: 0.3439 s/iter. ETA=0:00:25
[01/18 01:12:03] d2.evaluation.evaluator INFO: Inference done 1034/1093. Dataloading: 0.0092 s/iter. Inference: 0.1569 s/iter. Eval: 0.1777 s/iter. Total: 0.3439 s/iter. ETA=0:00:20
[01/18 01:12:08] d2.evaluation.evaluator INFO: Inference done 1049/1093. Dataloading: 0.0092 s/iter. Inference: 0.1568 s/iter. Eval: 0.1776 s/iter. Total: 0.3438 s/iter. ETA=0:00:15
[01/18 01:12:13] d2.evaluation.evaluator INFO: Inference done 1066/1093. Dataloading: 0.0092 s/iter. Inference: 0.1566 s/iter. Eval: 0.1773 s/iter. Total: 0.3431 s/iter. ETA=0:00:09
[01/18 01:12:18] d2.evaluation.evaluator INFO: Inference done 1085/1093. Dataloading: 0.0091 s/iter. Inference: 0.1560 s/iter. Eval: 0.1765 s/iter. Total: 0.3418 s/iter. ETA=0:00:02
[01/18 01:12:20] d2.evaluation.evaluator INFO: Total inference time: 0:06:11.636220 (0.341577 s / iter per device, on 4 devices)
[01/18 01:12:20] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:49 (0.155772 s / iter per device, on 4 devices)
[01/18 01:12:42] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.837977715458497, 'mIoU': 15.130841336825718, 'fwIoU': 34.04102987856222, 'IoU-0': nan, 'IoU-1': 95.15858275878949, 'IoU-2': 43.870298288533846, 'IoU-3': 51.3698605387299, 'IoU-4': 44.76259053750689, 'IoU-5': 36.09913735026511, 'IoU-6': 36.89655359477479, 'IoU-7': 32.459773719634285, 'IoU-8': 11.297754146139924, 'IoU-9': 20.971565065580673, 'IoU-10': 22.615157418751906, 'IoU-11': 32.04846810723795, 'IoU-12': 34.91670288720772, 'IoU-13': 32.671942705645684, 'IoU-14': 34.38438276988493, 'IoU-15': 33.227541029211174, 'IoU-16': 33.228894655197685, 'IoU-17': 31.698462282808975, 'IoU-18': 33.932269378392974, 'IoU-19': 32.8642959289447, 'IoU-20': 34.96270298363921, 'IoU-21': 35.403442430913614, 'IoU-22': 37.39209965407461, 'IoU-23': 32.370122263634734, 'IoU-24': 34.817869732018025, 'IoU-25': 33.67239937380796, 'IoU-26': 32.379030641138634, 'IoU-27': 32.85076162280863, 'IoU-28': 33.15691857418322, 'IoU-29': 34.09514757321867, 'IoU-30': 31.826201149807705, 'IoU-31': 32.65866060822746, 'IoU-32': 32.599381635787616, 'IoU-33': 31.707570419115743, 'IoU-34': 32.074647440433935, 'IoU-35': 33.71335823865995, 'IoU-36': 32.5442715288299, 'IoU-37': 31.106770972148823, 'IoU-38': 32.35611637918868, 'IoU-39': 30.91621899611078, 'IoU-40': 32.224900986782394, 'IoU-41': 29.331269987421933, 'IoU-42': 29.61204059202026, 'IoU-43': 28.97135218525908, 'IoU-44': 28.26876884861852, 'IoU-45': 28.870106912312686, 'IoU-46': 27.543002902933, 'IoU-47': 28.888762544209847, 'IoU-48': 27.937330337736327, 'IoU-49': 27.37188367448729, 'IoU-50': 28.522384101510532, 'IoU-51': 26.492932491802208, 'IoU-52': 25.59793521190682, 'IoU-53': 25.96853567951318, 'IoU-54': 25.679132152826305, 'IoU-55': 25.54049362953072, 'IoU-56': 22.902889250737985, 'IoU-57': 23.748344067442968, 'IoU-58': 22.4059233925816, 'IoU-59': 20.264562853784927, 'IoU-60': 19.538728406816023, 'IoU-61': 19.69741334343069, 'IoU-62': 21.848565495868, 'IoU-63': 18.95345607669733, 'IoU-64': 19.676721727069875, 'IoU-65': 17.872428906378918, 'IoU-66': 18.33390177567588, 'IoU-67': 16.9332089200665, 'IoU-68': 17.249226864688307, 'IoU-69': 16.94701409081354, 'IoU-70': 17.367851242938297, 'IoU-71': 15.484254350238889, 'IoU-72': 17.572798958942908, 'IoU-73': 16.90273572371488, 'IoU-74': 17.364909023817514, 'IoU-75': 15.917189942895703, 'IoU-76': 16.54012808039979, 'IoU-77': 15.310462990266243, 'IoU-78': 16.459985123516525, 'IoU-79': 15.657036384474521, 'IoU-80': 16.88657879987675, 'IoU-81': 13.491829285328617, 'IoU-82': 15.43922983451389, 'IoU-83': 14.141026167414767, 'IoU-84': 15.428320676952998, 'IoU-85': 15.377939310982589, 'IoU-86': 13.635897305296602, 'IoU-87': 13.32180733154125, 'IoU-88': 13.717831181238013, 'IoU-89': 12.238046028855864, 'IoU-90': 13.640429471142548, 'IoU-91': 12.155980957864978, 'IoU-92': 13.051174775993838, 'IoU-93': 13.2475929173441, 'IoU-94': 13.0523513950436, 'IoU-95': 12.454479031665432, 'IoU-96': 12.438980345245508, 'IoU-97': 13.75973766892865, 'IoU-98': 11.277046355980872, 'IoU-99': 12.704283849542275, 'IoU-100': 10.014995584796079, 'IoU-101': 10.567669030901627, 'IoU-102': 9.936278589280386, 'IoU-103': 8.616663114664394, 'IoU-104': 9.451673455070193, 'IoU-105': 8.993436084781251, 'IoU-106': 9.214624664280706, 'IoU-107': 9.57953038812219, 'IoU-108': 9.857230416944429, 'IoU-109': 10.226551798250043, 'IoU-110': 5.5002311605289425, 'IoU-111': 11.031460308782396, 'IoU-112': 10.13010658940137, 'IoU-113': 8.026557556918439, 'IoU-114': 8.117136060272925, 'IoU-115': 8.324205397442919, 'IoU-116': 8.15001527801148, 'IoU-117': 8.602349316066011, 'IoU-118': 6.69694690032442, 'IoU-119': 7.965493555501375, 'IoU-120': 6.0955105587166365, 'IoU-121': 5.11362702077147, 'IoU-122': 7.385953685315237, 'IoU-123': 6.160956221532391, 'IoU-124': 3.9982462995968393, 'IoU-125': 8.477440497518067, 'IoU-126': 4.233312775212133, 'IoU-127': 7.588182182758254, 'IoU-128': 4.526566647013816, 'IoU-129': 5.801375075359005, 'IoU-130': 5.182863010068893, 'IoU-131': 4.804604466467689, 'IoU-132': 4.7605109344940475, 'IoU-133': 4.88655782489255, 'IoU-134': 5.267568226263955, 'IoU-135': 3.6275084918307074, 'IoU-136': 3.9432544646917975, 'IoU-137': 3.955115655137408, 'IoU-138': 3.804860255757466, 'IoU-139': 4.4098745868199725, 'IoU-140': 3.0836559471810574, 'IoU-141': 5.627160535927818, 'IoU-142': 3.1594465330905495, 'IoU-143': 1.9405520352919747, 'IoU-144': 3.8833268179733724, 'IoU-145': 2.8249648572123305, 'IoU-146': 1.3600406658728061, 'IoU-147': 4.690604951169484, 'IoU-148': 2.9684054983018573, 'IoU-149': 2.997419686432016, 'IoU-150': 2.2249866581676225, 'IoU-151': 1.152084145829114, 'IoU-152': 3.36685314205306, 'IoU-153': 1.1776350873711756, 'IoU-154': 2.192812648928139, 'IoU-155': 0.9523340547748407, 'IoU-156': 2.1374442961521933, 'IoU-157': 2.318575639528276, 'IoU-158': 2.552771534370534, 'IoU-159': 0.8905381991959239, 'IoU-160': 1.3656626905485565, 'IoU-161': 2.028872601998381, 'IoU-162': 1.789231407080537, 'IoU-163': 1.6084367554671446, 'IoU-164': 3.13375476048174, 'IoU-165': 0.629404305782481, 'IoU-166': 1.1533834922336665, 'IoU-167': 1.4990104648344382, 'IoU-168': 2.2713585557899907, 'IoU-169': 1.713764079517517, 'IoU-170': 1.3799652320709024, 'IoU-171': 1.213351944537452, 'IoU-172': 2.3855616534491064, 'IoU-173': 1.1404856408845687, 'IoU-174': 0.21248615493754244, 'IoU-175': 0.41410064198669894, 'IoU-176': 1.0938750800750443, 'IoU-177': 0.9021791879462945, 'IoU-178': 0.24601392883527107, 'IoU-179': 1.1519756088529, 'IoU-180': 0.07222500900210847, 'IoU-181': 1.480092971337964, 'IoU-182': 0.13482232585449577, 'IoU-183': 0.6683441776909812, 'IoU-184': 0.010805960191782303, 'IoU-185': 0.3382465612910635, 'IoU-186': 2.2488931914663994, 'IoU-187': 0.7958735071914571, 'IoU-188': 0.36669185404030435, 'IoU-189': 1.4291781897314877, 'IoU-190': 2.5036157485938753, 'IoU-191': 1.7713172807570707, 'mACC': 24.104937989896463, 'pACC': 47.65398670680726, 'ACC-0': nan, 'ACC-1': 98.58225082864838, 'ACC-2': 56.45512354006177, 'ACC-3': 65.70187012854967, 'ACC-4': 62.3533497734456, 'ACC-5': 52.8531049818273, 'ACC-6': 58.64924107982071, 'ACC-7': 49.06290223189706, 'ACC-8': 13.967589307625087, 'ACC-9': 31.13679479267629, 'ACC-10': 30.65515547404028, 'ACC-11': 44.76634325367822, 'ACC-12': 53.43122663001262, 'ACC-13': 50.43567273959469, 'ACC-14': 53.74384017083202, 'ACC-15': 49.80303102087239, 'ACC-16': 50.00601808380557, 'ACC-17': 51.4846753847464, 'ACC-18': 52.76421702582292, 'ACC-19': 47.72990940754161, 'ACC-20': 52.902914889780504, 'ACC-21': 53.44763616472049, 'ACC-22': 53.913126459321504, 'ACC-23': 50.63659901497868, 'ACC-24': 51.157268097768906, 'ACC-25': 51.75112879697378, 'ACC-26': 52.22766470043098, 'ACC-27': 48.759717002650596, 'ACC-28': 51.73383410356026, 'ACC-29': 50.19157902852679, 'ACC-30': 49.06231873149963, 'ACC-31': 50.03621159410117, 'ACC-32': 47.84429128179231, 'ACC-33': 47.70539608600527, 'ACC-34': 47.161776610851405, 'ACC-35': 48.53482988979019, 'ACC-36': 49.396607398758725, 'ACC-37': 46.4577206676266, 'ACC-38': 48.88151586042447, 'ACC-39': 46.13719218529522, 'ACC-40': 47.59295142653088, 'ACC-41': 44.99597379167107, 'ACC-42': 46.94168065778977, 'ACC-43': 42.073582776664466, 'ACC-44': 44.003295000410134, 'ACC-45': 43.84284312946794, 'ACC-46': 45.00578393871618, 'ACC-47': 47.36078888164164, 'ACC-48': 44.39286645222633, 'ACC-49': 43.085359309585414, 'ACC-50': 44.460707084017095, 'ACC-51': 42.85583564709512, 'ACC-52': 39.534363783454964, 'ACC-53': 41.76140579429778, 'ACC-54': 41.865379720401315, 'ACC-55': 42.313283813999405, 'ACC-56': 39.0117875297352, 'ACC-57': 37.27748333566794, 'ACC-58': 39.39268693615629, 'ACC-59': 32.8847379180775, 'ACC-60': 33.2297122226969, 'ACC-61': 31.602036612141543, 'ACC-62': 35.689908259453226, 'ACC-63': 30.543817981397943, 'ACC-64': 32.76739524095842, 'ACC-65': 32.1729354805684, 'ACC-66': 31.321771882694865, 'ACC-67': 29.47267762973992, 'ACC-68': 27.666173226032736, 'ACC-69': 26.266409872896407, 'ACC-70': 28.78456820743699, 'ACC-71': 25.805622890258427, 'ACC-72': 31.087303008516216, 'ACC-73': 27.99598597877015, 'ACC-74': 28.690682546199263, 'ACC-75': 26.5821047263467, 'ACC-76': 28.11191120408344, 'ACC-77': 23.323404665025883, 'ACC-78': 28.494558177618973, 'ACC-79': 25.8094416734526, 'ACC-80': 30.97489160677196, 'ACC-81': 21.028013876746922, 'ACC-82': 25.41310295772559, 'ACC-83': 23.765622984505757, 'ACC-84': 27.49474226702399, 'ACC-85': 28.135251887145746, 'ACC-86': 26.79395726398935, 'ACC-87': 22.98455058241794, 'ACC-88': 25.051574534660325, 'ACC-89': 23.49053965038944, 'ACC-90': 23.295902192875502, 'ACC-91': 19.8558486131481, 'ACC-92': 22.38843387583668, 'ACC-93': 21.621409001974246, 'ACC-94': 24.12167224991546, 'ACC-95': 22.343492755037474, 'ACC-96': 21.462216744445218, 'ACC-97': 26.45781960060806, 'ACC-98': 18.965991292242794, 'ACC-99': 26.413040040666242, 'ACC-100': 16.63771634687511, 'ACC-101': 19.599283664419307, 'ACC-102': 18.942659719075667, 'ACC-103': 14.130041560404269, 'ACC-104': 16.65945254666634, 'ACC-105': 14.930772195279795, 'ACC-106': 14.824767663195612, 'ACC-107': 17.239016966263286, 'ACC-108': 16.027212929817296, 'ACC-109': 17.91362796835643, 'ACC-110': 7.390018394125683, 'ACC-111': 21.41179818363643, 'ACC-112': 20.624454885725363, 'ACC-113': 13.130642697502, 'ACC-114': 15.036277372023365, 'ACC-115': 14.579180009922874, 'ACC-116': 16.238046128619963, 'ACC-117': 16.56662597532642, 'ACC-118': 10.730961998944451, 'ACC-119': 14.727237869268212, 'ACC-120': 12.634453943923788, 'ACC-121': 8.19257758012597, 'ACC-122': 12.642965646730259, 'ACC-123': 10.478339902382029, 'ACC-124': 6.48100448510762, 'ACC-125': 22.003550284272915, 'ACC-126': 6.821840362844835, 'ACC-127': 15.648742161874235, 'ACC-128': 7.233766439685276, 'ACC-129': 11.05067336697292, 'ACC-130': 10.091824178432217, 'ACC-131': 9.634727685611807, 'ACC-132': 11.118282380459297, 'ACC-133': 9.133138731824152, 'ACC-134': 9.893183972425678, 'ACC-135': 6.411692605894761, 'ACC-136': 7.542207300596347, 'ACC-137': 6.660881365453203, 'ACC-138': 6.9171669162626594, 'ACC-139': 8.368415572661023, 'ACC-140': 5.740226309329173, 'ACC-141': 12.775714100572424, 'ACC-142': 5.7384715748767965, 'ACC-143': 3.235737334580578, 'ACC-144': 7.459296028880866, 'ACC-145': 5.933764171828529, 'ACC-146': 2.819126819126819, 'ACC-147': 13.75304902677201, 'ACC-148': 4.83094450744308, 'ACC-149': 5.228234535311423, 'ACC-150': 4.358620775915173, 'ACC-151': 1.4671479713476638, 'ACC-152': 6.408683997572324, 'ACC-153': 1.7397946096483996, 'ACC-154': 6.104968256372502, 'ACC-155': 1.5017391282912171, 'ACC-156': 3.6646047043766603, 'ACC-157': 6.52509142777036, 'ACC-158': 4.683784230069536, 'ACC-159': 1.1301805850513345, 'ACC-160': 2.300309912681536, 'ACC-161': 3.569171836428638, 'ACC-162': 2.93393950032877, 'ACC-163': 2.250801400865528, 'ACC-164': 6.010451580579593, 'ACC-165': 0.8032983193944611, 'ACC-166': 2.4274118407467413, 'ACC-167': 2.213084995318474, 'ACC-168': 10.510408608200295, 'ACC-169': 3.8114172309538805, 'ACC-170': 2.4371069182389937, 'ACC-171': 1.5376307151424755, 'ACC-172': 9.594584583472237, 'ACC-173': 2.3478568445135144, 'ACC-174': 0.23672786175092872, 'ACC-175': 0.4978932441451785, 'ACC-176': 2.0845426050615274, 'ACC-177': 1.4409386770603227, 'ACC-178': 0.2650127194515859, 'ACC-179': 1.5147672561936858, 'ACC-180': 0.07344474922957094, 'ACC-181': 3.398305466138931, 'ACC-182': 0.14143936468375154, 'ACC-183': 1.0055302989280779, 'ACC-184': 0.010848621635453295, 'ACC-185': 0.4965610261953263, 'ACC-186': 6.591629624208889, 'ACC-187': 0.8808997451044347, 'ACC-188': 0.40757031596787857, 'ACC-189': 3.0753155799045495, 'ACC-190': 7.347485248959829, 'ACC-191': 10.77742251223491})])
[01/18 01:12:42] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 01:12:42] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 01:12:42] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 01:12:42] d2.evaluation.testing INFO: copypaste: 2.8380,15.1308,34.0410,24.1049,47.6540
[01/18 01:12:42] d2.utils.events INFO:  eta: 10:42:48  iter: 13999  total_loss: 40.13  loss_ce: 0.3355  loss_mask: 0.417  loss_dice: 3.209  loss_ce_0: 0.6141  loss_mask_0: 0.4084  loss_dice_0: 3.343  loss_ce_1: 0.3495  loss_mask_1: 0.4242  loss_dice_1: 3.258  loss_ce_2: 0.339  loss_mask_2: 0.4197  loss_dice_2: 3.228  loss_ce_3: 0.3463  loss_mask_3: 0.4189  loss_dice_3: 3.214  loss_ce_4: 0.3179  loss_mask_4: 0.4172  loss_dice_4: 3.213  loss_ce_5: 0.3265  loss_mask_5: 0.4187  loss_dice_5: 3.214  loss_ce_6: 0.3374  loss_mask_6: 0.4176  loss_dice_6: 3.218  loss_ce_7: 0.3249  loss_mask_7: 0.4159  loss_dice_7: 3.21  loss_ce_8: 0.3323  loss_mask_8: 0.4155  loss_dice_8: 3.212  time: 1.5046  data_time: 0.0688  lr: 6.7864e-06  max_mem: 21589M
[01/18 01:13:11] d2.utils.events INFO:  eta: 10:41:36  iter: 14019  total_loss: 40.37  loss_ce: 0.3436  loss_mask: 0.4238  loss_dice: 3.196  loss_ce_0: 0.6062  loss_mask_0: 0.41  loss_dice_0: 3.33  loss_ce_1: 0.3643  loss_mask_1: 0.4215  loss_dice_1: 3.244  loss_ce_2: 0.377  loss_mask_2: 0.4219  loss_dice_2: 3.212  loss_ce_3: 0.3451  loss_mask_3: 0.4239  loss_dice_3: 3.202  loss_ce_4: 0.3479  loss_mask_4: 0.4246  loss_dice_4: 3.184  loss_ce_5: 0.3574  loss_mask_5: 0.4209  loss_dice_5: 3.204  loss_ce_6: 0.3473  loss_mask_6: 0.4197  loss_dice_6: 3.191  loss_ce_7: 0.3497  loss_mask_7: 0.4217  loss_dice_7: 3.195  loss_ce_8: 0.3539  loss_mask_8: 0.4222  loss_dice_8: 3.19  time: 1.5045  data_time: 0.0715  lr: 6.7817e-06  max_mem: 21589M
[01/18 01:13:40] d2.utils.events INFO:  eta: 10:41:14  iter: 14039  total_loss: 39.76  loss_ce: 0.3177  loss_mask: 0.4279  loss_dice: 3.164  loss_ce_0: 0.5678  loss_mask_0: 0.4186  loss_dice_0: 3.301  loss_ce_1: 0.3459  loss_mask_1: 0.4359  loss_dice_1: 3.217  loss_ce_2: 0.3386  loss_mask_2: 0.4371  loss_dice_2: 3.177  loss_ce_3: 0.3508  loss_mask_3: 0.4292  loss_dice_3: 3.174  loss_ce_4: 0.3339  loss_mask_4: 0.4267  loss_dice_4: 3.167  loss_ce_5: 0.316  loss_mask_5: 0.4267  loss_dice_5: 3.168  loss_ce_6: 0.3253  loss_mask_6: 0.431  loss_dice_6: 3.164  loss_ce_7: 0.3251  loss_mask_7: 0.4307  loss_dice_7: 3.16  loss_ce_8: 0.3264  loss_mask_8: 0.4291  loss_dice_8: 3.175  time: 1.5044  data_time: 0.0827  lr: 6.777e-06  max_mem: 21589M
[01/18 01:14:09] d2.utils.events INFO:  eta: 10:40:37  iter: 14059  total_loss: 40.36  loss_ce: 0.3598  loss_mask: 0.4189  loss_dice: 3.225  loss_ce_0: 0.6021  loss_mask_0: 0.4083  loss_dice_0: 3.356  loss_ce_1: 0.3801  loss_mask_1: 0.4219  loss_dice_1: 3.267  loss_ce_2: 0.3821  loss_mask_2: 0.4233  loss_dice_2: 3.242  loss_ce_3: 0.3557  loss_mask_3: 0.4193  loss_dice_3: 3.227  loss_ce_4: 0.3581  loss_mask_4: 0.4186  loss_dice_4: 3.233  loss_ce_5: 0.3473  loss_mask_5: 0.418  loss_dice_5: 3.234  loss_ce_6: 0.3506  loss_mask_6: 0.4179  loss_dice_6: 3.227  loss_ce_7: 0.3473  loss_mask_7: 0.4178  loss_dice_7: 3.237  loss_ce_8: 0.3597  loss_mask_8: 0.418  loss_dice_8: 3.228  time: 1.5043  data_time: 0.0691  lr: 6.7723e-06  max_mem: 21589M
[01/18 01:14:38] d2.utils.events INFO:  eta: 10:39:58  iter: 14079  total_loss: 39.87  loss_ce: 0.3386  loss_mask: 0.4187  loss_dice: 3.145  loss_ce_0: 0.6117  loss_mask_0: 0.408  loss_dice_0: 3.3  loss_ce_1: 0.3551  loss_mask_1: 0.4254  loss_dice_1: 3.196  loss_ce_2: 0.3753  loss_mask_2: 0.4222  loss_dice_2: 3.163  loss_ce_3: 0.3539  loss_mask_3: 0.4232  loss_dice_3: 3.156  loss_ce_4: 0.3711  loss_mask_4: 0.4182  loss_dice_4: 3.153  loss_ce_5: 0.3484  loss_mask_5: 0.4203  loss_dice_5: 3.155  loss_ce_6: 0.3521  loss_mask_6: 0.4219  loss_dice_6: 3.148  loss_ce_7: 0.3413  loss_mask_7: 0.4197  loss_dice_7: 3.15  loss_ce_8: 0.3502  loss_mask_8: 0.4204  loss_dice_8: 3.154  time: 1.5043  data_time: 0.0644  lr: 6.7676e-06  max_mem: 21589M
[01/18 01:15:07] d2.utils.events INFO:  eta: 10:39:04  iter: 14099  total_loss: 39.03  loss_ce: 0.3174  loss_mask: 0.4277  loss_dice: 3.081  loss_ce_0: 0.564  loss_mask_0: 0.4127  loss_dice_0: 3.228  loss_ce_1: 0.335  loss_mask_1: 0.4318  loss_dice_1: 3.134  loss_ce_2: 0.3449  loss_mask_2: 0.4297  loss_dice_2: 3.11  loss_ce_3: 0.3145  loss_mask_3: 0.429  loss_dice_3: 3.107  loss_ce_4: 0.3253  loss_mask_4: 0.429  loss_dice_4: 3.101  loss_ce_5: 0.306  loss_mask_5: 0.4287  loss_dice_5: 3.092  loss_ce_6: 0.3157  loss_mask_6: 0.4285  loss_dice_6: 3.092  loss_ce_7: 0.3222  loss_mask_7: 0.4311  loss_dice_7: 3.091  loss_ce_8: 0.3151  loss_mask_8: 0.4288  loss_dice_8: 3.093  time: 1.5042  data_time: 0.0641  lr: 6.7629e-06  max_mem: 21589M
[01/18 01:15:37] d2.utils.events INFO:  eta: 10:38:20  iter: 14119  total_loss: 40.02  loss_ce: 0.3545  loss_mask: 0.4253  loss_dice: 3.193  loss_ce_0: 0.584  loss_mask_0: 0.4088  loss_dice_0: 3.332  loss_ce_1: 0.3692  loss_mask_1: 0.4289  loss_dice_1: 3.233  loss_ce_2: 0.3618  loss_mask_2: 0.428  loss_dice_2: 3.214  loss_ce_3: 0.348  loss_mask_3: 0.4256  loss_dice_3: 3.201  loss_ce_4: 0.343  loss_mask_4: 0.4239  loss_dice_4: 3.201  loss_ce_5: 0.3529  loss_mask_5: 0.423  loss_dice_5: 3.203  loss_ce_6: 0.3436  loss_mask_6: 0.4256  loss_dice_6: 3.194  loss_ce_7: 0.3448  loss_mask_7: 0.4248  loss_dice_7: 3.191  loss_ce_8: 0.3327  loss_mask_8: 0.4252  loss_dice_8: 3.203  time: 1.5041  data_time: 0.0705  lr: 6.7582e-06  max_mem: 21589M
[01/18 01:16:05] d2.utils.events INFO:  eta: 10:37:18  iter: 14139  total_loss: 39.43  loss_ce: 0.3448  loss_mask: 0.434  loss_dice: 3.116  loss_ce_0: 0.5738  loss_mask_0: 0.4163  loss_dice_0: 3.256  loss_ce_1: 0.3428  loss_mask_1: 0.4376  loss_dice_1: 3.157  loss_ce_2: 0.3452  loss_mask_2: 0.4331  loss_dice_2: 3.134  loss_ce_3: 0.3449  loss_mask_3: 0.4315  loss_dice_3: 3.121  loss_ce_4: 0.3306  loss_mask_4: 0.4315  loss_dice_4: 3.12  loss_ce_5: 0.3214  loss_mask_5: 0.4325  loss_dice_5: 3.122  loss_ce_6: 0.3223  loss_mask_6: 0.4321  loss_dice_6: 3.118  loss_ce_7: 0.3261  loss_mask_7: 0.4341  loss_dice_7: 3.122  loss_ce_8: 0.3403  loss_mask_8: 0.4338  loss_dice_8: 3.124  time: 1.5040  data_time: 0.0663  lr: 6.7535e-06  max_mem: 21589M
[01/18 01:16:34] d2.utils.events INFO:  eta: 10:36:35  iter: 14159  total_loss: 38.95  loss_ce: 0.3321  loss_mask: 0.4192  loss_dice: 3.12  loss_ce_0: 0.5922  loss_mask_0: 0.4009  loss_dice_0: 3.255  loss_ce_1: 0.3612  loss_mask_1: 0.4205  loss_dice_1: 3.157  loss_ce_2: 0.3595  loss_mask_2: 0.4213  loss_dice_2: 3.136  loss_ce_3: 0.3479  loss_mask_3: 0.4205  loss_dice_3: 3.121  loss_ce_4: 0.334  loss_mask_4: 0.42  loss_dice_4: 3.125  loss_ce_5: 0.3312  loss_mask_5: 0.4184  loss_dice_5: 3.115  loss_ce_6: 0.3438  loss_mask_6: 0.4192  loss_dice_6: 3.113  loss_ce_7: 0.3303  loss_mask_7: 0.4192  loss_dice_7: 3.115  loss_ce_8: 0.3288  loss_mask_8: 0.4206  loss_dice_8: 3.114  time: 1.5039  data_time: 0.0650  lr: 6.7488e-06  max_mem: 21589M
[01/18 01:17:03] d2.utils.events INFO:  eta: 10:36:07  iter: 14179  total_loss: 39.65  loss_ce: 0.3225  loss_mask: 0.4235  loss_dice: 3.186  loss_ce_0: 0.5838  loss_mask_0: 0.4116  loss_dice_0: 3.325  loss_ce_1: 0.3321  loss_mask_1: 0.4276  loss_dice_1: 3.231  loss_ce_2: 0.3429  loss_mask_2: 0.4245  loss_dice_2: 3.208  loss_ce_3: 0.3295  loss_mask_3: 0.4236  loss_dice_3: 3.19  loss_ce_4: 0.3315  loss_mask_4: 0.4237  loss_dice_4: 3.186  loss_ce_5: 0.3285  loss_mask_5: 0.4231  loss_dice_5: 3.186  loss_ce_6: 0.321  loss_mask_6: 0.4233  loss_dice_6: 3.183  loss_ce_7: 0.3356  loss_mask_7: 0.4249  loss_dice_7: 3.183  loss_ce_8: 0.3172  loss_mask_8: 0.4245  loss_dice_8: 3.186  time: 1.5039  data_time: 0.0689  lr: 6.7441e-06  max_mem: 21589M
[01/18 01:17:33] d2.utils.events INFO:  eta: 10:35:28  iter: 14199  total_loss: 39.72  loss_ce: 0.3595  loss_mask: 0.4136  loss_dice: 3.135  loss_ce_0: 0.6006  loss_mask_0: 0.4003  loss_dice_0: 3.269  loss_ce_1: 0.378  loss_mask_1: 0.4154  loss_dice_1: 3.168  loss_ce_2: 0.3879  loss_mask_2: 0.414  loss_dice_2: 3.138  loss_ce_3: 0.3697  loss_mask_3: 0.4141  loss_dice_3: 3.13  loss_ce_4: 0.3681  loss_mask_4: 0.4137  loss_dice_4: 3.134  loss_ce_5: 0.3565  loss_mask_5: 0.4136  loss_dice_5: 3.133  loss_ce_6: 0.3698  loss_mask_6: 0.4146  loss_dice_6: 3.128  loss_ce_7: 0.3657  loss_mask_7: 0.4152  loss_dice_7: 3.13  loss_ce_8: 0.3593  loss_mask_8: 0.4133  loss_dice_8: 3.132  time: 1.5038  data_time: 0.0861  lr: 6.7394e-06  max_mem: 21589M
[01/18 01:18:02] d2.utils.events INFO:  eta: 10:34:54  iter: 14219  total_loss: 39.79  loss_ce: 0.325  loss_mask: 0.4168  loss_dice: 3.159  loss_ce_0: 0.5789  loss_mask_0: 0.406  loss_dice_0: 3.299  loss_ce_1: 0.3616  loss_mask_1: 0.4217  loss_dice_1: 3.184  loss_ce_2: 0.3629  loss_mask_2: 0.4174  loss_dice_2: 3.175  loss_ce_3: 0.3503  loss_mask_3: 0.4172  loss_dice_3: 3.161  loss_ce_4: 0.3316  loss_mask_4: 0.4177  loss_dice_4: 3.16  loss_ce_5: 0.3491  loss_mask_5: 0.4167  loss_dice_5: 3.154  loss_ce_6: 0.3331  loss_mask_6: 0.4178  loss_dice_6: 3.15  loss_ce_7: 0.3361  loss_mask_7: 0.4169  loss_dice_7: 3.167  loss_ce_8: 0.3366  loss_mask_8: 0.4165  loss_dice_8: 3.16  time: 1.5038  data_time: 0.0681  lr: 6.7347e-06  max_mem: 21589M
[01/18 01:18:32] d2.utils.events INFO:  eta: 10:34:02  iter: 14239  total_loss: 40.29  loss_ce: 0.3265  loss_mask: 0.4272  loss_dice: 3.241  loss_ce_0: 0.5894  loss_mask_0: 0.4179  loss_dice_0: 3.372  loss_ce_1: 0.3603  loss_mask_1: 0.4295  loss_dice_1: 3.284  loss_ce_2: 0.3337  loss_mask_2: 0.4281  loss_dice_2: 3.261  loss_ce_3: 0.3404  loss_mask_3: 0.4298  loss_dice_3: 3.245  loss_ce_4: 0.3352  loss_mask_4: 0.4288  loss_dice_4: 3.241  loss_ce_5: 0.3301  loss_mask_5: 0.4278  loss_dice_5: 3.245  loss_ce_6: 0.3304  loss_mask_6: 0.428  loss_dice_6: 3.242  loss_ce_7: 0.3261  loss_mask_7: 0.4286  loss_dice_7: 3.248  loss_ce_8: 0.3362  loss_mask_8: 0.4275  loss_dice_8: 3.248  time: 1.5037  data_time: 0.0756  lr: 6.73e-06  max_mem: 21589M
[01/18 01:19:01] d2.utils.events INFO:  eta: 10:32:58  iter: 14259  total_loss: 39.16  loss_ce: 0.3326  loss_mask: 0.42  loss_dice: 3.103  loss_ce_0: 0.5835  loss_mask_0: 0.4022  loss_dice_0: 3.234  loss_ce_1: 0.3625  loss_mask_1: 0.4185  loss_dice_1: 3.136  loss_ce_2: 0.3586  loss_mask_2: 0.4184  loss_dice_2: 3.115  loss_ce_3: 0.3324  loss_mask_3: 0.4217  loss_dice_3: 3.103  loss_ce_4: 0.3319  loss_mask_4: 0.42  loss_dice_4: 3.096  loss_ce_5: 0.3353  loss_mask_5: 0.4203  loss_dice_5: 3.107  loss_ce_6: 0.3257  loss_mask_6: 0.4221  loss_dice_6: 3.095  loss_ce_7: 0.3314  loss_mask_7: 0.4206  loss_dice_7: 3.1  loss_ce_8: 0.3227  loss_mask_8: 0.4224  loss_dice_8: 3.097  time: 1.5037  data_time: 0.0660  lr: 6.7253e-06  max_mem: 21589M
[01/18 01:19:30] d2.utils.events INFO:  eta: 10:31:55  iter: 14279  total_loss: 39.62  loss_ce: 0.3245  loss_mask: 0.4268  loss_dice: 3.151  loss_ce_0: 0.5812  loss_mask_0: 0.4121  loss_dice_0: 3.274  loss_ce_1: 0.3563  loss_mask_1: 0.4326  loss_dice_1: 3.186  loss_ce_2: 0.3705  loss_mask_2: 0.4282  loss_dice_2: 3.17  loss_ce_3: 0.3424  loss_mask_3: 0.4275  loss_dice_3: 3.144  loss_ce_4: 0.3331  loss_mask_4: 0.4248  loss_dice_4: 3.153  loss_ce_5: 0.3182  loss_mask_5: 0.4259  loss_dice_5: 3.157  loss_ce_6: 0.3322  loss_mask_6: 0.4259  loss_dice_6: 3.144  loss_ce_7: 0.3342  loss_mask_7: 0.4261  loss_dice_7: 3.137  loss_ce_8: 0.3372  loss_mask_8: 0.4285  loss_dice_8: 3.146  time: 1.5036  data_time: 0.0673  lr: 6.7206e-06  max_mem: 21589M
[01/18 01:19:59] d2.utils.events INFO:  eta: 10:31:03  iter: 14299  total_loss: 39.24  loss_ce: 0.3228  loss_mask: 0.4269  loss_dice: 3.118  loss_ce_0: 0.6028  loss_mask_0: 0.4047  loss_dice_0: 3.252  loss_ce_1: 0.3486  loss_mask_1: 0.4317  loss_dice_1: 3.154  loss_ce_2: 0.3538  loss_mask_2: 0.4301  loss_dice_2: 3.125  loss_ce_3: 0.3454  loss_mask_3: 0.427  loss_dice_3: 3.122  loss_ce_4: 0.3421  loss_mask_4: 0.4279  loss_dice_4: 3.111  loss_ce_5: 0.3348  loss_mask_5: 0.4269  loss_dice_5: 3.116  loss_ce_6: 0.3334  loss_mask_6: 0.4259  loss_dice_6: 3.107  loss_ce_7: 0.338  loss_mask_7: 0.4272  loss_dice_7: 3.115  loss_ce_8: 0.3421  loss_mask_8: 0.4272  loss_dice_8: 3.124  time: 1.5035  data_time: 0.0668  lr: 6.7159e-06  max_mem: 21589M
[01/18 01:20:28] d2.utils.events INFO:  eta: 10:29:58  iter: 14319  total_loss: 39.49  loss_ce: 0.328  loss_mask: 0.423  loss_dice: 3.107  loss_ce_0: 0.5767  loss_mask_0: 0.4113  loss_dice_0: 3.255  loss_ce_1: 0.3513  loss_mask_1: 0.4293  loss_dice_1: 3.145  loss_ce_2: 0.3566  loss_mask_2: 0.4243  loss_dice_2: 3.127  loss_ce_3: 0.3425  loss_mask_3: 0.4237  loss_dice_3: 3.11  loss_ce_4: 0.344  loss_mask_4: 0.423  loss_dice_4: 3.114  loss_ce_5: 0.3364  loss_mask_5: 0.4235  loss_dice_5: 3.115  loss_ce_6: 0.3411  loss_mask_6: 0.4223  loss_dice_6: 3.107  loss_ce_7: 0.344  loss_mask_7: 0.4231  loss_dice_7: 3.117  loss_ce_8: 0.3322  loss_mask_8: 0.4224  loss_dice_8: 3.113  time: 1.5035  data_time: 0.0678  lr: 6.7111e-06  max_mem: 21589M
[01/18 01:20:58] d2.utils.events INFO:  eta: 10:29:11  iter: 14339  total_loss: 39.53  loss_ce: 0.3241  loss_mask: 0.4257  loss_dice: 3.158  loss_ce_0: 0.5791  loss_mask_0: 0.4141  loss_dice_0: 3.275  loss_ce_1: 0.3473  loss_mask_1: 0.433  loss_dice_1: 3.192  loss_ce_2: 0.3426  loss_mask_2: 0.4294  loss_dice_2: 3.166  loss_ce_3: 0.3307  loss_mask_3: 0.4285  loss_dice_3: 3.162  loss_ce_4: 0.334  loss_mask_4: 0.4282  loss_dice_4: 3.146  loss_ce_5: 0.3131  loss_mask_5: 0.4285  loss_dice_5: 3.151  loss_ce_6: 0.3275  loss_mask_6: 0.4282  loss_dice_6: 3.157  loss_ce_7: 0.3278  loss_mask_7: 0.4287  loss_dice_7: 3.161  loss_ce_8: 0.3271  loss_mask_8: 0.427  loss_dice_8: 3.151  time: 1.5034  data_time: 0.0706  lr: 6.7064e-06  max_mem: 21589M
[01/18 01:21:27] d2.utils.events INFO:  eta: 10:28:41  iter: 14359  total_loss: 40.32  loss_ce: 0.3466  loss_mask: 0.4245  loss_dice: 3.212  loss_ce_0: 0.5941  loss_mask_0: 0.4112  loss_dice_0: 3.351  loss_ce_1: 0.4146  loss_mask_1: 0.4271  loss_dice_1: 3.251  loss_ce_2: 0.3808  loss_mask_2: 0.4266  loss_dice_2: 3.224  loss_ce_3: 0.3727  loss_mask_3: 0.4225  loss_dice_3: 3.214  loss_ce_4: 0.3686  loss_mask_4: 0.4222  loss_dice_4: 3.215  loss_ce_5: 0.3477  loss_mask_5: 0.4213  loss_dice_5: 3.215  loss_ce_6: 0.3483  loss_mask_6: 0.4233  loss_dice_6: 3.214  loss_ce_7: 0.3522  loss_mask_7: 0.4217  loss_dice_7: 3.216  loss_ce_8: 0.3446  loss_mask_8: 0.4227  loss_dice_8: 3.217  time: 1.5033  data_time: 0.0698  lr: 6.7017e-06  max_mem: 21589M
[01/18 01:21:56] d2.utils.events INFO:  eta: 10:28:01  iter: 14379  total_loss: 39.55  loss_ce: 0.329  loss_mask: 0.4259  loss_dice: 3.123  loss_ce_0: 0.5796  loss_mask_0: 0.4185  loss_dice_0: 3.278  loss_ce_1: 0.3646  loss_mask_1: 0.4292  loss_dice_1: 3.176  loss_ce_2: 0.3572  loss_mask_2: 0.4265  loss_dice_2: 3.154  loss_ce_3: 0.3263  loss_mask_3: 0.4257  loss_dice_3: 3.129  loss_ce_4: 0.3273  loss_mask_4: 0.4247  loss_dice_4: 3.134  loss_ce_5: 0.3237  loss_mask_5: 0.424  loss_dice_5: 3.136  loss_ce_6: 0.3141  loss_mask_6: 0.4245  loss_dice_6: 3.126  loss_ce_7: 0.3303  loss_mask_7: 0.4264  loss_dice_7: 3.135  loss_ce_8: 0.3266  loss_mask_8: 0.426  loss_dice_8: 3.14  time: 1.5033  data_time: 0.0701  lr: 6.697e-06  max_mem: 21589M
[01/18 01:22:26] d2.utils.events INFO:  eta: 10:27:15  iter: 14399  total_loss: 40.26  loss_ce: 0.3282  loss_mask: 0.4238  loss_dice: 3.193  loss_ce_0: 0.6085  loss_mask_0: 0.4218  loss_dice_0: 3.314  loss_ce_1: 0.3701  loss_mask_1: 0.4274  loss_dice_1: 3.229  loss_ce_2: 0.3698  loss_mask_2: 0.4241  loss_dice_2: 3.207  loss_ce_3: 0.3486  loss_mask_3: 0.4211  loss_dice_3: 3.201  loss_ce_4: 0.3415  loss_mask_4: 0.4231  loss_dice_4: 3.196  loss_ce_5: 0.3381  loss_mask_5: 0.423  loss_dice_5: 3.198  loss_ce_6: 0.3326  loss_mask_6: 0.4218  loss_dice_6: 3.187  loss_ce_7: 0.3382  loss_mask_7: 0.4233  loss_dice_7: 3.189  loss_ce_8: 0.3293  loss_mask_8: 0.4221  loss_dice_8: 3.186  time: 1.5032  data_time: 0.0684  lr: 6.6923e-06  max_mem: 21589M
[01/18 01:22:55] d2.utils.events INFO:  eta: 10:26:27  iter: 14419  total_loss: 39.02  loss_ce: 0.3116  loss_mask: 0.419  loss_dice: 3.112  loss_ce_0: 0.5805  loss_mask_0: 0.4142  loss_dice_0: 3.258  loss_ce_1: 0.3384  loss_mask_1: 0.4231  loss_dice_1: 3.152  loss_ce_2: 0.3423  loss_mask_2: 0.4205  loss_dice_2: 3.133  loss_ce_3: 0.3272  loss_mask_3: 0.4198  loss_dice_3: 3.126  loss_ce_4: 0.32  loss_mask_4: 0.4183  loss_dice_4: 3.11  loss_ce_5: 0.3086  loss_mask_5: 0.4175  loss_dice_5: 3.106  loss_ce_6: 0.3362  loss_mask_6: 0.418  loss_dice_6: 3.115  loss_ce_7: 0.3186  loss_mask_7: 0.4182  loss_dice_7: 3.109  loss_ce_8: 0.3155  loss_mask_8: 0.418  loss_dice_8: 3.113  time: 1.5032  data_time: 0.0850  lr: 6.6876e-06  max_mem: 21589M
[01/18 01:23:24] d2.utils.events INFO:  eta: 10:25:48  iter: 14439  total_loss: 39.41  loss_ce: 0.3258  loss_mask: 0.4174  loss_dice: 3.157  loss_ce_0: 0.5804  loss_mask_0: 0.4034  loss_dice_0: 3.294  loss_ce_1: 0.3591  loss_mask_1: 0.4209  loss_dice_1: 3.198  loss_ce_2: 0.3518  loss_mask_2: 0.4209  loss_dice_2: 3.177  loss_ce_3: 0.3306  loss_mask_3: 0.4221  loss_dice_3: 3.166  loss_ce_4: 0.3318  loss_mask_4: 0.4192  loss_dice_4: 3.163  loss_ce_5: 0.3387  loss_mask_5: 0.4185  loss_dice_5: 3.155  loss_ce_6: 0.3386  loss_mask_6: 0.4188  loss_dice_6: 3.147  loss_ce_7: 0.3202  loss_mask_7: 0.4203  loss_dice_7: 3.158  loss_ce_8: 0.3177  loss_mask_8: 0.4196  loss_dice_8: 3.156  time: 1.5031  data_time: 0.0776  lr: 6.6829e-06  max_mem: 21589M
[01/18 01:23:53] d2.utils.events INFO:  eta: 10:25:17  iter: 14459  total_loss: 39.54  loss_ce: 0.3153  loss_mask: 0.4292  loss_dice: 3.148  loss_ce_0: 0.5682  loss_mask_0: 0.4162  loss_dice_0: 3.274  loss_ce_1: 0.3358  loss_mask_1: 0.4323  loss_dice_1: 3.185  loss_ce_2: 0.3467  loss_mask_2: 0.4306  loss_dice_2: 3.164  loss_ce_3: 0.3377  loss_mask_3: 0.4259  loss_dice_3: 3.151  loss_ce_4: 0.3347  loss_mask_4: 0.4245  loss_dice_4: 3.156  loss_ce_5: 0.3346  loss_mask_5: 0.4252  loss_dice_5: 3.153  loss_ce_6: 0.3255  loss_mask_6: 0.4249  loss_dice_6: 3.151  loss_ce_7: 0.3122  loss_mask_7: 0.4263  loss_dice_7: 3.154  loss_ce_8: 0.3235  loss_mask_8: 0.4254  loss_dice_8: 3.146  time: 1.5030  data_time: 0.0666  lr: 6.6782e-06  max_mem: 21589M
[01/18 01:24:21] d2.utils.events INFO:  eta: 10:23:58  iter: 14479  total_loss: 39.06  loss_ce: 0.3148  loss_mask: 0.4326  loss_dice: 3.091  loss_ce_0: 0.5813  loss_mask_0: 0.4179  loss_dice_0: 3.23  loss_ce_1: 0.3472  loss_mask_1: 0.4346  loss_dice_1: 3.132  loss_ce_2: 0.3437  loss_mask_2: 0.4304  loss_dice_2: 3.1  loss_ce_3: 0.3286  loss_mask_3: 0.4311  loss_dice_3: 3.098  loss_ce_4: 0.3219  loss_mask_4: 0.4312  loss_dice_4: 3.094  loss_ce_5: 0.3192  loss_mask_5: 0.4308  loss_dice_5: 3.095  loss_ce_6: 0.3097  loss_mask_6: 0.434  loss_dice_6: 3.087  loss_ce_7: 0.3087  loss_mask_7: 0.4333  loss_dice_7: 3.088  loss_ce_8: 0.3088  loss_mask_8: 0.434  loss_dice_8: 3.094  time: 1.5029  data_time: 0.0679  lr: 6.6735e-06  max_mem: 21589M
[01/18 01:24:50] d2.utils.events INFO:  eta: 10:23:11  iter: 14499  total_loss: 39.98  loss_ce: 0.3406  loss_mask: 0.4105  loss_dice: 3.208  loss_ce_0: 0.5992  loss_mask_0: 0.3972  loss_dice_0: 3.353  loss_ce_1: 0.3487  loss_mask_1: 0.4093  loss_dice_1: 3.261  loss_ce_2: 0.3794  loss_mask_2: 0.4096  loss_dice_2: 3.221  loss_ce_3: 0.3606  loss_mask_3: 0.411  loss_dice_3: 3.211  loss_ce_4: 0.3444  loss_mask_4: 0.412  loss_dice_4: 3.204  loss_ce_5: 0.3518  loss_mask_5: 0.4117  loss_dice_5: 3.207  loss_ce_6: 0.3248  loss_mask_6: 0.4132  loss_dice_6: 3.209  loss_ce_7: 0.3466  loss_mask_7: 0.4111  loss_dice_7: 3.197  loss_ce_8: 0.3387  loss_mask_8: 0.4127  loss_dice_8: 3.214  time: 1.5028  data_time: 0.0719  lr: 6.6688e-06  max_mem: 21589M
[01/18 01:25:19] d2.utils.events INFO:  eta: 10:22:02  iter: 14519  total_loss: 39.27  loss_ce: 0.3312  loss_mask: 0.4325  loss_dice: 3.119  loss_ce_0: 0.5786  loss_mask_0: 0.4233  loss_dice_0: 3.268  loss_ce_1: 0.3485  loss_mask_1: 0.4389  loss_dice_1: 3.172  loss_ce_2: 0.3532  loss_mask_2: 0.4336  loss_dice_2: 3.15  loss_ce_3: 0.3248  loss_mask_3: 0.4311  loss_dice_3: 3.124  loss_ce_4: 0.3392  loss_mask_4: 0.4305  loss_dice_4: 3.126  loss_ce_5: 0.3122  loss_mask_5: 0.4308  loss_dice_5: 3.123  loss_ce_6: 0.3146  loss_mask_6: 0.4312  loss_dice_6: 3.123  loss_ce_7: 0.3247  loss_mask_7: 0.4322  loss_dice_7: 3.123  loss_ce_8: 0.3288  loss_mask_8: 0.4336  loss_dice_8: 3.117  time: 1.5027  data_time: 0.0662  lr: 6.6641e-06  max_mem: 21589M
[01/18 01:25:48] d2.utils.events INFO:  eta: 10:21:29  iter: 14539  total_loss: 39.02  loss_ce: 0.3311  loss_mask: 0.4202  loss_dice: 3.094  loss_ce_0: 0.5941  loss_mask_0: 0.4106  loss_dice_0: 3.238  loss_ce_1: 0.338  loss_mask_1: 0.4257  loss_dice_1: 3.149  loss_ce_2: 0.3435  loss_mask_2: 0.4231  loss_dice_2: 3.121  loss_ce_3: 0.3457  loss_mask_3: 0.4239  loss_dice_3: 3.092  loss_ce_4: 0.3367  loss_mask_4: 0.4233  loss_dice_4: 3.093  loss_ce_5: 0.3289  loss_mask_5: 0.4225  loss_dice_5: 3.105  loss_ce_6: 0.3257  loss_mask_6: 0.4216  loss_dice_6: 3.094  loss_ce_7: 0.3289  loss_mask_7: 0.4203  loss_dice_7: 3.092  loss_ce_8: 0.3279  loss_mask_8: 0.4217  loss_dice_8: 3.104  time: 1.5027  data_time: 0.0737  lr: 6.6594e-06  max_mem: 21589M
[01/18 01:26:17] d2.utils.events INFO:  eta: 10:20:49  iter: 14559  total_loss: 39.97  loss_ce: 0.3238  loss_mask: 0.4361  loss_dice: 3.136  loss_ce_0: 0.5943  loss_mask_0: 0.4272  loss_dice_0: 3.254  loss_ce_1: 0.3715  loss_mask_1: 0.4433  loss_dice_1: 3.17  loss_ce_2: 0.3814  loss_mask_2: 0.4402  loss_dice_2: 3.145  loss_ce_3: 0.3617  loss_mask_3: 0.4362  loss_dice_3: 3.135  loss_ce_4: 0.3293  loss_mask_4: 0.4382  loss_dice_4: 3.142  loss_ce_5: 0.3331  loss_mask_5: 0.4386  loss_dice_5: 3.145  loss_ce_6: 0.3465  loss_mask_6: 0.4378  loss_dice_6: 3.136  loss_ce_7: 0.3398  loss_mask_7: 0.4367  loss_dice_7: 3.135  loss_ce_8: 0.3331  loss_mask_8: 0.4356  loss_dice_8: 3.133  time: 1.5026  data_time: 0.0638  lr: 6.6547e-06  max_mem: 21589M
[01/18 01:26:46] d2.utils.events INFO:  eta: 10:20:12  iter: 14579  total_loss: 39.14  loss_ce: 0.334  loss_mask: 0.4229  loss_dice: 3.097  loss_ce_0: 0.6075  loss_mask_0: 0.4082  loss_dice_0: 3.235  loss_ce_1: 0.3662  loss_mask_1: 0.4281  loss_dice_1: 3.134  loss_ce_2: 0.3599  loss_mask_2: 0.4291  loss_dice_2: 3.116  loss_ce_3: 0.3341  loss_mask_3: 0.4269  loss_dice_3: 3.105  loss_ce_4: 0.3319  loss_mask_4: 0.4288  loss_dice_4: 3.101  loss_ce_5: 0.3377  loss_mask_5: 0.4255  loss_dice_5: 3.101  loss_ce_6: 0.3318  loss_mask_6: 0.4232  loss_dice_6: 3.101  loss_ce_7: 0.3341  loss_mask_7: 0.4247  loss_dice_7: 3.097  loss_ce_8: 0.3418  loss_mask_8: 0.4221  loss_dice_8: 3.092  time: 1.5025  data_time: 0.0689  lr: 6.65e-06  max_mem: 21589M
[01/18 01:27:15] d2.utils.events INFO:  eta: 10:19:13  iter: 14599  total_loss: 39.56  loss_ce: 0.3222  loss_mask: 0.4224  loss_dice: 3.15  loss_ce_0: 0.5824  loss_mask_0: 0.4096  loss_dice_0: 3.291  loss_ce_1: 0.3361  loss_mask_1: 0.4209  loss_dice_1: 3.202  loss_ce_2: 0.341  loss_mask_2: 0.4208  loss_dice_2: 3.173  loss_ce_3: 0.3335  loss_mask_3: 0.4202  loss_dice_3: 3.157  loss_ce_4: 0.3318  loss_mask_4: 0.4193  loss_dice_4: 3.154  loss_ce_5: 0.3211  loss_mask_5: 0.4187  loss_dice_5: 3.162  loss_ce_6: 0.3168  loss_mask_6: 0.4199  loss_dice_6: 3.153  loss_ce_7: 0.3295  loss_mask_7: 0.4201  loss_dice_7: 3.155  loss_ce_8: 0.3234  loss_mask_8: 0.4214  loss_dice_8: 3.153  time: 1.5024  data_time: 0.0660  lr: 6.6453e-06  max_mem: 21589M
[01/18 01:27:44] d2.utils.events INFO:  eta: 10:18:32  iter: 14619  total_loss: 39.31  loss_ce: 0.3294  loss_mask: 0.425  loss_dice: 3.093  loss_ce_0: 0.5899  loss_mask_0: 0.4163  loss_dice_0: 3.236  loss_ce_1: 0.3587  loss_mask_1: 0.4313  loss_dice_1: 3.138  loss_ce_2: 0.3673  loss_mask_2: 0.4285  loss_dice_2: 3.113  loss_ce_3: 0.3478  loss_mask_3: 0.4269  loss_dice_3: 3.109  loss_ce_4: 0.3384  loss_mask_4: 0.4251  loss_dice_4: 3.115  loss_ce_5: 0.3096  loss_mask_5: 0.4292  loss_dice_5: 3.111  loss_ce_6: 0.3216  loss_mask_6: 0.4266  loss_dice_6: 3.111  loss_ce_7: 0.324  loss_mask_7: 0.4262  loss_dice_7: 3.105  loss_ce_8: 0.322  loss_mask_8: 0.4255  loss_dice_8: 3.103  time: 1.5023  data_time: 0.0682  lr: 6.6405e-06  max_mem: 21589M
[01/18 01:28:12] d2.utils.events INFO:  eta: 10:17:16  iter: 14639  total_loss: 38.57  loss_ce: 0.3482  loss_mask: 0.4216  loss_dice: 3.044  loss_ce_0: 0.6105  loss_mask_0: 0.4154  loss_dice_0: 3.201  loss_ce_1: 0.3786  loss_mask_1: 0.4231  loss_dice_1: 3.081  loss_ce_2: 0.3499  loss_mask_2: 0.4221  loss_dice_2: 3.066  loss_ce_3: 0.3502  loss_mask_3: 0.4212  loss_dice_3: 3.052  loss_ce_4: 0.3467  loss_mask_4: 0.4215  loss_dice_4: 3.045  loss_ce_5: 0.3399  loss_mask_5: 0.4214  loss_dice_5: 3.051  loss_ce_6: 0.3378  loss_mask_6: 0.4198  loss_dice_6: 3.044  loss_ce_7: 0.3381  loss_mask_7: 0.4219  loss_dice_7: 3.052  loss_ce_8: 0.3427  loss_mask_8: 0.4225  loss_dice_8: 3.042  time: 1.5022  data_time: 0.0623  lr: 6.6358e-06  max_mem: 21589M
[01/18 01:28:41] d2.utils.events INFO:  eta: 10:16:30  iter: 14659  total_loss: 39.1  loss_ce: 0.3243  loss_mask: 0.4144  loss_dice: 3.099  loss_ce_0: 0.5971  loss_mask_0: 0.4008  loss_dice_0: 3.229  loss_ce_1: 0.3516  loss_mask_1: 0.4195  loss_dice_1: 3.137  loss_ce_2: 0.3554  loss_mask_2: 0.4193  loss_dice_2: 3.116  loss_ce_3: 0.3503  loss_mask_3: 0.4178  loss_dice_3: 3.105  loss_ce_4: 0.3346  loss_mask_4: 0.4175  loss_dice_4: 3.1  loss_ce_5: 0.3303  loss_mask_5: 0.4158  loss_dice_5: 3.103  loss_ce_6: 0.3374  loss_mask_6: 0.417  loss_dice_6: 3.099  loss_ce_7: 0.3429  loss_mask_7: 0.4145  loss_dice_7: 3.103  loss_ce_8: 0.3292  loss_mask_8: 0.4157  loss_dice_8: 3.103  time: 1.5021  data_time: 0.0699  lr: 6.6311e-06  max_mem: 21589M
[01/18 01:29:10] d2.utils.events INFO:  eta: 10:15:49  iter: 14679  total_loss: 39.59  loss_ce: 0.3448  loss_mask: 0.4154  loss_dice: 3.154  loss_ce_0: 0.5735  loss_mask_0: 0.4062  loss_dice_0: 3.297  loss_ce_1: 0.3936  loss_mask_1: 0.4221  loss_dice_1: 3.203  loss_ce_2: 0.3685  loss_mask_2: 0.4168  loss_dice_2: 3.172  loss_ce_3: 0.3667  loss_mask_3: 0.4141  loss_dice_3: 3.158  loss_ce_4: 0.3547  loss_mask_4: 0.4171  loss_dice_4: 3.158  loss_ce_5: 0.3614  loss_mask_5: 0.415  loss_dice_5: 3.164  loss_ce_6: 0.3512  loss_mask_6: 0.4145  loss_dice_6: 3.166  loss_ce_7: 0.3404  loss_mask_7: 0.4149  loss_dice_7: 3.157  loss_ce_8: 0.3427  loss_mask_8: 0.4148  loss_dice_8: 3.156  time: 1.5020  data_time: 0.0783  lr: 6.6264e-06  max_mem: 21589M
[01/18 01:29:39] d2.utils.events INFO:  eta: 10:14:56  iter: 14699  total_loss: 40.45  loss_ce: 0.3367  loss_mask: 0.4379  loss_dice: 3.172  loss_ce_0: 0.5828  loss_mask_0: 0.4283  loss_dice_0: 3.284  loss_ce_1: 0.3787  loss_mask_1: 0.4511  loss_dice_1: 3.205  loss_ce_2: 0.3804  loss_mask_2: 0.4426  loss_dice_2: 3.182  loss_ce_3: 0.357  loss_mask_3: 0.4356  loss_dice_3: 3.183  loss_ce_4: 0.3443  loss_mask_4: 0.4396  loss_dice_4: 3.17  loss_ce_5: 0.351  loss_mask_5: 0.4407  loss_dice_5: 3.167  loss_ce_6: 0.3367  loss_mask_6: 0.4385  loss_dice_6: 3.171  loss_ce_7: 0.3307  loss_mask_7: 0.439  loss_dice_7: 3.176  loss_ce_8: 0.3302  loss_mask_8: 0.4394  loss_dice_8: 3.172  time: 1.5020  data_time: 0.0609  lr: 6.6217e-06  max_mem: 21589M
[01/18 01:30:07] d2.utils.events INFO:  eta: 10:14:05  iter: 14719  total_loss: 39.75  loss_ce: 0.3465  loss_mask: 0.423  loss_dice: 3.159  loss_ce_0: 0.6049  loss_mask_0: 0.4204  loss_dice_0: 3.293  loss_ce_1: 0.3813  loss_mask_1: 0.4326  loss_dice_1: 3.203  loss_ce_2: 0.3761  loss_mask_2: 0.4311  loss_dice_2: 3.179  loss_ce_3: 0.3504  loss_mask_3: 0.4252  loss_dice_3: 3.17  loss_ce_4: 0.3561  loss_mask_4: 0.4242  loss_dice_4: 3.165  loss_ce_5: 0.3602  loss_mask_5: 0.4236  loss_dice_5: 3.172  loss_ce_6: 0.3383  loss_mask_6: 0.423  loss_dice_6: 3.166  loss_ce_7: 0.3547  loss_mask_7: 0.4242  loss_dice_7: 3.164  loss_ce_8: 0.3278  loss_mask_8: 0.4235  loss_dice_8: 3.162  time: 1.5019  data_time: 0.0641  lr: 6.617e-06  max_mem: 21589M
[01/18 01:30:36] d2.utils.events INFO:  eta: 10:13:35  iter: 14739  total_loss: 39.26  loss_ce: 0.3254  loss_mask: 0.4153  loss_dice: 3.133  loss_ce_0: 0.5639  loss_mask_0: 0.4052  loss_dice_0: 3.27  loss_ce_1: 0.3511  loss_mask_1: 0.4171  loss_dice_1: 3.172  loss_ce_2: 0.3456  loss_mask_2: 0.4165  loss_dice_2: 3.151  loss_ce_3: 0.3251  loss_mask_3: 0.4139  loss_dice_3: 3.141  loss_ce_4: 0.3233  loss_mask_4: 0.4139  loss_dice_4: 3.145  loss_ce_5: 0.3214  loss_mask_5: 0.4153  loss_dice_5: 3.136  loss_ce_6: 0.3197  loss_mask_6: 0.4159  loss_dice_6: 3.131  loss_ce_7: 0.3203  loss_mask_7: 0.4152  loss_dice_7: 3.139  loss_ce_8: 0.3184  loss_mask_8: 0.4154  loss_dice_8: 3.136  time: 1.5018  data_time: 0.0711  lr: 6.6123e-06  max_mem: 21589M
[01/18 01:31:05] d2.utils.events INFO:  eta: 10:12:38  iter: 14759  total_loss: 39.11  loss_ce: 0.3091  loss_mask: 0.423  loss_dice: 3.151  loss_ce_0: 0.5789  loss_mask_0: 0.408  loss_dice_0: 3.264  loss_ce_1: 0.3291  loss_mask_1: 0.4282  loss_dice_1: 3.179  loss_ce_2: 0.3386  loss_mask_2: 0.4294  loss_dice_2: 3.154  loss_ce_3: 0.3233  loss_mask_3: 0.4275  loss_dice_3: 3.139  loss_ce_4: 0.3202  loss_mask_4: 0.4262  loss_dice_4: 3.138  loss_ce_5: 0.3143  loss_mask_5: 0.4275  loss_dice_5: 3.137  loss_ce_6: 0.3156  loss_mask_6: 0.4257  loss_dice_6: 3.144  loss_ce_7: 0.3124  loss_mask_7: 0.424  loss_dice_7: 3.143  loss_ce_8: 0.317  loss_mask_8: 0.424  loss_dice_8: 3.136  time: 1.5017  data_time: 0.0702  lr: 6.6076e-06  max_mem: 21589M
[01/18 01:31:34] d2.utils.events INFO:  eta: 10:11:57  iter: 14779  total_loss: 39.79  loss_ce: 0.3458  loss_mask: 0.4165  loss_dice: 3.155  loss_ce_0: 0.6056  loss_mask_0: 0.4055  loss_dice_0: 3.284  loss_ce_1: 0.3819  loss_mask_1: 0.4228  loss_dice_1: 3.192  loss_ce_2: 0.3643  loss_mask_2: 0.4217  loss_dice_2: 3.189  loss_ce_3: 0.3559  loss_mask_3: 0.4177  loss_dice_3: 3.166  loss_ce_4: 0.3415  loss_mask_4: 0.4189  loss_dice_4: 3.174  loss_ce_5: 0.3481  loss_mask_5: 0.4159  loss_dice_5: 3.16  loss_ce_6: 0.33  loss_mask_6: 0.4173  loss_dice_6: 3.168  loss_ce_7: 0.333  loss_mask_7: 0.4183  loss_dice_7: 3.157  loss_ce_8: 0.3418  loss_mask_8: 0.4187  loss_dice_8: 3.158  time: 1.5016  data_time: 0.0701  lr: 6.6029e-06  max_mem: 21589M
[01/18 01:32:03] d2.utils.events INFO:  eta: 10:10:53  iter: 14799  total_loss: 39.86  loss_ce: 0.361  loss_mask: 0.4165  loss_dice: 3.148  loss_ce_0: 0.5919  loss_mask_0: 0.3983  loss_dice_0: 3.291  loss_ce_1: 0.3629  loss_mask_1: 0.4167  loss_dice_1: 3.191  loss_ce_2: 0.364  loss_mask_2: 0.4154  loss_dice_2: 3.164  loss_ce_3: 0.3512  loss_mask_3: 0.4163  loss_dice_3: 3.149  loss_ce_4: 0.3617  loss_mask_4: 0.417  loss_dice_4: 3.147  loss_ce_5: 0.3505  loss_mask_5: 0.4175  loss_dice_5: 3.147  loss_ce_6: 0.3536  loss_mask_6: 0.4182  loss_dice_6: 3.14  loss_ce_7: 0.3442  loss_mask_7: 0.4158  loss_dice_7: 3.158  loss_ce_8: 0.3519  loss_mask_8: 0.4169  loss_dice_8: 3.145  time: 1.5015  data_time: 0.0690  lr: 6.5981e-06  max_mem: 21589M
[01/18 01:32:32] d2.utils.events INFO:  eta: 10:10:21  iter: 14819  total_loss: 39.4  loss_ce: 0.3259  loss_mask: 0.4028  loss_dice: 3.143  loss_ce_0: 0.5991  loss_mask_0: 0.3952  loss_dice_0: 3.284  loss_ce_1: 0.3769  loss_mask_1: 0.4089  loss_dice_1: 3.18  loss_ce_2: 0.36  loss_mask_2: 0.4083  loss_dice_2: 3.162  loss_ce_3: 0.336  loss_mask_3: 0.4056  loss_dice_3: 3.151  loss_ce_4: 0.3204  loss_mask_4: 0.4036  loss_dice_4: 3.159  loss_ce_5: 0.325  loss_mask_5: 0.4013  loss_dice_5: 3.145  loss_ce_6: 0.337  loss_mask_6: 0.4043  loss_dice_6: 3.14  loss_ce_7: 0.3149  loss_mask_7: 0.4044  loss_dice_7: 3.141  loss_ce_8: 0.3074  loss_mask_8: 0.4035  loss_dice_8: 3.144  time: 1.5015  data_time: 0.0737  lr: 6.5934e-06  max_mem: 21589M
[01/18 01:33:01] d2.utils.events INFO:  eta: 10:09:20  iter: 14839  total_loss: 39.86  loss_ce: 0.3339  loss_mask: 0.4272  loss_dice: 3.206  loss_ce_0: 0.6157  loss_mask_0: 0.4158  loss_dice_0: 3.313  loss_ce_1: 0.3758  loss_mask_1: 0.4284  loss_dice_1: 3.237  loss_ce_2: 0.3799  loss_mask_2: 0.4266  loss_dice_2: 3.221  loss_ce_3: 0.3607  loss_mask_3: 0.4254  loss_dice_3: 3.202  loss_ce_4: 0.332  loss_mask_4: 0.4293  loss_dice_4: 3.204  loss_ce_5: 0.3422  loss_mask_5: 0.4269  loss_dice_5: 3.19  loss_ce_6: 0.3235  loss_mask_6: 0.4274  loss_dice_6: 3.194  loss_ce_7: 0.3271  loss_mask_7: 0.4269  loss_dice_7: 3.188  loss_ce_8: 0.3283  loss_mask_8: 0.4268  loss_dice_8: 3.197  time: 1.5014  data_time: 0.0675  lr: 6.5887e-06  max_mem: 21589M
[01/18 01:33:30] d2.utils.events INFO:  eta: 10:08:36  iter: 14859  total_loss: 39.28  loss_ce: 0.3399  loss_mask: 0.4203  loss_dice: 3.128  loss_ce_0: 0.6068  loss_mask_0: 0.4139  loss_dice_0: 3.261  loss_ce_1: 0.3753  loss_mask_1: 0.4247  loss_dice_1: 3.172  loss_ce_2: 0.3775  loss_mask_2: 0.423  loss_dice_2: 3.135  loss_ce_3: 0.3421  loss_mask_3: 0.4208  loss_dice_3: 3.127  loss_ce_4: 0.3327  loss_mask_4: 0.4202  loss_dice_4: 3.123  loss_ce_5: 0.3526  loss_mask_5: 0.4187  loss_dice_5: 3.13  loss_ce_6: 0.3464  loss_mask_6: 0.4187  loss_dice_6: 3.126  loss_ce_7: 0.3493  loss_mask_7: 0.4171  loss_dice_7: 3.129  loss_ce_8: 0.3467  loss_mask_8: 0.418  loss_dice_8: 3.126  time: 1.5013  data_time: 0.0655  lr: 6.584e-06  max_mem: 21589M
[01/18 01:33:59] d2.utils.events INFO:  eta: 10:07:27  iter: 14879  total_loss: 39.22  loss_ce: 0.3383  loss_mask: 0.4166  loss_dice: 3.102  loss_ce_0: 0.5916  loss_mask_0: 0.4062  loss_dice_0: 3.239  loss_ce_1: 0.3524  loss_mask_1: 0.4233  loss_dice_1: 3.154  loss_ce_2: 0.3479  loss_mask_2: 0.4179  loss_dice_2: 3.131  loss_ce_3: 0.3393  loss_mask_3: 0.4207  loss_dice_3: 3.115  loss_ce_4: 0.3473  loss_mask_4: 0.4195  loss_dice_4: 3.107  loss_ce_5: 0.3362  loss_mask_5: 0.4202  loss_dice_5: 3.106  loss_ce_6: 0.3297  loss_mask_6: 0.4188  loss_dice_6: 3.094  loss_ce_7: 0.3323  loss_mask_7: 0.4182  loss_dice_7: 3.108  loss_ce_8: 0.3387  loss_mask_8: 0.4175  loss_dice_8: 3.108  time: 1.5012  data_time: 0.0713  lr: 6.5793e-06  max_mem: 21589M
[01/18 01:34:28] d2.utils.events INFO:  eta: 10:06:45  iter: 14899  total_loss: 39.37  loss_ce: 0.3341  loss_mask: 0.4261  loss_dice: 3.161  loss_ce_0: 0.5784  loss_mask_0: 0.4098  loss_dice_0: 3.294  loss_ce_1: 0.3468  loss_mask_1: 0.4266  loss_dice_1: 3.197  loss_ce_2: 0.3474  loss_mask_2: 0.425  loss_dice_2: 3.158  loss_ce_3: 0.3278  loss_mask_3: 0.4248  loss_dice_3: 3.165  loss_ce_4: 0.3413  loss_mask_4: 0.4233  loss_dice_4: 3.153  loss_ce_5: 0.3326  loss_mask_5: 0.4238  loss_dice_5: 3.163  loss_ce_6: 0.3367  loss_mask_6: 0.4252  loss_dice_6: 3.153  loss_ce_7: 0.3215  loss_mask_7: 0.4245  loss_dice_7: 3.168  loss_ce_8: 0.3255  loss_mask_8: 0.4252  loss_dice_8: 3.155  time: 1.5012  data_time: 0.0687  lr: 6.5746e-06  max_mem: 21589M
[01/18 01:34:57] d2.utils.events INFO:  eta: 10:06:02  iter: 14919  total_loss: 39.05  loss_ce: 0.299  loss_mask: 0.4322  loss_dice: 3.11  loss_ce_0: 0.5618  loss_mask_0: 0.4196  loss_dice_0: 3.239  loss_ce_1: 0.3344  loss_mask_1: 0.4345  loss_dice_1: 3.144  loss_ce_2: 0.3319  loss_mask_2: 0.4315  loss_dice_2: 3.127  loss_ce_3: 0.3167  loss_mask_3: 0.429  loss_dice_3: 3.118  loss_ce_4: 0.3259  loss_mask_4: 0.4294  loss_dice_4: 3.116  loss_ce_5: 0.3159  loss_mask_5: 0.4313  loss_dice_5: 3.117  loss_ce_6: 0.3001  loss_mask_6: 0.4315  loss_dice_6: 3.108  loss_ce_7: 0.3118  loss_mask_7: 0.4306  loss_dice_7: 3.115  loss_ce_8: 0.3042  loss_mask_8: 0.4312  loss_dice_8: 3.113  time: 1.5011  data_time: 0.0638  lr: 6.5699e-06  max_mem: 21589M
[01/18 01:35:26] d2.utils.events INFO:  eta: 10:05:30  iter: 14939  total_loss: 39.66  loss_ce: 0.3554  loss_mask: 0.4215  loss_dice: 3.129  loss_ce_0: 0.5785  loss_mask_0: 0.4067  loss_dice_0: 3.256  loss_ce_1: 0.3604  loss_mask_1: 0.4242  loss_dice_1: 3.17  loss_ce_2: 0.3693  loss_mask_2: 0.4242  loss_dice_2: 3.148  loss_ce_3: 0.3449  loss_mask_3: 0.423  loss_dice_3: 3.146  loss_ce_4: 0.3471  loss_mask_4: 0.4207  loss_dice_4: 3.143  loss_ce_5: 0.3592  loss_mask_5: 0.4211  loss_dice_5: 3.142  loss_ce_6: 0.3548  loss_mask_6: 0.4222  loss_dice_6: 3.134  loss_ce_7: 0.3426  loss_mask_7: 0.4204  loss_dice_7: 3.143  loss_ce_8: 0.3551  loss_mask_8: 0.422  loss_dice_8: 3.138  time: 1.5010  data_time: 0.0704  lr: 6.5651e-06  max_mem: 21589M
[01/18 01:35:55] d2.utils.events INFO:  eta: 10:04:49  iter: 14959  total_loss: 39.57  loss_ce: 0.3299  loss_mask: 0.4168  loss_dice: 3.129  loss_ce_0: 0.5874  loss_mask_0: 0.4102  loss_dice_0: 3.265  loss_ce_1: 0.331  loss_mask_1: 0.4197  loss_dice_1: 3.167  loss_ce_2: 0.3455  loss_mask_2: 0.4175  loss_dice_2: 3.143  loss_ce_3: 0.3384  loss_mask_3: 0.4175  loss_dice_3: 3.134  loss_ce_4: 0.3345  loss_mask_4: 0.4166  loss_dice_4: 3.129  loss_ce_5: 0.3307  loss_mask_5: 0.417  loss_dice_5: 3.14  loss_ce_6: 0.3306  loss_mask_6: 0.4173  loss_dice_6: 3.13  loss_ce_7: 0.3295  loss_mask_7: 0.418  loss_dice_7: 3.14  loss_ce_8: 0.3309  loss_mask_8: 0.4185  loss_dice_8: 3.132  time: 1.5010  data_time: 0.0608  lr: 6.5604e-06  max_mem: 21589M
[01/18 01:36:24] d2.utils.events INFO:  eta: 10:04:10  iter: 14979  total_loss: 39.78  loss_ce: 0.3107  loss_mask: 0.4216  loss_dice: 3.193  loss_ce_0: 0.5715  loss_mask_0: 0.4098  loss_dice_0: 3.327  loss_ce_1: 0.3406  loss_mask_1: 0.4254  loss_dice_1: 3.225  loss_ce_2: 0.3396  loss_mask_2: 0.4247  loss_dice_2: 3.214  loss_ce_3: 0.3322  loss_mask_3: 0.4215  loss_dice_3: 3.194  loss_ce_4: 0.3233  loss_mask_4: 0.4197  loss_dice_4: 3.2  loss_ce_5: 0.326  loss_mask_5: 0.4199  loss_dice_5: 3.195  loss_ce_6: 0.3132  loss_mask_6: 0.4215  loss_dice_6: 3.2  loss_ce_7: 0.3103  loss_mask_7: 0.4229  loss_dice_7: 3.199  loss_ce_8: 0.3141  loss_mask_8: 0.4214  loss_dice_8: 3.195  time: 1.5009  data_time: 0.0784  lr: 6.5557e-06  max_mem: 21589M
[01/18 01:36:53] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0014999.pth
[01/18 01:36:54] d2.utils.events INFO:  eta: 10:02:50  iter: 14999  total_loss: 39.43  loss_ce: 0.3227  loss_mask: 0.4111  loss_dice: 3.141  loss_ce_0: 0.5773  loss_mask_0: 0.4049  loss_dice_0: 3.276  loss_ce_1: 0.3346  loss_mask_1: 0.4144  loss_dice_1: 3.176  loss_ce_2: 0.3368  loss_mask_2: 0.4141  loss_dice_2: 3.161  loss_ce_3: 0.3247  loss_mask_3: 0.4156  loss_dice_3: 3.144  loss_ce_4: 0.3309  loss_mask_4: 0.4145  loss_dice_4: 3.14  loss_ce_5: 0.3331  loss_mask_5: 0.4127  loss_dice_5: 3.143  loss_ce_6: 0.3211  loss_mask_6: 0.4136  loss_dice_6: 3.15  loss_ce_7: 0.319  loss_mask_7: 0.4112  loss_dice_7: 3.133  loss_ce_8: 0.3259  loss_mask_8: 0.4125  loss_dice_8: 3.138  time: 1.5008  data_time: 0.0665  lr: 6.551e-06  max_mem: 21589M
[01/18 01:37:23] d2.utils.events INFO:  eta: 10:02:21  iter: 15019  total_loss: 39.48  loss_ce: 0.3311  loss_mask: 0.4286  loss_dice: 3.138  loss_ce_0: 0.5831  loss_mask_0: 0.4235  loss_dice_0: 3.254  loss_ce_1: 0.3543  loss_mask_1: 0.4355  loss_dice_1: 3.172  loss_ce_2: 0.3433  loss_mask_2: 0.4302  loss_dice_2: 3.152  loss_ce_3: 0.3444  loss_mask_3: 0.4314  loss_dice_3: 3.151  loss_ce_4: 0.3389  loss_mask_4: 0.4315  loss_dice_4: 3.142  loss_ce_5: 0.3264  loss_mask_5: 0.4296  loss_dice_5: 3.144  loss_ce_6: 0.3264  loss_mask_6: 0.43  loss_dice_6: 3.138  loss_ce_7: 0.3245  loss_mask_7: 0.4304  loss_dice_7: 3.138  loss_ce_8: 0.3264  loss_mask_8: 0.4297  loss_dice_8: 3.137  time: 1.5007  data_time: 0.0643  lr: 6.5463e-06  max_mem: 21589M
[01/18 01:37:52] d2.utils.events INFO:  eta: 10:01:27  iter: 15039  total_loss: 39.18  loss_ce: 0.3387  loss_mask: 0.4265  loss_dice: 3.139  loss_ce_0: 0.588  loss_mask_0: 0.4175  loss_dice_0: 3.274  loss_ce_1: 0.3586  loss_mask_1: 0.4343  loss_dice_1: 3.187  loss_ce_2: 0.3596  loss_mask_2: 0.4302  loss_dice_2: 3.154  loss_ce_3: 0.3334  loss_mask_3: 0.4295  loss_dice_3: 3.139  loss_ce_4: 0.3321  loss_mask_4: 0.4279  loss_dice_4: 3.145  loss_ce_5: 0.3344  loss_mask_5: 0.4268  loss_dice_5: 3.143  loss_ce_6: 0.3215  loss_mask_6: 0.4262  loss_dice_6: 3.134  loss_ce_7: 0.3093  loss_mask_7: 0.4246  loss_dice_7: 3.145  loss_ce_8: 0.3126  loss_mask_8: 0.426  loss_dice_8: 3.144  time: 1.5007  data_time: 0.0686  lr: 6.5416e-06  max_mem: 21589M
[01/18 01:38:21] d2.utils.events INFO:  eta: 10:01:16  iter: 15059  total_loss: 39.31  loss_ce: 0.3328  loss_mask: 0.4278  loss_dice: 3.137  loss_ce_0: 0.5818  loss_mask_0: 0.4133  loss_dice_0: 3.278  loss_ce_1: 0.34  loss_mask_1: 0.4305  loss_dice_1: 3.174  loss_ce_2: 0.3436  loss_mask_2: 0.4266  loss_dice_2: 3.159  loss_ce_3: 0.34  loss_mask_3: 0.4244  loss_dice_3: 3.144  loss_ce_4: 0.333  loss_mask_4: 0.4239  loss_dice_4: 3.145  loss_ce_5: 0.3309  loss_mask_5: 0.4249  loss_dice_5: 3.146  loss_ce_6: 0.3354  loss_mask_6: 0.4257  loss_dice_6: 3.139  loss_ce_7: 0.3285  loss_mask_7: 0.4256  loss_dice_7: 3.138  loss_ce_8: 0.3281  loss_mask_8: 0.4274  loss_dice_8: 3.138  time: 1.5006  data_time: 0.0664  lr: 6.5368e-06  max_mem: 21589M
[01/18 01:38:50] d2.utils.events INFO:  eta: 10:00:40  iter: 15079  total_loss: 39.02  loss_ce: 0.3159  loss_mask: 0.4243  loss_dice: 3.128  loss_ce_0: 0.6032  loss_mask_0: 0.4104  loss_dice_0: 3.259  loss_ce_1: 0.3405  loss_mask_1: 0.4275  loss_dice_1: 3.184  loss_ce_2: 0.3434  loss_mask_2: 0.4283  loss_dice_2: 3.141  loss_ce_3: 0.3263  loss_mask_3: 0.4268  loss_dice_3: 3.137  loss_ce_4: 0.3273  loss_mask_4: 0.425  loss_dice_4: 3.125  loss_ce_5: 0.3112  loss_mask_5: 0.4279  loss_dice_5: 3.135  loss_ce_6: 0.3093  loss_mask_6: 0.4266  loss_dice_6: 3.129  loss_ce_7: 0.3144  loss_mask_7: 0.4266  loss_dice_7: 3.138  loss_ce_8: 0.31  loss_mask_8: 0.4277  loss_dice_8: 3.132  time: 1.5005  data_time: 0.0677  lr: 6.5321e-06  max_mem: 21589M
[01/18 01:39:19] d2.utils.events INFO:  eta: 10:00:24  iter: 15099  total_loss: 39.68  loss_ce: 0.318  loss_mask: 0.435  loss_dice: 3.151  loss_ce_0: 0.6116  loss_mask_0: 0.4227  loss_dice_0: 3.284  loss_ce_1: 0.3617  loss_mask_1: 0.4408  loss_dice_1: 3.174  loss_ce_2: 0.3453  loss_mask_2: 0.4368  loss_dice_2: 3.162  loss_ce_3: 0.3302  loss_mask_3: 0.434  loss_dice_3: 3.153  loss_ce_4: 0.3301  loss_mask_4: 0.4325  loss_dice_4: 3.148  loss_ce_5: 0.3281  loss_mask_5: 0.4316  loss_dice_5: 3.146  loss_ce_6: 0.329  loss_mask_6: 0.4324  loss_dice_6: 3.143  loss_ce_7: 0.3199  loss_mask_7: 0.4341  loss_dice_7: 3.154  loss_ce_8: 0.3148  loss_mask_8: 0.4342  loss_dice_8: 3.143  time: 1.5004  data_time: 0.0666  lr: 6.5274e-06  max_mem: 21589M
[01/18 01:39:48] d2.utils.events INFO:  eta: 9:59:49  iter: 15119  total_loss: 39.08  loss_ce: 0.3079  loss_mask: 0.4176  loss_dice: 3.104  loss_ce_0: 0.5823  loss_mask_0: 0.412  loss_dice_0: 3.248  loss_ce_1: 0.3385  loss_mask_1: 0.4185  loss_dice_1: 3.15  loss_ce_2: 0.3431  loss_mask_2: 0.4185  loss_dice_2: 3.127  loss_ce_3: 0.319  loss_mask_3: 0.4201  loss_dice_3: 3.094  loss_ce_4: 0.3147  loss_mask_4: 0.419  loss_dice_4: 3.1  loss_ce_5: 0.3243  loss_mask_5: 0.4203  loss_dice_5: 3.096  loss_ce_6: 0.3142  loss_mask_6: 0.4207  loss_dice_6: 3.094  loss_ce_7: 0.2876  loss_mask_7: 0.4188  loss_dice_7: 3.104  loss_ce_8: 0.3058  loss_mask_8: 0.4178  loss_dice_8: 3.095  time: 1.5004  data_time: 0.0780  lr: 6.5227e-06  max_mem: 21589M
[01/18 01:40:16] d2.utils.events INFO:  eta: 9:59:10  iter: 15139  total_loss: 39.15  loss_ce: 0.3134  loss_mask: 0.427  loss_dice: 3.096  loss_ce_0: 0.5595  loss_mask_0: 0.4176  loss_dice_0: 3.234  loss_ce_1: 0.342  loss_mask_1: 0.4338  loss_dice_1: 3.13  loss_ce_2: 0.3404  loss_mask_2: 0.4322  loss_dice_2: 3.101  loss_ce_3: 0.3317  loss_mask_3: 0.429  loss_dice_3: 3.105  loss_ce_4: 0.3185  loss_mask_4: 0.4299  loss_dice_4: 3.097  loss_ce_5: 0.3126  loss_mask_5: 0.4291  loss_dice_5: 3.099  loss_ce_6: 0.3112  loss_mask_6: 0.4305  loss_dice_6: 3.099  loss_ce_7: 0.3114  loss_mask_7: 0.4271  loss_dice_7: 3.1  loss_ce_8: 0.2898  loss_mask_8: 0.4291  loss_dice_8: 3.094  time: 1.5002  data_time: 0.0659  lr: 6.518e-06  max_mem: 21589M
[01/18 01:40:45] d2.utils.events INFO:  eta: 9:58:26  iter: 15159  total_loss: 39.27  loss_ce: 0.3212  loss_mask: 0.4221  loss_dice: 3.136  loss_ce_0: 0.5974  loss_mask_0: 0.4095  loss_dice_0: 3.275  loss_ce_1: 0.3476  loss_mask_1: 0.4193  loss_dice_1: 3.173  loss_ce_2: 0.3602  loss_mask_2: 0.4199  loss_dice_2: 3.149  loss_ce_3: 0.3421  loss_mask_3: 0.4219  loss_dice_3: 3.132  loss_ce_4: 0.3222  loss_mask_4: 0.4226  loss_dice_4: 3.137  loss_ce_5: 0.3257  loss_mask_5: 0.4207  loss_dice_5: 3.144  loss_ce_6: 0.3053  loss_mask_6: 0.4225  loss_dice_6: 3.141  loss_ce_7: 0.3192  loss_mask_7: 0.4212  loss_dice_7: 3.138  loss_ce_8: 0.3372  loss_mask_8: 0.4229  loss_dice_8: 3.134  time: 1.5002  data_time: 0.0701  lr: 6.5133e-06  max_mem: 21589M
[01/18 01:41:14] d2.utils.events INFO:  eta: 9:57:44  iter: 15179  total_loss: 39.37  loss_ce: 0.3232  loss_mask: 0.4174  loss_dice: 3.111  loss_ce_0: 0.5744  loss_mask_0: 0.4086  loss_dice_0: 3.252  loss_ce_1: 0.3436  loss_mask_1: 0.4231  loss_dice_1: 3.152  loss_ce_2: 0.3482  loss_mask_2: 0.4176  loss_dice_2: 3.125  loss_ce_3: 0.3487  loss_mask_3: 0.4166  loss_dice_3: 3.111  loss_ce_4: 0.3332  loss_mask_4: 0.4136  loss_dice_4: 3.114  loss_ce_5: 0.3279  loss_mask_5: 0.413  loss_dice_5: 3.104  loss_ce_6: 0.3288  loss_mask_6: 0.4155  loss_dice_6: 3.116  loss_ce_7: 0.3367  loss_mask_7: 0.415  loss_dice_7: 3.107  loss_ce_8: 0.3377  loss_mask_8: 0.4141  loss_dice_8: 3.115  time: 1.5001  data_time: 0.0769  lr: 6.5085e-06  max_mem: 21589M
[01/18 01:41:43] d2.utils.events INFO:  eta: 9:57:06  iter: 15199  total_loss: 39.41  loss_ce: 0.3222  loss_mask: 0.4208  loss_dice: 3.147  loss_ce_0: 0.5869  loss_mask_0: 0.4082  loss_dice_0: 3.272  loss_ce_1: 0.3436  loss_mask_1: 0.4222  loss_dice_1: 3.185  loss_ce_2: 0.3478  loss_mask_2: 0.4237  loss_dice_2: 3.162  loss_ce_3: 0.3374  loss_mask_3: 0.4204  loss_dice_3: 3.14  loss_ce_4: 0.328  loss_mask_4: 0.4206  loss_dice_4: 3.138  loss_ce_5: 0.3253  loss_mask_5: 0.4207  loss_dice_5: 3.142  loss_ce_6: 0.3211  loss_mask_6: 0.422  loss_dice_6: 3.142  loss_ce_7: 0.3206  loss_mask_7: 0.422  loss_dice_7: 3.146  loss_ce_8: 0.322  loss_mask_8: 0.4217  loss_dice_8: 3.15  time: 1.5000  data_time: 0.0746  lr: 6.5038e-06  max_mem: 21589M
[01/18 01:42:12] d2.utils.events INFO:  eta: 9:56:17  iter: 15219  total_loss: 39.79  loss_ce: 0.3726  loss_mask: 0.4267  loss_dice: 3.123  loss_ce_0: 0.6121  loss_mask_0: 0.4208  loss_dice_0: 3.285  loss_ce_1: 0.3846  loss_mask_1: 0.4294  loss_dice_1: 3.171  loss_ce_2: 0.3835  loss_mask_2: 0.4271  loss_dice_2: 3.154  loss_ce_3: 0.3848  loss_mask_3: 0.427  loss_dice_3: 3.128  loss_ce_4: 0.3797  loss_mask_4: 0.4275  loss_dice_4: 3.126  loss_ce_5: 0.3676  loss_mask_5: 0.4262  loss_dice_5: 3.132  loss_ce_6: 0.3549  loss_mask_6: 0.4263  loss_dice_6: 3.128  loss_ce_7: 0.3731  loss_mask_7: 0.4268  loss_dice_7: 3.132  loss_ce_8: 0.3698  loss_mask_8: 0.4277  loss_dice_8: 3.132  time: 1.5000  data_time: 0.0738  lr: 6.4991e-06  max_mem: 21589M
[01/18 01:42:41] d2.utils.events INFO:  eta: 9:55:19  iter: 15239  total_loss: 39.42  loss_ce: 0.3407  loss_mask: 0.4161  loss_dice: 3.146  loss_ce_0: 0.5904  loss_mask_0: 0.4033  loss_dice_0: 3.266  loss_ce_1: 0.36  loss_mask_1: 0.4201  loss_dice_1: 3.181  loss_ce_2: 0.3722  loss_mask_2: 0.4142  loss_dice_2: 3.161  loss_ce_3: 0.3405  loss_mask_3: 0.4171  loss_dice_3: 3.143  loss_ce_4: 0.3358  loss_mask_4: 0.415  loss_dice_4: 3.147  loss_ce_5: 0.3398  loss_mask_5: 0.4143  loss_dice_5: 3.144  loss_ce_6: 0.3364  loss_mask_6: 0.4134  loss_dice_6: 3.141  loss_ce_7: 0.3279  loss_mask_7: 0.4153  loss_dice_7: 3.139  loss_ce_8: 0.3282  loss_mask_8: 0.416  loss_dice_8: 3.147  time: 1.4999  data_time: 0.0672  lr: 6.4944e-06  max_mem: 21589M
[01/18 01:43:09] d2.utils.events INFO:  eta: 9:54:38  iter: 15259  total_loss: 38.81  loss_ce: 0.3122  loss_mask: 0.4249  loss_dice: 3.054  loss_ce_0: 0.5766  loss_mask_0: 0.4134  loss_dice_0: 3.195  loss_ce_1: 0.3562  loss_mask_1: 0.4332  loss_dice_1: 3.092  loss_ce_2: 0.3381  loss_mask_2: 0.4266  loss_dice_2: 3.077  loss_ce_3: 0.3214  loss_mask_3: 0.426  loss_dice_3: 3.056  loss_ce_4: 0.3205  loss_mask_4: 0.4256  loss_dice_4: 3.059  loss_ce_5: 0.3223  loss_mask_5: 0.4262  loss_dice_5: 3.054  loss_ce_6: 0.3155  loss_mask_6: 0.4242  loss_dice_6: 3.058  loss_ce_7: 0.3108  loss_mask_7: 0.4256  loss_dice_7: 3.053  loss_ce_8: 0.3225  loss_mask_8: 0.4242  loss_dice_8: 3.054  time: 1.4998  data_time: 0.0747  lr: 6.4897e-06  max_mem: 21589M
[01/18 01:43:38] d2.utils.events INFO:  eta: 9:54:08  iter: 15279  total_loss: 39.01  loss_ce: 0.3326  loss_mask: 0.4295  loss_dice: 3.079  loss_ce_0: 0.5867  loss_mask_0: 0.421  loss_dice_0: 3.205  loss_ce_1: 0.3458  loss_mask_1: 0.4366  loss_dice_1: 3.119  loss_ce_2: 0.3558  loss_mask_2: 0.4321  loss_dice_2: 3.091  loss_ce_3: 0.3235  loss_mask_3: 0.4303  loss_dice_3: 3.089  loss_ce_4: 0.3267  loss_mask_4: 0.433  loss_dice_4: 3.08  loss_ce_5: 0.3381  loss_mask_5: 0.433  loss_dice_5: 3.078  loss_ce_6: 0.3313  loss_mask_6: 0.4288  loss_dice_6: 3.07  loss_ce_7: 0.3249  loss_mask_7: 0.4322  loss_dice_7: 3.079  loss_ce_8: 0.3386  loss_mask_8: 0.4308  loss_dice_8: 3.078  time: 1.4997  data_time: 0.0689  lr: 6.4849e-06  max_mem: 21589M
[01/18 01:44:07] d2.utils.events INFO:  eta: 9:53:50  iter: 15299  total_loss: 39.6  loss_ce: 0.3319  loss_mask: 0.4252  loss_dice: 3.174  loss_ce_0: 0.6059  loss_mask_0: 0.4114  loss_dice_0: 3.307  loss_ce_1: 0.3503  loss_mask_1: 0.4245  loss_dice_1: 3.207  loss_ce_2: 0.3564  loss_mask_2: 0.4244  loss_dice_2: 3.202  loss_ce_3: 0.3405  loss_mask_3: 0.426  loss_dice_3: 3.185  loss_ce_4: 0.3373  loss_mask_4: 0.423  loss_dice_4: 3.179  loss_ce_5: 0.3437  loss_mask_5: 0.4252  loss_dice_5: 3.181  loss_ce_6: 0.3185  loss_mask_6: 0.4255  loss_dice_6: 3.179  loss_ce_7: 0.3251  loss_mask_7: 0.4255  loss_dice_7: 3.177  loss_ce_8: 0.3288  loss_mask_8: 0.4265  loss_dice_8: 3.176  time: 1.4997  data_time: 0.0710  lr: 6.4802e-06  max_mem: 21589M
[01/18 01:44:36] d2.utils.events INFO:  eta: 9:53:22  iter: 15319  total_loss: 39.16  loss_ce: 0.3289  loss_mask: 0.4184  loss_dice: 3.09  loss_ce_0: 0.5825  loss_mask_0: 0.4043  loss_dice_0: 3.225  loss_ce_1: 0.3396  loss_mask_1: 0.419  loss_dice_1: 3.132  loss_ce_2: 0.3444  loss_mask_2: 0.4176  loss_dice_2: 3.123  loss_ce_3: 0.3304  loss_mask_3: 0.4153  loss_dice_3: 3.105  loss_ce_4: 0.3213  loss_mask_4: 0.4169  loss_dice_4: 3.099  loss_ce_5: 0.3139  loss_mask_5: 0.4178  loss_dice_5: 3.094  loss_ce_6: 0.3274  loss_mask_6: 0.4179  loss_dice_6: 3.098  loss_ce_7: 0.3058  loss_mask_7: 0.4174  loss_dice_7: 3.101  loss_ce_8: 0.3126  loss_mask_8: 0.4181  loss_dice_8: 3.092  time: 1.4996  data_time: 0.0717  lr: 6.4755e-06  max_mem: 21589M
[01/18 01:45:05] d2.utils.events INFO:  eta: 9:52:40  iter: 15339  total_loss: 38.71  loss_ce: 0.3156  loss_mask: 0.4206  loss_dice: 3.08  loss_ce_0: 0.6031  loss_mask_0: 0.4052  loss_dice_0: 3.21  loss_ce_1: 0.3509  loss_mask_1: 0.4246  loss_dice_1: 3.118  loss_ce_2: 0.3491  loss_mask_2: 0.4219  loss_dice_2: 3.098  loss_ce_3: 0.3246  loss_mask_3: 0.4211  loss_dice_3: 3.074  loss_ce_4: 0.3148  loss_mask_4: 0.4223  loss_dice_4: 3.088  loss_ce_5: 0.3175  loss_mask_5: 0.4206  loss_dice_5: 3.078  loss_ce_6: 0.3111  loss_mask_6: 0.4211  loss_dice_6: 3.081  loss_ce_7: 0.322  loss_mask_7: 0.4202  loss_dice_7: 3.084  loss_ce_8: 0.3186  loss_mask_8: 0.4196  loss_dice_8: 3.08  time: 1.4995  data_time: 0.0652  lr: 6.4708e-06  max_mem: 21589M
[01/18 01:45:34] d2.utils.events INFO:  eta: 9:52:01  iter: 15359  total_loss: 39.05  loss_ce: 0.3399  loss_mask: 0.4259  loss_dice: 3.075  loss_ce_0: 0.6001  loss_mask_0: 0.4112  loss_dice_0: 3.211  loss_ce_1: 0.3724  loss_mask_1: 0.4255  loss_dice_1: 3.115  loss_ce_2: 0.3515  loss_mask_2: 0.425  loss_dice_2: 3.093  loss_ce_3: 0.3244  loss_mask_3: 0.4243  loss_dice_3: 3.081  loss_ce_4: 0.3373  loss_mask_4: 0.426  loss_dice_4: 3.085  loss_ce_5: 0.3311  loss_mask_5: 0.4255  loss_dice_5: 3.074  loss_ce_6: 0.3313  loss_mask_6: 0.4267  loss_dice_6: 3.077  loss_ce_7: 0.3206  loss_mask_7: 0.4265  loss_dice_7: 3.079  loss_ce_8: 0.3309  loss_mask_8: 0.4264  loss_dice_8: 3.075  time: 1.4994  data_time: 0.0644  lr: 6.466e-06  max_mem: 21589M
[01/18 01:46:02] d2.utils.events INFO:  eta: 9:51:14  iter: 15379  total_loss: 38.96  loss_ce: 0.345  loss_mask: 0.4214  loss_dice: 3.08  loss_ce_0: 0.6007  loss_mask_0: 0.409  loss_dice_0: 3.213  loss_ce_1: 0.3737  loss_mask_1: 0.428  loss_dice_1: 3.121  loss_ce_2: 0.3794  loss_mask_2: 0.4259  loss_dice_2: 3.11  loss_ce_3: 0.3517  loss_mask_3: 0.4206  loss_dice_3: 3.094  loss_ce_4: 0.3525  loss_mask_4: 0.4245  loss_dice_4: 3.085  loss_ce_5: 0.3356  loss_mask_5: 0.4225  loss_dice_5: 3.086  loss_ce_6: 0.351  loss_mask_6: 0.4231  loss_dice_6: 3.082  loss_ce_7: 0.3405  loss_mask_7: 0.4216  loss_dice_7: 3.085  loss_ce_8: 0.3473  loss_mask_8: 0.4221  loss_dice_8: 3.083  time: 1.4993  data_time: 0.0730  lr: 6.4613e-06  max_mem: 21589M
[01/18 01:46:31] d2.utils.events INFO:  eta: 9:50:14  iter: 15399  total_loss: 39.14  loss_ce: 0.3096  loss_mask: 0.4182  loss_dice: 3.134  loss_ce_0: 0.5817  loss_mask_0: 0.4052  loss_dice_0: 3.273  loss_ce_1: 0.3546  loss_mask_1: 0.4195  loss_dice_1: 3.175  loss_ce_2: 0.3466  loss_mask_2: 0.4177  loss_dice_2: 3.157  loss_ce_3: 0.322  loss_mask_3: 0.4162  loss_dice_3: 3.149  loss_ce_4: 0.3065  loss_mask_4: 0.4165  loss_dice_4: 3.142  loss_ce_5: 0.3055  loss_mask_5: 0.4182  loss_dice_5: 3.148  loss_ce_6: 0.3151  loss_mask_6: 0.4167  loss_dice_6: 3.136  loss_ce_7: 0.2985  loss_mask_7: 0.4149  loss_dice_7: 3.135  loss_ce_8: 0.3091  loss_mask_8: 0.4182  loss_dice_8: 3.132  time: 1.4992  data_time: 0.0704  lr: 6.4566e-06  max_mem: 21589M
[01/18 01:47:00] d2.utils.events INFO:  eta: 9:49:46  iter: 15419  total_loss: 39.52  loss_ce: 0.3212  loss_mask: 0.4187  loss_dice: 3.169  loss_ce_0: 0.5609  loss_mask_0: 0.4083  loss_dice_0: 3.294  loss_ce_1: 0.3303  loss_mask_1: 0.4206  loss_dice_1: 3.207  loss_ce_2: 0.3292  loss_mask_2: 0.4229  loss_dice_2: 3.184  loss_ce_3: 0.3265  loss_mask_3: 0.4246  loss_dice_3: 3.176  loss_ce_4: 0.33  loss_mask_4: 0.4206  loss_dice_4: 3.172  loss_ce_5: 0.321  loss_mask_5: 0.4226  loss_dice_5: 3.174  loss_ce_6: 0.3193  loss_mask_6: 0.4229  loss_dice_6: 3.172  loss_ce_7: 0.3113  loss_mask_7: 0.4223  loss_dice_7: 3.157  loss_ce_8: 0.3103  loss_mask_8: 0.4191  loss_dice_8: 3.178  time: 1.4992  data_time: 0.0682  lr: 6.4519e-06  max_mem: 21589M
[01/18 01:47:29] d2.utils.events INFO:  eta: 9:48:59  iter: 15439  total_loss: 38.52  loss_ce: 0.3116  loss_mask: 0.4043  loss_dice: 3.068  loss_ce_0: 0.5858  loss_mask_0: 0.3925  loss_dice_0: 3.218  loss_ce_1: 0.3566  loss_mask_1: 0.4029  loss_dice_1: 3.122  loss_ce_2: 0.3504  loss_mask_2: 0.4014  loss_dice_2: 3.089  loss_ce_3: 0.3345  loss_mask_3: 0.4028  loss_dice_3: 3.088  loss_ce_4: 0.3237  loss_mask_4: 0.4031  loss_dice_4: 3.076  loss_ce_5: 0.3147  loss_mask_5: 0.4037  loss_dice_5: 3.076  loss_ce_6: 0.3333  loss_mask_6: 0.4045  loss_dice_6: 3.071  loss_ce_7: 0.324  loss_mask_7: 0.404  loss_dice_7: 3.073  loss_ce_8: 0.3174  loss_mask_8: 0.4047  loss_dice_8: 3.07  time: 1.4991  data_time: 0.0652  lr: 6.4471e-06  max_mem: 21589M
[01/18 01:47:58] d2.utils.events INFO:  eta: 9:48:30  iter: 15459  total_loss: 38.78  loss_ce: 0.2974  loss_mask: 0.3991  loss_dice: 3.076  loss_ce_0: 0.5862  loss_mask_0: 0.3925  loss_dice_0: 3.219  loss_ce_1: 0.3451  loss_mask_1: 0.3999  loss_dice_1: 3.124  loss_ce_2: 0.3389  loss_mask_2: 0.3973  loss_dice_2: 3.106  loss_ce_3: 0.3429  loss_mask_3: 0.3996  loss_dice_3: 3.089  loss_ce_4: 0.3227  loss_mask_4: 0.398  loss_dice_4: 3.085  loss_ce_5: 0.3149  loss_mask_5: 0.398  loss_dice_5: 3.089  loss_ce_6: 0.3071  loss_mask_6: 0.3985  loss_dice_6: 3.08  loss_ce_7: 0.3064  loss_mask_7: 0.3981  loss_dice_7: 3.081  loss_ce_8: 0.3107  loss_mask_8: 0.3986  loss_dice_8: 3.077  time: 1.4991  data_time: 0.0686  lr: 6.4424e-06  max_mem: 21589M
[01/18 01:48:28] d2.utils.events INFO:  eta: 9:48:13  iter: 15479  total_loss: 39.62  loss_ce: 0.3293  loss_mask: 0.4198  loss_dice: 3.106  loss_ce_0: 0.5752  loss_mask_0: 0.4124  loss_dice_0: 3.256  loss_ce_1: 0.3728  loss_mask_1: 0.4228  loss_dice_1: 3.158  loss_ce_2: 0.3597  loss_mask_2: 0.4239  loss_dice_2: 3.133  loss_ce_3: 0.3305  loss_mask_3: 0.4257  loss_dice_3: 3.114  loss_ce_4: 0.3372  loss_mask_4: 0.423  loss_dice_4: 3.113  loss_ce_5: 0.3289  loss_mask_5: 0.4199  loss_dice_5: 3.126  loss_ce_6: 0.3391  loss_mask_6: 0.4219  loss_dice_6: 3.116  loss_ce_7: 0.3351  loss_mask_7: 0.4179  loss_dice_7: 3.114  loss_ce_8: 0.3189  loss_mask_8: 0.4188  loss_dice_8: 3.116  time: 1.4990  data_time: 0.0719  lr: 6.4377e-06  max_mem: 21589M
[01/18 01:48:56] d2.utils.events INFO:  eta: 9:47:44  iter: 15499  total_loss: 38.65  loss_ce: 0.3076  loss_mask: 0.4242  loss_dice: 3.04  loss_ce_0: 0.566  loss_mask_0: 0.403  loss_dice_0: 3.188  loss_ce_1: 0.352  loss_mask_1: 0.4222  loss_dice_1: 3.09  loss_ce_2: 0.3425  loss_mask_2: 0.421  loss_dice_2: 3.059  loss_ce_3: 0.3259  loss_mask_3: 0.4213  loss_dice_3: 3.051  loss_ce_4: 0.3227  loss_mask_4: 0.4229  loss_dice_4: 3.051  loss_ce_5: 0.3293  loss_mask_5: 0.4213  loss_dice_5: 3.047  loss_ce_6: 0.3242  loss_mask_6: 0.4222  loss_dice_6: 3.052  loss_ce_7: 0.3134  loss_mask_7: 0.4229  loss_dice_7: 3.046  loss_ce_8: 0.316  loss_mask_8: 0.4231  loss_dice_8: 3.047  time: 1.4989  data_time: 0.0675  lr: 6.433e-06  max_mem: 21589M
[01/18 01:49:25] d2.utils.events INFO:  eta: 9:47:12  iter: 15519  total_loss: 39.03  loss_ce: 0.329  loss_mask: 0.4095  loss_dice: 3.114  loss_ce_0: 0.6097  loss_mask_0: 0.4024  loss_dice_0: 3.248  loss_ce_1: 0.3612  loss_mask_1: 0.4153  loss_dice_1: 3.156  loss_ce_2: 0.3496  loss_mask_2: 0.4143  loss_dice_2: 3.135  loss_ce_3: 0.3301  loss_mask_3: 0.4145  loss_dice_3: 3.114  loss_ce_4: 0.3394  loss_mask_4: 0.4132  loss_dice_4: 3.109  loss_ce_5: 0.3212  loss_mask_5: 0.4121  loss_dice_5: 3.112  loss_ce_6: 0.3245  loss_mask_6: 0.4105  loss_dice_6: 3.112  loss_ce_7: 0.33  loss_mask_7: 0.4105  loss_dice_7: 3.12  loss_ce_8: 0.3283  loss_mask_8: 0.4114  loss_dice_8: 3.113  time: 1.4988  data_time: 0.0670  lr: 6.4282e-06  max_mem: 21589M
[01/18 01:49:54] d2.utils.events INFO:  eta: 9:46:45  iter: 15539  total_loss: 38.91  loss_ce: 0.3089  loss_mask: 0.415  loss_dice: 3.13  loss_ce_0: 0.5639  loss_mask_0: 0.3993  loss_dice_0: 3.255  loss_ce_1: 0.3517  loss_mask_1: 0.4177  loss_dice_1: 3.174  loss_ce_2: 0.3445  loss_mask_2: 0.4147  loss_dice_2: 3.139  loss_ce_3: 0.3192  loss_mask_3: 0.4136  loss_dice_3: 3.135  loss_ce_4: 0.3108  loss_mask_4: 0.413  loss_dice_4: 3.129  loss_ce_5: 0.3044  loss_mask_5: 0.4124  loss_dice_5: 3.127  loss_ce_6: 0.3154  loss_mask_6: 0.4128  loss_dice_6: 3.136  loss_ce_7: 0.3062  loss_mask_7: 0.4138  loss_dice_7: 3.133  loss_ce_8: 0.3179  loss_mask_8: 0.4143  loss_dice_8: 3.134  time: 1.4988  data_time: 0.0776  lr: 6.4235e-06  max_mem: 21589M
[01/18 01:50:23] d2.utils.events INFO:  eta: 9:46:20  iter: 15559  total_loss: 39.21  loss_ce: 0.3328  loss_mask: 0.4215  loss_dice: 3.099  loss_ce_0: 0.5933  loss_mask_0: 0.4167  loss_dice_0: 3.227  loss_ce_1: 0.3334  loss_mask_1: 0.4253  loss_dice_1: 3.136  loss_ce_2: 0.3494  loss_mask_2: 0.4217  loss_dice_2: 3.111  loss_ce_3: 0.3328  loss_mask_3: 0.422  loss_dice_3: 3.105  loss_ce_4: 0.3278  loss_mask_4: 0.42  loss_dice_4: 3.105  loss_ce_5: 0.319  loss_mask_5: 0.4179  loss_dice_5: 3.105  loss_ce_6: 0.3194  loss_mask_6: 0.4178  loss_dice_6: 3.105  loss_ce_7: 0.3241  loss_mask_7: 0.4189  loss_dice_7: 3.103  loss_ce_8: 0.3338  loss_mask_8: 0.4199  loss_dice_8: 3.102  time: 1.4987  data_time: 0.0679  lr: 6.4188e-06  max_mem: 21589M
[01/18 01:50:52] d2.utils.events INFO:  eta: 9:45:56  iter: 15579  total_loss: 38.75  loss_ce: 0.3178  loss_mask: 0.4032  loss_dice: 3.077  loss_ce_0: 0.6039  loss_mask_0: 0.3902  loss_dice_0: 3.207  loss_ce_1: 0.3523  loss_mask_1: 0.4041  loss_dice_1: 3.111  loss_ce_2: 0.3603  loss_mask_2: 0.4035  loss_dice_2: 3.101  loss_ce_3: 0.3447  loss_mask_3: 0.403  loss_dice_3: 3.083  loss_ce_4: 0.3333  loss_mask_4: 0.4018  loss_dice_4: 3.078  loss_ce_5: 0.3322  loss_mask_5: 0.4021  loss_dice_5: 3.083  loss_ce_6: 0.323  loss_mask_6: 0.4038  loss_dice_6: 3.082  loss_ce_7: 0.3182  loss_mask_7: 0.4049  loss_dice_7: 3.079  loss_ce_8: 0.3261  loss_mask_8: 0.404  loss_dice_8: 3.074  time: 1.4986  data_time: 0.0784  lr: 6.4141e-06  max_mem: 21589M
[01/18 01:51:21] d2.utils.events INFO:  eta: 9:45:17  iter: 15599  total_loss: 38.38  loss_ce: 0.2983  loss_mask: 0.4188  loss_dice: 3.04  loss_ce_0: 0.5606  loss_mask_0: 0.4073  loss_dice_0: 3.188  loss_ce_1: 0.3437  loss_mask_1: 0.4245  loss_dice_1: 3.089  loss_ce_2: 0.3476  loss_mask_2: 0.4202  loss_dice_2: 3.069  loss_ce_3: 0.321  loss_mask_3: 0.4188  loss_dice_3: 3.05  loss_ce_4: 0.3049  loss_mask_4: 0.4178  loss_dice_4: 3.053  loss_ce_5: 0.319  loss_mask_5: 0.42  loss_dice_5: 3.046  loss_ce_6: 0.2999  loss_mask_6: 0.4173  loss_dice_6: 3.04  loss_ce_7: 0.3065  loss_mask_7: 0.4161  loss_dice_7: 3.042  loss_ce_8: 0.3102  loss_mask_8: 0.4177  loss_dice_8: 3.041  time: 1.4985  data_time: 0.0709  lr: 6.4093e-06  max_mem: 21589M
[01/18 01:51:50] d2.utils.events INFO:  eta: 9:44:37  iter: 15619  total_loss: 38.22  loss_ce: 0.3334  loss_mask: 0.4145  loss_dice: 3.067  loss_ce_0: 0.5798  loss_mask_0: 0.3991  loss_dice_0: 3.204  loss_ce_1: 0.3705  loss_mask_1: 0.4181  loss_dice_1: 3.104  loss_ce_2: 0.3519  loss_mask_2: 0.4145  loss_dice_2: 3.07  loss_ce_3: 0.3506  loss_mask_3: 0.4145  loss_dice_3: 3.07  loss_ce_4: 0.3446  loss_mask_4: 0.4147  loss_dice_4: 3.063  loss_ce_5: 0.3263  loss_mask_5: 0.4155  loss_dice_5: 3.063  loss_ce_6: 0.3524  loss_mask_6: 0.415  loss_dice_6: 3.053  loss_ce_7: 0.3304  loss_mask_7: 0.415  loss_dice_7: 3.062  loss_ce_8: 0.3223  loss_mask_8: 0.4147  loss_dice_8: 3.06  time: 1.4985  data_time: 0.0633  lr: 6.4046e-06  max_mem: 21589M
[01/18 01:52:18] d2.utils.events INFO:  eta: 9:44:21  iter: 15639  total_loss: 38.6  loss_ce: 0.3215  loss_mask: 0.4103  loss_dice: 3.073  loss_ce_0: 0.6026  loss_mask_0: 0.4014  loss_dice_0: 3.199  loss_ce_1: 0.361  loss_mask_1: 0.417  loss_dice_1: 3.107  loss_ce_2: 0.3608  loss_mask_2: 0.4136  loss_dice_2: 3.089  loss_ce_3: 0.3394  loss_mask_3: 0.4108  loss_dice_3: 3.078  loss_ce_4: 0.3326  loss_mask_4: 0.4116  loss_dice_4: 3.075  loss_ce_5: 0.3185  loss_mask_5: 0.4098  loss_dice_5: 3.077  loss_ce_6: 0.3251  loss_mask_6: 0.4093  loss_dice_6: 3.077  loss_ce_7: 0.3357  loss_mask_7: 0.4089  loss_dice_7: 3.074  loss_ce_8: 0.3097  loss_mask_8: 0.4108  loss_dice_8: 3.074  time: 1.4984  data_time: 0.0687  lr: 6.3999e-06  max_mem: 21589M
[01/18 01:52:47] d2.utils.events INFO:  eta: 9:43:41  iter: 15659  total_loss: 38.7  loss_ce: 0.3377  loss_mask: 0.4046  loss_dice: 3.061  loss_ce_0: 0.6115  loss_mask_0: 0.3974  loss_dice_0: 3.223  loss_ce_1: 0.3549  loss_mask_1: 0.4132  loss_dice_1: 3.111  loss_ce_2: 0.3701  loss_mask_2: 0.4063  loss_dice_2: 3.075  loss_ce_3: 0.3458  loss_mask_3: 0.4027  loss_dice_3: 3.065  loss_ce_4: 0.3629  loss_mask_4: 0.4042  loss_dice_4: 3.063  loss_ce_5: 0.3319  loss_mask_5: 0.4049  loss_dice_5: 3.061  loss_ce_6: 0.3336  loss_mask_6: 0.4058  loss_dice_6: 3.051  loss_ce_7: 0.3166  loss_mask_7: 0.4062  loss_dice_7: 3.063  loss_ce_8: 0.3247  loss_mask_8: 0.4052  loss_dice_8: 3.063  time: 1.4983  data_time: 0.0575  lr: 6.3951e-06  max_mem: 21589M
[01/18 01:53:16] d2.utils.events INFO:  eta: 9:43:09  iter: 15679  total_loss: 39.05  loss_ce: 0.3295  loss_mask: 0.4255  loss_dice: 3.064  loss_ce_0: 0.5694  loss_mask_0: 0.4138  loss_dice_0: 3.207  loss_ce_1: 0.3527  loss_mask_1: 0.4286  loss_dice_1: 3.102  loss_ce_2: 0.3464  loss_mask_2: 0.4273  loss_dice_2: 3.08  loss_ce_3: 0.3238  loss_mask_3: 0.4256  loss_dice_3: 3.064  loss_ce_4: 0.3308  loss_mask_4: 0.4273  loss_dice_4: 3.064  loss_ce_5: 0.3284  loss_mask_5: 0.4255  loss_dice_5: 3.066  loss_ce_6: 0.3123  loss_mask_6: 0.4262  loss_dice_6: 3.071  loss_ce_7: 0.3128  loss_mask_7: 0.425  loss_dice_7: 3.074  loss_ce_8: 0.31  loss_mask_8: 0.4236  loss_dice_8: 3.065  time: 1.4982  data_time: 0.0615  lr: 6.3904e-06  max_mem: 21589M
[01/18 01:53:44] d2.utils.events INFO:  eta: 9:42:38  iter: 15699  total_loss: 38.42  loss_ce: 0.3118  loss_mask: 0.4144  loss_dice: 3.034  loss_ce_0: 0.5638  loss_mask_0: 0.3986  loss_dice_0: 3.168  loss_ce_1: 0.3243  loss_mask_1: 0.4172  loss_dice_1: 3.08  loss_ce_2: 0.3361  loss_mask_2: 0.4137  loss_dice_2: 3.053  loss_ce_3: 0.3186  loss_mask_3: 0.4122  loss_dice_3: 3.038  loss_ce_4: 0.3118  loss_mask_4: 0.4142  loss_dice_4: 3.031  loss_ce_5: 0.3073  loss_mask_5: 0.413  loss_dice_5: 3.033  loss_ce_6: 0.3176  loss_mask_6: 0.4136  loss_dice_6: 3.029  loss_ce_7: 0.3152  loss_mask_7: 0.4139  loss_dice_7: 3.034  loss_ce_8: 0.3047  loss_mask_8: 0.4158  loss_dice_8: 3.03  time: 1.4981  data_time: 0.0726  lr: 6.3857e-06  max_mem: 21589M
[01/18 01:54:14] d2.utils.events INFO:  eta: 9:42:24  iter: 15719  total_loss: 38.85  loss_ce: 0.3215  loss_mask: 0.4185  loss_dice: 3.11  loss_ce_0: 0.5482  loss_mask_0: 0.4076  loss_dice_0: 3.246  loss_ce_1: 0.3178  loss_mask_1: 0.4238  loss_dice_1: 3.151  loss_ce_2: 0.3521  loss_mask_2: 0.4201  loss_dice_2: 3.121  loss_ce_3: 0.3391  loss_mask_3: 0.4177  loss_dice_3: 3.114  loss_ce_4: 0.3225  loss_mask_4: 0.418  loss_dice_4: 3.103  loss_ce_5: 0.3004  loss_mask_5: 0.4189  loss_dice_5: 3.103  loss_ce_6: 0.3171  loss_mask_6: 0.4183  loss_dice_6: 3.106  loss_ce_7: 0.3134  loss_mask_7: 0.4174  loss_dice_7: 3.102  loss_ce_8: 0.306  loss_mask_8: 0.4185  loss_dice_8: 3.097  time: 1.4981  data_time: 0.0640  lr: 6.381e-06  max_mem: 21589M
[01/18 01:54:42] d2.utils.events INFO:  eta: 9:41:50  iter: 15739  total_loss: 39.04  loss_ce: 0.3201  loss_mask: 0.4068  loss_dice: 3.14  loss_ce_0: 0.571  loss_mask_0: 0.3977  loss_dice_0: 3.257  loss_ce_1: 0.3316  loss_mask_1: 0.4128  loss_dice_1: 3.164  loss_ce_2: 0.3323  loss_mask_2: 0.4093  loss_dice_2: 3.15  loss_ce_3: 0.3254  loss_mask_3: 0.4079  loss_dice_3: 3.134  loss_ce_4: 0.3284  loss_mask_4: 0.408  loss_dice_4: 3.138  loss_ce_5: 0.3114  loss_mask_5: 0.4078  loss_dice_5: 3.142  loss_ce_6: 0.3057  loss_mask_6: 0.4095  loss_dice_6: 3.14  loss_ce_7: 0.3078  loss_mask_7: 0.4094  loss_dice_7: 3.137  loss_ce_8: 0.2989  loss_mask_8: 0.4078  loss_dice_8: 3.138  time: 1.4980  data_time: 0.0606  lr: 6.3762e-06  max_mem: 21589M
[01/18 01:55:11] d2.utils.events INFO:  eta: 9:41:17  iter: 15759  total_loss: 37.77  loss_ce: 0.3211  loss_mask: 0.419  loss_dice: 2.969  loss_ce_0: 0.5737  loss_mask_0: 0.4053  loss_dice_0: 3.115  loss_ce_1: 0.3647  loss_mask_1: 0.4265  loss_dice_1: 3.005  loss_ce_2: 0.3747  loss_mask_2: 0.4207  loss_dice_2: 2.993  loss_ce_3: 0.3396  loss_mask_3: 0.418  loss_dice_3: 2.977  loss_ce_4: 0.3398  loss_mask_4: 0.4186  loss_dice_4: 2.971  loss_ce_5: 0.3369  loss_mask_5: 0.4188  loss_dice_5: 2.973  loss_ce_6: 0.3306  loss_mask_6: 0.4209  loss_dice_6: 2.969  loss_ce_7: 0.3471  loss_mask_7: 0.4202  loss_dice_7: 2.969  loss_ce_8: 0.3248  loss_mask_8: 0.4189  loss_dice_8: 2.972  time: 1.4979  data_time: 0.0661  lr: 6.3715e-06  max_mem: 21589M
[01/18 01:55:40] d2.utils.events INFO:  eta: 9:40:47  iter: 15779  total_loss: 38.39  loss_ce: 0.3256  loss_mask: 0.4087  loss_dice: 3.052  loss_ce_0: 0.5795  loss_mask_0: 0.3948  loss_dice_0: 3.196  loss_ce_1: 0.3362  loss_mask_1: 0.4098  loss_dice_1: 3.097  loss_ce_2: 0.3321  loss_mask_2: 0.4066  loss_dice_2: 3.081  loss_ce_3: 0.3268  loss_mask_3: 0.4068  loss_dice_3: 3.063  loss_ce_4: 0.3194  loss_mask_4: 0.4087  loss_dice_4: 3.063  loss_ce_5: 0.32  loss_mask_5: 0.4088  loss_dice_5: 3.064  loss_ce_6: 0.3166  loss_mask_6: 0.4087  loss_dice_6: 3.057  loss_ce_7: 0.3219  loss_mask_7: 0.4097  loss_dice_7: 3.053  loss_ce_8: 0.3084  loss_mask_8: 0.4091  loss_dice_8: 3.06  time: 1.4979  data_time: 0.0668  lr: 6.3668e-06  max_mem: 21589M
[01/18 01:56:09] d2.utils.events INFO:  eta: 9:40:14  iter: 15799  total_loss: 38.15  loss_ce: 0.3048  loss_mask: 0.3966  loss_dice: 3.06  loss_ce_0: 0.6093  loss_mask_0: 0.3876  loss_dice_0: 3.202  loss_ce_1: 0.3476  loss_mask_1: 0.3966  loss_dice_1: 3.103  loss_ce_2: 0.3345  loss_mask_2: 0.3972  loss_dice_2: 3.078  loss_ce_3: 0.3103  loss_mask_3: 0.3962  loss_dice_3: 3.059  loss_ce_4: 0.327  loss_mask_4: 0.396  loss_dice_4: 3.056  loss_ce_5: 0.3236  loss_mask_5: 0.3963  loss_dice_5: 3.062  loss_ce_6: 0.316  loss_mask_6: 0.394  loss_dice_6: 3.059  loss_ce_7: 0.3114  loss_mask_7: 0.3947  loss_dice_7: 3.059  loss_ce_8: 0.3046  loss_mask_8: 0.3956  loss_dice_8: 3.05  time: 1.4978  data_time: 0.0578  lr: 6.362e-06  max_mem: 21589M
[01/18 01:56:38] d2.utils.events INFO:  eta: 9:39:38  iter: 15819  total_loss: 38.5  loss_ce: 0.3062  loss_mask: 0.4101  loss_dice: 3.046  loss_ce_0: 0.5944  loss_mask_0: 0.3941  loss_dice_0: 3.186  loss_ce_1: 0.3437  loss_mask_1: 0.4078  loss_dice_1: 3.087  loss_ce_2: 0.3477  loss_mask_2: 0.4073  loss_dice_2: 3.06  loss_ce_3: 0.3263  loss_mask_3: 0.4119  loss_dice_3: 3.04  loss_ce_4: 0.3386  loss_mask_4: 0.4096  loss_dice_4: 3.041  loss_ce_5: 0.3231  loss_mask_5: 0.4106  loss_dice_5: 3.042  loss_ce_6: 0.3148  loss_mask_6: 0.4106  loss_dice_6: 3.046  loss_ce_7: 0.3032  loss_mask_7: 0.4112  loss_dice_7: 3.046  loss_ce_8: 0.3119  loss_mask_8: 0.4096  loss_dice_8: 3.047  time: 1.4977  data_time: 0.0649  lr: 6.3573e-06  max_mem: 21589M
[01/18 01:57:07] d2.utils.events INFO:  eta: 9:39:16  iter: 15839  total_loss: 38.76  loss_ce: 0.3212  loss_mask: 0.4102  loss_dice: 3.069  loss_ce_0: 0.5619  loss_mask_0: 0.4011  loss_dice_0: 3.2  loss_ce_1: 0.3385  loss_mask_1: 0.4144  loss_dice_1: 3.117  loss_ce_2: 0.3599  loss_mask_2: 0.4111  loss_dice_2: 3.094  loss_ce_3: 0.3313  loss_mask_3: 0.408  loss_dice_3: 3.082  loss_ce_4: 0.3236  loss_mask_4: 0.4083  loss_dice_4: 3.084  loss_ce_5: 0.3224  loss_mask_5: 0.4078  loss_dice_5: 3.073  loss_ce_6: 0.315  loss_mask_6: 0.4122  loss_dice_6: 3.066  loss_ce_7: 0.3264  loss_mask_7: 0.4108  loss_dice_7: 3.075  loss_ce_8: 0.3168  loss_mask_8: 0.4104  loss_dice_8: 3.067  time: 1.4976  data_time: 0.0740  lr: 6.3526e-06  max_mem: 21589M
[01/18 01:57:36] d2.utils.events INFO:  eta: 9:38:41  iter: 15859  total_loss: 38.2  loss_ce: 0.3079  loss_mask: 0.4071  loss_dice: 3.018  loss_ce_0: 0.603  loss_mask_0: 0.3991  loss_dice_0: 3.164  loss_ce_1: 0.3533  loss_mask_1: 0.4174  loss_dice_1: 3.057  loss_ce_2: 0.344  loss_mask_2: 0.4139  loss_dice_2: 3.023  loss_ce_3: 0.3273  loss_mask_3: 0.409  loss_dice_3: 3.011  loss_ce_4: 0.3421  loss_mask_4: 0.4075  loss_dice_4: 3.022  loss_ce_5: 0.3253  loss_mask_5: 0.4071  loss_dice_5: 3.022  loss_ce_6: 0.3223  loss_mask_6: 0.4065  loss_dice_6: 3.013  loss_ce_7: 0.3117  loss_mask_7: 0.4089  loss_dice_7: 3.019  loss_ce_8: 0.3173  loss_mask_8: 0.4093  loss_dice_8: 3.019  time: 1.4976  data_time: 0.0662  lr: 6.3478e-06  max_mem: 21589M
[01/18 01:58:04] d2.utils.events INFO:  eta: 9:38:18  iter: 15879  total_loss: 38.84  loss_ce: 0.3332  loss_mask: 0.4123  loss_dice: 3.088  loss_ce_0: 0.581  loss_mask_0: 0.3989  loss_dice_0: 3.211  loss_ce_1: 0.3668  loss_mask_1: 0.4183  loss_dice_1: 3.118  loss_ce_2: 0.3605  loss_mask_2: 0.4174  loss_dice_2: 3.1  loss_ce_3: 0.3462  loss_mask_3: 0.414  loss_dice_3: 3.09  loss_ce_4: 0.3372  loss_mask_4: 0.4141  loss_dice_4: 3.099  loss_ce_5: 0.3392  loss_mask_5: 0.4127  loss_dice_5: 3.086  loss_ce_6: 0.3207  loss_mask_6: 0.4125  loss_dice_6: 3.092  loss_ce_7: 0.3233  loss_mask_7: 0.4135  loss_dice_7: 3.091  loss_ce_8: 0.3317  loss_mask_8: 0.4121  loss_dice_8: 3.089  time: 1.4975  data_time: 0.0707  lr: 6.3431e-06  max_mem: 21589M
[01/18 01:58:33] d2.utils.events INFO:  eta: 9:37:47  iter: 15899  total_loss: 38.76  loss_ce: 0.3316  loss_mask: 0.4116  loss_dice: 3.106  loss_ce_0: 0.5811  loss_mask_0: 0.4011  loss_dice_0: 3.241  loss_ce_1: 0.3609  loss_mask_1: 0.4141  loss_dice_1: 3.139  loss_ce_2: 0.3514  loss_mask_2: 0.4132  loss_dice_2: 3.121  loss_ce_3: 0.3449  loss_mask_3: 0.4127  loss_dice_3: 3.102  loss_ce_4: 0.3385  loss_mask_4: 0.4127  loss_dice_4: 3.101  loss_ce_5: 0.3298  loss_mask_5: 0.4122  loss_dice_5: 3.108  loss_ce_6: 0.3286  loss_mask_6: 0.4137  loss_dice_6: 3.106  loss_ce_7: 0.329  loss_mask_7: 0.412  loss_dice_7: 3.101  loss_ce_8: 0.3304  loss_mask_8: 0.4121  loss_dice_8: 3.106  time: 1.4974  data_time: 0.0704  lr: 6.3384e-06  max_mem: 21589M
[01/18 01:59:02] d2.utils.events INFO:  eta: 9:37:08  iter: 15919  total_loss: 38.54  loss_ce: 0.3176  loss_mask: 0.4138  loss_dice: 3.083  loss_ce_0: 0.6138  loss_mask_0: 0.404  loss_dice_0: 3.209  loss_ce_1: 0.3412  loss_mask_1: 0.4212  loss_dice_1: 3.122  loss_ce_2: 0.3392  loss_mask_2: 0.415  loss_dice_2: 3.101  loss_ce_3: 0.3427  loss_mask_3: 0.4157  loss_dice_3: 3.083  loss_ce_4: 0.324  loss_mask_4: 0.4162  loss_dice_4: 3.085  loss_ce_5: 0.3239  loss_mask_5: 0.4151  loss_dice_5: 3.083  loss_ce_6: 0.3178  loss_mask_6: 0.4126  loss_dice_6: 3.086  loss_ce_7: 0.32  loss_mask_7: 0.4099  loss_dice_7: 3.086  loss_ce_8: 0.3128  loss_mask_8: 0.4117  loss_dice_8: 3.09  time: 1.4973  data_time: 0.0631  lr: 6.3336e-06  max_mem: 21589M
[01/18 01:59:31] d2.utils.events INFO:  eta: 9:36:26  iter: 15939  total_loss: 38.27  loss_ce: 0.3259  loss_mask: 0.4029  loss_dice: 3.049  loss_ce_0: 0.6167  loss_mask_0: 0.3939  loss_dice_0: 3.203  loss_ce_1: 0.35  loss_mask_1: 0.4073  loss_dice_1: 3.077  loss_ce_2: 0.3652  loss_mask_2: 0.4042  loss_dice_2: 3.062  loss_ce_3: 0.3417  loss_mask_3: 0.4042  loss_dice_3: 3.053  loss_ce_4: 0.333  loss_mask_4: 0.4033  loss_dice_4: 3.051  loss_ce_5: 0.324  loss_mask_5: 0.4042  loss_dice_5: 3.04  loss_ce_6: 0.3382  loss_mask_6: 0.4053  loss_dice_6: 3.037  loss_ce_7: 0.3229  loss_mask_7: 0.4042  loss_dice_7: 3.043  loss_ce_8: 0.3232  loss_mask_8: 0.4035  loss_dice_8: 3.04  time: 1.4972  data_time: 0.0620  lr: 6.3289e-06  max_mem: 21589M
[01/18 02:00:00] d2.utils.events INFO:  eta: 9:35:57  iter: 15959  total_loss: 38.47  loss_ce: 0.3215  loss_mask: 0.3946  loss_dice: 3.076  loss_ce_0: 0.5946  loss_mask_0: 0.3866  loss_dice_0: 3.231  loss_ce_1: 0.3228  loss_mask_1: 0.3961  loss_dice_1: 3.128  loss_ce_2: 0.3358  loss_mask_2: 0.3947  loss_dice_2: 3.1  loss_ce_3: 0.3069  loss_mask_3: 0.3945  loss_dice_3: 3.079  loss_ce_4: 0.2989  loss_mask_4: 0.3915  loss_dice_4: 3.08  loss_ce_5: 0.3092  loss_mask_5: 0.3919  loss_dice_5: 3.082  loss_ce_6: 0.2979  loss_mask_6: 0.3937  loss_dice_6: 3.079  loss_ce_7: 0.3047  loss_mask_7: 0.3932  loss_dice_7: 3.084  loss_ce_8: 0.2977  loss_mask_8: 0.3937  loss_dice_8: 3.073  time: 1.4972  data_time: 0.0685  lr: 6.3242e-06  max_mem: 21589M
[01/18 02:00:28] d2.utils.events INFO:  eta: 9:35:19  iter: 15979  total_loss: 38.36  loss_ce: 0.3063  loss_mask: 0.4339  loss_dice: 3.061  loss_ce_0: 0.5557  loss_mask_0: 0.4275  loss_dice_0: 3.198  loss_ce_1: 0.3283  loss_mask_1: 0.438  loss_dice_1: 3.098  loss_ce_2: 0.3352  loss_mask_2: 0.4361  loss_dice_2: 3.074  loss_ce_3: 0.32  loss_mask_3: 0.4318  loss_dice_3: 3.061  loss_ce_4: 0.2972  loss_mask_4: 0.4305  loss_dice_4: 3.066  loss_ce_5: 0.3032  loss_mask_5: 0.4318  loss_dice_5: 3.064  loss_ce_6: 0.3067  loss_mask_6: 0.4322  loss_dice_6: 3.051  loss_ce_7: 0.2959  loss_mask_7: 0.4331  loss_dice_7: 3.064  loss_ce_8: 0.2883  loss_mask_8: 0.434  loss_dice_8: 3.062  time: 1.4971  data_time: 0.0729  lr: 6.3194e-06  max_mem: 21589M
[01/18 02:00:58] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 02:00:59] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 02:00:59] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 02:00:59] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 02:01:13] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0061 s/iter. Inference: 0.1509 s/iter. Eval: 0.2051 s/iter. Total: 0.3621 s/iter. ETA=0:06:31
[01/18 02:01:18] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0089 s/iter. Inference: 0.1570 s/iter. Eval: 0.2219 s/iter. Total: 0.3879 s/iter. ETA=0:06:54
[01/18 02:01:23] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0088 s/iter. Inference: 0.1618 s/iter. Eval: 0.2152 s/iter. Total: 0.3858 s/iter. ETA=0:06:47
[01/18 02:01:28] d2.evaluation.evaluator INFO: Inference done 52/1093. Dataloading: 0.0095 s/iter. Inference: 0.1610 s/iter. Eval: 0.2129 s/iter. Total: 0.3834 s/iter. ETA=0:06:39
[01/18 02:01:34] d2.evaluation.evaluator INFO: Inference done 63/1093. Dataloading: 0.0099 s/iter. Inference: 0.1644 s/iter. Eval: 0.2272 s/iter. Total: 0.4016 s/iter. ETA=0:06:53
[01/18 02:01:39] d2.evaluation.evaluator INFO: Inference done 76/1093. Dataloading: 0.0100 s/iter. Inference: 0.1636 s/iter. Eval: 0.2256 s/iter. Total: 0.3992 s/iter. ETA=0:06:45
[01/18 02:01:44] d2.evaluation.evaluator INFO: Inference done 89/1093. Dataloading: 0.0101 s/iter. Inference: 0.1627 s/iter. Eval: 0.2286 s/iter. Total: 0.4015 s/iter. ETA=0:06:43
[01/18 02:01:49] d2.evaluation.evaluator INFO: Inference done 103/1093. Dataloading: 0.0101 s/iter. Inference: 0.1640 s/iter. Eval: 0.2226 s/iter. Total: 0.3968 s/iter. ETA=0:06:32
[01/18 02:01:54] d2.evaluation.evaluator INFO: Inference done 117/1093. Dataloading: 0.0100 s/iter. Inference: 0.1646 s/iter. Eval: 0.2177 s/iter. Total: 0.3924 s/iter. ETA=0:06:22
[01/18 02:02:00] d2.evaluation.evaluator INFO: Inference done 130/1093. Dataloading: 0.0100 s/iter. Inference: 0.1645 s/iter. Eval: 0.2190 s/iter. Total: 0.3936 s/iter. ETA=0:06:19
[01/18 02:02:05] d2.evaluation.evaluator INFO: Inference done 144/1093. Dataloading: 0.0099 s/iter. Inference: 0.1651 s/iter. Eval: 0.2156 s/iter. Total: 0.3907 s/iter. ETA=0:06:10
[01/18 02:02:10] d2.evaluation.evaluator INFO: Inference done 159/1093. Dataloading: 0.0100 s/iter. Inference: 0.1645 s/iter. Eval: 0.2107 s/iter. Total: 0.3853 s/iter. ETA=0:05:59
[01/18 02:02:15] d2.evaluation.evaluator INFO: Inference done 173/1093. Dataloading: 0.0101 s/iter. Inference: 0.1638 s/iter. Eval: 0.2109 s/iter. Total: 0.3849 s/iter. ETA=0:05:54
[01/18 02:02:20] d2.evaluation.evaluator INFO: Inference done 187/1093. Dataloading: 0.0100 s/iter. Inference: 0.1635 s/iter. Eval: 0.2096 s/iter. Total: 0.3832 s/iter. ETA=0:05:47
[01/18 02:02:25] d2.evaluation.evaluator INFO: Inference done 201/1093. Dataloading: 0.0100 s/iter. Inference: 0.1636 s/iter. Eval: 0.2086 s/iter. Total: 0.3823 s/iter. ETA=0:05:40
[01/18 02:02:31] d2.evaluation.evaluator INFO: Inference done 215/1093. Dataloading: 0.0100 s/iter. Inference: 0.1627 s/iter. Eval: 0.2087 s/iter. Total: 0.3815 s/iter. ETA=0:05:34
[01/18 02:02:36] d2.evaluation.evaluator INFO: Inference done 229/1093. Dataloading: 0.0099 s/iter. Inference: 0.1631 s/iter. Eval: 0.2083 s/iter. Total: 0.3815 s/iter. ETA=0:05:29
[01/18 02:02:41] d2.evaluation.evaluator INFO: Inference done 242/1093. Dataloading: 0.0101 s/iter. Inference: 0.1635 s/iter. Eval: 0.2095 s/iter. Total: 0.3831 s/iter. ETA=0:05:26
[01/18 02:02:46] d2.evaluation.evaluator INFO: Inference done 256/1093. Dataloading: 0.0100 s/iter. Inference: 0.1625 s/iter. Eval: 0.2092 s/iter. Total: 0.3818 s/iter. ETA=0:05:19
[01/18 02:02:51] d2.evaluation.evaluator INFO: Inference done 270/1093. Dataloading: 0.0100 s/iter. Inference: 0.1630 s/iter. Eval: 0.2085 s/iter. Total: 0.3815 s/iter. ETA=0:05:13
[01/18 02:02:57] d2.evaluation.evaluator INFO: Inference done 284/1093. Dataloading: 0.0100 s/iter. Inference: 0.1629 s/iter. Eval: 0.2081 s/iter. Total: 0.3811 s/iter. ETA=0:05:08
[01/18 02:03:02] d2.evaluation.evaluator INFO: Inference done 297/1093. Dataloading: 0.0100 s/iter. Inference: 0.1634 s/iter. Eval: 0.2084 s/iter. Total: 0.3819 s/iter. ETA=0:05:04
[01/18 02:03:07] d2.evaluation.evaluator INFO: Inference done 308/1093. Dataloading: 0.0101 s/iter. Inference: 0.1637 s/iter. Eval: 0.2108 s/iter. Total: 0.3848 s/iter. ETA=0:05:02
[01/18 02:03:12] d2.evaluation.evaluator INFO: Inference done 320/1093. Dataloading: 0.0102 s/iter. Inference: 0.1638 s/iter. Eval: 0.2120 s/iter. Total: 0.3861 s/iter. ETA=0:04:58
[01/18 02:03:17] d2.evaluation.evaluator INFO: Inference done 333/1093. Dataloading: 0.0102 s/iter. Inference: 0.1642 s/iter. Eval: 0.2117 s/iter. Total: 0.3863 s/iter. ETA=0:04:53
[01/18 02:03:22] d2.evaluation.evaluator INFO: Inference done 349/1093. Dataloading: 0.0101 s/iter. Inference: 0.1644 s/iter. Eval: 0.2089 s/iter. Total: 0.3836 s/iter. ETA=0:04:45
[01/18 02:03:28] d2.evaluation.evaluator INFO: Inference done 363/1093. Dataloading: 0.0102 s/iter. Inference: 0.1646 s/iter. Eval: 0.2083 s/iter. Total: 0.3832 s/iter. ETA=0:04:39
[01/18 02:03:33] d2.evaluation.evaluator INFO: Inference done 377/1093. Dataloading: 0.0102 s/iter. Inference: 0.1645 s/iter. Eval: 0.2075 s/iter. Total: 0.3823 s/iter. ETA=0:04:33
[01/18 02:03:38] d2.evaluation.evaluator INFO: Inference done 389/1093. Dataloading: 0.0102 s/iter. Inference: 0.1651 s/iter. Eval: 0.2085 s/iter. Total: 0.3839 s/iter. ETA=0:04:30
[01/18 02:03:43] d2.evaluation.evaluator INFO: Inference done 403/1093. Dataloading: 0.0102 s/iter. Inference: 0.1644 s/iter. Eval: 0.2084 s/iter. Total: 0.3831 s/iter. ETA=0:04:24
[01/18 02:03:48] d2.evaluation.evaluator INFO: Inference done 417/1093. Dataloading: 0.0102 s/iter. Inference: 0.1641 s/iter. Eval: 0.2083 s/iter. Total: 0.3827 s/iter. ETA=0:04:18
[01/18 02:03:53] d2.evaluation.evaluator INFO: Inference done 428/1093. Dataloading: 0.0103 s/iter. Inference: 0.1641 s/iter. Eval: 0.2103 s/iter. Total: 0.3848 s/iter. ETA=0:04:15
[01/18 02:03:58] d2.evaluation.evaluator INFO: Inference done 444/1093. Dataloading: 0.0102 s/iter. Inference: 0.1640 s/iter. Eval: 0.2085 s/iter. Total: 0.3828 s/iter. ETA=0:04:08
[01/18 02:04:04] d2.evaluation.evaluator INFO: Inference done 458/1093. Dataloading: 0.0102 s/iter. Inference: 0.1644 s/iter. Eval: 0.2081 s/iter. Total: 0.3828 s/iter. ETA=0:04:03
[01/18 02:04:09] d2.evaluation.evaluator INFO: Inference done 474/1093. Dataloading: 0.0102 s/iter. Inference: 0.1640 s/iter. Eval: 0.2065 s/iter. Total: 0.3808 s/iter. ETA=0:03:55
[01/18 02:04:14] d2.evaluation.evaluator INFO: Inference done 488/1093. Dataloading: 0.0102 s/iter. Inference: 0.1639 s/iter. Eval: 0.2060 s/iter. Total: 0.3802 s/iter. ETA=0:03:50
[01/18 02:04:19] d2.evaluation.evaluator INFO: Inference done 503/1093. Dataloading: 0.0101 s/iter. Inference: 0.1640 s/iter. Eval: 0.2050 s/iter. Total: 0.3793 s/iter. ETA=0:03:43
[01/18 02:04:24] d2.evaluation.evaluator INFO: Inference done 518/1093. Dataloading: 0.0101 s/iter. Inference: 0.1637 s/iter. Eval: 0.2042 s/iter. Total: 0.3781 s/iter. ETA=0:03:37
[01/18 02:04:29] d2.evaluation.evaluator INFO: Inference done 529/1093. Dataloading: 0.0102 s/iter. Inference: 0.1638 s/iter. Eval: 0.2058 s/iter. Total: 0.3799 s/iter. ETA=0:03:34
[01/18 02:04:35] d2.evaluation.evaluator INFO: Inference done 544/1093. Dataloading: 0.0101 s/iter. Inference: 0.1637 s/iter. Eval: 0.2050 s/iter. Total: 0.3789 s/iter. ETA=0:03:28
[01/18 02:04:40] d2.evaluation.evaluator INFO: Inference done 556/1093. Dataloading: 0.0101 s/iter. Inference: 0.1640 s/iter. Eval: 0.2055 s/iter. Total: 0.3797 s/iter. ETA=0:03:23
[01/18 02:04:45] d2.evaluation.evaluator INFO: Inference done 570/1093. Dataloading: 0.0101 s/iter. Inference: 0.1639 s/iter. Eval: 0.2056 s/iter. Total: 0.3797 s/iter. ETA=0:03:18
[01/18 02:04:50] d2.evaluation.evaluator INFO: Inference done 585/1093. Dataloading: 0.0102 s/iter. Inference: 0.1643 s/iter. Eval: 0.2043 s/iter. Total: 0.3788 s/iter. ETA=0:03:12
[01/18 02:04:55] d2.evaluation.evaluator INFO: Inference done 598/1093. Dataloading: 0.0102 s/iter. Inference: 0.1644 s/iter. Eval: 0.2048 s/iter. Total: 0.3795 s/iter. ETA=0:03:07
[01/18 02:05:01] d2.evaluation.evaluator INFO: Inference done 611/1093. Dataloading: 0.0102 s/iter. Inference: 0.1645 s/iter. Eval: 0.2051 s/iter. Total: 0.3799 s/iter. ETA=0:03:03
[01/18 02:05:06] d2.evaluation.evaluator INFO: Inference done 626/1093. Dataloading: 0.0102 s/iter. Inference: 0.1642 s/iter. Eval: 0.2048 s/iter. Total: 0.3794 s/iter. ETA=0:02:57
[01/18 02:05:11] d2.evaluation.evaluator INFO: Inference done 640/1093. Dataloading: 0.0102 s/iter. Inference: 0.1644 s/iter. Eval: 0.2044 s/iter. Total: 0.3790 s/iter. ETA=0:02:51
[01/18 02:05:16] d2.evaluation.evaluator INFO: Inference done 654/1093. Dataloading: 0.0102 s/iter. Inference: 0.1646 s/iter. Eval: 0.2039 s/iter. Total: 0.3789 s/iter. ETA=0:02:46
[01/18 02:05:21] d2.evaluation.evaluator INFO: Inference done 667/1093. Dataloading: 0.0102 s/iter. Inference: 0.1646 s/iter. Eval: 0.2043 s/iter. Total: 0.3792 s/iter. ETA=0:02:41
[01/18 02:05:27] d2.evaluation.evaluator INFO: Inference done 681/1093. Dataloading: 0.0102 s/iter. Inference: 0.1648 s/iter. Eval: 0.2039 s/iter. Total: 0.3790 s/iter. ETA=0:02:36
[01/18 02:05:32] d2.evaluation.evaluator INFO: Inference done 697/1093. Dataloading: 0.0102 s/iter. Inference: 0.1645 s/iter. Eval: 0.2032 s/iter. Total: 0.3780 s/iter. ETA=0:02:29
[01/18 02:05:37] d2.evaluation.evaluator INFO: Inference done 710/1093. Dataloading: 0.0102 s/iter. Inference: 0.1644 s/iter. Eval: 0.2038 s/iter. Total: 0.3785 s/iter. ETA=0:02:24
[01/18 02:05:43] d2.evaluation.evaluator INFO: Inference done 724/1093. Dataloading: 0.0102 s/iter. Inference: 0.1644 s/iter. Eval: 0.2038 s/iter. Total: 0.3785 s/iter. ETA=0:02:19
[01/18 02:05:48] d2.evaluation.evaluator INFO: Inference done 739/1093. Dataloading: 0.0102 s/iter. Inference: 0.1643 s/iter. Eval: 0.2030 s/iter. Total: 0.3776 s/iter. ETA=0:02:13
[01/18 02:05:53] d2.evaluation.evaluator INFO: Inference done 755/1093. Dataloading: 0.0101 s/iter. Inference: 0.1639 s/iter. Eval: 0.2024 s/iter. Total: 0.3766 s/iter. ETA=0:02:07
[01/18 02:05:58] d2.evaluation.evaluator INFO: Inference done 769/1093. Dataloading: 0.0102 s/iter. Inference: 0.1636 s/iter. Eval: 0.2027 s/iter. Total: 0.3766 s/iter. ETA=0:02:02
[01/18 02:06:03] d2.evaluation.evaluator INFO: Inference done 784/1093. Dataloading: 0.0101 s/iter. Inference: 0.1636 s/iter. Eval: 0.2023 s/iter. Total: 0.3761 s/iter. ETA=0:01:56
[01/18 02:06:09] d2.evaluation.evaluator INFO: Inference done 800/1093. Dataloading: 0.0101 s/iter. Inference: 0.1633 s/iter. Eval: 0.2015 s/iter. Total: 0.3751 s/iter. ETA=0:01:49
[01/18 02:06:14] d2.evaluation.evaluator INFO: Inference done 814/1093. Dataloading: 0.0101 s/iter. Inference: 0.1635 s/iter. Eval: 0.2014 s/iter. Total: 0.3751 s/iter. ETA=0:01:44
[01/18 02:06:19] d2.evaluation.evaluator INFO: Inference done 830/1093. Dataloading: 0.0101 s/iter. Inference: 0.1631 s/iter. Eval: 0.2008 s/iter. Total: 0.3741 s/iter. ETA=0:01:38
[01/18 02:06:24] d2.evaluation.evaluator INFO: Inference done 845/1093. Dataloading: 0.0100 s/iter. Inference: 0.1632 s/iter. Eval: 0.2004 s/iter. Total: 0.3737 s/iter. ETA=0:01:32
[01/18 02:06:30] d2.evaluation.evaluator INFO: Inference done 857/1093. Dataloading: 0.0101 s/iter. Inference: 0.1635 s/iter. Eval: 0.2010 s/iter. Total: 0.3747 s/iter. ETA=0:01:28
[01/18 02:06:35] d2.evaluation.evaluator INFO: Inference done 870/1093. Dataloading: 0.0101 s/iter. Inference: 0.1634 s/iter. Eval: 0.2014 s/iter. Total: 0.3750 s/iter. ETA=0:01:23
[01/18 02:06:40] d2.evaluation.evaluator INFO: Inference done 884/1093. Dataloading: 0.0101 s/iter. Inference: 0.1634 s/iter. Eval: 0.2015 s/iter. Total: 0.3751 s/iter. ETA=0:01:18
[01/18 02:06:45] d2.evaluation.evaluator INFO: Inference done 897/1093. Dataloading: 0.0101 s/iter. Inference: 0.1634 s/iter. Eval: 0.2017 s/iter. Total: 0.3753 s/iter. ETA=0:01:13
[01/18 02:06:50] d2.evaluation.evaluator INFO: Inference done 912/1093. Dataloading: 0.0100 s/iter. Inference: 0.1635 s/iter. Eval: 0.2011 s/iter. Total: 0.3748 s/iter. ETA=0:01:07
[01/18 02:06:56] d2.evaluation.evaluator INFO: Inference done 926/1093. Dataloading: 0.0100 s/iter. Inference: 0.1634 s/iter. Eval: 0.2013 s/iter. Total: 0.3749 s/iter. ETA=0:01:02
[01/18 02:07:01] d2.evaluation.evaluator INFO: Inference done 941/1093. Dataloading: 0.0100 s/iter. Inference: 0.1633 s/iter. Eval: 0.2012 s/iter. Total: 0.3746 s/iter. ETA=0:00:56
[01/18 02:07:06] d2.evaluation.evaluator INFO: Inference done 955/1093. Dataloading: 0.0100 s/iter. Inference: 0.1632 s/iter. Eval: 0.2014 s/iter. Total: 0.3747 s/iter. ETA=0:00:51
[01/18 02:07:12] d2.evaluation.evaluator INFO: Inference done 970/1093. Dataloading: 0.0100 s/iter. Inference: 0.1630 s/iter. Eval: 0.2014 s/iter. Total: 0.3744 s/iter. ETA=0:00:46
[01/18 02:07:17] d2.evaluation.evaluator INFO: Inference done 987/1093. Dataloading: 0.0100 s/iter. Inference: 0.1628 s/iter. Eval: 0.2004 s/iter. Total: 0.3733 s/iter. ETA=0:00:39
[01/18 02:07:22] d2.evaluation.evaluator INFO: Inference done 1001/1093. Dataloading: 0.0100 s/iter. Inference: 0.1629 s/iter. Eval: 0.2003 s/iter. Total: 0.3732 s/iter. ETA=0:00:34
[01/18 02:07:27] d2.evaluation.evaluator INFO: Inference done 1016/1093. Dataloading: 0.0099 s/iter. Inference: 0.1626 s/iter. Eval: 0.2001 s/iter. Total: 0.3727 s/iter. ETA=0:00:28
[01/18 02:07:32] d2.evaluation.evaluator INFO: Inference done 1031/1093. Dataloading: 0.0099 s/iter. Inference: 0.1622 s/iter. Eval: 0.1999 s/iter. Total: 0.3722 s/iter. ETA=0:00:23
[01/18 02:07:37] d2.evaluation.evaluator INFO: Inference done 1046/1093. Dataloading: 0.0099 s/iter. Inference: 0.1620 s/iter. Eval: 0.1998 s/iter. Total: 0.3717 s/iter. ETA=0:00:17
[01/18 02:07:43] d2.evaluation.evaluator INFO: Inference done 1062/1093. Dataloading: 0.0099 s/iter. Inference: 0.1617 s/iter. Eval: 0.1993 s/iter. Total: 0.3710 s/iter. ETA=0:00:11
[01/18 02:07:48] d2.evaluation.evaluator INFO: Inference done 1080/1093. Dataloading: 0.0098 s/iter. Inference: 0.1613 s/iter. Eval: 0.1983 s/iter. Total: 0.3696 s/iter. ETA=0:00:04
[01/18 02:07:52] d2.evaluation.evaluator INFO: Total inference time: 0:06:41.708629 (0.369217 s / iter per device, on 4 devices)
[01/18 02:07:52] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:55 (0.160984 s / iter per device, on 4 devices)
[01/18 02:08:15] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.6326468289110423, 'mIoU': 16.42501505970103, 'fwIoU': 35.89340505890964, 'IoU-0': nan, 'IoU-1': 95.40700798716664, 'IoU-2': 46.70229546093834, 'IoU-3': 56.29951256315958, 'IoU-4': 50.40567799924567, 'IoU-5': 43.184069677027395, 'IoU-6': 39.967136779979306, 'IoU-7': 33.154906671043406, 'IoU-8': 14.279360995159335, 'IoU-9': 29.99202357584554, 'IoU-10': 31.352972153275193, 'IoU-11': 40.77323605824319, 'IoU-12': 41.026996012813576, 'IoU-13': 37.50512250317194, 'IoU-14': 36.290380897833515, 'IoU-15': 36.96852122998737, 'IoU-16': 35.375197050380294, 'IoU-17': 32.4421361851447, 'IoU-18': 36.15232865493466, 'IoU-19': 36.24772728645928, 'IoU-20': 36.060171219886264, 'IoU-21': 35.38894449116334, 'IoU-22': 39.29753269311919, 'IoU-23': 37.575528338870846, 'IoU-24': 36.741833001450466, 'IoU-25': 35.767373586900106, 'IoU-26': 34.20762668438581, 'IoU-27': 33.93820226258467, 'IoU-28': 35.126267046782615, 'IoU-29': 36.580383416498364, 'IoU-30': 35.470934383693006, 'IoU-31': 36.447935392544636, 'IoU-32': 35.359952585486184, 'IoU-33': 35.48543789000429, 'IoU-34': 34.18603792642908, 'IoU-35': 34.27466081304956, 'IoU-36': 34.5235279489942, 'IoU-37': 33.51896673563344, 'IoU-38': 32.935459181193224, 'IoU-39': 33.1400183892329, 'IoU-40': 32.60486381586855, 'IoU-41': 29.965074157242285, 'IoU-42': 31.21720601255541, 'IoU-43': 29.778157083664407, 'IoU-44': 29.91176689499152, 'IoU-45': 30.3282291518062, 'IoU-46': 29.429129946021646, 'IoU-47': 29.757715044504597, 'IoU-48': 29.790294291973794, 'IoU-49': 27.525186517049015, 'IoU-50': 29.253471924075843, 'IoU-51': 26.434265695528918, 'IoU-52': 27.175381261856018, 'IoU-53': 26.05026910186372, 'IoU-54': 26.375583729045342, 'IoU-55': 27.14907399244862, 'IoU-56': 25.62877825015753, 'IoU-57': 26.057163731121136, 'IoU-58': 23.887848070607845, 'IoU-59': 22.094779986601537, 'IoU-60': 22.493939336865246, 'IoU-61': 22.254498611583546, 'IoU-62': 22.055695507203442, 'IoU-63': 22.913110194344792, 'IoU-64': 21.328877025206364, 'IoU-65': 20.45087194588874, 'IoU-66': 19.507373067310876, 'IoU-67': 18.776579876838248, 'IoU-68': 19.46082568377539, 'IoU-69': 17.528162159527085, 'IoU-70': 18.3402263501148, 'IoU-71': 16.434251634384584, 'IoU-72': 16.999320040347868, 'IoU-73': 16.79287966440981, 'IoU-74': 16.443370767473475, 'IoU-75': 16.993637682031128, 'IoU-76': 16.666610417326915, 'IoU-77': 13.436446454664466, 'IoU-78': 15.736061218630686, 'IoU-79': 13.83901070628, 'IoU-80': 14.868846058073556, 'IoU-81': 11.479097258055804, 'IoU-82': 15.718096478723023, 'IoU-83': 15.298318094576905, 'IoU-84': 14.240334550339046, 'IoU-85': 15.38307352569808, 'IoU-86': 13.586265053169114, 'IoU-87': 14.6443652292918, 'IoU-88': 14.74355934902904, 'IoU-89': 15.699391263061596, 'IoU-90': 13.861236240752916, 'IoU-91': 15.51522401785051, 'IoU-92': 12.338319996770926, 'IoU-93': 14.621679094231666, 'IoU-94': 14.335250864340585, 'IoU-95': 14.557393101020514, 'IoU-96': 12.353623750137995, 'IoU-97': 14.851276323109841, 'IoU-98': 13.650202857944125, 'IoU-99': 14.389967592247807, 'IoU-100': 12.52627785275784, 'IoU-101': 12.75917987406436, 'IoU-102': 12.786256588189366, 'IoU-103': 10.730524001037669, 'IoU-104': 12.830508890614592, 'IoU-105': 10.145036924865359, 'IoU-106': 11.367900613381176, 'IoU-107': 11.863367373078928, 'IoU-108': 10.31234452697836, 'IoU-109': 12.445679046546907, 'IoU-110': 10.756809443068189, 'IoU-111': 10.360518051010263, 'IoU-112': 9.725380721445037, 'IoU-113': 10.922078656766379, 'IoU-114': 9.912560395152001, 'IoU-115': 11.416525941047286, 'IoU-116': 7.812411853601373, 'IoU-117': 8.606587022930496, 'IoU-118': 9.639925786846723, 'IoU-119': 7.71354092197078, 'IoU-120': 8.860692997429005, 'IoU-121': 7.565831964717389, 'IoU-122': 8.05140445144678, 'IoU-123': 7.412156347714357, 'IoU-124': 6.0217287352366515, 'IoU-125': 6.27377594056364, 'IoU-126': 7.229292671914743, 'IoU-127': 5.616663317947994, 'IoU-128': 4.689738648398881, 'IoU-129': 6.570094887220812, 'IoU-130': 5.560141589366655, 'IoU-131': 5.222295388821787, 'IoU-132': 5.941651144256763, 'IoU-133': 5.4471812764263055, 'IoU-134': 5.562475710542807, 'IoU-135': 4.993485292939771, 'IoU-136': 5.35070859697356, 'IoU-137': 5.115149812265285, 'IoU-138': 4.594524709263164, 'IoU-139': 5.140833184176414, 'IoU-140': 2.8230784882079667, 'IoU-141': 4.176258611313568, 'IoU-142': 3.5558431559252077, 'IoU-143': 3.569667810635034, 'IoU-144': 5.8751902587519025, 'IoU-145': 2.227414530940836, 'IoU-146': 3.4655134171221764, 'IoU-147': 2.3063082655458205, 'IoU-148': 4.162506540610841, 'IoU-149': 2.6144004557674476, 'IoU-150': 3.7403214686888795, 'IoU-151': 2.834904035656108, 'IoU-152': 2.8989906542056074, 'IoU-153': 3.401479478061261, 'IoU-154': 3.457100200919165, 'IoU-155': 2.2114065132556084, 'IoU-156': 2.5546072494813368, 'IoU-157': 1.3279258180015705, 'IoU-158': 2.8875785468061377, 'IoU-159': 1.5767567486623262, 'IoU-160': 1.915595018431588, 'IoU-161': 3.32499068994285, 'IoU-162': 1.6506141752462233, 'IoU-163': 1.3241880649608544, 'IoU-164': 2.518185955572907, 'IoU-165': 2.172405571123259, 'IoU-166': 1.133356232723228, 'IoU-167': 0.635453886657144, 'IoU-168': 1.6332427151602338, 'IoU-169': 1.5787609726672474, 'IoU-170': 1.1972065903951825, 'IoU-171': 0.6519386596985773, 'IoU-172': 3.1157009440494488, 'IoU-173': 1.5176835256907126, 'IoU-174': 0.3386416723521376, 'IoU-175': 1.2550539083557952, 'IoU-176': 0.40894942833369285, 'IoU-177': 1.3008196805400247, 'IoU-178': 0.06907533163687832, 'IoU-179': 0.4628325090445052, 'IoU-180': 1.0910884856115544, 'IoU-181': 1.3777508392390898, 'IoU-182': 0.8976851930133072, 'IoU-183': 2.841475392417791, 'IoU-184': 1.2532721382896608, 'IoU-185': 1.0414880341852686, 'IoU-186': 2.5963352393208643, 'IoU-187': 2.1194088781093123, 'IoU-188': 1.425352314185564, 'IoU-189': 1.245575351854181, 'IoU-190': 0.5774561488471632, 'IoU-191': 0.4308880223782594, 'mACC': 25.863525080714066, 'pACC': 49.704586033674794, 'ACC-0': nan, 'ACC-1': 98.15178784286913, 'ACC-2': 65.78672846628089, 'ACC-3': 70.00781049610515, 'ACC-4': 67.92032579850537, 'ACC-5': 60.12660125654351, 'ACC-6': 57.55341587377142, 'ACC-7': 48.6140433398602, 'ACC-8': 17.131262859953786, 'ACC-9': 45.71200094885276, 'ACC-10': 46.87868049988093, 'ACC-11': 60.650204066537064, 'ACC-12': 61.79015063470099, 'ACC-13': 56.94724091416734, 'ACC-14': 52.48651886716307, 'ACC-15': 55.53291878686216, 'ACC-16': 51.100340087278504, 'ACC-17': 49.896153434453055, 'ACC-18': 52.39680746232773, 'ACC-19': 52.965756932399344, 'ACC-20': 54.094929198804984, 'ACC-21': 50.741914234804256, 'ACC-22': 52.40389310379769, 'ACC-23': 52.80374894282173, 'ACC-24': 51.55101728882604, 'ACC-25': 52.189201925274496, 'ACC-26': 47.85811781144968, 'ACC-27': 48.54133097173269, 'ACC-28': 55.05896539450137, 'ACC-29': 52.76122307553054, 'ACC-30': 52.26377145760427, 'ACC-31': 51.940134704510776, 'ACC-32': 54.758958301719694, 'ACC-33': 53.46121658787509, 'ACC-34': 49.84076477141728, 'ACC-35': 49.037617676620386, 'ACC-36': 51.708977085467545, 'ACC-37': 49.48357169524738, 'ACC-38': 50.521734357753886, 'ACC-39': 51.084666909416455, 'ACC-40': 48.611912330132775, 'ACC-41': 45.75888956358245, 'ACC-42': 49.287834070211524, 'ACC-43': 45.67940118338637, 'ACC-44': 45.082028688888975, 'ACC-45': 46.16742562569232, 'ACC-46': 44.788623891720384, 'ACC-47': 47.21400972248451, 'ACC-48': 45.244850060759276, 'ACC-49': 43.24411423798701, 'ACC-50': 46.52859021642659, 'ACC-51': 40.9075748196754, 'ACC-52': 43.058776002525256, 'ACC-53': 40.62661500289908, 'ACC-54': 43.39769521090001, 'ACC-55': 44.137949725366646, 'ACC-56': 42.509329818278495, 'ACC-57': 42.92542234384328, 'ACC-58': 40.1839348543624, 'ACC-59': 36.000956579994224, 'ACC-60': 36.95565960555934, 'ACC-61': 37.34274167699475, 'ACC-62': 34.36692748732407, 'ACC-63': 38.50996322677657, 'ACC-64': 34.13492576008864, 'ACC-65': 32.973563804505396, 'ACC-66': 33.427591473697596, 'ACC-67': 30.88003304453924, 'ACC-68': 33.921183187435204, 'ACC-69': 30.734435055294227, 'ACC-70': 30.33728081861887, 'ACC-71': 29.11284867416025, 'ACC-72': 28.79367578157365, 'ACC-73': 29.282532378018082, 'ACC-74': 27.012292752938986, 'ACC-75': 33.320957172294726, 'ACC-76': 32.42615414399725, 'ACC-77': 21.122814272044053, 'ACC-78': 28.975460831767702, 'ACC-79': 22.91283722350738, 'ACC-80': 27.897529102153868, 'ACC-81': 17.707854810491106, 'ACC-82': 26.804105500711184, 'ACC-83': 26.158189594103735, 'ACC-84': 23.49953436611696, 'ACC-85': 27.44384621758454, 'ACC-86': 20.58597629413721, 'ACC-87': 24.507824788169394, 'ACC-88': 24.816268469615704, 'ACC-89': 30.745723017896946, 'ACC-90': 22.159902122009832, 'ACC-91': 26.888981360499304, 'ACC-92': 19.8646436301467, 'ACC-93': 25.299035299667306, 'ACC-94': 23.72413946201323, 'ACC-95': 29.84960761043728, 'ACC-96': 21.337251487084895, 'ACC-97': 23.664911265164584, 'ACC-98': 24.85879276046952, 'ACC-99': 27.46136051409781, 'ACC-100': 21.56885999803754, 'ACC-101': 20.493112620814998, 'ACC-102': 21.897032170367016, 'ACC-103': 19.955275337678284, 'ACC-104': 23.010894960773225, 'ACC-105': 17.438260924196875, 'ACC-106': 19.432065297911098, 'ACC-107': 21.007907653747242, 'ACC-108': 17.271095976234978, 'ACC-109': 21.014510626053358, 'ACC-110': 18.744725125400123, 'ACC-111': 18.10471778797141, 'ACC-112': 18.021819919268033, 'ACC-113': 21.022781456494336, 'ACC-114': 16.4891215260075, 'ACC-115': 23.84774405003533, 'ACC-116': 13.879106920236255, 'ACC-117': 14.916497030121503, 'ACC-118': 18.562856652210588, 'ACC-119': 14.091219654674148, 'ACC-120': 15.18824430646187, 'ACC-121': 14.711847193007715, 'ACC-122': 12.921962127430723, 'ACC-123': 12.342031977484314, 'ACC-124': 10.217457664771368, 'ACC-125': 10.994460294791066, 'ACC-126': 13.808185866902248, 'ACC-127': 9.32275556261623, 'ACC-128': 8.052533812421407, 'ACC-129': 14.34217371305361, 'ACC-130': 9.786904445585243, 'ACC-131': 8.870262967090735, 'ACC-132': 14.475441753542908, 'ACC-133': 10.49977205767281, 'ACC-134': 13.369478672985782, 'ACC-135': 8.974331566045265, 'ACC-136': 10.546438222139397, 'ACC-137': 10.44872719557142, 'ACC-138': 9.062555524532325, 'ACC-139': 9.913318516125806, 'ACC-140': 4.389857658024772, 'ACC-141': 7.78765575059035, 'ACC-142': 7.735832414168328, 'ACC-143': 9.362512389327252, 'ACC-144': 13.203429602888086, 'ACC-145': 3.4014177418994165, 'ACC-146': 5.768294075986383, 'ACC-147': 4.214458352638807, 'ACC-148': 8.642831953964208, 'ACC-149': 4.784147965304659, 'ACC-150': 7.979078787044941, 'ACC-151': 4.666609114734041, 'ACC-152': 6.128300121383775, 'ACC-153': 7.730880924897853, 'ACC-154': 7.360036258302833, 'ACC-155': 5.162289881009607, 'ACC-156': 3.9159184781554806, 'ACC-157': 2.4648409694405995, 'ACC-158': 5.620343489741741, 'ACC-159': 2.836225888677496, 'ACC-160': 4.030561462244944, 'ACC-161': 6.732532570202513, 'ACC-162': 2.5295967581173513, 'ACC-163': 2.0071327352189883, 'ACC-164': 3.7634885955371495, 'ACC-165': 4.731646269220874, 'ACC-166': 2.3265145092377297, 'ACC-167': 0.7905612098661206, 'ACC-168': 3.800548561783529, 'ACC-169': 2.6361088835974473, 'ACC-170': 1.9552933827391854, 'ACC-171': 0.9527763133967294, 'ACC-172': 13.667444530873802, 'ACC-173': 3.613882063419792, 'ACC-174': 0.4061653627994281, 'ACC-175': 1.7079605604950459, 'ACC-176': 0.4814110749941955, 'ACC-177': 2.574053876490443, 'ACC-178': 0.07088897087371139, 'ACC-179': 0.5798559895796349, 'ACC-180': 1.5260421958075112, 'ACC-181': 3.0797217307268685, 'ACC-182': 1.2577916031926917, 'ACC-183': 12.907091448872398, 'ACC-184': 3.6798052908256036, 'ACC-185': 4.460503766935041, 'ACC-186': 4.191089508584486, 'ACC-187': 3.994491356503461, 'ACC-188': 3.015162826358904, 'ACC-189': 2.4915216381145044, 'ACC-190': 0.6560675770564994, 'ACC-191': 0.6123327895595433})])
[01/18 02:08:15] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 02:08:15] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 02:08:15] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 02:08:15] d2.evaluation.testing INFO: copypaste: 2.6326,16.4250,35.8934,25.8635,49.7046
[01/18 02:08:15] d2.utils.events INFO:  eta: 9:35:23  iter: 15999  total_loss: 38.78  loss_ce: 0.3071  loss_mask: 0.4151  loss_dice: 3.069  loss_ce_0: 0.5894  loss_mask_0: 0.3993  loss_dice_0: 3.225  loss_ce_1: 0.3474  loss_mask_1: 0.4146  loss_dice_1: 3.106  loss_ce_2: 0.3388  loss_mask_2: 0.4152  loss_dice_2: 3.079  loss_ce_3: 0.323  loss_mask_3: 0.4114  loss_dice_3: 3.07  loss_ce_4: 0.3086  loss_mask_4: 0.4138  loss_dice_4: 3.065  loss_ce_5: 0.3102  loss_mask_5: 0.4126  loss_dice_5: 3.07  loss_ce_6: 0.3053  loss_mask_6: 0.4123  loss_dice_6: 3.065  loss_ce_7: 0.3009  loss_mask_7: 0.4123  loss_dice_7: 3.072  loss_ce_8: 0.3004  loss_mask_8: 0.414  loss_dice_8: 3.08  time: 1.4971  data_time: 0.0746  lr: 6.3147e-06  max_mem: 21589M
[01/18 02:08:45] d2.utils.events INFO:  eta: 9:34:54  iter: 16019  total_loss: 38.07  loss_ce: 0.3007  loss_mask: 0.4064  loss_dice: 3.023  loss_ce_0: 0.5603  loss_mask_0: 0.3987  loss_dice_0: 3.161  loss_ce_1: 0.3328  loss_mask_1: 0.4155  loss_dice_1: 3.063  loss_ce_2: 0.3409  loss_mask_2: 0.4102  loss_dice_2: 3.041  loss_ce_3: 0.3236  loss_mask_3: 0.4072  loss_dice_3: 3.029  loss_ce_4: 0.3056  loss_mask_4: 0.4064  loss_dice_4: 3.032  loss_ce_5: 0.3077  loss_mask_5: 0.4061  loss_dice_5: 3.035  loss_ce_6: 0.3096  loss_mask_6: 0.4081  loss_dice_6: 3.021  loss_ce_7: 0.3081  loss_mask_7: 0.4066  loss_dice_7: 3.025  loss_ce_8: 0.3007  loss_mask_8: 0.4074  loss_dice_8: 3.029  time: 1.4971  data_time: 0.0682  lr: 6.31e-06  max_mem: 21589M
[01/18 02:09:14] d2.utils.events INFO:  eta: 9:34:33  iter: 16039  total_loss: 38.55  loss_ce: 0.3261  loss_mask: 0.4107  loss_dice: 3.058  loss_ce_0: 0.5912  loss_mask_0: 0.3971  loss_dice_0: 3.189  loss_ce_1: 0.3679  loss_mask_1: 0.4154  loss_dice_1: 3.103  loss_ce_2: 0.3402  loss_mask_2: 0.4157  loss_dice_2: 3.077  loss_ce_3: 0.3373  loss_mask_3: 0.4137  loss_dice_3: 3.063  loss_ce_4: 0.3255  loss_mask_4: 0.4129  loss_dice_4: 3.067  loss_ce_5: 0.3188  loss_mask_5: 0.4117  loss_dice_5: 3.06  loss_ce_6: 0.3091  loss_mask_6: 0.411  loss_dice_6: 3.059  loss_ce_7: 0.3148  loss_mask_7: 0.4101  loss_dice_7: 3.062  loss_ce_8: 0.3334  loss_mask_8: 0.4116  loss_dice_8: 3.066  time: 1.4970  data_time: 0.0801  lr: 6.3052e-06  max_mem: 21589M
[01/18 02:09:44] d2.utils.events INFO:  eta: 9:34:13  iter: 16059  total_loss: 38.4  loss_ce: 0.3118  loss_mask: 0.4097  loss_dice: 3.048  loss_ce_0: 0.5788  loss_mask_0: 0.3997  loss_dice_0: 3.193  loss_ce_1: 0.328  loss_mask_1: 0.4131  loss_dice_1: 3.097  loss_ce_2: 0.3342  loss_mask_2: 0.4122  loss_dice_2: 3.077  loss_ce_3: 0.3215  loss_mask_3: 0.4114  loss_dice_3: 3.065  loss_ce_4: 0.3153  loss_mask_4: 0.4101  loss_dice_4: 3.065  loss_ce_5: 0.3038  loss_mask_5: 0.4098  loss_dice_5: 3.062  loss_ce_6: 0.3073  loss_mask_6: 0.4088  loss_dice_6: 3.055  loss_ce_7: 0.3106  loss_mask_7: 0.4109  loss_dice_7: 3.053  loss_ce_8: 0.3032  loss_mask_8: 0.4099  loss_dice_8: 3.058  time: 1.4970  data_time: 0.0735  lr: 6.3005e-06  max_mem: 21589M
[01/18 02:10:13] d2.utils.events INFO:  eta: 9:33:43  iter: 16079  total_loss: 38.77  loss_ce: 0.3459  loss_mask: 0.4087  loss_dice: 3.071  loss_ce_0: 0.5966  loss_mask_0: 0.4034  loss_dice_0: 3.195  loss_ce_1: 0.3855  loss_mask_1: 0.4202  loss_dice_1: 3.102  loss_ce_2: 0.376  loss_mask_2: 0.4123  loss_dice_2: 3.07  loss_ce_3: 0.3615  loss_mask_3: 0.4096  loss_dice_3: 3.073  loss_ce_4: 0.3486  loss_mask_4: 0.4077  loss_dice_4: 3.068  loss_ce_5: 0.343  loss_mask_5: 0.4087  loss_dice_5: 3.061  loss_ce_6: 0.3451  loss_mask_6: 0.4075  loss_dice_6: 3.071  loss_ce_7: 0.3342  loss_mask_7: 0.407  loss_dice_7: 3.069  loss_ce_8: 0.3386  loss_mask_8: 0.4074  loss_dice_8: 3.08  time: 1.4970  data_time: 0.0645  lr: 6.2957e-06  max_mem: 21589M
[01/18 02:10:43] d2.utils.events INFO:  eta: 9:33:27  iter: 16099  total_loss: 38.99  loss_ce: 0.3249  loss_mask: 0.4162  loss_dice: 3.114  loss_ce_0: 0.5634  loss_mask_0: 0.4116  loss_dice_0: 3.224  loss_ce_1: 0.3632  loss_mask_1: 0.4212  loss_dice_1: 3.138  loss_ce_2: 0.3515  loss_mask_2: 0.4189  loss_dice_2: 3.125  loss_ce_3: 0.348  loss_mask_3: 0.4164  loss_dice_3: 3.112  loss_ce_4: 0.329  loss_mask_4: 0.4154  loss_dice_4: 3.11  loss_ce_5: 0.3245  loss_mask_5: 0.4132  loss_dice_5: 3.112  loss_ce_6: 0.307  loss_mask_6: 0.4127  loss_dice_6: 3.112  loss_ce_7: 0.3219  loss_mask_7: 0.4146  loss_dice_7: 3.114  loss_ce_8: 0.3075  loss_mask_8: 0.4155  loss_dice_8: 3.108  time: 1.4970  data_time: 0.0760  lr: 6.291e-06  max_mem: 21589M
[01/18 02:11:13] d2.utils.events INFO:  eta: 9:33:21  iter: 16119  total_loss: 39.08  loss_ce: 0.3311  loss_mask: 0.4074  loss_dice: 3.093  loss_ce_0: 0.5842  loss_mask_0: 0.3989  loss_dice_0: 3.223  loss_ce_1: 0.3892  loss_mask_1: 0.4091  loss_dice_1: 3.129  loss_ce_2: 0.372  loss_mask_2: 0.4067  loss_dice_2: 3.11  loss_ce_3: 0.3551  loss_mask_3: 0.4095  loss_dice_3: 3.09  loss_ce_4: 0.329  loss_mask_4: 0.4098  loss_dice_4: 3.094  loss_ce_5: 0.3358  loss_mask_5: 0.4066  loss_dice_5: 3.088  loss_ce_6: 0.3347  loss_mask_6: 0.4063  loss_dice_6: 3.09  loss_ce_7: 0.3173  loss_mask_7: 0.4076  loss_dice_7: 3.086  loss_ce_8: 0.3212  loss_mask_8: 0.4094  loss_dice_8: 3.087  time: 1.4969  data_time: 0.0759  lr: 6.2863e-06  max_mem: 21589M
[01/18 02:11:43] d2.utils.events INFO:  eta: 9:33:32  iter: 16139  total_loss: 37.93  loss_ce: 0.3157  loss_mask: 0.4124  loss_dice: 3.023  loss_ce_0: 0.5515  loss_mask_0: 0.3942  loss_dice_0: 3.149  loss_ce_1: 0.3393  loss_mask_1: 0.4125  loss_dice_1: 3.069  loss_ce_2: 0.3452  loss_mask_2: 0.4122  loss_dice_2: 3.048  loss_ce_3: 0.3248  loss_mask_3: 0.412  loss_dice_3: 3.045  loss_ce_4: 0.3104  loss_mask_4: 0.4133  loss_dice_4: 3.027  loss_ce_5: 0.3191  loss_mask_5: 0.4101  loss_dice_5: 3.025  loss_ce_6: 0.3117  loss_mask_6: 0.4112  loss_dice_6: 3.03  loss_ce_7: 0.312  loss_mask_7: 0.4123  loss_dice_7: 3.032  loss_ce_8: 0.3201  loss_mask_8: 0.4106  loss_dice_8: 3.029  time: 1.4969  data_time: 0.0751  lr: 6.2815e-06  max_mem: 21589M
[01/18 02:12:13] d2.utils.events INFO:  eta: 9:33:32  iter: 16159  total_loss: 38.89  loss_ce: 0.3276  loss_mask: 0.4189  loss_dice: 3.075  loss_ce_0: 0.626  loss_mask_0: 0.4091  loss_dice_0: 3.197  loss_ce_1: 0.3601  loss_mask_1: 0.4253  loss_dice_1: 3.113  loss_ce_2: 0.3576  loss_mask_2: 0.4237  loss_dice_2: 3.084  loss_ce_3: 0.3292  loss_mask_3: 0.423  loss_dice_3: 3.077  loss_ce_4: 0.3263  loss_mask_4: 0.4219  loss_dice_4: 3.071  loss_ce_5: 0.3263  loss_mask_5: 0.4199  loss_dice_5: 3.071  loss_ce_6: 0.3271  loss_mask_6: 0.4182  loss_dice_6: 3.074  loss_ce_7: 0.3224  loss_mask_7: 0.4157  loss_dice_7: 3.081  loss_ce_8: 0.3159  loss_mask_8: 0.4183  loss_dice_8: 3.075  time: 1.4969  data_time: 0.0805  lr: 6.2768e-06  max_mem: 21589M
[01/18 02:12:43] d2.utils.events INFO:  eta: 9:33:18  iter: 16179  total_loss: 38.41  loss_ce: 0.3068  loss_mask: 0.412  loss_dice: 3.072  loss_ce_0: 0.5566  loss_mask_0: 0.4013  loss_dice_0: 3.208  loss_ce_1: 0.3382  loss_mask_1: 0.4128  loss_dice_1: 3.112  loss_ce_2: 0.3322  loss_mask_2: 0.4124  loss_dice_2: 3.091  loss_ce_3: 0.3206  loss_mask_3: 0.4091  loss_dice_3: 3.072  loss_ce_4: 0.2941  loss_mask_4: 0.4096  loss_dice_4: 3.067  loss_ce_5: 0.2988  loss_mask_5: 0.4113  loss_dice_5: 3.076  loss_ce_6: 0.2945  loss_mask_6: 0.4116  loss_dice_6: 3.074  loss_ce_7: 0.2932  loss_mask_7: 0.4115  loss_dice_7: 3.068  loss_ce_8: 0.304  loss_mask_8: 0.4108  loss_dice_8: 3.068  time: 1.4970  data_time: 0.0760  lr: 6.2721e-06  max_mem: 21589M
[01/18 02:13:13] d2.utils.events INFO:  eta: 9:33:09  iter: 16199  total_loss: 38.89  loss_ce: 0.3239  loss_mask: 0.4193  loss_dice: 3.103  loss_ce_0: 0.6095  loss_mask_0: 0.4036  loss_dice_0: 3.234  loss_ce_1: 0.3575  loss_mask_1: 0.422  loss_dice_1: 3.137  loss_ce_2: 0.3352  loss_mask_2: 0.4209  loss_dice_2: 3.128  loss_ce_3: 0.3252  loss_mask_3: 0.4181  loss_dice_3: 3.11  loss_ce_4: 0.3207  loss_mask_4: 0.4183  loss_dice_4: 3.114  loss_ce_5: 0.3365  loss_mask_5: 0.4196  loss_dice_5: 3.113  loss_ce_6: 0.3247  loss_mask_6: 0.4199  loss_dice_6: 3.104  loss_ce_7: 0.3165  loss_mask_7: 0.4195  loss_dice_7: 3.103  loss_ce_8: 0.3343  loss_mask_8: 0.419  loss_dice_8: 3.104  time: 1.4970  data_time: 0.0795  lr: 6.2673e-06  max_mem: 21589M
[01/18 02:13:43] d2.utils.events INFO:  eta: 9:33:27  iter: 16219  total_loss: 38.15  loss_ce: 0.3082  loss_mask: 0.4104  loss_dice: 3.037  loss_ce_0: 0.5946  loss_mask_0: 0.4055  loss_dice_0: 3.169  loss_ce_1: 0.3499  loss_mask_1: 0.4184  loss_dice_1: 3.071  loss_ce_2: 0.3531  loss_mask_2: 0.4131  loss_dice_2: 3.053  loss_ce_3: 0.3443  loss_mask_3: 0.4112  loss_dice_3: 3.039  loss_ce_4: 0.3316  loss_mask_4: 0.4111  loss_dice_4: 3.043  loss_ce_5: 0.3239  loss_mask_5: 0.4108  loss_dice_5: 3.04  loss_ce_6: 0.3277  loss_mask_6: 0.4108  loss_dice_6: 3.029  loss_ce_7: 0.3068  loss_mask_7: 0.412  loss_dice_7: 3.044  loss_ce_8: 0.3259  loss_mask_8: 0.4103  loss_dice_8: 3.031  time: 1.4970  data_time: 0.0833  lr: 6.2626e-06  max_mem: 21589M
[01/18 02:14:13] d2.utils.events INFO:  eta: 9:33:46  iter: 16239  total_loss: 38.51  loss_ce: 0.3272  loss_mask: 0.407  loss_dice: 3.049  loss_ce_0: 0.5733  loss_mask_0: 0.3993  loss_dice_0: 3.191  loss_ce_1: 0.3556  loss_mask_1: 0.4098  loss_dice_1: 3.091  loss_ce_2: 0.3562  loss_mask_2: 0.407  loss_dice_2: 3.071  loss_ce_3: 0.3336  loss_mask_3: 0.4068  loss_dice_3: 3.053  loss_ce_4: 0.3318  loss_mask_4: 0.4062  loss_dice_4: 3.043  loss_ce_5: 0.3235  loss_mask_5: 0.4067  loss_dice_5: 3.052  loss_ce_6: 0.3233  loss_mask_6: 0.4059  loss_dice_6: 3.05  loss_ce_7: 0.332  loss_mask_7: 0.4029  loss_dice_7: 3.052  loss_ce_8: 0.3278  loss_mask_8: 0.4065  loss_dice_8: 3.046  time: 1.4970  data_time: 0.0736  lr: 6.2578e-06  max_mem: 21589M
[01/18 02:14:44] d2.utils.events INFO:  eta: 9:33:50  iter: 16259  total_loss: 38.76  loss_ce: 0.328  loss_mask: 0.405  loss_dice: 3.088  loss_ce_0: 0.5765  loss_mask_0: 0.3996  loss_dice_0: 3.216  loss_ce_1: 0.3495  loss_mask_1: 0.4124  loss_dice_1: 3.133  loss_ce_2: 0.3476  loss_mask_2: 0.408  loss_dice_2: 3.116  loss_ce_3: 0.3543  loss_mask_3: 0.4054  loss_dice_3: 3.095  loss_ce_4: 0.3377  loss_mask_4: 0.4054  loss_dice_4: 3.101  loss_ce_5: 0.3263  loss_mask_5: 0.4031  loss_dice_5: 3.104  loss_ce_6: 0.3306  loss_mask_6: 0.4034  loss_dice_6: 3.095  loss_ce_7: 0.3228  loss_mask_7: 0.4039  loss_dice_7: 3.087  loss_ce_8: 0.3423  loss_mask_8: 0.4051  loss_dice_8: 3.089  time: 1.4970  data_time: 0.0775  lr: 6.2531e-06  max_mem: 21589M
[01/18 02:15:14] d2.utils.events INFO:  eta: 9:33:35  iter: 16279  total_loss: 38.34  loss_ce: 0.3205  loss_mask: 0.4194  loss_dice: 3.066  loss_ce_0: 0.5709  loss_mask_0: 0.4063  loss_dice_0: 3.185  loss_ce_1: 0.3418  loss_mask_1: 0.4219  loss_dice_1: 3.099  loss_ce_2: 0.348  loss_mask_2: 0.4189  loss_dice_2: 3.082  loss_ce_3: 0.333  loss_mask_3: 0.4191  loss_dice_3: 3.073  loss_ce_4: 0.3215  loss_mask_4: 0.418  loss_dice_4: 3.066  loss_ce_5: 0.3287  loss_mask_5: 0.4181  loss_dice_5: 3.075  loss_ce_6: 0.3227  loss_mask_6: 0.4195  loss_dice_6: 3.069  loss_ce_7: 0.3216  loss_mask_7: 0.4191  loss_dice_7: 3.069  loss_ce_8: 0.3095  loss_mask_8: 0.4187  loss_dice_8: 3.072  time: 1.4970  data_time: 0.0747  lr: 6.2484e-06  max_mem: 21589M
[01/18 02:15:45] d2.utils.events INFO:  eta: 9:33:20  iter: 16299  total_loss: 37.97  loss_ce: 0.3142  loss_mask: 0.4122  loss_dice: 3.035  loss_ce_0: 0.5927  loss_mask_0: 0.4039  loss_dice_0: 3.175  loss_ce_1: 0.3413  loss_mask_1: 0.4184  loss_dice_1: 3.087  loss_ce_2: 0.3567  loss_mask_2: 0.4139  loss_dice_2: 3.048  loss_ce_3: 0.3251  loss_mask_3: 0.4148  loss_dice_3: 3.035  loss_ce_4: 0.3116  loss_mask_4: 0.4134  loss_dice_4: 3.036  loss_ce_5: 0.3287  loss_mask_5: 0.4142  loss_dice_5: 3.039  loss_ce_6: 0.3138  loss_mask_6: 0.414  loss_dice_6: 3.033  loss_ce_7: 0.3114  loss_mask_7: 0.4132  loss_dice_7: 3.035  loss_ce_8: 0.3113  loss_mask_8: 0.4103  loss_dice_8: 3.038  time: 1.4971  data_time: 0.0885  lr: 6.2436e-06  max_mem: 21589M
[01/18 02:16:14] d2.utils.events INFO:  eta: 9:34:01  iter: 16319  total_loss: 38.37  loss_ce: 0.2975  loss_mask: 0.399  loss_dice: 3.05  loss_ce_0: 0.6072  loss_mask_0: 0.3904  loss_dice_0: 3.195  loss_ce_1: 0.3442  loss_mask_1: 0.4056  loss_dice_1: 3.107  loss_ce_2: 0.3345  loss_mask_2: 0.4005  loss_dice_2: 3.076  loss_ce_3: 0.3096  loss_mask_3: 0.398  loss_dice_3: 3.073  loss_ce_4: 0.3104  loss_mask_4: 0.3958  loss_dice_4: 3.063  loss_ce_5: 0.2973  loss_mask_5: 0.3972  loss_dice_5: 3.06  loss_ce_6: 0.304  loss_mask_6: 0.3959  loss_dice_6: 3.06  loss_ce_7: 0.2949  loss_mask_7: 0.3983  loss_dice_7: 3.051  loss_ce_8: 0.3022  loss_mask_8: 0.3991  loss_dice_8: 3.053  time: 1.4971  data_time: 0.0756  lr: 6.2389e-06  max_mem: 21589M
[01/18 02:16:45] d2.utils.events INFO:  eta: 9:34:19  iter: 16339  total_loss: 38.54  loss_ce: 0.3164  loss_mask: 0.409  loss_dice: 3.051  loss_ce_0: 0.6267  loss_mask_0: 0.4006  loss_dice_0: 3.205  loss_ce_1: 0.3601  loss_mask_1: 0.4175  loss_dice_1: 3.093  loss_ce_2: 0.3452  loss_mask_2: 0.412  loss_dice_2: 3.072  loss_ce_3: 0.3271  loss_mask_3: 0.4105  loss_dice_3: 3.069  loss_ce_4: 0.3289  loss_mask_4: 0.4084  loss_dice_4: 3.058  loss_ce_5: 0.31  loss_mask_5: 0.405  loss_dice_5: 3.052  loss_ce_6: 0.329  loss_mask_6: 0.4067  loss_dice_6: 3.05  loss_ce_7: 0.3125  loss_mask_7: 0.4061  loss_dice_7: 3.06  loss_ce_8: 0.3128  loss_mask_8: 0.4071  loss_dice_8: 3.057  time: 1.4971  data_time: 0.0751  lr: 6.2341e-06  max_mem: 21589M
[01/18 02:17:15] d2.utils.events INFO:  eta: 9:34:27  iter: 16359  total_loss: 38.4  loss_ce: 0.3028  loss_mask: 0.4175  loss_dice: 3.052  loss_ce_0: 0.575  loss_mask_0: 0.4121  loss_dice_0: 3.176  loss_ce_1: 0.3404  loss_mask_1: 0.423  loss_dice_1: 3.092  loss_ce_2: 0.3471  loss_mask_2: 0.42  loss_dice_2: 3.071  loss_ce_3: 0.32  loss_mask_3: 0.4195  loss_dice_3: 3.054  loss_ce_4: 0.3132  loss_mask_4: 0.4204  loss_dice_4: 3.06  loss_ce_5: 0.3003  loss_mask_5: 0.42  loss_dice_5: 3.059  loss_ce_6: 0.3071  loss_mask_6: 0.4203  loss_dice_6: 3.054  loss_ce_7: 0.3069  loss_mask_7: 0.4201  loss_dice_7: 3.045  loss_ce_8: 0.2951  loss_mask_8: 0.4187  loss_dice_8: 3.047  time: 1.4971  data_time: 0.0696  lr: 6.2294e-06  max_mem: 21589M
[01/18 02:17:45] d2.utils.events INFO:  eta: 9:34:42  iter: 16379  total_loss: 38.66  loss_ce: 0.3017  loss_mask: 0.4153  loss_dice: 3.084  loss_ce_0: 0.5682  loss_mask_0: 0.4087  loss_dice_0: 3.195  loss_ce_1: 0.3237  loss_mask_1: 0.4215  loss_dice_1: 3.111  loss_ce_2: 0.3385  loss_mask_2: 0.4183  loss_dice_2: 3.091  loss_ce_3: 0.3142  loss_mask_3: 0.4174  loss_dice_3: 3.082  loss_ce_4: 0.3113  loss_mask_4: 0.4179  loss_dice_4: 3.085  loss_ce_5: 0.2982  loss_mask_5: 0.4167  loss_dice_5: 3.091  loss_ce_6: 0.2987  loss_mask_6: 0.4178  loss_dice_6: 3.089  loss_ce_7: 0.2876  loss_mask_7: 0.4157  loss_dice_7: 3.09  loss_ce_8: 0.2914  loss_mask_8: 0.4156  loss_dice_8: 3.083  time: 1.4971  data_time: 0.0764  lr: 6.2246e-06  max_mem: 21589M
[01/18 02:18:16] d2.utils.events INFO:  eta: 9:35:01  iter: 16399  total_loss: 38.36  loss_ce: 0.3327  loss_mask: 0.4063  loss_dice: 3.068  loss_ce_0: 0.5678  loss_mask_0: 0.3922  loss_dice_0: 3.2  loss_ce_1: 0.3531  loss_mask_1: 0.4077  loss_dice_1: 3.1  loss_ce_2: 0.3478  loss_mask_2: 0.4068  loss_dice_2: 3.08  loss_ce_3: 0.3366  loss_mask_3: 0.4067  loss_dice_3: 3.068  loss_ce_4: 0.3249  loss_mask_4: 0.404  loss_dice_4: 3.062  loss_ce_5: 0.3271  loss_mask_5: 0.4041  loss_dice_5: 3.062  loss_ce_6: 0.3101  loss_mask_6: 0.4021  loss_dice_6: 3.061  loss_ce_7: 0.3315  loss_mask_7: 0.4031  loss_dice_7: 3.058  loss_ce_8: 0.327  loss_mask_8: 0.4041  loss_dice_8: 3.064  time: 1.4971  data_time: 0.0786  lr: 6.2199e-06  max_mem: 21589M
[01/18 02:18:48] d2.utils.events INFO:  eta: 9:35:37  iter: 16419  total_loss: 38.53  loss_ce: 0.3202  loss_mask: 0.4041  loss_dice: 3.067  loss_ce_0: 0.561  loss_mask_0: 0.3949  loss_dice_0: 3.216  loss_ce_1: 0.3464  loss_mask_1: 0.4057  loss_dice_1: 3.116  loss_ce_2: 0.3338  loss_mask_2: 0.4029  loss_dice_2: 3.092  loss_ce_3: 0.3234  loss_mask_3: 0.4022  loss_dice_3: 3.075  loss_ce_4: 0.3219  loss_mask_4: 0.4042  loss_dice_4: 3.073  loss_ce_5: 0.3082  loss_mask_5: 0.4049  loss_dice_5: 3.073  loss_ce_6: 0.3156  loss_mask_6: 0.4051  loss_dice_6: 3.068  loss_ce_7: 0.3158  loss_mask_7: 0.4055  loss_dice_7: 3.069  loss_ce_8: 0.3138  loss_mask_8: 0.4042  loss_dice_8: 3.066  time: 1.4972  data_time: 0.1021  lr: 6.2152e-06  max_mem: 21589M
[01/18 02:19:18] d2.utils.events INFO:  eta: 9:35:32  iter: 16439  total_loss: 38.58  loss_ce: 0.3233  loss_mask: 0.4259  loss_dice: 3.087  loss_ce_0: 0.5884  loss_mask_0: 0.4128  loss_dice_0: 3.208  loss_ce_1: 0.3248  loss_mask_1: 0.4333  loss_dice_1: 3.12  loss_ce_2: 0.3394  loss_mask_2: 0.4316  loss_dice_2: 3.094  loss_ce_3: 0.3135  loss_mask_3: 0.43  loss_dice_3: 3.086  loss_ce_4: 0.3074  loss_mask_4: 0.4276  loss_dice_4: 3.075  loss_ce_5: 0.309  loss_mask_5: 0.4267  loss_dice_5: 3.08  loss_ce_6: 0.3204  loss_mask_6: 0.4255  loss_dice_6: 3.079  loss_ce_7: 0.3116  loss_mask_7: 0.4254  loss_dice_7: 3.084  loss_ce_8: 0.3102  loss_mask_8: 0.4255  loss_dice_8: 3.086  time: 1.4973  data_time: 0.0803  lr: 6.2104e-06  max_mem: 21589M
[01/18 02:19:49] d2.utils.events INFO:  eta: 9:35:34  iter: 16459  total_loss: 38.76  loss_ce: 0.3194  loss_mask: 0.419  loss_dice: 3.094  loss_ce_0: 0.6024  loss_mask_0: 0.4112  loss_dice_0: 3.204  loss_ce_1: 0.3358  loss_mask_1: 0.419  loss_dice_1: 3.134  loss_ce_2: 0.3444  loss_mask_2: 0.4188  loss_dice_2: 3.116  loss_ce_3: 0.3213  loss_mask_3: 0.4171  loss_dice_3: 3.094  loss_ce_4: 0.3158  loss_mask_4: 0.4145  loss_dice_4: 3.095  loss_ce_5: 0.3144  loss_mask_5: 0.4154  loss_dice_5: 3.104  loss_ce_6: 0.3279  loss_mask_6: 0.4173  loss_dice_6: 3.1  loss_ce_7: 0.3239  loss_mask_7: 0.4176  loss_dice_7: 3.107  loss_ce_8: 0.3245  loss_mask_8: 0.4187  loss_dice_8: 3.101  time: 1.4973  data_time: 0.0865  lr: 6.2057e-06  max_mem: 21589M
[01/18 02:20:20] d2.utils.events INFO:  eta: 9:35:23  iter: 16479  total_loss: 38.71  loss_ce: 0.3221  loss_mask: 0.4125  loss_dice: 3.076  loss_ce_0: 0.5668  loss_mask_0: 0.4078  loss_dice_0: 3.215  loss_ce_1: 0.3474  loss_mask_1: 0.419  loss_dice_1: 3.105  loss_ce_2: 0.3574  loss_mask_2: 0.4157  loss_dice_2: 3.094  loss_ce_3: 0.3302  loss_mask_3: 0.4138  loss_dice_3: 3.083  loss_ce_4: 0.3179  loss_mask_4: 0.4142  loss_dice_4: 3.075  loss_ce_5: 0.3241  loss_mask_5: 0.4139  loss_dice_5: 3.081  loss_ce_6: 0.3056  loss_mask_6: 0.414  loss_dice_6: 3.068  loss_ce_7: 0.3264  loss_mask_7: 0.412  loss_dice_7: 3.068  loss_ce_8: 0.3196  loss_mask_8: 0.412  loss_dice_8: 3.078  time: 1.4974  data_time: 0.0884  lr: 6.2009e-06  max_mem: 21589M
[01/18 02:20:51] d2.utils.events INFO:  eta: 9:36:18  iter: 16499  total_loss: 38.26  loss_ce: 0.3097  loss_mask: 0.4027  loss_dice: 3.065  loss_ce_0: 0.5778  loss_mask_0: 0.3862  loss_dice_0: 3.195  loss_ce_1: 0.3559  loss_mask_1: 0.4072  loss_dice_1: 3.109  loss_ce_2: 0.3414  loss_mask_2: 0.4034  loss_dice_2: 3.088  loss_ce_3: 0.3416  loss_mask_3: 0.4024  loss_dice_3: 3.074  loss_ce_4: 0.321  loss_mask_4: 0.4005  loss_dice_4: 3.074  loss_ce_5: 0.3269  loss_mask_5: 0.4017  loss_dice_5: 3.065  loss_ce_6: 0.311  loss_mask_6: 0.4029  loss_dice_6: 3.068  loss_ce_7: 0.3078  loss_mask_7: 0.4019  loss_dice_7: 3.061  loss_ce_8: 0.3029  loss_mask_8: 0.4039  loss_dice_8: 3.062  time: 1.4975  data_time: 0.0887  lr: 6.1962e-06  max_mem: 21589M
[01/18 02:21:21] d2.utils.events INFO:  eta: 9:36:18  iter: 16519  total_loss: 38.05  loss_ce: 0.2982  loss_mask: 0.4103  loss_dice: 3.031  loss_ce_0: 0.5526  loss_mask_0: 0.4033  loss_dice_0: 3.174  loss_ce_1: 0.33  loss_mask_1: 0.421  loss_dice_1: 3.071  loss_ce_2: 0.3243  loss_mask_2: 0.4155  loss_dice_2: 3.035  loss_ce_3: 0.321  loss_mask_3: 0.4125  loss_dice_3: 3.03  loss_ce_4: 0.3189  loss_mask_4: 0.4137  loss_dice_4: 3.023  loss_ce_5: 0.3123  loss_mask_5: 0.4117  loss_dice_5: 3.03  loss_ce_6: 0.2937  loss_mask_6: 0.4115  loss_dice_6: 3.023  loss_ce_7: 0.3014  loss_mask_7: 0.4128  loss_dice_7: 3.038  loss_ce_8: 0.2965  loss_mask_8: 0.4116  loss_dice_8: 3.032  time: 1.4974  data_time: 0.0766  lr: 6.1914e-06  max_mem: 21589M
[01/18 02:21:51] d2.utils.events INFO:  eta: 9:36:14  iter: 16539  total_loss: 38.34  loss_ce: 0.3148  loss_mask: 0.4092  loss_dice: 3.049  loss_ce_0: 0.558  loss_mask_0: 0.3979  loss_dice_0: 3.186  loss_ce_1: 0.3356  loss_mask_1: 0.4109  loss_dice_1: 3.093  loss_ce_2: 0.3226  loss_mask_2: 0.41  loss_dice_2: 3.078  loss_ce_3: 0.3271  loss_mask_3: 0.4092  loss_dice_3: 3.06  loss_ce_4: 0.3117  loss_mask_4: 0.4085  loss_dice_4: 3.057  loss_ce_5: 0.3191  loss_mask_5: 0.4076  loss_dice_5: 3.049  loss_ce_6: 0.3179  loss_mask_6: 0.4089  loss_dice_6: 3.051  loss_ce_7: 0.3011  loss_mask_7: 0.4084  loss_dice_7: 3.055  loss_ce_8: 0.3103  loss_mask_8: 0.4092  loss_dice_8: 3.048  time: 1.4974  data_time: 0.0651  lr: 6.1867e-06  max_mem: 21589M
[01/18 02:22:21] d2.utils.events INFO:  eta: 9:36:32  iter: 16559  total_loss: 39.72  loss_ce: 0.3363  loss_mask: 0.3995  loss_dice: 3.164  loss_ce_0: 0.5925  loss_mask_0: 0.3998  loss_dice_0: 3.286  loss_ce_1: 0.3739  loss_mask_1: 0.4116  loss_dice_1: 3.196  loss_ce_2: 0.3779  loss_mask_2: 0.4086  loss_dice_2: 3.183  loss_ce_3: 0.3669  loss_mask_3: 0.4025  loss_dice_3: 3.172  loss_ce_4: 0.3442  loss_mask_4: 0.4049  loss_dice_4: 3.158  loss_ce_5: 0.3377  loss_mask_5: 0.4028  loss_dice_5: 3.16  loss_ce_6: 0.3533  loss_mask_6: 0.4021  loss_dice_6: 3.148  loss_ce_7: 0.3393  loss_mask_7: 0.4011  loss_dice_7: 3.162  loss_ce_8: 0.3416  loss_mask_8: 0.402  loss_dice_8: 3.156  time: 1.4975  data_time: 0.0746  lr: 6.1819e-06  max_mem: 21589M
[01/18 02:22:52] d2.utils.events INFO:  eta: 9:36:50  iter: 16579  total_loss: 38.56  loss_ce: 0.3107  loss_mask: 0.402  loss_dice: 3.092  loss_ce_0: 0.6043  loss_mask_0: 0.3933  loss_dice_0: 3.226  loss_ce_1: 0.3562  loss_mask_1: 0.4089  loss_dice_1: 3.133  loss_ce_2: 0.3568  loss_mask_2: 0.4044  loss_dice_2: 3.118  loss_ce_3: 0.3266  loss_mask_3: 0.4043  loss_dice_3: 3.096  loss_ce_4: 0.3133  loss_mask_4: 0.4032  loss_dice_4: 3.095  loss_ce_5: 0.3248  loss_mask_5: 0.4023  loss_dice_5: 3.099  loss_ce_6: 0.3075  loss_mask_6: 0.4007  loss_dice_6: 3.097  loss_ce_7: 0.318  loss_mask_7: 0.3999  loss_dice_7: 3.093  loss_ce_8: 0.3253  loss_mask_8: 0.4016  loss_dice_8: 3.1  time: 1.4975  data_time: 0.0774  lr: 6.1772e-06  max_mem: 21589M
[01/18 02:23:22] d2.utils.events INFO:  eta: 9:36:40  iter: 16599  total_loss: 38.63  loss_ce: 0.3169  loss_mask: 0.3999  loss_dice: 3.086  loss_ce_0: 0.613  loss_mask_0: 0.3926  loss_dice_0: 3.21  loss_ce_1: 0.3507  loss_mask_1: 0.4019  loss_dice_1: 3.13  loss_ce_2: 0.3605  loss_mask_2: 0.4031  loss_dice_2: 3.105  loss_ce_3: 0.3454  loss_mask_3: 0.4001  loss_dice_3: 3.09  loss_ce_4: 0.3324  loss_mask_4: 0.4013  loss_dice_4: 3.099  loss_ce_5: 0.3458  loss_mask_5: 0.3998  loss_dice_5: 3.09  loss_ce_6: 0.3266  loss_mask_6: 0.4008  loss_dice_6: 3.086  loss_ce_7: 0.3286  loss_mask_7: 0.3992  loss_dice_7: 3.079  loss_ce_8: 0.3146  loss_mask_8: 0.3989  loss_dice_8: 3.089  time: 1.4975  data_time: 0.0824  lr: 6.1724e-06  max_mem: 21589M
[01/18 02:23:52] d2.utils.events INFO:  eta: 9:36:45  iter: 16619  total_loss: 38.01  loss_ce: 0.3112  loss_mask: 0.4118  loss_dice: 3.02  loss_ce_0: 0.6064  loss_mask_0: 0.4033  loss_dice_0: 3.152  loss_ce_1: 0.348  loss_mask_1: 0.4161  loss_dice_1: 3.055  loss_ce_2: 0.3493  loss_mask_2: 0.4123  loss_dice_2: 3.033  loss_ce_3: 0.3277  loss_mask_3: 0.41  loss_dice_3: 3.034  loss_ce_4: 0.3358  loss_mask_4: 0.4117  loss_dice_4: 3.027  loss_ce_5: 0.3252  loss_mask_5: 0.4115  loss_dice_5: 3.03  loss_ce_6: 0.3367  loss_mask_6: 0.4128  loss_dice_6: 3.027  loss_ce_7: 0.3338  loss_mask_7: 0.4132  loss_dice_7: 3.031  loss_ce_8: 0.3213  loss_mask_8: 0.4128  loss_dice_8: 3.033  time: 1.4975  data_time: 0.0708  lr: 6.1677e-06  max_mem: 21589M
[01/18 02:24:23] d2.utils.events INFO:  eta: 9:37:04  iter: 16639  total_loss: 38.98  loss_ce: 0.3105  loss_mask: 0.4042  loss_dice: 3.107  loss_ce_0: 0.5755  loss_mask_0: 0.402  loss_dice_0: 3.237  loss_ce_1: 0.3517  loss_mask_1: 0.4103  loss_dice_1: 3.152  loss_ce_2: 0.3378  loss_mask_2: 0.4082  loss_dice_2: 3.125  loss_ce_3: 0.3256  loss_mask_3: 0.4028  loss_dice_3: 3.107  loss_ce_4: 0.3218  loss_mask_4: 0.4046  loss_dice_4: 3.111  loss_ce_5: 0.3198  loss_mask_5: 0.4051  loss_dice_5: 3.113  loss_ce_6: 0.313  loss_mask_6: 0.4032  loss_dice_6: 3.109  loss_ce_7: 0.3119  loss_mask_7: 0.4043  loss_dice_7: 3.116  loss_ce_8: 0.308  loss_mask_8: 0.4054  loss_dice_8: 3.115  time: 1.4976  data_time: 0.0860  lr: 6.1629e-06  max_mem: 21589M
[01/18 02:24:53] d2.utils.events INFO:  eta: 9:37:07  iter: 16659  total_loss: 38.31  loss_ce: 0.3281  loss_mask: 0.4026  loss_dice: 3.062  loss_ce_0: 0.5699  loss_mask_0: 0.3958  loss_dice_0: 3.199  loss_ce_1: 0.3297  loss_mask_1: 0.4055  loss_dice_1: 3.099  loss_ce_2: 0.3566  loss_mask_2: 0.4056  loss_dice_2: 3.074  loss_ce_3: 0.3379  loss_mask_3: 0.4051  loss_dice_3: 3.061  loss_ce_4: 0.3459  loss_mask_4: 0.4039  loss_dice_4: 3.06  loss_ce_5: 0.3332  loss_mask_5: 0.4031  loss_dice_5: 3.063  loss_ce_6: 0.3386  loss_mask_6: 0.4039  loss_dice_6: 3.052  loss_ce_7: 0.3366  loss_mask_7: 0.4052  loss_dice_7: 3.056  loss_ce_8: 0.3196  loss_mask_8: 0.4046  loss_dice_8: 3.06  time: 1.4976  data_time: 0.0751  lr: 6.1582e-06  max_mem: 21589M
[01/18 02:25:23] d2.utils.events INFO:  eta: 9:37:07  iter: 16679  total_loss: 38.39  loss_ce: 0.3306  loss_mask: 0.4197  loss_dice: 3.008  loss_ce_0: 0.5769  loss_mask_0: 0.4092  loss_dice_0: 3.134  loss_ce_1: 0.3552  loss_mask_1: 0.4186  loss_dice_1: 3.048  loss_ce_2: 0.3585  loss_mask_2: 0.4197  loss_dice_2: 3.021  loss_ce_3: 0.3499  loss_mask_3: 0.4211  loss_dice_3: 3.006  loss_ce_4: 0.3391  loss_mask_4: 0.4213  loss_dice_4: 3.005  loss_ce_5: 0.3406  loss_mask_5: 0.4205  loss_dice_5: 3.014  loss_ce_6: 0.3363  loss_mask_6: 0.4206  loss_dice_6: 3.003  loss_ce_7: 0.329  loss_mask_7: 0.4212  loss_dice_7: 3.007  loss_ce_8: 0.3292  loss_mask_8: 0.4194  loss_dice_8: 3.007  time: 1.4976  data_time: 0.0828  lr: 6.1534e-06  max_mem: 21589M
[01/18 02:25:53] d2.utils.events INFO:  eta: 9:37:11  iter: 16699  total_loss: 38.87  loss_ce: 0.3153  loss_mask: 0.4174  loss_dice: 3.099  loss_ce_0: 0.5988  loss_mask_0: 0.4165  loss_dice_0: 3.218  loss_ce_1: 0.3565  loss_mask_1: 0.4251  loss_dice_1: 3.133  loss_ce_2: 0.3411  loss_mask_2: 0.4188  loss_dice_2: 3.114  loss_ce_3: 0.3263  loss_mask_3: 0.4159  loss_dice_3: 3.101  loss_ce_4: 0.3297  loss_mask_4: 0.417  loss_dice_4: 3.096  loss_ce_5: 0.3059  loss_mask_5: 0.4171  loss_dice_5: 3.105  loss_ce_6: 0.3142  loss_mask_6: 0.4166  loss_dice_6: 3.097  loss_ce_7: 0.3062  loss_mask_7: 0.4156  loss_dice_7: 3.097  loss_ce_8: 0.3181  loss_mask_8: 0.4195  loss_dice_8: 3.101  time: 1.4976  data_time: 0.0756  lr: 6.1487e-06  max_mem: 21589M
[01/18 02:26:23] d2.utils.events INFO:  eta: 9:37:05  iter: 16719  total_loss: 38.82  loss_ce: 0.321  loss_mask: 0.417  loss_dice: 3.093  loss_ce_0: 0.577  loss_mask_0: 0.4105  loss_dice_0: 3.209  loss_ce_1: 0.3598  loss_mask_1: 0.4282  loss_dice_1: 3.125  loss_ce_2: 0.3352  loss_mask_2: 0.421  loss_dice_2: 3.11  loss_ce_3: 0.3097  loss_mask_3: 0.4187  loss_dice_3: 3.095  loss_ce_4: 0.3297  loss_mask_4: 0.4175  loss_dice_4: 3.09  loss_ce_5: 0.3061  loss_mask_5: 0.4178  loss_dice_5: 3.089  loss_ce_6: 0.3157  loss_mask_6: 0.4175  loss_dice_6: 3.084  loss_ce_7: 0.3169  loss_mask_7: 0.4162  loss_dice_7: 3.094  loss_ce_8: 0.3287  loss_mask_8: 0.4152  loss_dice_8: 3.091  time: 1.4976  data_time: 0.0727  lr: 6.1439e-06  max_mem: 21589M
[01/18 02:26:54] d2.utils.events INFO:  eta: 9:37:11  iter: 16739  total_loss: 38.49  loss_ce: 0.3412  loss_mask: 0.398  loss_dice: 3.081  loss_ce_0: 0.6136  loss_mask_0: 0.395  loss_dice_0: 3.226  loss_ce_1: 0.3655  loss_mask_1: 0.4046  loss_dice_1: 3.128  loss_ce_2: 0.3717  loss_mask_2: 0.4019  loss_dice_2: 3.108  loss_ce_3: 0.3318  loss_mask_3: 0.4045  loss_dice_3: 3.086  loss_ce_4: 0.3395  loss_mask_4: 0.4018  loss_dice_4: 3.089  loss_ce_5: 0.3406  loss_mask_5: 0.3996  loss_dice_5: 3.087  loss_ce_6: 0.3387  loss_mask_6: 0.3995  loss_dice_6: 3.081  loss_ce_7: 0.3368  loss_mask_7: 0.3973  loss_dice_7: 3.077  loss_ce_8: 0.3219  loss_mask_8: 0.3984  loss_dice_8: 3.09  time: 1.4976  data_time: 0.0836  lr: 6.1392e-06  max_mem: 21589M
[01/18 02:27:24] d2.utils.events INFO:  eta: 9:37:22  iter: 16759  total_loss: 38.14  loss_ce: 0.3092  loss_mask: 0.401  loss_dice: 3.03  loss_ce_0: 0.5847  loss_mask_0: 0.3939  loss_dice_0: 3.166  loss_ce_1: 0.3491  loss_mask_1: 0.4031  loss_dice_1: 3.064  loss_ce_2: 0.3378  loss_mask_2: 0.4002  loss_dice_2: 3.048  loss_ce_3: 0.3232  loss_mask_3: 0.4003  loss_dice_3: 3.034  loss_ce_4: 0.3223  loss_mask_4: 0.4005  loss_dice_4: 3.03  loss_ce_5: 0.3097  loss_mask_5: 0.4015  loss_dice_5: 3.04  loss_ce_6: 0.3137  loss_mask_6: 0.4  loss_dice_6: 3.026  loss_ce_7: 0.3034  loss_mask_7: 0.4004  loss_dice_7: 3.037  loss_ce_8: 0.3055  loss_mask_8: 0.4011  loss_dice_8: 3.027  time: 1.4976  data_time: 0.0686  lr: 6.1344e-06  max_mem: 21589M
[01/18 02:27:54] d2.utils.events INFO:  eta: 9:36:59  iter: 16779  total_loss: 38.39  loss_ce: 0.3198  loss_mask: 0.3983  loss_dice: 3.036  loss_ce_0: 0.6041  loss_mask_0: 0.3959  loss_dice_0: 3.174  loss_ce_1: 0.3453  loss_mask_1: 0.4061  loss_dice_1: 3.08  loss_ce_2: 0.358  loss_mask_2: 0.4001  loss_dice_2: 3.054  loss_ce_3: 0.3219  loss_mask_3: 0.3986  loss_dice_3: 3.04  loss_ce_4: 0.3246  loss_mask_4: 0.3981  loss_dice_4: 3.039  loss_ce_5: 0.319  loss_mask_5: 0.3943  loss_dice_5: 3.042  loss_ce_6: 0.3201  loss_mask_6: 0.3958  loss_dice_6: 3.035  loss_ce_7: 0.3166  loss_mask_7: 0.3952  loss_dice_7: 3.044  loss_ce_8: 0.3344  loss_mask_8: 0.398  loss_dice_8: 3.041  time: 1.4976  data_time: 0.0808  lr: 6.1297e-06  max_mem: 21589M
[01/18 02:28:24] d2.utils.events INFO:  eta: 9:37:05  iter: 16799  total_loss: 38.74  loss_ce: 0.3045  loss_mask: 0.3977  loss_dice: 3.078  loss_ce_0: 0.6047  loss_mask_0: 0.4004  loss_dice_0: 3.2  loss_ce_1: 0.3341  loss_mask_1: 0.4078  loss_dice_1: 3.108  loss_ce_2: 0.3309  loss_mask_2: 0.4049  loss_dice_2: 3.09  loss_ce_3: 0.3174  loss_mask_3: 0.4007  loss_dice_3: 3.082  loss_ce_4: 0.3156  loss_mask_4: 0.4002  loss_dice_4: 3.074  loss_ce_5: 0.3026  loss_mask_5: 0.4007  loss_dice_5: 3.081  loss_ce_6: 0.307  loss_mask_6: 0.4004  loss_dice_6: 3.075  loss_ce_7: 0.2982  loss_mask_7: 0.3999  loss_dice_7: 3.08  loss_ce_8: 0.305  loss_mask_8: 0.4007  loss_dice_8: 3.074  time: 1.4976  data_time: 0.0790  lr: 6.1249e-06  max_mem: 21589M
[01/18 02:28:54] d2.utils.events INFO:  eta: 9:36:56  iter: 16819  total_loss: 37.53  loss_ce: 0.3152  loss_mask: 0.4021  loss_dice: 2.98  loss_ce_0: 0.5773  loss_mask_0: 0.3842  loss_dice_0: 3.114  loss_ce_1: 0.3429  loss_mask_1: 0.4016  loss_dice_1: 3.013  loss_ce_2: 0.3351  loss_mask_2: 0.4012  loss_dice_2: 2.994  loss_ce_3: 0.352  loss_mask_3: 0.4005  loss_dice_3: 2.977  loss_ce_4: 0.3278  loss_mask_4: 0.4012  loss_dice_4: 2.98  loss_ce_5: 0.3206  loss_mask_5: 0.3992  loss_dice_5: 2.985  loss_ce_6: 0.3241  loss_mask_6: 0.4012  loss_dice_6: 2.978  loss_ce_7: 0.3201  loss_mask_7: 0.4035  loss_dice_7: 2.979  loss_ce_8: 0.3235  loss_mask_8: 0.4023  loss_dice_8: 2.977  time: 1.4976  data_time: 0.0780  lr: 6.1202e-06  max_mem: 21589M
[01/18 02:29:25] d2.utils.events INFO:  eta: 9:36:54  iter: 16839  total_loss: 38.26  loss_ce: 0.3148  loss_mask: 0.4046  loss_dice: 3.039  loss_ce_0: 0.5457  loss_mask_0: 0.3956  loss_dice_0: 3.173  loss_ce_1: 0.3333  loss_mask_1: 0.4101  loss_dice_1: 3.084  loss_ce_2: 0.3471  loss_mask_2: 0.408  loss_dice_2: 3.061  loss_ce_3: 0.3162  loss_mask_3: 0.4048  loss_dice_3: 3.037  loss_ce_4: 0.3196  loss_mask_4: 0.4059  loss_dice_4: 3.045  loss_ce_5: 0.3212  loss_mask_5: 0.4049  loss_dice_5: 3.052  loss_ce_6: 0.3101  loss_mask_6: 0.4045  loss_dice_6: 3.047  loss_ce_7: 0.3089  loss_mask_7: 0.4037  loss_dice_7: 3.042  loss_ce_8: 0.2946  loss_mask_8: 0.4039  loss_dice_8: 3.045  time: 1.4976  data_time: 0.0733  lr: 6.1154e-06  max_mem: 21589M
[01/18 02:29:55] d2.utils.events INFO:  eta: 9:36:50  iter: 16859  total_loss: 38.24  loss_ce: 0.322  loss_mask: 0.3967  loss_dice: 3.037  loss_ce_0: 0.57  loss_mask_0: 0.3859  loss_dice_0: 3.154  loss_ce_1: 0.3417  loss_mask_1: 0.4045  loss_dice_1: 3.078  loss_ce_2: 0.3446  loss_mask_2: 0.3998  loss_dice_2: 3.05  loss_ce_3: 0.3257  loss_mask_3: 0.3976  loss_dice_3: 3.039  loss_ce_4: 0.3192  loss_mask_4: 0.3956  loss_dice_4: 3.04  loss_ce_5: 0.3245  loss_mask_5: 0.3958  loss_dice_5: 3.05  loss_ce_6: 0.3295  loss_mask_6: 0.3976  loss_dice_6: 3.043  loss_ce_7: 0.3109  loss_mask_7: 0.3973  loss_dice_7: 3.051  loss_ce_8: 0.3043  loss_mask_8: 0.3956  loss_dice_8: 3.043  time: 1.4977  data_time: 0.0816  lr: 6.1107e-06  max_mem: 21589M
[01/18 02:30:25] d2.utils.events INFO:  eta: 9:36:30  iter: 16879  total_loss: 37.41  loss_ce: 0.2908  loss_mask: 0.3946  loss_dice: 2.995  loss_ce_0: 0.5755  loss_mask_0: 0.3859  loss_dice_0: 3.145  loss_ce_1: 0.3307  loss_mask_1: 0.4005  loss_dice_1: 3.046  loss_ce_2: 0.3359  loss_mask_2: 0.399  loss_dice_2: 3.018  loss_ce_3: 0.3306  loss_mask_3: 0.3957  loss_dice_3: 3.011  loss_ce_4: 0.3194  loss_mask_4: 0.3951  loss_dice_4: 2.998  loss_ce_5: 0.3078  loss_mask_5: 0.3965  loss_dice_5: 2.999  loss_ce_6: 0.3198  loss_mask_6: 0.3964  loss_dice_6: 3.001  loss_ce_7: 0.3075  loss_mask_7: 0.3973  loss_dice_7: 2.998  loss_ce_8: 0.3246  loss_mask_8: 0.395  loss_dice_8: 2.999  time: 1.4976  data_time: 0.0837  lr: 6.1059e-06  max_mem: 21589M
[01/18 02:30:54] d2.utils.events INFO:  eta: 9:36:36  iter: 16899  total_loss: 37.85  loss_ce: 0.3162  loss_mask: 0.4018  loss_dice: 2.991  loss_ce_0: 0.5861  loss_mask_0: 0.3913  loss_dice_0: 3.162  loss_ce_1: 0.3365  loss_mask_1: 0.4067  loss_dice_1: 3.054  loss_ce_2: 0.3335  loss_mask_2: 0.4054  loss_dice_2: 3.018  loss_ce_3: 0.3195  loss_mask_3: 0.4015  loss_dice_3: 3.008  loss_ce_4: 0.316  loss_mask_4: 0.4024  loss_dice_4: 3.002  loss_ce_5: 0.3202  loss_mask_5: 0.4021  loss_dice_5: 3.003  loss_ce_6: 0.3267  loss_mask_6: 0.4008  loss_dice_6: 3  loss_ce_7: 0.3094  loss_mask_7: 0.4013  loss_dice_7: 3  loss_ce_8: 0.3051  loss_mask_8: 0.4024  loss_dice_8: 3.009  time: 1.4976  data_time: 0.0750  lr: 6.1012e-06  max_mem: 21589M
[01/18 02:31:25] d2.utils.events INFO:  eta: 9:37:18  iter: 16919  total_loss: 37.9  loss_ce: 0.2942  loss_mask: 0.402  loss_dice: 3.023  loss_ce_0: 0.5548  loss_mask_0: 0.3933  loss_dice_0: 3.172  loss_ce_1: 0.3305  loss_mask_1: 0.4036  loss_dice_1: 3.07  loss_ce_2: 0.3338  loss_mask_2: 0.4027  loss_dice_2: 3.052  loss_ce_3: 0.307  loss_mask_3: 0.3995  loss_dice_3: 3.032  loss_ce_4: 0.3039  loss_mask_4: 0.3994  loss_dice_4: 3.039  loss_ce_5: 0.2895  loss_mask_5: 0.401  loss_dice_5: 3.038  loss_ce_6: 0.2975  loss_mask_6: 0.4024  loss_dice_6: 3.027  loss_ce_7: 0.2979  loss_mask_7: 0.4016  loss_dice_7: 3.029  loss_ce_8: 0.295  loss_mask_8: 0.4025  loss_dice_8: 3.023  time: 1.4976  data_time: 0.0765  lr: 6.0964e-06  max_mem: 21589M
[01/18 02:31:55] d2.utils.events INFO:  eta: 9:37:23  iter: 16939  total_loss: 38.12  loss_ce: 0.3019  loss_mask: 0.3988  loss_dice: 3.032  loss_ce_0: 0.5838  loss_mask_0: 0.3892  loss_dice_0: 3.159  loss_ce_1: 0.3163  loss_mask_1: 0.4038  loss_dice_1: 3.076  loss_ce_2: 0.3173  loss_mask_2: 0.4022  loss_dice_2: 3.048  loss_ce_3: 0.3112  loss_mask_3: 0.402  loss_dice_3: 3.043  loss_ce_4: 0.3036  loss_mask_4: 0.4011  loss_dice_4: 3.032  loss_ce_5: 0.2952  loss_mask_5: 0.3991  loss_dice_5: 3.037  loss_ce_6: 0.3084  loss_mask_6: 0.3978  loss_dice_6: 3.035  loss_ce_7: 0.3065  loss_mask_7: 0.398  loss_dice_7: 3.033  loss_ce_8: 0.2943  loss_mask_8: 0.3993  loss_dice_8: 3.036  time: 1.4977  data_time: 0.0938  lr: 6.0917e-06  max_mem: 21589M
[01/18 02:32:26] d2.utils.events INFO:  eta: 9:37:30  iter: 16959  total_loss: 37.7  loss_ce: 0.3053  loss_mask: 0.3917  loss_dice: 2.996  loss_ce_0: 0.5741  loss_mask_0: 0.3834  loss_dice_0: 3.132  loss_ce_1: 0.3355  loss_mask_1: 0.396  loss_dice_1: 3.036  loss_ce_2: 0.3557  loss_mask_2: 0.3946  loss_dice_2: 3.009  loss_ce_3: 0.3224  loss_mask_3: 0.3908  loss_dice_3: 3.002  loss_ce_4: 0.3164  loss_mask_4: 0.3916  loss_dice_4: 3.002  loss_ce_5: 0.3036  loss_mask_5: 0.3898  loss_dice_5: 2.994  loss_ce_6: 0.3092  loss_mask_6: 0.3924  loss_dice_6: 2.997  loss_ce_7: 0.2984  loss_mask_7: 0.3902  loss_dice_7: 3.003  loss_ce_8: 0.302  loss_mask_8: 0.3888  loss_dice_8: 2.999  time: 1.4977  data_time: 0.0878  lr: 6.0869e-06  max_mem: 21589M
[01/18 02:32:55] d2.utils.events INFO:  eta: 9:37:14  iter: 16979  total_loss: 37.81  loss_ce: 0.3079  loss_mask: 0.4052  loss_dice: 2.992  loss_ce_0: 0.5953  loss_mask_0: 0.3908  loss_dice_0: 3.12  loss_ce_1: 0.3324  loss_mask_1: 0.4017  loss_dice_1: 3.039  loss_ce_2: 0.3208  loss_mask_2: 0.4027  loss_dice_2: 3.021  loss_ce_3: 0.3283  loss_mask_3: 0.405  loss_dice_3: 3.014  loss_ce_4: 0.3185  loss_mask_4: 0.4036  loss_dice_4: 3.004  loss_ce_5: 0.3181  loss_mask_5: 0.4022  loss_dice_5: 3.004  loss_ce_6: 0.3037  loss_mask_6: 0.4028  loss_dice_6: 2.997  loss_ce_7: 0.292  loss_mask_7: 0.4045  loss_dice_7: 3.013  loss_ce_8: 0.3104  loss_mask_8: 0.4054  loss_dice_8: 2.993  time: 1.4977  data_time: 0.0692  lr: 6.0822e-06  max_mem: 21589M
[01/18 02:33:26] d2.utils.events INFO:  eta: 9:36:56  iter: 16999  total_loss: 37.77  loss_ce: 0.308  loss_mask: 0.3989  loss_dice: 3.017  loss_ce_0: 0.5845  loss_mask_0: 0.3915  loss_dice_0: 3.163  loss_ce_1: 0.3484  loss_mask_1: 0.4071  loss_dice_1: 3.059  loss_ce_2: 0.3326  loss_mask_2: 0.4034  loss_dice_2: 3.037  loss_ce_3: 0.3136  loss_mask_3: 0.4021  loss_dice_3: 3.021  loss_ce_4: 0.3077  loss_mask_4: 0.4009  loss_dice_4: 3.026  loss_ce_5: 0.2962  loss_mask_5: 0.4001  loss_dice_5: 3.033  loss_ce_6: 0.2975  loss_mask_6: 0.4014  loss_dice_6: 3.032  loss_ce_7: 0.2907  loss_mask_7: 0.4003  loss_dice_7: 3.022  loss_ce_8: 0.3083  loss_mask_8: 0.3999  loss_dice_8: 3.025  time: 1.4977  data_time: 0.0821  lr: 6.0774e-06  max_mem: 21589M
[01/18 02:33:56] d2.utils.events INFO:  eta: 9:36:14  iter: 17019  total_loss: 37.87  loss_ce: 0.306  loss_mask: 0.4028  loss_dice: 3.03  loss_ce_0: 0.587  loss_mask_0: 0.3874  loss_dice_0: 3.144  loss_ce_1: 0.3247  loss_mask_1: 0.4026  loss_dice_1: 3.06  loss_ce_2: 0.3506  loss_mask_2: 0.4039  loss_dice_2: 3.031  loss_ce_3: 0.3259  loss_mask_3: 0.4037  loss_dice_3: 3.025  loss_ce_4: 0.3318  loss_mask_4: 0.4033  loss_dice_4: 3.019  loss_ce_5: 0.3241  loss_mask_5: 0.4014  loss_dice_5: 3.034  loss_ce_6: 0.3145  loss_mask_6: 0.4028  loss_dice_6: 3.021  loss_ce_7: 0.3002  loss_mask_7: 0.4025  loss_dice_7: 3.029  loss_ce_8: 0.3186  loss_mask_8: 0.403  loss_dice_8: 3.022  time: 1.4977  data_time: 0.0731  lr: 6.0726e-06  max_mem: 21589M
[01/18 02:34:26] d2.utils.events INFO:  eta: 9:35:54  iter: 17039  total_loss: 38.84  loss_ce: 0.3207  loss_mask: 0.4021  loss_dice: 3.074  loss_ce_0: 0.5674  loss_mask_0: 0.3949  loss_dice_0: 3.209  loss_ce_1: 0.3477  loss_mask_1: 0.4061  loss_dice_1: 3.127  loss_ce_2: 0.3548  loss_mask_2: 0.4052  loss_dice_2: 3.1  loss_ce_3: 0.3457  loss_mask_3: 0.4024  loss_dice_3: 3.084  loss_ce_4: 0.331  loss_mask_4: 0.4046  loss_dice_4: 3.085  loss_ce_5: 0.3438  loss_mask_5: 0.4028  loss_dice_5: 3.084  loss_ce_6: 0.3272  loss_mask_6: 0.4024  loss_dice_6: 3.083  loss_ce_7: 0.3307  loss_mask_7: 0.4041  loss_dice_7: 3.07  loss_ce_8: 0.3359  loss_mask_8: 0.4032  loss_dice_8: 3.077  time: 1.4977  data_time: 0.0773  lr: 6.0679e-06  max_mem: 21589M
[01/18 02:34:56] d2.utils.events INFO:  eta: 9:35:24  iter: 17059  total_loss: 37.88  loss_ce: 0.3258  loss_mask: 0.4146  loss_dice: 2.987  loss_ce_0: 0.5791  loss_mask_0: 0.4043  loss_dice_0: 3.113  loss_ce_1: 0.3556  loss_mask_1: 0.4236  loss_dice_1: 3.026  loss_ce_2: 0.3543  loss_mask_2: 0.4199  loss_dice_2: 3.003  loss_ce_3: 0.3322  loss_mask_3: 0.4177  loss_dice_3: 2.987  loss_ce_4: 0.3303  loss_mask_4: 0.4145  loss_dice_4: 2.992  loss_ce_5: 0.3293  loss_mask_5: 0.4159  loss_dice_5: 3  loss_ce_6: 0.3032  loss_mask_6: 0.4149  loss_dice_6: 2.982  loss_ce_7: 0.3153  loss_mask_7: 0.414  loss_dice_7: 2.989  loss_ce_8: 0.3261  loss_mask_8: 0.4143  loss_dice_8: 2.986  time: 1.4977  data_time: 0.0841  lr: 6.0631e-06  max_mem: 21589M
[01/18 02:35:26] d2.utils.events INFO:  eta: 9:35:18  iter: 17079  total_loss: 37.69  loss_ce: 0.3098  loss_mask: 0.3967  loss_dice: 3.015  loss_ce_0: 0.6017  loss_mask_0: 0.386  loss_dice_0: 3.15  loss_ce_1: 0.333  loss_mask_1: 0.4013  loss_dice_1: 3.049  loss_ce_2: 0.3379  loss_mask_2: 0.3966  loss_dice_2: 3.035  loss_ce_3: 0.3207  loss_mask_3: 0.3981  loss_dice_3: 3.02  loss_ce_4: 0.3082  loss_mask_4: 0.3975  loss_dice_4: 3.017  loss_ce_5: 0.3068  loss_mask_5: 0.3961  loss_dice_5: 3.017  loss_ce_6: 0.3118  loss_mask_6: 0.3958  loss_dice_6: 3.018  loss_ce_7: 0.2971  loss_mask_7: 0.3956  loss_dice_7: 3.016  loss_ce_8: 0.308  loss_mask_8: 0.3971  loss_dice_8: 3.013  time: 1.4977  data_time: 0.0784  lr: 6.0584e-06  max_mem: 21589M
[01/18 02:35:57] d2.utils.events INFO:  eta: 9:35:14  iter: 17099  total_loss: 37.76  loss_ce: 0.3074  loss_mask: 0.3991  loss_dice: 2.98  loss_ce_0: 0.5584  loss_mask_0: 0.3881  loss_dice_0: 3.143  loss_ce_1: 0.3076  loss_mask_1: 0.3989  loss_dice_1: 3.043  loss_ce_2: 0.3151  loss_mask_2: 0.3986  loss_dice_2: 3.009  loss_ce_3: 0.3041  loss_mask_3: 0.3959  loss_dice_3: 3.006  loss_ce_4: 0.3186  loss_mask_4: 0.3954  loss_dice_4: 2.997  loss_ce_5: 0.3008  loss_mask_5: 0.3966  loss_dice_5: 2.998  loss_ce_6: 0.3012  loss_mask_6: 0.397  loss_dice_6: 2.989  loss_ce_7: 0.2797  loss_mask_7: 0.3994  loss_dice_7: 2.996  loss_ce_8: 0.3027  loss_mask_8: 0.3988  loss_dice_8: 2.998  time: 1.4977  data_time: 0.0754  lr: 6.0536e-06  max_mem: 21589M
[01/18 02:36:27] d2.utils.events INFO:  eta: 9:34:52  iter: 17119  total_loss: 38.29  loss_ce: 0.2981  loss_mask: 0.3961  loss_dice: 3.087  loss_ce_0: 0.564  loss_mask_0: 0.3909  loss_dice_0: 3.214  loss_ce_1: 0.3176  loss_mask_1: 0.4007  loss_dice_1: 3.127  loss_ce_2: 0.3185  loss_mask_2: 0.3995  loss_dice_2: 3.105  loss_ce_3: 0.3114  loss_mask_3: 0.3991  loss_dice_3: 3.095  loss_ce_4: 0.3002  loss_mask_4: 0.3981  loss_dice_4: 3.088  loss_ce_5: 0.2967  loss_mask_5: 0.3965  loss_dice_5: 3.088  loss_ce_6: 0.2924  loss_mask_6: 0.3963  loss_dice_6: 3.091  loss_ce_7: 0.312  loss_mask_7: 0.3968  loss_dice_7: 3.092  loss_ce_8: 0.2878  loss_mask_8: 0.397  loss_dice_8: 3.088  time: 1.4978  data_time: 0.0710  lr: 6.0489e-06  max_mem: 21589M
[01/18 02:36:57] d2.utils.events INFO:  eta: 9:34:27  iter: 17139  total_loss: 38.74  loss_ce: 0.3162  loss_mask: 0.4108  loss_dice: 3.081  loss_ce_0: 0.5862  loss_mask_0: 0.4048  loss_dice_0: 3.211  loss_ce_1: 0.3631  loss_mask_1: 0.4235  loss_dice_1: 3.121  loss_ce_2: 0.3419  loss_mask_2: 0.4153  loss_dice_2: 3.099  loss_ce_3: 0.3345  loss_mask_3: 0.4134  loss_dice_3: 3.08  loss_ce_4: 0.3448  loss_mask_4: 0.4134  loss_dice_4: 3.088  loss_ce_5: 0.3227  loss_mask_5: 0.4109  loss_dice_5: 3.08  loss_ce_6: 0.3338  loss_mask_6: 0.4126  loss_dice_6: 3.08  loss_ce_7: 0.3214  loss_mask_7: 0.4123  loss_dice_7: 3.09  loss_ce_8: 0.3133  loss_mask_8: 0.4096  loss_dice_8: 3.089  time: 1.4978  data_time: 0.0730  lr: 6.0441e-06  max_mem: 21589M
[01/18 02:37:28] d2.utils.events INFO:  eta: 9:33:58  iter: 17159  total_loss: 37.58  loss_ce: 0.3069  loss_mask: 0.4041  loss_dice: 2.998  loss_ce_0: 0.5777  loss_mask_0: 0.394  loss_dice_0: 3.134  loss_ce_1: 0.3215  loss_mask_1: 0.4106  loss_dice_1: 3.042  loss_ce_2: 0.3262  loss_mask_2: 0.4049  loss_dice_2: 3.024  loss_ce_3: 0.3133  loss_mask_3: 0.4028  loss_dice_3: 3.008  loss_ce_4: 0.3023  loss_mask_4: 0.4021  loss_dice_4: 3.003  loss_ce_5: 0.2918  loss_mask_5: 0.4035  loss_dice_5: 3.008  loss_ce_6: 0.2935  loss_mask_6: 0.4026  loss_dice_6: 3.005  loss_ce_7: 0.3007  loss_mask_7: 0.4018  loss_dice_7: 3.009  loss_ce_8: 0.2899  loss_mask_8: 0.4029  loss_dice_8: 3.004  time: 1.4978  data_time: 0.0805  lr: 6.0393e-06  max_mem: 21589M
[01/18 02:37:57] d2.utils.events INFO:  eta: 9:33:26  iter: 17179  total_loss: 37.69  loss_ce: 0.2896  loss_mask: 0.3982  loss_dice: 3.016  loss_ce_0: 0.5842  loss_mask_0: 0.3879  loss_dice_0: 3.155  loss_ce_1: 0.32  loss_mask_1: 0.3986  loss_dice_1: 3.066  loss_ce_2: 0.3249  loss_mask_2: 0.3957  loss_dice_2: 3.041  loss_ce_3: 0.3124  loss_mask_3: 0.3945  loss_dice_3: 3.027  loss_ce_4: 0.3068  loss_mask_4: 0.3953  loss_dice_4: 3.025  loss_ce_5: 0.304  loss_mask_5: 0.3972  loss_dice_5: 3.028  loss_ce_6: 0.2939  loss_mask_6: 0.3994  loss_dice_6: 3.021  loss_ce_7: 0.2808  loss_mask_7: 0.3987  loss_dice_7: 3.023  loss_ce_8: 0.2938  loss_mask_8: 0.3989  loss_dice_8: 3.022  time: 1.4978  data_time: 0.0736  lr: 6.0346e-06  max_mem: 21589M
[01/18 02:38:28] d2.utils.events INFO:  eta: 9:33:12  iter: 17199  total_loss: 38.14  loss_ce: 0.3045  loss_mask: 0.3918  loss_dice: 3.05  loss_ce_0: 0.5422  loss_mask_0: 0.3843  loss_dice_0: 3.195  loss_ce_1: 0.321  loss_mask_1: 0.3976  loss_dice_1: 3.104  loss_ce_2: 0.3216  loss_mask_2: 0.3944  loss_dice_2: 3.071  loss_ce_3: 0.3194  loss_mask_3: 0.3921  loss_dice_3: 3.061  loss_ce_4: 0.3119  loss_mask_4: 0.3912  loss_dice_4: 3.051  loss_ce_5: 0.3302  loss_mask_5: 0.3906  loss_dice_5: 3.06  loss_ce_6: 0.3185  loss_mask_6: 0.389  loss_dice_6: 3.053  loss_ce_7: 0.3067  loss_mask_7: 0.3895  loss_dice_7: 3.054  loss_ce_8: 0.3034  loss_mask_8: 0.3907  loss_dice_8: 3.054  time: 1.4978  data_time: 0.0754  lr: 6.0298e-06  max_mem: 21589M
[01/18 02:38:58] d2.utils.events INFO:  eta: 9:32:58  iter: 17219  total_loss: 37.97  loss_ce: 0.3043  loss_mask: 0.3986  loss_dice: 3.046  loss_ce_0: 0.5934  loss_mask_0: 0.3869  loss_dice_0: 3.192  loss_ce_1: 0.3504  loss_mask_1: 0.4  loss_dice_1: 3.08  loss_ce_2: 0.3408  loss_mask_2: 0.3995  loss_dice_2: 3.052  loss_ce_3: 0.3196  loss_mask_3: 0.4018  loss_dice_3: 3.037  loss_ce_4: 0.3171  loss_mask_4: 0.4004  loss_dice_4: 3.038  loss_ce_5: 0.3043  loss_mask_5: 0.3993  loss_dice_5: 3.037  loss_ce_6: 0.3023  loss_mask_6: 0.3987  loss_dice_6: 3.038  loss_ce_7: 0.3033  loss_mask_7: 0.3981  loss_dice_7: 3.044  loss_ce_8: 0.3071  loss_mask_8: 0.3973  loss_dice_8: 3.035  time: 1.4978  data_time: 0.0736  lr: 6.0251e-06  max_mem: 21589M
[01/18 02:39:29] d2.utils.events INFO:  eta: 9:32:32  iter: 17239  total_loss: 37.58  loss_ce: 0.3108  loss_mask: 0.4073  loss_dice: 2.976  loss_ce_0: 0.6005  loss_mask_0: 0.4007  loss_dice_0: 3.112  loss_ce_1: 0.3432  loss_mask_1: 0.4148  loss_dice_1: 3.015  loss_ce_2: 0.3579  loss_mask_2: 0.4118  loss_dice_2: 2.991  loss_ce_3: 0.3328  loss_mask_3: 0.4075  loss_dice_3: 2.988  loss_ce_4: 0.3247  loss_mask_4: 0.4062  loss_dice_4: 2.981  loss_ce_5: 0.3212  loss_mask_5: 0.407  loss_dice_5: 2.987  loss_ce_6: 0.323  loss_mask_6: 0.4066  loss_dice_6: 2.985  loss_ce_7: 0.319  loss_mask_7: 0.4062  loss_dice_7: 2.99  loss_ce_8: 0.3114  loss_mask_8: 0.4069  loss_dice_8: 2.992  time: 1.4979  data_time: 0.0702  lr: 6.0203e-06  max_mem: 21589M
[01/18 02:39:59] d2.utils.events INFO:  eta: 9:32:04  iter: 17259  total_loss: 37.92  loss_ce: 0.2924  loss_mask: 0.4063  loss_dice: 3.032  loss_ce_0: 0.5811  loss_mask_0: 0.3955  loss_dice_0: 3.165  loss_ce_1: 0.3355  loss_mask_1: 0.4115  loss_dice_1: 3.077  loss_ce_2: 0.3201  loss_mask_2: 0.4072  loss_dice_2: 3.043  loss_ce_3: 0.3176  loss_mask_3: 0.4038  loss_dice_3: 3.039  loss_ce_4: 0.2886  loss_mask_4: 0.4065  loss_dice_4: 3.03  loss_ce_5: 0.2828  loss_mask_5: 0.4086  loss_dice_5: 3.038  loss_ce_6: 0.3024  loss_mask_6: 0.4073  loss_dice_6: 3.03  loss_ce_7: 0.2995  loss_mask_7: 0.4078  loss_dice_7: 3.029  loss_ce_8: 0.2802  loss_mask_8: 0.4073  loss_dice_8: 3.03  time: 1.4979  data_time: 0.0748  lr: 6.0155e-06  max_mem: 21589M
[01/18 02:40:29] d2.utils.events INFO:  eta: 9:31:32  iter: 17279  total_loss: 38.25  loss_ce: 0.2853  loss_mask: 0.3959  loss_dice: 3.041  loss_ce_0: 0.5603  loss_mask_0: 0.3853  loss_dice_0: 3.191  loss_ce_1: 0.3228  loss_mask_1: 0.4003  loss_dice_1: 3.098  loss_ce_2: 0.3124  loss_mask_2: 0.3971  loss_dice_2: 3.077  loss_ce_3: 0.2966  loss_mask_3: 0.3981  loss_dice_3: 3.052  loss_ce_4: 0.2981  loss_mask_4: 0.3967  loss_dice_4: 3.047  loss_ce_5: 0.2897  loss_mask_5: 0.3965  loss_dice_5: 3.05  loss_ce_6: 0.2803  loss_mask_6: 0.3987  loss_dice_6: 3.044  loss_ce_7: 0.2934  loss_mask_7: 0.3972  loss_dice_7: 3.036  loss_ce_8: 0.2761  loss_mask_8: 0.3975  loss_dice_8: 3.043  time: 1.4979  data_time: 0.0777  lr: 6.0108e-06  max_mem: 21589M
[01/18 02:41:00] d2.utils.events INFO:  eta: 9:31:02  iter: 17299  total_loss: 37.71  loss_ce: 0.3055  loss_mask: 0.3932  loss_dice: 3.007  loss_ce_0: 0.5653  loss_mask_0: 0.3878  loss_dice_0: 3.141  loss_ce_1: 0.3358  loss_mask_1: 0.3986  loss_dice_1: 3.041  loss_ce_2: 0.3316  loss_mask_2: 0.3965  loss_dice_2: 3.016  loss_ce_3: 0.3191  loss_mask_3: 0.3939  loss_dice_3: 3.006  loss_ce_4: 0.3159  loss_mask_4: 0.3938  loss_dice_4: 2.998  loss_ce_5: 0.3087  loss_mask_5: 0.3956  loss_dice_5: 3.002  loss_ce_6: 0.3014  loss_mask_6: 0.3953  loss_dice_6: 2.998  loss_ce_7: 0.3055  loss_mask_7: 0.3943  loss_dice_7: 2.999  loss_ce_8: 0.3011  loss_mask_8: 0.3952  loss_dice_8: 2.991  time: 1.4979  data_time: 0.0679  lr: 6.006e-06  max_mem: 21589M
[01/18 02:41:30] d2.utils.events INFO:  eta: 9:30:30  iter: 17319  total_loss: 37.8  loss_ce: 0.3215  loss_mask: 0.3958  loss_dice: 3.007  loss_ce_0: 0.5684  loss_mask_0: 0.383  loss_dice_0: 3.14  loss_ce_1: 0.366  loss_mask_1: 0.3978  loss_dice_1: 3.058  loss_ce_2: 0.3258  loss_mask_2: 0.3978  loss_dice_2: 3.024  loss_ce_3: 0.3243  loss_mask_3: 0.3951  loss_dice_3: 3.019  loss_ce_4: 0.3182  loss_mask_4: 0.3954  loss_dice_4: 3.021  loss_ce_5: 0.3172  loss_mask_5: 0.3949  loss_dice_5: 3.021  loss_ce_6: 0.318  loss_mask_6: 0.3946  loss_dice_6: 3.012  loss_ce_7: 0.311  loss_mask_7: 0.3939  loss_dice_7: 3.014  loss_ce_8: 0.3018  loss_mask_8: 0.3956  loss_dice_8: 3.01  time: 1.4979  data_time: 0.0711  lr: 6.0013e-06  max_mem: 21589M
[01/18 02:42:00] d2.utils.events INFO:  eta: 9:30:03  iter: 17339  total_loss: 37.72  loss_ce: 0.3207  loss_mask: 0.4046  loss_dice: 3.022  loss_ce_0: 0.6031  loss_mask_0: 0.3876  loss_dice_0: 3.135  loss_ce_1: 0.3666  loss_mask_1: 0.4009  loss_dice_1: 3.043  loss_ce_2: 0.3595  loss_mask_2: 0.4012  loss_dice_2: 3.026  loss_ce_3: 0.3391  loss_mask_3: 0.4034  loss_dice_3: 3.015  loss_ce_4: 0.3409  loss_mask_4: 0.4038  loss_dice_4: 3.017  loss_ce_5: 0.3168  loss_mask_5: 0.4033  loss_dice_5: 3.021  loss_ce_6: 0.3188  loss_mask_6: 0.4059  loss_dice_6: 3.018  loss_ce_7: 0.3249  loss_mask_7: 0.402  loss_dice_7: 3.015  loss_ce_8: 0.3218  loss_mask_8: 0.4017  loss_dice_8: 3.013  time: 1.4979  data_time: 0.0758  lr: 5.9965e-06  max_mem: 21589M
[01/18 02:42:30] d2.utils.events INFO:  eta: 9:29:37  iter: 17359  total_loss: 37.86  loss_ce: 0.3154  loss_mask: 0.3999  loss_dice: 3.012  loss_ce_0: 0.6165  loss_mask_0: 0.3896  loss_dice_0: 3.159  loss_ce_1: 0.351  loss_mask_1: 0.4037  loss_dice_1: 3.054  loss_ce_2: 0.3538  loss_mask_2: 0.3969  loss_dice_2: 3.03  loss_ce_3: 0.3376  loss_mask_3: 0.3975  loss_dice_3: 3.012  loss_ce_4: 0.3296  loss_mask_4: 0.3963  loss_dice_4: 3.01  loss_ce_5: 0.3269  loss_mask_5: 0.3986  loss_dice_5: 3.013  loss_ce_6: 0.3272  loss_mask_6: 0.3988  loss_dice_6: 3.009  loss_ce_7: 0.3161  loss_mask_7: 0.3986  loss_dice_7: 3.013  loss_ce_8: 0.3216  loss_mask_8: 0.399  loss_dice_8: 3.008  time: 1.4979  data_time: 0.0810  lr: 5.9917e-06  max_mem: 21589M
[01/18 02:43:00] d2.utils.events INFO:  eta: 9:29:07  iter: 17379  total_loss: 38.72  loss_ce: 0.3005  loss_mask: 0.4128  loss_dice: 3.081  loss_ce_0: 0.5936  loss_mask_0: 0.412  loss_dice_0: 3.192  loss_ce_1: 0.3293  loss_mask_1: 0.4255  loss_dice_1: 3.102  loss_ce_2: 0.3223  loss_mask_2: 0.4207  loss_dice_2: 3.091  loss_ce_3: 0.2965  loss_mask_3: 0.4152  loss_dice_3: 3.083  loss_ce_4: 0.3149  loss_mask_4: 0.4103  loss_dice_4: 3.079  loss_ce_5: 0.2926  loss_mask_5: 0.4103  loss_dice_5: 3.079  loss_ce_6: 0.2975  loss_mask_6: 0.4111  loss_dice_6: 3.08  loss_ce_7: 0.298  loss_mask_7: 0.412  loss_dice_7: 3.076  loss_ce_8: 0.2944  loss_mask_8: 0.413  loss_dice_8: 3.081  time: 1.4979  data_time: 0.0763  lr: 5.987e-06  max_mem: 21589M
[01/18 02:43:31] d2.utils.events INFO:  eta: 9:28:34  iter: 17399  total_loss: 38.74  loss_ce: 0.3033  loss_mask: 0.4083  loss_dice: 3.066  loss_ce_0: 0.5701  loss_mask_0: 0.4007  loss_dice_0: 3.193  loss_ce_1: 0.352  loss_mask_1: 0.4095  loss_dice_1: 3.102  loss_ce_2: 0.332  loss_mask_2: 0.4111  loss_dice_2: 3.077  loss_ce_3: 0.3047  loss_mask_3: 0.4067  loss_dice_3: 3.063  loss_ce_4: 0.3196  loss_mask_4: 0.4083  loss_dice_4: 3.066  loss_ce_5: 0.314  loss_mask_5: 0.4065  loss_dice_5: 3.053  loss_ce_6: 0.3107  loss_mask_6: 0.4061  loss_dice_6: 3.064  loss_ce_7: 0.3068  loss_mask_7: 0.4068  loss_dice_7: 3.064  loss_ce_8: 0.3011  loss_mask_8: 0.4062  loss_dice_8: 3.072  time: 1.4980  data_time: 0.0822  lr: 5.9822e-06  max_mem: 21589M
[01/18 02:44:01] d2.utils.events INFO:  eta: 9:27:31  iter: 17419  total_loss: 37.46  loss_ce: 0.2942  loss_mask: 0.4051  loss_dice: 2.985  loss_ce_0: 0.5636  loss_mask_0: 0.3991  loss_dice_0: 3.118  loss_ce_1: 0.3365  loss_mask_1: 0.4046  loss_dice_1: 3.032  loss_ce_2: 0.3278  loss_mask_2: 0.4018  loss_dice_2: 3.013  loss_ce_3: 0.3264  loss_mask_3: 0.4024  loss_dice_3: 3.002  loss_ce_4: 0.2894  loss_mask_4: 0.4033  loss_dice_4: 2.995  loss_ce_5: 0.2881  loss_mask_5: 0.4041  loss_dice_5: 2.993  loss_ce_6: 0.3063  loss_mask_6: 0.4056  loss_dice_6: 2.991  loss_ce_7: 0.286  loss_mask_7: 0.4063  loss_dice_7: 2.988  loss_ce_8: 0.2892  loss_mask_8: 0.4053  loss_dice_8: 2.993  time: 1.4980  data_time: 0.0770  lr: 5.9774e-06  max_mem: 21589M
[01/18 02:44:31] d2.utils.events INFO:  eta: 9:26:47  iter: 17439  total_loss: 37.47  loss_ce: 0.3301  loss_mask: 0.4001  loss_dice: 2.982  loss_ce_0: 0.619  loss_mask_0: 0.3905  loss_dice_0: 3.12  loss_ce_1: 0.362  loss_mask_1: 0.4022  loss_dice_1: 3.03  loss_ce_2: 0.3452  loss_mask_2: 0.4017  loss_dice_2: 2.986  loss_ce_3: 0.342  loss_mask_3: 0.4022  loss_dice_3: 2.971  loss_ce_4: 0.3292  loss_mask_4: 0.4002  loss_dice_4: 2.975  loss_ce_5: 0.3216  loss_mask_5: 0.3999  loss_dice_5: 2.979  loss_ce_6: 0.3266  loss_mask_6: 0.4013  loss_dice_6: 2.99  loss_ce_7: 0.3135  loss_mask_7: 0.3998  loss_dice_7: 2.985  loss_ce_8: 0.3214  loss_mask_8: 0.4017  loss_dice_8: 2.988  time: 1.4980  data_time: 0.0788  lr: 5.9727e-06  max_mem: 21589M
[01/18 02:45:01] d2.utils.events INFO:  eta: 9:25:41  iter: 17459  total_loss: 37.79  loss_ce: 0.2998  loss_mask: 0.3952  loss_dice: 3.019  loss_ce_0: 0.5808  loss_mask_0: 0.391  loss_dice_0: 3.152  loss_ce_1: 0.3277  loss_mask_1: 0.3994  loss_dice_1: 3.078  loss_ce_2: 0.3186  loss_mask_2: 0.3991  loss_dice_2: 3.041  loss_ce_3: 0.3095  loss_mask_3: 0.3936  loss_dice_3: 3.037  loss_ce_4: 0.3055  loss_mask_4: 0.3926  loss_dice_4: 3.035  loss_ce_5: 0.2888  loss_mask_5: 0.3947  loss_dice_5: 3.038  loss_ce_6: 0.3007  loss_mask_6: 0.3927  loss_dice_6: 3.023  loss_ce_7: 0.2893  loss_mask_7: 0.3954  loss_dice_7: 3.025  loss_ce_8: 0.2914  loss_mask_8: 0.3956  loss_dice_8: 3.021  time: 1.4980  data_time: 0.0757  lr: 5.9679e-06  max_mem: 21589M
[01/18 02:45:31] d2.utils.events INFO:  eta: 9:24:54  iter: 17479  total_loss: 37.75  loss_ce: 0.3039  loss_mask: 0.4032  loss_dice: 2.994  loss_ce_0: 0.5659  loss_mask_0: 0.399  loss_dice_0: 3.118  loss_ce_1: 0.3515  loss_mask_1: 0.4133  loss_dice_1: 3.034  loss_ce_2: 0.3302  loss_mask_2: 0.4074  loss_dice_2: 3.022  loss_ce_3: 0.3141  loss_mask_3: 0.4042  loss_dice_3: 2.993  loss_ce_4: 0.3137  loss_mask_4: 0.4046  loss_dice_4: 3  loss_ce_5: 0.3057  loss_mask_5: 0.4057  loss_dice_5: 3.003  loss_ce_6: 0.3206  loss_mask_6: 0.4033  loss_dice_6: 2.989  loss_ce_7: 0.3063  loss_mask_7: 0.4044  loss_dice_7: 2.986  loss_ce_8: 0.2957  loss_mask_8: 0.4046  loss_dice_8: 3  time: 1.4980  data_time: 0.0743  lr: 5.9631e-06  max_mem: 21589M
[01/18 02:46:00] d2.utils.events INFO:  eta: 9:24:05  iter: 17499  total_loss: 37.97  loss_ce: 0.3137  loss_mask: 0.3963  loss_dice: 3.03  loss_ce_0: 0.5961  loss_mask_0: 0.3899  loss_dice_0: 3.169  loss_ce_1: 0.3409  loss_mask_1: 0.3992  loss_dice_1: 3.074  loss_ce_2: 0.3339  loss_mask_2: 0.3955  loss_dice_2: 3.05  loss_ce_3: 0.3226  loss_mask_3: 0.3942  loss_dice_3: 3.028  loss_ce_4: 0.3124  loss_mask_4: 0.3937  loss_dice_4: 3.043  loss_ce_5: 0.3262  loss_mask_5: 0.3937  loss_dice_5: 3.035  loss_ce_6: 0.3135  loss_mask_6: 0.3955  loss_dice_6: 3.036  loss_ce_7: 0.307  loss_mask_7: 0.3951  loss_dice_7: 3.031  loss_ce_8: 0.3101  loss_mask_8: 0.3961  loss_dice_8: 3.03  time: 1.4980  data_time: 0.0774  lr: 5.9584e-06  max_mem: 21589M
[01/18 02:46:30] d2.utils.events INFO:  eta: 9:23:36  iter: 17519  total_loss: 37.92  loss_ce: 0.3001  loss_mask: 0.4137  loss_dice: 3.028  loss_ce_0: 0.5729  loss_mask_0: 0.3996  loss_dice_0: 3.149  loss_ce_1: 0.3183  loss_mask_1: 0.4201  loss_dice_1: 3.063  loss_ce_2: 0.3292  loss_mask_2: 0.4151  loss_dice_2: 3.029  loss_ce_3: 0.3093  loss_mask_3: 0.4163  loss_dice_3: 3.017  loss_ce_4: 0.3034  loss_mask_4: 0.4139  loss_dice_4: 3.023  loss_ce_5: 0.3046  loss_mask_5: 0.4161  loss_dice_5: 3.026  loss_ce_6: 0.3111  loss_mask_6: 0.4157  loss_dice_6: 3.021  loss_ce_7: 0.3017  loss_mask_7: 0.4138  loss_dice_7: 3.02  loss_ce_8: 0.2902  loss_mask_8: 0.4126  loss_dice_8: 3.014  time: 1.4979  data_time: 0.0706  lr: 5.9536e-06  max_mem: 21589M
[01/18 02:47:01] d2.utils.events INFO:  eta: 9:23:06  iter: 17539  total_loss: 37.26  loss_ce: 0.3042  loss_mask: 0.4143  loss_dice: 2.984  loss_ce_0: 0.561  loss_mask_0: 0.4091  loss_dice_0: 3.118  loss_ce_1: 0.3198  loss_mask_1: 0.4202  loss_dice_1: 3.031  loss_ce_2: 0.3144  loss_mask_2: 0.4163  loss_dice_2: 3.009  loss_ce_3: 0.2996  loss_mask_3: 0.4141  loss_dice_3: 2.998  loss_ce_4: 0.2799  loss_mask_4: 0.4142  loss_dice_4: 2.994  loss_ce_5: 0.2934  loss_mask_5: 0.4158  loss_dice_5: 2.99  loss_ce_6: 0.281  loss_mask_6: 0.4162  loss_dice_6: 2.993  loss_ce_7: 0.2861  loss_mask_7: 0.414  loss_dice_7: 2.987  loss_ce_8: 0.286  loss_mask_8: 0.4142  loss_dice_8: 2.979  time: 1.4980  data_time: 0.0727  lr: 5.9488e-06  max_mem: 21589M
[01/18 02:47:31] d2.utils.events INFO:  eta: 9:22:34  iter: 17559  total_loss: 37.71  loss_ce: 0.3006  loss_mask: 0.3987  loss_dice: 3.001  loss_ce_0: 0.5816  loss_mask_0: 0.3876  loss_dice_0: 3.134  loss_ce_1: 0.3271  loss_mask_1: 0.3998  loss_dice_1: 3.045  loss_ce_2: 0.3198  loss_mask_2: 0.3974  loss_dice_2: 3.017  loss_ce_3: 0.3013  loss_mask_3: 0.3973  loss_dice_3: 3.017  loss_ce_4: 0.3055  loss_mask_4: 0.398  loss_dice_4: 3.007  loss_ce_5: 0.2932  loss_mask_5: 0.3964  loss_dice_5: 3.003  loss_ce_6: 0.2993  loss_mask_6: 0.3981  loss_dice_6: 3.006  loss_ce_7: 0.2956  loss_mask_7: 0.399  loss_dice_7: 3.005  loss_ce_8: 0.2951  loss_mask_8: 0.3976  loss_dice_8: 3.005  time: 1.4980  data_time: 0.0706  lr: 5.9441e-06  max_mem: 21589M
[01/18 02:48:01] d2.utils.events INFO:  eta: 9:21:59  iter: 17579  total_loss: 37.98  loss_ce: 0.2901  loss_mask: 0.4037  loss_dice: 3.02  loss_ce_0: 0.5838  loss_mask_0: 0.3955  loss_dice_0: 3.155  loss_ce_1: 0.3533  loss_mask_1: 0.4091  loss_dice_1: 3.062  loss_ce_2: 0.3323  loss_mask_2: 0.4078  loss_dice_2: 3.035  loss_ce_3: 0.3083  loss_mask_3: 0.4091  loss_dice_3: 3.023  loss_ce_4: 0.3045  loss_mask_4: 0.4084  loss_dice_4: 3.019  loss_ce_5: 0.3015  loss_mask_5: 0.4067  loss_dice_5: 3.021  loss_ce_6: 0.2998  loss_mask_6: 0.4063  loss_dice_6: 3.017  loss_ce_7: 0.2881  loss_mask_7: 0.4059  loss_dice_7: 3.014  loss_ce_8: 0.288  loss_mask_8: 0.4061  loss_dice_8: 3.016  time: 1.4980  data_time: 0.0724  lr: 5.9393e-06  max_mem: 21589M
[01/18 02:48:31] d2.utils.events INFO:  eta: 9:21:35  iter: 17599  total_loss: 37.78  loss_ce: 0.3238  loss_mask: 0.3959  loss_dice: 3.017  loss_ce_0: 0.6188  loss_mask_0: 0.3858  loss_dice_0: 3.158  loss_ce_1: 0.3548  loss_mask_1: 0.3951  loss_dice_1: 3.068  loss_ce_2: 0.3513  loss_mask_2: 0.3951  loss_dice_2: 3.04  loss_ce_3: 0.3393  loss_mask_3: 0.3936  loss_dice_3: 3.021  loss_ce_4: 0.3152  loss_mask_4: 0.3948  loss_dice_4: 3.024  loss_ce_5: 0.3123  loss_mask_5: 0.3926  loss_dice_5: 3.019  loss_ce_6: 0.3213  loss_mask_6: 0.3936  loss_dice_6: 3.028  loss_ce_7: 0.3006  loss_mask_7: 0.3944  loss_dice_7: 3.022  loss_ce_8: 0.3155  loss_mask_8: 0.393  loss_dice_8: 3.02  time: 1.4980  data_time: 0.0708  lr: 5.9345e-06  max_mem: 21589M
[01/18 02:49:01] d2.utils.events INFO:  eta: 9:21:04  iter: 17619  total_loss: 37.62  loss_ce: 0.3289  loss_mask: 0.4067  loss_dice: 2.967  loss_ce_0: 0.5975  loss_mask_0: 0.3997  loss_dice_0: 3.099  loss_ce_1: 0.35  loss_mask_1: 0.4121  loss_dice_1: 3.008  loss_ce_2: 0.3497  loss_mask_2: 0.4102  loss_dice_2: 2.977  loss_ce_3: 0.3477  loss_mask_3: 0.4087  loss_dice_3: 2.969  loss_ce_4: 0.3314  loss_mask_4: 0.4081  loss_dice_4: 2.968  loss_ce_5: 0.3211  loss_mask_5: 0.4086  loss_dice_5: 2.964  loss_ce_6: 0.3185  loss_mask_6: 0.408  loss_dice_6: 2.961  loss_ce_7: 0.3254  loss_mask_7: 0.4083  loss_dice_7: 2.961  loss_ce_8: 0.3115  loss_mask_8: 0.4077  loss_dice_8: 2.963  time: 1.4980  data_time: 0.0812  lr: 5.9298e-06  max_mem: 21589M
[01/18 02:49:32] d2.utils.events INFO:  eta: 9:20:16  iter: 17639  total_loss: 37.61  loss_ce: 0.3004  loss_mask: 0.3988  loss_dice: 3.022  loss_ce_0: 0.5565  loss_mask_0: 0.3868  loss_dice_0: 3.137  loss_ce_1: 0.3202  loss_mask_1: 0.4031  loss_dice_1: 3.063  loss_ce_2: 0.3324  loss_mask_2: 0.3988  loss_dice_2: 3.032  loss_ce_3: 0.3184  loss_mask_3: 0.4027  loss_dice_3: 3.02  loss_ce_4: 0.3114  loss_mask_4: 0.3987  loss_dice_4: 3.03  loss_ce_5: 0.3005  loss_mask_5: 0.3975  loss_dice_5: 3.028  loss_ce_6: 0.2876  loss_mask_6: 0.3961  loss_dice_6: 3.021  loss_ce_7: 0.2969  loss_mask_7: 0.3975  loss_dice_7: 3.026  loss_ce_8: 0.29  loss_mask_8: 0.3973  loss_dice_8: 3.025  time: 1.4980  data_time: 0.0780  lr: 5.925e-06  max_mem: 21589M
[01/18 02:50:02] d2.utils.events INFO:  eta: 9:19:41  iter: 17659  total_loss: 37.51  loss_ce: 0.3184  loss_mask: 0.3949  loss_dice: 2.97  loss_ce_0: 0.5727  loss_mask_0: 0.3843  loss_dice_0: 3.117  loss_ce_1: 0.3545  loss_mask_1: 0.3968  loss_dice_1: 3.018  loss_ce_2: 0.3466  loss_mask_2: 0.3954  loss_dice_2: 2.995  loss_ce_3: 0.3492  loss_mask_3: 0.3931  loss_dice_3: 2.975  loss_ce_4: 0.3251  loss_mask_4: 0.3927  loss_dice_4: 2.982  loss_ce_5: 0.3228  loss_mask_5: 0.3922  loss_dice_5: 2.981  loss_ce_6: 0.3084  loss_mask_6: 0.3921  loss_dice_6: 2.981  loss_ce_7: 0.3174  loss_mask_7: 0.3956  loss_dice_7: 2.98  loss_ce_8: 0.3208  loss_mask_8: 0.3946  loss_dice_8: 2.978  time: 1.4980  data_time: 0.0828  lr: 5.9202e-06  max_mem: 21589M
[01/18 02:50:33] d2.utils.events INFO:  eta: 9:19:34  iter: 17679  total_loss: 37.64  loss_ce: 0.3074  loss_mask: 0.3914  loss_dice: 3.005  loss_ce_0: 0.5651  loss_mask_0: 0.3855  loss_dice_0: 3.131  loss_ce_1: 0.3186  loss_mask_1: 0.4035  loss_dice_1: 3.037  loss_ce_2: 0.3287  loss_mask_2: 0.3992  loss_dice_2: 3.024  loss_ce_3: 0.3278  loss_mask_3: 0.3924  loss_dice_3: 3.005  loss_ce_4: 0.3179  loss_mask_4: 0.3924  loss_dice_4: 3.003  loss_ce_5: 0.3198  loss_mask_5: 0.393  loss_dice_5: 3.007  loss_ce_6: 0.2963  loss_mask_6: 0.3927  loss_dice_6: 3.006  loss_ce_7: 0.3036  loss_mask_7: 0.3916  loss_dice_7: 3.001  loss_ce_8: 0.3055  loss_mask_8: 0.3916  loss_dice_8: 3  time: 1.4981  data_time: 0.0737  lr: 5.9155e-06  max_mem: 21589M
[01/18 02:51:02] d2.utils.events INFO:  eta: 9:18:41  iter: 17699  total_loss: 37.46  loss_ce: 0.3085  loss_mask: 0.3968  loss_dice: 3.002  loss_ce_0: 0.5728  loss_mask_0: 0.3807  loss_dice_0: 3.154  loss_ce_1: 0.3391  loss_mask_1: 0.3945  loss_dice_1: 3.059  loss_ce_2: 0.3114  loss_mask_2: 0.3923  loss_dice_2: 3.022  loss_ce_3: 0.3247  loss_mask_3: 0.3948  loss_dice_3: 3.012  loss_ce_4: 0.2955  loss_mask_4: 0.3942  loss_dice_4: 3.011  loss_ce_5: 0.31  loss_mask_5: 0.3967  loss_dice_5: 3.014  loss_ce_6: 0.2991  loss_mask_6: 0.3963  loss_dice_6: 3.004  loss_ce_7: 0.2979  loss_mask_7: 0.396  loss_dice_7: 3.001  loss_ce_8: 0.2996  loss_mask_8: 0.3971  loss_dice_8: 3.009  time: 1.4980  data_time: 0.0757  lr: 5.9107e-06  max_mem: 21589M
[01/18 02:51:32] d2.utils.events INFO:  eta: 9:18:08  iter: 17719  total_loss: 37.9  loss_ce: 0.3283  loss_mask: 0.3928  loss_dice: 3.004  loss_ce_0: 0.6083  loss_mask_0: 0.3859  loss_dice_0: 3.131  loss_ce_1: 0.3831  loss_mask_1: 0.3979  loss_dice_1: 3.041  loss_ce_2: 0.3668  loss_mask_2: 0.3973  loss_dice_2: 3.026  loss_ce_3: 0.3485  loss_mask_3: 0.3933  loss_dice_3: 2.995  loss_ce_4: 0.3393  loss_mask_4: 0.3936  loss_dice_4: 3.01  loss_ce_5: 0.3367  loss_mask_5: 0.3926  loss_dice_5: 3.003  loss_ce_6: 0.3262  loss_mask_6: 0.3951  loss_dice_6: 3.005  loss_ce_7: 0.3283  loss_mask_7: 0.3942  loss_dice_7: 2.998  loss_ce_8: 0.3221  loss_mask_8: 0.394  loss_dice_8: 3.002  time: 1.4980  data_time: 0.0709  lr: 5.9059e-06  max_mem: 21589M
[01/18 02:52:02] d2.utils.events INFO:  eta: 9:17:31  iter: 17739  total_loss: 37.49  loss_ce: 0.3038  loss_mask: 0.4017  loss_dice: 2.957  loss_ce_0: 0.5951  loss_mask_0: 0.3885  loss_dice_0: 3.099  loss_ce_1: 0.3633  loss_mask_1: 0.4046  loss_dice_1: 3.006  loss_ce_2: 0.349  loss_mask_2: 0.3978  loss_dice_2: 2.982  loss_ce_3: 0.3191  loss_mask_3: 0.3992  loss_dice_3: 2.963  loss_ce_4: 0.3098  loss_mask_4: 0.3998  loss_dice_4: 2.964  loss_ce_5: 0.307  loss_mask_5: 0.4017  loss_dice_5: 2.971  loss_ce_6: 0.2945  loss_mask_6: 0.401  loss_dice_6: 2.962  loss_ce_7: 0.3096  loss_mask_7: 0.4014  loss_dice_7: 2.965  loss_ce_8: 0.3079  loss_mask_8: 0.4001  loss_dice_8: 2.969  time: 1.4980  data_time: 0.0760  lr: 5.9011e-06  max_mem: 21589M
[01/18 02:52:32] d2.utils.events INFO:  eta: 9:16:39  iter: 17759  total_loss: 36.94  loss_ce: 0.3021  loss_mask: 0.396  loss_dice: 2.959  loss_ce_0: 0.5669  loss_mask_0: 0.3846  loss_dice_0: 3.084  loss_ce_1: 0.3254  loss_mask_1: 0.398  loss_dice_1: 2.992  loss_ce_2: 0.3413  loss_mask_2: 0.3972  loss_dice_2: 2.978  loss_ce_3: 0.3312  loss_mask_3: 0.3939  loss_dice_3: 2.96  loss_ce_4: 0.3055  loss_mask_4: 0.3938  loss_dice_4: 2.956  loss_ce_5: 0.3  loss_mask_5: 0.3952  loss_dice_5: 2.96  loss_ce_6: 0.3002  loss_mask_6: 0.395  loss_dice_6: 2.962  loss_ce_7: 0.3031  loss_mask_7: 0.396  loss_dice_7: 2.962  loss_ce_8: 0.3116  loss_mask_8: 0.3945  loss_dice_8: 2.953  time: 1.4980  data_time: 0.0790  lr: 5.8964e-06  max_mem: 21589M
[01/18 02:53:02] d2.utils.events INFO:  eta: 9:16:31  iter: 17779  total_loss: 38.23  loss_ce: 0.3058  loss_mask: 0.3954  loss_dice: 3.024  loss_ce_0: 0.5874  loss_mask_0: 0.3886  loss_dice_0: 3.168  loss_ce_1: 0.3582  loss_mask_1: 0.3972  loss_dice_1: 3.073  loss_ce_2: 0.3572  loss_mask_2: 0.3953  loss_dice_2: 3.047  loss_ce_3: 0.3341  loss_mask_3: 0.3946  loss_dice_3: 3.027  loss_ce_4: 0.3126  loss_mask_4: 0.3952  loss_dice_4: 3.021  loss_ce_5: 0.3157  loss_mask_5: 0.3932  loss_dice_5: 3.023  loss_ce_6: 0.3049  loss_mask_6: 0.3941  loss_dice_6: 3.016  loss_ce_7: 0.3003  loss_mask_7: 0.3939  loss_dice_7: 3.022  loss_ce_8: 0.3093  loss_mask_8: 0.3941  loss_dice_8: 3.026  time: 1.4980  data_time: 0.0834  lr: 5.8916e-06  max_mem: 21589M
[01/18 02:53:33] d2.utils.events INFO:  eta: 9:16:06  iter: 17799  total_loss: 38.04  loss_ce: 0.3211  loss_mask: 0.3929  loss_dice: 2.991  loss_ce_0: 0.5823  loss_mask_0: 0.3819  loss_dice_0: 3.122  loss_ce_1: 0.3644  loss_mask_1: 0.3972  loss_dice_1: 3.037  loss_ce_2: 0.3407  loss_mask_2: 0.3941  loss_dice_2: 3.008  loss_ce_3: 0.3264  loss_mask_3: 0.3923  loss_dice_3: 2.998  loss_ce_4: 0.313  loss_mask_4: 0.3948  loss_dice_4: 2.999  loss_ce_5: 0.3109  loss_mask_5: 0.3939  loss_dice_5: 2.999  loss_ce_6: 0.3136  loss_mask_6: 0.3933  loss_dice_6: 2.99  loss_ce_7: 0.3344  loss_mask_7: 0.3942  loss_dice_7: 2.991  loss_ce_8: 0.3184  loss_mask_8: 0.3942  loss_dice_8: 2.995  time: 1.4981  data_time: 0.0831  lr: 5.8868e-06  max_mem: 21589M
[01/18 02:54:03] d2.utils.events INFO:  eta: 9:15:38  iter: 17819  total_loss: 38.03  loss_ce: 0.3103  loss_mask: 0.4008  loss_dice: 2.994  loss_ce_0: 0.553  loss_mask_0: 0.3917  loss_dice_0: 3.133  loss_ce_1: 0.3264  loss_mask_1: 0.4067  loss_dice_1: 3.034  loss_ce_2: 0.3379  loss_mask_2: 0.401  loss_dice_2: 3.008  loss_ce_3: 0.304  loss_mask_3: 0.4032  loss_dice_3: 2.999  loss_ce_4: 0.3073  loss_mask_4: 0.4036  loss_dice_4: 3.002  loss_ce_5: 0.3073  loss_mask_5: 0.4021  loss_dice_5: 3  loss_ce_6: 0.3109  loss_mask_6: 0.4037  loss_dice_6: 2.991  loss_ce_7: 0.3001  loss_mask_7: 0.4017  loss_dice_7: 2.994  loss_ce_8: 0.3047  loss_mask_8: 0.4013  loss_dice_8: 2.99  time: 1.4981  data_time: 0.0836  lr: 5.8821e-06  max_mem: 21589M
[01/18 02:54:33] d2.utils.events INFO:  eta: 9:15:11  iter: 17839  total_loss: 37.55  loss_ce: 0.2976  loss_mask: 0.3959  loss_dice: 2.983  loss_ce_0: 0.5868  loss_mask_0: 0.3932  loss_dice_0: 3.114  loss_ce_1: 0.3215  loss_mask_1: 0.3981  loss_dice_1: 3.031  loss_ce_2: 0.3244  loss_mask_2: 0.3956  loss_dice_2: 3.005  loss_ce_3: 0.3037  loss_mask_3: 0.3964  loss_dice_3: 2.997  loss_ce_4: 0.2924  loss_mask_4: 0.3964  loss_dice_4: 2.995  loss_ce_5: 0.2932  loss_mask_5: 0.3964  loss_dice_5: 2.99  loss_ce_6: 0.298  loss_mask_6: 0.3956  loss_dice_6: 2.979  loss_ce_7: 0.2903  loss_mask_7: 0.395  loss_dice_7: 2.981  loss_ce_8: 0.2994  loss_mask_8: 0.3952  loss_dice_8: 2.99  time: 1.4981  data_time: 0.0704  lr: 5.8773e-06  max_mem: 21589M
[01/18 02:55:03] d2.utils.events INFO:  eta: 9:14:36  iter: 17859  total_loss: 37.56  loss_ce: 0.316  loss_mask: 0.3975  loss_dice: 2.989  loss_ce_0: 0.568  loss_mask_0: 0.3894  loss_dice_0: 3.118  loss_ce_1: 0.3457  loss_mask_1: 0.3989  loss_dice_1: 3.027  loss_ce_2: 0.3517  loss_mask_2: 0.4003  loss_dice_2: 3.011  loss_ce_3: 0.3334  loss_mask_3: 0.3951  loss_dice_3: 3  loss_ce_4: 0.32  loss_mask_4: 0.3963  loss_dice_4: 2.996  loss_ce_5: 0.3229  loss_mask_5: 0.3953  loss_dice_5: 2.993  loss_ce_6: 0.3102  loss_mask_6: 0.3934  loss_dice_6: 2.982  loss_ce_7: 0.3082  loss_mask_7: 0.3961  loss_dice_7: 2.987  loss_ce_8: 0.3164  loss_mask_8: 0.3967  loss_dice_8: 2.991  time: 1.4981  data_time: 0.0738  lr: 5.8725e-06  max_mem: 21589M
[01/18 02:55:33] d2.utils.events INFO:  eta: 9:14:07  iter: 17879  total_loss: 37.53  loss_ce: 0.293  loss_mask: 0.3957  loss_dice: 2.981  loss_ce_0: 0.5619  loss_mask_0: 0.3897  loss_dice_0: 3.104  loss_ce_1: 0.3415  loss_mask_1: 0.4067  loss_dice_1: 3.016  loss_ce_2: 0.3277  loss_mask_2: 0.4016  loss_dice_2: 2.998  loss_ce_3: 0.316  loss_mask_3: 0.3996  loss_dice_3: 2.993  loss_ce_4: 0.3198  loss_mask_4: 0.3996  loss_dice_4: 2.976  loss_ce_5: 0.2978  loss_mask_5: 0.3982  loss_dice_5: 2.99  loss_ce_6: 0.2985  loss_mask_6: 0.399  loss_dice_6: 2.978  loss_ce_7: 0.2999  loss_mask_7: 0.3975  loss_dice_7: 2.978  loss_ce_8: 0.3102  loss_mask_8: 0.3964  loss_dice_8: 2.976  time: 1.4981  data_time: 0.0693  lr: 5.8677e-06  max_mem: 21589M
[01/18 02:56:03] d2.utils.events INFO:  eta: 9:13:49  iter: 17899  total_loss: 37.41  loss_ce: 0.2871  loss_mask: 0.4011  loss_dice: 2.964  loss_ce_0: 0.5764  loss_mask_0: 0.3969  loss_dice_0: 3.076  loss_ce_1: 0.3319  loss_mask_1: 0.4075  loss_dice_1: 2.991  loss_ce_2: 0.3228  loss_mask_2: 0.4062  loss_dice_2: 2.974  loss_ce_3: 0.3037  loss_mask_3: 0.4019  loss_dice_3: 2.966  loss_ce_4: 0.2942  loss_mask_4: 0.4025  loss_dice_4: 2.977  loss_ce_5: 0.3052  loss_mask_5: 0.402  loss_dice_5: 2.971  loss_ce_6: 0.2886  loss_mask_6: 0.4023  loss_dice_6: 2.967  loss_ce_7: 0.2893  loss_mask_7: 0.4022  loss_dice_7: 2.961  loss_ce_8: 0.2859  loss_mask_8: 0.4005  loss_dice_8: 2.961  time: 1.4981  data_time: 0.0791  lr: 5.863e-06  max_mem: 21589M
[01/18 02:56:33] d2.utils.events INFO:  eta: 9:12:56  iter: 17919  total_loss: 37.24  loss_ce: 0.3012  loss_mask: 0.3974  loss_dice: 2.94  loss_ce_0: 0.5653  loss_mask_0: 0.3909  loss_dice_0: 3.064  loss_ce_1: 0.3265  loss_mask_1: 0.4007  loss_dice_1: 2.974  loss_ce_2: 0.3143  loss_mask_2: 0.3977  loss_dice_2: 2.953  loss_ce_3: 0.3075  loss_mask_3: 0.3965  loss_dice_3: 2.944  loss_ce_4: 0.3027  loss_mask_4: 0.3969  loss_dice_4: 2.945  loss_ce_5: 0.3028  loss_mask_5: 0.3954  loss_dice_5: 2.943  loss_ce_6: 0.3058  loss_mask_6: 0.3947  loss_dice_6: 2.94  loss_ce_7: 0.2909  loss_mask_7: 0.3951  loss_dice_7: 2.944  loss_ce_8: 0.3006  loss_mask_8: 0.3979  loss_dice_8: 2.947  time: 1.4981  data_time: 0.0847  lr: 5.8582e-06  max_mem: 21589M
[01/18 02:57:03] d2.utils.events INFO:  eta: 9:12:24  iter: 17939  total_loss: 37.84  loss_ce: 0.3158  loss_mask: 0.3945  loss_dice: 3.047  loss_ce_0: 0.5852  loss_mask_0: 0.3918  loss_dice_0: 3.172  loss_ce_1: 0.361  loss_mask_1: 0.4008  loss_dice_1: 3.078  loss_ce_2: 0.3421  loss_mask_2: 0.3977  loss_dice_2: 3.058  loss_ce_3: 0.3177  loss_mask_3: 0.3961  loss_dice_3: 3.052  loss_ce_4: 0.3068  loss_mask_4: 0.3945  loss_dice_4: 3.042  loss_ce_5: 0.3009  loss_mask_5: 0.395  loss_dice_5: 3.049  loss_ce_6: 0.3102  loss_mask_6: 0.3945  loss_dice_6: 3.053  loss_ce_7: 0.3046  loss_mask_7: 0.3935  loss_dice_7: 3.048  loss_ce_8: 0.3003  loss_mask_8: 0.3938  loss_dice_8: 3.051  time: 1.4981  data_time: 0.0739  lr: 5.8534e-06  max_mem: 21589M
[01/18 02:57:33] d2.utils.events INFO:  eta: 9:11:21  iter: 17959  total_loss: 38.04  loss_ce: 0.3413  loss_mask: 0.4077  loss_dice: 2.983  loss_ce_0: 0.6012  loss_mask_0: 0.3927  loss_dice_0: 3.111  loss_ce_1: 0.3797  loss_mask_1: 0.4016  loss_dice_1: 3.016  loss_ce_2: 0.3617  loss_mask_2: 0.3995  loss_dice_2: 2.999  loss_ce_3: 0.3525  loss_mask_3: 0.3998  loss_dice_3: 2.983  loss_ce_4: 0.3499  loss_mask_4: 0.4002  loss_dice_4: 2.981  loss_ce_5: 0.3492  loss_mask_5: 0.4036  loss_dice_5: 2.989  loss_ce_6: 0.341  loss_mask_6: 0.4061  loss_dice_6: 2.978  loss_ce_7: 0.3336  loss_mask_7: 0.4047  loss_dice_7: 2.983  loss_ce_8: 0.34  loss_mask_8: 0.4082  loss_dice_8: 2.984  time: 1.4981  data_time: 0.0743  lr: 5.8486e-06  max_mem: 21589M
[01/18 02:58:03] d2.utils.events INFO:  eta: 9:11:03  iter: 17979  total_loss: 38.01  loss_ce: 0.3024  loss_mask: 0.3895  loss_dice: 3.03  loss_ce_0: 0.5939  loss_mask_0: 0.3908  loss_dice_0: 3.16  loss_ce_1: 0.3494  loss_mask_1: 0.3997  loss_dice_1: 3.063  loss_ce_2: 0.3336  loss_mask_2: 0.3957  loss_dice_2: 3.042  loss_ce_3: 0.3302  loss_mask_3: 0.3916  loss_dice_3: 3.032  loss_ce_4: 0.3139  loss_mask_4: 0.3913  loss_dice_4: 3.024  loss_ce_5: 0.3088  loss_mask_5: 0.391  loss_dice_5: 3.035  loss_ce_6: 0.3237  loss_mask_6: 0.3913  loss_dice_6: 3.015  loss_ce_7: 0.3126  loss_mask_7: 0.3909  loss_dice_7: 3.018  loss_ce_8: 0.3072  loss_mask_8: 0.3894  loss_dice_8: 3.019  time: 1.4981  data_time: 0.0722  lr: 5.8439e-06  max_mem: 21589M
[01/18 02:58:33] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 02:58:34] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 02:58:34] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 02:58:35] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 02:58:49] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0070 s/iter. Inference: 0.2083 s/iter. Eval: 0.2305 s/iter. Total: 0.4458 s/iter. ETA=0:08:02
[01/18 02:58:55] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0097 s/iter. Inference: 0.1891 s/iter. Eval: 0.2478 s/iter. Total: 0.4468 s/iter. ETA=0:07:58
[01/18 02:59:00] d2.evaluation.evaluator INFO: Inference done 35/1093. Dataloading: 0.0099 s/iter. Inference: 0.1908 s/iter. Eval: 0.2373 s/iter. Total: 0.4381 s/iter. ETA=0:07:43
[01/18 02:59:05] d2.evaluation.evaluator INFO: Inference done 48/1093. Dataloading: 0.0104 s/iter. Inference: 0.1821 s/iter. Eval: 0.2311 s/iter. Total: 0.4237 s/iter. ETA=0:07:22
[01/18 02:59:10] d2.evaluation.evaluator INFO: Inference done 60/1093. Dataloading: 0.0110 s/iter. Inference: 0.1781 s/iter. Eval: 0.2339 s/iter. Total: 0.4231 s/iter. ETA=0:07:17
[01/18 02:59:15] d2.evaluation.evaluator INFO: Inference done 72/1093. Dataloading: 0.0110 s/iter. Inference: 0.1795 s/iter. Eval: 0.2356 s/iter. Total: 0.4262 s/iter. ETA=0:07:15
[01/18 02:59:20] d2.evaluation.evaluator INFO: Inference done 86/1093. Dataloading: 0.0107 s/iter. Inference: 0.1744 s/iter. Eval: 0.2341 s/iter. Total: 0.4193 s/iter. ETA=0:07:02
[01/18 02:59:26] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0107 s/iter. Inference: 0.1722 s/iter. Eval: 0.2277 s/iter. Total: 0.4107 s/iter. ETA=0:06:47
[01/18 02:59:31] d2.evaluation.evaluator INFO: Inference done 112/1093. Dataloading: 0.0106 s/iter. Inference: 0.1759 s/iter. Eval: 0.2277 s/iter. Total: 0.4143 s/iter. ETA=0:06:46
[01/18 02:59:36] d2.evaluation.evaluator INFO: Inference done 123/1093. Dataloading: 0.0109 s/iter. Inference: 0.1786 s/iter. Eval: 0.2309 s/iter. Total: 0.4204 s/iter. ETA=0:06:47
[01/18 02:59:41] d2.evaluation.evaluator INFO: Inference done 134/1093. Dataloading: 0.0108 s/iter. Inference: 0.1821 s/iter. Eval: 0.2311 s/iter. Total: 0.4242 s/iter. ETA=0:06:46
[01/18 02:59:47] d2.evaluation.evaluator INFO: Inference done 147/1093. Dataloading: 0.0108 s/iter. Inference: 0.1835 s/iter. Eval: 0.2286 s/iter. Total: 0.4229 s/iter. ETA=0:06:40
[01/18 02:59:52] d2.evaluation.evaluator INFO: Inference done 159/1093. Dataloading: 0.0110 s/iter. Inference: 0.1857 s/iter. Eval: 0.2272 s/iter. Total: 0.4239 s/iter. ETA=0:06:35
[01/18 02:59:57] d2.evaluation.evaluator INFO: Inference done 169/1093. Dataloading: 0.0111 s/iter. Inference: 0.1878 s/iter. Eval: 0.2298 s/iter. Total: 0.4288 s/iter. ETA=0:06:36
[01/18 03:00:02] d2.evaluation.evaluator INFO: Inference done 180/1093. Dataloading: 0.0111 s/iter. Inference: 0.1882 s/iter. Eval: 0.2318 s/iter. Total: 0.4312 s/iter. ETA=0:06:33
[01/18 03:00:07] d2.evaluation.evaluator INFO: Inference done 194/1093. Dataloading: 0.0110 s/iter. Inference: 0.1862 s/iter. Eval: 0.2303 s/iter. Total: 0.4276 s/iter. ETA=0:06:24
[01/18 03:00:12] d2.evaluation.evaluator INFO: Inference done 205/1093. Dataloading: 0.0111 s/iter. Inference: 0.1855 s/iter. Eval: 0.2325 s/iter. Total: 0.4293 s/iter. ETA=0:06:21
[01/18 03:00:17] d2.evaluation.evaluator INFO: Inference done 216/1093. Dataloading: 0.0112 s/iter. Inference: 0.1861 s/iter. Eval: 0.2335 s/iter. Total: 0.4309 s/iter. ETA=0:06:17
[01/18 03:00:22] d2.evaluation.evaluator INFO: Inference done 227/1093. Dataloading: 0.0112 s/iter. Inference: 0.1875 s/iter. Eval: 0.2333 s/iter. Total: 0.4322 s/iter. ETA=0:06:14
[01/18 03:00:28] d2.evaluation.evaluator INFO: Inference done 238/1093. Dataloading: 0.0112 s/iter. Inference: 0.1881 s/iter. Eval: 0.2344 s/iter. Total: 0.4339 s/iter. ETA=0:06:10
[01/18 03:00:33] d2.evaluation.evaluator INFO: Inference done 249/1093. Dataloading: 0.0114 s/iter. Inference: 0.1891 s/iter. Eval: 0.2360 s/iter. Total: 0.4366 s/iter. ETA=0:06:08
[01/18 03:00:38] d2.evaluation.evaluator INFO: Inference done 260/1093. Dataloading: 0.0114 s/iter. Inference: 0.1897 s/iter. Eval: 0.2368 s/iter. Total: 0.4380 s/iter. ETA=0:06:04
[01/18 03:00:44] d2.evaluation.evaluator INFO: Inference done 272/1093. Dataloading: 0.0115 s/iter. Inference: 0.1903 s/iter. Eval: 0.2369 s/iter. Total: 0.4388 s/iter. ETA=0:06:00
[01/18 03:00:49] d2.evaluation.evaluator INFO: Inference done 283/1093. Dataloading: 0.0117 s/iter. Inference: 0.1901 s/iter. Eval: 0.2378 s/iter. Total: 0.4397 s/iter. ETA=0:05:56
[01/18 03:00:54] d2.evaluation.evaluator INFO: Inference done 294/1093. Dataloading: 0.0118 s/iter. Inference: 0.1911 s/iter. Eval: 0.2385 s/iter. Total: 0.4416 s/iter. ETA=0:05:52
[01/18 03:01:00] d2.evaluation.evaluator INFO: Inference done 304/1093. Dataloading: 0.0118 s/iter. Inference: 0.1921 s/iter. Eval: 0.2409 s/iter. Total: 0.4449 s/iter. ETA=0:05:51
[01/18 03:01:05] d2.evaluation.evaluator INFO: Inference done 315/1093. Dataloading: 0.0119 s/iter. Inference: 0.1924 s/iter. Eval: 0.2420 s/iter. Total: 0.4465 s/iter. ETA=0:05:47
[01/18 03:01:10] d2.evaluation.evaluator INFO: Inference done 325/1093. Dataloading: 0.0121 s/iter. Inference: 0.1934 s/iter. Eval: 0.2432 s/iter. Total: 0.4487 s/iter. ETA=0:05:44
[01/18 03:01:15] d2.evaluation.evaluator INFO: Inference done 338/1093. Dataloading: 0.0120 s/iter. Inference: 0.1927 s/iter. Eval: 0.2416 s/iter. Total: 0.4465 s/iter. ETA=0:05:37
[01/18 03:01:21] d2.evaluation.evaluator INFO: Inference done 351/1093. Dataloading: 0.0121 s/iter. Inference: 0.1928 s/iter. Eval: 0.2403 s/iter. Total: 0.4453 s/iter. ETA=0:05:30
[01/18 03:01:26] d2.evaluation.evaluator INFO: Inference done 362/1093. Dataloading: 0.0121 s/iter. Inference: 0.1935 s/iter. Eval: 0.2407 s/iter. Total: 0.4464 s/iter. ETA=0:05:26
[01/18 03:01:31] d2.evaluation.evaluator INFO: Inference done 372/1093. Dataloading: 0.0128 s/iter. Inference: 0.1932 s/iter. Eval: 0.2418 s/iter. Total: 0.4479 s/iter. ETA=0:05:22
[01/18 03:01:36] d2.evaluation.evaluator INFO: Inference done 384/1093. Dataloading: 0.0127 s/iter. Inference: 0.1937 s/iter. Eval: 0.2411 s/iter. Total: 0.4477 s/iter. ETA=0:05:17
[01/18 03:01:42] d2.evaluation.evaluator INFO: Inference done 396/1093. Dataloading: 0.0127 s/iter. Inference: 0.1935 s/iter. Eval: 0.2415 s/iter. Total: 0.4478 s/iter. ETA=0:05:12
[01/18 03:01:47] d2.evaluation.evaluator INFO: Inference done 407/1093. Dataloading: 0.0127 s/iter. Inference: 0.1934 s/iter. Eval: 0.2418 s/iter. Total: 0.4480 s/iter. ETA=0:05:07
[01/18 03:01:52] d2.evaluation.evaluator INFO: Inference done 418/1093. Dataloading: 0.0127 s/iter. Inference: 0.1934 s/iter. Eval: 0.2429 s/iter. Total: 0.4491 s/iter. ETA=0:05:03
[01/18 03:01:58] d2.evaluation.evaluator INFO: Inference done 429/1093. Dataloading: 0.0127 s/iter. Inference: 0.1932 s/iter. Eval: 0.2444 s/iter. Total: 0.4504 s/iter. ETA=0:04:59
[01/18 03:02:03] d2.evaluation.evaluator INFO: Inference done 440/1093. Dataloading: 0.0128 s/iter. Inference: 0.1942 s/iter. Eval: 0.2437 s/iter. Total: 0.4508 s/iter. ETA=0:04:54
[01/18 03:02:08] d2.evaluation.evaluator INFO: Inference done 451/1093. Dataloading: 0.0128 s/iter. Inference: 0.1942 s/iter. Eval: 0.2441 s/iter. Total: 0.4512 s/iter. ETA=0:04:49
[01/18 03:02:13] d2.evaluation.evaluator INFO: Inference done 461/1093. Dataloading: 0.0127 s/iter. Inference: 0.1947 s/iter. Eval: 0.2449 s/iter. Total: 0.4525 s/iter. ETA=0:04:45
[01/18 03:02:18] d2.evaluation.evaluator INFO: Inference done 473/1093. Dataloading: 0.0127 s/iter. Inference: 0.1950 s/iter. Eval: 0.2444 s/iter. Total: 0.4522 s/iter. ETA=0:04:40
[01/18 03:02:24] d2.evaluation.evaluator INFO: Inference done 485/1093. Dataloading: 0.0126 s/iter. Inference: 0.1954 s/iter. Eval: 0.2439 s/iter. Total: 0.4520 s/iter. ETA=0:04:34
[01/18 03:02:29] d2.evaluation.evaluator INFO: Inference done 497/1093. Dataloading: 0.0126 s/iter. Inference: 0.1956 s/iter. Eval: 0.2432 s/iter. Total: 0.4516 s/iter. ETA=0:04:29
[01/18 03:02:34] d2.evaluation.evaluator INFO: Inference done 509/1093. Dataloading: 0.0126 s/iter. Inference: 0.1961 s/iter. Eval: 0.2425 s/iter. Total: 0.4514 s/iter. ETA=0:04:23
[01/18 03:02:40] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0126 s/iter. Inference: 0.1966 s/iter. Eval: 0.2425 s/iter. Total: 0.4517 s/iter. ETA=0:04:18
[01/18 03:02:45] d2.evaluation.evaluator INFO: Inference done 530/1093. Dataloading: 0.0126 s/iter. Inference: 0.1973 s/iter. Eval: 0.2441 s/iter. Total: 0.4541 s/iter. ETA=0:04:15
[01/18 03:02:50] d2.evaluation.evaluator INFO: Inference done 538/1093. Dataloading: 0.0127 s/iter. Inference: 0.2005 s/iter. Eval: 0.2442 s/iter. Total: 0.4576 s/iter. ETA=0:04:13
[01/18 03:02:56] d2.evaluation.evaluator INFO: Inference done 545/1093. Dataloading: 0.0128 s/iter. Inference: 0.2033 s/iter. Eval: 0.2456 s/iter. Total: 0.4619 s/iter. ETA=0:04:13
[01/18 03:03:01] d2.evaluation.evaluator INFO: Inference done 552/1093. Dataloading: 0.0129 s/iter. Inference: 0.2059 s/iter. Eval: 0.2462 s/iter. Total: 0.4652 s/iter. ETA=0:04:11
[01/18 03:03:06] d2.evaluation.evaluator INFO: Inference done 559/1093. Dataloading: 0.0129 s/iter. Inference: 0.2089 s/iter. Eval: 0.2467 s/iter. Total: 0.4687 s/iter. ETA=0:04:10
[01/18 03:03:12] d2.evaluation.evaluator INFO: Inference done 566/1093. Dataloading: 0.0130 s/iter. Inference: 0.2126 s/iter. Eval: 0.2473 s/iter. Total: 0.4730 s/iter. ETA=0:04:09
[01/18 03:03:17] d2.evaluation.evaluator INFO: Inference done 577/1093. Dataloading: 0.0131 s/iter. Inference: 0.2138 s/iter. Eval: 0.2457 s/iter. Total: 0.4727 s/iter. ETA=0:04:03
[01/18 03:03:22] d2.evaluation.evaluator INFO: Inference done 592/1093. Dataloading: 0.0130 s/iter. Inference: 0.2126 s/iter. Eval: 0.2435 s/iter. Total: 0.4693 s/iter. ETA=0:03:55
[01/18 03:03:27] d2.evaluation.evaluator INFO: Inference done 606/1093. Dataloading: 0.0129 s/iter. Inference: 0.2112 s/iter. Eval: 0.2430 s/iter. Total: 0.4673 s/iter. ETA=0:03:47
[01/18 03:03:32] d2.evaluation.evaluator INFO: Inference done 622/1093. Dataloading: 0.0128 s/iter. Inference: 0.2094 s/iter. Eval: 0.2411 s/iter. Total: 0.4635 s/iter. ETA=0:03:38
[01/18 03:03:37] d2.evaluation.evaluator INFO: Inference done 638/1093. Dataloading: 0.0127 s/iter. Inference: 0.2075 s/iter. Eval: 0.2393 s/iter. Total: 0.4597 s/iter. ETA=0:03:29
[01/18 03:03:43] d2.evaluation.evaluator INFO: Inference done 649/1093. Dataloading: 0.0127 s/iter. Inference: 0.2080 s/iter. Eval: 0.2388 s/iter. Total: 0.4597 s/iter. ETA=0:03:24
[01/18 03:03:48] d2.evaluation.evaluator INFO: Inference done 662/1093. Dataloading: 0.0127 s/iter. Inference: 0.2075 s/iter. Eval: 0.2384 s/iter. Total: 0.4586 s/iter. ETA=0:03:17
[01/18 03:03:53] d2.evaluation.evaluator INFO: Inference done 676/1093. Dataloading: 0.0126 s/iter. Inference: 0.2067 s/iter. Eval: 0.2376 s/iter. Total: 0.4570 s/iter. ETA=0:03:10
[01/18 03:03:59] d2.evaluation.evaluator INFO: Inference done 692/1093. Dataloading: 0.0128 s/iter. Inference: 0.2056 s/iter. Eval: 0.2360 s/iter. Total: 0.4546 s/iter. ETA=0:03:02
[01/18 03:04:04] d2.evaluation.evaluator INFO: Inference done 703/1093. Dataloading: 0.0128 s/iter. Inference: 0.2059 s/iter. Eval: 0.2362 s/iter. Total: 0.4550 s/iter. ETA=0:02:57
[01/18 03:04:09] d2.evaluation.evaluator INFO: Inference done 715/1093. Dataloading: 0.0128 s/iter. Inference: 0.2053 s/iter. Eval: 0.2365 s/iter. Total: 0.4547 s/iter. ETA=0:02:51
[01/18 03:04:14] d2.evaluation.evaluator INFO: Inference done 729/1093. Dataloading: 0.0127 s/iter. Inference: 0.2049 s/iter. Eval: 0.2352 s/iter. Total: 0.4529 s/iter. ETA=0:02:44
[01/18 03:04:20] d2.evaluation.evaluator INFO: Inference done 743/1093. Dataloading: 0.0127 s/iter. Inference: 0.2042 s/iter. Eval: 0.2342 s/iter. Total: 0.4512 s/iter. ETA=0:02:37
[01/18 03:04:25] d2.evaluation.evaluator INFO: Inference done 756/1093. Dataloading: 0.0126 s/iter. Inference: 0.2038 s/iter. Eval: 0.2337 s/iter. Total: 0.4502 s/iter. ETA=0:02:31
[01/18 03:04:30] d2.evaluation.evaluator INFO: Inference done 769/1093. Dataloading: 0.0126 s/iter. Inference: 0.2034 s/iter. Eval: 0.2337 s/iter. Total: 0.4497 s/iter. ETA=0:02:25
[01/18 03:04:35] d2.evaluation.evaluator INFO: Inference done 784/1093. Dataloading: 0.0125 s/iter. Inference: 0.2026 s/iter. Eval: 0.2327 s/iter. Total: 0.4480 s/iter. ETA=0:02:18
[01/18 03:04:41] d2.evaluation.evaluator INFO: Inference done 797/1093. Dataloading: 0.0125 s/iter. Inference: 0.2024 s/iter. Eval: 0.2320 s/iter. Total: 0.4470 s/iter. ETA=0:02:12
[01/18 03:04:46] d2.evaluation.evaluator INFO: Inference done 811/1093. Dataloading: 0.0124 s/iter. Inference: 0.2018 s/iter. Eval: 0.2312 s/iter. Total: 0.4456 s/iter. ETA=0:02:05
[01/18 03:04:51] d2.evaluation.evaluator INFO: Inference done 825/1093. Dataloading: 0.0124 s/iter. Inference: 0.2012 s/iter. Eval: 0.2305 s/iter. Total: 0.4442 s/iter. ETA=0:01:59
[01/18 03:04:56] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0123 s/iter. Inference: 0.2008 s/iter. Eval: 0.2298 s/iter. Total: 0.4430 s/iter. ETA=0:01:52
[01/18 03:05:01] d2.evaluation.evaluator INFO: Inference done 852/1093. Dataloading: 0.0123 s/iter. Inference: 0.2005 s/iter. Eval: 0.2294 s/iter. Total: 0.4422 s/iter. ETA=0:01:46
[01/18 03:05:06] d2.evaluation.evaluator INFO: Inference done 864/1093. Dataloading: 0.0123 s/iter. Inference: 0.2000 s/iter. Eval: 0.2296 s/iter. Total: 0.4420 s/iter. ETA=0:01:41
[01/18 03:05:12] d2.evaluation.evaluator INFO: Inference done 879/1093. Dataloading: 0.0122 s/iter. Inference: 0.1992 s/iter. Eval: 0.2290 s/iter. Total: 0.4405 s/iter. ETA=0:01:34
[01/18 03:05:17] d2.evaluation.evaluator INFO: Inference done 890/1093. Dataloading: 0.0122 s/iter. Inference: 0.1988 s/iter. Eval: 0.2297 s/iter. Total: 0.4410 s/iter. ETA=0:01:29
[01/18 03:05:22] d2.evaluation.evaluator INFO: Inference done 904/1093. Dataloading: 0.0122 s/iter. Inference: 0.1983 s/iter. Eval: 0.2292 s/iter. Total: 0.4398 s/iter. ETA=0:01:23
[01/18 03:05:27] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0122 s/iter. Inference: 0.1976 s/iter. Eval: 0.2291 s/iter. Total: 0.4390 s/iter. ETA=0:01:17
[01/18 03:05:32] d2.evaluation.evaluator INFO: Inference done 930/1093. Dataloading: 0.0121 s/iter. Inference: 0.1973 s/iter. Eval: 0.2287 s/iter. Total: 0.4383 s/iter. ETA=0:01:11
[01/18 03:05:37] d2.evaluation.evaluator INFO: Inference done 943/1093. Dataloading: 0.0121 s/iter. Inference: 0.1969 s/iter. Eval: 0.2288 s/iter. Total: 0.4379 s/iter. ETA=0:01:05
[01/18 03:05:42] d2.evaluation.evaluator INFO: Inference done 957/1093. Dataloading: 0.0121 s/iter. Inference: 0.1962 s/iter. Eval: 0.2285 s/iter. Total: 0.4369 s/iter. ETA=0:00:59
[01/18 03:05:48] d2.evaluation.evaluator INFO: Inference done 971/1093. Dataloading: 0.0121 s/iter. Inference: 0.1956 s/iter. Eval: 0.2282 s/iter. Total: 0.4360 s/iter. ETA=0:00:53
[01/18 03:05:53] d2.evaluation.evaluator INFO: Inference done 987/1093. Dataloading: 0.0120 s/iter. Inference: 0.1949 s/iter. Eval: 0.2272 s/iter. Total: 0.4342 s/iter. ETA=0:00:46
[01/18 03:05:58] d2.evaluation.evaluator INFO: Inference done 1001/1093. Dataloading: 0.0120 s/iter. Inference: 0.1942 s/iter. Eval: 0.2268 s/iter. Total: 0.4332 s/iter. ETA=0:00:39
[01/18 03:06:03] d2.evaluation.evaluator INFO: Inference done 1014/1093. Dataloading: 0.0120 s/iter. Inference: 0.1939 s/iter. Eval: 0.2266 s/iter. Total: 0.4326 s/iter. ETA=0:00:34
[01/18 03:06:08] d2.evaluation.evaluator INFO: Inference done 1028/1093. Dataloading: 0.0119 s/iter. Inference: 0.1936 s/iter. Eval: 0.2262 s/iter. Total: 0.4319 s/iter. ETA=0:00:28
[01/18 03:06:14] d2.evaluation.evaluator INFO: Inference done 1041/1093. Dataloading: 0.0119 s/iter. Inference: 0.1931 s/iter. Eval: 0.2263 s/iter. Total: 0.4315 s/iter. ETA=0:00:22
[01/18 03:06:19] d2.evaluation.evaluator INFO: Inference done 1055/1093. Dataloading: 0.0119 s/iter. Inference: 0.1928 s/iter. Eval: 0.2259 s/iter. Total: 0.4307 s/iter. ETA=0:00:16
[01/18 03:06:24] d2.evaluation.evaluator INFO: Inference done 1072/1093. Dataloading: 0.0118 s/iter. Inference: 0.1920 s/iter. Eval: 0.2247 s/iter. Total: 0.4286 s/iter. ETA=0:00:09
[01/18 03:06:29] d2.evaluation.evaluator INFO: Inference done 1088/1093. Dataloading: 0.0118 s/iter. Inference: 0.1913 s/iter. Eval: 0.2238 s/iter. Total: 0.4270 s/iter. ETA=0:00:02
[01/18 03:06:31] d2.evaluation.evaluator INFO: Total inference time: 0:07:44.251008 (0.426701 s / iter per device, on 4 devices)
[01/18 03:06:31] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:27 (0.190967 s / iter per device, on 4 devices)
[01/18 03:06:55] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.5680566821352517, 'mIoU': 17.614119232759286, 'fwIoU': 38.29867203270143, 'IoU-0': nan, 'IoU-1': 94.7886069676258, 'IoU-2': 46.79130929936028, 'IoU-3': 57.73880350318098, 'IoU-4': 52.275468877995436, 'IoU-5': 46.254780233424384, 'IoU-6': 41.85068987540559, 'IoU-7': 33.40173102344322, 'IoU-8': 17.74082464094426, 'IoU-9': 27.289552143590463, 'IoU-10': 30.767504610481705, 'IoU-11': 42.395088412472624, 'IoU-12': 45.642573789834984, 'IoU-13': 44.64875009355032, 'IoU-14': 42.739754415368466, 'IoU-15': 42.91811515778598, 'IoU-16': 43.75505649462999, 'IoU-17': 39.61745783766449, 'IoU-18': 40.13124554173412, 'IoU-19': 40.77897695526684, 'IoU-20': 40.464015060435884, 'IoU-21': 41.038575202260915, 'IoU-22': 41.56199159883961, 'IoU-23': 39.15412267092934, 'IoU-24': 40.12524759596175, 'IoU-25': 39.28317286157727, 'IoU-26': 39.766417513916295, 'IoU-27': 39.52122243366783, 'IoU-28': 38.554141321817774, 'IoU-29': 40.41255207262901, 'IoU-30': 36.95138937326274, 'IoU-31': 38.81993192021545, 'IoU-32': 38.55496602566898, 'IoU-33': 37.0880479941684, 'IoU-34': 36.396613327436185, 'IoU-35': 38.345734786419044, 'IoU-36': 37.95787433454787, 'IoU-37': 36.9760227716246, 'IoU-38': 35.77477894854725, 'IoU-39': 35.79577565273484, 'IoU-40': 35.792333238894095, 'IoU-41': 33.79732506239889, 'IoU-42': 33.864654211634, 'IoU-43': 33.57941737777777, 'IoU-44': 33.26148012787918, 'IoU-45': 31.660734453171713, 'IoU-46': 30.89539589773465, 'IoU-47': 29.81720175811186, 'IoU-48': 30.944910264984028, 'IoU-49': 29.943494721066603, 'IoU-50': 30.36906665688947, 'IoU-51': 29.624330473194014, 'IoU-52': 28.16152932302078, 'IoU-53': 27.664722630904876, 'IoU-54': 27.23860995206643, 'IoU-55': 28.132373921397622, 'IoU-56': 26.937844576634628, 'IoU-57': 26.279304222536247, 'IoU-58': 24.437331507883137, 'IoU-59': 23.758543427694473, 'IoU-60': 22.972913470614746, 'IoU-61': 23.921598060972983, 'IoU-62': 25.32188812969428, 'IoU-63': 22.48686825303525, 'IoU-64': 22.176719602520677, 'IoU-65': 22.222804646231147, 'IoU-66': 20.295599510540235, 'IoU-67': 19.167290404508073, 'IoU-68': 18.62524270549065, 'IoU-69': 19.57344218605869, 'IoU-70': 19.040421079694475, 'IoU-71': 17.056965152161357, 'IoU-72': 17.613804834354653, 'IoU-73': 16.536159034783175, 'IoU-74': 16.43691590413729, 'IoU-75': 16.751143448290787, 'IoU-76': 15.67500977446921, 'IoU-77': 15.48640367650587, 'IoU-78': 14.895453684292784, 'IoU-79': 15.935062882928843, 'IoU-80': 15.450564586186596, 'IoU-81': 17.85527476363046, 'IoU-82': 16.40257967642998, 'IoU-83': 15.424247605526043, 'IoU-84': 17.089541578548236, 'IoU-85': 14.696809567756846, 'IoU-86': 15.327898628482057, 'IoU-87': 15.329884671240206, 'IoU-88': 15.985282466190446, 'IoU-89': 14.482034024999887, 'IoU-90': 15.98894432589367, 'IoU-91': 13.31482228934959, 'IoU-92': 15.68866140590654, 'IoU-93': 16.018150513496675, 'IoU-94': 14.1320323956875, 'IoU-95': 14.860693527130376, 'IoU-96': 14.424335426947263, 'IoU-97': 16.115691315153068, 'IoU-98': 14.957276458646168, 'IoU-99': 12.898066505283618, 'IoU-100': 13.442998120261377, 'IoU-101': 12.01298663408338, 'IoU-102': 13.974071166340845, 'IoU-103': 11.506409393994389, 'IoU-104': 12.864107077444991, 'IoU-105': 11.290079134053519, 'IoU-106': 11.872973073581774, 'IoU-107': 13.658485698742535, 'IoU-108': 10.189590503213184, 'IoU-109': 11.966260197875153, 'IoU-110': 10.278122477683317, 'IoU-111': 11.387523607736865, 'IoU-112': 11.1350266504752, 'IoU-113': 10.11602669215094, 'IoU-114': 10.400881664141066, 'IoU-115': 10.703613577512417, 'IoU-116': 10.05894269511725, 'IoU-117': 9.739818489453702, 'IoU-118': 8.819324955304323, 'IoU-119': 9.446277071760022, 'IoU-120': 10.373616189882874, 'IoU-121': 6.860424156626105, 'IoU-122': 9.484617381335614, 'IoU-123': 8.4210567191458, 'IoU-124': 8.4697301712229, 'IoU-125': 8.32410178642751, 'IoU-126': 7.305721001860254, 'IoU-127': 5.847395579647389, 'IoU-128': 6.8773807422279605, 'IoU-129': 5.087041334809839, 'IoU-130': 6.451446270101141, 'IoU-131': 7.0522006219182005, 'IoU-132': 5.602796821793417, 'IoU-133': 6.240021386445969, 'IoU-134': 4.586655918361739, 'IoU-135': 3.927165780026815, 'IoU-136': 5.506662702284565, 'IoU-137': 4.732525607257829, 'IoU-138': 4.825445619714704, 'IoU-139': 5.5309694171535275, 'IoU-140': 5.0058660665304355, 'IoU-141': 3.96244045880485, 'IoU-142': 3.8108224725068043, 'IoU-143': 4.079140833155477, 'IoU-144': 5.271575300129414, 'IoU-145': 3.689356645589273, 'IoU-146': 4.396207621882335, 'IoU-147': 4.6541665719056065, 'IoU-148': 4.755954748433738, 'IoU-149': 2.9092256625882236, 'IoU-150': 1.7363607310188558, 'IoU-151': 4.160003178010096, 'IoU-152': 1.9415951703302643, 'IoU-153': 2.8039081576347518, 'IoU-154': 3.253145304372778, 'IoU-155': 1.9746448596486672, 'IoU-156': 3.0849856138422913, 'IoU-157': 2.633230794687495, 'IoU-158': 1.797445147715741, 'IoU-159': 1.8583486111964371, 'IoU-160': 3.1382440566385212, 'IoU-161': 3.17254463238721, 'IoU-162': 3.048796213471387, 'IoU-163': 1.4971654297019825, 'IoU-164': 3.1272082264675567, 'IoU-165': 1.508418325201858, 'IoU-166': 2.0436386127483774, 'IoU-167': 2.027523313408125, 'IoU-168': 1.694131878151052, 'IoU-169': 2.3217087285199742, 'IoU-170': 1.5870319590380741, 'IoU-171': 0.953862529168543, 'IoU-172': 0.7635057440950281, 'IoU-173': 0.7386669567720089, 'IoU-174': 1.5514809590973202, 'IoU-175': 0.9714244885877308, 'IoU-176': 0.7488279105838037, 'IoU-177': 2.494022641613439, 'IoU-178': 2.1171753702025917, 'IoU-179': 1.5236783775132419, 'IoU-180': 1.7735872805185742, 'IoU-181': 1.3430436953900564, 'IoU-182': 0.588676873657932, 'IoU-183': 2.956835802282345, 'IoU-184': 0.0, 'IoU-185': 0.039075463820402744, 'IoU-186': 0.9259669219901546, 'IoU-187': 2.4174809645049455, 'IoU-188': 2.384971822166562, 'IoU-189': 0.8161529731430485, 'IoU-190': 0.43694258668377123, 'IoU-191': 0.5720793602780412, 'mACC': 27.281010673968115, 'pACC': 52.45471776664071, 'ACC-0': nan, 'ACC-1': 98.58164408034168, 'ACC-2': 63.50840388961861, 'ACC-3': 74.06233607779069, 'ACC-4': 70.25559949419166, 'ACC-5': 66.14181802687003, 'ACC-6': 61.81633803982843, 'ACC-7': 48.04795606944004, 'ACC-8': 21.46664268177715, 'ACC-9': 36.2027944207952, 'ACC-10': 41.93480052516673, 'ACC-11': 57.39217648399957, 'ACC-12': 67.8026434168558, 'ACC-13': 63.97800849571412, 'ACC-14': 62.53300996170452, 'ACC-15': 60.17051625376057, 'ACC-16': 60.23963387381684, 'ACC-17': 57.82892780432998, 'ACC-18': 54.466629188280535, 'ACC-19': 58.04846122954691, 'ACC-20': 55.31552128124988, 'ACC-21': 56.62393599845987, 'ACC-22': 56.087450231203896, 'ACC-23': 57.46796400845182, 'ACC-24': 58.7108917143889, 'ACC-25': 56.700993446988065, 'ACC-26': 58.5100226323636, 'ACC-27': 57.08227003504902, 'ACC-28': 57.147233957003074, 'ACC-29': 59.452531419105846, 'ACC-30': 54.863802006605134, 'ACC-31': 56.4862731210701, 'ACC-32': 56.420557672996466, 'ACC-33': 53.30324876799468, 'ACC-34': 51.044465723419094, 'ACC-35': 53.36673496760804, 'ACC-36': 56.64242098665754, 'ACC-37': 52.6956893042034, 'ACC-38': 51.957260317745856, 'ACC-39': 51.24606094977139, 'ACC-40': 54.60387410316629, 'ACC-41': 48.831878129597754, 'ACC-42': 49.43177357100797, 'ACC-43': 49.36713211187476, 'ACC-44': 47.71175149412932, 'ACC-45': 46.887820216525164, 'ACC-46': 46.77380910738867, 'ACC-47': 45.94899105782426, 'ACC-48': 46.56733980547476, 'ACC-49': 47.10625488333947, 'ACC-50': 48.52580394104158, 'ACC-51': 46.63881102609553, 'ACC-52': 41.70066078972839, 'ACC-53': 44.09110214858475, 'ACC-54': 41.7143199323858, 'ACC-55': 46.59049527499965, 'ACC-56': 43.6662005836866, 'ACC-57': 43.935752344331796, 'ACC-58': 41.21516224762535, 'ACC-59': 39.245905535343056, 'ACC-60': 37.27189857366453, 'ACC-61': 40.46408567139622, 'ACC-62': 40.47683598511907, 'ACC-63': 36.225258866080246, 'ACC-64': 35.63439671460233, 'ACC-65': 36.82007946042484, 'ACC-66': 34.85177620398106, 'ACC-67': 30.29951057431604, 'ACC-68': 30.111503631874804, 'ACC-69': 33.20538131449303, 'ACC-70': 32.2506554313859, 'ACC-71': 32.96637589468479, 'ACC-72': 30.35174489307736, 'ACC-73': 29.332232519759614, 'ACC-74': 26.96951631250744, 'ACC-75': 30.262356413781653, 'ACC-76': 24.346553083401005, 'ACC-77': 26.284037054899212, 'ACC-78': 23.51116615676965, 'ACC-79': 25.452518914567207, 'ACC-80': 26.362004907316866, 'ACC-81': 28.283943681947264, 'ACC-82': 26.12496340562499, 'ACC-83': 24.407075330111176, 'ACC-84': 29.34897566783671, 'ACC-85': 26.28165514266021, 'ACC-86': 29.515880295531744, 'ACC-87': 26.010557028606552, 'ACC-88': 28.628988369933005, 'ACC-89': 24.237865563594518, 'ACC-90': 29.410866624564125, 'ACC-91': 22.314371784680006, 'ACC-92': 27.234959222308063, 'ACC-93': 27.52534616320102, 'ACC-94': 25.857329747403607, 'ACC-95': 27.303325159206594, 'ACC-96': 26.266651279153635, 'ACC-97': 28.39074727234781, 'ACC-98': 27.791978587517796, 'ACC-99': 23.414251851463312, 'ACC-100': 22.76703426274402, 'ACC-101': 19.912293597858216, 'ACC-102': 24.9570231082918, 'ACC-103': 18.984249551336546, 'ACC-104': 22.35215562824328, 'ACC-105': 17.51773888272685, 'ACC-106': 20.82539123129001, 'ACC-107': 26.67103442423926, 'ACC-108': 15.661225167731724, 'ACC-109': 20.5638123101478, 'ACC-110': 17.365030204726107, 'ACC-111': 19.848562199593154, 'ACC-112': 22.480963124750172, 'ACC-113': 16.922188642735648, 'ACC-114': 18.462301525839862, 'ACC-115': 21.914362605806385, 'ACC-116': 20.459729818532253, 'ACC-117': 20.371132347043826, 'ACC-118': 15.873723651665179, 'ACC-119': 15.836506133078737, 'ACC-120': 19.982289503368158, 'ACC-121': 10.76572007883261, 'ACC-122': 16.30676403709889, 'ACC-123': 16.05037016276899, 'ACC-124': 17.004146287280225, 'ACC-125': 15.61907538154135, 'ACC-126': 13.296683029900091, 'ACC-127': 9.949630404919214, 'ACC-128': 11.831816668313227, 'ACC-129': 7.676653327551726, 'ACC-130': 11.840211225276866, 'ACC-131': 12.31071971864402, 'ACC-132': 10.338155509310921, 'ACC-133': 11.467132077269047, 'ACC-134': 7.593554502369668, 'ACC-135': 6.1470269653953595, 'ACC-136': 11.203716106038017, 'ACC-137': 7.713913038832498, 'ACC-138': 9.373334264030255, 'ACC-139': 9.94709584645416, 'ACC-140': 10.633656182756958, 'ACC-141': 6.0737470524574215, 'ACC-142': 6.016186037250417, 'ACC-143': 8.982138701158195, 'ACC-144': 11.007039711191336, 'ACC-145': 7.917618909188488, 'ACC-146': 7.574786959402344, 'ACC-147': 11.020972475888662, 'ACC-148': 9.79743377459838, 'ACC-149': 5.031696564844854, 'ACC-150': 2.7012462880794326, 'ACC-151': 7.366114487846417, 'ACC-152': 3.1800020230629173, 'ACC-153': 4.8587826819839135, 'ACC-154': 11.341521979834841, 'ACC-155': 2.905244007957344, 'ACC-156': 6.263840679477981, 'ACC-157': 5.977953896177251, 'ACC-158': 2.792318407566992, 'ACC-159': 2.444377025521213, 'ACC-160': 5.709914943672804, 'ACC-161': 8.92770383933812, 'ACC-162': 6.6828126069247045, 'ACC-163': 2.3766102539565583, 'ACC-164': 5.171249274636208, 'ACC-165': 2.291099556264234, 'ACC-166': 3.738741385533572, 'ACC-167': 3.850489953393194, 'ACC-168': 3.2996342921443143, 'ACC-169': 6.637210844842804, 'ACC-170': 3.8300183142382553, 'ACC-171': 1.540753367413103, 'ACC-172': 1.0723413712634737, 'ACC-173': 1.0092077982186467, 'ACC-174': 2.70037522298087, 'ACC-175': 1.2316987718140286, 'ACC-176': 0.809184176921291, 'ACC-177': 10.019764924732732, 'ACC-178': 4.351269337526149, 'ACC-179': 2.559238436864071, 'ACC-180': 3.9368078837752716, 'ACC-181': 2.5671453712980923, 'ACC-182': 0.798883647761491, 'ACC-183': 12.14664591725505, 'ACC-184': 0.0, 'ACC-185': 0.05057993708687278, 'ACC-186': 1.0861328618337964, 'ACC-187': 4.884088959760326, 'ACC-188': 6.003843670554054, 'ACC-189': 2.0843548871562767, 'ACC-190': 0.5604347233721454, 'ACC-191': 0.7355301794453507})])
[01/18 03:06:55] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 03:06:55] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 03:06:55] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 03:06:55] d2.evaluation.testing INFO: copypaste: 2.5681,17.6141,38.2987,27.2810,52.4547
[01/18 03:06:55] d2.utils.events INFO:  eta: 9:10:29  iter: 17999  total_loss: 37.57  loss_ce: 0.2963  loss_mask: 0.3939  loss_dice: 3.002  loss_ce_0: 0.5622  loss_mask_0: 0.3954  loss_dice_0: 3.142  loss_ce_1: 0.315  loss_mask_1: 0.4015  loss_dice_1: 3.054  loss_ce_2: 0.3232  loss_mask_2: 0.3963  loss_dice_2: 3.028  loss_ce_3: 0.304  loss_mask_3: 0.3967  loss_dice_3: 3.012  loss_ce_4: 0.2955  loss_mask_4: 0.3947  loss_dice_4: 3.008  loss_ce_5: 0.2948  loss_mask_5: 0.3935  loss_dice_5: 3.01  loss_ce_6: 0.2992  loss_mask_6: 0.3927  loss_dice_6: 3.003  loss_ce_7: 0.2897  loss_mask_7: 0.3915  loss_dice_7: 3.004  loss_ce_8: 0.2898  loss_mask_8: 0.3928  loss_dice_8: 3.007  time: 1.4981  data_time: 0.0728  lr: 5.8391e-06  max_mem: 21589M
[01/18 03:07:25] d2.utils.events INFO:  eta: 9:09:49  iter: 18019  total_loss: 37.69  loss_ce: 0.3025  loss_mask: 0.397  loss_dice: 2.959  loss_ce_0: 0.5906  loss_mask_0: 0.3911  loss_dice_0: 3.091  loss_ce_1: 0.3366  loss_mask_1: 0.404  loss_dice_1: 3.011  loss_ce_2: 0.3331  loss_mask_2: 0.4012  loss_dice_2: 2.983  loss_ce_3: 0.3074  loss_mask_3: 0.3977  loss_dice_3: 2.976  loss_ce_4: 0.2988  loss_mask_4: 0.3955  loss_dice_4: 2.977  loss_ce_5: 0.2954  loss_mask_5: 0.3958  loss_dice_5: 2.959  loss_ce_6: 0.3131  loss_mask_6: 0.3978  loss_dice_6: 2.963  loss_ce_7: 0.3029  loss_mask_7: 0.3971  loss_dice_7: 2.976  loss_ce_8: 0.2986  loss_mask_8: 0.3965  loss_dice_8: 2.968  time: 1.4981  data_time: 0.0764  lr: 5.8343e-06  max_mem: 21589M
[01/18 03:07:54] d2.utils.events INFO:  eta: 9:09:09  iter: 18039  total_loss: 37.88  loss_ce: 0.3031  loss_mask: 0.4109  loss_dice: 3.008  loss_ce_0: 0.5808  loss_mask_0: 0.4092  loss_dice_0: 3.142  loss_ce_1: 0.3341  loss_mask_1: 0.4189  loss_dice_1: 3.049  loss_ce_2: 0.3227  loss_mask_2: 0.4151  loss_dice_2: 3.03  loss_ce_3: 0.3146  loss_mask_3: 0.4135  loss_dice_3: 3.021  loss_ce_4: 0.305  loss_mask_4: 0.4123  loss_dice_4: 3.018  loss_ce_5: 0.2914  loss_mask_5: 0.4115  loss_dice_5: 3.021  loss_ce_6: 0.2997  loss_mask_6: 0.4107  loss_dice_6: 3.012  loss_ce_7: 0.2915  loss_mask_7: 0.4102  loss_dice_7: 3.016  loss_ce_8: 0.2999  loss_mask_8: 0.4101  loss_dice_8: 3.022  time: 1.4980  data_time: 0.0738  lr: 5.8295e-06  max_mem: 21589M
[01/18 03:08:23] d2.utils.events INFO:  eta: 9:08:16  iter: 18059  total_loss: 37.04  loss_ce: 0.3131  loss_mask: 0.3941  loss_dice: 2.957  loss_ce_0: 0.5884  loss_mask_0: 0.3861  loss_dice_0: 3.09  loss_ce_1: 0.3419  loss_mask_1: 0.3973  loss_dice_1: 2.986  loss_ce_2: 0.3104  loss_mask_2: 0.3945  loss_dice_2: 2.978  loss_ce_3: 0.3086  loss_mask_3: 0.3989  loss_dice_3: 2.96  loss_ce_4: 0.3093  loss_mask_4: 0.3964  loss_dice_4: 2.951  loss_ce_5: 0.3084  loss_mask_5: 0.3956  loss_dice_5: 2.956  loss_ce_6: 0.2928  loss_mask_6: 0.3954  loss_dice_6: 2.956  loss_ce_7: 0.3078  loss_mask_7: 0.3939  loss_dice_7: 2.95  loss_ce_8: 0.304  loss_mask_8: 0.3944  loss_dice_8: 2.954  time: 1.4980  data_time: 0.0730  lr: 5.8247e-06  max_mem: 21589M
[01/18 03:08:52] d2.utils.events INFO:  eta: 9:07:21  iter: 18079  total_loss: 37.96  loss_ce: 0.3403  loss_mask: 0.3938  loss_dice: 2.996  loss_ce_0: 0.5887  loss_mask_0: 0.3937  loss_dice_0: 3.129  loss_ce_1: 0.374  loss_mask_1: 0.4037  loss_dice_1: 3.038  loss_ce_2: 0.3476  loss_mask_2: 0.3995  loss_dice_2: 3.015  loss_ce_3: 0.3327  loss_mask_3: 0.3973  loss_dice_3: 3.002  loss_ce_4: 0.3443  loss_mask_4: 0.3945  loss_dice_4: 2.988  loss_ce_5: 0.3317  loss_mask_5: 0.3957  loss_dice_5: 2.997  loss_ce_6: 0.3406  loss_mask_6: 0.393  loss_dice_6: 3  loss_ce_7: 0.3142  loss_mask_7: 0.395  loss_dice_7: 3.001  loss_ce_8: 0.3252  loss_mask_8: 0.3941  loss_dice_8: 3.001  time: 1.4979  data_time: 0.0739  lr: 5.82e-06  max_mem: 21589M
[01/18 03:09:22] d2.utils.events INFO:  eta: 9:06:45  iter: 18099  total_loss: 38.53  loss_ce: 0.3367  loss_mask: 0.3894  loss_dice: 3.044  loss_ce_0: 0.5936  loss_mask_0: 0.3897  loss_dice_0: 3.182  loss_ce_1: 0.3553  loss_mask_1: 0.3982  loss_dice_1: 3.088  loss_ce_2: 0.3564  loss_mask_2: 0.3927  loss_dice_2: 3.065  loss_ce_3: 0.3543  loss_mask_3: 0.3874  loss_dice_3: 3.05  loss_ce_4: 0.3412  loss_mask_4: 0.3867  loss_dice_4: 3.049  loss_ce_5: 0.3439  loss_mask_5: 0.3873  loss_dice_5: 3.059  loss_ce_6: 0.3304  loss_mask_6: 0.3892  loss_dice_6: 3.048  loss_ce_7: 0.3497  loss_mask_7: 0.3888  loss_dice_7: 3.044  loss_ce_8: 0.3343  loss_mask_8: 0.3885  loss_dice_8: 3.051  time: 1.4979  data_time: 0.0771  lr: 5.8152e-06  max_mem: 21589M
[01/18 03:09:52] d2.utils.events INFO:  eta: 9:05:35  iter: 18119  total_loss: 37.3  loss_ce: 0.3057  loss_mask: 0.395  loss_dice: 2.957  loss_ce_0: 0.5676  loss_mask_0: 0.3884  loss_dice_0: 3.078  loss_ce_1: 0.3549  loss_mask_1: 0.3965  loss_dice_1: 2.991  loss_ce_2: 0.3323  loss_mask_2: 0.3946  loss_dice_2: 2.966  loss_ce_3: 0.3081  loss_mask_3: 0.3949  loss_dice_3: 2.95  loss_ce_4: 0.3077  loss_mask_4: 0.3949  loss_dice_4: 2.948  loss_ce_5: 0.2967  loss_mask_5: 0.3958  loss_dice_5: 2.957  loss_ce_6: 0.2947  loss_mask_6: 0.396  loss_dice_6: 2.948  loss_ce_7: 0.2897  loss_mask_7: 0.3962  loss_dice_7: 2.949  loss_ce_8: 0.2905  loss_mask_8: 0.3952  loss_dice_8: 2.957  time: 1.4979  data_time: 0.0794  lr: 5.8104e-06  max_mem: 21589M
[01/18 03:10:21] d2.utils.events INFO:  eta: 9:05:10  iter: 18139  total_loss: 37.08  loss_ce: 0.2857  loss_mask: 0.4008  loss_dice: 2.949  loss_ce_0: 0.5379  loss_mask_0: 0.3842  loss_dice_0: 3.082  loss_ce_1: 0.3323  loss_mask_1: 0.3968  loss_dice_1: 2.991  loss_ce_2: 0.3198  loss_mask_2: 0.3948  loss_dice_2: 2.977  loss_ce_3: 0.308  loss_mask_3: 0.3958  loss_dice_3: 2.961  loss_ce_4: 0.2971  loss_mask_4: 0.3977  loss_dice_4: 2.962  loss_ce_5: 0.2932  loss_mask_5: 0.3965  loss_dice_5: 2.953  loss_ce_6: 0.285  loss_mask_6: 0.3993  loss_dice_6: 2.946  loss_ce_7: 0.2869  loss_mask_7: 0.4017  loss_dice_7: 2.943  loss_ce_8: 0.2954  loss_mask_8: 0.3999  loss_dice_8: 2.954  time: 1.4979  data_time: 0.0736  lr: 5.8056e-06  max_mem: 21589M
[01/18 03:10:51] d2.utils.events INFO:  eta: 9:04:17  iter: 18159  total_loss: 37.83  loss_ce: 0.3226  loss_mask: 0.4007  loss_dice: 2.991  loss_ce_0: 0.592  loss_mask_0: 0.3968  loss_dice_0: 3.104  loss_ce_1: 0.3587  loss_mask_1: 0.4066  loss_dice_1: 3.025  loss_ce_2: 0.35  loss_mask_2: 0.4048  loss_dice_2: 3.012  loss_ce_3: 0.3351  loss_mask_3: 0.3999  loss_dice_3: 2.998  loss_ce_4: 0.3348  loss_mask_4: 0.3985  loss_dice_4: 2.999  loss_ce_5: 0.3342  loss_mask_5: 0.3987  loss_dice_5: 3  loss_ce_6: 0.3337  loss_mask_6: 0.3976  loss_dice_6: 2.998  loss_ce_7: 0.3254  loss_mask_7: 0.398  loss_dice_7: 2.996  loss_ce_8: 0.3231  loss_mask_8: 0.3998  loss_dice_8: 2.994  time: 1.4978  data_time: 0.0746  lr: 5.8008e-06  max_mem: 21589M
[01/18 03:11:21] d2.utils.events INFO:  eta: 9:03:44  iter: 18179  total_loss: 36.78  loss_ce: 0.2963  loss_mask: 0.4044  loss_dice: 2.901  loss_ce_0: 0.5305  loss_mask_0: 0.3956  loss_dice_0: 3.04  loss_ce_1: 0.3195  loss_mask_1: 0.4113  loss_dice_1: 2.932  loss_ce_2: 0.3161  loss_mask_2: 0.4072  loss_dice_2: 2.918  loss_ce_3: 0.3042  loss_mask_3: 0.4034  loss_dice_3: 2.908  loss_ce_4: 0.3176  loss_mask_4: 0.403  loss_dice_4: 2.903  loss_ce_5: 0.289  loss_mask_5: 0.4029  loss_dice_5: 2.897  loss_ce_6: 0.2991  loss_mask_6: 0.4043  loss_dice_6: 2.906  loss_ce_7: 0.289  loss_mask_7: 0.4026  loss_dice_7: 2.905  loss_ce_8: 0.2936  loss_mask_8: 0.4035  loss_dice_8: 2.907  time: 1.4978  data_time: 0.0732  lr: 5.7961e-06  max_mem: 21589M
[01/18 03:11:51] d2.utils.events INFO:  eta: 9:03:05  iter: 18199  total_loss: 38.01  loss_ce: 0.3204  loss_mask: 0.3861  loss_dice: 3.005  loss_ce_0: 0.6113  loss_mask_0: 0.3771  loss_dice_0: 3.138  loss_ce_1: 0.3602  loss_mask_1: 0.388  loss_dice_1: 3.051  loss_ce_2: 0.3535  loss_mask_2: 0.3887  loss_dice_2: 3.029  loss_ce_3: 0.3442  loss_mask_3: 0.3882  loss_dice_3: 3.003  loss_ce_4: 0.3511  loss_mask_4: 0.3874  loss_dice_4: 3.008  loss_ce_5: 0.3251  loss_mask_5: 0.3873  loss_dice_5: 3.007  loss_ce_6: 0.327  loss_mask_6: 0.3861  loss_dice_6: 3.011  loss_ce_7: 0.3179  loss_mask_7: 0.3861  loss_dice_7: 3.008  loss_ce_8: 0.3229  loss_mask_8: 0.3875  loss_dice_8: 3.006  time: 1.4978  data_time: 0.0768  lr: 5.7913e-06  max_mem: 21589M
[01/18 03:12:20] d2.utils.events INFO:  eta: 9:02:13  iter: 18219  total_loss: 36.3  loss_ce: 0.2974  loss_mask: 0.4017  loss_dice: 2.885  loss_ce_0: 0.5513  loss_mask_0: 0.3915  loss_dice_0: 3.032  loss_ce_1: 0.3133  loss_mask_1: 0.4045  loss_dice_1: 2.92  loss_ce_2: 0.3072  loss_mask_2: 0.4039  loss_dice_2: 2.905  loss_ce_3: 0.2949  loss_mask_3: 0.3994  loss_dice_3: 2.897  loss_ce_4: 0.2963  loss_mask_4: 0.4009  loss_dice_4: 2.881  loss_ce_5: 0.2849  loss_mask_5: 0.3985  loss_dice_5: 2.882  loss_ce_6: 0.3004  loss_mask_6: 0.3996  loss_dice_6: 2.887  loss_ce_7: 0.2899  loss_mask_7: 0.4006  loss_dice_7: 2.886  loss_ce_8: 0.286  loss_mask_8: 0.401  loss_dice_8: 2.881  time: 1.4978  data_time: 0.0793  lr: 5.7865e-06  max_mem: 21589M
[01/18 03:12:50] d2.utils.events INFO:  eta: 9:01:38  iter: 18239  total_loss: 37.32  loss_ce: 0.2826  loss_mask: 0.3961  loss_dice: 3.003  loss_ce_0: 0.5768  loss_mask_0: 0.3829  loss_dice_0: 3.136  loss_ce_1: 0.3106  loss_mask_1: 0.4001  loss_dice_1: 3.051  loss_ce_2: 0.323  loss_mask_2: 0.3965  loss_dice_2: 3.011  loss_ce_3: 0.3008  loss_mask_3: 0.3954  loss_dice_3: 3  loss_ce_4: 0.2901  loss_mask_4: 0.3963  loss_dice_4: 3.006  loss_ce_5: 0.2953  loss_mask_5: 0.3956  loss_dice_5: 3.013  loss_ce_6: 0.2897  loss_mask_6: 0.3968  loss_dice_6: 2.991  loss_ce_7: 0.284  loss_mask_7: 0.3948  loss_dice_7: 3.003  loss_ce_8: 0.2866  loss_mask_8: 0.3953  loss_dice_8: 3.002  time: 1.4978  data_time: 0.0792  lr: 5.7817e-06  max_mem: 21589M
[01/18 03:13:20] d2.utils.events INFO:  eta: 9:00:47  iter: 18259  total_loss: 37.38  loss_ce: 0.3222  loss_mask: 0.3844  loss_dice: 2.994  loss_ce_0: 0.5953  loss_mask_0: 0.3751  loss_dice_0: 3.133  loss_ce_1: 0.3394  loss_mask_1: 0.3907  loss_dice_1: 3.04  loss_ce_2: 0.3568  loss_mask_2: 0.388  loss_dice_2: 3.005  loss_ce_3: 0.3284  loss_mask_3: 0.3879  loss_dice_3: 3.002  loss_ce_4: 0.3277  loss_mask_4: 0.3856  loss_dice_4: 2.997  loss_ce_5: 0.3371  loss_mask_5: 0.3863  loss_dice_5: 2.992  loss_ce_6: 0.3111  loss_mask_6: 0.3848  loss_dice_6: 2.992  loss_ce_7: 0.3175  loss_mask_7: 0.3834  loss_dice_7: 2.99  loss_ce_8: 0.3186  loss_mask_8: 0.3847  loss_dice_8: 2.987  time: 1.4978  data_time: 0.0761  lr: 5.7769e-06  max_mem: 21589M
[01/18 03:13:50] d2.utils.events INFO:  eta: 9:00:14  iter: 18279  total_loss: 37.3  loss_ce: 0.2811  loss_mask: 0.3881  loss_dice: 2.99  loss_ce_0: 0.5649  loss_mask_0: 0.3921  loss_dice_0: 3.126  loss_ce_1: 0.311  loss_mask_1: 0.3937  loss_dice_1: 3.037  loss_ce_2: 0.3106  loss_mask_2: 0.3889  loss_dice_2: 3.017  loss_ce_3: 0.3068  loss_mask_3: 0.3871  loss_dice_3: 3.008  loss_ce_4: 0.3094  loss_mask_4: 0.3853  loss_dice_4: 3.006  loss_ce_5: 0.2949  loss_mask_5: 0.3854  loss_dice_5: 2.999  loss_ce_6: 0.2919  loss_mask_6: 0.3882  loss_dice_6: 2.993  loss_ce_7: 0.2812  loss_mask_7: 0.3892  loss_dice_7: 2.994  loss_ce_8: 0.2929  loss_mask_8: 0.3896  loss_dice_8: 2.99  time: 1.4978  data_time: 0.0744  lr: 5.7722e-06  max_mem: 21589M
[01/18 03:14:20] d2.utils.events INFO:  eta: 8:59:41  iter: 18299  total_loss: 37.87  loss_ce: 0.3194  loss_mask: 0.4044  loss_dice: 2.993  loss_ce_0: 0.6219  loss_mask_0: 0.398  loss_dice_0: 3.131  loss_ce_1: 0.3541  loss_mask_1: 0.41  loss_dice_1: 3.041  loss_ce_2: 0.3577  loss_mask_2: 0.4067  loss_dice_2: 3.011  loss_ce_3: 0.3339  loss_mask_3: 0.4027  loss_dice_3: 2.997  loss_ce_4: 0.34  loss_mask_4: 0.4016  loss_dice_4: 2.987  loss_ce_5: 0.338  loss_mask_5: 0.4057  loss_dice_5: 2.992  loss_ce_6: 0.3241  loss_mask_6: 0.4058  loss_dice_6: 2.993  loss_ce_7: 0.3219  loss_mask_7: 0.4039  loss_dice_7: 3.001  loss_ce_8: 0.3265  loss_mask_8: 0.4017  loss_dice_8: 2.989  time: 1.4978  data_time: 0.0732  lr: 5.7674e-06  max_mem: 21589M
[01/18 03:14:50] d2.utils.events INFO:  eta: 8:59:12  iter: 18319  total_loss: 37.41  loss_ce: 0.3121  loss_mask: 0.3895  loss_dice: 2.983  loss_ce_0: 0.5798  loss_mask_0: 0.3785  loss_dice_0: 3.121  loss_ce_1: 0.3538  loss_mask_1: 0.3926  loss_dice_1: 3.029  loss_ce_2: 0.3527  loss_mask_2: 0.3924  loss_dice_2: 2.991  loss_ce_3: 0.3254  loss_mask_3: 0.3924  loss_dice_3: 2.986  loss_ce_4: 0.3221  loss_mask_4: 0.3919  loss_dice_4: 2.983  loss_ce_5: 0.3222  loss_mask_5: 0.3887  loss_dice_5: 2.989  loss_ce_6: 0.3106  loss_mask_6: 0.3903  loss_dice_6: 2.98  loss_ce_7: 0.3108  loss_mask_7: 0.389  loss_dice_7: 2.979  loss_ce_8: 0.3027  loss_mask_8: 0.389  loss_dice_8: 2.986  time: 1.4978  data_time: 0.0757  lr: 5.7626e-06  max_mem: 21589M
[01/18 03:15:20] d2.utils.events INFO:  eta: 8:58:44  iter: 18339  total_loss: 36.96  loss_ce: 0.2833  loss_mask: 0.3957  loss_dice: 2.971  loss_ce_0: 0.566  loss_mask_0: 0.3904  loss_dice_0: 3.094  loss_ce_1: 0.3336  loss_mask_1: 0.4015  loss_dice_1: 3.001  loss_ce_2: 0.3349  loss_mask_2: 0.3979  loss_dice_2: 2.973  loss_ce_3: 0.3082  loss_mask_3: 0.3952  loss_dice_3: 2.978  loss_ce_4: 0.3022  loss_mask_4: 0.3931  loss_dice_4: 2.972  loss_ce_5: 0.2884  loss_mask_5: 0.394  loss_dice_5: 2.975  loss_ce_6: 0.2993  loss_mask_6: 0.3963  loss_dice_6: 2.973  loss_ce_7: 0.2901  loss_mask_7: 0.3955  loss_dice_7: 2.972  loss_ce_8: 0.2953  loss_mask_8: 0.396  loss_dice_8: 2.97  time: 1.4978  data_time: 0.0754  lr: 5.7578e-06  max_mem: 21589M
[01/18 03:15:50] d2.utils.events INFO:  eta: 8:58:19  iter: 18359  total_loss: 38.39  loss_ce: 0.3093  loss_mask: 0.392  loss_dice: 3.053  loss_ce_0: 0.5698  loss_mask_0: 0.3858  loss_dice_0: 3.191  loss_ce_1: 0.3249  loss_mask_1: 0.3945  loss_dice_1: 3.093  loss_ce_2: 0.3577  loss_mask_2: 0.3935  loss_dice_2: 3.077  loss_ce_3: 0.3199  loss_mask_3: 0.3908  loss_dice_3: 3.063  loss_ce_4: 0.2989  loss_mask_4: 0.3899  loss_dice_4: 3.061  loss_ce_5: 0.3223  loss_mask_5: 0.389  loss_dice_5: 3.064  loss_ce_6: 0.318  loss_mask_6: 0.3925  loss_dice_6: 3.063  loss_ce_7: 0.3084  loss_mask_7: 0.3923  loss_dice_7: 3.065  loss_ce_8: 0.3084  loss_mask_8: 0.3912  loss_dice_8: 3.064  time: 1.4978  data_time: 0.0776  lr: 5.753e-06  max_mem: 21589M
[01/18 03:16:20] d2.utils.events INFO:  eta: 8:57:42  iter: 18379  total_loss: 36.84  loss_ce: 0.2793  loss_mask: 0.3933  loss_dice: 2.933  loss_ce_0: 0.5823  loss_mask_0: 0.3826  loss_dice_0: 3.087  loss_ce_1: 0.3151  loss_mask_1: 0.3916  loss_dice_1: 2.979  loss_ce_2: 0.319  loss_mask_2: 0.3901  loss_dice_2: 2.954  loss_ce_3: 0.2935  loss_mask_3: 0.3903  loss_dice_3: 2.941  loss_ce_4: 0.2877  loss_mask_4: 0.391  loss_dice_4: 2.939  loss_ce_5: 0.2998  loss_mask_5: 0.3907  loss_dice_5: 2.948  loss_ce_6: 0.2854  loss_mask_6: 0.3894  loss_dice_6: 2.946  loss_ce_7: 0.2868  loss_mask_7: 0.3919  loss_dice_7: 2.941  loss_ce_8: 0.2846  loss_mask_8: 0.3918  loss_dice_8: 2.937  time: 1.4978  data_time: 0.0761  lr: 5.7482e-06  max_mem: 21589M
[01/18 03:16:50] d2.utils.events INFO:  eta: 8:56:54  iter: 18399  total_loss: 37.32  loss_ce: 0.3059  loss_mask: 0.4022  loss_dice: 2.972  loss_ce_0: 0.572  loss_mask_0: 0.3906  loss_dice_0: 3.119  loss_ce_1: 0.3338  loss_mask_1: 0.4075  loss_dice_1: 3.011  loss_ce_2: 0.334  loss_mask_2: 0.4022  loss_dice_2: 2.996  loss_ce_3: 0.3247  loss_mask_3: 0.4006  loss_dice_3: 2.984  loss_ce_4: 0.3154  loss_mask_4: 0.4016  loss_dice_4: 2.984  loss_ce_5: 0.3053  loss_mask_5: 0.4024  loss_dice_5: 2.988  loss_ce_6: 0.3095  loss_mask_6: 0.4015  loss_dice_6: 2.977  loss_ce_7: 0.3142  loss_mask_7: 0.4011  loss_dice_7: 2.984  loss_ce_8: 0.2909  loss_mask_8: 0.4012  loss_dice_8: 2.979  time: 1.4977  data_time: 0.0749  lr: 5.7434e-06  max_mem: 21589M
[01/18 03:17:19] d2.utils.events INFO:  eta: 8:56:24  iter: 18419  total_loss: 37.3  loss_ce: 0.3316  loss_mask: 0.38  loss_dice: 2.951  loss_ce_0: 0.5947  loss_mask_0: 0.3702  loss_dice_0: 3.092  loss_ce_1: 0.3469  loss_mask_1: 0.3841  loss_dice_1: 2.988  loss_ce_2: 0.3511  loss_mask_2: 0.3821  loss_dice_2: 2.963  loss_ce_3: 0.3417  loss_mask_3: 0.38  loss_dice_3: 2.948  loss_ce_4: 0.3476  loss_mask_4: 0.3799  loss_dice_4: 2.95  loss_ce_5: 0.3245  loss_mask_5: 0.3786  loss_dice_5: 2.946  loss_ce_6: 0.3409  loss_mask_6: 0.3799  loss_dice_6: 2.946  loss_ce_7: 0.3306  loss_mask_7: 0.3788  loss_dice_7: 2.948  loss_ce_8: 0.3359  loss_mask_8: 0.3806  loss_dice_8: 2.948  time: 1.4977  data_time: 0.0769  lr: 5.7387e-06  max_mem: 21589M
[01/18 03:17:49] d2.utils.events INFO:  eta: 8:55:44  iter: 18439  total_loss: 37.43  loss_ce: 0.2781  loss_mask: 0.3905  loss_dice: 2.971  loss_ce_0: 0.6005  loss_mask_0: 0.3874  loss_dice_0: 3.103  loss_ce_1: 0.3278  loss_mask_1: 0.4004  loss_dice_1: 3.013  loss_ce_2: 0.3211  loss_mask_2: 0.3953  loss_dice_2: 2.997  loss_ce_3: 0.3124  loss_mask_3: 0.393  loss_dice_3: 2.976  loss_ce_4: 0.2977  loss_mask_4: 0.3931  loss_dice_4: 2.97  loss_ce_5: 0.2959  loss_mask_5: 0.3926  loss_dice_5: 2.969  loss_ce_6: 0.2896  loss_mask_6: 0.3929  loss_dice_6: 2.97  loss_ce_7: 0.2936  loss_mask_7: 0.3897  loss_dice_7: 2.977  loss_ce_8: 0.2852  loss_mask_8: 0.3913  loss_dice_8: 2.966  time: 1.4977  data_time: 0.0701  lr: 5.7339e-06  max_mem: 21589M
[01/18 03:18:19] d2.utils.events INFO:  eta: 8:55:03  iter: 18459  total_loss: 37.14  loss_ce: 0.2951  loss_mask: 0.3904  loss_dice: 2.977  loss_ce_0: 0.5647  loss_mask_0: 0.385  loss_dice_0: 3.101  loss_ce_1: 0.3187  loss_mask_1: 0.3954  loss_dice_1: 3.008  loss_ce_2: 0.328  loss_mask_2: 0.3923  loss_dice_2: 2.984  loss_ce_3: 0.3044  loss_mask_3: 0.3909  loss_dice_3: 2.975  loss_ce_4: 0.3001  loss_mask_4: 0.39  loss_dice_4: 2.97  loss_ce_5: 0.2988  loss_mask_5: 0.3907  loss_dice_5: 2.979  loss_ce_6: 0.3023  loss_mask_6: 0.391  loss_dice_6: 2.974  loss_ce_7: 0.2958  loss_mask_7: 0.3908  loss_dice_7: 2.977  loss_ce_8: 0.2942  loss_mask_8: 0.3906  loss_dice_8: 2.973  time: 1.4977  data_time: 0.0751  lr: 5.7291e-06  max_mem: 21589M
[01/18 03:18:49] d2.utils.events INFO:  eta: 8:54:44  iter: 18479  total_loss: 36.9  loss_ce: 0.2926  loss_mask: 0.3894  loss_dice: 2.97  loss_ce_0: 0.5537  loss_mask_0: 0.3776  loss_dice_0: 3.083  loss_ce_1: 0.3187  loss_mask_1: 0.3984  loss_dice_1: 2.998  loss_ce_2: 0.3226  loss_mask_2: 0.3925  loss_dice_2: 2.974  loss_ce_3: 0.3154  loss_mask_3: 0.3878  loss_dice_3: 2.964  loss_ce_4: 0.2989  loss_mask_4: 0.3884  loss_dice_4: 2.966  loss_ce_5: 0.2915  loss_mask_5: 0.3877  loss_dice_5: 2.964  loss_ce_6: 0.2984  loss_mask_6: 0.3879  loss_dice_6: 2.961  loss_ce_7: 0.2904  loss_mask_7: 0.3898  loss_dice_7: 2.964  loss_ce_8: 0.2865  loss_mask_8: 0.3901  loss_dice_8: 2.96  time: 1.4977  data_time: 0.0765  lr: 5.7243e-06  max_mem: 21589M
[01/18 03:19:19] d2.utils.events INFO:  eta: 8:54:02  iter: 18499  total_loss: 36.6  loss_ce: 0.2893  loss_mask: 0.3901  loss_dice: 2.937  loss_ce_0: 0.5775  loss_mask_0: 0.3845  loss_dice_0: 3.06  loss_ce_1: 0.3324  loss_mask_1: 0.396  loss_dice_1: 2.98  loss_ce_2: 0.318  loss_mask_2: 0.3925  loss_dice_2: 2.965  loss_ce_3: 0.3055  loss_mask_3: 0.3895  loss_dice_3: 2.943  loss_ce_4: 0.3031  loss_mask_4: 0.3887  loss_dice_4: 2.936  loss_ce_5: 0.3084  loss_mask_5: 0.3908  loss_dice_5: 2.942  loss_ce_6: 0.3023  loss_mask_6: 0.3891  loss_dice_6: 2.936  loss_ce_7: 0.2945  loss_mask_7: 0.39  loss_dice_7: 2.943  loss_ce_8: 0.2843  loss_mask_8: 0.3896  loss_dice_8: 2.937  time: 1.4977  data_time: 0.0790  lr: 5.7195e-06  max_mem: 21589M
[01/18 03:19:48] d2.utils.events INFO:  eta: 8:53:17  iter: 18519  total_loss: 36.74  loss_ce: 0.3125  loss_mask: 0.3982  loss_dice: 2.924  loss_ce_0: 0.5758  loss_mask_0: 0.3937  loss_dice_0: 3.053  loss_ce_1: 0.3548  loss_mask_1: 0.4085  loss_dice_1: 2.969  loss_ce_2: 0.3422  loss_mask_2: 0.4065  loss_dice_2: 2.938  loss_ce_3: 0.3344  loss_mask_3: 0.4001  loss_dice_3: 2.93  loss_ce_4: 0.3387  loss_mask_4: 0.4012  loss_dice_4: 2.933  loss_ce_5: 0.3184  loss_mask_5: 0.3998  loss_dice_5: 2.929  loss_ce_6: 0.3312  loss_mask_6: 0.3971  loss_dice_6: 2.924  loss_ce_7: 0.3171  loss_mask_7: 0.3956  loss_dice_7: 2.92  loss_ce_8: 0.3125  loss_mask_8: 0.3968  loss_dice_8: 2.922  time: 1.4977  data_time: 0.0753  lr: 5.7147e-06  max_mem: 21589M
[01/18 03:20:18] d2.utils.events INFO:  eta: 8:52:46  iter: 18539  total_loss: 36.83  loss_ce: 0.3127  loss_mask: 0.389  loss_dice: 2.924  loss_ce_0: 0.5913  loss_mask_0: 0.3796  loss_dice_0: 3.06  loss_ce_1: 0.3469  loss_mask_1: 0.3928  loss_dice_1: 2.969  loss_ce_2: 0.3402  loss_mask_2: 0.3907  loss_dice_2: 2.949  loss_ce_3: 0.3246  loss_mask_3: 0.3876  loss_dice_3: 2.932  loss_ce_4: 0.3204  loss_mask_4: 0.3877  loss_dice_4: 2.943  loss_ce_5: 0.3188  loss_mask_5: 0.389  loss_dice_5: 2.935  loss_ce_6: 0.3007  loss_mask_6: 0.3879  loss_dice_6: 2.922  loss_ce_7: 0.3042  loss_mask_7: 0.388  loss_dice_7: 2.934  loss_ce_8: 0.3113  loss_mask_8: 0.389  loss_dice_8: 2.929  time: 1.4977  data_time: 0.0738  lr: 5.7099e-06  max_mem: 21589M
[01/18 03:20:48] d2.utils.events INFO:  eta: 8:52:16  iter: 18559  total_loss: 37.66  loss_ce: 0.3124  loss_mask: 0.3792  loss_dice: 3.006  loss_ce_0: 0.5663  loss_mask_0: 0.3774  loss_dice_0: 3.127  loss_ce_1: 0.3453  loss_mask_1: 0.3852  loss_dice_1: 3.035  loss_ce_2: 0.3461  loss_mask_2: 0.3828  loss_dice_2: 3.011  loss_ce_3: 0.3311  loss_mask_3: 0.379  loss_dice_3: 3.008  loss_ce_4: 0.3189  loss_mask_4: 0.3787  loss_dice_4: 3.014  loss_ce_5: 0.3176  loss_mask_5: 0.3793  loss_dice_5: 3.011  loss_ce_6: 0.3141  loss_mask_6: 0.3794  loss_dice_6: 3.001  loss_ce_7: 0.3171  loss_mask_7: 0.3798  loss_dice_7: 3.009  loss_ce_8: 0.3135  loss_mask_8: 0.3798  loss_dice_8: 3.004  time: 1.4977  data_time: 0.0749  lr: 5.7051e-06  max_mem: 21589M
[01/18 03:21:17] d2.utils.events INFO:  eta: 8:51:26  iter: 18579  total_loss: 37.59  loss_ce: 0.3241  loss_mask: 0.3973  loss_dice: 2.954  loss_ce_0: 0.6024  loss_mask_0: 0.3913  loss_dice_0: 3.111  loss_ce_1: 0.3772  loss_mask_1: 0.4086  loss_dice_1: 3.008  loss_ce_2: 0.3621  loss_mask_2: 0.4037  loss_dice_2: 2.981  loss_ce_3: 0.3374  loss_mask_3: 0.3999  loss_dice_3: 2.969  loss_ce_4: 0.3344  loss_mask_4: 0.3987  loss_dice_4: 2.957  loss_ce_5: 0.3325  loss_mask_5: 0.3996  loss_dice_5: 2.957  loss_ce_6: 0.3163  loss_mask_6: 0.3981  loss_dice_6: 2.962  loss_ce_7: 0.3185  loss_mask_7: 0.3983  loss_dice_7: 2.967  loss_ce_8: 0.3275  loss_mask_8: 0.3984  loss_dice_8: 2.965  time: 1.4976  data_time: 0.0719  lr: 5.7004e-06  max_mem: 21589M
[01/18 03:21:47] d2.utils.events INFO:  eta: 8:50:32  iter: 18599  total_loss: 36.4  loss_ce: 0.3077  loss_mask: 0.3856  loss_dice: 2.907  loss_ce_0: 0.5557  loss_mask_0: 0.3753  loss_dice_0: 3.04  loss_ce_1: 0.3398  loss_mask_1: 0.3888  loss_dice_1: 2.95  loss_ce_2: 0.3329  loss_mask_2: 0.3891  loss_dice_2: 2.916  loss_ce_3: 0.3064  loss_mask_3: 0.386  loss_dice_3: 2.913  loss_ce_4: 0.3036  loss_mask_4: 0.3872  loss_dice_4: 2.909  loss_ce_5: 0.3089  loss_mask_5: 0.3854  loss_dice_5: 2.913  loss_ce_6: 0.3006  loss_mask_6: 0.3872  loss_dice_6: 2.903  loss_ce_7: 0.2927  loss_mask_7: 0.3871  loss_dice_7: 2.905  loss_ce_8: 0.3045  loss_mask_8: 0.3872  loss_dice_8: 2.908  time: 1.4976  data_time: 0.0728  lr: 5.6956e-06  max_mem: 21589M
[01/18 03:22:16] d2.utils.events INFO:  eta: 8:49:50  iter: 18619  total_loss: 37.01  loss_ce: 0.2909  loss_mask: 0.3972  loss_dice: 2.971  loss_ce_0: 0.5494  loss_mask_0: 0.383  loss_dice_0: 3.095  loss_ce_1: 0.3094  loss_mask_1: 0.396  loss_dice_1: 3.003  loss_ce_2: 0.3148  loss_mask_2: 0.3953  loss_dice_2: 2.988  loss_ce_3: 0.2955  loss_mask_3: 0.3969  loss_dice_3: 2.974  loss_ce_4: 0.2955  loss_mask_4: 0.395  loss_dice_4: 2.978  loss_ce_5: 0.2936  loss_mask_5: 0.3941  loss_dice_5: 2.976  loss_ce_6: 0.2921  loss_mask_6: 0.3947  loss_dice_6: 2.971  loss_ce_7: 0.2819  loss_mask_7: 0.3949  loss_dice_7: 2.965  loss_ce_8: 0.2822  loss_mask_8: 0.3955  loss_dice_8: 2.971  time: 1.4976  data_time: 0.0755  lr: 5.6908e-06  max_mem: 21589M
[01/18 03:22:46] d2.utils.events INFO:  eta: 8:49:10  iter: 18639  total_loss: 36.94  loss_ce: 0.2943  loss_mask: 0.3884  loss_dice: 2.946  loss_ce_0: 0.586  loss_mask_0: 0.3815  loss_dice_0: 3.072  loss_ce_1: 0.3396  loss_mask_1: 0.3941  loss_dice_1: 2.99  loss_ce_2: 0.3344  loss_mask_2: 0.3889  loss_dice_2: 2.966  loss_ce_3: 0.3232  loss_mask_3: 0.3878  loss_dice_3: 2.964  loss_ce_4: 0.2993  loss_mask_4: 0.3867  loss_dice_4: 2.962  loss_ce_5: 0.2985  loss_mask_5: 0.3876  loss_dice_5: 2.959  loss_ce_6: 0.3115  loss_mask_6: 0.3867  loss_dice_6: 2.953  loss_ce_7: 0.3086  loss_mask_7: 0.3873  loss_dice_7: 2.96  loss_ce_8: 0.3019  loss_mask_8: 0.387  loss_dice_8: 2.948  time: 1.4975  data_time: 0.0729  lr: 5.686e-06  max_mem: 21589M
[01/18 03:23:15] d2.utils.events INFO:  eta: 8:48:25  iter: 18659  total_loss: 37.13  loss_ce: 0.2981  loss_mask: 0.3945  loss_dice: 2.952  loss_ce_0: 0.572  loss_mask_0: 0.3898  loss_dice_0: 3.078  loss_ce_1: 0.3205  loss_mask_1: 0.4039  loss_dice_1: 2.987  loss_ce_2: 0.3128  loss_mask_2: 0.3992  loss_dice_2: 2.969  loss_ce_3: 0.3071  loss_mask_3: 0.3952  loss_dice_3: 2.952  loss_ce_4: 0.3102  loss_mask_4: 0.3984  loss_dice_4: 2.953  loss_ce_5: 0.3061  loss_mask_5: 0.3981  loss_dice_5: 2.956  loss_ce_6: 0.2934  loss_mask_6: 0.3975  loss_dice_6: 2.944  loss_ce_7: 0.2899  loss_mask_7: 0.3975  loss_dice_7: 2.95  loss_ce_8: 0.2927  loss_mask_8: 0.3938  loss_dice_8: 2.959  time: 1.4975  data_time: 0.0685  lr: 5.6812e-06  max_mem: 21589M
[01/18 03:23:45] d2.utils.events INFO:  eta: 8:47:37  iter: 18679  total_loss: 36.76  loss_ce: 0.3014  loss_mask: 0.3911  loss_dice: 2.911  loss_ce_0: 0.5484  loss_mask_0: 0.3816  loss_dice_0: 3.063  loss_ce_1: 0.3189  loss_mask_1: 0.3964  loss_dice_1: 2.963  loss_ce_2: 0.3115  loss_mask_2: 0.3951  loss_dice_2: 2.945  loss_ce_3: 0.298  loss_mask_3: 0.3899  loss_dice_3: 2.941  loss_ce_4: 0.2926  loss_mask_4: 0.3901  loss_dice_4: 2.931  loss_ce_5: 0.2894  loss_mask_5: 0.3908  loss_dice_5: 2.927  loss_ce_6: 0.302  loss_mask_6: 0.3911  loss_dice_6: 2.924  loss_ce_7: 0.3051  loss_mask_7: 0.3902  loss_dice_7: 2.929  loss_ce_8: 0.297  loss_mask_8: 0.3913  loss_dice_8: 2.92  time: 1.4975  data_time: 0.0782  lr: 5.6764e-06  max_mem: 21589M
[01/18 03:24:15] d2.utils.events INFO:  eta: 8:47:22  iter: 18699  total_loss: 37.3  loss_ce: 0.3009  loss_mask: 0.3945  loss_dice: 2.99  loss_ce_0: 0.5517  loss_mask_0: 0.3868  loss_dice_0: 3.121  loss_ce_1: 0.3215  loss_mask_1: 0.3963  loss_dice_1: 3.023  loss_ce_2: 0.3249  loss_mask_2: 0.3928  loss_dice_2: 3.01  loss_ce_3: 0.299  loss_mask_3: 0.3949  loss_dice_3: 2.994  loss_ce_4: 0.2943  loss_mask_4: 0.3959  loss_dice_4: 2.989  loss_ce_5: 0.2936  loss_mask_5: 0.3966  loss_dice_5: 2.992  loss_ce_6: 0.2996  loss_mask_6: 0.3958  loss_dice_6: 2.99  loss_ce_7: 0.2998  loss_mask_7: 0.3963  loss_dice_7: 2.992  loss_ce_8: 0.2976  loss_mask_8: 0.3966  loss_dice_8: 2.992  time: 1.4975  data_time: 0.0793  lr: 5.6716e-06  max_mem: 21589M
[01/18 03:24:45] d2.utils.events INFO:  eta: 8:46:30  iter: 18719  total_loss: 37.52  loss_ce: 0.3337  loss_mask: 0.3916  loss_dice: 2.987  loss_ce_0: 0.565  loss_mask_0: 0.3862  loss_dice_0: 3.118  loss_ce_1: 0.3426  loss_mask_1: 0.3978  loss_dice_1: 3.021  loss_ce_2: 0.3451  loss_mask_2: 0.3967  loss_dice_2: 2.998  loss_ce_3: 0.3326  loss_mask_3: 0.3963  loss_dice_3: 2.99  loss_ce_4: 0.312  loss_mask_4: 0.3921  loss_dice_4: 2.996  loss_ce_5: 0.3182  loss_mask_5: 0.39  loss_dice_5: 2.993  loss_ce_6: 0.3156  loss_mask_6: 0.3882  loss_dice_6: 2.992  loss_ce_7: 0.3148  loss_mask_7: 0.3919  loss_dice_7: 2.995  loss_ce_8: 0.3147  loss_mask_8: 0.3912  loss_dice_8: 2.987  time: 1.4975  data_time: 0.0735  lr: 5.6668e-06  max_mem: 21589M
[01/18 03:25:14] d2.utils.events INFO:  eta: 8:45:58  iter: 18739  total_loss: 36.96  loss_ce: 0.2902  loss_mask: 0.3924  loss_dice: 2.958  loss_ce_0: 0.5616  loss_mask_0: 0.3831  loss_dice_0: 3.096  loss_ce_1: 0.3167  loss_mask_1: 0.3949  loss_dice_1: 2.996  loss_ce_2: 0.3083  loss_mask_2: 0.3928  loss_dice_2: 2.985  loss_ce_3: 0.3189  loss_mask_3: 0.3908  loss_dice_3: 2.962  loss_ce_4: 0.2935  loss_mask_4: 0.3896  loss_dice_4: 2.977  loss_ce_5: 0.3061  loss_mask_5: 0.3897  loss_dice_5: 2.97  loss_ce_6: 0.2807  loss_mask_6: 0.3901  loss_dice_6: 2.965  loss_ce_7: 0.2889  loss_mask_7: 0.3906  loss_dice_7: 2.965  loss_ce_8: 0.2894  loss_mask_8: 0.3923  loss_dice_8: 2.964  time: 1.4975  data_time: 0.0742  lr: 5.662e-06  max_mem: 21589M
[01/18 03:25:44] d2.utils.events INFO:  eta: 8:44:39  iter: 18759  total_loss: 36.6  loss_ce: 0.2938  loss_mask: 0.3848  loss_dice: 2.917  loss_ce_0: 0.5911  loss_mask_0: 0.3797  loss_dice_0: 3.07  loss_ce_1: 0.3117  loss_mask_1: 0.3866  loss_dice_1: 2.984  loss_ce_2: 0.3101  loss_mask_2: 0.3844  loss_dice_2: 2.944  loss_ce_3: 0.2945  loss_mask_3: 0.3826  loss_dice_3: 2.936  loss_ce_4: 0.2859  loss_mask_4: 0.383  loss_dice_4: 2.937  loss_ce_5: 0.293  loss_mask_5: 0.3823  loss_dice_5: 2.93  loss_ce_6: 0.2824  loss_mask_6: 0.3826  loss_dice_6: 2.925  loss_ce_7: 0.2838  loss_mask_7: 0.3829  loss_dice_7: 2.929  loss_ce_8: 0.2907  loss_mask_8: 0.3835  loss_dice_8: 2.921  time: 1.4974  data_time: 0.0687  lr: 5.6572e-06  max_mem: 21589M
[01/18 03:26:14] d2.utils.events INFO:  eta: 8:43:43  iter: 18779  total_loss: 36.98  loss_ce: 0.2895  loss_mask: 0.3871  loss_dice: 2.952  loss_ce_0: 0.5617  loss_mask_0: 0.3822  loss_dice_0: 3.103  loss_ce_1: 0.3371  loss_mask_1: 0.3916  loss_dice_1: 2.996  loss_ce_2: 0.3093  loss_mask_2: 0.3887  loss_dice_2: 2.973  loss_ce_3: 0.2955  loss_mask_3: 0.3864  loss_dice_3: 2.953  loss_ce_4: 0.2926  loss_mask_4: 0.3874  loss_dice_4: 2.955  loss_ce_5: 0.2902  loss_mask_5: 0.386  loss_dice_5: 2.949  loss_ce_6: 0.2982  loss_mask_6: 0.3852  loss_dice_6: 2.942  loss_ce_7: 0.2873  loss_mask_7: 0.3874  loss_dice_7: 2.943  loss_ce_8: 0.2797  loss_mask_8: 0.3871  loss_dice_8: 2.953  time: 1.4974  data_time: 0.0760  lr: 5.6524e-06  max_mem: 21589M
[01/18 03:26:43] d2.utils.events INFO:  eta: 8:42:53  iter: 18799  total_loss: 36.59  loss_ce: 0.3003  loss_mask: 0.388  loss_dice: 2.913  loss_ce_0: 0.5763  loss_mask_0: 0.3787  loss_dice_0: 3.05  loss_ce_1: 0.3389  loss_mask_1: 0.3925  loss_dice_1: 2.958  loss_ce_2: 0.3177  loss_mask_2: 0.3909  loss_dice_2: 2.939  loss_ce_3: 0.309  loss_mask_3: 0.3886  loss_dice_3: 2.912  loss_ce_4: 0.317  loss_mask_4: 0.3877  loss_dice_4: 2.92  loss_ce_5: 0.3045  loss_mask_5: 0.3884  loss_dice_5: 2.917  loss_ce_6: 0.295  loss_mask_6: 0.3865  loss_dice_6: 2.927  loss_ce_7: 0.291  loss_mask_7: 0.3882  loss_dice_7: 2.917  loss_ce_8: 0.2857  loss_mask_8: 0.3864  loss_dice_8: 2.907  time: 1.4974  data_time: 0.0802  lr: 5.6476e-06  max_mem: 21589M
[01/18 03:27:13] d2.utils.events INFO:  eta: 8:42:14  iter: 18819  total_loss: 36.9  loss_ce: 0.2798  loss_mask: 0.3926  loss_dice: 2.978  loss_ce_0: 0.5645  loss_mask_0: 0.3836  loss_dice_0: 3.1  loss_ce_1: 0.3289  loss_mask_1: 0.3962  loss_dice_1: 3.013  loss_ce_2: 0.3158  loss_mask_2: 0.3939  loss_dice_2: 2.99  loss_ce_3: 0.2882  loss_mask_3: 0.3925  loss_dice_3: 2.979  loss_ce_4: 0.2844  loss_mask_4: 0.3932  loss_dice_4: 2.975  loss_ce_5: 0.2981  loss_mask_5: 0.3914  loss_dice_5: 2.976  loss_ce_6: 0.2863  loss_mask_6: 0.3912  loss_dice_6: 2.972  loss_ce_7: 0.274  loss_mask_7: 0.3909  loss_dice_7: 2.965  loss_ce_8: 0.2838  loss_mask_8: 0.3916  loss_dice_8: 2.974  time: 1.4974  data_time: 0.0769  lr: 5.6428e-06  max_mem: 21589M
[01/18 03:27:42] d2.utils.events INFO:  eta: 8:41:35  iter: 18839  total_loss: 36.67  loss_ce: 0.2813  loss_mask: 0.3902  loss_dice: 2.891  loss_ce_0: 0.5514  loss_mask_0: 0.3834  loss_dice_0: 3.012  loss_ce_1: 0.3254  loss_mask_1: 0.3963  loss_dice_1: 2.927  loss_ce_2: 0.3107  loss_mask_2: 0.3933  loss_dice_2: 2.904  loss_ce_3: 0.2799  loss_mask_3: 0.3898  loss_dice_3: 2.892  loss_ce_4: 0.3009  loss_mask_4: 0.3903  loss_dice_4: 2.885  loss_ce_5: 0.2792  loss_mask_5: 0.3892  loss_dice_5: 2.885  loss_ce_6: 0.2848  loss_mask_6: 0.3903  loss_dice_6: 2.88  loss_ce_7: 0.2882  loss_mask_7: 0.3881  loss_dice_7: 2.886  loss_ce_8: 0.2758  loss_mask_8: 0.3913  loss_dice_8: 2.888  time: 1.4973  data_time: 0.0781  lr: 5.638e-06  max_mem: 21589M
[01/18 03:28:12] d2.utils.events INFO:  eta: 8:41:05  iter: 18859  total_loss: 37.13  loss_ce: 0.2967  loss_mask: 0.3842  loss_dice: 2.984  loss_ce_0: 0.6245  loss_mask_0: 0.3791  loss_dice_0: 3.117  loss_ce_1: 0.3377  loss_mask_1: 0.387  loss_dice_1: 3.033  loss_ce_2: 0.3088  loss_mask_2: 0.3834  loss_dice_2: 3.005  loss_ce_3: 0.3027  loss_mask_3: 0.3837  loss_dice_3: 2.995  loss_ce_4: 0.2995  loss_mask_4: 0.3832  loss_dice_4: 2.996  loss_ce_5: 0.2791  loss_mask_5: 0.3831  loss_dice_5: 2.99  loss_ce_6: 0.3153  loss_mask_6: 0.3833  loss_dice_6: 2.991  loss_ce_7: 0.2982  loss_mask_7: 0.384  loss_dice_7: 2.984  loss_ce_8: 0.2778  loss_mask_8: 0.3848  loss_dice_8: 2.989  time: 1.4973  data_time: 0.0766  lr: 5.6332e-06  max_mem: 21589M
[01/18 03:28:41] d2.utils.events INFO:  eta: 8:40:29  iter: 18879  total_loss: 36.91  loss_ce: 0.3057  loss_mask: 0.3947  loss_dice: 2.925  loss_ce_0: 0.5607  loss_mask_0: 0.3815  loss_dice_0: 3.057  loss_ce_1: 0.3237  loss_mask_1: 0.3955  loss_dice_1: 2.965  loss_ce_2: 0.3247  loss_mask_2: 0.3959  loss_dice_2: 2.945  loss_ce_3: 0.2959  loss_mask_3: 0.3978  loss_dice_3: 2.926  loss_ce_4: 0.3047  loss_mask_4: 0.3948  loss_dice_4: 2.933  loss_ce_5: 0.2983  loss_mask_5: 0.3948  loss_dice_5: 2.932  loss_ce_6: 0.2915  loss_mask_6: 0.3958  loss_dice_6: 2.935  loss_ce_7: 0.2956  loss_mask_7: 0.3946  loss_dice_7: 2.927  loss_ce_8: 0.3084  loss_mask_8: 0.3938  loss_dice_8: 2.92  time: 1.4973  data_time: 0.0755  lr: 5.6285e-06  max_mem: 21589M
[01/18 03:29:11] d2.utils.events INFO:  eta: 8:39:56  iter: 18899  total_loss: 36.93  loss_ce: 0.301  loss_mask: 0.3903  loss_dice: 2.94  loss_ce_0: 0.5678  loss_mask_0: 0.3815  loss_dice_0: 3.072  loss_ce_1: 0.35  loss_mask_1: 0.3913  loss_dice_1: 2.98  loss_ce_2: 0.3421  loss_mask_2: 0.3901  loss_dice_2: 2.956  loss_ce_3: 0.3257  loss_mask_3: 0.3907  loss_dice_3: 2.955  loss_ce_4: 0.3019  loss_mask_4: 0.3892  loss_dice_4: 2.948  loss_ce_5: 0.292  loss_mask_5: 0.3886  loss_dice_5: 2.949  loss_ce_6: 0.298  loss_mask_6: 0.3899  loss_dice_6: 2.935  loss_ce_7: 0.293  loss_mask_7: 0.3893  loss_dice_7: 2.943  loss_ce_8: 0.2879  loss_mask_8: 0.3895  loss_dice_8: 2.946  time: 1.4973  data_time: 0.0697  lr: 5.6237e-06  max_mem: 21589M
[01/18 03:29:41] d2.utils.events INFO:  eta: 8:39:31  iter: 18919  total_loss: 37.68  loss_ce: 0.2965  loss_mask: 0.3884  loss_dice: 2.984  loss_ce_0: 0.5933  loss_mask_0: 0.3904  loss_dice_0: 3.111  loss_ce_1: 0.3295  loss_mask_1: 0.3998  loss_dice_1: 3.025  loss_ce_2: 0.3307  loss_mask_2: 0.3964  loss_dice_2: 3.013  loss_ce_3: 0.302  loss_mask_3: 0.3903  loss_dice_3: 2.993  loss_ce_4: 0.2992  loss_mask_4: 0.3902  loss_dice_4: 2.989  loss_ce_5: 0.2917  loss_mask_5: 0.3907  loss_dice_5: 2.993  loss_ce_6: 0.2893  loss_mask_6: 0.3885  loss_dice_6: 2.988  loss_ce_7: 0.2837  loss_mask_7: 0.3885  loss_dice_7: 2.99  loss_ce_8: 0.2799  loss_mask_8: 0.3883  loss_dice_8: 2.992  time: 1.4973  data_time: 0.0718  lr: 5.6189e-06  max_mem: 21589M
[01/18 03:30:10] d2.utils.events INFO:  eta: 8:38:54  iter: 18939  total_loss: 36.6  loss_ce: 0.2993  loss_mask: 0.389  loss_dice: 2.864  loss_ce_0: 0.5959  loss_mask_0: 0.3793  loss_dice_0: 3.023  loss_ce_1: 0.3369  loss_mask_1: 0.3852  loss_dice_1: 2.932  loss_ce_2: 0.3218  loss_mask_2: 0.3872  loss_dice_2: 2.915  loss_ce_3: 0.3044  loss_mask_3: 0.3889  loss_dice_3: 2.886  loss_ce_4: 0.3176  loss_mask_4: 0.3879  loss_dice_4: 2.877  loss_ce_5: 0.2937  loss_mask_5: 0.387  loss_dice_5: 2.88  loss_ce_6: 0.2848  loss_mask_6: 0.3875  loss_dice_6: 2.874  loss_ce_7: 0.2788  loss_mask_7: 0.3887  loss_dice_7: 2.878  loss_ce_8: 0.2801  loss_mask_8: 0.3895  loss_dice_8: 2.88  time: 1.4972  data_time: 0.0714  lr: 5.6141e-06  max_mem: 21589M
[01/18 03:30:40] d2.utils.events INFO:  eta: 8:38:17  iter: 18959  total_loss: 36.25  loss_ce: 0.2811  loss_mask: 0.3898  loss_dice: 2.922  loss_ce_0: 0.555  loss_mask_0: 0.381  loss_dice_0: 3.053  loss_ce_1: 0.3029  loss_mask_1: 0.3968  loss_dice_1: 2.967  loss_ce_2: 0.3054  loss_mask_2: 0.3949  loss_dice_2: 2.945  loss_ce_3: 0.2868  loss_mask_3: 0.3938  loss_dice_3: 2.928  loss_ce_4: 0.2822  loss_mask_4: 0.3941  loss_dice_4: 2.932  loss_ce_5: 0.2817  loss_mask_5: 0.3912  loss_dice_5: 2.937  loss_ce_6: 0.2647  loss_mask_6: 0.3912  loss_dice_6: 2.933  loss_ce_7: 0.2733  loss_mask_7: 0.3921  loss_dice_7: 2.924  loss_ce_8: 0.2724  loss_mask_8: 0.3918  loss_dice_8: 2.927  time: 1.4972  data_time: 0.0702  lr: 5.6093e-06  max_mem: 21589M
[01/18 03:31:09] d2.utils.events INFO:  eta: 8:37:27  iter: 18979  total_loss: 37.15  loss_ce: 0.2924  loss_mask: 0.3827  loss_dice: 2.938  loss_ce_0: 0.6023  loss_mask_0: 0.383  loss_dice_0: 3.075  loss_ce_1: 0.3318  loss_mask_1: 0.3905  loss_dice_1: 2.981  loss_ce_2: 0.327  loss_mask_2: 0.3841  loss_dice_2: 2.952  loss_ce_3: 0.3147  loss_mask_3: 0.383  loss_dice_3: 2.949  loss_ce_4: 0.3051  loss_mask_4: 0.3817  loss_dice_4: 2.944  loss_ce_5: 0.3025  loss_mask_5: 0.3839  loss_dice_5: 2.944  loss_ce_6: 0.3055  loss_mask_6: 0.3831  loss_dice_6: 2.94  loss_ce_7: 0.3037  loss_mask_7: 0.3843  loss_dice_7: 2.952  loss_ce_8: 0.3088  loss_mask_8: 0.3823  loss_dice_8: 2.94  time: 1.4972  data_time: 0.0670  lr: 5.6045e-06  max_mem: 21589M
[01/18 03:31:38] d2.utils.events INFO:  eta: 8:36:50  iter: 18999  total_loss: 36.74  loss_ce: 0.281  loss_mask: 0.3924  loss_dice: 2.928  loss_ce_0: 0.5715  loss_mask_0: 0.3913  loss_dice_0: 3.057  loss_ce_1: 0.3212  loss_mask_1: 0.3975  loss_dice_1: 2.974  loss_ce_2: 0.3277  loss_mask_2: 0.3956  loss_dice_2: 2.957  loss_ce_3: 0.2957  loss_mask_3: 0.3947  loss_dice_3: 2.931  loss_ce_4: 0.2933  loss_mask_4: 0.3965  loss_dice_4: 2.933  loss_ce_5: 0.2843  loss_mask_5: 0.3948  loss_dice_5: 2.934  loss_ce_6: 0.278  loss_mask_6: 0.3942  loss_dice_6: 2.933  loss_ce_7: 0.3006  loss_mask_7: 0.3939  loss_dice_7: 2.935  loss_ce_8: 0.2735  loss_mask_8: 0.3935  loss_dice_8: 2.936  time: 1.4971  data_time: 0.0766  lr: 5.5997e-06  max_mem: 21589M
[01/18 03:32:08] d2.utils.events INFO:  eta: 8:36:22  iter: 19019  total_loss: 36.33  loss_ce: 0.2805  loss_mask: 0.3939  loss_dice: 2.901  loss_ce_0: 0.5616  loss_mask_0: 0.3855  loss_dice_0: 3.038  loss_ce_1: 0.3175  loss_mask_1: 0.3987  loss_dice_1: 2.935  loss_ce_2: 0.3063  loss_mask_2: 0.3977  loss_dice_2: 2.913  loss_ce_3: 0.3019  loss_mask_3: 0.3964  loss_dice_3: 2.907  loss_ce_4: 0.2853  loss_mask_4: 0.396  loss_dice_4: 2.904  loss_ce_5: 0.2837  loss_mask_5: 0.3949  loss_dice_5: 2.903  loss_ce_6: 0.2786  loss_mask_6: 0.3937  loss_dice_6: 2.898  loss_ce_7: 0.2813  loss_mask_7: 0.3947  loss_dice_7: 2.897  loss_ce_8: 0.2782  loss_mask_8: 0.393  loss_dice_8: 2.901  time: 1.4971  data_time: 0.0754  lr: 5.5949e-06  max_mem: 21589M
[01/18 03:32:37] d2.utils.events INFO:  eta: 8:36:03  iter: 19039  total_loss: 37.26  loss_ce: 0.2713  loss_mask: 0.3882  loss_dice: 2.942  loss_ce_0: 0.5713  loss_mask_0: 0.3807  loss_dice_0: 3.083  loss_ce_1: 0.3245  loss_mask_1: 0.3915  loss_dice_1: 2.989  loss_ce_2: 0.2953  loss_mask_2: 0.3878  loss_dice_2: 2.962  loss_ce_3: 0.3014  loss_mask_3: 0.3888  loss_dice_3: 2.962  loss_ce_4: 0.2886  loss_mask_4: 0.3858  loss_dice_4: 2.963  loss_ce_5: 0.2802  loss_mask_5: 0.3847  loss_dice_5: 2.955  loss_ce_6: 0.2797  loss_mask_6: 0.387  loss_dice_6: 2.952  loss_ce_7: 0.2737  loss_mask_7: 0.3861  loss_dice_7: 2.939  loss_ce_8: 0.2706  loss_mask_8: 0.3877  loss_dice_8: 2.951  time: 1.4971  data_time: 0.0747  lr: 5.5901e-06  max_mem: 21589M
[01/18 03:33:07] d2.utils.events INFO:  eta: 8:35:41  iter: 19059  total_loss: 37.08  loss_ce: 0.2884  loss_mask: 0.3894  loss_dice: 2.962  loss_ce_0: 0.5719  loss_mask_0: 0.3802  loss_dice_0: 3.088  loss_ce_1: 0.3418  loss_mask_1: 0.3926  loss_dice_1: 2.99  loss_ce_2: 0.3377  loss_mask_2: 0.3889  loss_dice_2: 2.972  loss_ce_3: 0.3147  loss_mask_3: 0.386  loss_dice_3: 2.958  loss_ce_4: 0.3099  loss_mask_4: 0.3893  loss_dice_4: 2.96  loss_ce_5: 0.2888  loss_mask_5: 0.3886  loss_dice_5: 2.963  loss_ce_6: 0.2929  loss_mask_6: 0.3896  loss_dice_6: 2.965  loss_ce_7: 0.2966  loss_mask_7: 0.3883  loss_dice_7: 2.958  loss_ce_8: 0.2836  loss_mask_8: 0.3887  loss_dice_8: 2.965  time: 1.4970  data_time: 0.0709  lr: 5.5853e-06  max_mem: 21589M
[01/18 03:33:37] d2.utils.events INFO:  eta: 8:35:14  iter: 19079  total_loss: 37.33  loss_ce: 0.3098  loss_mask: 0.387  loss_dice: 2.964  loss_ce_0: 0.5771  loss_mask_0: 0.3821  loss_dice_0: 3.101  loss_ce_1: 0.3174  loss_mask_1: 0.3929  loss_dice_1: 3.011  loss_ce_2: 0.3093  loss_mask_2: 0.3915  loss_dice_2: 2.977  loss_ce_3: 0.3121  loss_mask_3: 0.3883  loss_dice_3: 2.97  loss_ce_4: 0.294  loss_mask_4: 0.3904  loss_dice_4: 2.966  loss_ce_5: 0.3166  loss_mask_5: 0.389  loss_dice_5: 2.961  loss_ce_6: 0.3017  loss_mask_6: 0.3871  loss_dice_6: 2.964  loss_ce_7: 0.2918  loss_mask_7: 0.3873  loss_dice_7: 2.968  loss_ce_8: 0.3052  loss_mask_8: 0.3881  loss_dice_8: 2.973  time: 1.4970  data_time: 0.0805  lr: 5.5805e-06  max_mem: 21589M
[01/18 03:34:06] d2.utils.events INFO:  eta: 8:34:43  iter: 19099  total_loss: 37.23  loss_ce: 0.2882  loss_mask: 0.3805  loss_dice: 2.943  loss_ce_0: 0.5713  loss_mask_0: 0.372  loss_dice_0: 3.093  loss_ce_1: 0.323  loss_mask_1: 0.3871  loss_dice_1: 2.987  loss_ce_2: 0.3269  loss_mask_2: 0.384  loss_dice_2: 2.964  loss_ce_3: 0.314  loss_mask_3: 0.3811  loss_dice_3: 2.949  loss_ce_4: 0.299  loss_mask_4: 0.3808  loss_dice_4: 2.949  loss_ce_5: 0.2984  loss_mask_5: 0.382  loss_dice_5: 2.946  loss_ce_6: 0.3017  loss_mask_6: 0.3787  loss_dice_6: 2.946  loss_ce_7: 0.2982  loss_mask_7: 0.3802  loss_dice_7: 2.941  loss_ce_8: 0.295  loss_mask_8: 0.3802  loss_dice_8: 2.951  time: 1.4970  data_time: 0.0788  lr: 5.5757e-06  max_mem: 21589M
[01/18 03:34:35] d2.utils.events INFO:  eta: 8:34:13  iter: 19119  total_loss: 36.26  loss_ce: 0.3057  loss_mask: 0.3877  loss_dice: 2.887  loss_ce_0: 0.5431  loss_mask_0: 0.3802  loss_dice_0: 3.019  loss_ce_1: 0.336  loss_mask_1: 0.3897  loss_dice_1: 2.928  loss_ce_2: 0.3383  loss_mask_2: 0.388  loss_dice_2: 2.906  loss_ce_3: 0.3347  loss_mask_3: 0.3857  loss_dice_3: 2.886  loss_ce_4: 0.3284  loss_mask_4: 0.3849  loss_dice_4: 2.894  loss_ce_5: 0.3196  loss_mask_5: 0.3835  loss_dice_5: 2.886  loss_ce_6: 0.3222  loss_mask_6: 0.3851  loss_dice_6: 2.887  loss_ce_7: 0.3158  loss_mask_7: 0.3873  loss_dice_7: 2.894  loss_ce_8: 0.2924  loss_mask_8: 0.3873  loss_dice_8: 2.891  time: 1.4970  data_time: 0.0716  lr: 5.5709e-06  max_mem: 21589M
[01/18 03:35:05] d2.utils.events INFO:  eta: 8:33:44  iter: 19139  total_loss: 37.89  loss_ce: 0.3336  loss_mask: 0.3895  loss_dice: 2.981  loss_ce_0: 0.5823  loss_mask_0: 0.3823  loss_dice_0: 3.104  loss_ce_1: 0.3743  loss_mask_1: 0.3955  loss_dice_1: 3.023  loss_ce_2: 0.3699  loss_mask_2: 0.3921  loss_dice_2: 2.999  loss_ce_3: 0.3422  loss_mask_3: 0.3904  loss_dice_3: 2.973  loss_ce_4: 0.3397  loss_mask_4: 0.3888  loss_dice_4: 2.979  loss_ce_5: 0.321  loss_mask_5: 0.3892  loss_dice_5: 2.982  loss_ce_6: 0.3415  loss_mask_6: 0.3879  loss_dice_6: 2.981  loss_ce_7: 0.3411  loss_mask_7: 0.391  loss_dice_7: 2.982  loss_ce_8: 0.3257  loss_mask_8: 0.3901  loss_dice_8: 2.975  time: 1.4970  data_time: 0.0759  lr: 5.5661e-06  max_mem: 21589M
[01/18 03:35:35] d2.utils.events INFO:  eta: 8:33:24  iter: 19159  total_loss: 36.93  loss_ce: 0.2987  loss_mask: 0.3772  loss_dice: 2.927  loss_ce_0: 0.5736  loss_mask_0: 0.3795  loss_dice_0: 3.052  loss_ce_1: 0.3472  loss_mask_1: 0.3842  loss_dice_1: 2.975  loss_ce_2: 0.328  loss_mask_2: 0.38  loss_dice_2: 2.956  loss_ce_3: 0.3144  loss_mask_3: 0.3809  loss_dice_3: 2.933  loss_ce_4: 0.3159  loss_mask_4: 0.3795  loss_dice_4: 2.934  loss_ce_5: 0.3126  loss_mask_5: 0.3787  loss_dice_5: 2.938  loss_ce_6: 0.3075  loss_mask_6: 0.3773  loss_dice_6: 2.941  loss_ce_7: 0.3103  loss_mask_7: 0.3796  loss_dice_7: 2.931  loss_ce_8: 0.3127  loss_mask_8: 0.3779  loss_dice_8: 2.933  time: 1.4969  data_time: 0.0764  lr: 5.5613e-06  max_mem: 21589M
[01/18 03:36:04] d2.utils.events INFO:  eta: 8:32:44  iter: 19179  total_loss: 36.81  loss_ce: 0.2805  loss_mask: 0.3851  loss_dice: 2.94  loss_ce_0: 0.5607  loss_mask_0: 0.3751  loss_dice_0: 3.078  loss_ce_1: 0.3145  loss_mask_1: 0.3853  loss_dice_1: 2.989  loss_ce_2: 0.3142  loss_mask_2: 0.3848  loss_dice_2: 2.966  loss_ce_3: 0.2985  loss_mask_3: 0.384  loss_dice_3: 2.951  loss_ce_4: 0.2983  loss_mask_4: 0.3845  loss_dice_4: 2.95  loss_ce_5: 0.2858  loss_mask_5: 0.3817  loss_dice_5: 2.947  loss_ce_6: 0.28  loss_mask_6: 0.3827  loss_dice_6: 2.943  loss_ce_7: 0.2767  loss_mask_7: 0.3833  loss_dice_7: 2.939  loss_ce_8: 0.2843  loss_mask_8: 0.3839  loss_dice_8: 2.948  time: 1.4969  data_time: 0.0701  lr: 5.5565e-06  max_mem: 21589M
[01/18 03:36:34] d2.utils.events INFO:  eta: 8:32:06  iter: 19199  total_loss: 36.79  loss_ce: 0.2778  loss_mask: 0.3905  loss_dice: 2.943  loss_ce_0: 0.5435  loss_mask_0: 0.3904  loss_dice_0: 3.072  loss_ce_1: 0.3172  loss_mask_1: 0.3967  loss_dice_1: 2.981  loss_ce_2: 0.303  loss_mask_2: 0.3932  loss_dice_2: 2.963  loss_ce_3: 0.2911  loss_mask_3: 0.3903  loss_dice_3: 2.953  loss_ce_4: 0.2847  loss_mask_4: 0.3901  loss_dice_4: 2.949  loss_ce_5: 0.3014  loss_mask_5: 0.3896  loss_dice_5: 2.945  loss_ce_6: 0.2815  loss_mask_6: 0.3907  loss_dice_6: 2.939  loss_ce_7: 0.3018  loss_mask_7: 0.3896  loss_dice_7: 2.928  loss_ce_8: 0.2876  loss_mask_8: 0.3904  loss_dice_8: 2.945  time: 1.4969  data_time: 0.0685  lr: 5.5516e-06  max_mem: 21589M
[01/18 03:37:03] d2.utils.events INFO:  eta: 8:31:26  iter: 19219  total_loss: 37.17  loss_ce: 0.3041  loss_mask: 0.3968  loss_dice: 2.938  loss_ce_0: 0.5618  loss_mask_0: 0.3935  loss_dice_0: 3.086  loss_ce_1: 0.3484  loss_mask_1: 0.3976  loss_dice_1: 2.98  loss_ce_2: 0.3492  loss_mask_2: 0.3972  loss_dice_2: 2.947  loss_ce_3: 0.3303  loss_mask_3: 0.3977  loss_dice_3: 2.937  loss_ce_4: 0.3074  loss_mask_4: 0.3989  loss_dice_4: 2.945  loss_ce_5: 0.3024  loss_mask_5: 0.3984  loss_dice_5: 2.938  loss_ce_6: 0.2969  loss_mask_6: 0.3991  loss_dice_6: 2.937  loss_ce_7: 0.3073  loss_mask_7: 0.3965  loss_dice_7: 2.936  loss_ce_8: 0.3121  loss_mask_8: 0.3974  loss_dice_8: 2.946  time: 1.4968  data_time: 0.0721  lr: 5.5468e-06  max_mem: 21589M
[01/18 03:37:32] d2.utils.events INFO:  eta: 8:30:37  iter: 19239  total_loss: 36.55  loss_ce: 0.2905  loss_mask: 0.3859  loss_dice: 2.928  loss_ce_0: 0.5409  loss_mask_0: 0.3847  loss_dice_0: 3.06  loss_ce_1: 0.3372  loss_mask_1: 0.3966  loss_dice_1: 2.963  loss_ce_2: 0.3234  loss_mask_2: 0.3916  loss_dice_2: 2.948  loss_ce_3: 0.3283  loss_mask_3: 0.3873  loss_dice_3: 2.943  loss_ce_4: 0.3133  loss_mask_4: 0.3884  loss_dice_4: 2.931  loss_ce_5: 0.3101  loss_mask_5: 0.386  loss_dice_5: 2.938  loss_ce_6: 0.2984  loss_mask_6: 0.3873  loss_dice_6: 2.938  loss_ce_7: 0.3016  loss_mask_7: 0.386  loss_dice_7: 2.937  loss_ce_8: 0.3014  loss_mask_8: 0.3862  loss_dice_8: 2.935  time: 1.4968  data_time: 0.0763  lr: 5.542e-06  max_mem: 21589M
[01/18 03:38:02] d2.utils.events INFO:  eta: 8:30:07  iter: 19259  total_loss: 36.55  loss_ce: 0.3244  loss_mask: 0.3764  loss_dice: 2.919  loss_ce_0: 0.5969  loss_mask_0: 0.3704  loss_dice_0: 3.058  loss_ce_1: 0.342  loss_mask_1: 0.3827  loss_dice_1: 2.957  loss_ce_2: 0.3355  loss_mask_2: 0.3796  loss_dice_2: 2.936  loss_ce_3: 0.3289  loss_mask_3: 0.3779  loss_dice_3: 2.926  loss_ce_4: 0.3316  loss_mask_4: 0.3775  loss_dice_4: 2.918  loss_ce_5: 0.3194  loss_mask_5: 0.3782  loss_dice_5: 2.916  loss_ce_6: 0.3244  loss_mask_6: 0.3775  loss_dice_6: 2.921  loss_ce_7: 0.3135  loss_mask_7: 0.376  loss_dice_7: 2.923  loss_ce_8: 0.3071  loss_mask_8: 0.3784  loss_dice_8: 2.923  time: 1.4968  data_time: 0.0693  lr: 5.5372e-06  max_mem: 21589M
[01/18 03:38:31] d2.utils.events INFO:  eta: 8:29:31  iter: 19279  total_loss: 36.81  loss_ce: 0.3049  loss_mask: 0.3978  loss_dice: 2.934  loss_ce_0: 0.5875  loss_mask_0: 0.3888  loss_dice_0: 3.071  loss_ce_1: 0.3328  loss_mask_1: 0.4005  loss_dice_1: 2.977  loss_ce_2: 0.3159  loss_mask_2: 0.3986  loss_dice_2: 2.942  loss_ce_3: 0.3082  loss_mask_3: 0.3993  loss_dice_3: 2.941  loss_ce_4: 0.299  loss_mask_4: 0.3997  loss_dice_4: 2.935  loss_ce_5: 0.3034  loss_mask_5: 0.4002  loss_dice_5: 2.937  loss_ce_6: 0.3023  loss_mask_6: 0.3977  loss_dice_6: 2.95  loss_ce_7: 0.293  loss_mask_7: 0.3978  loss_dice_7: 2.944  loss_ce_8: 0.2934  loss_mask_8: 0.3968  loss_dice_8: 2.931  time: 1.4967  data_time: 0.0717  lr: 5.5324e-06  max_mem: 21589M
[01/18 03:39:00] d2.utils.events INFO:  eta: 8:28:52  iter: 19299  total_loss: 36.55  loss_ce: 0.2986  loss_mask: 0.3864  loss_dice: 2.932  loss_ce_0: 0.5589  loss_mask_0: 0.3817  loss_dice_0: 3.075  loss_ce_1: 0.3331  loss_mask_1: 0.3889  loss_dice_1: 2.98  loss_ce_2: 0.3129  loss_mask_2: 0.3854  loss_dice_2: 2.965  loss_ce_3: 0.3241  loss_mask_3: 0.3829  loss_dice_3: 2.944  loss_ce_4: 0.3002  loss_mask_4: 0.3819  loss_dice_4: 2.954  loss_ce_5: 0.3042  loss_mask_5: 0.3848  loss_dice_5: 2.952  loss_ce_6: 0.2982  loss_mask_6: 0.3834  loss_dice_6: 2.947  loss_ce_7: 0.3158  loss_mask_7: 0.3832  loss_dice_7: 2.942  loss_ce_8: 0.2839  loss_mask_8: 0.3832  loss_dice_8: 2.94  time: 1.4967  data_time: 0.0720  lr: 5.5276e-06  max_mem: 21589M
[01/18 03:39:30] d2.utils.events INFO:  eta: 8:28:14  iter: 19319  total_loss: 37.28  loss_ce: 0.2973  loss_mask: 0.3857  loss_dice: 2.968  loss_ce_0: 0.5948  loss_mask_0: 0.3827  loss_dice_0: 3.094  loss_ce_1: 0.3594  loss_mask_1: 0.3905  loss_dice_1: 3  loss_ce_2: 0.3403  loss_mask_2: 0.387  loss_dice_2: 2.987  loss_ce_3: 0.3387  loss_mask_3: 0.3848  loss_dice_3: 2.974  loss_ce_4: 0.3265  loss_mask_4: 0.3859  loss_dice_4: 2.969  loss_ce_5: 0.3114  loss_mask_5: 0.3847  loss_dice_5: 2.966  loss_ce_6: 0.3201  loss_mask_6: 0.3855  loss_dice_6: 2.972  loss_ce_7: 0.3012  loss_mask_7: 0.3856  loss_dice_7: 2.97  loss_ce_8: 0.3327  loss_mask_8: 0.386  loss_dice_8: 2.967  time: 1.4967  data_time: 0.0705  lr: 5.5228e-06  max_mem: 21589M
[01/18 03:40:00] d2.utils.events INFO:  eta: 8:27:26  iter: 19339  total_loss: 36.08  loss_ce: 0.2801  loss_mask: 0.3816  loss_dice: 2.931  loss_ce_0: 0.5332  loss_mask_0: 0.3727  loss_dice_0: 3.068  loss_ce_1: 0.3048  loss_mask_1: 0.3868  loss_dice_1: 2.977  loss_ce_2: 0.2973  loss_mask_2: 0.3836  loss_dice_2: 2.964  loss_ce_3: 0.2831  loss_mask_3: 0.3799  loss_dice_3: 2.949  loss_ce_4: 0.2757  loss_mask_4: 0.3802  loss_dice_4: 2.943  loss_ce_5: 0.2776  loss_mask_5: 0.383  loss_dice_5: 2.946  loss_ce_6: 0.2747  loss_mask_6: 0.3826  loss_dice_6: 2.938  loss_ce_7: 0.2766  loss_mask_7: 0.3829  loss_dice_7: 2.945  loss_ce_8: 0.2851  loss_mask_8: 0.3816  loss_dice_8: 2.941  time: 1.4967  data_time: 0.0718  lr: 5.518e-06  max_mem: 21589M
[01/18 03:40:29] d2.utils.events INFO:  eta: 8:26:49  iter: 19359  total_loss: 36.65  loss_ce: 0.3066  loss_mask: 0.3942  loss_dice: 2.906  loss_ce_0: 0.568  loss_mask_0: 0.3822  loss_dice_0: 3.025  loss_ce_1: 0.3377  loss_mask_1: 0.3959  loss_dice_1: 2.937  loss_ce_2: 0.3475  loss_mask_2: 0.3926  loss_dice_2: 2.911  loss_ce_3: 0.3303  loss_mask_3: 0.3912  loss_dice_3: 2.902  loss_ce_4: 0.3151  loss_mask_4: 0.3925  loss_dice_4: 2.9  loss_ce_5: 0.3088  loss_mask_5: 0.3937  loss_dice_5: 2.901  loss_ce_6: 0.3028  loss_mask_6: 0.3935  loss_dice_6: 2.905  loss_ce_7: 0.2987  loss_mask_7: 0.3945  loss_dice_7: 2.904  loss_ce_8: 0.3127  loss_mask_8: 0.3923  loss_dice_8: 2.911  time: 1.4967  data_time: 0.0716  lr: 5.5132e-06  max_mem: 21589M
[01/18 03:40:59] d2.utils.events INFO:  eta: 8:26:22  iter: 19379  total_loss: 37.3  loss_ce: 0.2966  loss_mask: 0.3952  loss_dice: 2.955  loss_ce_0: 0.5668  loss_mask_0: 0.3881  loss_dice_0: 3.11  loss_ce_1: 0.3321  loss_mask_1: 0.3993  loss_dice_1: 3.002  loss_ce_2: 0.3226  loss_mask_2: 0.3962  loss_dice_2: 2.988  loss_ce_3: 0.3212  loss_mask_3: 0.3946  loss_dice_3: 2.978  loss_ce_4: 0.3108  loss_mask_4: 0.3935  loss_dice_4: 2.98  loss_ce_5: 0.2963  loss_mask_5: 0.3933  loss_dice_5: 2.965  loss_ce_6: 0.3062  loss_mask_6: 0.3922  loss_dice_6: 2.967  loss_ce_7: 0.3  loss_mask_7: 0.3935  loss_dice_7: 2.967  loss_ce_8: 0.3076  loss_mask_8: 0.3929  loss_dice_8: 2.96  time: 1.4966  data_time: 0.0735  lr: 5.5084e-06  max_mem: 21589M
[01/18 03:41:28] d2.utils.events INFO:  eta: 8:25:47  iter: 19399  total_loss: 36.03  loss_ce: 0.2818  loss_mask: 0.3863  loss_dice: 2.888  loss_ce_0: 0.5807  loss_mask_0: 0.3722  loss_dice_0: 3.03  loss_ce_1: 0.3096  loss_mask_1: 0.3881  loss_dice_1: 2.926  loss_ce_2: 0.2833  loss_mask_2: 0.3861  loss_dice_2: 2.911  loss_ce_3: 0.285  loss_mask_3: 0.3867  loss_dice_3: 2.894  loss_ce_4: 0.2781  loss_mask_4: 0.3863  loss_dice_4: 2.886  loss_ce_5: 0.2719  loss_mask_5: 0.387  loss_dice_5: 2.889  loss_ce_6: 0.2749  loss_mask_6: 0.387  loss_dice_6: 2.888  loss_ce_7: 0.2782  loss_mask_7: 0.3884  loss_dice_7: 2.899  loss_ce_8: 0.2819  loss_mask_8: 0.3857  loss_dice_8: 2.892  time: 1.4966  data_time: 0.0784  lr: 5.5036e-06  max_mem: 21589M
[01/18 03:41:58] d2.utils.events INFO:  eta: 8:25:06  iter: 19419  total_loss: 36.66  loss_ce: 0.2715  loss_mask: 0.3921  loss_dice: 2.892  loss_ce_0: 0.5984  loss_mask_0: 0.3831  loss_dice_0: 3.042  loss_ce_1: 0.3325  loss_mask_1: 0.3967  loss_dice_1: 2.955  loss_ce_2: 0.314  loss_mask_2: 0.3942  loss_dice_2: 2.926  loss_ce_3: 0.2802  loss_mask_3: 0.3943  loss_dice_3: 2.915  loss_ce_4: 0.2953  loss_mask_4: 0.3912  loss_dice_4: 2.907  loss_ce_5: 0.2926  loss_mask_5: 0.3934  loss_dice_5: 2.909  loss_ce_6: 0.2873  loss_mask_6: 0.3938  loss_dice_6: 2.9  loss_ce_7: 0.2842  loss_mask_7: 0.3921  loss_dice_7: 2.916  loss_ce_8: 0.2879  loss_mask_8: 0.3927  loss_dice_8: 2.907  time: 1.4966  data_time: 0.0672  lr: 5.4988e-06  max_mem: 21589M
[01/18 03:42:27] d2.utils.events INFO:  eta: 8:24:31  iter: 19439  total_loss: 36.5  loss_ce: 0.2865  loss_mask: 0.3929  loss_dice: 2.892  loss_ce_0: 0.5472  loss_mask_0: 0.3856  loss_dice_0: 3.04  loss_ce_1: 0.308  loss_mask_1: 0.3922  loss_dice_1: 2.955  loss_ce_2: 0.3163  loss_mask_2: 0.3899  loss_dice_2: 2.921  loss_ce_3: 0.2946  loss_mask_3: 0.3891  loss_dice_3: 2.908  loss_ce_4: 0.289  loss_mask_4: 0.3916  loss_dice_4: 2.903  loss_ce_5: 0.2808  loss_mask_5: 0.3899  loss_dice_5: 2.909  loss_ce_6: 0.2964  loss_mask_6: 0.3931  loss_dice_6: 2.913  loss_ce_7: 0.2829  loss_mask_7: 0.3933  loss_dice_7: 2.905  loss_ce_8: 0.2828  loss_mask_8: 0.3938  loss_dice_8: 2.902  time: 1.4965  data_time: 0.0692  lr: 5.494e-06  max_mem: 21589M
[01/18 03:42:57] d2.utils.events INFO:  eta: 8:23:52  iter: 19459  total_loss: 36.9  loss_ce: 0.3031  loss_mask: 0.3868  loss_dice: 2.941  loss_ce_0: 0.5519  loss_mask_0: 0.3821  loss_dice_0: 3.07  loss_ce_1: 0.3268  loss_mask_1: 0.387  loss_dice_1: 2.982  loss_ce_2: 0.323  loss_mask_2: 0.3861  loss_dice_2: 2.951  loss_ce_3: 0.3037  loss_mask_3: 0.3859  loss_dice_3: 2.945  loss_ce_4: 0.3237  loss_mask_4: 0.3863  loss_dice_4: 2.934  loss_ce_5: 0.3062  loss_mask_5: 0.385  loss_dice_5: 2.938  loss_ce_6: 0.3047  loss_mask_6: 0.3864  loss_dice_6: 2.934  loss_ce_7: 0.2971  loss_mask_7: 0.3869  loss_dice_7: 2.942  loss_ce_8: 0.293  loss_mask_8: 0.387  loss_dice_8: 2.941  time: 1.4965  data_time: 0.0793  lr: 5.4892e-06  max_mem: 21589M
[01/18 03:43:27] d2.utils.events INFO:  eta: 8:23:32  iter: 19479  total_loss: 37.07  loss_ce: 0.3033  loss_mask: 0.3828  loss_dice: 2.981  loss_ce_0: 0.5403  loss_mask_0: 0.3809  loss_dice_0: 3.123  loss_ce_1: 0.3284  loss_mask_1: 0.3928  loss_dice_1: 3.03  loss_ce_2: 0.3189  loss_mask_2: 0.3886  loss_dice_2: 3.006  loss_ce_3: 0.2955  loss_mask_3: 0.3853  loss_dice_3: 2.991  loss_ce_4: 0.3018  loss_mask_4: 0.384  loss_dice_4: 2.986  loss_ce_5: 0.3044  loss_mask_5: 0.3836  loss_dice_5: 2.991  loss_ce_6: 0.3025  loss_mask_6: 0.3836  loss_dice_6: 2.986  loss_ce_7: 0.2948  loss_mask_7: 0.384  loss_dice_7: 2.989  loss_ce_8: 0.2859  loss_mask_8: 0.3835  loss_dice_8: 2.983  time: 1.4966  data_time: 0.0837  lr: 5.4843e-06  max_mem: 21589M
[01/18 03:43:58] d2.utils.events INFO:  eta: 8:23:32  iter: 19499  total_loss: 36.98  loss_ce: 0.2894  loss_mask: 0.3955  loss_dice: 2.973  loss_ce_0: 0.5691  loss_mask_0: 0.3926  loss_dice_0: 3.089  loss_ce_1: 0.3341  loss_mask_1: 0.4036  loss_dice_1: 3.004  loss_ce_2: 0.3181  loss_mask_2: 0.3986  loss_dice_2: 2.981  loss_ce_3: 0.3169  loss_mask_3: 0.3932  loss_dice_3: 2.979  loss_ce_4: 0.3055  loss_mask_4: 0.3955  loss_dice_4: 2.974  loss_ce_5: 0.2997  loss_mask_5: 0.3939  loss_dice_5: 2.972  loss_ce_6: 0.3062  loss_mask_6: 0.3949  loss_dice_6: 2.969  loss_ce_7: 0.2974  loss_mask_7: 0.3958  loss_dice_7: 2.964  loss_ce_8: 0.2917  loss_mask_8: 0.3969  loss_dice_8: 2.969  time: 1.4966  data_time: 0.0800  lr: 5.4795e-06  max_mem: 21589M
[01/18 03:44:28] d2.utils.events INFO:  eta: 8:23:24  iter: 19519  total_loss: 37.44  loss_ce: 0.2934  loss_mask: 0.3892  loss_dice: 2.966  loss_ce_0: 0.5586  loss_mask_0: 0.3889  loss_dice_0: 3.082  loss_ce_1: 0.3282  loss_mask_1: 0.3916  loss_dice_1: 2.993  loss_ce_2: 0.3274  loss_mask_2: 0.3911  loss_dice_2: 2.982  loss_ce_3: 0.3131  loss_mask_3: 0.3885  loss_dice_3: 2.975  loss_ce_4: 0.3145  loss_mask_4: 0.3911  loss_dice_4: 2.971  loss_ce_5: 0.2952  loss_mask_5: 0.3904  loss_dice_5: 2.965  loss_ce_6: 0.3041  loss_mask_6: 0.3896  loss_dice_6: 2.964  loss_ce_7: 0.3054  loss_mask_7: 0.3886  loss_dice_7: 2.964  loss_ce_8: 0.3039  loss_mask_8: 0.3885  loss_dice_8: 2.967  time: 1.4966  data_time: 0.0749  lr: 5.4747e-06  max_mem: 21589M
[01/18 03:44:58] d2.utils.events INFO:  eta: 8:22:57  iter: 19539  total_loss: 36.74  loss_ce: 0.3025  loss_mask: 0.3969  loss_dice: 2.919  loss_ce_0: 0.6034  loss_mask_0: 0.391  loss_dice_0: 3.043  loss_ce_1: 0.3332  loss_mask_1: 0.4049  loss_dice_1: 2.949  loss_ce_2: 0.3196  loss_mask_2: 0.3992  loss_dice_2: 2.937  loss_ce_3: 0.3168  loss_mask_3: 0.3968  loss_dice_3: 2.924  loss_ce_4: 0.2988  loss_mask_4: 0.397  loss_dice_4: 2.923  loss_ce_5: 0.3008  loss_mask_5: 0.3964  loss_dice_5: 2.924  loss_ce_6: 0.2965  loss_mask_6: 0.3975  loss_dice_6: 2.921  loss_ce_7: 0.2919  loss_mask_7: 0.3959  loss_dice_7: 2.924  loss_ce_8: 0.2891  loss_mask_8: 0.3972  loss_dice_8: 2.924  time: 1.4966  data_time: 0.0754  lr: 5.4699e-06  max_mem: 21589M
[01/18 03:45:28] d2.utils.events INFO:  eta: 8:22:28  iter: 19559  total_loss: 36.51  loss_ce: 0.3011  loss_mask: 0.3872  loss_dice: 2.908  loss_ce_0: 0.5621  loss_mask_0: 0.3904  loss_dice_0: 3.043  loss_ce_1: 0.3393  loss_mask_1: 0.3924  loss_dice_1: 2.953  loss_ce_2: 0.3076  loss_mask_2: 0.3907  loss_dice_2: 2.936  loss_ce_3: 0.2992  loss_mask_3: 0.3889  loss_dice_3: 2.921  loss_ce_4: 0.2893  loss_mask_4: 0.3888  loss_dice_4: 2.921  loss_ce_5: 0.297  loss_mask_5: 0.3887  loss_dice_5: 2.916  loss_ce_6: 0.2899  loss_mask_6: 0.3907  loss_dice_6: 2.914  loss_ce_7: 0.2939  loss_mask_7: 0.3904  loss_dice_7: 2.916  loss_ce_8: 0.2835  loss_mask_8: 0.3876  loss_dice_8: 2.916  time: 1.4966  data_time: 0.0731  lr: 5.4651e-06  max_mem: 21589M
[01/18 03:45:58] d2.utils.events INFO:  eta: 8:22:33  iter: 19579  total_loss: 36.37  loss_ce: 0.2808  loss_mask: 0.3926  loss_dice: 2.87  loss_ce_0: 0.5563  loss_mask_0: 0.3889  loss_dice_0: 2.999  loss_ce_1: 0.3154  loss_mask_1: 0.4006  loss_dice_1: 2.912  loss_ce_2: 0.3077  loss_mask_2: 0.3972  loss_dice_2: 2.88  loss_ce_3: 0.2875  loss_mask_3: 0.3949  loss_dice_3: 2.875  loss_ce_4: 0.293  loss_mask_4: 0.3931  loss_dice_4: 2.868  loss_ce_5: 0.2936  loss_mask_5: 0.3918  loss_dice_5: 2.878  loss_ce_6: 0.285  loss_mask_6: 0.3926  loss_dice_6: 2.875  loss_ce_7: 0.2745  loss_mask_7: 0.393  loss_dice_7: 2.88  loss_ce_8: 0.2766  loss_mask_8: 0.3936  loss_dice_8: 2.867  time: 1.4966  data_time: 0.0768  lr: 5.4603e-06  max_mem: 21589M
[01/18 03:46:28] d2.utils.events INFO:  eta: 8:22:05  iter: 19599  total_loss: 36.63  loss_ce: 0.2995  loss_mask: 0.3772  loss_dice: 2.885  loss_ce_0: 0.5978  loss_mask_0: 0.3745  loss_dice_0: 3.052  loss_ce_1: 0.3393  loss_mask_1: 0.3804  loss_dice_1: 2.949  loss_ce_2: 0.3206  loss_mask_2: 0.3777  loss_dice_2: 2.917  loss_ce_3: 0.3118  loss_mask_3: 0.3785  loss_dice_3: 2.898  loss_ce_4: 0.3067  loss_mask_4: 0.3776  loss_dice_4: 2.889  loss_ce_5: 0.2972  loss_mask_5: 0.3769  loss_dice_5: 2.888  loss_ce_6: 0.3038  loss_mask_6: 0.3773  loss_dice_6: 2.89  loss_ce_7: 0.3057  loss_mask_7: 0.3765  loss_dice_7: 2.889  loss_ce_8: 0.3015  loss_mask_8: 0.3768  loss_dice_8: 2.882  time: 1.4966  data_time: 0.0751  lr: 5.4555e-06  max_mem: 21589M
[01/18 03:46:57] d2.utils.events INFO:  eta: 8:21:34  iter: 19619  total_loss: 35.7  loss_ce: 0.2798  loss_mask: 0.3864  loss_dice: 2.841  loss_ce_0: 0.539  loss_mask_0: 0.38  loss_dice_0: 2.982  loss_ce_1: 0.3015  loss_mask_1: 0.3887  loss_dice_1: 2.89  loss_ce_2: 0.2965  loss_mask_2: 0.3857  loss_dice_2: 2.866  loss_ce_3: 0.2771  loss_mask_3: 0.3837  loss_dice_3: 2.857  loss_ce_4: 0.2664  loss_mask_4: 0.3827  loss_dice_4: 2.851  loss_ce_5: 0.2778  loss_mask_5: 0.3829  loss_dice_5: 2.852  loss_ce_6: 0.2756  loss_mask_6: 0.3833  loss_dice_6: 2.842  loss_ce_7: 0.2681  loss_mask_7: 0.3852  loss_dice_7: 2.844  loss_ce_8: 0.2627  loss_mask_8: 0.3853  loss_dice_8: 2.843  time: 1.4965  data_time: 0.0735  lr: 5.4507e-06  max_mem: 21589M
[01/18 03:47:26] d2.utils.events INFO:  eta: 8:21:17  iter: 19639  total_loss: 36.7  loss_ce: 0.3006  loss_mask: 0.3789  loss_dice: 2.92  loss_ce_0: 0.5964  loss_mask_0: 0.3739  loss_dice_0: 3.051  loss_ce_1: 0.343  loss_mask_1: 0.3815  loss_dice_1: 2.962  loss_ce_2: 0.3416  loss_mask_2: 0.3791  loss_dice_2: 2.935  loss_ce_3: 0.32  loss_mask_3: 0.3782  loss_dice_3: 2.93  loss_ce_4: 0.3159  loss_mask_4: 0.3772  loss_dice_4: 2.922  loss_ce_5: 0.3336  loss_mask_5: 0.3798  loss_dice_5: 2.92  loss_ce_6: 0.3204  loss_mask_6: 0.3793  loss_dice_6: 2.917  loss_ce_7: 0.3116  loss_mask_7: 0.3805  loss_dice_7: 2.922  loss_ce_8: 0.3172  loss_mask_8: 0.3789  loss_dice_8: 2.921  time: 1.4965  data_time: 0.0698  lr: 5.4458e-06  max_mem: 21589M
[01/18 03:47:56] d2.utils.events INFO:  eta: 8:20:37  iter: 19659  total_loss: 36.24  loss_ce: 0.2832  loss_mask: 0.3703  loss_dice: 2.898  loss_ce_0: 0.5853  loss_mask_0: 0.3674  loss_dice_0: 3.039  loss_ce_1: 0.3252  loss_mask_1: 0.3739  loss_dice_1: 2.941  loss_ce_2: 0.3456  loss_mask_2: 0.3715  loss_dice_2: 2.919  loss_ce_3: 0.307  loss_mask_3: 0.3709  loss_dice_3: 2.912  loss_ce_4: 0.315  loss_mask_4: 0.3698  loss_dice_4: 2.901  loss_ce_5: 0.295  loss_mask_5: 0.3687  loss_dice_5: 2.908  loss_ce_6: 0.2947  loss_mask_6: 0.3708  loss_dice_6: 2.903  loss_ce_7: 0.2871  loss_mask_7: 0.3701  loss_dice_7: 2.902  loss_ce_8: 0.2841  loss_mask_8: 0.3727  loss_dice_8: 2.896  time: 1.4965  data_time: 0.0694  lr: 5.441e-06  max_mem: 21589M
[01/18 03:48:25] d2.utils.events INFO:  eta: 8:19:46  iter: 19679  total_loss: 36.07  loss_ce: 0.305  loss_mask: 0.3825  loss_dice: 2.853  loss_ce_0: 0.5582  loss_mask_0: 0.3741  loss_dice_0: 2.997  loss_ce_1: 0.3497  loss_mask_1: 0.3842  loss_dice_1: 2.902  loss_ce_2: 0.3514  loss_mask_2: 0.3838  loss_dice_2: 2.88  loss_ce_3: 0.3322  loss_mask_3: 0.383  loss_dice_3: 2.871  loss_ce_4: 0.3144  loss_mask_4: 0.3812  loss_dice_4: 2.86  loss_ce_5: 0.3204  loss_mask_5: 0.3812  loss_dice_5: 2.859  loss_ce_6: 0.3171  loss_mask_6: 0.3823  loss_dice_6: 2.856  loss_ce_7: 0.3054  loss_mask_7: 0.381  loss_dice_7: 2.859  loss_ce_8: 0.3111  loss_mask_8: 0.3823  loss_dice_8: 2.848  time: 1.4965  data_time: 0.0771  lr: 5.4362e-06  max_mem: 21589M
[01/18 03:48:55] d2.utils.events INFO:  eta: 8:19:14  iter: 19699  total_loss: 36.47  loss_ce: 0.2976  loss_mask: 0.386  loss_dice: 2.917  loss_ce_0: 0.5783  loss_mask_0: 0.3829  loss_dice_0: 3.059  loss_ce_1: 0.3445  loss_mask_1: 0.3944  loss_dice_1: 2.955  loss_ce_2: 0.3344  loss_mask_2: 0.3907  loss_dice_2: 2.949  loss_ce_3: 0.321  loss_mask_3: 0.3862  loss_dice_3: 2.923  loss_ce_4: 0.2982  loss_mask_4: 0.3864  loss_dice_4: 2.922  loss_ce_5: 0.2939  loss_mask_5: 0.3847  loss_dice_5: 2.926  loss_ce_6: 0.2949  loss_mask_6: 0.3846  loss_dice_6: 2.922  loss_ce_7: 0.2908  loss_mask_7: 0.3829  loss_dice_7: 2.922  loss_ce_8: 0.2983  loss_mask_8: 0.3837  loss_dice_8: 2.921  time: 1.4964  data_time: 0.0727  lr: 5.4314e-06  max_mem: 21589M
[01/18 03:49:25] d2.utils.events INFO:  eta: 8:19:07  iter: 19719  total_loss: 36.44  loss_ce: 0.3022  loss_mask: 0.3792  loss_dice: 2.929  loss_ce_0: 0.5945  loss_mask_0: 0.3767  loss_dice_0: 3.057  loss_ce_1: 0.3268  loss_mask_1: 0.3811  loss_dice_1: 2.97  loss_ce_2: 0.339  loss_mask_2: 0.3776  loss_dice_2: 2.955  loss_ce_3: 0.3171  loss_mask_3: 0.3756  loss_dice_3: 2.945  loss_ce_4: 0.3069  loss_mask_4: 0.3785  loss_dice_4: 2.937  loss_ce_5: 0.3096  loss_mask_5: 0.3766  loss_dice_5: 2.935  loss_ce_6: 0.2957  loss_mask_6: 0.3779  loss_dice_6: 2.941  loss_ce_7: 0.3012  loss_mask_7: 0.378  loss_dice_7: 2.936  loss_ce_8: 0.2943  loss_mask_8: 0.3812  loss_dice_8: 2.932  time: 1.4964  data_time: 0.0802  lr: 5.4266e-06  max_mem: 21589M
[01/18 03:49:55] d2.utils.events INFO:  eta: 8:18:38  iter: 19739  total_loss: 37.53  loss_ce: 0.3031  loss_mask: 0.3819  loss_dice: 2.977  loss_ce_0: 0.5895  loss_mask_0: 0.375  loss_dice_0: 3.112  loss_ce_1: 0.3383  loss_mask_1: 0.3855  loss_dice_1: 3.01  loss_ce_2: 0.3407  loss_mask_2: 0.3843  loss_dice_2: 3.001  loss_ce_3: 0.3205  loss_mask_3: 0.3848  loss_dice_3: 2.982  loss_ce_4: 0.3124  loss_mask_4: 0.3849  loss_dice_4: 2.979  loss_ce_5: 0.3086  loss_mask_5: 0.3859  loss_dice_5: 2.987  loss_ce_6: 0.3095  loss_mask_6: 0.3853  loss_dice_6: 2.977  loss_ce_7: 0.3036  loss_mask_7: 0.384  loss_dice_7: 2.973  loss_ce_8: 0.2954  loss_mask_8: 0.3855  loss_dice_8: 2.978  time: 1.4964  data_time: 0.0812  lr: 5.4218e-06  max_mem: 21589M
[01/18 03:50:25] d2.utils.events INFO:  eta: 8:18:20  iter: 19759  total_loss: 36.35  loss_ce: 0.2685  loss_mask: 0.3812  loss_dice: 2.9  loss_ce_0: 0.5595  loss_mask_0: 0.37  loss_dice_0: 3.034  loss_ce_1: 0.3193  loss_mask_1: 0.3819  loss_dice_1: 2.94  loss_ce_2: 0.307  loss_mask_2: 0.3809  loss_dice_2: 2.918  loss_ce_3: 0.292  loss_mask_3: 0.38  loss_dice_3: 2.902  loss_ce_4: 0.2859  loss_mask_4: 0.3813  loss_dice_4: 2.902  loss_ce_5: 0.2742  loss_mask_5: 0.3801  loss_dice_5: 2.903  loss_ce_6: 0.2887  loss_mask_6: 0.38  loss_dice_6: 2.905  loss_ce_7: 0.2851  loss_mask_7: 0.3794  loss_dice_7: 2.903  loss_ce_8: 0.2901  loss_mask_8: 0.3819  loss_dice_8: 2.899  time: 1.4964  data_time: 0.0776  lr: 5.4169e-06  max_mem: 21589M
[01/18 03:50:54] d2.utils.events INFO:  eta: 8:17:50  iter: 19779  total_loss: 36.01  loss_ce: 0.2813  loss_mask: 0.3771  loss_dice: 2.874  loss_ce_0: 0.5678  loss_mask_0: 0.3684  loss_dice_0: 2.999  loss_ce_1: 0.32  loss_mask_1: 0.3761  loss_dice_1: 2.914  loss_ce_2: 0.3203  loss_mask_2: 0.3753  loss_dice_2: 2.891  loss_ce_3: 0.302  loss_mask_3: 0.3761  loss_dice_3: 2.884  loss_ce_4: 0.2881  loss_mask_4: 0.3764  loss_dice_4: 2.875  loss_ce_5: 0.2819  loss_mask_5: 0.3769  loss_dice_5: 2.876  loss_ce_6: 0.2799  loss_mask_6: 0.3764  loss_dice_6: 2.874  loss_ce_7: 0.2832  loss_mask_7: 0.3759  loss_dice_7: 2.869  loss_ce_8: 0.2846  loss_mask_8: 0.3769  loss_dice_8: 2.873  time: 1.4964  data_time: 0.0751  lr: 5.4121e-06  max_mem: 21589M
[01/18 03:51:24] d2.utils.events INFO:  eta: 8:17:19  iter: 19799  total_loss: 36.63  loss_ce: 0.2786  loss_mask: 0.3866  loss_dice: 2.903  loss_ce_0: 0.5714  loss_mask_0: 0.3779  loss_dice_0: 3.042  loss_ce_1: 0.3172  loss_mask_1: 0.3922  loss_dice_1: 2.952  loss_ce_2: 0.3033  loss_mask_2: 0.3869  loss_dice_2: 2.914  loss_ce_3: 0.2989  loss_mask_3: 0.3821  loss_dice_3: 2.909  loss_ce_4: 0.2823  loss_mask_4: 0.3841  loss_dice_4: 2.913  loss_ce_5: 0.289  loss_mask_5: 0.3842  loss_dice_5: 2.905  loss_ce_6: 0.2788  loss_mask_6: 0.3837  loss_dice_6: 2.914  loss_ce_7: 0.2876  loss_mask_7: 0.3862  loss_dice_7: 2.906  loss_ce_8: 0.2832  loss_mask_8: 0.3859  loss_dice_8: 2.91  time: 1.4964  data_time: 0.0782  lr: 5.4073e-06  max_mem: 21589M
[01/18 03:51:54] d2.utils.events INFO:  eta: 8:16:51  iter: 19819  total_loss: 36.49  loss_ce: 0.2716  loss_mask: 0.3868  loss_dice: 2.937  loss_ce_0: 0.5461  loss_mask_0: 0.3775  loss_dice_0: 3.065  loss_ce_1: 0.3242  loss_mask_1: 0.391  loss_dice_1: 2.974  loss_ce_2: 0.3227  loss_mask_2: 0.3877  loss_dice_2: 2.949  loss_ce_3: 0.2885  loss_mask_3: 0.3882  loss_dice_3: 2.938  loss_ce_4: 0.2943  loss_mask_4: 0.3868  loss_dice_4: 2.951  loss_ce_5: 0.2909  loss_mask_5: 0.3874  loss_dice_5: 2.938  loss_ce_6: 0.2782  loss_mask_6: 0.3882  loss_dice_6: 2.931  loss_ce_7: 0.2703  loss_mask_7: 0.3853  loss_dice_7: 2.943  loss_ce_8: 0.2702  loss_mask_8: 0.3867  loss_dice_8: 2.927  time: 1.4964  data_time: 0.0749  lr: 5.4025e-06  max_mem: 21589M
[01/18 03:52:24] d2.utils.events INFO:  eta: 8:16:29  iter: 19839  total_loss: 36.95  loss_ce: 0.2951  loss_mask: 0.3738  loss_dice: 2.952  loss_ce_0: 0.5652  loss_mask_0: 0.3686  loss_dice_0: 3.069  loss_ce_1: 0.3339  loss_mask_1: 0.3797  loss_dice_1: 2.998  loss_ce_2: 0.3245  loss_mask_2: 0.377  loss_dice_2: 2.98  loss_ce_3: 0.3055  loss_mask_3: 0.3743  loss_dice_3: 2.963  loss_ce_4: 0.3068  loss_mask_4: 0.3726  loss_dice_4: 2.971  loss_ce_5: 0.2952  loss_mask_5: 0.3737  loss_dice_5: 2.965  loss_ce_6: 0.2828  loss_mask_6: 0.3725  loss_dice_6: 2.969  loss_ce_7: 0.2887  loss_mask_7: 0.3735  loss_dice_7: 2.957  loss_ce_8: 0.2828  loss_mask_8: 0.374  loss_dice_8: 2.954  time: 1.4964  data_time: 0.0732  lr: 5.3977e-06  max_mem: 21589M
[01/18 03:52:54] d2.utils.events INFO:  eta: 8:15:53  iter: 19859  total_loss: 37.04  loss_ce: 0.2934  loss_mask: 0.3898  loss_dice: 2.935  loss_ce_0: 0.5713  loss_mask_0: 0.3828  loss_dice_0: 3.077  loss_ce_1: 0.3264  loss_mask_1: 0.3948  loss_dice_1: 2.986  loss_ce_2: 0.3257  loss_mask_2: 0.3931  loss_dice_2: 2.952  loss_ce_3: 0.3264  loss_mask_3: 0.3908  loss_dice_3: 2.949  loss_ce_4: 0.3055  loss_mask_4: 0.3903  loss_dice_4: 2.938  loss_ce_5: 0.2987  loss_mask_5: 0.3888  loss_dice_5: 2.949  loss_ce_6: 0.2987  loss_mask_6: 0.3877  loss_dice_6: 2.939  loss_ce_7: 0.2958  loss_mask_7: 0.3882  loss_dice_7: 2.939  loss_ce_8: 0.2958  loss_mask_8: 0.3883  loss_dice_8: 2.938  time: 1.4964  data_time: 0.0773  lr: 5.3929e-06  max_mem: 21589M
[01/18 03:53:23] d2.utils.events INFO:  eta: 8:15:35  iter: 19879  total_loss: 36.6  loss_ce: 0.2902  loss_mask: 0.3843  loss_dice: 2.923  loss_ce_0: 0.5702  loss_mask_0: 0.3798  loss_dice_0: 3.087  loss_ce_1: 0.3277  loss_mask_1: 0.3924  loss_dice_1: 2.98  loss_ce_2: 0.307  loss_mask_2: 0.3884  loss_dice_2: 2.958  loss_ce_3: 0.3003  loss_mask_3: 0.3888  loss_dice_3: 2.932  loss_ce_4: 0.3023  loss_mask_4: 0.3868  loss_dice_4: 2.932  loss_ce_5: 0.2921  loss_mask_5: 0.3857  loss_dice_5: 2.932  loss_ce_6: 0.2897  loss_mask_6: 0.3858  loss_dice_6: 2.929  loss_ce_7: 0.2778  loss_mask_7: 0.3847  loss_dice_7: 2.934  loss_ce_8: 0.2929  loss_mask_8: 0.3843  loss_dice_8: 2.932  time: 1.4964  data_time: 0.0706  lr: 5.388e-06  max_mem: 21589M
[01/18 03:53:53] d2.utils.events INFO:  eta: 8:15:10  iter: 19899  total_loss: 36.99  loss_ce: 0.3101  loss_mask: 0.3874  loss_dice: 2.921  loss_ce_0: 0.5942  loss_mask_0: 0.377  loss_dice_0: 3.058  loss_ce_1: 0.3445  loss_mask_1: 0.3884  loss_dice_1: 2.974  loss_ce_2: 0.3509  loss_mask_2: 0.3859  loss_dice_2: 2.953  loss_ce_3: 0.328  loss_mask_3: 0.3816  loss_dice_3: 2.932  loss_ce_4: 0.3218  loss_mask_4: 0.3833  loss_dice_4: 2.935  loss_ce_5: 0.3091  loss_mask_5: 0.3852  loss_dice_5: 2.93  loss_ce_6: 0.3125  loss_mask_6: 0.3854  loss_dice_6: 2.931  loss_ce_7: 0.3252  loss_mask_7: 0.3853  loss_dice_7: 2.923  loss_ce_8: 0.3187  loss_mask_8: 0.3869  loss_dice_8: 2.922  time: 1.4964  data_time: 0.0739  lr: 5.3832e-06  max_mem: 21589M
[01/18 03:54:22] d2.utils.events INFO:  eta: 8:14:32  iter: 19919  total_loss: 35.68  loss_ce: 0.2946  loss_mask: 0.3774  loss_dice: 2.853  loss_ce_0: 0.5629  loss_mask_0: 0.3624  loss_dice_0: 2.998  loss_ce_1: 0.3622  loss_mask_1: 0.3818  loss_dice_1: 2.896  loss_ce_2: 0.3351  loss_mask_2: 0.3798  loss_dice_2: 2.879  loss_ce_3: 0.3296  loss_mask_3: 0.378  loss_dice_3: 2.868  loss_ce_4: 0.3049  loss_mask_4: 0.3795  loss_dice_4: 2.865  loss_ce_5: 0.3126  loss_mask_5: 0.3791  loss_dice_5: 2.867  loss_ce_6: 0.3004  loss_mask_6: 0.379  loss_dice_6: 2.863  loss_ce_7: 0.3074  loss_mask_7: 0.3786  loss_dice_7: 2.861  loss_ce_8: 0.3025  loss_mask_8: 0.3775  loss_dice_8: 2.861  time: 1.4963  data_time: 0.0789  lr: 5.3784e-06  max_mem: 21589M
[01/18 03:54:52] d2.utils.events INFO:  eta: 8:14:10  iter: 19939  total_loss: 36.27  loss_ce: 0.2737  loss_mask: 0.3909  loss_dice: 2.899  loss_ce_0: 0.5567  loss_mask_0: 0.3812  loss_dice_0: 3.037  loss_ce_1: 0.3287  loss_mask_1: 0.3945  loss_dice_1: 2.931  loss_ce_2: 0.2979  loss_mask_2: 0.3941  loss_dice_2: 2.916  loss_ce_3: 0.2844  loss_mask_3: 0.3893  loss_dice_3: 2.901  loss_ce_4: 0.2878  loss_mask_4: 0.3919  loss_dice_4: 2.897  loss_ce_5: 0.2821  loss_mask_5: 0.3901  loss_dice_5: 2.899  loss_ce_6: 0.2757  loss_mask_6: 0.3905  loss_dice_6: 2.898  loss_ce_7: 0.2876  loss_mask_7: 0.39  loss_dice_7: 2.901  loss_ce_8: 0.2834  loss_mask_8: 0.3913  loss_dice_8: 2.899  time: 1.4963  data_time: 0.0666  lr: 5.3736e-06  max_mem: 21589M
[01/18 03:55:22] d2.utils.events INFO:  eta: 8:13:54  iter: 19959  total_loss: 36.64  loss_ce: 0.2843  loss_mask: 0.3956  loss_dice: 2.922  loss_ce_0: 0.5766  loss_mask_0: 0.3919  loss_dice_0: 3.064  loss_ce_1: 0.325  loss_mask_1: 0.3968  loss_dice_1: 2.969  loss_ce_2: 0.3181  loss_mask_2: 0.3976  loss_dice_2: 2.958  loss_ce_3: 0.3185  loss_mask_3: 0.3946  loss_dice_3: 2.939  loss_ce_4: 0.3042  loss_mask_4: 0.3962  loss_dice_4: 2.94  loss_ce_5: 0.2991  loss_mask_5: 0.3959  loss_dice_5: 2.931  loss_ce_6: 0.2993  loss_mask_6: 0.3944  loss_dice_6: 2.933  loss_ce_7: 0.2915  loss_mask_7: 0.3949  loss_dice_7: 2.932  loss_ce_8: 0.2977  loss_mask_8: 0.3941  loss_dice_8: 2.935  time: 1.4963  data_time: 0.0762  lr: 5.3688e-06  max_mem: 21589M
[01/18 03:55:51] d2.utils.events INFO:  eta: 8:13:28  iter: 19979  total_loss: 36.35  loss_ce: 0.2713  loss_mask: 0.3814  loss_dice: 2.896  loss_ce_0: 0.5441  loss_mask_0: 0.3778  loss_dice_0: 3.039  loss_ce_1: 0.3276  loss_mask_1: 0.3862  loss_dice_1: 2.937  loss_ce_2: 0.3157  loss_mask_2: 0.3853  loss_dice_2: 2.909  loss_ce_3: 0.2979  loss_mask_3: 0.3827  loss_dice_3: 2.902  loss_ce_4: 0.2898  loss_mask_4: 0.3821  loss_dice_4: 2.895  loss_ce_5: 0.2713  loss_mask_5: 0.3813  loss_dice_5: 2.897  loss_ce_6: 0.2759  loss_mask_6: 0.381  loss_dice_6: 2.9  loss_ce_7: 0.2791  loss_mask_7: 0.3816  loss_dice_7: 2.894  loss_ce_8: 0.2825  loss_mask_8: 0.3802  loss_dice_8: 2.903  time: 1.4963  data_time: 0.0690  lr: 5.3639e-06  max_mem: 21589M
[01/18 03:56:21] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0019999.pth
[01/18 03:56:22] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 03:56:23] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 03:56:23] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 03:56:24] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 03:56:39] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0100 s/iter. Inference: 0.1809 s/iter. Eval: 0.2467 s/iter. Total: 0.4376 s/iter. ETA=0:07:53
[01/18 03:56:44] d2.evaluation.evaluator INFO: Inference done 22/1093. Dataloading: 0.0126 s/iter. Inference: 0.1803 s/iter. Eval: 0.2567 s/iter. Total: 0.4496 s/iter. ETA=0:08:01
[01/18 03:56:49] d2.evaluation.evaluator INFO: Inference done 34/1093. Dataloading: 0.0128 s/iter. Inference: 0.1826 s/iter. Eval: 0.2457 s/iter. Total: 0.4412 s/iter. ETA=0:07:47
[01/18 03:56:54] d2.evaluation.evaluator INFO: Inference done 46/1093. Dataloading: 0.0134 s/iter. Inference: 0.1784 s/iter. Eval: 0.2441 s/iter. Total: 0.4361 s/iter. ETA=0:07:36
[01/18 03:56:59] d2.evaluation.evaluator INFO: Inference done 58/1093. Dataloading: 0.0139 s/iter. Inference: 0.1740 s/iter. Eval: 0.2525 s/iter. Total: 0.4404 s/iter. ETA=0:07:35
[01/18 03:57:05] d2.evaluation.evaluator INFO: Inference done 70/1093. Dataloading: 0.0141 s/iter. Inference: 0.1741 s/iter. Eval: 0.2529 s/iter. Total: 0.4413 s/iter. ETA=0:07:31
[01/18 03:57:10] d2.evaluation.evaluator INFO: Inference done 82/1093. Dataloading: 0.0141 s/iter. Inference: 0.1726 s/iter. Eval: 0.2511 s/iter. Total: 0.4380 s/iter. ETA=0:07:22
[01/18 03:57:15] d2.evaluation.evaluator INFO: Inference done 93/1093. Dataloading: 0.0142 s/iter. Inference: 0.1733 s/iter. Eval: 0.2540 s/iter. Total: 0.4416 s/iter. ETA=0:07:21
[01/18 03:57:20] d2.evaluation.evaluator INFO: Inference done 106/1093. Dataloading: 0.0141 s/iter. Inference: 0.1743 s/iter. Eval: 0.2505 s/iter. Total: 0.4390 s/iter. ETA=0:07:13
[01/18 03:57:26] d2.evaluation.evaluator INFO: Inference done 118/1093. Dataloading: 0.0140 s/iter. Inference: 0.1744 s/iter. Eval: 0.2511 s/iter. Total: 0.4397 s/iter. ETA=0:07:08
[01/18 03:57:31] d2.evaluation.evaluator INFO: Inference done 130/1093. Dataloading: 0.0139 s/iter. Inference: 0.1755 s/iter. Eval: 0.2501 s/iter. Total: 0.4396 s/iter. ETA=0:07:03
[01/18 03:57:36] d2.evaluation.evaluator INFO: Inference done 144/1093. Dataloading: 0.0137 s/iter. Inference: 0.1738 s/iter. Eval: 0.2443 s/iter. Total: 0.4319 s/iter. ETA=0:06:49
[01/18 03:57:41] d2.evaluation.evaluator INFO: Inference done 158/1093. Dataloading: 0.0136 s/iter. Inference: 0.1740 s/iter. Eval: 0.2386 s/iter. Total: 0.4264 s/iter. ETA=0:06:38
[01/18 03:57:47] d2.evaluation.evaluator INFO: Inference done 170/1093. Dataloading: 0.0137 s/iter. Inference: 0.1737 s/iter. Eval: 0.2401 s/iter. Total: 0.4276 s/iter. ETA=0:06:34
[01/18 03:57:52] d2.evaluation.evaluator INFO: Inference done 184/1093. Dataloading: 0.0136 s/iter. Inference: 0.1728 s/iter. Eval: 0.2363 s/iter. Total: 0.4228 s/iter. ETA=0:06:24
[01/18 03:57:57] d2.evaluation.evaluator INFO: Inference done 198/1093. Dataloading: 0.0135 s/iter. Inference: 0.1718 s/iter. Eval: 0.2337 s/iter. Total: 0.4191 s/iter. ETA=0:06:15
[01/18 03:58:02] d2.evaluation.evaluator INFO: Inference done 210/1093. Dataloading: 0.0135 s/iter. Inference: 0.1712 s/iter. Eval: 0.2356 s/iter. Total: 0.4205 s/iter. ETA=0:06:11
[01/18 03:58:07] d2.evaluation.evaluator INFO: Inference done 224/1093. Dataloading: 0.0134 s/iter. Inference: 0.1703 s/iter. Eval: 0.2332 s/iter. Total: 0.4170 s/iter. ETA=0:06:02
[01/18 03:58:13] d2.evaluation.evaluator INFO: Inference done 236/1093. Dataloading: 0.0134 s/iter. Inference: 0.1712 s/iter. Eval: 0.2333 s/iter. Total: 0.4180 s/iter. ETA=0:05:58
[01/18 03:58:18] d2.evaluation.evaluator INFO: Inference done 249/1093. Dataloading: 0.0134 s/iter. Inference: 0.1706 s/iter. Eval: 0.2327 s/iter. Total: 0.4167 s/iter. ETA=0:05:51
[01/18 03:58:23] d2.evaluation.evaluator INFO: Inference done 262/1093. Dataloading: 0.0133 s/iter. Inference: 0.1699 s/iter. Eval: 0.2321 s/iter. Total: 0.4154 s/iter. ETA=0:05:45
[01/18 03:58:28] d2.evaluation.evaluator INFO: Inference done 275/1093. Dataloading: 0.0133 s/iter. Inference: 0.1700 s/iter. Eval: 0.2316 s/iter. Total: 0.4150 s/iter. ETA=0:05:39
[01/18 03:58:33] d2.evaluation.evaluator INFO: Inference done 290/1093. Dataloading: 0.0132 s/iter. Inference: 0.1694 s/iter. Eval: 0.2296 s/iter. Total: 0.4123 s/iter. ETA=0:05:31
[01/18 03:58:38] d2.evaluation.evaluator INFO: Inference done 302/1093. Dataloading: 0.0132 s/iter. Inference: 0.1692 s/iter. Eval: 0.2300 s/iter. Total: 0.4125 s/iter. ETA=0:05:26
[01/18 03:58:44] d2.evaluation.evaluator INFO: Inference done 313/1093. Dataloading: 0.0133 s/iter. Inference: 0.1689 s/iter. Eval: 0.2330 s/iter. Total: 0.4153 s/iter. ETA=0:05:23
[01/18 03:58:49] d2.evaluation.evaluator INFO: Inference done 324/1093. Dataloading: 0.0133 s/iter. Inference: 0.1692 s/iter. Eval: 0.2343 s/iter. Total: 0.4170 s/iter. ETA=0:05:20
[01/18 03:58:54] d2.evaluation.evaluator INFO: Inference done 339/1093. Dataloading: 0.0132 s/iter. Inference: 0.1690 s/iter. Eval: 0.2320 s/iter. Total: 0.4143 s/iter. ETA=0:05:12
[01/18 03:59:00] d2.evaluation.evaluator INFO: Inference done 353/1093. Dataloading: 0.0131 s/iter. Inference: 0.1690 s/iter. Eval: 0.2307 s/iter. Total: 0.4128 s/iter. ETA=0:05:05
[01/18 03:59:05] d2.evaluation.evaluator INFO: Inference done 365/1093. Dataloading: 0.0131 s/iter. Inference: 0.1689 s/iter. Eval: 0.2317 s/iter. Total: 0.4138 s/iter. ETA=0:05:01
[01/18 03:59:10] d2.evaluation.evaluator INFO: Inference done 379/1093. Dataloading: 0.0131 s/iter. Inference: 0.1684 s/iter. Eval: 0.2304 s/iter. Total: 0.4119 s/iter. ETA=0:04:54
[01/18 03:59:15] d2.evaluation.evaluator INFO: Inference done 391/1093. Dataloading: 0.0131 s/iter. Inference: 0.1682 s/iter. Eval: 0.2316 s/iter. Total: 0.4130 s/iter. ETA=0:04:49
[01/18 03:59:21] d2.evaluation.evaluator INFO: Inference done 405/1093. Dataloading: 0.0131 s/iter. Inference: 0.1679 s/iter. Eval: 0.2309 s/iter. Total: 0.4119 s/iter. ETA=0:04:43
[01/18 03:59:26] d2.evaluation.evaluator INFO: Inference done 417/1093. Dataloading: 0.0130 s/iter. Inference: 0.1682 s/iter. Eval: 0.2318 s/iter. Total: 0.4131 s/iter. ETA=0:04:39
[01/18 03:59:31] d2.evaluation.evaluator INFO: Inference done 430/1093. Dataloading: 0.0131 s/iter. Inference: 0.1679 s/iter. Eval: 0.2319 s/iter. Total: 0.4130 s/iter. ETA=0:04:33
[01/18 03:59:37] d2.evaluation.evaluator INFO: Inference done 444/1093. Dataloading: 0.0130 s/iter. Inference: 0.1677 s/iter. Eval: 0.2307 s/iter. Total: 0.4116 s/iter. ETA=0:04:27
[01/18 03:59:42] d2.evaluation.evaluator INFO: Inference done 458/1093. Dataloading: 0.0130 s/iter. Inference: 0.1673 s/iter. Eval: 0.2301 s/iter. Total: 0.4105 s/iter. ETA=0:04:20
[01/18 03:59:47] d2.evaluation.evaluator INFO: Inference done 474/1093. Dataloading: 0.0128 s/iter. Inference: 0.1670 s/iter. Eval: 0.2278 s/iter. Total: 0.4077 s/iter. ETA=0:04:12
[01/18 03:59:52] d2.evaluation.evaluator INFO: Inference done 488/1093. Dataloading: 0.0128 s/iter. Inference: 0.1667 s/iter. Eval: 0.2271 s/iter. Total: 0.4067 s/iter. ETA=0:04:06
[01/18 03:59:57] d2.evaluation.evaluator INFO: Inference done 503/1093. Dataloading: 0.0127 s/iter. Inference: 0.1662 s/iter. Eval: 0.2255 s/iter. Total: 0.4046 s/iter. ETA=0:03:58
[01/18 04:00:03] d2.evaluation.evaluator INFO: Inference done 518/1093. Dataloading: 0.0126 s/iter. Inference: 0.1661 s/iter. Eval: 0.2244 s/iter. Total: 0.4032 s/iter. ETA=0:03:51
[01/18 04:00:08] d2.evaluation.evaluator INFO: Inference done 530/1093. Dataloading: 0.0127 s/iter. Inference: 0.1660 s/iter. Eval: 0.2251 s/iter. Total: 0.4039 s/iter. ETA=0:03:47
[01/18 04:00:13] d2.evaluation.evaluator INFO: Inference done 544/1093. Dataloading: 0.0126 s/iter. Inference: 0.1658 s/iter. Eval: 0.2244 s/iter. Total: 0.4030 s/iter. ETA=0:03:41
[01/18 04:00:19] d2.evaluation.evaluator INFO: Inference done 557/1093. Dataloading: 0.0126 s/iter. Inference: 0.1657 s/iter. Eval: 0.2246 s/iter. Total: 0.4031 s/iter. ETA=0:03:36
[01/18 04:00:24] d2.evaluation.evaluator INFO: Inference done 570/1093. Dataloading: 0.0126 s/iter. Inference: 0.1657 s/iter. Eval: 0.2247 s/iter. Total: 0.4031 s/iter. ETA=0:03:30
[01/18 04:00:29] d2.evaluation.evaluator INFO: Inference done 586/1093. Dataloading: 0.0127 s/iter. Inference: 0.1656 s/iter. Eval: 0.2224 s/iter. Total: 0.4008 s/iter. ETA=0:03:23
[01/18 04:00:34] d2.evaluation.evaluator INFO: Inference done 599/1093. Dataloading: 0.0127 s/iter. Inference: 0.1655 s/iter. Eval: 0.2227 s/iter. Total: 0.4011 s/iter. ETA=0:03:18
[01/18 04:00:39] d2.evaluation.evaluator INFO: Inference done 612/1093. Dataloading: 0.0127 s/iter. Inference: 0.1652 s/iter. Eval: 0.2229 s/iter. Total: 0.4009 s/iter. ETA=0:03:12
[01/18 04:00:45] d2.evaluation.evaluator INFO: Inference done 625/1093. Dataloading: 0.0127 s/iter. Inference: 0.1651 s/iter. Eval: 0.2232 s/iter. Total: 0.4010 s/iter. ETA=0:03:07
[01/18 04:00:50] d2.evaluation.evaluator INFO: Inference done 638/1093. Dataloading: 0.0127 s/iter. Inference: 0.1652 s/iter. Eval: 0.2229 s/iter. Total: 0.4009 s/iter. ETA=0:03:02
[01/18 04:00:55] d2.evaluation.evaluator INFO: Inference done 652/1093. Dataloading: 0.0126 s/iter. Inference: 0.1652 s/iter. Eval: 0.2224 s/iter. Total: 0.4004 s/iter. ETA=0:02:56
[01/18 04:01:00] d2.evaluation.evaluator INFO: Inference done 665/1093. Dataloading: 0.0126 s/iter. Inference: 0.1652 s/iter. Eval: 0.2222 s/iter. Total: 0.4001 s/iter. ETA=0:02:51
[01/18 04:01:05] d2.evaluation.evaluator INFO: Inference done 679/1093. Dataloading: 0.0126 s/iter. Inference: 0.1652 s/iter. Eval: 0.2216 s/iter. Total: 0.3994 s/iter. ETA=0:02:45
[01/18 04:01:11] d2.evaluation.evaluator INFO: Inference done 695/1093. Dataloading: 0.0125 s/iter. Inference: 0.1649 s/iter. Eval: 0.2206 s/iter. Total: 0.3981 s/iter. ETA=0:02:38
[01/18 04:01:16] d2.evaluation.evaluator INFO: Inference done 709/1093. Dataloading: 0.0125 s/iter. Inference: 0.1646 s/iter. Eval: 0.2208 s/iter. Total: 0.3980 s/iter. ETA=0:02:32
[01/18 04:01:21] d2.evaluation.evaluator INFO: Inference done 722/1093. Dataloading: 0.0125 s/iter. Inference: 0.1644 s/iter. Eval: 0.2210 s/iter. Total: 0.3981 s/iter. ETA=0:02:27
[01/18 04:01:27] d2.evaluation.evaluator INFO: Inference done 738/1093. Dataloading: 0.0124 s/iter. Inference: 0.1641 s/iter. Eval: 0.2199 s/iter. Total: 0.3966 s/iter. ETA=0:02:20
[01/18 04:01:32] d2.evaluation.evaluator INFO: Inference done 753/1093. Dataloading: 0.0124 s/iter. Inference: 0.1639 s/iter. Eval: 0.2193 s/iter. Total: 0.3957 s/iter. ETA=0:02:14
[01/18 04:01:37] d2.evaluation.evaluator INFO: Inference done 766/1093. Dataloading: 0.0124 s/iter. Inference: 0.1638 s/iter. Eval: 0.2194 s/iter. Total: 0.3958 s/iter. ETA=0:02:09
[01/18 04:01:43] d2.evaluation.evaluator INFO: Inference done 778/1093. Dataloading: 0.0124 s/iter. Inference: 0.1645 s/iter. Eval: 0.2195 s/iter. Total: 0.3966 s/iter. ETA=0:02:04
[01/18 04:01:48] d2.evaluation.evaluator INFO: Inference done 791/1093. Dataloading: 0.0124 s/iter. Inference: 0.1652 s/iter. Eval: 0.2190 s/iter. Total: 0.3967 s/iter. ETA=0:01:59
[01/18 04:01:53] d2.evaluation.evaluator INFO: Inference done 804/1093. Dataloading: 0.0124 s/iter. Inference: 0.1655 s/iter. Eval: 0.2186 s/iter. Total: 0.3966 s/iter. ETA=0:01:54
[01/18 04:01:58] d2.evaluation.evaluator INFO: Inference done 821/1093. Dataloading: 0.0123 s/iter. Inference: 0.1652 s/iter. Eval: 0.2171 s/iter. Total: 0.3947 s/iter. ETA=0:01:47
[01/18 04:02:03] d2.evaluation.evaluator INFO: Inference done 837/1093. Dataloading: 0.0123 s/iter. Inference: 0.1648 s/iter. Eval: 0.2161 s/iter. Total: 0.3933 s/iter. ETA=0:01:40
[01/18 04:02:09] d2.evaluation.evaluator INFO: Inference done 851/1093. Dataloading: 0.0122 s/iter. Inference: 0.1649 s/iter. Eval: 0.2160 s/iter. Total: 0.3932 s/iter. ETA=0:01:35
[01/18 04:02:14] d2.evaluation.evaluator INFO: Inference done 864/1093. Dataloading: 0.0122 s/iter. Inference: 0.1648 s/iter. Eval: 0.2159 s/iter. Total: 0.3931 s/iter. ETA=0:01:30
[01/18 04:02:19] d2.evaluation.evaluator INFO: Inference done 876/1093. Dataloading: 0.0122 s/iter. Inference: 0.1652 s/iter. Eval: 0.2159 s/iter. Total: 0.3935 s/iter. ETA=0:01:25
[01/18 04:02:24] d2.evaluation.evaluator INFO: Inference done 889/1093. Dataloading: 0.0122 s/iter. Inference: 0.1650 s/iter. Eval: 0.2160 s/iter. Total: 0.3934 s/iter. ETA=0:01:20
[01/18 04:02:29] d2.evaluation.evaluator INFO: Inference done 904/1093. Dataloading: 0.0122 s/iter. Inference: 0.1650 s/iter. Eval: 0.2156 s/iter. Total: 0.3928 s/iter. ETA=0:01:14
[01/18 04:02:34] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0122 s/iter. Inference: 0.1649 s/iter. Eval: 0.2160 s/iter. Total: 0.3931 s/iter. ETA=0:01:09
[01/18 04:02:40] d2.evaluation.evaluator INFO: Inference done 931/1093. Dataloading: 0.0122 s/iter. Inference: 0.1649 s/iter. Eval: 0.2155 s/iter. Total: 0.3927 s/iter. ETA=0:01:03
[01/18 04:02:45] d2.evaluation.evaluator INFO: Inference done 944/1093. Dataloading: 0.0122 s/iter. Inference: 0.1650 s/iter. Eval: 0.2157 s/iter. Total: 0.3930 s/iter. ETA=0:00:58
[01/18 04:02:50] d2.evaluation.evaluator INFO: Inference done 957/1093. Dataloading: 0.0122 s/iter. Inference: 0.1650 s/iter. Eval: 0.2159 s/iter. Total: 0.3932 s/iter. ETA=0:00:53
[01/18 04:02:55] d2.evaluation.evaluator INFO: Inference done 969/1093. Dataloading: 0.0122 s/iter. Inference: 0.1651 s/iter. Eval: 0.2161 s/iter. Total: 0.3935 s/iter. ETA=0:00:48
[01/18 04:03:00] d2.evaluation.evaluator INFO: Inference done 986/1093. Dataloading: 0.0121 s/iter. Inference: 0.1648 s/iter. Eval: 0.2149 s/iter. Total: 0.3919 s/iter. ETA=0:00:41
[01/18 04:03:06] d2.evaluation.evaluator INFO: Inference done 999/1093. Dataloading: 0.0121 s/iter. Inference: 0.1649 s/iter. Eval: 0.2149 s/iter. Total: 0.3920 s/iter. ETA=0:00:36
[01/18 04:03:11] d2.evaluation.evaluator INFO: Inference done 1014/1093. Dataloading: 0.0121 s/iter. Inference: 0.1648 s/iter. Eval: 0.2144 s/iter. Total: 0.3914 s/iter. ETA=0:00:30
[01/18 04:03:16] d2.evaluation.evaluator INFO: Inference done 1027/1093. Dataloading: 0.0121 s/iter. Inference: 0.1646 s/iter. Eval: 0.2146 s/iter. Total: 0.3914 s/iter. ETA=0:00:25
[01/18 04:03:21] d2.evaluation.evaluator INFO: Inference done 1041/1093. Dataloading: 0.0121 s/iter. Inference: 0.1646 s/iter. Eval: 0.2144 s/iter. Total: 0.3912 s/iter. ETA=0:00:20
[01/18 04:03:26] d2.evaluation.evaluator INFO: Inference done 1055/1093. Dataloading: 0.0121 s/iter. Inference: 0.1645 s/iter. Eval: 0.2141 s/iter. Total: 0.3908 s/iter. ETA=0:00:14
[01/18 04:03:31] d2.evaluation.evaluator INFO: Inference done 1071/1093. Dataloading: 0.0120 s/iter. Inference: 0.1642 s/iter. Eval: 0.2132 s/iter. Total: 0.3896 s/iter. ETA=0:00:08
[01/18 04:03:36] d2.evaluation.evaluator INFO: Inference done 1086/1093. Dataloading: 0.0120 s/iter. Inference: 0.1642 s/iter. Eval: 0.2126 s/iter. Total: 0.3889 s/iter. ETA=0:00:02
[01/18 04:03:39] d2.evaluation.evaluator INFO: Total inference time: 0:07:03.038959 (0.388823 s / iter per device, on 4 devices)
[01/18 04:03:39] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:58 (0.164031 s / iter per device, on 4 devices)
[01/18 04:04:03] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.5437446927639287, 'mIoU': 18.076352724944392, 'fwIoU': 39.4002816593743, 'IoU-0': nan, 'IoU-1': 95.34207005319917, 'IoU-2': 42.65513283326061, 'IoU-3': 55.80394191825791, 'IoU-4': 49.24355879144565, 'IoU-5': 43.51806808943426, 'IoU-6': 40.15065320479507, 'IoU-7': 32.31043010805767, 'IoU-8': 17.621302507238028, 'IoU-9': 27.788455380595234, 'IoU-10': 34.76414649930102, 'IoU-11': 42.26763394487737, 'IoU-12': 45.560543998456, 'IoU-13': 45.57990361488276, 'IoU-14': 45.12672152585804, 'IoU-15': 46.40734142964597, 'IoU-16': 47.59161419905505, 'IoU-17': 42.327985171106945, 'IoU-18': 44.089638824626725, 'IoU-19': 42.858499907866396, 'IoU-20': 43.87516390595579, 'IoU-21': 40.494206123444535, 'IoU-22': 43.85198383377023, 'IoU-23': 41.37210559888394, 'IoU-24': 41.794812858348465, 'IoU-25': 40.67680191418723, 'IoU-26': 38.76379005753668, 'IoU-27': 41.54436541326142, 'IoU-28': 39.64425677421793, 'IoU-29': 40.95545652476523, 'IoU-30': 37.821254770849, 'IoU-31': 39.83752913918464, 'IoU-32': 40.40594833471958, 'IoU-33': 38.66256646275907, 'IoU-34': 38.810577484041914, 'IoU-35': 39.721376144988575, 'IoU-36': 39.692990534366594, 'IoU-37': 37.826909216487806, 'IoU-38': 37.91154556601916, 'IoU-39': 37.03345390219875, 'IoU-40': 37.46068538990184, 'IoU-41': 35.72718089621262, 'IoU-42': 34.22667554948733, 'IoU-43': 34.39468414767574, 'IoU-44': 34.802500860090596, 'IoU-45': 32.65717257066058, 'IoU-46': 31.73503202673262, 'IoU-47': 29.731951189089635, 'IoU-48': 29.794733932027977, 'IoU-49': 29.082278499799045, 'IoU-50': 29.986014682469676, 'IoU-51': 28.187056047500704, 'IoU-52': 28.96699333521746, 'IoU-53': 28.701366743710167, 'IoU-54': 29.22076047477053, 'IoU-55': 28.591188651901422, 'IoU-56': 27.319489283340747, 'IoU-57': 26.897434596928793, 'IoU-58': 25.76613440805506, 'IoU-59': 25.434907162967168, 'IoU-60': 23.70104005886791, 'IoU-61': 24.31359329826495, 'IoU-62': 24.00272025662015, 'IoU-63': 24.45039258816979, 'IoU-64': 22.859479938405574, 'IoU-65': 21.38553170473821, 'IoU-66': 21.405069315400006, 'IoU-67': 19.848388078068652, 'IoU-68': 19.172339120851518, 'IoU-69': 18.755111092293607, 'IoU-70': 20.329679122085647, 'IoU-71': 18.75235909926767, 'IoU-72': 18.975170181094864, 'IoU-73': 17.93732722499288, 'IoU-74': 19.4139137945571, 'IoU-75': 18.4323298684298, 'IoU-76': 18.500207713667248, 'IoU-77': 17.551868738920724, 'IoU-78': 18.370432818809114, 'IoU-79': 16.658596248481857, 'IoU-80': 17.205241717672344, 'IoU-81': 18.671106731525427, 'IoU-82': 16.40533858877765, 'IoU-83': 17.094511253975234, 'IoU-84': 16.206107262597456, 'IoU-85': 16.746908654272655, 'IoU-86': 16.42313640348048, 'IoU-87': 15.295116629348282, 'IoU-88': 16.17589245503831, 'IoU-89': 15.88729271618754, 'IoU-90': 16.62157689689447, 'IoU-91': 13.76062659407182, 'IoU-92': 14.73237669750666, 'IoU-93': 16.66912902797857, 'IoU-94': 15.350121155469997, 'IoU-95': 16.018969402301455, 'IoU-96': 14.195405025180655, 'IoU-97': 14.934296534545863, 'IoU-98': 15.82218293224937, 'IoU-99': 14.722130216393031, 'IoU-100': 14.015656122912578, 'IoU-101': 13.79591981407842, 'IoU-102': 12.722756662986912, 'IoU-103': 12.850820251553785, 'IoU-104': 11.773995139725756, 'IoU-105': 13.1282280540676, 'IoU-106': 11.05682407666594, 'IoU-107': 12.391167964745305, 'IoU-108': 12.664049176060042, 'IoU-109': 12.06505656677, 'IoU-110': 10.751534873052995, 'IoU-111': 11.176132127951872, 'IoU-112': 10.82174699317481, 'IoU-113': 9.508692230461929, 'IoU-114': 10.825181325337706, 'IoU-115': 7.437176175899901, 'IoU-116': 8.197259719628201, 'IoU-117': 10.202099282942502, 'IoU-118': 9.119602874200613, 'IoU-119': 10.047738048426984, 'IoU-120': 8.264696205040483, 'IoU-121': 8.163176412856565, 'IoU-122': 8.401020667061927, 'IoU-123': 7.451232520989848, 'IoU-124': 7.481348790832422, 'IoU-125': 7.001487490703183, 'IoU-126': 7.925242086805875, 'IoU-127': 6.377519778389951, 'IoU-128': 6.845976023743764, 'IoU-129': 6.652813977290036, 'IoU-130': 6.1766289907940655, 'IoU-131': 6.888288921629485, 'IoU-132': 4.666495668920073, 'IoU-133': 5.9093880703406105, 'IoU-134': 6.019148750301951, 'IoU-135': 4.077081597883758, 'IoU-136': 5.791402085686536, 'IoU-137': 6.548012978289375, 'IoU-138': 5.582639044105735, 'IoU-139': 4.69828649348725, 'IoU-140': 5.2657638091692265, 'IoU-141': 4.114954323447297, 'IoU-142': 5.422957668179375, 'IoU-143': 2.642749155390271, 'IoU-144': 5.892549907922153, 'IoU-145': 3.926440922446527, 'IoU-146': 5.382200261823811, 'IoU-147': 3.756380184716659, 'IoU-148': 3.898420950467976, 'IoU-149': 2.7865210003920997, 'IoU-150': 3.478944553475132, 'IoU-151': 1.6679456117812401, 'IoU-152': 4.169440964981474, 'IoU-153': 4.385671055750754, 'IoU-154': 2.3168894482816627, 'IoU-155': 1.6763916357087123, 'IoU-156': 3.7386016444302888, 'IoU-157': 1.9904216150375107, 'IoU-158': 1.4956517997838001, 'IoU-159': 1.9504160595976352, 'IoU-160': 1.724305618384149, 'IoU-161': 3.965695165849734, 'IoU-162': 1.9949136902752458, 'IoU-163': 2.854698369382871, 'IoU-164': 1.1240676806007106, 'IoU-165': 2.363754003033018, 'IoU-166': 1.9084803190833024, 'IoU-167': 1.3503826933595144, 'IoU-168': 2.310236156177641, 'IoU-169': 0.9685216136444196, 'IoU-170': 1.1300383319194374, 'IoU-171': 1.6713916637262178, 'IoU-172': 0.5018955082788396, 'IoU-173': 1.5223144532883666, 'IoU-174': 1.3496280046918756, 'IoU-175': 1.203985484743898, 'IoU-176': 0.9908170711227426, 'IoU-177': 1.142188192115421, 'IoU-178': 1.2678421188734015, 'IoU-179': 0.31632613099497653, 'IoU-180': 1.1956823984428322, 'IoU-181': 2.4013540774780777, 'IoU-182': 0.9661016275300532, 'IoU-183': 2.910394657291324, 'IoU-184': 1.2210403792735645, 'IoU-185': 0.334728363658364, 'IoU-186': 2.8819486006657313, 'IoU-187': 2.360375813618992, 'IoU-188': 2.320783119349559, 'IoU-189': 0.5784240638781357, 'IoU-190': 2.077137284812621, 'IoU-191': 2.099087032313344, 'mACC': 27.81425204160647, 'pACC': 53.58546353228169, 'ACC-0': nan, 'ACC-1': 98.95385792757236, 'ACC-2': 48.996995071414204, 'ACC-3': 70.07759658556537, 'ACC-4': 67.85864185362936, 'ACC-5': 62.26061046993338, 'ACC-6': 59.82543369947071, 'ACC-7': 47.73255156772446, 'ACC-8': 22.29922926334187, 'ACC-9': 36.052111537147645, 'ACC-10': 52.562669024015285, 'ACC-11': 56.02650407033287, 'ACC-12': 65.61666256738008, 'ACC-13': 62.92196926439804, 'ACC-14': 63.40122125029465, 'ACC-15': 65.16294684230888, 'ACC-16': 63.233829845771005, 'ACC-17': 58.43751821475479, 'ACC-18': 60.635949612215114, 'ACC-19': 60.03611217086829, 'ACC-20': 60.96855650497987, 'ACC-21': 58.419630342785176, 'ACC-22': 61.41710098758879, 'ACC-23': 57.747981379276126, 'ACC-24': 57.50754018164036, 'ACC-25': 59.43874696505449, 'ACC-26': 55.90264662570997, 'ACC-27': 56.94487089155218, 'ACC-28': 57.14230587259645, 'ACC-29': 55.96755608721771, 'ACC-30': 56.77638434831482, 'ACC-31': 57.44684295142958, 'ACC-32': 58.4294040604013, 'ACC-33': 55.75774436087219, 'ACC-34': 55.41531954861236, 'ACC-35': 57.35054745874836, 'ACC-36': 57.43083078470441, 'ACC-37': 54.56325971293986, 'ACC-38': 56.352641172246386, 'ACC-39': 52.27437664412192, 'ACC-40': 53.09073201542097, 'ACC-41': 52.845395462508414, 'ACC-42': 50.26980216299175, 'ACC-43': 51.31391522230572, 'ACC-44': 50.7697978228934, 'ACC-45': 48.533397974863895, 'ACC-46': 47.68667965703774, 'ACC-47': 44.81153883986145, 'ACC-48': 45.085240831295195, 'ACC-49': 46.70878186806778, 'ACC-50': 48.436593585726946, 'ACC-51': 44.085186253298694, 'ACC-52': 45.24633283650419, 'ACC-53': 46.01546049400553, 'ACC-54': 46.380082716255195, 'ACC-55': 45.00071541602596, 'ACC-56': 43.204029530427654, 'ACC-57': 42.81146307529216, 'ACC-58': 39.5846719015246, 'ACC-59': 41.44737224169846, 'ACC-60': 39.89950323122308, 'ACC-61': 40.04875341894953, 'ACC-62': 38.09623118589746, 'ACC-63': 39.39905218934964, 'ACC-64': 37.45978669486975, 'ACC-65': 34.403960260885, 'ACC-66': 36.07177300696517, 'ACC-67': 31.969724125920518, 'ACC-68': 32.42195139500087, 'ACC-69': 28.621023597085028, 'ACC-70': 34.30819529631344, 'ACC-71': 30.763219944572683, 'ACC-72': 31.364855088926248, 'ACC-73': 29.28567040386393, 'ACC-74': 33.635429916297824, 'ACC-75': 30.42963115036053, 'ACC-76': 30.095074937758387, 'ACC-77': 29.04777353815951, 'ACC-78': 32.28110643548378, 'ACC-79': 27.68872993973246, 'ACC-80': 28.86028234820055, 'ACC-81': 32.10851429945364, 'ACC-82': 27.28798383614361, 'ACC-83': 28.32589451912443, 'ACC-84': 28.250032068683577, 'ACC-85': 28.77181495116589, 'ACC-86': 27.867106766505373, 'ACC-87': 25.808416259742444, 'ACC-88': 28.63431738240918, 'ACC-89': 26.195986047763718, 'ACC-90': 28.190726602345656, 'ACC-91': 26.884395579034514, 'ACC-92': 24.9926518674495, 'ACC-93': 29.968807781952357, 'ACC-94': 27.9004764615583, 'ACC-95': 28.1680145056647, 'ACC-96': 25.21547097863417, 'ACC-97': 27.22096469525131, 'ACC-98': 26.967057872446905, 'ACC-99': 26.28846496509198, 'ACC-100': 23.603228163390092, 'ACC-101': 27.439598948365717, 'ACC-102': 20.83586316266425, 'ACC-103': 24.743813167091716, 'ACC-104': 23.353336120606972, 'ACC-105': 24.26953447931083, 'ACC-106': 18.69576721033518, 'ACC-107': 21.583249568119612, 'ACC-108': 23.472547601485395, 'ACC-109': 19.672621452253278, 'ACC-110': 18.969676038768277, 'ACC-111': 20.179379926734967, 'ACC-112': 19.9483296731016, 'ACC-113': 15.83345037027019, 'ACC-114': 22.527976524755676, 'ACC-115': 12.731195405409467, 'ACC-116': 13.184001974560244, 'ACC-117': 23.38497034943387, 'ACC-118': 17.41090491359732, 'ACC-119': 18.565692931315265, 'ACC-120': 14.872820010050003, 'ACC-121': 16.003255774025302, 'ACC-122': 15.073735458122222, 'ACC-123': 13.203985244804795, 'ACC-124': 12.43518595962604, 'ACC-125': 12.185012883920715, 'ACC-126': 14.603104308782411, 'ACC-127': 12.135544673812483, 'ACC-128': 11.348947604010776, 'ACC-129': 11.731413772568194, 'ACC-130': 11.094550914114274, 'ACC-131': 12.242068753310432, 'ACC-132': 8.254868253058682, 'ACC-133': 8.98582675022284, 'ACC-134': 11.732046531667386, 'ACC-135': 5.939726925488855, 'ACC-136': 11.238367149645919, 'ACC-137': 12.097614364643414, 'ACC-138': 10.797459197400817, 'ACC-139': 8.214367990896518, 'ACC-140': 10.121056110202804, 'ACC-141': 7.795650512758799, 'ACC-142': 10.180243672607835, 'ACC-143': 4.638528179373197, 'ACC-144': 12.30234657039711, 'ACC-145': 6.910082962499161, 'ACC-146': 11.489250873866258, 'ACC-147': 6.807442643012935, 'ACC-148': 6.585992504165672, 'ACC-149': 5.513407276772721, 'ACC-150': 6.562894448561563, 'ACC-151': 2.4740936369943825, 'ACC-152': 7.50208628363342, 'ACC-153': 9.024991297019, 'ACC-154': 3.6873566781382863, 'ACC-155': 2.9472739687868996, 'ACC-156': 6.555359485996108, 'ACC-157': 3.7186297607430183, 'ACC-158': 2.3549469268972874, 'ACC-159': 3.287050557576548, 'ACC-160': 2.1556745690630232, 'ACC-161': 6.167767084085746, 'ACC-162': 2.637302477529076, 'ACC-163': 6.243054143666678, 'ACC-164': 1.4164808995462315, 'ACC-165': 3.678528207555261, 'ACC-166': 3.8429963653457118, 'ACC-167': 2.0715100917718052, 'ACC-168': 12.535691680146282, 'ACC-169': 1.4815061774754208, 'ACC-170': 1.9833326234791373, 'ACC-171': 2.270902942456866, 'ACC-172': 0.5722858095343927, 'ACC-173': 2.56817420687631, 'ACC-174': 2.1147067650112494, 'ACC-175': 1.7562153457612515, 'ACC-176': 1.209332191780822, 'ACC-177': 1.6655317060027424, 'ACC-178': 2.0171485635808395, 'ACC-179': 0.3605332727372327, 'ACC-180': 1.902155508144536, 'ACC-181': 6.025377740826801, 'ACC-182': 1.7271239004096293, 'ACC-183': 11.526047025198292, 'ACC-184': 2.8105005223847157, 'ACC-185': 0.5877896798451654, 'ACC-186': 5.342612242778197, 'ACC-187': 4.6944273306031725, 'ACC-188': 8.025553851741254, 'ACC-189': 0.8266563291837925, 'ACC-190': 4.252648899043147, 'ACC-191': 11.126133768352366})])
[01/18 04:04:03] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 04:04:03] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 04:04:03] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 04:04:03] d2.evaluation.testing INFO: copypaste: 2.5437,18.0764,39.4003,27.8143,53.5855
[01/18 04:04:04] d2.utils.events INFO:  eta: 8:13:07  iter: 19999  total_loss: 36.58  loss_ce: 0.2756  loss_mask: 0.3831  loss_dice: 2.96  loss_ce_0: 0.5778  loss_mask_0: 0.3776  loss_dice_0: 3.099  loss_ce_1: 0.3053  loss_mask_1: 0.3835  loss_dice_1: 3.01  loss_ce_2: 0.2964  loss_mask_2: 0.3832  loss_dice_2: 2.986  loss_ce_3: 0.2817  loss_mask_3: 0.3821  loss_dice_3: 2.961  loss_ce_4: 0.2887  loss_mask_4: 0.3822  loss_dice_4: 2.964  loss_ce_5: 0.2817  loss_mask_5: 0.3809  loss_dice_5: 2.964  loss_ce_6: 0.2774  loss_mask_6: 0.3828  loss_dice_6: 2.962  loss_ce_7: 0.2747  loss_mask_7: 0.3822  loss_dice_7: 2.961  loss_ce_8: 0.2777  loss_mask_8: 0.3811  loss_dice_8: 2.957  time: 1.4963  data_time: 0.0652  lr: 5.3591e-06  max_mem: 21589M
[01/18 04:04:33] d2.utils.events INFO:  eta: 8:12:37  iter: 20019  total_loss: 36.68  loss_ce: 0.293  loss_mask: 0.3714  loss_dice: 2.929  loss_ce_0: 0.5984  loss_mask_0: 0.3612  loss_dice_0: 3.071  loss_ce_1: 0.3455  loss_mask_1: 0.3708  loss_dice_1: 2.991  loss_ce_2: 0.3445  loss_mask_2: 0.3702  loss_dice_2: 2.962  loss_ce_3: 0.3181  loss_mask_3: 0.3725  loss_dice_3: 2.937  loss_ce_4: 0.3082  loss_mask_4: 0.3702  loss_dice_4: 2.938  loss_ce_5: 0.2983  loss_mask_5: 0.3699  loss_dice_5: 2.932  loss_ce_6: 0.297  loss_mask_6: 0.3694  loss_dice_6: 2.938  loss_ce_7: 0.294  loss_mask_7: 0.3696  loss_dice_7: 2.936  loss_ce_8: 0.3059  loss_mask_8: 0.3705  loss_dice_8: 2.931  time: 1.4962  data_time: 0.0754  lr: 5.3543e-06  max_mem: 21589M
[01/18 04:05:02] d2.utils.events INFO:  eta: 8:11:47  iter: 20039  total_loss: 35.77  loss_ce: 0.2879  loss_mask: 0.3867  loss_dice: 2.864  loss_ce_0: 0.5533  loss_mask_0: 0.3768  loss_dice_0: 2.986  loss_ce_1: 0.3355  loss_mask_1: 0.3914  loss_dice_1: 2.878  loss_ce_2: 0.3131  loss_mask_2: 0.3861  loss_dice_2: 2.871  loss_ce_3: 0.2966  loss_mask_3: 0.3846  loss_dice_3: 2.858  loss_ce_4: 0.2954  loss_mask_4: 0.3859  loss_dice_4: 2.859  loss_ce_5: 0.2763  loss_mask_5: 0.3847  loss_dice_5: 2.865  loss_ce_6: 0.298  loss_mask_6: 0.3876  loss_dice_6: 2.858  loss_ce_7: 0.2953  loss_mask_7: 0.3858  loss_dice_7: 2.845  loss_ce_8: 0.2894  loss_mask_8: 0.3849  loss_dice_8: 2.854  time: 1.4962  data_time: 0.0704  lr: 5.3495e-06  max_mem: 21589M
[01/18 04:05:31] d2.utils.events INFO:  eta: 8:11:22  iter: 20059  total_loss: 35.9  loss_ce: 0.2877  loss_mask: 0.3736  loss_dice: 2.879  loss_ce_0: 0.5427  loss_mask_0: 0.3686  loss_dice_0: 3.029  loss_ce_1: 0.3115  loss_mask_1: 0.3748  loss_dice_1: 2.93  loss_ce_2: 0.3109  loss_mask_2: 0.3699  loss_dice_2: 2.909  loss_ce_3: 0.2997  loss_mask_3: 0.3702  loss_dice_3: 2.891  loss_ce_4: 0.3001  loss_mask_4: 0.3703  loss_dice_4: 2.889  loss_ce_5: 0.2889  loss_mask_5: 0.3742  loss_dice_5: 2.885  loss_ce_6: 0.2814  loss_mask_6: 0.3752  loss_dice_6: 2.883  loss_ce_7: 0.2861  loss_mask_7: 0.375  loss_dice_7: 2.871  loss_ce_8: 0.2968  loss_mask_8: 0.3741  loss_dice_8: 2.878  time: 1.4961  data_time: 0.0727  lr: 5.3446e-06  max_mem: 21589M
[01/18 04:06:00] d2.utils.events INFO:  eta: 8:10:46  iter: 20079  total_loss: 36.45  loss_ce: 0.2855  loss_mask: 0.3791  loss_dice: 2.932  loss_ce_0: 0.5657  loss_mask_0: 0.3711  loss_dice_0: 3.062  loss_ce_1: 0.311  loss_mask_1: 0.3853  loss_dice_1: 2.966  loss_ce_2: 0.3173  loss_mask_2: 0.3831  loss_dice_2: 2.956  loss_ce_3: 0.3039  loss_mask_3: 0.3819  loss_dice_3: 2.941  loss_ce_4: 0.3027  loss_mask_4: 0.3804  loss_dice_4: 2.927  loss_ce_5: 0.2819  loss_mask_5: 0.3788  loss_dice_5: 2.94  loss_ce_6: 0.2944  loss_mask_6: 0.3809  loss_dice_6: 2.941  loss_ce_7: 0.2884  loss_mask_7: 0.3798  loss_dice_7: 2.932  loss_ce_8: 0.2763  loss_mask_8: 0.3807  loss_dice_8: 2.932  time: 1.4961  data_time: 0.0714  lr: 5.3398e-06  max_mem: 21589M
[01/18 04:06:30] d2.utils.events INFO:  eta: 8:10:17  iter: 20099  total_loss: 36.16  loss_ce: 0.2906  loss_mask: 0.3815  loss_dice: 2.929  loss_ce_0: 0.5715  loss_mask_0: 0.373  loss_dice_0: 3.054  loss_ce_1: 0.3116  loss_mask_1: 0.3826  loss_dice_1: 2.959  loss_ce_2: 0.311  loss_mask_2: 0.3824  loss_dice_2: 2.937  loss_ce_3: 0.2959  loss_mask_3: 0.3823  loss_dice_3: 2.924  loss_ce_4: 0.2972  loss_mask_4: 0.3805  loss_dice_4: 2.923  loss_ce_5: 0.2856  loss_mask_5: 0.3786  loss_dice_5: 2.929  loss_ce_6: 0.2835  loss_mask_6: 0.3833  loss_dice_6: 2.924  loss_ce_7: 0.289  loss_mask_7: 0.3805  loss_dice_7: 2.929  loss_ce_8: 0.2822  loss_mask_8: 0.3829  loss_dice_8: 2.929  time: 1.4961  data_time: 0.0738  lr: 5.335e-06  max_mem: 21589M
[01/18 04:06:59] d2.utils.events INFO:  eta: 8:09:31  iter: 20119  total_loss: 36.42  loss_ce: 0.2781  loss_mask: 0.3781  loss_dice: 2.914  loss_ce_0: 0.5783  loss_mask_0: 0.3699  loss_dice_0: 3.059  loss_ce_1: 0.3264  loss_mask_1: 0.3853  loss_dice_1: 2.956  loss_ce_2: 0.3125  loss_mask_2: 0.3806  loss_dice_2: 2.933  loss_ce_3: 0.3143  loss_mask_3: 0.3811  loss_dice_3: 2.916  loss_ce_4: 0.2931  loss_mask_4: 0.3787  loss_dice_4: 2.923  loss_ce_5: 0.288  loss_mask_5: 0.3782  loss_dice_5: 2.922  loss_ce_6: 0.3039  loss_mask_6: 0.3781  loss_dice_6: 2.917  loss_ce_7: 0.284  loss_mask_7: 0.3777  loss_dice_7: 2.915  loss_ce_8: 0.2872  loss_mask_8: 0.3782  loss_dice_8: 2.925  time: 1.4960  data_time: 0.0745  lr: 5.3302e-06  max_mem: 21589M
[01/18 04:07:28] d2.utils.events INFO:  eta: 8:08:52  iter: 20139  total_loss: 36.07  loss_ce: 0.2918  loss_mask: 0.3831  loss_dice: 2.878  loss_ce_0: 0.5593  loss_mask_0: 0.3839  loss_dice_0: 3.001  loss_ce_1: 0.3161  loss_mask_1: 0.3933  loss_dice_1: 2.91  loss_ce_2: 0.3237  loss_mask_2: 0.3861  loss_dice_2: 2.897  loss_ce_3: 0.3124  loss_mask_3: 0.3849  loss_dice_3: 2.884  loss_ce_4: 0.3064  loss_mask_4: 0.3831  loss_dice_4: 2.878  loss_ce_5: 0.291  loss_mask_5: 0.3832  loss_dice_5: 2.871  loss_ce_6: 0.289  loss_mask_6: 0.3839  loss_dice_6: 2.876  loss_ce_7: 0.2919  loss_mask_7: 0.3826  loss_dice_7: 2.884  loss_ce_8: 0.2827  loss_mask_8: 0.3823  loss_dice_8: 2.872  time: 1.4960  data_time: 0.0678  lr: 5.3253e-06  max_mem: 21589M
[01/18 04:07:57] d2.utils.events INFO:  eta: 8:08:16  iter: 20159  total_loss: 36.27  loss_ce: 0.2837  loss_mask: 0.3957  loss_dice: 2.875  loss_ce_0: 0.5606  loss_mask_0: 0.3882  loss_dice_0: 3.014  loss_ce_1: 0.3291  loss_mask_1: 0.4008  loss_dice_1: 2.939  loss_ce_2: 0.3212  loss_mask_2: 0.3973  loss_dice_2: 2.91  loss_ce_3: 0.309  loss_mask_3: 0.3956  loss_dice_3: 2.893  loss_ce_4: 0.3035  loss_mask_4: 0.3969  loss_dice_4: 2.893  loss_ce_5: 0.297  loss_mask_5: 0.3941  loss_dice_5: 2.891  loss_ce_6: 0.294  loss_mask_6: 0.3939  loss_dice_6: 2.898  loss_ce_7: 0.3001  loss_mask_7: 0.3936  loss_dice_7: 2.889  loss_ce_8: 0.2906  loss_mask_8: 0.3937  loss_dice_8: 2.884  time: 1.4959  data_time: 0.0689  lr: 5.3205e-06  max_mem: 21589M
[01/18 04:08:26] d2.utils.events INFO:  eta: 8:07:33  iter: 20179  total_loss: 35.73  loss_ce: 0.2721  loss_mask: 0.3877  loss_dice: 2.87  loss_ce_0: 0.5724  loss_mask_0: 0.3811  loss_dice_0: 2.992  loss_ce_1: 0.3117  loss_mask_1: 0.3929  loss_dice_1: 2.893  loss_ce_2: 0.3098  loss_mask_2: 0.3901  loss_dice_2: 2.881  loss_ce_3: 0.3025  loss_mask_3: 0.3908  loss_dice_3: 2.864  loss_ce_4: 0.2681  loss_mask_4: 0.3873  loss_dice_4: 2.873  loss_ce_5: 0.2851  loss_mask_5: 0.388  loss_dice_5: 2.868  loss_ce_6: 0.2705  loss_mask_6: 0.389  loss_dice_6: 2.864  loss_ce_7: 0.2732  loss_mask_7: 0.3875  loss_dice_7: 2.87  loss_ce_8: 0.2834  loss_mask_8: 0.388  loss_dice_8: 2.877  time: 1.4959  data_time: 0.0637  lr: 5.3157e-06  max_mem: 21589M
[01/18 04:08:55] d2.utils.events INFO:  eta: 8:06:47  iter: 20199  total_loss: 37.05  loss_ce: 0.2769  loss_mask: 0.3756  loss_dice: 2.989  loss_ce_0: 0.5705  loss_mask_0: 0.3675  loss_dice_0: 3.12  loss_ce_1: 0.315  loss_mask_1: 0.3842  loss_dice_1: 3.039  loss_ce_2: 0.3169  loss_mask_2: 0.3807  loss_dice_2: 3.014  loss_ce_3: 0.2891  loss_mask_3: 0.3763  loss_dice_3: 2.998  loss_ce_4: 0.2984  loss_mask_4: 0.3764  loss_dice_4: 3  loss_ce_5: 0.2889  loss_mask_5: 0.3771  loss_dice_5: 2.99  loss_ce_6: 0.28  loss_mask_6: 0.3753  loss_dice_6: 2.995  loss_ce_7: 0.2684  loss_mask_7: 0.3751  loss_dice_7: 2.998  loss_ce_8: 0.2821  loss_mask_8: 0.3751  loss_dice_8: 2.996  time: 1.4958  data_time: 0.0782  lr: 5.3109e-06  max_mem: 21589M
[01/18 04:09:24] d2.utils.events INFO:  eta: 8:06:16  iter: 20219  total_loss: 36.32  loss_ce: 0.2755  loss_mask: 0.3776  loss_dice: 2.923  loss_ce_0: 0.5617  loss_mask_0: 0.378  loss_dice_0: 3.048  loss_ce_1: 0.313  loss_mask_1: 0.3809  loss_dice_1: 2.947  loss_ce_2: 0.3207  loss_mask_2: 0.3777  loss_dice_2: 2.929  loss_ce_3: 0.2817  loss_mask_3: 0.376  loss_dice_3: 2.926  loss_ce_4: 0.2812  loss_mask_4: 0.3754  loss_dice_4: 2.927  loss_ce_5: 0.284  loss_mask_5: 0.3771  loss_dice_5: 2.924  loss_ce_6: 0.2879  loss_mask_6: 0.377  loss_dice_6: 2.917  loss_ce_7: 0.2677  loss_mask_7: 0.3769  loss_dice_7: 2.916  loss_ce_8: 0.2725  loss_mask_8: 0.3769  loss_dice_8: 2.921  time: 1.4958  data_time: 0.0715  lr: 5.306e-06  max_mem: 21589M
[01/18 04:09:53] d2.utils.events INFO:  eta: 8:05:42  iter: 20239  total_loss: 36.85  loss_ce: 0.2919  loss_mask: 0.3772  loss_dice: 2.941  loss_ce_0: 0.5882  loss_mask_0: 0.3744  loss_dice_0: 3.062  loss_ce_1: 0.3319  loss_mask_1: 0.3871  loss_dice_1: 2.981  loss_ce_2: 0.3253  loss_mask_2: 0.3848  loss_dice_2: 2.956  loss_ce_3: 0.3093  loss_mask_3: 0.3776  loss_dice_3: 2.943  loss_ce_4: 0.3024  loss_mask_4: 0.3791  loss_dice_4: 2.941  loss_ce_5: 0.2921  loss_mask_5: 0.3777  loss_dice_5: 2.94  loss_ce_6: 0.2816  loss_mask_6: 0.3773  loss_dice_6: 2.945  loss_ce_7: 0.2859  loss_mask_7: 0.376  loss_dice_7: 2.943  loss_ce_8: 0.2879  loss_mask_8: 0.3768  loss_dice_8: 2.942  time: 1.4957  data_time: 0.0702  lr: 5.3012e-06  max_mem: 21589M
[01/18 04:10:22] d2.utils.events INFO:  eta: 8:05:06  iter: 20259  total_loss: 35.41  loss_ce: 0.2682  loss_mask: 0.3855  loss_dice: 2.82  loss_ce_0: 0.5625  loss_mask_0: 0.3741  loss_dice_0: 2.95  loss_ce_1: 0.3153  loss_mask_1: 0.3879  loss_dice_1: 2.872  loss_ce_2: 0.3091  loss_mask_2: 0.386  loss_dice_2: 2.84  loss_ce_3: 0.292  loss_mask_3: 0.3841  loss_dice_3: 2.827  loss_ce_4: 0.2771  loss_mask_4: 0.3847  loss_dice_4: 2.825  loss_ce_5: 0.2762  loss_mask_5: 0.382  loss_dice_5: 2.832  loss_ce_6: 0.2708  loss_mask_6: 0.385  loss_dice_6: 2.832  loss_ce_7: 0.2675  loss_mask_7: 0.3838  loss_dice_7: 2.823  loss_ce_8: 0.2679  loss_mask_8: 0.3857  loss_dice_8: 2.824  time: 1.4957  data_time: 0.0706  lr: 5.2964e-06  max_mem: 21589M
[01/18 04:10:51] d2.utils.events INFO:  eta: 8:04:37  iter: 20279  total_loss: 36.18  loss_ce: 0.2862  loss_mask: 0.3919  loss_dice: 2.898  loss_ce_0: 0.5614  loss_mask_0: 0.3781  loss_dice_0: 3.03  loss_ce_1: 0.3135  loss_mask_1: 0.3929  loss_dice_1: 2.935  loss_ce_2: 0.3139  loss_mask_2: 0.3908  loss_dice_2: 2.915  loss_ce_3: 0.3016  loss_mask_3: 0.3922  loss_dice_3: 2.904  loss_ce_4: 0.2908  loss_mask_4: 0.3904  loss_dice_4: 2.902  loss_ce_5: 0.2918  loss_mask_5: 0.3917  loss_dice_5: 2.9  loss_ce_6: 0.2846  loss_mask_6: 0.3901  loss_dice_6: 2.905  loss_ce_7: 0.2735  loss_mask_7: 0.3911  loss_dice_7: 2.895  loss_ce_8: 0.2842  loss_mask_8: 0.3934  loss_dice_8: 2.898  time: 1.4956  data_time: 0.0644  lr: 5.2915e-06  max_mem: 21589M
[01/18 04:11:20] d2.utils.events INFO:  eta: 8:04:03  iter: 20299  total_loss: 36.22  loss_ce: 0.2926  loss_mask: 0.386  loss_dice: 2.882  loss_ce_0: 0.5827  loss_mask_0: 0.3867  loss_dice_0: 3.011  loss_ce_1: 0.343  loss_mask_1: 0.3941  loss_dice_1: 2.927  loss_ce_2: 0.3499  loss_mask_2: 0.3914  loss_dice_2: 2.896  loss_ce_3: 0.3188  loss_mask_3: 0.3887  loss_dice_3: 2.887  loss_ce_4: 0.3115  loss_mask_4: 0.3892  loss_dice_4: 2.887  loss_ce_5: 0.3061  loss_mask_5: 0.3878  loss_dice_5: 2.887  loss_ce_6: 0.3065  loss_mask_6: 0.3857  loss_dice_6: 2.888  loss_ce_7: 0.312  loss_mask_7: 0.3865  loss_dice_7: 2.885  loss_ce_8: 0.2957  loss_mask_8: 0.3871  loss_dice_8: 2.884  time: 1.4956  data_time: 0.0728  lr: 5.2867e-06  max_mem: 21589M
[01/18 04:11:49] d2.utils.events INFO:  eta: 8:03:19  iter: 20319  total_loss: 35.99  loss_ce: 0.2731  loss_mask: 0.3858  loss_dice: 2.878  loss_ce_0: 0.5521  loss_mask_0: 0.3809  loss_dice_0: 2.996  loss_ce_1: 0.3091  loss_mask_1: 0.3921  loss_dice_1: 2.908  loss_ce_2: 0.3039  loss_mask_2: 0.3904  loss_dice_2: 2.892  loss_ce_3: 0.2817  loss_mask_3: 0.388  loss_dice_3: 2.887  loss_ce_4: 0.2653  loss_mask_4: 0.3895  loss_dice_4: 2.885  loss_ce_5: 0.2721  loss_mask_5: 0.389  loss_dice_5: 2.881  loss_ce_6: 0.281  loss_mask_6: 0.3898  loss_dice_6: 2.883  loss_ce_7: 0.2837  loss_mask_7: 0.3883  loss_dice_7: 2.879  loss_ce_8: 0.2791  loss_mask_8: 0.3879  loss_dice_8: 2.885  time: 1.4955  data_time: 0.0735  lr: 5.2819e-06  max_mem: 21589M
[01/18 04:12:18] d2.utils.events INFO:  eta: 8:02:26  iter: 20339  total_loss: 35.75  loss_ce: 0.2683  loss_mask: 0.3773  loss_dice: 2.874  loss_ce_0: 0.5785  loss_mask_0: 0.3698  loss_dice_0: 2.994  loss_ce_1: 0.3329  loss_mask_1: 0.3812  loss_dice_1: 2.907  loss_ce_2: 0.3216  loss_mask_2: 0.3787  loss_dice_2: 2.891  loss_ce_3: 0.2939  loss_mask_3: 0.3777  loss_dice_3: 2.886  loss_ce_4: 0.2988  loss_mask_4: 0.3762  loss_dice_4: 2.88  loss_ce_5: 0.2944  loss_mask_5: 0.3776  loss_dice_5: 2.874  loss_ce_6: 0.292  loss_mask_6: 0.3777  loss_dice_6: 2.871  loss_ce_7: 0.2839  loss_mask_7: 0.3782  loss_dice_7: 2.867  loss_ce_8: 0.2857  loss_mask_8: 0.3774  loss_dice_8: 2.872  time: 1.4955  data_time: 0.0676  lr: 5.277e-06  max_mem: 21589M
[01/18 04:12:47] d2.utils.events INFO:  eta: 8:01:44  iter: 20359  total_loss: 36.36  loss_ce: 0.2807  loss_mask: 0.3774  loss_dice: 2.932  loss_ce_0: 0.5632  loss_mask_0: 0.3724  loss_dice_0: 3.057  loss_ce_1: 0.3127  loss_mask_1: 0.3868  loss_dice_1: 2.959  loss_ce_2: 0.2937  loss_mask_2: 0.3817  loss_dice_2: 2.949  loss_ce_3: 0.3107  loss_mask_3: 0.38  loss_dice_3: 2.932  loss_ce_4: 0.2826  loss_mask_4: 0.3787  loss_dice_4: 2.934  loss_ce_5: 0.2904  loss_mask_5: 0.3783  loss_dice_5: 2.933  loss_ce_6: 0.2717  loss_mask_6: 0.377  loss_dice_6: 2.932  loss_ce_7: 0.2726  loss_mask_7: 0.3789  loss_dice_7: 2.928  loss_ce_8: 0.2629  loss_mask_8: 0.3782  loss_dice_8: 2.927  time: 1.4954  data_time: 0.0715  lr: 5.2722e-06  max_mem: 21589M
[01/18 04:13:16] d2.utils.events INFO:  eta: 8:01:01  iter: 20379  total_loss: 35.91  loss_ce: 0.2782  loss_mask: 0.3785  loss_dice: 2.867  loss_ce_0: 0.5598  loss_mask_0: 0.3747  loss_dice_0: 3  loss_ce_1: 0.3204  loss_mask_1: 0.3862  loss_dice_1: 2.904  loss_ce_2: 0.3108  loss_mask_2: 0.3838  loss_dice_2: 2.885  loss_ce_3: 0.3024  loss_mask_3: 0.3792  loss_dice_3: 2.871  loss_ce_4: 0.2835  loss_mask_4: 0.3789  loss_dice_4: 2.873  loss_ce_5: 0.2877  loss_mask_5: 0.3792  loss_dice_5: 2.874  loss_ce_6: 0.2805  loss_mask_6: 0.3792  loss_dice_6: 2.861  loss_ce_7: 0.2856  loss_mask_7: 0.3772  loss_dice_7: 2.864  loss_ce_8: 0.2654  loss_mask_8: 0.3791  loss_dice_8: 2.874  time: 1.4954  data_time: 0.0699  lr: 5.2674e-06  max_mem: 21589M
[01/18 04:13:45] d2.utils.events INFO:  eta: 8:00:26  iter: 20399  total_loss: 36.82  loss_ce: 0.2614  loss_mask: 0.3806  loss_dice: 2.984  loss_ce_0: 0.5587  loss_mask_0: 0.3786  loss_dice_0: 3.098  loss_ce_1: 0.3042  loss_mask_1: 0.3846  loss_dice_1: 3.02  loss_ce_2: 0.2977  loss_mask_2: 0.381  loss_dice_2: 3.01  loss_ce_3: 0.2802  loss_mask_3: 0.3793  loss_dice_3: 2.993  loss_ce_4: 0.2665  loss_mask_4: 0.378  loss_dice_4: 2.991  loss_ce_5: 0.2676  loss_mask_5: 0.3794  loss_dice_5: 2.992  loss_ce_6: 0.2602  loss_mask_6: 0.3778  loss_dice_6: 2.989  loss_ce_7: 0.2797  loss_mask_7: 0.3796  loss_dice_7: 2.988  loss_ce_8: 0.272  loss_mask_8: 0.3802  loss_dice_8: 2.995  time: 1.4953  data_time: 0.0694  lr: 5.2626e-06  max_mem: 21589M
[01/18 04:14:14] d2.utils.events INFO:  eta: 8:00:03  iter: 20419  total_loss: 36.34  loss_ce: 0.2906  loss_mask: 0.3763  loss_dice: 2.9  loss_ce_0: 0.5857  loss_mask_0: 0.3704  loss_dice_0: 3.039  loss_ce_1: 0.3411  loss_mask_1: 0.3801  loss_dice_1: 2.942  loss_ce_2: 0.3333  loss_mask_2: 0.3785  loss_dice_2: 2.92  loss_ce_3: 0.3177  loss_mask_3: 0.375  loss_dice_3: 2.905  loss_ce_4: 0.2913  loss_mask_4: 0.3749  loss_dice_4: 2.909  loss_ce_5: 0.3049  loss_mask_5: 0.375  loss_dice_5: 2.904  loss_ce_6: 0.3029  loss_mask_6: 0.3744  loss_dice_6: 2.915  loss_ce_7: 0.3015  loss_mask_7: 0.3758  loss_dice_7: 2.9  loss_ce_8: 0.2859  loss_mask_8: 0.3762  loss_dice_8: 2.901  time: 1.4953  data_time: 0.0718  lr: 5.2577e-06  max_mem: 21589M
[01/18 04:14:44] d2.utils.events INFO:  eta: 7:59:36  iter: 20439  total_loss: 36.29  loss_ce: 0.2977  loss_mask: 0.3845  loss_dice: 2.87  loss_ce_0: 0.5694  loss_mask_0: 0.3816  loss_dice_0: 3.018  loss_ce_1: 0.3339  loss_mask_1: 0.3914  loss_dice_1: 2.922  loss_ce_2: 0.3305  loss_mask_2: 0.3861  loss_dice_2: 2.892  loss_ce_3: 0.3145  loss_mask_3: 0.3872  loss_dice_3: 2.873  loss_ce_4: 0.3011  loss_mask_4: 0.3838  loss_dice_4: 2.883  loss_ce_5: 0.3017  loss_mask_5: 0.3827  loss_dice_5: 2.876  loss_ce_6: 0.3049  loss_mask_6: 0.3835  loss_dice_6: 2.87  loss_ce_7: 0.3089  loss_mask_7: 0.3828  loss_dice_7: 2.873  loss_ce_8: 0.2935  loss_mask_8: 0.385  loss_dice_8: 2.875  time: 1.4953  data_time: 0.0717  lr: 5.2529e-06  max_mem: 21589M
[01/18 04:15:13] d2.utils.events INFO:  eta: 7:58:55  iter: 20459  total_loss: 36  loss_ce: 0.2694  loss_mask: 0.3792  loss_dice: 2.884  loss_ce_0: 0.5623  loss_mask_0: 0.37  loss_dice_0: 3.021  loss_ce_1: 0.2976  loss_mask_1: 0.3831  loss_dice_1: 2.926  loss_ce_2: 0.3082  loss_mask_2: 0.3797  loss_dice_2: 2.914  loss_ce_3: 0.2953  loss_mask_3: 0.3779  loss_dice_3: 2.893  loss_ce_4: 0.28  loss_mask_4: 0.3798  loss_dice_4: 2.889  loss_ce_5: 0.2803  loss_mask_5: 0.3815  loss_dice_5: 2.893  loss_ce_6: 0.2626  loss_mask_6: 0.3813  loss_dice_6: 2.891  loss_ce_7: 0.286  loss_mask_7: 0.3795  loss_dice_7: 2.888  loss_ce_8: 0.2662  loss_mask_8: 0.3799  loss_dice_8: 2.889  time: 1.4953  data_time: 0.0707  lr: 5.2481e-06  max_mem: 21589M
[01/18 04:15:42] d2.utils.events INFO:  eta: 7:58:00  iter: 20479  total_loss: 36.44  loss_ce: 0.2817  loss_mask: 0.3826  loss_dice: 2.93  loss_ce_0: 0.5681  loss_mask_0: 0.38  loss_dice_0: 3.068  loss_ce_1: 0.3073  loss_mask_1: 0.3877  loss_dice_1: 2.978  loss_ce_2: 0.3229  loss_mask_2: 0.3841  loss_dice_2: 2.953  loss_ce_3: 0.2983  loss_mask_3: 0.381  loss_dice_3: 2.933  loss_ce_4: 0.2988  loss_mask_4: 0.3814  loss_dice_4: 2.925  loss_ce_5: 0.284  loss_mask_5: 0.3828  loss_dice_5: 2.939  loss_ce_6: 0.2869  loss_mask_6: 0.3844  loss_dice_6: 2.921  loss_ce_7: 0.2962  loss_mask_7: 0.3849  loss_dice_7: 2.932  loss_ce_8: 0.2835  loss_mask_8: 0.383  loss_dice_8: 2.924  time: 1.4952  data_time: 0.0704  lr: 5.2432e-06  max_mem: 21589M
[01/18 04:16:11] d2.utils.events INFO:  eta: 7:56:52  iter: 20499  total_loss: 35.93  loss_ce: 0.294  loss_mask: 0.3836  loss_dice: 2.866  loss_ce_0: 0.5841  loss_mask_0: 0.3807  loss_dice_0: 3.007  loss_ce_1: 0.3156  loss_mask_1: 0.3877  loss_dice_1: 2.898  loss_ce_2: 0.3303  loss_mask_2: 0.3835  loss_dice_2: 2.879  loss_ce_3: 0.2934  loss_mask_3: 0.3823  loss_dice_3: 2.865  loss_ce_4: 0.2995  loss_mask_4: 0.385  loss_dice_4: 2.872  loss_ce_5: 0.293  loss_mask_5: 0.3838  loss_dice_5: 2.859  loss_ce_6: 0.2941  loss_mask_6: 0.3845  loss_dice_6: 2.865  loss_ce_7: 0.2854  loss_mask_7: 0.3832  loss_dice_7: 2.874  loss_ce_8: 0.2807  loss_mask_8: 0.3823  loss_dice_8: 2.86  time: 1.4952  data_time: 0.0675  lr: 5.2384e-06  max_mem: 21589M
[01/18 04:16:40] d2.utils.events INFO:  eta: 7:55:30  iter: 20519  total_loss: 36.2  loss_ce: 0.2571  loss_mask: 0.3818  loss_dice: 2.887  loss_ce_0: 0.5466  loss_mask_0: 0.3684  loss_dice_0: 3.019  loss_ce_1: 0.3183  loss_mask_1: 0.388  loss_dice_1: 2.921  loss_ce_2: 0.293  loss_mask_2: 0.3848  loss_dice_2: 2.894  loss_ce_3: 0.2751  loss_mask_3: 0.3839  loss_dice_3: 2.888  loss_ce_4: 0.2801  loss_mask_4: 0.3849  loss_dice_4: 2.873  loss_ce_5: 0.272  loss_mask_5: 0.3803  loss_dice_5: 2.883  loss_ce_6: 0.2621  loss_mask_6: 0.3806  loss_dice_6: 2.885  loss_ce_7: 0.2628  loss_mask_7: 0.3801  loss_dice_7: 2.885  loss_ce_8: 0.2665  loss_mask_8: 0.3834  loss_dice_8: 2.873  time: 1.4951  data_time: 0.0666  lr: 5.2335e-06  max_mem: 21589M
[01/18 04:17:08] d2.utils.events INFO:  eta: 7:54:39  iter: 20539  total_loss: 36.52  loss_ce: 0.2998  loss_mask: 0.3855  loss_dice: 2.889  loss_ce_0: 0.5825  loss_mask_0: 0.372  loss_dice_0: 3.019  loss_ce_1: 0.3463  loss_mask_1: 0.3899  loss_dice_1: 2.935  loss_ce_2: 0.3152  loss_mask_2: 0.3877  loss_dice_2: 2.912  loss_ce_3: 0.304  loss_mask_3: 0.3853  loss_dice_3: 2.894  loss_ce_4: 0.3031  loss_mask_4: 0.3847  loss_dice_4: 2.897  loss_ce_5: 0.284  loss_mask_5: 0.3839  loss_dice_5: 2.891  loss_ce_6: 0.3024  loss_mask_6: 0.3855  loss_dice_6: 2.891  loss_ce_7: 0.2884  loss_mask_7: 0.3849  loss_dice_7: 2.896  loss_ce_8: 0.29  loss_mask_8: 0.3862  loss_dice_8: 2.894  time: 1.4950  data_time: 0.0699  lr: 5.2287e-06  max_mem: 21589M
[01/18 04:17:37] d2.utils.events INFO:  eta: 7:53:43  iter: 20559  total_loss: 36.18  loss_ce: 0.2768  loss_mask: 0.3806  loss_dice: 2.903  loss_ce_0: 0.5574  loss_mask_0: 0.378  loss_dice_0: 3.037  loss_ce_1: 0.3401  loss_mask_1: 0.3886  loss_dice_1: 2.937  loss_ce_2: 0.3176  loss_mask_2: 0.3843  loss_dice_2: 2.919  loss_ce_3: 0.3026  loss_mask_3: 0.3814  loss_dice_3: 2.899  loss_ce_4: 0.2914  loss_mask_4: 0.3799  loss_dice_4: 2.908  loss_ce_5: 0.2945  loss_mask_5: 0.38  loss_dice_5: 2.9  loss_ce_6: 0.2911  loss_mask_6: 0.3809  loss_dice_6: 2.901  loss_ce_7: 0.2931  loss_mask_7: 0.3821  loss_dice_7: 2.895  loss_ce_8: 0.2878  loss_mask_8: 0.3819  loss_dice_8: 2.893  time: 1.4950  data_time: 0.0657  lr: 5.2239e-06  max_mem: 21589M
[01/18 04:18:06] d2.utils.events INFO:  eta: 7:52:57  iter: 20579  total_loss: 36.02  loss_ce: 0.2758  loss_mask: 0.3871  loss_dice: 2.867  loss_ce_0: 0.5571  loss_mask_0: 0.3841  loss_dice_0: 2.987  loss_ce_1: 0.3015  loss_mask_1: 0.3947  loss_dice_1: 2.915  loss_ce_2: 0.2984  loss_mask_2: 0.3897  loss_dice_2: 2.889  loss_ce_3: 0.277  loss_mask_3: 0.3867  loss_dice_3: 2.884  loss_ce_4: 0.2768  loss_mask_4: 0.3855  loss_dice_4: 2.872  loss_ce_5: 0.2811  loss_mask_5: 0.3856  loss_dice_5: 2.875  loss_ce_6: 0.2557  loss_mask_6: 0.3873  loss_dice_6: 2.867  loss_ce_7: 0.2669  loss_mask_7: 0.3878  loss_dice_7: 2.867  loss_ce_8: 0.2735  loss_mask_8: 0.3886  loss_dice_8: 2.874  time: 1.4950  data_time: 0.0706  lr: 5.219e-06  max_mem: 21589M
[01/18 04:18:36] d2.utils.events INFO:  eta: 7:52:28  iter: 20599  total_loss: 36.41  loss_ce: 0.2872  loss_mask: 0.3888  loss_dice: 2.863  loss_ce_0: 0.5944  loss_mask_0: 0.3836  loss_dice_0: 2.996  loss_ce_1: 0.3291  loss_mask_1: 0.3947  loss_dice_1: 2.909  loss_ce_2: 0.3132  loss_mask_2: 0.3904  loss_dice_2: 2.888  loss_ce_3: 0.2955  loss_mask_3: 0.3909  loss_dice_3: 2.866  loss_ce_4: 0.2946  loss_mask_4: 0.3886  loss_dice_4: 2.867  loss_ce_5: 0.302  loss_mask_5: 0.388  loss_dice_5: 2.867  loss_ce_6: 0.2983  loss_mask_6: 0.3889  loss_dice_6: 2.859  loss_ce_7: 0.2768  loss_mask_7: 0.3894  loss_dice_7: 2.865  loss_ce_8: 0.2834  loss_mask_8: 0.3897  loss_dice_8: 2.859  time: 1.4949  data_time: 0.0711  lr: 5.2142e-06  max_mem: 21589M
[01/18 04:19:05] d2.utils.events INFO:  eta: 7:52:01  iter: 20619  total_loss: 36.58  loss_ce: 0.3094  loss_mask: 0.3834  loss_dice: 2.896  loss_ce_0: 0.5593  loss_mask_0: 0.3831  loss_dice_0: 3.029  loss_ce_1: 0.3332  loss_mask_1: 0.3873  loss_dice_1: 2.931  loss_ce_2: 0.344  loss_mask_2: 0.3832  loss_dice_2: 2.914  loss_ce_3: 0.3287  loss_mask_3: 0.3797  loss_dice_3: 2.901  loss_ce_4: 0.3193  loss_mask_4: 0.3814  loss_dice_4: 2.903  loss_ce_5: 0.3143  loss_mask_5: 0.3818  loss_dice_5: 2.903  loss_ce_6: 0.3078  loss_mask_6: 0.3825  loss_dice_6: 2.897  loss_ce_7: 0.3156  loss_mask_7: 0.3816  loss_dice_7: 2.894  loss_ce_8: 0.3053  loss_mask_8: 0.383  loss_dice_8: 2.897  time: 1.4949  data_time: 0.0696  lr: 5.2094e-06  max_mem: 21589M
[01/18 04:19:34] d2.utils.events INFO:  eta: 7:51:29  iter: 20639  total_loss: 36.36  loss_ce: 0.2903  loss_mask: 0.3696  loss_dice: 2.892  loss_ce_0: 0.585  loss_mask_0: 0.3709  loss_dice_0: 3.028  loss_ce_1: 0.333  loss_mask_1: 0.3781  loss_dice_1: 2.936  loss_ce_2: 0.3355  loss_mask_2: 0.3751  loss_dice_2: 2.903  loss_ce_3: 0.2998  loss_mask_3: 0.3708  loss_dice_3: 2.896  loss_ce_4: 0.3006  loss_mask_4: 0.3717  loss_dice_4: 2.892  loss_ce_5: 0.3043  loss_mask_5: 0.3714  loss_dice_5: 2.897  loss_ce_6: 0.3048  loss_mask_6: 0.3706  loss_dice_6: 2.898  loss_ce_7: 0.3064  loss_mask_7: 0.3696  loss_dice_7: 2.898  loss_ce_8: 0.2908  loss_mask_8: 0.3704  loss_dice_8: 2.899  time: 1.4949  data_time: 0.0756  lr: 5.2045e-06  max_mem: 21589M
[01/18 04:20:03] d2.utils.events INFO:  eta: 7:50:59  iter: 20659  total_loss: 36.39  loss_ce: 0.2937  loss_mask: 0.3782  loss_dice: 2.922  loss_ce_0: 0.571  loss_mask_0: 0.3778  loss_dice_0: 3.052  loss_ce_1: 0.33  loss_mask_1: 0.3801  loss_dice_1: 2.97  loss_ce_2: 0.3196  loss_mask_2: 0.3796  loss_dice_2: 2.943  loss_ce_3: 0.2962  loss_mask_3: 0.3785  loss_dice_3: 2.925  loss_ce_4: 0.2966  loss_mask_4: 0.3811  loss_dice_4: 2.925  loss_ce_5: 0.3016  loss_mask_5: 0.3813  loss_dice_5: 2.921  loss_ce_6: 0.285  loss_mask_6: 0.3807  loss_dice_6: 2.919  loss_ce_7: 0.2832  loss_mask_7: 0.3786  loss_dice_7: 2.926  loss_ce_8: 0.2845  loss_mask_8: 0.3792  loss_dice_8: 2.918  time: 1.4948  data_time: 0.0714  lr: 5.1997e-06  max_mem: 21589M
[01/18 04:20:33] d2.utils.events INFO:  eta: 7:50:33  iter: 20679  total_loss: 36.26  loss_ce: 0.3029  loss_mask: 0.3795  loss_dice: 2.887  loss_ce_0: 0.5828  loss_mask_0: 0.3736  loss_dice_0: 3.027  loss_ce_1: 0.3261  loss_mask_1: 0.3833  loss_dice_1: 2.929  loss_ce_2: 0.3358  loss_mask_2: 0.3795  loss_dice_2: 2.918  loss_ce_3: 0.3151  loss_mask_3: 0.3813  loss_dice_3: 2.897  loss_ce_4: 0.3232  loss_mask_4: 0.3791  loss_dice_4: 2.901  loss_ce_5: 0.3135  loss_mask_5: 0.3791  loss_dice_5: 2.9  loss_ce_6: 0.2871  loss_mask_6: 0.3784  loss_dice_6: 2.897  loss_ce_7: 0.2928  loss_mask_7: 0.3775  loss_dice_7: 2.897  loss_ce_8: 0.2946  loss_mask_8: 0.378  loss_dice_8: 2.894  time: 1.4948  data_time: 0.0711  lr: 5.1948e-06  max_mem: 21589M
[01/18 04:21:02] d2.utils.events INFO:  eta: 7:50:01  iter: 20699  total_loss: 36.21  loss_ce: 0.2919  loss_mask: 0.3845  loss_dice: 2.865  loss_ce_0: 0.5585  loss_mask_0: 0.3778  loss_dice_0: 2.999  loss_ce_1: 0.3387  loss_mask_1: 0.3911  loss_dice_1: 2.909  loss_ce_2: 0.3395  loss_mask_2: 0.389  loss_dice_2: 2.885  loss_ce_3: 0.3118  loss_mask_3: 0.3862  loss_dice_3: 2.873  loss_ce_4: 0.3016  loss_mask_4: 0.3847  loss_dice_4: 2.868  loss_ce_5: 0.2942  loss_mask_5: 0.3823  loss_dice_5: 2.88  loss_ce_6: 0.2902  loss_mask_6: 0.382  loss_dice_6: 2.868  loss_ce_7: 0.2867  loss_mask_7: 0.3849  loss_dice_7: 2.869  loss_ce_8: 0.293  loss_mask_8: 0.3827  loss_dice_8: 2.865  time: 1.4947  data_time: 0.0679  lr: 5.19e-06  max_mem: 21589M
[01/18 04:21:30] d2.utils.events INFO:  eta: 7:49:13  iter: 20719  total_loss: 36.89  loss_ce: 0.2952  loss_mask: 0.3801  loss_dice: 2.964  loss_ce_0: 0.5599  loss_mask_0: 0.3797  loss_dice_0: 3.095  loss_ce_1: 0.3197  loss_mask_1: 0.3871  loss_dice_1: 2.996  loss_ce_2: 0.3157  loss_mask_2: 0.383  loss_dice_2: 2.982  loss_ce_3: 0.2986  loss_mask_3: 0.3819  loss_dice_3: 2.969  loss_ce_4: 0.3013  loss_mask_4: 0.3805  loss_dice_4: 2.975  loss_ce_5: 0.2878  loss_mask_5: 0.3807  loss_dice_5: 2.973  loss_ce_6: 0.2911  loss_mask_6: 0.3822  loss_dice_6: 2.962  loss_ce_7: 0.2767  loss_mask_7: 0.3812  loss_dice_7: 2.963  loss_ce_8: 0.2908  loss_mask_8: 0.3817  loss_dice_8: 2.967  time: 1.4947  data_time: 0.0690  lr: 5.1852e-06  max_mem: 21589M
[01/18 04:22:00] d2.utils.events INFO:  eta: 7:48:16  iter: 20739  total_loss: 35.97  loss_ce: 0.2744  loss_mask: 0.3768  loss_dice: 2.86  loss_ce_0: 0.5602  loss_mask_0: 0.3761  loss_dice_0: 2.999  loss_ce_1: 0.3126  loss_mask_1: 0.3802  loss_dice_1: 2.91  loss_ce_2: 0.3011  loss_mask_2: 0.38  loss_dice_2: 2.885  loss_ce_3: 0.3136  loss_mask_3: 0.378  loss_dice_3: 2.873  loss_ce_4: 0.2854  loss_mask_4: 0.379  loss_dice_4: 2.864  loss_ce_5: 0.2939  loss_mask_5: 0.378  loss_dice_5: 2.864  loss_ce_6: 0.2782  loss_mask_6: 0.3786  loss_dice_6: 2.864  loss_ce_7: 0.2931  loss_mask_7: 0.3773  loss_dice_7: 2.865  loss_ce_8: 0.2931  loss_mask_8: 0.3772  loss_dice_8: 2.859  time: 1.4946  data_time: 0.0729  lr: 5.1803e-06  max_mem: 21589M
[01/18 04:22:28] d2.utils.events INFO:  eta: 7:47:22  iter: 20759  total_loss: 35.82  loss_ce: 0.2772  loss_mask: 0.3741  loss_dice: 2.815  loss_ce_0: 0.5514  loss_mask_0: 0.3677  loss_dice_0: 2.95  loss_ce_1: 0.3093  loss_mask_1: 0.3776  loss_dice_1: 2.858  loss_ce_2: 0.2984  loss_mask_2: 0.3778  loss_dice_2: 2.831  loss_ce_3: 0.3063  loss_mask_3: 0.3752  loss_dice_3: 2.825  loss_ce_4: 0.2882  loss_mask_4: 0.3752  loss_dice_4: 2.822  loss_ce_5: 0.2895  loss_mask_5: 0.3737  loss_dice_5: 2.828  loss_ce_6: 0.2685  loss_mask_6: 0.3743  loss_dice_6: 2.824  loss_ce_7: 0.2799  loss_mask_7: 0.3748  loss_dice_7: 2.82  loss_ce_8: 0.2858  loss_mask_8: 0.3745  loss_dice_8: 2.818  time: 1.4946  data_time: 0.0680  lr: 5.1755e-06  max_mem: 21589M
[01/18 04:22:58] d2.utils.events INFO:  eta: 7:46:39  iter: 20779  total_loss: 36.79  loss_ce: 0.3021  loss_mask: 0.3751  loss_dice: 2.913  loss_ce_0: 0.605  loss_mask_0: 0.3713  loss_dice_0: 3.036  loss_ce_1: 0.3311  loss_mask_1: 0.3812  loss_dice_1: 2.938  loss_ce_2: 0.339  loss_mask_2: 0.3782  loss_dice_2: 2.911  loss_ce_3: 0.3202  loss_mask_3: 0.375  loss_dice_3: 2.907  loss_ce_4: 0.3114  loss_mask_4: 0.3749  loss_dice_4: 2.928  loss_ce_5: 0.3068  loss_mask_5: 0.3732  loss_dice_5: 2.928  loss_ce_6: 0.2952  loss_mask_6: 0.3737  loss_dice_6: 2.92  loss_ce_7: 0.3026  loss_mask_7: 0.3741  loss_dice_7: 2.919  loss_ce_8: 0.3076  loss_mask_8: 0.3746  loss_dice_8: 2.913  time: 1.4945  data_time: 0.0697  lr: 5.1706e-06  max_mem: 21589M
[01/18 04:23:26] d2.utils.events INFO:  eta: 7:45:36  iter: 20799  total_loss: 35.7  loss_ce: 0.3158  loss_mask: 0.3764  loss_dice: 2.826  loss_ce_0: 0.5942  loss_mask_0: 0.3672  loss_dice_0: 2.959  loss_ce_1: 0.3328  loss_mask_1: 0.3768  loss_dice_1: 2.865  loss_ce_2: 0.3224  loss_mask_2: 0.3773  loss_dice_2: 2.839  loss_ce_3: 0.322  loss_mask_3: 0.3751  loss_dice_3: 2.832  loss_ce_4: 0.3132  loss_mask_4: 0.3738  loss_dice_4: 2.832  loss_ce_5: 0.3139  loss_mask_5: 0.3755  loss_dice_5: 2.823  loss_ce_6: 0.3112  loss_mask_6: 0.3752  loss_dice_6: 2.82  loss_ce_7: 0.3061  loss_mask_7: 0.3748  loss_dice_7: 2.827  loss_ce_8: 0.314  loss_mask_8: 0.3769  loss_dice_8: 2.815  time: 1.4945  data_time: 0.0726  lr: 5.1658e-06  max_mem: 21589M
[01/18 04:23:55] d2.utils.events INFO:  eta: 7:44:47  iter: 20819  total_loss: 35.42  loss_ce: 0.295  loss_mask: 0.3827  loss_dice: 2.858  loss_ce_0: 0.5416  loss_mask_0: 0.3745  loss_dice_0: 2.979  loss_ce_1: 0.3211  loss_mask_1: 0.3866  loss_dice_1: 2.879  loss_ce_2: 0.3275  loss_mask_2: 0.3853  loss_dice_2: 2.871  loss_ce_3: 0.2997  loss_mask_3: 0.3837  loss_dice_3: 2.86  loss_ce_4: 0.2954  loss_mask_4: 0.3828  loss_dice_4: 2.863  loss_ce_5: 0.2986  loss_mask_5: 0.384  loss_dice_5: 2.864  loss_ce_6: 0.2891  loss_mask_6: 0.3842  loss_dice_6: 2.855  loss_ce_7: 0.2877  loss_mask_7: 0.3836  loss_dice_7: 2.855  loss_ce_8: 0.2917  loss_mask_8: 0.383  loss_dice_8: 2.861  time: 1.4944  data_time: 0.0668  lr: 5.161e-06  max_mem: 21589M
[01/18 04:24:23] d2.utils.events INFO:  eta: 7:44:09  iter: 20839  total_loss: 36.08  loss_ce: 0.2872  loss_mask: 0.3861  loss_dice: 2.869  loss_ce_0: 0.5579  loss_mask_0: 0.3814  loss_dice_0: 2.973  loss_ce_1: 0.3079  loss_mask_1: 0.3919  loss_dice_1: 2.889  loss_ce_2: 0.3011  loss_mask_2: 0.3895  loss_dice_2: 2.87  loss_ce_3: 0.294  loss_mask_3: 0.3862  loss_dice_3: 2.855  loss_ce_4: 0.2983  loss_mask_4: 0.3862  loss_dice_4: 2.851  loss_ce_5: 0.3021  loss_mask_5: 0.3861  loss_dice_5: 2.859  loss_ce_6: 0.2871  loss_mask_6: 0.3859  loss_dice_6: 2.862  loss_ce_7: 0.2782  loss_mask_7: 0.3862  loss_dice_7: 2.861  loss_ce_8: 0.2816  loss_mask_8: 0.3853  loss_dice_8: 2.859  time: 1.4943  data_time: 0.0687  lr: 5.1561e-06  max_mem: 21589M
[01/18 04:24:52] d2.utils.events INFO:  eta: 7:43:12  iter: 20859  total_loss: 36.06  loss_ce: 0.286  loss_mask: 0.3838  loss_dice: 2.853  loss_ce_0: 0.5706  loss_mask_0: 0.3717  loss_dice_0: 3.001  loss_ce_1: 0.3374  loss_mask_1: 0.3872  loss_dice_1: 2.905  loss_ce_2: 0.3142  loss_mask_2: 0.3855  loss_dice_2: 2.887  loss_ce_3: 0.3149  loss_mask_3: 0.3842  loss_dice_3: 2.86  loss_ce_4: 0.2938  loss_mask_4: 0.383  loss_dice_4: 2.863  loss_ce_5: 0.2936  loss_mask_5: 0.3834  loss_dice_5: 2.867  loss_ce_6: 0.2968  loss_mask_6: 0.3831  loss_dice_6: 2.861  loss_ce_7: 0.2884  loss_mask_7: 0.3821  loss_dice_7: 2.859  loss_ce_8: 0.2814  loss_mask_8: 0.3844  loss_dice_8: 2.858  time: 1.4943  data_time: 0.0699  lr: 5.1513e-06  max_mem: 21589M
[01/18 04:25:21] d2.utils.events INFO:  eta: 7:42:04  iter: 20879  total_loss: 36.13  loss_ce: 0.2942  loss_mask: 0.3895  loss_dice: 2.875  loss_ce_0: 0.5557  loss_mask_0: 0.3871  loss_dice_0: 3  loss_ce_1: 0.3294  loss_mask_1: 0.3932  loss_dice_1: 2.915  loss_ce_2: 0.3244  loss_mask_2: 0.391  loss_dice_2: 2.892  loss_ce_3: 0.3084  loss_mask_3: 0.3878  loss_dice_3: 2.89  loss_ce_4: 0.3007  loss_mask_4: 0.3887  loss_dice_4: 2.881  loss_ce_5: 0.2933  loss_mask_5: 0.3898  loss_dice_5: 2.883  loss_ce_6: 0.2996  loss_mask_6: 0.3892  loss_dice_6: 2.878  loss_ce_7: 0.2923  loss_mask_7: 0.3891  loss_dice_7: 2.883  loss_ce_8: 0.2818  loss_mask_8: 0.3888  loss_dice_8: 2.886  time: 1.4943  data_time: 0.0705  lr: 5.1464e-06  max_mem: 21589M
[01/18 04:25:50] d2.utils.events INFO:  eta: 7:41:35  iter: 20899  total_loss: 36.05  loss_ce: 0.2863  loss_mask: 0.383  loss_dice: 2.87  loss_ce_0: 0.6153  loss_mask_0: 0.3727  loss_dice_0: 2.994  loss_ce_1: 0.3295  loss_mask_1: 0.3864  loss_dice_1: 2.911  loss_ce_2: 0.3213  loss_mask_2: 0.3835  loss_dice_2: 2.873  loss_ce_3: 0.3099  loss_mask_3: 0.3809  loss_dice_3: 2.872  loss_ce_4: 0.3051  loss_mask_4: 0.3804  loss_dice_4: 2.875  loss_ce_5: 0.2759  loss_mask_5: 0.3813  loss_dice_5: 2.867  loss_ce_6: 0.2818  loss_mask_6: 0.3826  loss_dice_6: 2.868  loss_ce_7: 0.2783  loss_mask_7: 0.3829  loss_dice_7: 2.861  loss_ce_8: 0.2792  loss_mask_8: 0.3825  loss_dice_8: 2.862  time: 1.4942  data_time: 0.0675  lr: 5.1416e-06  max_mem: 21589M
[01/18 04:26:20] d2.utils.events INFO:  eta: 7:41:03  iter: 20919  total_loss: 36.08  loss_ce: 0.305  loss_mask: 0.3776  loss_dice: 2.867  loss_ce_0: 0.5468  loss_mask_0: 0.3724  loss_dice_0: 3.004  loss_ce_1: 0.3149  loss_mask_1: 0.3838  loss_dice_1: 2.911  loss_ce_2: 0.3236  loss_mask_2: 0.3811  loss_dice_2: 2.892  loss_ce_3: 0.3184  loss_mask_3: 0.3779  loss_dice_3: 2.875  loss_ce_4: 0.3148  loss_mask_4: 0.3771  loss_dice_4: 2.873  loss_ce_5: 0.3051  loss_mask_5: 0.3772  loss_dice_5: 2.871  loss_ce_6: 0.2921  loss_mask_6: 0.3749  loss_dice_6: 2.881  loss_ce_7: 0.2883  loss_mask_7: 0.3778  loss_dice_7: 2.865  loss_ce_8: 0.2988  loss_mask_8: 0.3763  loss_dice_8: 2.877  time: 1.4942  data_time: 0.0753  lr: 5.1367e-06  max_mem: 21589M
[01/18 04:26:50] d2.utils.events INFO:  eta: 7:40:39  iter: 20939  total_loss: 36.26  loss_ce: 0.296  loss_mask: 0.3782  loss_dice: 2.902  loss_ce_0: 0.5839  loss_mask_0: 0.377  loss_dice_0: 3.027  loss_ce_1: 0.3177  loss_mask_1: 0.3843  loss_dice_1: 2.925  loss_ce_2: 0.3413  loss_mask_2: 0.3828  loss_dice_2: 2.918  loss_ce_3: 0.3124  loss_mask_3: 0.3807  loss_dice_3: 2.911  loss_ce_4: 0.3077  loss_mask_4: 0.3801  loss_dice_4: 2.911  loss_ce_5: 0.2879  loss_mask_5: 0.3807  loss_dice_5: 2.902  loss_ce_6: 0.2991  loss_mask_6: 0.3782  loss_dice_6: 2.907  loss_ce_7: 0.2923  loss_mask_7: 0.3775  loss_dice_7: 2.904  loss_ce_8: 0.2895  loss_mask_8: 0.3773  loss_dice_8: 2.902  time: 1.4942  data_time: 0.0841  lr: 5.1319e-06  max_mem: 21589M
[01/18 04:27:19] d2.utils.events INFO:  eta: 7:40:05  iter: 20959  total_loss: 36.24  loss_ce: 0.2978  loss_mask: 0.3857  loss_dice: 2.881  loss_ce_0: 0.582  loss_mask_0: 0.3811  loss_dice_0: 2.996  loss_ce_1: 0.3287  loss_mask_1: 0.3915  loss_dice_1: 2.909  loss_ce_2: 0.3437  loss_mask_2: 0.3902  loss_dice_2: 2.902  loss_ce_3: 0.324  loss_mask_3: 0.388  loss_dice_3: 2.885  loss_ce_4: 0.3066  loss_mask_4: 0.3869  loss_dice_4: 2.882  loss_ce_5: 0.299  loss_mask_5: 0.3841  loss_dice_5: 2.884  loss_ce_6: 0.3002  loss_mask_6: 0.3842  loss_dice_6: 2.884  loss_ce_7: 0.2941  loss_mask_7: 0.3858  loss_dice_7: 2.885  loss_ce_8: 0.3067  loss_mask_8: 0.3844  loss_dice_8: 2.879  time: 1.4942  data_time: 0.0702  lr: 5.127e-06  max_mem: 21589M
[01/18 04:27:48] d2.utils.events INFO:  eta: 7:39:39  iter: 20979  total_loss: 35.92  loss_ce: 0.2839  loss_mask: 0.3802  loss_dice: 2.838  loss_ce_0: 0.6095  loss_mask_0: 0.3734  loss_dice_0: 2.974  loss_ce_1: 0.3329  loss_mask_1: 0.3847  loss_dice_1: 2.888  loss_ce_2: 0.3407  loss_mask_2: 0.3833  loss_dice_2: 2.857  loss_ce_3: 0.3184  loss_mask_3: 0.381  loss_dice_3: 2.842  loss_ce_4: 0.3208  loss_mask_4: 0.3811  loss_dice_4: 2.844  loss_ce_5: 0.3137  loss_mask_5: 0.3794  loss_dice_5: 2.845  loss_ce_6: 0.2982  loss_mask_6: 0.3812  loss_dice_6: 2.834  loss_ce_7: 0.2819  loss_mask_7: 0.3803  loss_dice_7: 2.847  loss_ce_8: 0.3009  loss_mask_8: 0.3802  loss_dice_8: 2.843  time: 1.4941  data_time: 0.0716  lr: 5.1222e-06  max_mem: 21589M
[01/18 04:28:18] d2.utils.events INFO:  eta: 7:39:10  iter: 20999  total_loss: 35.83  loss_ce: 0.2572  loss_mask: 0.3745  loss_dice: 2.863  loss_ce_0: 0.5489  loss_mask_0: 0.3784  loss_dice_0: 2.98  loss_ce_1: 0.2936  loss_mask_1: 0.3826  loss_dice_1: 2.898  loss_ce_2: 0.2837  loss_mask_2: 0.379  loss_dice_2: 2.879  loss_ce_3: 0.2747  loss_mask_3: 0.3779  loss_dice_3: 2.863  loss_ce_4: 0.2631  loss_mask_4: 0.3736  loss_dice_4: 2.866  loss_ce_5: 0.2569  loss_mask_5: 0.3759  loss_dice_5: 2.865  loss_ce_6: 0.2523  loss_mask_6: 0.3747  loss_dice_6: 2.863  loss_ce_7: 0.2649  loss_mask_7: 0.3742  loss_dice_7: 2.863  loss_ce_8: 0.2483  loss_mask_8: 0.3733  loss_dice_8: 2.864  time: 1.4941  data_time: 0.0749  lr: 5.1173e-06  max_mem: 21589M
[01/18 04:28:49] d2.utils.events INFO:  eta: 7:39:06  iter: 21019  total_loss: 36  loss_ce: 0.3018  loss_mask: 0.3681  loss_dice: 2.852  loss_ce_0: 0.5668  loss_mask_0: 0.3542  loss_dice_0: 3.004  loss_ce_1: 0.3479  loss_mask_1: 0.3667  loss_dice_1: 2.901  loss_ce_2: 0.3358  loss_mask_2: 0.3676  loss_dice_2: 2.876  loss_ce_3: 0.3174  loss_mask_3: 0.3667  loss_dice_3: 2.859  loss_ce_4: 0.3076  loss_mask_4: 0.3649  loss_dice_4: 2.863  loss_ce_5: 0.2973  loss_mask_5: 0.3658  loss_dice_5: 2.855  loss_ce_6: 0.3143  loss_mask_6: 0.3669  loss_dice_6: 2.856  loss_ce_7: 0.2964  loss_mask_7: 0.3662  loss_dice_7: 2.858  loss_ce_8: 0.304  loss_mask_8: 0.3672  loss_dice_8: 2.854  time: 1.4941  data_time: 0.0749  lr: 5.1125e-06  max_mem: 21589M
[01/18 04:29:18] d2.utils.events INFO:  eta: 7:39:01  iter: 21039  total_loss: 35.85  loss_ce: 0.2592  loss_mask: 0.3784  loss_dice: 2.874  loss_ce_0: 0.556  loss_mask_0: 0.3781  loss_dice_0: 3.001  loss_ce_1: 0.3009  loss_mask_1: 0.3898  loss_dice_1: 2.918  loss_ce_2: 0.2837  loss_mask_2: 0.3858  loss_dice_2: 2.887  loss_ce_3: 0.2705  loss_mask_3: 0.3822  loss_dice_3: 2.888  loss_ce_4: 0.2722  loss_mask_4: 0.3827  loss_dice_4: 2.877  loss_ce_5: 0.2684  loss_mask_5: 0.3827  loss_dice_5: 2.874  loss_ce_6: 0.2764  loss_mask_6: 0.3819  loss_dice_6: 2.871  loss_ce_7: 0.2614  loss_mask_7: 0.3818  loss_dice_7: 2.879  loss_ce_8: 0.2575  loss_mask_8: 0.3817  loss_dice_8: 2.879  time: 1.4941  data_time: 0.0752  lr: 5.1076e-06  max_mem: 21589M
[01/18 04:29:47] d2.utils.events INFO:  eta: 7:38:21  iter: 21059  total_loss: 36.45  loss_ce: 0.2725  loss_mask: 0.3856  loss_dice: 2.916  loss_ce_0: 0.5413  loss_mask_0: 0.3851  loss_dice_0: 3.038  loss_ce_1: 0.3034  loss_mask_1: 0.3918  loss_dice_1: 2.951  loss_ce_2: 0.3007  loss_mask_2: 0.3874  loss_dice_2: 2.933  loss_ce_3: 0.2767  loss_mask_3: 0.3862  loss_dice_3: 2.92  loss_ce_4: 0.277  loss_mask_4: 0.3845  loss_dice_4: 2.92  loss_ce_5: 0.2882  loss_mask_5: 0.3845  loss_dice_5: 2.917  loss_ce_6: 0.2876  loss_mask_6: 0.381  loss_dice_6: 2.92  loss_ce_7: 0.2911  loss_mask_7: 0.3821  loss_dice_7: 2.915  loss_ce_8: 0.2781  loss_mask_8: 0.3849  loss_dice_8: 2.911  time: 1.4941  data_time: 0.0684  lr: 5.1028e-06  max_mem: 21589M
[01/18 04:30:16] d2.utils.events INFO:  eta: 7:37:39  iter: 21079  total_loss: 35.65  loss_ce: 0.3012  loss_mask: 0.3828  loss_dice: 2.83  loss_ce_0: 0.5539  loss_mask_0: 0.3689  loss_dice_0: 2.982  loss_ce_1: 0.3271  loss_mask_1: 0.3817  loss_dice_1: 2.878  loss_ce_2: 0.3471  loss_mask_2: 0.3826  loss_dice_2: 2.861  loss_ce_3: 0.3289  loss_mask_3: 0.382  loss_dice_3: 2.842  loss_ce_4: 0.3174  loss_mask_4: 0.3819  loss_dice_4: 2.844  loss_ce_5: 0.3039  loss_mask_5: 0.3824  loss_dice_5: 2.839  loss_ce_6: 0.3066  loss_mask_6: 0.3816  loss_dice_6: 2.836  loss_ce_7: 0.2913  loss_mask_7: 0.3822  loss_dice_7: 2.837  loss_ce_8: 0.2926  loss_mask_8: 0.3824  loss_dice_8: 2.833  time: 1.4940  data_time: 0.0668  lr: 5.098e-06  max_mem: 21589M
[01/18 04:30:45] d2.utils.events INFO:  eta: 7:36:40  iter: 21099  total_loss: 35.18  loss_ce: 0.27  loss_mask: 0.366  loss_dice: 2.805  loss_ce_0: 0.5821  loss_mask_0: 0.3605  loss_dice_0: 2.953  loss_ce_1: 0.3176  loss_mask_1: 0.3691  loss_dice_1: 2.858  loss_ce_2: 0.3072  loss_mask_2: 0.3666  loss_dice_2: 2.823  loss_ce_3: 0.2921  loss_mask_3: 0.3654  loss_dice_3: 2.811  loss_ce_4: 0.2854  loss_mask_4: 0.3663  loss_dice_4: 2.811  loss_ce_5: 0.2733  loss_mask_5: 0.3629  loss_dice_5: 2.808  loss_ce_6: 0.2748  loss_mask_6: 0.365  loss_dice_6: 2.809  loss_ce_7: 0.2744  loss_mask_7: 0.3646  loss_dice_7: 2.802  loss_ce_8: 0.2622  loss_mask_8: 0.3654  loss_dice_8: 2.811  time: 1.4940  data_time: 0.0689  lr: 5.0931e-06  max_mem: 21589M
[01/18 04:31:14] d2.utils.events INFO:  eta: 7:36:11  iter: 21119  total_loss: 35.59  loss_ce: 0.2529  loss_mask: 0.3833  loss_dice: 2.852  loss_ce_0: 0.5519  loss_mask_0: 0.3758  loss_dice_0: 2.992  loss_ce_1: 0.2812  loss_mask_1: 0.3908  loss_dice_1: 2.886  loss_ce_2: 0.2946  loss_mask_2: 0.3839  loss_dice_2: 2.864  loss_ce_3: 0.2881  loss_mask_3: 0.3821  loss_dice_3: 2.853  loss_ce_4: 0.2735  loss_mask_4: 0.3826  loss_dice_4: 2.849  loss_ce_5: 0.2616  loss_mask_5: 0.3821  loss_dice_5: 2.852  loss_ce_6: 0.2716  loss_mask_6: 0.3837  loss_dice_6: 2.857  loss_ce_7: 0.2469  loss_mask_7: 0.3814  loss_dice_7: 2.855  loss_ce_8: 0.2462  loss_mask_8: 0.3839  loss_dice_8: 2.845  time: 1.4939  data_time: 0.0669  lr: 5.0883e-06  max_mem: 21589M
[01/18 04:31:43] d2.utils.events INFO:  eta: 7:35:42  iter: 21139  total_loss: 34.95  loss_ce: 0.2886  loss_mask: 0.3763  loss_dice: 2.779  loss_ce_0: 0.581  loss_mask_0: 0.3689  loss_dice_0: 2.909  loss_ce_1: 0.3398  loss_mask_1: 0.381  loss_dice_1: 2.827  loss_ce_2: 0.3217  loss_mask_2: 0.3782  loss_dice_2: 2.803  loss_ce_3: 0.2947  loss_mask_3: 0.3773  loss_dice_3: 2.789  loss_ce_4: 0.2914  loss_mask_4: 0.3759  loss_dice_4: 2.783  loss_ce_5: 0.2934  loss_mask_5: 0.3744  loss_dice_5: 2.781  loss_ce_6: 0.283  loss_mask_6: 0.376  loss_dice_6: 2.779  loss_ce_7: 0.2955  loss_mask_7: 0.3766  loss_dice_7: 2.78  loss_ce_8: 0.2649  loss_mask_8: 0.3785  loss_dice_8: 2.783  time: 1.4939  data_time: 0.0672  lr: 5.0834e-06  max_mem: 21589M
[01/18 04:32:12] d2.utils.events INFO:  eta: 7:35:20  iter: 21159  total_loss: 36.33  loss_ce: 0.2803  loss_mask: 0.3689  loss_dice: 2.919  loss_ce_0: 0.562  loss_mask_0: 0.3624  loss_dice_0: 3.05  loss_ce_1: 0.3332  loss_mask_1: 0.3737  loss_dice_1: 2.956  loss_ce_2: 0.3142  loss_mask_2: 0.3722  loss_dice_2: 2.935  loss_ce_3: 0.3109  loss_mask_3: 0.3714  loss_dice_3: 2.924  loss_ce_4: 0.3144  loss_mask_4: 0.372  loss_dice_4: 2.924  loss_ce_5: 0.3165  loss_mask_5: 0.37  loss_dice_5: 2.922  loss_ce_6: 0.2968  loss_mask_6: 0.3691  loss_dice_6: 2.915  loss_ce_7: 0.2878  loss_mask_7: 0.37  loss_dice_7: 2.923  loss_ce_8: 0.2957  loss_mask_8: 0.3683  loss_dice_8: 2.921  time: 1.4939  data_time: 0.0721  lr: 5.0785e-06  max_mem: 21589M
[01/18 04:32:42] d2.utils.events INFO:  eta: 7:35:27  iter: 21179  total_loss: 37.03  loss_ce: 0.2963  loss_mask: 0.3741  loss_dice: 2.931  loss_ce_0: 0.5932  loss_mask_0: 0.3721  loss_dice_0: 3.039  loss_ce_1: 0.3501  loss_mask_1: 0.3805  loss_dice_1: 2.954  loss_ce_2: 0.3217  loss_mask_2: 0.3782  loss_dice_2: 2.936  loss_ce_3: 0.3102  loss_mask_3: 0.3753  loss_dice_3: 2.924  loss_ce_4: 0.2999  loss_mask_4: 0.376  loss_dice_4: 2.93  loss_ce_5: 0.2996  loss_mask_5: 0.3748  loss_dice_5: 2.928  loss_ce_6: 0.3066  loss_mask_6: 0.3757  loss_dice_6: 2.917  loss_ce_7: 0.2833  loss_mask_7: 0.3742  loss_dice_7: 2.926  loss_ce_8: 0.2832  loss_mask_8: 0.3741  loss_dice_8: 2.925  time: 1.4939  data_time: 0.0744  lr: 5.0737e-06  max_mem: 21589M
[01/18 04:33:12] d2.utils.events INFO:  eta: 7:35:28  iter: 21199  total_loss: 36.21  loss_ce: 0.2825  loss_mask: 0.3765  loss_dice: 2.937  loss_ce_0: 0.5836  loss_mask_0: 0.3701  loss_dice_0: 3.088  loss_ce_1: 0.333  loss_mask_1: 0.3805  loss_dice_1: 2.99  loss_ce_2: 0.3111  loss_mask_2: 0.3763  loss_dice_2: 2.964  loss_ce_3: 0.2817  loss_mask_3: 0.376  loss_dice_3: 2.95  loss_ce_4: 0.3064  loss_mask_4: 0.3765  loss_dice_4: 2.94  loss_ce_5: 0.2882  loss_mask_5: 0.378  loss_dice_5: 2.953  loss_ce_6: 0.2923  loss_mask_6: 0.3766  loss_dice_6: 2.936  loss_ce_7: 0.2875  loss_mask_7: 0.376  loss_dice_7: 2.941  loss_ce_8: 0.2805  loss_mask_8: 0.3772  loss_dice_8: 2.941  time: 1.4938  data_time: 0.0758  lr: 5.0688e-06  max_mem: 21589M
[01/18 04:33:41] d2.utils.events INFO:  eta: 7:35:04  iter: 21219  total_loss: 35.69  loss_ce: 0.2874  loss_mask: 0.381  loss_dice: 2.832  loss_ce_0: 0.5574  loss_mask_0: 0.3747  loss_dice_0: 2.963  loss_ce_1: 0.3128  loss_mask_1: 0.3879  loss_dice_1: 2.875  loss_ce_2: 0.2919  loss_mask_2: 0.3889  loss_dice_2: 2.846  loss_ce_3: 0.2864  loss_mask_3: 0.388  loss_dice_3: 2.829  loss_ce_4: 0.281  loss_mask_4: 0.3843  loss_dice_4: 2.833  loss_ce_5: 0.2868  loss_mask_5: 0.3821  loss_dice_5: 2.832  loss_ce_6: 0.2824  loss_mask_6: 0.3831  loss_dice_6: 2.838  loss_ce_7: 0.284  loss_mask_7: 0.3822  loss_dice_7: 2.831  loss_ce_8: 0.291  loss_mask_8: 0.3816  loss_dice_8: 2.83  time: 1.4938  data_time: 0.0716  lr: 5.064e-06  max_mem: 21589M
[01/18 04:34:11] d2.utils.events INFO:  eta: 7:34:56  iter: 21239  total_loss: 36.49  loss_ce: 0.2738  loss_mask: 0.3738  loss_dice: 2.917  loss_ce_0: 0.5688  loss_mask_0: 0.3696  loss_dice_0: 3.035  loss_ce_1: 0.3224  loss_mask_1: 0.3789  loss_dice_1: 2.95  loss_ce_2: 0.3209  loss_mask_2: 0.3765  loss_dice_2: 2.933  loss_ce_3: 0.2954  loss_mask_3: 0.3743  loss_dice_3: 2.918  loss_ce_4: 0.2847  loss_mask_4: 0.3732  loss_dice_4: 2.916  loss_ce_5: 0.2784  loss_mask_5: 0.3737  loss_dice_5: 2.925  loss_ce_6: 0.2791  loss_mask_6: 0.374  loss_dice_6: 2.911  loss_ce_7: 0.2745  loss_mask_7: 0.3756  loss_dice_7: 2.913  loss_ce_8: 0.2882  loss_mask_8: 0.3749  loss_dice_8: 2.916  time: 1.4938  data_time: 0.0690  lr: 5.0591e-06  max_mem: 21589M
[01/18 04:34:39] d2.utils.events INFO:  eta: 7:34:19  iter: 21259  total_loss: 36.35  loss_ce: 0.2822  loss_mask: 0.3824  loss_dice: 2.89  loss_ce_0: 0.5991  loss_mask_0: 0.3758  loss_dice_0: 3.011  loss_ce_1: 0.3242  loss_mask_1: 0.3901  loss_dice_1: 2.924  loss_ce_2: 0.3242  loss_mask_2: 0.3867  loss_dice_2: 2.907  loss_ce_3: 0.3097  loss_mask_3: 0.3835  loss_dice_3: 2.893  loss_ce_4: 0.3119  loss_mask_4: 0.3827  loss_dice_4: 2.903  loss_ce_5: 0.2922  loss_mask_5: 0.3816  loss_dice_5: 2.892  loss_ce_6: 0.2878  loss_mask_6: 0.3827  loss_dice_6: 2.899  loss_ce_7: 0.289  loss_mask_7: 0.381  loss_dice_7: 2.899  loss_ce_8: 0.2836  loss_mask_8: 0.3828  loss_dice_8: 2.892  time: 1.4937  data_time: 0.0692  lr: 5.0543e-06  max_mem: 21589M
[01/18 04:35:08] d2.utils.events INFO:  eta: 7:33:43  iter: 21279  total_loss: 36.61  loss_ce: 0.3015  loss_mask: 0.3854  loss_dice: 2.854  loss_ce_0: 0.5783  loss_mask_0: 0.381  loss_dice_0: 2.982  loss_ce_1: 0.3253  loss_mask_1: 0.3852  loss_dice_1: 2.888  loss_ce_2: 0.3326  loss_mask_2: 0.3866  loss_dice_2: 2.864  loss_ce_3: 0.3152  loss_mask_3: 0.3839  loss_dice_3: 2.859  loss_ce_4: 0.3134  loss_mask_4: 0.3841  loss_dice_4: 2.859  loss_ce_5: 0.2894  loss_mask_5: 0.3841  loss_dice_5: 2.865  loss_ce_6: 0.2961  loss_mask_6: 0.3836  loss_dice_6: 2.85  loss_ce_7: 0.2976  loss_mask_7: 0.384  loss_dice_7: 2.846  loss_ce_8: 0.2845  loss_mask_8: 0.3837  loss_dice_8: 2.856  time: 1.4937  data_time: 0.0672  lr: 5.0494e-06  max_mem: 21589M
[01/18 04:35:37] d2.utils.events INFO:  eta: 7:33:14  iter: 21299  total_loss: 35.45  loss_ce: 0.2668  loss_mask: 0.3693  loss_dice: 2.855  loss_ce_0: 0.5366  loss_mask_0: 0.3694  loss_dice_0: 2.989  loss_ce_1: 0.2909  loss_mask_1: 0.3771  loss_dice_1: 2.907  loss_ce_2: 0.2913  loss_mask_2: 0.3727  loss_dice_2: 2.878  loss_ce_3: 0.2837  loss_mask_3: 0.3698  loss_dice_3: 2.862  loss_ce_4: 0.2748  loss_mask_4: 0.3695  loss_dice_4: 2.86  loss_ce_5: 0.2658  loss_mask_5: 0.3699  loss_dice_5: 2.868  loss_ce_6: 0.2705  loss_mask_6: 0.3698  loss_dice_6: 2.859  loss_ce_7: 0.2689  loss_mask_7: 0.3703  loss_dice_7: 2.865  loss_ce_8: 0.2598  loss_mask_8: 0.3698  loss_dice_8: 2.863  time: 1.4936  data_time: 0.0672  lr: 5.0446e-06  max_mem: 21589M
[01/18 04:36:06] d2.utils.events INFO:  eta: 7:32:45  iter: 21319  total_loss: 35.27  loss_ce: 0.2622  loss_mask: 0.3611  loss_dice: 2.813  loss_ce_0: 0.5597  loss_mask_0: 0.3647  loss_dice_0: 2.949  loss_ce_1: 0.3263  loss_mask_1: 0.366  loss_dice_1: 2.854  loss_ce_2: 0.3039  loss_mask_2: 0.3629  loss_dice_2: 2.836  loss_ce_3: 0.2985  loss_mask_3: 0.3625  loss_dice_3: 2.829  loss_ce_4: 0.2786  loss_mask_4: 0.3607  loss_dice_4: 2.824  loss_ce_5: 0.2912  loss_mask_5: 0.3584  loss_dice_5: 2.827  loss_ce_6: 0.2786  loss_mask_6: 0.359  loss_dice_6: 2.828  loss_ce_7: 0.2767  loss_mask_7: 0.3608  loss_dice_7: 2.823  loss_ce_8: 0.2751  loss_mask_8: 0.3606  loss_dice_8: 2.83  time: 1.4936  data_time: 0.0695  lr: 5.0397e-06  max_mem: 21589M
[01/18 04:36:35] d2.utils.events INFO:  eta: 7:32:11  iter: 21339  total_loss: 35.79  loss_ce: 0.2715  loss_mask: 0.365  loss_dice: 2.864  loss_ce_0: 0.5886  loss_mask_0: 0.3651  loss_dice_0: 3.008  loss_ce_1: 0.3079  loss_mask_1: 0.3703  loss_dice_1: 2.91  loss_ce_2: 0.2904  loss_mask_2: 0.3668  loss_dice_2: 2.881  loss_ce_3: 0.3028  loss_mask_3: 0.3669  loss_dice_3: 2.867  loss_ce_4: 0.2902  loss_mask_4: 0.3673  loss_dice_4: 2.865  loss_ce_5: 0.2775  loss_mask_5: 0.3667  loss_dice_5: 2.873  loss_ce_6: 0.2729  loss_mask_6: 0.3677  loss_dice_6: 2.866  loss_ce_7: 0.286  loss_mask_7: 0.367  loss_dice_7: 2.87  loss_ce_8: 0.2713  loss_mask_8: 0.3674  loss_dice_8: 2.86  time: 1.4936  data_time: 0.0694  lr: 5.0349e-06  max_mem: 21589M
[01/18 04:37:04] d2.utils.events INFO:  eta: 7:31:40  iter: 21359  total_loss: 35.45  loss_ce: 0.283  loss_mask: 0.3818  loss_dice: 2.799  loss_ce_0: 0.55  loss_mask_0: 0.3763  loss_dice_0: 2.94  loss_ce_1: 0.3091  loss_mask_1: 0.3873  loss_dice_1: 2.851  loss_ce_2: 0.3081  loss_mask_2: 0.386  loss_dice_2: 2.822  loss_ce_3: 0.2826  loss_mask_3: 0.3829  loss_dice_3: 2.805  loss_ce_4: 0.2891  loss_mask_4: 0.3823  loss_dice_4: 2.813  loss_ce_5: 0.2902  loss_mask_5: 0.383  loss_dice_5: 2.811  loss_ce_6: 0.2745  loss_mask_6: 0.3817  loss_dice_6: 2.808  loss_ce_7: 0.2711  loss_mask_7: 0.3814  loss_dice_7: 2.802  loss_ce_8: 0.2644  loss_mask_8: 0.3806  loss_dice_8: 2.798  time: 1.4935  data_time: 0.0691  lr: 5.03e-06  max_mem: 21589M
[01/18 04:37:33] d2.utils.events INFO:  eta: 7:31:09  iter: 21379  total_loss: 35.31  loss_ce: 0.2878  loss_mask: 0.3741  loss_dice: 2.823  loss_ce_0: 0.5771  loss_mask_0: 0.373  loss_dice_0: 2.962  loss_ce_1: 0.308  loss_mask_1: 0.3808  loss_dice_1: 2.866  loss_ce_2: 0.2992  loss_mask_2: 0.3767  loss_dice_2: 2.86  loss_ce_3: 0.2846  loss_mask_3: 0.3744  loss_dice_3: 2.838  loss_ce_4: 0.2882  loss_mask_4: 0.3736  loss_dice_4: 2.834  loss_ce_5: 0.2755  loss_mask_5: 0.3726  loss_dice_5: 2.829  loss_ce_6: 0.2772  loss_mask_6: 0.3718  loss_dice_6: 2.839  loss_ce_7: 0.2656  loss_mask_7: 0.3732  loss_dice_7: 2.835  loss_ce_8: 0.2763  loss_mask_8: 0.3731  loss_dice_8: 2.834  time: 1.4935  data_time: 0.0677  lr: 5.0251e-06  max_mem: 21589M
[01/18 04:38:02] d2.utils.events INFO:  eta: 7:30:54  iter: 21399  total_loss: 36.42  loss_ce: 0.2766  loss_mask: 0.3747  loss_dice: 2.937  loss_ce_0: 0.5545  loss_mask_0: 0.3771  loss_dice_0: 3.068  loss_ce_1: 0.2962  loss_mask_1: 0.3841  loss_dice_1: 2.991  loss_ce_2: 0.2867  loss_mask_2: 0.3766  loss_dice_2: 2.964  loss_ce_3: 0.284  loss_mask_3: 0.3775  loss_dice_3: 2.949  loss_ce_4: 0.2806  loss_mask_4: 0.377  loss_dice_4: 2.944  loss_ce_5: 0.2731  loss_mask_5: 0.3766  loss_dice_5: 2.941  loss_ce_6: 0.2773  loss_mask_6: 0.376  loss_dice_6: 2.94  loss_ce_7: 0.2655  loss_mask_7: 0.3751  loss_dice_7: 2.941  loss_ce_8: 0.2652  loss_mask_8: 0.3736  loss_dice_8: 2.941  time: 1.4934  data_time: 0.0696  lr: 5.0203e-06  max_mem: 21589M
[01/18 04:38:31] d2.utils.events INFO:  eta: 7:30:08  iter: 21419  total_loss: 35.49  loss_ce: 0.253  loss_mask: 0.3739  loss_dice: 2.86  loss_ce_0: 0.5676  loss_mask_0: 0.3702  loss_dice_0: 2.985  loss_ce_1: 0.2952  loss_mask_1: 0.3793  loss_dice_1: 2.906  loss_ce_2: 0.2914  loss_mask_2: 0.3755  loss_dice_2: 2.881  loss_ce_3: 0.267  loss_mask_3: 0.374  loss_dice_3: 2.872  loss_ce_4: 0.2689  loss_mask_4: 0.3754  loss_dice_4: 2.881  loss_ce_5: 0.2596  loss_mask_5: 0.3745  loss_dice_5: 2.875  loss_ce_6: 0.2544  loss_mask_6: 0.3759  loss_dice_6: 2.862  loss_ce_7: 0.2469  loss_mask_7: 0.3754  loss_dice_7: 2.872  loss_ce_8: 0.2468  loss_mask_8: 0.3743  loss_dice_8: 2.869  time: 1.4934  data_time: 0.0728  lr: 5.0154e-06  max_mem: 21589M
[01/18 04:39:00] d2.utils.events INFO:  eta: 7:29:28  iter: 21439  total_loss: 35.01  loss_ce: 0.2706  loss_mask: 0.3838  loss_dice: 2.771  loss_ce_0: 0.5364  loss_mask_0: 0.37  loss_dice_0: 2.912  loss_ce_1: 0.3094  loss_mask_1: 0.3866  loss_dice_1: 2.81  loss_ce_2: 0.2982  loss_mask_2: 0.3832  loss_dice_2: 2.791  loss_ce_3: 0.2785  loss_mask_3: 0.3858  loss_dice_3: 2.771  loss_ce_4: 0.2773  loss_mask_4: 0.3841  loss_dice_4: 2.769  loss_ce_5: 0.2684  loss_mask_5: 0.381  loss_dice_5: 2.773  loss_ce_6: 0.2727  loss_mask_6: 0.3821  loss_dice_6: 2.78  loss_ce_7: 0.2618  loss_mask_7: 0.3824  loss_dice_7: 2.78  loss_ce_8: 0.2685  loss_mask_8: 0.3819  loss_dice_8: 2.773  time: 1.4934  data_time: 0.0700  lr: 5.0106e-06  max_mem: 21589M
[01/18 04:39:30] d2.utils.events INFO:  eta: 7:28:50  iter: 21459  total_loss: 35.76  loss_ce: 0.2936  loss_mask: 0.3746  loss_dice: 2.828  loss_ce_0: 0.565  loss_mask_0: 0.3653  loss_dice_0: 2.972  loss_ce_1: 0.3288  loss_mask_1: 0.3735  loss_dice_1: 2.884  loss_ce_2: 0.3235  loss_mask_2: 0.3729  loss_dice_2: 2.847  loss_ce_3: 0.3142  loss_mask_3: 0.3715  loss_dice_3: 2.833  loss_ce_4: 0.2929  loss_mask_4: 0.3717  loss_dice_4: 2.837  loss_ce_5: 0.2832  loss_mask_5: 0.3725  loss_dice_5: 2.834  loss_ce_6: 0.2851  loss_mask_6: 0.3747  loss_dice_6: 2.832  loss_ce_7: 0.2732  loss_mask_7: 0.3748  loss_dice_7: 2.834  loss_ce_8: 0.2823  loss_mask_8: 0.3736  loss_dice_8: 2.834  time: 1.4933  data_time: 0.0719  lr: 5.0057e-06  max_mem: 21589M
[01/18 04:39:58] d2.utils.events INFO:  eta: 7:28:12  iter: 21479  total_loss: 35.74  loss_ce: 0.2582  loss_mask: 0.3755  loss_dice: 2.831  loss_ce_0: 0.5681  loss_mask_0: 0.377  loss_dice_0: 2.969  loss_ce_1: 0.3154  loss_mask_1: 0.3851  loss_dice_1: 2.872  loss_ce_2: 0.323  loss_mask_2: 0.3805  loss_dice_2: 2.868  loss_ce_3: 0.2883  loss_mask_3: 0.3802  loss_dice_3: 2.833  loss_ce_4: 0.2812  loss_mask_4: 0.3804  loss_dice_4: 2.835  loss_ce_5: 0.2714  loss_mask_5: 0.3797  loss_dice_5: 2.849  loss_ce_6: 0.2739  loss_mask_6: 0.379  loss_dice_6: 2.829  loss_ce_7: 0.2595  loss_mask_7: 0.3776  loss_dice_7: 2.835  loss_ce_8: 0.2629  loss_mask_8: 0.3759  loss_dice_8: 2.832  time: 1.4933  data_time: 0.0694  lr: 5.0009e-06  max_mem: 21589M
[01/18 04:40:27] d2.utils.events INFO:  eta: 7:27:24  iter: 21499  total_loss: 35.49  loss_ce: 0.2847  loss_mask: 0.3736  loss_dice: 2.841  loss_ce_0: 0.5834  loss_mask_0: 0.3731  loss_dice_0: 2.957  loss_ce_1: 0.3126  loss_mask_1: 0.381  loss_dice_1: 2.872  loss_ce_2: 0.3233  loss_mask_2: 0.3753  loss_dice_2: 2.857  loss_ce_3: 0.3073  loss_mask_3: 0.3746  loss_dice_3: 2.834  loss_ce_4: 0.2974  loss_mask_4: 0.3751  loss_dice_4: 2.838  loss_ce_5: 0.2854  loss_mask_5: 0.3745  loss_dice_5: 2.836  loss_ce_6: 0.281  loss_mask_6: 0.3733  loss_dice_6: 2.834  loss_ce_7: 0.2791  loss_mask_7: 0.3735  loss_dice_7: 2.829  loss_ce_8: 0.2904  loss_mask_8: 0.3737  loss_dice_8: 2.841  time: 1.4932  data_time: 0.0667  lr: 4.996e-06  max_mem: 21589M
[01/18 04:40:56] d2.utils.events INFO:  eta: 7:27:11  iter: 21519  total_loss: 35.94  loss_ce: 0.2978  loss_mask: 0.3642  loss_dice: 2.861  loss_ce_0: 0.5844  loss_mask_0: 0.3599  loss_dice_0: 2.998  loss_ce_1: 0.3416  loss_mask_1: 0.3668  loss_dice_1: 2.9  loss_ce_2: 0.3324  loss_mask_2: 0.3655  loss_dice_2: 2.871  loss_ce_3: 0.3196  loss_mask_3: 0.3635  loss_dice_3: 2.859  loss_ce_4: 0.3183  loss_mask_4: 0.3624  loss_dice_4: 2.859  loss_ce_5: 0.3118  loss_mask_5: 0.3623  loss_dice_5: 2.853  loss_ce_6: 0.3142  loss_mask_6: 0.3617  loss_dice_6: 2.854  loss_ce_7: 0.2909  loss_mask_7: 0.3634  loss_dice_7: 2.854  loss_ce_8: 0.2999  loss_mask_8: 0.3639  loss_dice_8: 2.859  time: 1.4932  data_time: 0.0659  lr: 4.9911e-06  max_mem: 21589M
[01/18 04:41:25] d2.utils.events INFO:  eta: 7:26:44  iter: 21539  total_loss: 36.11  loss_ce: 0.2715  loss_mask: 0.3827  loss_dice: 2.848  loss_ce_0: 0.6061  loss_mask_0: 0.3747  loss_dice_0: 2.985  loss_ce_1: 0.3277  loss_mask_1: 0.3919  loss_dice_1: 2.886  loss_ce_2: 0.3078  loss_mask_2: 0.3861  loss_dice_2: 2.865  loss_ce_3: 0.2906  loss_mask_3: 0.3795  loss_dice_3: 2.859  loss_ce_4: 0.2942  loss_mask_4: 0.3818  loss_dice_4: 2.854  loss_ce_5: 0.2994  loss_mask_5: 0.3787  loss_dice_5: 2.857  loss_ce_6: 0.2962  loss_mask_6: 0.3796  loss_dice_6: 2.848  loss_ce_7: 0.2811  loss_mask_7: 0.38  loss_dice_7: 2.849  loss_ce_8: 0.2824  loss_mask_8: 0.3812  loss_dice_8: 2.847  time: 1.4931  data_time: 0.0721  lr: 4.9863e-06  max_mem: 21589M
[01/18 04:41:55] d2.utils.events INFO:  eta: 7:26:47  iter: 21559  total_loss: 36.18  loss_ce: 0.2775  loss_mask: 0.3673  loss_dice: 2.888  loss_ce_0: 0.5616  loss_mask_0: 0.3642  loss_dice_0: 3.022  loss_ce_1: 0.3254  loss_mask_1: 0.3751  loss_dice_1: 2.914  loss_ce_2: 0.3176  loss_mask_2: 0.3707  loss_dice_2: 2.906  loss_ce_3: 0.3043  loss_mask_3: 0.3679  loss_dice_3: 2.901  loss_ce_4: 0.2895  loss_mask_4: 0.3687  loss_dice_4: 2.889  loss_ce_5: 0.2921  loss_mask_5: 0.3683  loss_dice_5: 2.892  loss_ce_6: 0.2819  loss_mask_6: 0.368  loss_dice_6: 2.886  loss_ce_7: 0.286  loss_mask_7: 0.3685  loss_dice_7: 2.898  loss_ce_8: 0.2814  loss_mask_8: 0.3677  loss_dice_8: 2.896  time: 1.4931  data_time: 0.0733  lr: 4.9814e-06  max_mem: 21589M
[01/18 04:42:24] d2.utils.events INFO:  eta: 7:26:32  iter: 21579  total_loss: 35.58  loss_ce: 0.2632  loss_mask: 0.3664  loss_dice: 2.834  loss_ce_0: 0.572  loss_mask_0: 0.3651  loss_dice_0: 2.977  loss_ce_1: 0.2909  loss_mask_1: 0.3661  loss_dice_1: 2.89  loss_ce_2: 0.2923  loss_mask_2: 0.3654  loss_dice_2: 2.856  loss_ce_3: 0.2808  loss_mask_3: 0.364  loss_dice_3: 2.849  loss_ce_4: 0.2568  loss_mask_4: 0.3654  loss_dice_4: 2.833  loss_ce_5: 0.2681  loss_mask_5: 0.365  loss_dice_5: 2.846  loss_ce_6: 0.2564  loss_mask_6: 0.3666  loss_dice_6: 2.824  loss_ce_7: 0.2674  loss_mask_7: 0.366  loss_dice_7: 2.836  loss_ce_8: 0.2648  loss_mask_8: 0.3684  loss_dice_8: 2.825  time: 1.4931  data_time: 0.0691  lr: 4.9765e-06  max_mem: 21589M
[01/18 04:42:53] d2.utils.events INFO:  eta: 7:25:21  iter: 21599  total_loss: 35.51  loss_ce: 0.2756  loss_mask: 0.3873  loss_dice: 2.827  loss_ce_0: 0.5332  loss_mask_0: 0.3793  loss_dice_0: 2.95  loss_ce_1: 0.2994  loss_mask_1: 0.3893  loss_dice_1: 2.873  loss_ce_2: 0.2999  loss_mask_2: 0.3867  loss_dice_2: 2.849  loss_ce_3: 0.2766  loss_mask_3: 0.3851  loss_dice_3: 2.84  loss_ce_4: 0.2601  loss_mask_4: 0.3863  loss_dice_4: 2.841  loss_ce_5: 0.2765  loss_mask_5: 0.3848  loss_dice_5: 2.836  loss_ce_6: 0.2748  loss_mask_6: 0.3876  loss_dice_6: 2.826  loss_ce_7: 0.2585  loss_mask_7: 0.3854  loss_dice_7: 2.83  loss_ce_8: 0.2571  loss_mask_8: 0.3864  loss_dice_8: 2.832  time: 1.4930  data_time: 0.0660  lr: 4.9717e-06  max_mem: 21589M
[01/18 04:43:22] d2.utils.events INFO:  eta: 7:24:48  iter: 21619  total_loss: 35.62  loss_ce: 0.2759  loss_mask: 0.3847  loss_dice: 2.814  loss_ce_0: 0.5817  loss_mask_0: 0.3842  loss_dice_0: 2.94  loss_ce_1: 0.3135  loss_mask_1: 0.3942  loss_dice_1: 2.852  loss_ce_2: 0.304  loss_mask_2: 0.3896  loss_dice_2: 2.831  loss_ce_3: 0.2945  loss_mask_3: 0.3898  loss_dice_3: 2.819  loss_ce_4: 0.2988  loss_mask_4: 0.3876  loss_dice_4: 2.816  loss_ce_5: 0.2848  loss_mask_5: 0.3877  loss_dice_5: 2.808  loss_ce_6: 0.2852  loss_mask_6: 0.3884  loss_dice_6: 2.809  loss_ce_7: 0.2877  loss_mask_7: 0.3871  loss_dice_7: 2.817  loss_ce_8: 0.2843  loss_mask_8: 0.3861  loss_dice_8: 2.819  time: 1.4930  data_time: 0.0675  lr: 4.9668e-06  max_mem: 21589M
[01/18 04:43:51] d2.utils.events INFO:  eta: 7:24:17  iter: 21639  total_loss: 35.4  loss_ce: 0.2925  loss_mask: 0.3732  loss_dice: 2.794  loss_ce_0: 0.6123  loss_mask_0: 0.375  loss_dice_0: 2.941  loss_ce_1: 0.3372  loss_mask_1: 0.3814  loss_dice_1: 2.84  loss_ce_2: 0.3223  loss_mask_2: 0.379  loss_dice_2: 2.82  loss_ce_3: 0.3044  loss_mask_3: 0.3753  loss_dice_3: 2.806  loss_ce_4: 0.3106  loss_mask_4: 0.3737  loss_dice_4: 2.808  loss_ce_5: 0.2973  loss_mask_5: 0.3719  loss_dice_5: 2.803  loss_ce_6: 0.2923  loss_mask_6: 0.3694  loss_dice_6: 2.81  loss_ce_7: 0.2943  loss_mask_7: 0.3716  loss_dice_7: 2.802  loss_ce_8: 0.2815  loss_mask_8: 0.3722  loss_dice_8: 2.798  time: 1.4929  data_time: 0.0705  lr: 4.962e-06  max_mem: 21589M
[01/18 04:44:20] d2.utils.events INFO:  eta: 7:23:50  iter: 21659  total_loss: 35.66  loss_ce: 0.2885  loss_mask: 0.384  loss_dice: 2.845  loss_ce_0: 0.5817  loss_mask_0: 0.375  loss_dice_0: 2.987  loss_ce_1: 0.3209  loss_mask_1: 0.3904  loss_dice_1: 2.895  loss_ce_2: 0.3217  loss_mask_2: 0.3881  loss_dice_2: 2.868  loss_ce_3: 0.3023  loss_mask_3: 0.3839  loss_dice_3: 2.86  loss_ce_4: 0.2925  loss_mask_4: 0.3832  loss_dice_4: 2.852  loss_ce_5: 0.2903  loss_mask_5: 0.3836  loss_dice_5: 2.856  loss_ce_6: 0.2892  loss_mask_6: 0.3845  loss_dice_6: 2.851  loss_ce_7: 0.2784  loss_mask_7: 0.385  loss_dice_7: 2.851  loss_ce_8: 0.2745  loss_mask_8: 0.3834  loss_dice_8: 2.85  time: 1.4929  data_time: 0.0726  lr: 4.9571e-06  max_mem: 21589M
[01/18 04:44:49] d2.utils.events INFO:  eta: 7:23:17  iter: 21679  total_loss: 35.78  loss_ce: 0.2972  loss_mask: 0.3866  loss_dice: 2.871  loss_ce_0: 0.5922  loss_mask_0: 0.3778  loss_dice_0: 3.004  loss_ce_1: 0.3281  loss_mask_1: 0.3888  loss_dice_1: 2.899  loss_ce_2: 0.326  loss_mask_2: 0.3878  loss_dice_2: 2.89  loss_ce_3: 0.3223  loss_mask_3: 0.3845  loss_dice_3: 2.878  loss_ce_4: 0.2929  loss_mask_4: 0.3847  loss_dice_4: 2.88  loss_ce_5: 0.2911  loss_mask_5: 0.3845  loss_dice_5: 2.879  loss_ce_6: 0.2919  loss_mask_6: 0.3862  loss_dice_6: 2.867  loss_ce_7: 0.2858  loss_mask_7: 0.3856  loss_dice_7: 2.875  loss_ce_8: 0.2905  loss_mask_8: 0.3871  loss_dice_8: 2.878  time: 1.4929  data_time: 0.0669  lr: 4.9522e-06  max_mem: 21589M
[01/18 04:45:18] d2.utils.events INFO:  eta: 7:22:42  iter: 21699  total_loss: 36.06  loss_ce: 0.3032  loss_mask: 0.3635  loss_dice: 2.889  loss_ce_0: 0.5886  loss_mask_0: 0.3594  loss_dice_0: 3.017  loss_ce_1: 0.3254  loss_mask_1: 0.3685  loss_dice_1: 2.928  loss_ce_2: 0.315  loss_mask_2: 0.3661  loss_dice_2: 2.907  loss_ce_3: 0.3164  loss_mask_3: 0.3663  loss_dice_3: 2.889  loss_ce_4: 0.3188  loss_mask_4: 0.3641  loss_dice_4: 2.895  loss_ce_5: 0.3023  loss_mask_5: 0.3623  loss_dice_5: 2.896  loss_ce_6: 0.2753  loss_mask_6: 0.364  loss_dice_6: 2.893  loss_ce_7: 0.3119  loss_mask_7: 0.3641  loss_dice_7: 2.886  loss_ce_8: 0.3041  loss_mask_8: 0.3636  loss_dice_8: 2.894  time: 1.4928  data_time: 0.0674  lr: 4.9474e-06  max_mem: 21589M
[01/18 04:45:47] d2.utils.events INFO:  eta: 7:22:21  iter: 21719  total_loss: 35.44  loss_ce: 0.2726  loss_mask: 0.38  loss_dice: 2.807  loss_ce_0: 0.5283  loss_mask_0: 0.3691  loss_dice_0: 2.952  loss_ce_1: 0.3227  loss_mask_1: 0.3831  loss_dice_1: 2.862  loss_ce_2: 0.3111  loss_mask_2: 0.3817  loss_dice_2: 2.825  loss_ce_3: 0.2888  loss_mask_3: 0.3812  loss_dice_3: 2.816  loss_ce_4: 0.3056  loss_mask_4: 0.38  loss_dice_4: 2.809  loss_ce_5: 0.2755  loss_mask_5: 0.3781  loss_dice_5: 2.822  loss_ce_6: 0.2696  loss_mask_6: 0.3801  loss_dice_6: 2.809  loss_ce_7: 0.2703  loss_mask_7: 0.3797  loss_dice_7: 2.818  loss_ce_8: 0.2693  loss_mask_8: 0.3789  loss_dice_8: 2.819  time: 1.4928  data_time: 0.0698  lr: 4.9425e-06  max_mem: 21589M
[01/18 04:46:16] d2.utils.events INFO:  eta: 7:21:51  iter: 21739  total_loss: 36.03  loss_ce: 0.2952  loss_mask: 0.3769  loss_dice: 2.86  loss_ce_0: 0.5402  loss_mask_0: 0.3758  loss_dice_0: 2.989  loss_ce_1: 0.329  loss_mask_1: 0.3856  loss_dice_1: 2.897  loss_ce_2: 0.32  loss_mask_2: 0.3803  loss_dice_2: 2.873  loss_ce_3: 0.2991  loss_mask_3: 0.3763  loss_dice_3: 2.871  loss_ce_4: 0.302  loss_mask_4: 0.3774  loss_dice_4: 2.865  loss_ce_5: 0.2931  loss_mask_5: 0.3776  loss_dice_5: 2.868  loss_ce_6: 0.2908  loss_mask_6: 0.3787  loss_dice_6: 2.855  loss_ce_7: 0.3019  loss_mask_7: 0.3769  loss_dice_7: 2.857  loss_ce_8: 0.2888  loss_mask_8: 0.3782  loss_dice_8: 2.852  time: 1.4927  data_time: 0.0700  lr: 4.9376e-06  max_mem: 21589M
[01/18 04:46:46] d2.utils.events INFO:  eta: 7:21:41  iter: 21759  total_loss: 35.65  loss_ce: 0.2853  loss_mask: 0.3763  loss_dice: 2.85  loss_ce_0: 0.5686  loss_mask_0: 0.3773  loss_dice_0: 2.964  loss_ce_1: 0.3206  loss_mask_1: 0.3872  loss_dice_1: 2.883  loss_ce_2: 0.3269  loss_mask_2: 0.3817  loss_dice_2: 2.865  loss_ce_3: 0.2965  loss_mask_3: 0.378  loss_dice_3: 2.845  loss_ce_4: 0.2915  loss_mask_4: 0.378  loss_dice_4: 2.848  loss_ce_5: 0.284  loss_mask_5: 0.3776  loss_dice_5: 2.852  loss_ce_6: 0.2905  loss_mask_6: 0.3766  loss_dice_6: 2.847  loss_ce_7: 0.2857  loss_mask_7: 0.3763  loss_dice_7: 2.852  loss_ce_8: 0.2884  loss_mask_8: 0.3758  loss_dice_8: 2.857  time: 1.4927  data_time: 0.0736  lr: 4.9328e-06  max_mem: 21589M
[01/18 04:47:14] d2.utils.events INFO:  eta: 7:21:12  iter: 21779  total_loss: 35.2  loss_ce: 0.2617  loss_mask: 0.3725  loss_dice: 2.813  loss_ce_0: 0.5813  loss_mask_0: 0.3652  loss_dice_0: 2.95  loss_ce_1: 0.317  loss_mask_1: 0.3792  loss_dice_1: 2.86  loss_ce_2: 0.301  loss_mask_2: 0.3755  loss_dice_2: 2.834  loss_ce_3: 0.284  loss_mask_3: 0.3733  loss_dice_3: 2.826  loss_ce_4: 0.2892  loss_mask_4: 0.3709  loss_dice_4: 2.817  loss_ce_5: 0.2835  loss_mask_5: 0.3708  loss_dice_5: 2.823  loss_ce_6: 0.2718  loss_mask_6: 0.3715  loss_dice_6: 2.818  loss_ce_7: 0.2683  loss_mask_7: 0.3713  loss_dice_7: 2.813  loss_ce_8: 0.2739  loss_mask_8: 0.3725  loss_dice_8: 2.815  time: 1.4927  data_time: 0.0685  lr: 4.9279e-06  max_mem: 21589M
[01/18 04:47:43] d2.utils.events INFO:  eta: 7:20:32  iter: 21799  total_loss: 35.38  loss_ce: 0.2552  loss_mask: 0.3684  loss_dice: 2.822  loss_ce_0: 0.557  loss_mask_0: 0.3674  loss_dice_0: 2.954  loss_ce_1: 0.3135  loss_mask_1: 0.3763  loss_dice_1: 2.853  loss_ce_2: 0.3017  loss_mask_2: 0.3748  loss_dice_2: 2.83  loss_ce_3: 0.279  loss_mask_3: 0.3704  loss_dice_3: 2.828  loss_ce_4: 0.2767  loss_mask_4: 0.3697  loss_dice_4: 2.813  loss_ce_5: 0.2677  loss_mask_5: 0.3684  loss_dice_5: 2.827  loss_ce_6: 0.2777  loss_mask_6: 0.3686  loss_dice_6: 2.81  loss_ce_7: 0.2744  loss_mask_7: 0.3686  loss_dice_7: 2.824  loss_ce_8: 0.2684  loss_mask_8: 0.3683  loss_dice_8: 2.813  time: 1.4926  data_time: 0.0651  lr: 4.923e-06  max_mem: 21589M
[01/18 04:48:12] d2.utils.events INFO:  eta: 7:20:10  iter: 21819  total_loss: 35.82  loss_ce: 0.284  loss_mask: 0.3734  loss_dice: 2.867  loss_ce_0: 0.5638  loss_mask_0: 0.3665  loss_dice_0: 3.012  loss_ce_1: 0.33  loss_mask_1: 0.3702  loss_dice_1: 2.909  loss_ce_2: 0.3146  loss_mask_2: 0.3704  loss_dice_2: 2.887  loss_ce_3: 0.3036  loss_mask_3: 0.3703  loss_dice_3: 2.879  loss_ce_4: 0.2921  loss_mask_4: 0.3721  loss_dice_4: 2.872  loss_ce_5: 0.2772  loss_mask_5: 0.3731  loss_dice_5: 2.873  loss_ce_6: 0.2865  loss_mask_6: 0.375  loss_dice_6: 2.862  loss_ce_7: 0.2736  loss_mask_7: 0.3757  loss_dice_7: 2.873  loss_ce_8: 0.2765  loss_mask_8: 0.3746  loss_dice_8: 2.857  time: 1.4926  data_time: 0.0730  lr: 4.9182e-06  max_mem: 21589M
[01/18 04:48:41] d2.utils.events INFO:  eta: 7:20:00  iter: 21839  total_loss: 35.36  loss_ce: 0.2708  loss_mask: 0.3793  loss_dice: 2.833  loss_ce_0: 0.5394  loss_mask_0: 0.3778  loss_dice_0: 2.971  loss_ce_1: 0.3142  loss_mask_1: 0.3898  loss_dice_1: 2.876  loss_ce_2: 0.2909  loss_mask_2: 0.3842  loss_dice_2: 2.851  loss_ce_3: 0.2739  loss_mask_3: 0.3829  loss_dice_3: 2.848  loss_ce_4: 0.2683  loss_mask_4: 0.3817  loss_dice_4: 2.837  loss_ce_5: 0.2684  loss_mask_5: 0.3786  loss_dice_5: 2.843  loss_ce_6: 0.2641  loss_mask_6: 0.3807  loss_dice_6: 2.835  loss_ce_7: 0.2623  loss_mask_7: 0.3795  loss_dice_7: 2.839  loss_ce_8: 0.2751  loss_mask_8: 0.3782  loss_dice_8: 2.845  time: 1.4925  data_time: 0.0703  lr: 4.9133e-06  max_mem: 21589M
[01/18 04:49:10] d2.utils.events INFO:  eta: 7:19:21  iter: 21859  total_loss: 35.27  loss_ce: 0.2815  loss_mask: 0.3703  loss_dice: 2.8  loss_ce_0: 0.5651  loss_mask_0: 0.3644  loss_dice_0: 2.933  loss_ce_1: 0.334  loss_mask_1: 0.3761  loss_dice_1: 2.835  loss_ce_2: 0.3163  loss_mask_2: 0.3725  loss_dice_2: 2.819  loss_ce_3: 0.2907  loss_mask_3: 0.3707  loss_dice_3: 2.796  loss_ce_4: 0.294  loss_mask_4: 0.3726  loss_dice_4: 2.801  loss_ce_5: 0.2987  loss_mask_5: 0.371  loss_dice_5: 2.801  loss_ce_6: 0.2927  loss_mask_6: 0.3714  loss_dice_6: 2.793  loss_ce_7: 0.3025  loss_mask_7: 0.3713  loss_dice_7: 2.794  loss_ce_8: 0.2899  loss_mask_8: 0.3713  loss_dice_8: 2.798  time: 1.4925  data_time: 0.0700  lr: 4.9084e-06  max_mem: 21589M
[01/18 04:49:39] d2.utils.events INFO:  eta: 7:19:15  iter: 21879  total_loss: 35.59  loss_ce: 0.2945  loss_mask: 0.3666  loss_dice: 2.867  loss_ce_0: 0.556  loss_mask_0: 0.3686  loss_dice_0: 2.999  loss_ce_1: 0.321  loss_mask_1: 0.3741  loss_dice_1: 2.898  loss_ce_2: 0.3029  loss_mask_2: 0.3719  loss_dice_2: 2.888  loss_ce_3: 0.3081  loss_mask_3: 0.3694  loss_dice_3: 2.873  loss_ce_4: 0.2984  loss_mask_4: 0.3667  loss_dice_4: 2.867  loss_ce_5: 0.2951  loss_mask_5: 0.3674  loss_dice_5: 2.869  loss_ce_6: 0.2905  loss_mask_6: 0.3658  loss_dice_6: 2.858  loss_ce_7: 0.2817  loss_mask_7: 0.366  loss_dice_7: 2.861  loss_ce_8: 0.2817  loss_mask_8: 0.368  loss_dice_8: 2.869  time: 1.4925  data_time: 0.0722  lr: 4.9035e-06  max_mem: 21589M
[01/18 04:50:08] d2.utils.events INFO:  eta: 7:18:32  iter: 21899  total_loss: 36.13  loss_ce: 0.2833  loss_mask: 0.3767  loss_dice: 2.878  loss_ce_0: 0.6071  loss_mask_0: 0.3761  loss_dice_0: 3.009  loss_ce_1: 0.3584  loss_mask_1: 0.3836  loss_dice_1: 2.912  loss_ce_2: 0.3253  loss_mask_2: 0.3791  loss_dice_2: 2.888  loss_ce_3: 0.3148  loss_mask_3: 0.3773  loss_dice_3: 2.879  loss_ce_4: 0.305  loss_mask_4: 0.376  loss_dice_4: 2.881  loss_ce_5: 0.293  loss_mask_5: 0.3762  loss_dice_5: 2.873  loss_ce_6: 0.2926  loss_mask_6: 0.376  loss_dice_6: 2.875  loss_ce_7: 0.2764  loss_mask_7: 0.3762  loss_dice_7: 2.873  loss_ce_8: 0.2895  loss_mask_8: 0.3767  loss_dice_8: 2.878  time: 1.4924  data_time: 0.0684  lr: 4.8987e-06  max_mem: 21589M
[01/18 04:50:37] d2.utils.events INFO:  eta: 7:18:00  iter: 21919  total_loss: 36.25  loss_ce: 0.3052  loss_mask: 0.3799  loss_dice: 2.891  loss_ce_0: 0.5452  loss_mask_0: 0.3722  loss_dice_0: 3.016  loss_ce_1: 0.3544  loss_mask_1: 0.3908  loss_dice_1: 2.922  loss_ce_2: 0.337  loss_mask_2: 0.3838  loss_dice_2: 2.913  loss_ce_3: 0.3062  loss_mask_3: 0.3803  loss_dice_3: 2.898  loss_ce_4: 0.2928  loss_mask_4: 0.381  loss_dice_4: 2.89  loss_ce_5: 0.3039  loss_mask_5: 0.3821  loss_dice_5: 2.891  loss_ce_6: 0.3001  loss_mask_6: 0.3823  loss_dice_6: 2.889  loss_ce_7: 0.2918  loss_mask_7: 0.3829  loss_dice_7: 2.886  loss_ce_8: 0.2973  loss_mask_8: 0.3811  loss_dice_8: 2.891  time: 1.4924  data_time: 0.0699  lr: 4.8938e-06  max_mem: 21589M
[01/18 04:51:06] d2.utils.events INFO:  eta: 7:17:16  iter: 21939  total_loss: 35.48  loss_ce: 0.2882  loss_mask: 0.3791  loss_dice: 2.807  loss_ce_0: 0.5764  loss_mask_0: 0.3748  loss_dice_0: 2.928  loss_ce_1: 0.3245  loss_mask_1: 0.3863  loss_dice_1: 2.848  loss_ce_2: 0.3082  loss_mask_2: 0.3846  loss_dice_2: 2.814  loss_ce_3: 0.28  loss_mask_3: 0.3828  loss_dice_3: 2.804  loss_ce_4: 0.2959  loss_mask_4: 0.3804  loss_dice_4: 2.804  loss_ce_5: 0.2873  loss_mask_5: 0.3803  loss_dice_5: 2.807  loss_ce_6: 0.2754  loss_mask_6: 0.3798  loss_dice_6: 2.797  loss_ce_7: 0.2825  loss_mask_7: 0.3804  loss_dice_7: 2.802  loss_ce_8: 0.2754  loss_mask_8: 0.3796  loss_dice_8: 2.8  time: 1.4923  data_time: 0.0672  lr: 4.8889e-06  max_mem: 21589M
[01/18 04:51:36] d2.utils.events INFO:  eta: 7:16:47  iter: 21959  total_loss: 35.81  loss_ce: 0.2847  loss_mask: 0.3618  loss_dice: 2.85  loss_ce_0: 0.5906  loss_mask_0: 0.3616  loss_dice_0: 2.976  loss_ce_1: 0.3142  loss_mask_1: 0.3675  loss_dice_1: 2.879  loss_ce_2: 0.3269  loss_mask_2: 0.3632  loss_dice_2: 2.869  loss_ce_3: 0.302  loss_mask_3: 0.3603  loss_dice_3: 2.856  loss_ce_4: 0.3027  loss_mask_4: 0.3601  loss_dice_4: 2.859  loss_ce_5: 0.2859  loss_mask_5: 0.3623  loss_dice_5: 2.854  loss_ce_6: 0.2873  loss_mask_6: 0.3617  loss_dice_6: 2.847  loss_ce_7: 0.3093  loss_mask_7: 0.3614  loss_dice_7: 2.845  loss_ce_8: 0.2853  loss_mask_8: 0.3603  loss_dice_8: 2.843  time: 1.4923  data_time: 0.0746  lr: 4.8841e-06  max_mem: 21589M
[01/18 04:52:05] d2.utils.events INFO:  eta: 7:16:06  iter: 21979  total_loss: 36.26  loss_ce: 0.2724  loss_mask: 0.3771  loss_dice: 2.909  loss_ce_0: 0.5348  loss_mask_0: 0.3759  loss_dice_0: 3.044  loss_ce_1: 0.2969  loss_mask_1: 0.3806  loss_dice_1: 2.944  loss_ce_2: 0.3121  loss_mask_2: 0.378  loss_dice_2: 2.932  loss_ce_3: 0.2774  loss_mask_3: 0.3754  loss_dice_3: 2.923  loss_ce_4: 0.2751  loss_mask_4: 0.3762  loss_dice_4: 2.911  loss_ce_5: 0.2652  loss_mask_5: 0.3766  loss_dice_5: 2.915  loss_ce_6: 0.2778  loss_mask_6: 0.3778  loss_dice_6: 2.908  loss_ce_7: 0.2724  loss_mask_7: 0.3771  loss_dice_7: 2.902  loss_ce_8: 0.2786  loss_mask_8: 0.3773  loss_dice_8: 2.916  time: 1.4923  data_time: 0.0696  lr: 4.8792e-06  max_mem: 21589M
[01/18 04:52:34] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 04:52:35] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 04:52:35] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 04:52:35] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 04:52:49] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0081 s/iter. Inference: 0.1599 s/iter. Eval: 0.1821 s/iter. Total: 0.3501 s/iter. ETA=0:06:18
[01/18 04:52:54] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0103 s/iter. Inference: 0.1586 s/iter. Eval: 0.2054 s/iter. Total: 0.3744 s/iter. ETA=0:06:40
[01/18 04:52:59] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0106 s/iter. Inference: 0.1563 s/iter. Eval: 0.2108 s/iter. Total: 0.3777 s/iter. ETA=0:06:38
[01/18 04:53:04] d2.evaluation.evaluator INFO: Inference done 53/1093. Dataloading: 0.0115 s/iter. Inference: 0.1522 s/iter. Eval: 0.2027 s/iter. Total: 0.3666 s/iter. ETA=0:06:21
[01/18 04:53:09] d2.evaluation.evaluator INFO: Inference done 66/1093. Dataloading: 0.0122 s/iter. Inference: 0.1527 s/iter. Eval: 0.2062 s/iter. Total: 0.3712 s/iter. ETA=0:06:21
[01/18 04:53:14] d2.evaluation.evaluator INFO: Inference done 79/1093. Dataloading: 0.0120 s/iter. Inference: 0.1542 s/iter. Eval: 0.2082 s/iter. Total: 0.3746 s/iter. ETA=0:06:19
[01/18 04:53:20] d2.evaluation.evaluator INFO: Inference done 90/1093. Dataloading: 0.0121 s/iter. Inference: 0.1554 s/iter. Eval: 0.2183 s/iter. Total: 0.3859 s/iter. ETA=0:06:27
[01/18 04:53:25] d2.evaluation.evaluator INFO: Inference done 105/1093. Dataloading: 0.0119 s/iter. Inference: 0.1545 s/iter. Eval: 0.2127 s/iter. Total: 0.3792 s/iter. ETA=0:06:14
[01/18 04:53:30] d2.evaluation.evaluator INFO: Inference done 118/1093. Dataloading: 0.0118 s/iter. Inference: 0.1567 s/iter. Eval: 0.2115 s/iter. Total: 0.3801 s/iter. ETA=0:06:10
[01/18 04:53:35] d2.evaluation.evaluator INFO: Inference done 132/1093. Dataloading: 0.0117 s/iter. Inference: 0.1564 s/iter. Eval: 0.2097 s/iter. Total: 0.3779 s/iter. ETA=0:06:03
[01/18 04:53:40] d2.evaluation.evaluator INFO: Inference done 147/1093. Dataloading: 0.0115 s/iter. Inference: 0.1576 s/iter. Eval: 0.2060 s/iter. Total: 0.3752 s/iter. ETA=0:05:54
[01/18 04:53:45] d2.evaluation.evaluator INFO: Inference done 161/1093. Dataloading: 0.0116 s/iter. Inference: 0.1580 s/iter. Eval: 0.2062 s/iter. Total: 0.3758 s/iter. ETA=0:05:50
[01/18 04:53:51] d2.evaluation.evaluator INFO: Inference done 174/1093. Dataloading: 0.0117 s/iter. Inference: 0.1588 s/iter. Eval: 0.2078 s/iter. Total: 0.3784 s/iter. ETA=0:05:47
[01/18 04:53:56] d2.evaluation.evaluator INFO: Inference done 188/1093. Dataloading: 0.0116 s/iter. Inference: 0.1596 s/iter. Eval: 0.2060 s/iter. Total: 0.3773 s/iter. ETA=0:05:41
[01/18 04:54:01] d2.evaluation.evaluator INFO: Inference done 201/1093. Dataloading: 0.0116 s/iter. Inference: 0.1610 s/iter. Eval: 0.2055 s/iter. Total: 0.3783 s/iter. ETA=0:05:37
[01/18 04:54:06] d2.evaluation.evaluator INFO: Inference done 215/1093. Dataloading: 0.0117 s/iter. Inference: 0.1606 s/iter. Eval: 0.2052 s/iter. Total: 0.3776 s/iter. ETA=0:05:31
[01/18 04:54:11] d2.evaluation.evaluator INFO: Inference done 230/1093. Dataloading: 0.0117 s/iter. Inference: 0.1599 s/iter. Eval: 0.2039 s/iter. Total: 0.3756 s/iter. ETA=0:05:24
[01/18 04:54:16] d2.evaluation.evaluator INFO: Inference done 243/1093. Dataloading: 0.0117 s/iter. Inference: 0.1600 s/iter. Eval: 0.2047 s/iter. Total: 0.3764 s/iter. ETA=0:05:19
[01/18 04:54:22] d2.evaluation.evaluator INFO: Inference done 256/1093. Dataloading: 0.0118 s/iter. Inference: 0.1605 s/iter. Eval: 0.2056 s/iter. Total: 0.3780 s/iter. ETA=0:05:16
[01/18 04:54:27] d2.evaluation.evaluator INFO: Inference done 271/1093. Dataloading: 0.0118 s/iter. Inference: 0.1603 s/iter. Eval: 0.2048 s/iter. Total: 0.3770 s/iter. ETA=0:05:09
[01/18 04:54:32] d2.evaluation.evaluator INFO: Inference done 284/1093. Dataloading: 0.0118 s/iter. Inference: 0.1610 s/iter. Eval: 0.2051 s/iter. Total: 0.3780 s/iter. ETA=0:05:05
[01/18 04:54:37] d2.evaluation.evaluator INFO: Inference done 297/1093. Dataloading: 0.0118 s/iter. Inference: 0.1618 s/iter. Eval: 0.2050 s/iter. Total: 0.3786 s/iter. ETA=0:05:01
[01/18 04:54:43] d2.evaluation.evaluator INFO: Inference done 309/1093. Dataloading: 0.0119 s/iter. Inference: 0.1621 s/iter. Eval: 0.2073 s/iter. Total: 0.3815 s/iter. ETA=0:04:59
[01/18 04:54:48] d2.evaluation.evaluator INFO: Inference done 322/1093. Dataloading: 0.0120 s/iter. Inference: 0.1622 s/iter. Eval: 0.2080 s/iter. Total: 0.3823 s/iter. ETA=0:04:54
[01/18 04:54:53] d2.evaluation.evaluator INFO: Inference done 337/1093. Dataloading: 0.0119 s/iter. Inference: 0.1618 s/iter. Eval: 0.2068 s/iter. Total: 0.3805 s/iter. ETA=0:04:47
[01/18 04:54:58] d2.evaluation.evaluator INFO: Inference done 352/1093. Dataloading: 0.0118 s/iter. Inference: 0.1617 s/iter. Eval: 0.2052 s/iter. Total: 0.3788 s/iter. ETA=0:04:40
[01/18 04:55:03] d2.evaluation.evaluator INFO: Inference done 366/1093. Dataloading: 0.0118 s/iter. Inference: 0.1616 s/iter. Eval: 0.2050 s/iter. Total: 0.3785 s/iter. ETA=0:04:35
[01/18 04:55:09] d2.evaluation.evaluator INFO: Inference done 380/1093. Dataloading: 0.0118 s/iter. Inference: 0.1621 s/iter. Eval: 0.2045 s/iter. Total: 0.3785 s/iter. ETA=0:04:29
[01/18 04:55:14] d2.evaluation.evaluator INFO: Inference done 393/1093. Dataloading: 0.0118 s/iter. Inference: 0.1621 s/iter. Eval: 0.2048 s/iter. Total: 0.3788 s/iter. ETA=0:04:25
[01/18 04:55:19] d2.evaluation.evaluator INFO: Inference done 407/1093. Dataloading: 0.0118 s/iter. Inference: 0.1617 s/iter. Eval: 0.2047 s/iter. Total: 0.3783 s/iter. ETA=0:04:19
[01/18 04:55:24] d2.evaluation.evaluator INFO: Inference done 421/1093. Dataloading: 0.0118 s/iter. Inference: 0.1613 s/iter. Eval: 0.2046 s/iter. Total: 0.3778 s/iter. ETA=0:04:13
[01/18 04:55:29] d2.evaluation.evaluator INFO: Inference done 435/1093. Dataloading: 0.0118 s/iter. Inference: 0.1610 s/iter. Eval: 0.2047 s/iter. Total: 0.3776 s/iter. ETA=0:04:08
[01/18 04:55:34] d2.evaluation.evaluator INFO: Inference done 449/1093. Dataloading: 0.0118 s/iter. Inference: 0.1610 s/iter. Eval: 0.2046 s/iter. Total: 0.3775 s/iter. ETA=0:04:03
[01/18 04:55:39] d2.evaluation.evaluator INFO: Inference done 463/1093. Dataloading: 0.0118 s/iter. Inference: 0.1607 s/iter. Eval: 0.2043 s/iter. Total: 0.3769 s/iter. ETA=0:03:57
[01/18 04:55:45] d2.evaluation.evaluator INFO: Inference done 479/1093. Dataloading: 0.0117 s/iter. Inference: 0.1606 s/iter. Eval: 0.2030 s/iter. Total: 0.3753 s/iter. ETA=0:03:50
[01/18 04:55:50] d2.evaluation.evaluator INFO: Inference done 494/1093. Dataloading: 0.0117 s/iter. Inference: 0.1604 s/iter. Eval: 0.2024 s/iter. Total: 0.3746 s/iter. ETA=0:03:44
[01/18 04:55:55] d2.evaluation.evaluator INFO: Inference done 510/1093. Dataloading: 0.0116 s/iter. Inference: 0.1603 s/iter. Eval: 0.2007 s/iter. Total: 0.3727 s/iter. ETA=0:03:37
[01/18 04:56:00] d2.evaluation.evaluator INFO: Inference done 524/1093. Dataloading: 0.0116 s/iter. Inference: 0.1601 s/iter. Eval: 0.2009 s/iter. Total: 0.3727 s/iter. ETA=0:03:32
[01/18 04:56:05] d2.evaluation.evaluator INFO: Inference done 538/1093. Dataloading: 0.0116 s/iter. Inference: 0.1598 s/iter. Eval: 0.2010 s/iter. Total: 0.3726 s/iter. ETA=0:03:26
[01/18 04:56:11] d2.evaluation.evaluator INFO: Inference done 552/1093. Dataloading: 0.0116 s/iter. Inference: 0.1597 s/iter. Eval: 0.2013 s/iter. Total: 0.3728 s/iter. ETA=0:03:21
[01/18 04:56:16] d2.evaluation.evaluator INFO: Inference done 567/1093. Dataloading: 0.0116 s/iter. Inference: 0.1596 s/iter. Eval: 0.2012 s/iter. Total: 0.3725 s/iter. ETA=0:03:15
[01/18 04:56:21] d2.evaluation.evaluator INFO: Inference done 582/1093. Dataloading: 0.0117 s/iter. Inference: 0.1597 s/iter. Eval: 0.2000 s/iter. Total: 0.3715 s/iter. ETA=0:03:09
[01/18 04:56:26] d2.evaluation.evaluator INFO: Inference done 597/1093. Dataloading: 0.0117 s/iter. Inference: 0.1595 s/iter. Eval: 0.1995 s/iter. Total: 0.3708 s/iter. ETA=0:03:03
[01/18 04:56:31] d2.evaluation.evaluator INFO: Inference done 609/1093. Dataloading: 0.0117 s/iter. Inference: 0.1595 s/iter. Eval: 0.2005 s/iter. Total: 0.3719 s/iter. ETA=0:02:59
[01/18 04:56:37] d2.evaluation.evaluator INFO: Inference done 623/1093. Dataloading: 0.0120 s/iter. Inference: 0.1597 s/iter. Eval: 0.2004 s/iter. Total: 0.3722 s/iter. ETA=0:02:54
[01/18 04:56:42] d2.evaluation.evaluator INFO: Inference done 638/1093. Dataloading: 0.0120 s/iter. Inference: 0.1595 s/iter. Eval: 0.2000 s/iter. Total: 0.3716 s/iter. ETA=0:02:49
[01/18 04:56:47] d2.evaluation.evaluator INFO: Inference done 653/1093. Dataloading: 0.0120 s/iter. Inference: 0.1595 s/iter. Eval: 0.1997 s/iter. Total: 0.3712 s/iter. ETA=0:02:43
[01/18 04:56:53] d2.evaluation.evaluator INFO: Inference done 665/1093. Dataloading: 0.0119 s/iter. Inference: 0.1599 s/iter. Eval: 0.2004 s/iter. Total: 0.3724 s/iter. ETA=0:02:39
[01/18 04:56:58] d2.evaluation.evaluator INFO: Inference done 679/1093. Dataloading: 0.0119 s/iter. Inference: 0.1601 s/iter. Eval: 0.2001 s/iter. Total: 0.3722 s/iter. ETA=0:02:34
[01/18 04:57:03] d2.evaluation.evaluator INFO: Inference done 694/1093. Dataloading: 0.0119 s/iter. Inference: 0.1601 s/iter. Eval: 0.1995 s/iter. Total: 0.3716 s/iter. ETA=0:02:28
[01/18 04:57:08] d2.evaluation.evaluator INFO: Inference done 708/1093. Dataloading: 0.0119 s/iter. Inference: 0.1600 s/iter. Eval: 0.1995 s/iter. Total: 0.3715 s/iter. ETA=0:02:23
[01/18 04:57:13] d2.evaluation.evaluator INFO: Inference done 720/1093. Dataloading: 0.0120 s/iter. Inference: 0.1599 s/iter. Eval: 0.2004 s/iter. Total: 0.3724 s/iter. ETA=0:02:18
[01/18 04:57:18] d2.evaluation.evaluator INFO: Inference done 735/1093. Dataloading: 0.0119 s/iter. Inference: 0.1599 s/iter. Eval: 0.1997 s/iter. Total: 0.3716 s/iter. ETA=0:02:13
[01/18 04:57:23] d2.evaluation.evaluator INFO: Inference done 749/1093. Dataloading: 0.0119 s/iter. Inference: 0.1601 s/iter. Eval: 0.1995 s/iter. Total: 0.3716 s/iter. ETA=0:02:07
[01/18 04:57:28] d2.evaluation.evaluator INFO: Inference done 762/1093. Dataloading: 0.0119 s/iter. Inference: 0.1603 s/iter. Eval: 0.1998 s/iter. Total: 0.3720 s/iter. ETA=0:02:03
[01/18 04:57:33] d2.evaluation.evaluator INFO: Inference done 774/1093. Dataloading: 0.0119 s/iter. Inference: 0.1605 s/iter. Eval: 0.2003 s/iter. Total: 0.3727 s/iter. ETA=0:01:58
[01/18 04:57:39] d2.evaluation.evaluator INFO: Inference done 791/1093. Dataloading: 0.0118 s/iter. Inference: 0.1602 s/iter. Eval: 0.1993 s/iter. Total: 0.3714 s/iter. ETA=0:01:52
[01/18 04:57:44] d2.evaluation.evaluator INFO: Inference done 805/1093. Dataloading: 0.0118 s/iter. Inference: 0.1601 s/iter. Eval: 0.1994 s/iter. Total: 0.3714 s/iter. ETA=0:01:46
[01/18 04:57:49] d2.evaluation.evaluator INFO: Inference done 818/1093. Dataloading: 0.0118 s/iter. Inference: 0.1602 s/iter. Eval: 0.1996 s/iter. Total: 0.3717 s/iter. ETA=0:01:42
[01/18 04:57:54] d2.evaluation.evaluator INFO: Inference done 833/1093. Dataloading: 0.0118 s/iter. Inference: 0.1601 s/iter. Eval: 0.1991 s/iter. Total: 0.3711 s/iter. ETA=0:01:36
[01/18 04:57:59] d2.evaluation.evaluator INFO: Inference done 848/1093. Dataloading: 0.0118 s/iter. Inference: 0.1601 s/iter. Eval: 0.1987 s/iter. Total: 0.3706 s/iter. ETA=0:01:30
[01/18 04:58:04] d2.evaluation.evaluator INFO: Inference done 860/1093. Dataloading: 0.0118 s/iter. Inference: 0.1601 s/iter. Eval: 0.1995 s/iter. Total: 0.3715 s/iter. ETA=0:01:26
[01/18 04:58:10] d2.evaluation.evaluator INFO: Inference done 875/1093. Dataloading: 0.0118 s/iter. Inference: 0.1599 s/iter. Eval: 0.1993 s/iter. Total: 0.3711 s/iter. ETA=0:01:20
[01/18 04:58:15] d2.evaluation.evaluator INFO: Inference done 888/1093. Dataloading: 0.0118 s/iter. Inference: 0.1601 s/iter. Eval: 0.2000 s/iter. Total: 0.3719 s/iter. ETA=0:01:16
[01/18 04:58:20] d2.evaluation.evaluator INFO: Inference done 902/1093. Dataloading: 0.0117 s/iter. Inference: 0.1602 s/iter. Eval: 0.2000 s/iter. Total: 0.3720 s/iter. ETA=0:01:11
[01/18 04:58:26] d2.evaluation.evaluator INFO: Inference done 916/1093. Dataloading: 0.0117 s/iter. Inference: 0.1602 s/iter. Eval: 0.1999 s/iter. Total: 0.3719 s/iter. ETA=0:01:05
[01/18 04:58:31] d2.evaluation.evaluator INFO: Inference done 930/1093. Dataloading: 0.0117 s/iter. Inference: 0.1603 s/iter. Eval: 0.1999 s/iter. Total: 0.3721 s/iter. ETA=0:01:00
[01/18 04:58:36] d2.evaluation.evaluator INFO: Inference done 944/1093. Dataloading: 0.0117 s/iter. Inference: 0.1601 s/iter. Eval: 0.2002 s/iter. Total: 0.3721 s/iter. ETA=0:00:55
[01/18 04:58:41] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0117 s/iter. Inference: 0.1599 s/iter. Eval: 0.2004 s/iter. Total: 0.3721 s/iter. ETA=0:00:50
[01/18 04:58:46] d2.evaluation.evaluator INFO: Inference done 971/1093. Dataloading: 0.0117 s/iter. Inference: 0.1599 s/iter. Eval: 0.2006 s/iter. Total: 0.3723 s/iter. ETA=0:00:45
[01/18 04:58:51] d2.evaluation.evaluator INFO: Inference done 987/1093. Dataloading: 0.0116 s/iter. Inference: 0.1598 s/iter. Eval: 0.1999 s/iter. Total: 0.3714 s/iter. ETA=0:00:39
[01/18 04:58:56] d2.evaluation.evaluator INFO: Inference done 1002/1093. Dataloading: 0.0116 s/iter. Inference: 0.1596 s/iter. Eval: 0.1995 s/iter. Total: 0.3708 s/iter. ETA=0:00:33
[01/18 04:59:02] d2.evaluation.evaluator INFO: Inference done 1017/1093. Dataloading: 0.0116 s/iter. Inference: 0.1595 s/iter. Eval: 0.1993 s/iter. Total: 0.3705 s/iter. ETA=0:00:28
[01/18 04:59:07] d2.evaluation.evaluator INFO: Inference done 1031/1093. Dataloading: 0.0116 s/iter. Inference: 0.1594 s/iter. Eval: 0.1993 s/iter. Total: 0.3705 s/iter. ETA=0:00:22
[01/18 04:59:12] d2.evaluation.evaluator INFO: Inference done 1045/1093. Dataloading: 0.0116 s/iter. Inference: 0.1594 s/iter. Eval: 0.1993 s/iter. Total: 0.3704 s/iter. ETA=0:00:17
[01/18 04:59:17] d2.evaluation.evaluator INFO: Inference done 1060/1093. Dataloading: 0.0116 s/iter. Inference: 0.1592 s/iter. Eval: 0.1992 s/iter. Total: 0.3700 s/iter. ETA=0:00:12
[01/18 04:59:22] d2.evaluation.evaluator INFO: Inference done 1077/1093. Dataloading: 0.0115 s/iter. Inference: 0.1590 s/iter. Eval: 0.1984 s/iter. Total: 0.3690 s/iter. ETA=0:00:05
[01/18 04:59:28] d2.evaluation.evaluator INFO: Inference done 1093/1093. Dataloading: 0.0115 s/iter. Inference: 0.1587 s/iter. Eval: 0.1980 s/iter. Total: 0.3684 s/iter. ETA=0:00:00
[01/18 04:59:28] d2.evaluation.evaluator INFO: Total inference time: 0:06:41.236165 (0.368783 s / iter per device, on 4 devices)
[01/18 04:59:28] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:52 (0.158727 s / iter per device, on 4 devices)
[01/18 04:59:53] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.6074342759260585, 'mIoU': 17.638045842910245, 'fwIoU': 39.33598127679857, 'IoU-0': nan, 'IoU-1': 95.40425891995108, 'IoU-2': 48.48229430804708, 'IoU-3': 57.26740430185245, 'IoU-4': 52.426190838154916, 'IoU-5': 46.19526597277681, 'IoU-6': 40.687632567453875, 'IoU-7': 32.39317687030773, 'IoU-8': 20.85054051678303, 'IoU-9': 36.31938814772834, 'IoU-10': 39.14011389282639, 'IoU-11': 45.242800500804336, 'IoU-12': 46.82284701224196, 'IoU-13': 43.58616970290379, 'IoU-14': 44.081951178539185, 'IoU-15': 44.41871832799059, 'IoU-16': 46.760464249232726, 'IoU-17': 42.951797370560044, 'IoU-18': 43.95179887532109, 'IoU-19': 44.55366165346051, 'IoU-20': 44.40901208170926, 'IoU-21': 44.021972880981394, 'IoU-22': 46.17753551265285, 'IoU-23': 43.43210055915811, 'IoU-24': 42.62293771819267, 'IoU-25': 41.409340516538315, 'IoU-26': 41.12169980268119, 'IoU-27': 40.96491445367714, 'IoU-28': 39.93683633460536, 'IoU-29': 40.82232195111105, 'IoU-30': 38.784478026853996, 'IoU-31': 39.06280748155022, 'IoU-32': 39.17508059477136, 'IoU-33': 38.919328556376634, 'IoU-34': 39.5367303770436, 'IoU-35': 39.82722445217671, 'IoU-36': 40.95288861477944, 'IoU-37': 39.35586559721203, 'IoU-38': 39.33067749914064, 'IoU-39': 38.73095455101035, 'IoU-40': 40.463057113122375, 'IoU-41': 36.27859051009065, 'IoU-42': 36.67112207994622, 'IoU-43': 34.708316105423656, 'IoU-44': 32.929352310953895, 'IoU-45': 31.50041807026105, 'IoU-46': 30.208321745378043, 'IoU-47': 29.52618531185327, 'IoU-48': 29.291767262496755, 'IoU-49': 29.721986872320645, 'IoU-50': 28.857239719718063, 'IoU-51': 27.695853570211348, 'IoU-52': 27.63921395650295, 'IoU-53': 28.887877464605864, 'IoU-54': 28.623558749034565, 'IoU-55': 26.844499633078296, 'IoU-56': 25.93104801991509, 'IoU-57': 25.232718955236322, 'IoU-58': 24.089582106335435, 'IoU-59': 23.412564330333847, 'IoU-60': 20.404262045042096, 'IoU-61': 19.753958930002877, 'IoU-62': 19.220708982432775, 'IoU-63': 18.065671551172567, 'IoU-64': 17.226370882149038, 'IoU-65': 17.168798784622, 'IoU-66': 17.194185492367005, 'IoU-67': 17.206828055383703, 'IoU-68': 17.02071035574261, 'IoU-69': 17.815674932018425, 'IoU-70': 17.02630074787811, 'IoU-71': 18.07004096851134, 'IoU-72': 16.79958246468556, 'IoU-73': 17.303301464740244, 'IoU-74': 17.501614468785874, 'IoU-75': 17.913869388346793, 'IoU-76': 17.759696844575572, 'IoU-77': 16.23826815919685, 'IoU-78': 17.20682603821433, 'IoU-79': 16.38587704926676, 'IoU-80': 16.388232526707405, 'IoU-81': 16.533949002629946, 'IoU-82': 16.01737873710924, 'IoU-83': 17.310127451036724, 'IoU-84': 16.11218631341116, 'IoU-85': 16.640323787535216, 'IoU-86': 15.357113630904959, 'IoU-87': 15.593971214997454, 'IoU-88': 16.008462268850746, 'IoU-89': 15.21970954961254, 'IoU-90': 15.915468537127506, 'IoU-91': 15.061091045673727, 'IoU-92': 16.049915313990276, 'IoU-93': 14.900575070793565, 'IoU-94': 14.819081200793377, 'IoU-95': 14.587553410908393, 'IoU-96': 14.077596339291764, 'IoU-97': 14.658855034248731, 'IoU-98': 13.904886911799629, 'IoU-99': 13.476192784523791, 'IoU-100': 12.225534586079364, 'IoU-101': 11.993031067596757, 'IoU-102': 11.414333828777837, 'IoU-103': 11.145037258413517, 'IoU-104': 11.415619650406207, 'IoU-105': 10.0212424983573, 'IoU-106': 10.28758754410196, 'IoU-107': 10.521524255429835, 'IoU-108': 10.438057019445472, 'IoU-109': 9.067992804462929, 'IoU-110': 7.894911913179362, 'IoU-111': 7.67338392432064, 'IoU-112': 6.476233948618376, 'IoU-113': 7.4328683678751215, 'IoU-114': 9.078150828784672, 'IoU-115': 5.873950711003667, 'IoU-116': 6.366171132146673, 'IoU-117': 7.141947365847834, 'IoU-118': 6.517864708729338, 'IoU-119': 7.60391185020554, 'IoU-120': 5.318216999764099, 'IoU-121': 7.160509542557351, 'IoU-122': 7.100079393859036, 'IoU-123': 6.64603230292847, 'IoU-124': 5.788843412765498, 'IoU-125': 6.016500789228247, 'IoU-126': 4.451890013989191, 'IoU-127': 5.336102823249597, 'IoU-128': 6.352279111651833, 'IoU-129': 3.1026290147152156, 'IoU-130': 6.642609865887048, 'IoU-131': 5.982445894779389, 'IoU-132': 3.62600155848539, 'IoU-133': 5.56711573738245, 'IoU-134': 7.222394342493034, 'IoU-135': 4.763966541269006, 'IoU-136': 6.503421461080805, 'IoU-137': 4.641141921952646, 'IoU-138': 5.653642342156809, 'IoU-139': 3.869825844040145, 'IoU-140': 3.9358980483451163, 'IoU-141': 4.440337963390721, 'IoU-142': 4.29336163864823, 'IoU-143': 3.582855854874273, 'IoU-144': 5.452828548443701, 'IoU-145': 5.114842318944853, 'IoU-146': 4.7755507562637325, 'IoU-147': 3.0033235720598257, 'IoU-148': 4.483363949392053, 'IoU-149': 3.476163442916396, 'IoU-150': 2.8978194930519563, 'IoU-151': 3.0528830425742433, 'IoU-152': 3.480385375157908, 'IoU-153': 4.026781229048209, 'IoU-154': 2.9963923639740426, 'IoU-155': 3.3978640815595944, 'IoU-156': 3.0890685396271302, 'IoU-157': 1.5086443993449514, 'IoU-158': 3.567607187956866, 'IoU-159': 2.802763309467039, 'IoU-160': 2.150456418215477, 'IoU-161': 2.9291049737648187, 'IoU-162': 1.9955604289802358, 'IoU-163': 2.76367856710963, 'IoU-164': 0.7053669164977557, 'IoU-165': 1.2646916001808748, 'IoU-166': 2.106580766104872, 'IoU-167': 1.7043034041319265, 'IoU-168': 2.3124402066216567, 'IoU-169': 1.7841758105425674, 'IoU-170': 1.5535670784845943, 'IoU-171': 1.6853153456202161, 'IoU-172': 0.6410315210777524, 'IoU-173': 2.1954410479401116, 'IoU-174': 1.3713069066052785, 'IoU-175': 0.9177996011118895, 'IoU-176': 0.8132968711155307, 'IoU-177': 1.1803595587068025, 'IoU-178': 1.6976872545648674, 'IoU-179': 0.36237828063536504, 'IoU-180': 2.394587578700475, 'IoU-181': 1.0434772498706553, 'IoU-182': 1.4095361430104045, 'IoU-183': 1.8945982044466632, 'IoU-184': 0.8843827896056888, 'IoU-185': 2.04518029926103, 'IoU-186': 3.3617854735766013, 'IoU-187': 1.3235022901285856, 'IoU-188': 1.146783213260114, 'IoU-189': 0.45941610248084697, 'IoU-190': 2.6819365634372834, 'IoU-191': 1.813613843950316, 'mACC': 26.97570913483041, 'pACC': 53.40164023461307, 'ACC-0': nan, 'ACC-1': 98.5980189116985, 'ACC-2': 63.21562119114393, 'ACC-3': 70.67251144422542, 'ACC-4': 70.75202115491118, 'ACC-5': 66.27564732822741, 'ACC-6': 59.21784915415884, 'ACC-7': 47.418240954454134, 'ACC-8': 27.129130869774265, 'ACC-9': 49.78189050263869, 'ACC-10': 59.81464556856281, 'ACC-11': 63.04460569055244, 'ACC-12': 67.53366293759599, 'ACC-13': 64.32727863306573, 'ACC-14': 59.86641223416953, 'ACC-15': 61.40647453469865, 'ACC-16': 62.34617241528068, 'ACC-17': 59.34741133662899, 'ACC-18': 61.21903947781375, 'ACC-19': 60.171392216163625, 'ACC-20': 59.713551622648495, 'ACC-21': 60.48604144674877, 'ACC-22': 65.10014017951588, 'ACC-23': 60.24334576608712, 'ACC-24': 56.87277395525584, 'ACC-25': 55.99502942461923, 'ACC-26': 56.89243735427137, 'ACC-27': 58.235072870999815, 'ACC-28': 57.34033993580406, 'ACC-29': 55.279703392306935, 'ACC-30': 57.83074043653467, 'ACC-31': 55.04004884439833, 'ACC-32': 56.33349485952067, 'ACC-33': 58.61098008461724, 'ACC-34': 57.17031949332235, 'ACC-35': 56.79606473018476, 'ACC-36': 58.76507392247857, 'ACC-37': 57.03055835875691, 'ACC-38': 57.15423866600421, 'ACC-39': 55.66180605428836, 'ACC-40': 58.02633297527448, 'ACC-41': 55.40504881546923, 'ACC-42': 55.401634052882365, 'ACC-43': 53.733492937570745, 'ACC-44': 51.20400450748369, 'ACC-45': 48.34442669434365, 'ACC-46': 48.498813788449, 'ACC-47': 44.70590726155756, 'ACC-48': 43.656344933603656, 'ACC-49': 46.23381105857065, 'ACC-50': 44.924847704741836, 'ACC-51': 41.94720902610438, 'ACC-52': 41.72006534731702, 'ACC-53': 43.85384824053962, 'ACC-54': 44.879108476690845, 'ACC-55': 42.253636224738386, 'ACC-56': 41.28732737881281, 'ACC-57': 39.497910584768036, 'ACC-58': 38.71460604050628, 'ACC-59': 40.435964608327104, 'ACC-60': 34.16798571043808, 'ACC-61': 32.95021789125603, 'ACC-62': 32.08168567909526, 'ACC-63': 29.418532627381882, 'ACC-64': 28.35848316140504, 'ACC-65': 27.117311699417424, 'ACC-66': 27.46857051280217, 'ACC-67': 30.944911731340447, 'ACC-68': 28.610168343332848, 'ACC-69': 28.47921717090672, 'ACC-70': 26.243434566364733, 'ACC-71': 31.055065116348302, 'ACC-72': 28.8129305882594, 'ACC-73': 29.96015812114459, 'ACC-74': 28.121778763561533, 'ACC-75': 31.44215764535528, 'ACC-76': 28.570722019657207, 'ACC-77': 27.32659089207153, 'ACC-78': 29.07624230412667, 'ACC-79': 30.63536706350864, 'ACC-80': 28.366111625369296, 'ACC-81': 26.56848569057425, 'ACC-82': 28.073944377450076, 'ACC-83': 30.17593946586335, 'ACC-84': 27.915678472970377, 'ACC-85': 28.44037535996608, 'ACC-86': 30.131348101224543, 'ACC-87': 24.423209998345875, 'ACC-88': 27.612806019169867, 'ACC-89': 26.775488437820282, 'ACC-90': 28.53066034294814, 'ACC-91': 26.540230638953126, 'ACC-92': 25.901573559748247, 'ACC-93': 27.130408751676327, 'ACC-94': 31.181517013703896, 'ACC-95': 24.3903402598302, 'ACC-96': 24.35002328943396, 'ACC-97': 25.29673441903538, 'ACC-98': 24.57535827833591, 'ACC-99': 23.642131676240563, 'ACC-100': 20.172371988092255, 'ACC-101': 21.02885976088904, 'ACC-102': 19.151835070231083, 'ACC-103': 20.763011240200246, 'ACC-104': 22.320749612650943, 'ACC-105': 18.46631546875523, 'ACC-106': 19.11362076728245, 'ACC-107': 17.84359511594411, 'ACC-108': 20.219670038942336, 'ACC-109': 16.384207698838953, 'ACC-110': 13.149406127782115, 'ACC-111': 13.375396951500228, 'ACC-112': 11.30461069081285, 'ACC-113': 11.557666266140261, 'ACC-114': 18.050457052248962, 'ACC-115': 10.464270142678874, 'ACC-116': 11.768913567398375, 'ACC-117': 13.676589835308564, 'ACC-118': 11.981624866639207, 'ACC-119': 14.694555565706228, 'ACC-120': 9.02119637215938, 'ACC-121': 14.307964064757565, 'ACC-122': 12.580599507486767, 'ACC-123': 12.234960102157425, 'ACC-124': 9.788604875283749, 'ACC-125': 12.079233121193695, 'ACC-126': 7.052090638348255, 'ACC-127': 9.712442411385958, 'ACC-128': 11.666550797606023, 'ACC-129': 4.126110172262213, 'ACC-130': 11.539419001503962, 'ACC-131': 10.702040854302448, 'ACC-132': 5.8755483172649985, 'ACC-133': 8.423115393251546, 'ACC-134': 15.057785437311505, 'ACC-135': 8.107434011743914, 'ACC-136': 11.721953037644429, 'ACC-137': 10.878427008655818, 'ACC-138': 14.654986420285809, 'ACC-139': 7.243515695390788, 'ACC-140': 6.8922305764411025, 'ACC-141': 7.279777964415734, 'ACC-142': 6.534935693382274, 'ACC-143': 7.741813159509836, 'ACC-144': 10.699458483754512, 'ACC-145': 12.463337731165723, 'ACC-146': 9.303420072650843, 'ACC-147': 4.629042426853534, 'ACC-148': 8.574299154698775, 'ACC-149': 6.489201059966391, 'ACC-150': 4.60950439280849, 'ACC-151': 6.51445246263654, 'ACC-152': 6.467795367185919, 'ACC-153': 6.967079829238351, 'ACC-154': 6.421876155821465, 'ACC-155': 6.481118563931145, 'ACC-156': 5.703943583677532, 'ACC-157': 1.9597595316938263, 'ACC-158': 6.319799139370348, 'ACC-159': 4.642785245841213, 'ACC-160': 4.186366104148758, 'ACC-161': 5.103811815983621, 'ACC-162': 2.775058687715895, 'ACC-163': 5.03067259583562, 'ACC-164': 0.8541059554349171, 'ACC-165': 1.6834642517954306, 'ACC-166': 4.173053193543242, 'ACC-167': 3.307988819925408, 'ACC-168': 10.726492721007103, 'ACC-169': 3.633840036638457, 'ACC-170': 2.704899413660434, 'ACC-171': 2.098238640433938, 'ACC-172': 0.7765677667889025, 'ACC-173': 3.8044725741572063, 'ACC-174': 1.6999670072192676, 'ACC-175': 1.1409206125584466, 'ACC-176': 0.9553851288599954, 'ACC-177': 1.550559268014963, 'ACC-178': 2.8993009613471608, 'ACC-179': 0.40334279471109663, 'ACC-180': 5.181770462934742, 'ACC-181': 1.2755192678021987, 'ACC-182': 2.6302509660284916, 'ACC-183': 3.359615398198003, 'ACC-184': 1.3530117896216425, 'ACC-185': 5.2727852223438605, 'ACC-186': 7.017726084893523, 'ACC-187': 1.6818275166406937, 'ACC-188': 1.377819700577055, 'ACC-189': 0.5506763435090791, 'ACC-190': 4.875441483173858, 'ACC-191': 5.040130505709625})])
[01/18 04:59:53] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 04:59:53] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 04:59:53] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 04:59:53] d2.evaluation.testing INFO: copypaste: 2.6074,17.6380,39.3360,26.9757,53.4016
[01/18 04:59:53] d2.utils.events INFO:  eta: 7:15:32  iter: 21999  total_loss: 35.64  loss_ce: 0.2769  loss_mask: 0.3583  loss_dice: 2.831  loss_ce_0: 0.5329  loss_mask_0: 0.3609  loss_dice_0: 2.967  loss_ce_1: 0.3114  loss_mask_1: 0.3655  loss_dice_1: 2.863  loss_ce_2: 0.3081  loss_mask_2: 0.3619  loss_dice_2: 2.841  loss_ce_3: 0.2899  loss_mask_3: 0.3583  loss_dice_3: 2.838  loss_ce_4: 0.281  loss_mask_4: 0.3595  loss_dice_4: 2.838  loss_ce_5: 0.2807  loss_mask_5: 0.3607  loss_dice_5: 2.84  loss_ce_6: 0.2674  loss_mask_6: 0.3612  loss_dice_6: 2.839  loss_ce_7: 0.2738  loss_mask_7: 0.3592  loss_dice_7: 2.834  loss_ce_8: 0.2746  loss_mask_8: 0.3597  loss_dice_8: 2.838  time: 1.4922  data_time: 0.0721  lr: 4.8743e-06  max_mem: 21589M
[01/18 05:00:22] d2.utils.events INFO:  eta: 7:14:26  iter: 22019  total_loss: 36.21  loss_ce: 0.2667  loss_mask: 0.374  loss_dice: 2.872  loss_ce_0: 0.545  loss_mask_0: 0.3762  loss_dice_0: 3.003  loss_ce_1: 0.3091  loss_mask_1: 0.377  loss_dice_1: 2.919  loss_ce_2: 0.2988  loss_mask_2: 0.3731  loss_dice_2: 2.899  loss_ce_3: 0.2937  loss_mask_3: 0.3734  loss_dice_3: 2.881  loss_ce_4: 0.2853  loss_mask_4: 0.3716  loss_dice_4: 2.881  loss_ce_5: 0.2797  loss_mask_5: 0.3733  loss_dice_5: 2.883  loss_ce_6: 0.2831  loss_mask_6: 0.3737  loss_dice_6: 2.88  loss_ce_7: 0.263  loss_mask_7: 0.3738  loss_dice_7: 2.877  loss_ce_8: 0.2735  loss_mask_8: 0.3737  loss_dice_8: 2.872  time: 1.4922  data_time: 0.0700  lr: 4.8694e-06  max_mem: 21589M
[01/18 05:00:50] d2.utils.events INFO:  eta: 7:13:33  iter: 22039  total_loss: 34.95  loss_ce: 0.2664  loss_mask: 0.3714  loss_dice: 2.805  loss_ce_0: 0.5691  loss_mask_0: 0.366  loss_dice_0: 2.944  loss_ce_1: 0.3035  loss_mask_1: 0.3758  loss_dice_1: 2.852  loss_ce_2: 0.29  loss_mask_2: 0.3734  loss_dice_2: 2.832  loss_ce_3: 0.2779  loss_mask_3: 0.3761  loss_dice_3: 2.813  loss_ce_4: 0.2738  loss_mask_4: 0.3723  loss_dice_4: 2.812  loss_ce_5: 0.2691  loss_mask_5: 0.373  loss_dice_5: 2.811  loss_ce_6: 0.2793  loss_mask_6: 0.3718  loss_dice_6: 2.803  loss_ce_7: 0.2711  loss_mask_7: 0.371  loss_dice_7: 2.804  loss_ce_8: 0.2692  loss_mask_8: 0.3718  loss_dice_8: 2.809  time: 1.4921  data_time: 0.0649  lr: 4.8646e-06  max_mem: 21589M
[01/18 05:01:19] d2.utils.events INFO:  eta: 7:13:03  iter: 22059  total_loss: 35.42  loss_ce: 0.2934  loss_mask: 0.3687  loss_dice: 2.819  loss_ce_0: 0.5433  loss_mask_0: 0.3652  loss_dice_0: 2.928  loss_ce_1: 0.32  loss_mask_1: 0.3739  loss_dice_1: 2.846  loss_ce_2: 0.2999  loss_mask_2: 0.3747  loss_dice_2: 2.831  loss_ce_3: 0.3009  loss_mask_3: 0.3696  loss_dice_3: 2.817  loss_ce_4: 0.2982  loss_mask_4: 0.3701  loss_dice_4: 2.807  loss_ce_5: 0.2852  loss_mask_5: 0.3704  loss_dice_5: 2.82  loss_ce_6: 0.2949  loss_mask_6: 0.3707  loss_dice_6: 2.815  loss_ce_7: 0.2829  loss_mask_7: 0.3686  loss_dice_7: 2.815  loss_ce_8: 0.2797  loss_mask_8: 0.367  loss_dice_8: 2.826  time: 1.4921  data_time: 0.0749  lr: 4.8597e-06  max_mem: 21589M
[01/18 05:01:49] d2.utils.events INFO:  eta: 7:12:37  iter: 22079  total_loss: 35.63  loss_ce: 0.2823  loss_mask: 0.3803  loss_dice: 2.824  loss_ce_0: 0.565  loss_mask_0: 0.374  loss_dice_0: 2.957  loss_ce_1: 0.3224  loss_mask_1: 0.3867  loss_dice_1: 2.866  loss_ce_2: 0.3251  loss_mask_2: 0.3785  loss_dice_2: 2.843  loss_ce_3: 0.313  loss_mask_3: 0.3775  loss_dice_3: 2.833  loss_ce_4: 0.2845  loss_mask_4: 0.3793  loss_dice_4: 2.835  loss_ce_5: 0.2996  loss_mask_5: 0.3788  loss_dice_5: 2.841  loss_ce_6: 0.2892  loss_mask_6: 0.3804  loss_dice_6: 2.827  loss_ce_7: 0.2926  loss_mask_7: 0.3804  loss_dice_7: 2.83  loss_ce_8: 0.302  loss_mask_8: 0.3806  loss_dice_8: 2.829  time: 1.4921  data_time: 0.0717  lr: 4.8548e-06  max_mem: 21589M
[01/18 05:02:18] d2.utils.events INFO:  eta: 7:12:14  iter: 22099  total_loss: 35.57  loss_ce: 0.2639  loss_mask: 0.3785  loss_dice: 2.836  loss_ce_0: 0.519  loss_mask_0: 0.3744  loss_dice_0: 2.969  loss_ce_1: 0.3067  loss_mask_1: 0.3827  loss_dice_1: 2.883  loss_ce_2: 0.2929  loss_mask_2: 0.3823  loss_dice_2: 2.851  loss_ce_3: 0.2802  loss_mask_3: 0.3775  loss_dice_3: 2.845  loss_ce_4: 0.2719  loss_mask_4: 0.379  loss_dice_4: 2.832  loss_ce_5: 0.2631  loss_mask_5: 0.3783  loss_dice_5: 2.842  loss_ce_6: 0.2634  loss_mask_6: 0.3777  loss_dice_6: 2.833  loss_ce_7: 0.2596  loss_mask_7: 0.3782  loss_dice_7: 2.838  loss_ce_8: 0.2492  loss_mask_8: 0.38  loss_dice_8: 2.842  time: 1.4920  data_time: 0.0699  lr: 4.8499e-06  max_mem: 21589M
[01/18 05:02:47] d2.utils.events INFO:  eta: 7:11:44  iter: 22119  total_loss: 35.7  loss_ce: 0.3113  loss_mask: 0.3704  loss_dice: 2.853  loss_ce_0: 0.5789  loss_mask_0: 0.3663  loss_dice_0: 2.982  loss_ce_1: 0.3623  loss_mask_1: 0.3705  loss_dice_1: 2.894  loss_ce_2: 0.3307  loss_mask_2: 0.3681  loss_dice_2: 2.872  loss_ce_3: 0.3002  loss_mask_3: 0.3701  loss_dice_3: 2.859  loss_ce_4: 0.3157  loss_mask_4: 0.3702  loss_dice_4: 2.853  loss_ce_5: 0.3068  loss_mask_5: 0.3702  loss_dice_5: 2.855  loss_ce_6: 0.2997  loss_mask_6: 0.3709  loss_dice_6: 2.849  loss_ce_7: 0.3036  loss_mask_7: 0.3693  loss_dice_7: 2.851  loss_ce_8: 0.3137  loss_mask_8: 0.3699  loss_dice_8: 2.849  time: 1.4920  data_time: 0.0737  lr: 4.8451e-06  max_mem: 21589M
[01/18 05:03:16] d2.utils.events INFO:  eta: 7:11:16  iter: 22139  total_loss: 35.22  loss_ce: 0.2712  loss_mask: 0.373  loss_dice: 2.828  loss_ce_0: 0.5643  loss_mask_0: 0.369  loss_dice_0: 2.947  loss_ce_1: 0.3113  loss_mask_1: 0.3733  loss_dice_1: 2.864  loss_ce_2: 0.304  loss_mask_2: 0.3732  loss_dice_2: 2.851  loss_ce_3: 0.2998  loss_mask_3: 0.3737  loss_dice_3: 2.841  loss_ce_4: 0.2814  loss_mask_4: 0.3726  loss_dice_4: 2.831  loss_ce_5: 0.2761  loss_mask_5: 0.3731  loss_dice_5: 2.837  loss_ce_6: 0.2775  loss_mask_6: 0.3745  loss_dice_6: 2.827  loss_ce_7: 0.2749  loss_mask_7: 0.3748  loss_dice_7: 2.836  loss_ce_8: 0.2729  loss_mask_8: 0.3736  loss_dice_8: 2.833  time: 1.4919  data_time: 0.0800  lr: 4.8402e-06  max_mem: 21589M
[01/18 05:03:45] d2.utils.events INFO:  eta: 7:10:46  iter: 22159  total_loss: 35.8  loss_ce: 0.2664  loss_mask: 0.3808  loss_dice: 2.869  loss_ce_0: 0.5498  loss_mask_0: 0.374  loss_dice_0: 3.022  loss_ce_1: 0.3032  loss_mask_1: 0.3853  loss_dice_1: 2.928  loss_ce_2: 0.2935  loss_mask_2: 0.3829  loss_dice_2: 2.887  loss_ce_3: 0.2739  loss_mask_3: 0.3825  loss_dice_3: 2.881  loss_ce_4: 0.2664  loss_mask_4: 0.38  loss_dice_4: 2.867  loss_ce_5: 0.2658  loss_mask_5: 0.3808  loss_dice_5: 2.874  loss_ce_6: 0.275  loss_mask_6: 0.3788  loss_dice_6: 2.874  loss_ce_7: 0.2631  loss_mask_7: 0.3801  loss_dice_7: 2.881  loss_ce_8: 0.2704  loss_mask_8: 0.3792  loss_dice_8: 2.87  time: 1.4919  data_time: 0.0709  lr: 4.8353e-06  max_mem: 21589M
[01/18 05:04:14] d2.utils.events INFO:  eta: 7:10:18  iter: 22179  total_loss: 35.98  loss_ce: 0.271  loss_mask: 0.3704  loss_dice: 2.861  loss_ce_0: 0.573  loss_mask_0: 0.3704  loss_dice_0: 2.999  loss_ce_1: 0.3  loss_mask_1: 0.3757  loss_dice_1: 2.904  loss_ce_2: 0.3128  loss_mask_2: 0.3742  loss_dice_2: 2.871  loss_ce_3: 0.2945  loss_mask_3: 0.3721  loss_dice_3: 2.862  loss_ce_4: 0.2923  loss_mask_4: 0.3719  loss_dice_4: 2.855  loss_ce_5: 0.2903  loss_mask_5: 0.3728  loss_dice_5: 2.864  loss_ce_6: 0.2986  loss_mask_6: 0.3724  loss_dice_6: 2.859  loss_ce_7: 0.2925  loss_mask_7: 0.3694  loss_dice_7: 2.851  loss_ce_8: 0.2823  loss_mask_8: 0.3706  loss_dice_8: 2.867  time: 1.4919  data_time: 0.0705  lr: 4.8304e-06  max_mem: 21589M
[01/18 05:04:43] d2.utils.events INFO:  eta: 7:09:43  iter: 22199  total_loss: 35.15  loss_ce: 0.2733  loss_mask: 0.3713  loss_dice: 2.802  loss_ce_0: 0.5595  loss_mask_0: 0.3698  loss_dice_0: 2.932  loss_ce_1: 0.2968  loss_mask_1: 0.3731  loss_dice_1: 2.841  loss_ce_2: 0.2952  loss_mask_2: 0.371  loss_dice_2: 2.83  loss_ce_3: 0.2893  loss_mask_3: 0.37  loss_dice_3: 2.814  loss_ce_4: 0.2755  loss_mask_4: 0.3722  loss_dice_4: 2.818  loss_ce_5: 0.2699  loss_mask_5: 0.3708  loss_dice_5: 2.816  loss_ce_6: 0.2658  loss_mask_6: 0.3709  loss_dice_6: 2.802  loss_ce_7: 0.2583  loss_mask_7: 0.3708  loss_dice_7: 2.812  loss_ce_8: 0.2621  loss_mask_8: 0.3697  loss_dice_8: 2.809  time: 1.4918  data_time: 0.0674  lr: 4.8255e-06  max_mem: 21589M
[01/18 05:05:12] d2.utils.events INFO:  eta: 7:09:05  iter: 22219  total_loss: 35.53  loss_ce: 0.2854  loss_mask: 0.3685  loss_dice: 2.805  loss_ce_0: 0.5885  loss_mask_0: 0.3667  loss_dice_0: 2.942  loss_ce_1: 0.326  loss_mask_1: 0.3742  loss_dice_1: 2.855  loss_ce_2: 0.311  loss_mask_2: 0.3709  loss_dice_2: 2.828  loss_ce_3: 0.31  loss_mask_3: 0.3705  loss_dice_3: 2.809  loss_ce_4: 0.2904  loss_mask_4: 0.3696  loss_dice_4: 2.807  loss_ce_5: 0.2828  loss_mask_5: 0.3693  loss_dice_5: 2.81  loss_ce_6: 0.2744  loss_mask_6: 0.3702  loss_dice_6: 2.811  loss_ce_7: 0.2795  loss_mask_7: 0.3707  loss_dice_7: 2.807  loss_ce_8: 0.2895  loss_mask_8: 0.3705  loss_dice_8: 2.804  time: 1.4918  data_time: 0.0686  lr: 4.8207e-06  max_mem: 21589M
[01/18 05:05:41] d2.utils.events INFO:  eta: 7:08:08  iter: 22239  total_loss: 34.99  loss_ce: 0.2911  loss_mask: 0.3742  loss_dice: 2.766  loss_ce_0: 0.5805  loss_mask_0: 0.3723  loss_dice_0: 2.911  loss_ce_1: 0.3385  loss_mask_1: 0.3797  loss_dice_1: 2.8  loss_ce_2: 0.3285  loss_mask_2: 0.3756  loss_dice_2: 2.779  loss_ce_3: 0.3012  loss_mask_3: 0.3719  loss_dice_3: 2.769  loss_ce_4: 0.2884  loss_mask_4: 0.3723  loss_dice_4: 2.765  loss_ce_5: 0.295  loss_mask_5: 0.3738  loss_dice_5: 2.77  loss_ce_6: 0.2903  loss_mask_6: 0.3734  loss_dice_6: 2.772  loss_ce_7: 0.2939  loss_mask_7: 0.3744  loss_dice_7: 2.768  loss_ce_8: 0.2916  loss_mask_8: 0.3756  loss_dice_8: 2.766  time: 1.4917  data_time: 0.0700  lr: 4.8158e-06  max_mem: 21589M
[01/18 05:06:10] d2.utils.events INFO:  eta: 7:07:58  iter: 22259  total_loss: 36.36  loss_ce: 0.2854  loss_mask: 0.3863  loss_dice: 2.91  loss_ce_0: 0.5609  loss_mask_0: 0.3768  loss_dice_0: 3.056  loss_ce_1: 0.3261  loss_mask_1: 0.3844  loss_dice_1: 2.959  loss_ce_2: 0.312  loss_mask_2: 0.384  loss_dice_2: 2.931  loss_ce_3: 0.2897  loss_mask_3: 0.3841  loss_dice_3: 2.924  loss_ce_4: 0.2942  loss_mask_4: 0.3826  loss_dice_4: 2.917  loss_ce_5: 0.2843  loss_mask_5: 0.3825  loss_dice_5: 2.926  loss_ce_6: 0.2916  loss_mask_6: 0.3837  loss_dice_6: 2.916  loss_ce_7: 0.275  loss_mask_7: 0.3836  loss_dice_7: 2.912  loss_ce_8: 0.2815  loss_mask_8: 0.3839  loss_dice_8: 2.915  time: 1.4917  data_time: 0.0708  lr: 4.8109e-06  max_mem: 21589M
[01/18 05:06:39] d2.utils.events INFO:  eta: 7:07:33  iter: 22279  total_loss: 36.22  loss_ce: 0.2789  loss_mask: 0.369  loss_dice: 2.943  loss_ce_0: 0.5606  loss_mask_0: 0.3721  loss_dice_0: 3.07  loss_ce_1: 0.3326  loss_mask_1: 0.376  loss_dice_1: 2.992  loss_ce_2: 0.3264  loss_mask_2: 0.3725  loss_dice_2: 2.968  loss_ce_3: 0.3052  loss_mask_3: 0.3708  loss_dice_3: 2.946  loss_ce_4: 0.2955  loss_mask_4: 0.3688  loss_dice_4: 2.952  loss_ce_5: 0.3002  loss_mask_5: 0.3686  loss_dice_5: 2.942  loss_ce_6: 0.3011  loss_mask_6: 0.3682  loss_dice_6: 2.937  loss_ce_7: 0.2731  loss_mask_7: 0.3685  loss_dice_7: 2.942  loss_ce_8: 0.2878  loss_mask_8: 0.3697  loss_dice_8: 2.948  time: 1.4917  data_time: 0.0678  lr: 4.806e-06  max_mem: 21589M
[01/18 05:07:08] d2.utils.events INFO:  eta: 7:07:00  iter: 22299  total_loss: 35.7  loss_ce: 0.2738  loss_mask: 0.3801  loss_dice: 2.871  loss_ce_0: 0.5695  loss_mask_0: 0.3821  loss_dice_0: 2.983  loss_ce_1: 0.318  loss_mask_1: 0.3912  loss_dice_1: 2.911  loss_ce_2: 0.2998  loss_mask_2: 0.385  loss_dice_2: 2.89  loss_ce_3: 0.2878  loss_mask_3: 0.3819  loss_dice_3: 2.872  loss_ce_4: 0.2882  loss_mask_4: 0.3801  loss_dice_4: 2.881  loss_ce_5: 0.2814  loss_mask_5: 0.3801  loss_dice_5: 2.879  loss_ce_6: 0.267  loss_mask_6: 0.3787  loss_dice_6: 2.877  loss_ce_7: 0.275  loss_mask_7: 0.3786  loss_dice_7: 2.874  loss_ce_8: 0.2755  loss_mask_8: 0.3791  loss_dice_8: 2.873  time: 1.4916  data_time: 0.0728  lr: 4.8011e-06  max_mem: 21589M
[01/18 05:07:37] d2.utils.events INFO:  eta: 7:06:38  iter: 22319  total_loss: 35.92  loss_ce: 0.2628  loss_mask: 0.3798  loss_dice: 2.873  loss_ce_0: 0.5694  loss_mask_0: 0.3782  loss_dice_0: 3.001  loss_ce_1: 0.3068  loss_mask_1: 0.388  loss_dice_1: 2.918  loss_ce_2: 0.2996  loss_mask_2: 0.3816  loss_dice_2: 2.901  loss_ce_3: 0.2859  loss_mask_3: 0.379  loss_dice_3: 2.877  loss_ce_4: 0.2697  loss_mask_4: 0.3759  loss_dice_4: 2.882  loss_ce_5: 0.271  loss_mask_5: 0.3769  loss_dice_5: 2.875  loss_ce_6: 0.2658  loss_mask_6: 0.3786  loss_dice_6: 2.872  loss_ce_7: 0.2619  loss_mask_7: 0.3775  loss_dice_7: 2.877  loss_ce_8: 0.2576  loss_mask_8: 0.3799  loss_dice_8: 2.871  time: 1.4916  data_time: 0.0708  lr: 4.7963e-06  max_mem: 21589M
[01/18 05:08:05] d2.utils.events INFO:  eta: 7:05:57  iter: 22339  total_loss: 34.94  loss_ce: 0.2717  loss_mask: 0.3676  loss_dice: 2.787  loss_ce_0: 0.5688  loss_mask_0: 0.3677  loss_dice_0: 2.929  loss_ce_1: 0.3215  loss_mask_1: 0.3765  loss_dice_1: 2.844  loss_ce_2: 0.3067  loss_mask_2: 0.3724  loss_dice_2: 2.813  loss_ce_3: 0.2961  loss_mask_3: 0.371  loss_dice_3: 2.79  loss_ce_4: 0.2818  loss_mask_4: 0.3681  loss_dice_4: 2.799  loss_ce_5: 0.273  loss_mask_5: 0.3689  loss_dice_5: 2.793  loss_ce_6: 0.2709  loss_mask_6: 0.3675  loss_dice_6: 2.788  loss_ce_7: 0.2673  loss_mask_7: 0.3682  loss_dice_7: 2.79  loss_ce_8: 0.261  loss_mask_8: 0.3693  loss_dice_8: 2.784  time: 1.4915  data_time: 0.0661  lr: 4.7914e-06  max_mem: 21589M
[01/18 05:08:35] d2.utils.events INFO:  eta: 7:05:37  iter: 22359  total_loss: 35.31  loss_ce: 0.2711  loss_mask: 0.3728  loss_dice: 2.817  loss_ce_0: 0.5663  loss_mask_0: 0.3676  loss_dice_0: 2.935  loss_ce_1: 0.2915  loss_mask_1: 0.3736  loss_dice_1: 2.856  loss_ce_2: 0.2955  loss_mask_2: 0.3741  loss_dice_2: 2.831  loss_ce_3: 0.2836  loss_mask_3: 0.3709  loss_dice_3: 2.823  loss_ce_4: 0.277  loss_mask_4: 0.3723  loss_dice_4: 2.819  loss_ce_5: 0.2672  loss_mask_5: 0.3729  loss_dice_5: 2.819  loss_ce_6: 0.2786  loss_mask_6: 0.3728  loss_dice_6: 2.817  loss_ce_7: 0.2789  loss_mask_7: 0.3718  loss_dice_7: 2.819  loss_ce_8: 0.2685  loss_mask_8: 0.3714  loss_dice_8: 2.815  time: 1.4915  data_time: 0.0713  lr: 4.7865e-06  max_mem: 21589M
[01/18 05:09:04] d2.utils.events INFO:  eta: 7:05:17  iter: 22379  total_loss: 35.67  loss_ce: 0.283  loss_mask: 0.3692  loss_dice: 2.866  loss_ce_0: 0.5635  loss_mask_0: 0.3644  loss_dice_0: 2.984  loss_ce_1: 0.296  loss_mask_1: 0.373  loss_dice_1: 2.903  loss_ce_2: 0.3058  loss_mask_2: 0.372  loss_dice_2: 2.876  loss_ce_3: 0.3061  loss_mask_3: 0.3724  loss_dice_3: 2.869  loss_ce_4: 0.2855  loss_mask_4: 0.3717  loss_dice_4: 2.863  loss_ce_5: 0.2938  loss_mask_5: 0.3713  loss_dice_5: 2.865  loss_ce_6: 0.2835  loss_mask_6: 0.3703  loss_dice_6: 2.867  loss_ce_7: 0.2845  loss_mask_7: 0.3704  loss_dice_7: 2.869  loss_ce_8: 0.2725  loss_mask_8: 0.3697  loss_dice_8: 2.872  time: 1.4915  data_time: 0.0734  lr: 4.7816e-06  max_mem: 21589M
[01/18 05:09:34] d2.utils.events INFO:  eta: 7:04:48  iter: 22399  total_loss: 35.26  loss_ce: 0.2975  loss_mask: 0.3739  loss_dice: 2.801  loss_ce_0: 0.5741  loss_mask_0: 0.3676  loss_dice_0: 2.924  loss_ce_1: 0.3399  loss_mask_1: 0.3799  loss_dice_1: 2.84  loss_ce_2: 0.3228  loss_mask_2: 0.3769  loss_dice_2: 2.824  loss_ce_3: 0.3135  loss_mask_3: 0.3754  loss_dice_3: 2.809  loss_ce_4: 0.2938  loss_mask_4: 0.3743  loss_dice_4: 2.799  loss_ce_5: 0.2943  loss_mask_5: 0.3722  loss_dice_5: 2.815  loss_ce_6: 0.2971  loss_mask_6: 0.3728  loss_dice_6: 2.81  loss_ce_7: 0.3104  loss_mask_7: 0.3743  loss_dice_7: 2.802  loss_ce_8: 0.2923  loss_mask_8: 0.3749  loss_dice_8: 2.803  time: 1.4915  data_time: 0.0760  lr: 4.7767e-06  max_mem: 21589M
[01/18 05:10:03] d2.utils.events INFO:  eta: 7:04:29  iter: 22419  total_loss: 35.46  loss_ce: 0.2544  loss_mask: 0.3713  loss_dice: 2.814  loss_ce_0: 0.5659  loss_mask_0: 0.3662  loss_dice_0: 2.95  loss_ce_1: 0.31  loss_mask_1: 0.3791  loss_dice_1: 2.853  loss_ce_2: 0.2961  loss_mask_2: 0.3744  loss_dice_2: 2.837  loss_ce_3: 0.2815  loss_mask_3: 0.3733  loss_dice_3: 2.82  loss_ce_4: 0.264  loss_mask_4: 0.3733  loss_dice_4: 2.818  loss_ce_5: 0.2834  loss_mask_5: 0.3718  loss_dice_5: 2.806  loss_ce_6: 0.2706  loss_mask_6: 0.3745  loss_dice_6: 2.8  loss_ce_7: 0.2575  loss_mask_7: 0.3745  loss_dice_7: 2.808  loss_ce_8: 0.2526  loss_mask_8: 0.3712  loss_dice_8: 2.819  time: 1.4915  data_time: 0.0714  lr: 4.7718e-06  max_mem: 21589M
[01/18 05:10:33] d2.utils.events INFO:  eta: 7:04:11  iter: 22439  total_loss: 34.97  loss_ce: 0.2796  loss_mask: 0.3657  loss_dice: 2.786  loss_ce_0: 0.5687  loss_mask_0: 0.3607  loss_dice_0: 2.92  loss_ce_1: 0.3099  loss_mask_1: 0.3683  loss_dice_1: 2.833  loss_ce_2: 0.307  loss_mask_2: 0.3642  loss_dice_2: 2.812  loss_ce_3: 0.2848  loss_mask_3: 0.3625  loss_dice_3: 2.796  loss_ce_4: 0.2756  loss_mask_4: 0.365  loss_dice_4: 2.796  loss_ce_5: 0.2839  loss_mask_5: 0.3632  loss_dice_5: 2.798  loss_ce_6: 0.2722  loss_mask_6: 0.3642  loss_dice_6: 2.794  loss_ce_7: 0.2731  loss_mask_7: 0.3642  loss_dice_7: 2.787  loss_ce_8: 0.276  loss_mask_8: 0.3653  loss_dice_8: 2.784  time: 1.4914  data_time: 0.0722  lr: 4.7669e-06  max_mem: 21589M
[01/18 05:11:02] d2.utils.events INFO:  eta: 7:03:42  iter: 22459  total_loss: 35.58  loss_ce: 0.2791  loss_mask: 0.3676  loss_dice: 2.851  loss_ce_0: 0.5652  loss_mask_0: 0.3673  loss_dice_0: 2.976  loss_ce_1: 0.3129  loss_mask_1: 0.3748  loss_dice_1: 2.897  loss_ce_2: 0.3235  loss_mask_2: 0.3721  loss_dice_2: 2.871  loss_ce_3: 0.3046  loss_mask_3: 0.3691  loss_dice_3: 2.85  loss_ce_4: 0.2933  loss_mask_4: 0.3681  loss_dice_4: 2.852  loss_ce_5: 0.2941  loss_mask_5: 0.3682  loss_dice_5: 2.849  loss_ce_6: 0.2794  loss_mask_6: 0.3664  loss_dice_6: 2.85  loss_ce_7: 0.272  loss_mask_7: 0.367  loss_dice_7: 2.848  loss_ce_8: 0.2767  loss_mask_8: 0.3666  loss_dice_8: 2.856  time: 1.4914  data_time: 0.0770  lr: 4.7621e-06  max_mem: 21589M
[01/18 05:11:33] d2.utils.events INFO:  eta: 7:03:46  iter: 22479  total_loss: 34.91  loss_ce: 0.2656  loss_mask: 0.3664  loss_dice: 2.804  loss_ce_0: 0.544  loss_mask_0: 0.3637  loss_dice_0: 2.933  loss_ce_1: 0.3161  loss_mask_1: 0.3736  loss_dice_1: 2.842  loss_ce_2: 0.3055  loss_mask_2: 0.3703  loss_dice_2: 2.819  loss_ce_3: 0.2857  loss_mask_3: 0.3686  loss_dice_3: 2.811  loss_ce_4: 0.2818  loss_mask_4: 0.3665  loss_dice_4: 2.804  loss_ce_5: 0.2718  loss_mask_5: 0.3661  loss_dice_5: 2.803  loss_ce_6: 0.2804  loss_mask_6: 0.368  loss_dice_6: 2.802  loss_ce_7: 0.2705  loss_mask_7: 0.3667  loss_dice_7: 2.801  loss_ce_8: 0.258  loss_mask_8: 0.3671  loss_dice_8: 2.805  time: 1.4915  data_time: 0.0808  lr: 4.7572e-06  max_mem: 21589M
[01/18 05:12:02] d2.utils.events INFO:  eta: 7:03:20  iter: 22499  total_loss: 35.09  loss_ce: 0.2953  loss_mask: 0.3703  loss_dice: 2.813  loss_ce_0: 0.5592  loss_mask_0: 0.3659  loss_dice_0: 2.974  loss_ce_1: 0.3247  loss_mask_1: 0.3716  loss_dice_1: 2.875  loss_ce_2: 0.315  loss_mask_2: 0.3695  loss_dice_2: 2.845  loss_ce_3: 0.3018  loss_mask_3: 0.3701  loss_dice_3: 2.829  loss_ce_4: 0.2816  loss_mask_4: 0.3686  loss_dice_4: 2.833  loss_ce_5: 0.2776  loss_mask_5: 0.3699  loss_dice_5: 2.829  loss_ce_6: 0.2668  loss_mask_6: 0.3686  loss_dice_6: 2.835  loss_ce_7: 0.2772  loss_mask_7: 0.3701  loss_dice_7: 2.829  loss_ce_8: 0.2839  loss_mask_8: 0.3686  loss_dice_8: 2.821  time: 1.4914  data_time: 0.0712  lr: 4.7523e-06  max_mem: 21589M
[01/18 05:12:31] d2.utils.events INFO:  eta: 7:02:54  iter: 22519  total_loss: 35.9  loss_ce: 0.2697  loss_mask: 0.3842  loss_dice: 2.879  loss_ce_0: 0.5608  loss_mask_0: 0.3925  loss_dice_0: 3.005  loss_ce_1: 0.3203  loss_mask_1: 0.3976  loss_dice_1: 2.912  loss_ce_2: 0.3131  loss_mask_2: 0.3906  loss_dice_2: 2.898  loss_ce_3: 0.2984  loss_mask_3: 0.3843  loss_dice_3: 2.884  loss_ce_4: 0.2936  loss_mask_4: 0.3838  loss_dice_4: 2.883  loss_ce_5: 0.2935  loss_mask_5: 0.3845  loss_dice_5: 2.883  loss_ce_6: 0.284  loss_mask_6: 0.3821  loss_dice_6: 2.879  loss_ce_7: 0.2793  loss_mask_7: 0.3841  loss_dice_7: 2.889  loss_ce_8: 0.267  loss_mask_8: 0.3828  loss_dice_8: 2.891  time: 1.4914  data_time: 0.0764  lr: 4.7474e-06  max_mem: 21589M
[01/18 05:13:00] d2.utils.events INFO:  eta: 7:02:20  iter: 22539  total_loss: 34.95  loss_ce: 0.2592  loss_mask: 0.3641  loss_dice: 2.799  loss_ce_0: 0.5476  loss_mask_0: 0.3586  loss_dice_0: 2.925  loss_ce_1: 0.2881  loss_mask_1: 0.3705  loss_dice_1: 2.844  loss_ce_2: 0.274  loss_mask_2: 0.3688  loss_dice_2: 2.819  loss_ce_3: 0.2655  loss_mask_3: 0.3668  loss_dice_3: 2.807  loss_ce_4: 0.2704  loss_mask_4: 0.3673  loss_dice_4: 2.805  loss_ce_5: 0.2641  loss_mask_5: 0.3651  loss_dice_5: 2.81  loss_ce_6: 0.2584  loss_mask_6: 0.3649  loss_dice_6: 2.805  loss_ce_7: 0.2619  loss_mask_7: 0.3642  loss_dice_7: 2.804  loss_ce_8: 0.2508  loss_mask_8: 0.3642  loss_dice_8: 2.807  time: 1.4913  data_time: 0.0655  lr: 4.7425e-06  max_mem: 21589M
[01/18 05:13:29] d2.utils.events INFO:  eta: 7:01:48  iter: 22559  total_loss: 34.86  loss_ce: 0.2536  loss_mask: 0.3716  loss_dice: 2.824  loss_ce_0: 0.5795  loss_mask_0: 0.3753  loss_dice_0: 2.96  loss_ce_1: 0.3  loss_mask_1: 0.3773  loss_dice_1: 2.868  loss_ce_2: 0.2796  loss_mask_2: 0.3741  loss_dice_2: 2.849  loss_ce_3: 0.2865  loss_mask_3: 0.3713  loss_dice_3: 2.835  loss_ce_4: 0.273  loss_mask_4: 0.3709  loss_dice_4: 2.841  loss_ce_5: 0.2617  loss_mask_5: 0.3732  loss_dice_5: 2.833  loss_ce_6: 0.2611  loss_mask_6: 0.3719  loss_dice_6: 2.831  loss_ce_7: 0.2539  loss_mask_7: 0.3712  loss_dice_7: 2.828  loss_ce_8: 0.2588  loss_mask_8: 0.3721  loss_dice_8: 2.822  time: 1.4913  data_time: 0.0779  lr: 4.7376e-06  max_mem: 21589M
[01/18 05:13:59] d2.utils.events INFO:  eta: 7:01:22  iter: 22579  total_loss: 35.22  loss_ce: 0.2661  loss_mask: 0.3669  loss_dice: 2.83  loss_ce_0: 0.5379  loss_mask_0: 0.3614  loss_dice_0: 2.958  loss_ce_1: 0.3162  loss_mask_1: 0.3703  loss_dice_1: 2.878  loss_ce_2: 0.2865  loss_mask_2: 0.3696  loss_dice_2: 2.849  loss_ce_3: 0.2965  loss_mask_3: 0.3691  loss_dice_3: 2.828  loss_ce_4: 0.2737  loss_mask_4: 0.366  loss_dice_4: 2.832  loss_ce_5: 0.2766  loss_mask_5: 0.3669  loss_dice_5: 2.825  loss_ce_6: 0.2656  loss_mask_6: 0.3666  loss_dice_6: 2.826  loss_ce_7: 0.2729  loss_mask_7: 0.3653  loss_dice_7: 2.825  loss_ce_8: 0.2625  loss_mask_8: 0.3665  loss_dice_8: 2.823  time: 1.4913  data_time: 0.0734  lr: 4.7327e-06  max_mem: 21589M
[01/18 05:14:28] d2.utils.events INFO:  eta: 7:00:57  iter: 22599  total_loss: 35.16  loss_ce: 0.2808  loss_mask: 0.3687  loss_dice: 2.804  loss_ce_0: 0.5525  loss_mask_0: 0.3626  loss_dice_0: 2.934  loss_ce_1: 0.3018  loss_mask_1: 0.3716  loss_dice_1: 2.849  loss_ce_2: 0.2984  loss_mask_2: 0.367  loss_dice_2: 2.826  loss_ce_3: 0.2889  loss_mask_3: 0.3683  loss_dice_3: 2.815  loss_ce_4: 0.2647  loss_mask_4: 0.3676  loss_dice_4: 2.811  loss_ce_5: 0.274  loss_mask_5: 0.3673  loss_dice_5: 2.81  loss_ce_6: 0.2769  loss_mask_6: 0.3689  loss_dice_6: 2.803  loss_ce_7: 0.2685  loss_mask_7: 0.3691  loss_dice_7: 2.814  loss_ce_8: 0.2619  loss_mask_8: 0.3678  loss_dice_8: 2.811  time: 1.4913  data_time: 0.0746  lr: 4.7278e-06  max_mem: 21589M
[01/18 05:14:57] d2.utils.events INFO:  eta: 7:00:29  iter: 22619  total_loss: 35.55  loss_ce: 0.2894  loss_mask: 0.3608  loss_dice: 2.825  loss_ce_0: 0.608  loss_mask_0: 0.3573  loss_dice_0: 2.971  loss_ce_1: 0.3337  loss_mask_1: 0.3613  loss_dice_1: 2.877  loss_ce_2: 0.3142  loss_mask_2: 0.3602  loss_dice_2: 2.855  loss_ce_3: 0.3016  loss_mask_3: 0.3616  loss_dice_3: 2.83  loss_ce_4: 0.2921  loss_mask_4: 0.3603  loss_dice_4: 2.836  loss_ce_5: 0.2968  loss_mask_5: 0.3619  loss_dice_5: 2.829  loss_ce_6: 0.2957  loss_mask_6: 0.3614  loss_dice_6: 2.817  loss_ce_7: 0.2923  loss_mask_7: 0.3617  loss_dice_7: 2.831  loss_ce_8: 0.2825  loss_mask_8: 0.3596  loss_dice_8: 2.825  time: 1.4912  data_time: 0.0683  lr: 4.7229e-06  max_mem: 21589M
[01/18 05:15:27] d2.utils.events INFO:  eta: 7:00:07  iter: 22639  total_loss: 35.93  loss_ce: 0.282  loss_mask: 0.3711  loss_dice: 2.836  loss_ce_0: 0.5776  loss_mask_0: 0.3651  loss_dice_0: 2.983  loss_ce_1: 0.3022  loss_mask_1: 0.3809  loss_dice_1: 2.884  loss_ce_2: 0.3008  loss_mask_2: 0.3758  loss_dice_2: 2.853  loss_ce_3: 0.298  loss_mask_3: 0.3696  loss_dice_3: 2.835  loss_ce_4: 0.3042  loss_mask_4: 0.3709  loss_dice_4: 2.84  loss_ce_5: 0.2836  loss_mask_5: 0.3698  loss_dice_5: 2.83  loss_ce_6: 0.2993  loss_mask_6: 0.3691  loss_dice_6: 2.832  loss_ce_7: 0.2734  loss_mask_7: 0.3684  loss_dice_7: 2.842  loss_ce_8: 0.2978  loss_mask_8: 0.3693  loss_dice_8: 2.835  time: 1.4912  data_time: 0.0738  lr: 4.7181e-06  max_mem: 21589M
[01/18 05:15:56] d2.utils.events INFO:  eta: 6:59:34  iter: 22659  total_loss: 35.28  loss_ce: 0.285  loss_mask: 0.3625  loss_dice: 2.79  loss_ce_0: 0.5972  loss_mask_0: 0.3596  loss_dice_0: 2.93  loss_ce_1: 0.3257  loss_mask_1: 0.3698  loss_dice_1: 2.826  loss_ce_2: 0.3313  loss_mask_2: 0.3637  loss_dice_2: 2.804  loss_ce_3: 0.2976  loss_mask_3: 0.3634  loss_dice_3: 2.776  loss_ce_4: 0.2876  loss_mask_4: 0.3647  loss_dice_4: 2.776  loss_ce_5: 0.2909  loss_mask_5: 0.3604  loss_dice_5: 2.783  loss_ce_6: 0.2887  loss_mask_6: 0.3628  loss_dice_6: 2.772  loss_ce_7: 0.2763  loss_mask_7: 0.3608  loss_dice_7: 2.79  loss_ce_8: 0.2663  loss_mask_8: 0.3618  loss_dice_8: 2.78  time: 1.4912  data_time: 0.0647  lr: 4.7132e-06  max_mem: 21589M
[01/18 05:16:25] d2.utils.events INFO:  eta: 6:59:05  iter: 22679  total_loss: 35.15  loss_ce: 0.2611  loss_mask: 0.3707  loss_dice: 2.8  loss_ce_0: 0.5573  loss_mask_0: 0.3682  loss_dice_0: 2.926  loss_ce_1: 0.303  loss_mask_1: 0.3749  loss_dice_1: 2.837  loss_ce_2: 0.2855  loss_mask_2: 0.3708  loss_dice_2: 2.813  loss_ce_3: 0.2794  loss_mask_3: 0.3695  loss_dice_3: 2.806  loss_ce_4: 0.2723  loss_mask_4: 0.3702  loss_dice_4: 2.807  loss_ce_5: 0.2687  loss_mask_5: 0.3703  loss_dice_5: 2.8  loss_ce_6: 0.2526  loss_mask_6: 0.3689  loss_dice_6: 2.799  loss_ce_7: 0.2525  loss_mask_7: 0.3695  loss_dice_7: 2.803  loss_ce_8: 0.2486  loss_mask_8: 0.3692  loss_dice_8: 2.794  time: 1.4911  data_time: 0.0702  lr: 4.7083e-06  max_mem: 21589M
[01/18 05:16:54] d2.utils.events INFO:  eta: 6:58:37  iter: 22699  total_loss: 35.13  loss_ce: 0.2666  loss_mask: 0.3672  loss_dice: 2.81  loss_ce_0: 0.5719  loss_mask_0: 0.364  loss_dice_0: 2.954  loss_ce_1: 0.3044  loss_mask_1: 0.373  loss_dice_1: 2.86  loss_ce_2: 0.2929  loss_mask_2: 0.368  loss_dice_2: 2.838  loss_ce_3: 0.2777  loss_mask_3: 0.3694  loss_dice_3: 2.816  loss_ce_4: 0.2668  loss_mask_4: 0.3689  loss_dice_4: 2.818  loss_ce_5: 0.2681  loss_mask_5: 0.3684  loss_dice_5: 2.817  loss_ce_6: 0.269  loss_mask_6: 0.3678  loss_dice_6: 2.805  loss_ce_7: 0.2727  loss_mask_7: 0.3669  loss_dice_7: 2.819  loss_ce_8: 0.2584  loss_mask_8: 0.3667  loss_dice_8: 2.821  time: 1.4911  data_time: 0.0713  lr: 4.7034e-06  max_mem: 21589M
[01/18 05:17:23] d2.utils.events INFO:  eta: 6:58:14  iter: 22719  total_loss: 35.06  loss_ce: 0.2879  loss_mask: 0.3671  loss_dice: 2.79  loss_ce_0: 0.5589  loss_mask_0: 0.3567  loss_dice_0: 2.91  loss_ce_1: 0.3089  loss_mask_1: 0.3702  loss_dice_1: 2.814  loss_ce_2: 0.317  loss_mask_2: 0.369  loss_dice_2: 2.802  loss_ce_3: 0.2996  loss_mask_3: 0.3687  loss_dice_3: 2.795  loss_ce_4: 0.2847  loss_mask_4: 0.3651  loss_dice_4: 2.799  loss_ce_5: 0.2983  loss_mask_5: 0.3682  loss_dice_5: 2.8  loss_ce_6: 0.2749  loss_mask_6: 0.3686  loss_dice_6: 2.793  loss_ce_7: 0.2664  loss_mask_7: 0.3679  loss_dice_7: 2.794  loss_ce_8: 0.2791  loss_mask_8: 0.3673  loss_dice_8: 2.794  time: 1.4911  data_time: 0.0680  lr: 4.6985e-06  max_mem: 21589M
[01/18 05:17:52] d2.utils.events INFO:  eta: 6:57:45  iter: 22739  total_loss: 34.9  loss_ce: 0.2682  loss_mask: 0.3686  loss_dice: 2.796  loss_ce_0: 0.5611  loss_mask_0: 0.3685  loss_dice_0: 2.923  loss_ce_1: 0.2948  loss_mask_1: 0.3796  loss_dice_1: 2.835  loss_ce_2: 0.2935  loss_mask_2: 0.3735  loss_dice_2: 2.814  loss_ce_3: 0.2891  loss_mask_3: 0.3744  loss_dice_3: 2.804  loss_ce_4: 0.2807  loss_mask_4: 0.3698  loss_dice_4: 2.795  loss_ce_5: 0.2831  loss_mask_5: 0.369  loss_dice_5: 2.798  loss_ce_6: 0.2598  loss_mask_6: 0.3692  loss_dice_6: 2.796  loss_ce_7: 0.2693  loss_mask_7: 0.3684  loss_dice_7: 2.798  loss_ce_8: 0.2588  loss_mask_8: 0.368  loss_dice_8: 2.803  time: 1.4910  data_time: 0.0687  lr: 4.6936e-06  max_mem: 21589M
[01/18 05:18:21] d2.utils.events INFO:  eta: 6:57:09  iter: 22759  total_loss: 35.34  loss_ce: 0.2542  loss_mask: 0.3649  loss_dice: 2.843  loss_ce_0: 0.5545  loss_mask_0: 0.3589  loss_dice_0: 2.96  loss_ce_1: 0.3246  loss_mask_1: 0.3669  loss_dice_1: 2.883  loss_ce_2: 0.3102  loss_mask_2: 0.3633  loss_dice_2: 2.861  loss_ce_3: 0.3002  loss_mask_3: 0.3609  loss_dice_3: 2.843  loss_ce_4: 0.2752  loss_mask_4: 0.3596  loss_dice_4: 2.85  loss_ce_5: 0.2699  loss_mask_5: 0.3604  loss_dice_5: 2.844  loss_ce_6: 0.2741  loss_mask_6: 0.3592  loss_dice_6: 2.843  loss_ce_7: 0.264  loss_mask_7: 0.3625  loss_dice_7: 2.848  loss_ce_8: 0.2715  loss_mask_8: 0.3644  loss_dice_8: 2.842  time: 1.4910  data_time: 0.0712  lr: 4.6887e-06  max_mem: 21589M
[01/18 05:18:50] d2.utils.events INFO:  eta: 6:57:01  iter: 22779  total_loss: 34.66  loss_ce: 0.2487  loss_mask: 0.3611  loss_dice: 2.789  loss_ce_0: 0.563  loss_mask_0: 0.357  loss_dice_0: 2.914  loss_ce_1: 0.3074  loss_mask_1: 0.3664  loss_dice_1: 2.821  loss_ce_2: 0.2864  loss_mask_2: 0.3649  loss_dice_2: 2.805  loss_ce_3: 0.2567  loss_mask_3: 0.3613  loss_dice_3: 2.779  loss_ce_4: 0.2565  loss_mask_4: 0.363  loss_dice_4: 2.784  loss_ce_5: 0.2605  loss_mask_5: 0.3624  loss_dice_5: 2.779  loss_ce_6: 0.2602  loss_mask_6: 0.3616  loss_dice_6: 2.787  loss_ce_7: 0.2507  loss_mask_7: 0.3617  loss_dice_7: 2.787  loss_ce_8: 0.26  loss_mask_8: 0.3635  loss_dice_8: 2.783  time: 1.4910  data_time: 0.0674  lr: 4.6838e-06  max_mem: 21589M
[01/18 05:19:19] d2.utils.events INFO:  eta: 6:56:40  iter: 22799  total_loss: 34.52  loss_ce: 0.2531  loss_mask: 0.3749  loss_dice: 2.776  loss_ce_0: 0.5522  loss_mask_0: 0.37  loss_dice_0: 2.919  loss_ce_1: 0.3032  loss_mask_1: 0.3771  loss_dice_1: 2.817  loss_ce_2: 0.3049  loss_mask_2: 0.3749  loss_dice_2: 2.794  loss_ce_3: 0.291  loss_mask_3: 0.3744  loss_dice_3: 2.781  loss_ce_4: 0.2689  loss_mask_4: 0.3747  loss_dice_4: 2.781  loss_ce_5: 0.2739  loss_mask_5: 0.3747  loss_dice_5: 2.78  loss_ce_6: 0.2588  loss_mask_6: 0.3741  loss_dice_6: 2.771  loss_ce_7: 0.2541  loss_mask_7: 0.3757  loss_dice_7: 2.773  loss_ce_8: 0.259  loss_mask_8: 0.3759  loss_dice_8: 2.775  time: 1.4909  data_time: 0.0693  lr: 4.6789e-06  max_mem: 21589M
[01/18 05:19:48] d2.utils.events INFO:  eta: 6:56:06  iter: 22819  total_loss: 35.55  loss_ce: 0.2812  loss_mask: 0.3623  loss_dice: 2.831  loss_ce_0: 0.5697  loss_mask_0: 0.3646  loss_dice_0: 2.96  loss_ce_1: 0.329  loss_mask_1: 0.3683  loss_dice_1: 2.88  loss_ce_2: 0.3146  loss_mask_2: 0.3656  loss_dice_2: 2.869  loss_ce_3: 0.2927  loss_mask_3: 0.3656  loss_dice_3: 2.848  loss_ce_4: 0.2839  loss_mask_4: 0.3635  loss_dice_4: 2.85  loss_ce_5: 0.2891  loss_mask_5: 0.3636  loss_dice_5: 2.839  loss_ce_6: 0.2721  loss_mask_6: 0.3626  loss_dice_6: 2.843  loss_ce_7: 0.2778  loss_mask_7: 0.3626  loss_dice_7: 2.845  loss_ce_8: 0.2718  loss_mask_8: 0.3626  loss_dice_8: 2.846  time: 1.4909  data_time: 0.0689  lr: 4.674e-06  max_mem: 21589M
[01/18 05:20:18] d2.utils.events INFO:  eta: 6:55:42  iter: 22839  total_loss: 34.56  loss_ce: 0.2717  loss_mask: 0.3684  loss_dice: 2.719  loss_ce_0: 0.5357  loss_mask_0: 0.3658  loss_dice_0: 2.878  loss_ce_1: 0.306  loss_mask_1: 0.3754  loss_dice_1: 2.771  loss_ce_2: 0.299  loss_mask_2: 0.371  loss_dice_2: 2.749  loss_ce_3: 0.2711  loss_mask_3: 0.3682  loss_dice_3: 2.729  loss_ce_4: 0.2734  loss_mask_4: 0.3676  loss_dice_4: 2.732  loss_ce_5: 0.2708  loss_mask_5: 0.368  loss_dice_5: 2.735  loss_ce_6: 0.2605  loss_mask_6: 0.3682  loss_dice_6: 2.721  loss_ce_7: 0.2618  loss_mask_7: 0.3667  loss_dice_7: 2.732  loss_ce_8: 0.2509  loss_mask_8: 0.3673  loss_dice_8: 2.725  time: 1.4909  data_time: 0.0692  lr: 4.6691e-06  max_mem: 21589M
[01/18 05:20:47] d2.utils.events INFO:  eta: 6:55:26  iter: 22859  total_loss: 34.95  loss_ce: 0.2787  loss_mask: 0.3689  loss_dice: 2.806  loss_ce_0: 0.5378  loss_mask_0: 0.365  loss_dice_0: 2.934  loss_ce_1: 0.3174  loss_mask_1: 0.3786  loss_dice_1: 2.847  loss_ce_2: 0.3052  loss_mask_2: 0.3748  loss_dice_2: 2.827  loss_ce_3: 0.291  loss_mask_3: 0.3703  loss_dice_3: 2.81  loss_ce_4: 0.2901  loss_mask_4: 0.3696  loss_dice_4: 2.812  loss_ce_5: 0.2819  loss_mask_5: 0.3696  loss_dice_5: 2.817  loss_ce_6: 0.2821  loss_mask_6: 0.3677  loss_dice_6: 2.812  loss_ce_7: 0.2781  loss_mask_7: 0.3684  loss_dice_7: 2.816  loss_ce_8: 0.2711  loss_mask_8: 0.369  loss_dice_8: 2.804  time: 1.4909  data_time: 0.0718  lr: 4.6642e-06  max_mem: 21589M
[01/18 05:21:16] d2.utils.events INFO:  eta: 6:54:49  iter: 22879  total_loss: 34.87  loss_ce: 0.2627  loss_mask: 0.3726  loss_dice: 2.795  loss_ce_0: 0.5627  loss_mask_0: 0.3685  loss_dice_0: 2.911  loss_ce_1: 0.308  loss_mask_1: 0.3748  loss_dice_1: 2.825  loss_ce_2: 0.304  loss_mask_2: 0.3726  loss_dice_2: 2.804  loss_ce_3: 0.2852  loss_mask_3: 0.3718  loss_dice_3: 2.793  loss_ce_4: 0.2816  loss_mask_4: 0.375  loss_dice_4: 2.8  loss_ce_5: 0.2725  loss_mask_5: 0.3749  loss_dice_5: 2.79  loss_ce_6: 0.2623  loss_mask_6: 0.3723  loss_dice_6: 2.79  loss_ce_7: 0.2722  loss_mask_7: 0.3719  loss_dice_7: 2.782  loss_ce_8: 0.2565  loss_mask_8: 0.3718  loss_dice_8: 2.786  time: 1.4908  data_time: 0.0700  lr: 4.6593e-06  max_mem: 21589M
[01/18 05:21:45] d2.utils.events INFO:  eta: 6:54:27  iter: 22899  total_loss: 35.55  loss_ce: 0.2839  loss_mask: 0.3633  loss_dice: 2.833  loss_ce_0: 0.6217  loss_mask_0: 0.3587  loss_dice_0: 2.972  loss_ce_1: 0.3403  loss_mask_1: 0.3634  loss_dice_1: 2.871  loss_ce_2: 0.3349  loss_mask_2: 0.362  loss_dice_2: 2.852  loss_ce_3: 0.3324  loss_mask_3: 0.3624  loss_dice_3: 2.838  loss_ce_4: 0.2985  loss_mask_4: 0.3633  loss_dice_4: 2.841  loss_ce_5: 0.2953  loss_mask_5: 0.3629  loss_dice_5: 2.829  loss_ce_6: 0.2942  loss_mask_6: 0.3624  loss_dice_6: 2.837  loss_ce_7: 0.299  loss_mask_7: 0.3629  loss_dice_7: 2.828  loss_ce_8: 0.2915  loss_mask_8: 0.3642  loss_dice_8: 2.831  time: 1.4908  data_time: 0.0681  lr: 4.6544e-06  max_mem: 21589M
[01/18 05:22:14] d2.utils.events INFO:  eta: 6:53:58  iter: 22919  total_loss: 34.99  loss_ce: 0.268  loss_mask: 0.3714  loss_dice: 2.792  loss_ce_0: 0.5591  loss_mask_0: 0.3634  loss_dice_0: 2.92  loss_ce_1: 0.3363  loss_mask_1: 0.3774  loss_dice_1: 2.838  loss_ce_2: 0.2995  loss_mask_2: 0.3729  loss_dice_2: 2.815  loss_ce_3: 0.289  loss_mask_3: 0.3711  loss_dice_3: 2.81  loss_ce_4: 0.2966  loss_mask_4: 0.3708  loss_dice_4: 2.8  loss_ce_5: 0.2879  loss_mask_5: 0.3706  loss_dice_5: 2.802  loss_ce_6: 0.2797  loss_mask_6: 0.3712  loss_dice_6: 2.804  loss_ce_7: 0.2718  loss_mask_7: 0.3705  loss_dice_7: 2.805  loss_ce_8: 0.2803  loss_mask_8: 0.3694  loss_dice_8: 2.803  time: 1.4907  data_time: 0.0711  lr: 4.6495e-06  max_mem: 21589M
[01/18 05:22:44] d2.utils.events INFO:  eta: 6:53:30  iter: 22939  total_loss: 35.62  loss_ce: 0.2794  loss_mask: 0.3597  loss_dice: 2.833  loss_ce_0: 0.5967  loss_mask_0: 0.358  loss_dice_0: 2.962  loss_ce_1: 0.3305  loss_mask_1: 0.3648  loss_dice_1: 2.869  loss_ce_2: 0.323  loss_mask_2: 0.3616  loss_dice_2: 2.842  loss_ce_3: 0.3071  loss_mask_3: 0.3625  loss_dice_3: 2.828  loss_ce_4: 0.3076  loss_mask_4: 0.3626  loss_dice_4: 2.816  loss_ce_5: 0.2932  loss_mask_5: 0.3589  loss_dice_5: 2.819  loss_ce_6: 0.2817  loss_mask_6: 0.3617  loss_dice_6: 2.827  loss_ce_7: 0.2824  loss_mask_7: 0.3618  loss_dice_7: 2.828  loss_ce_8: 0.2826  loss_mask_8: 0.3619  loss_dice_8: 2.826  time: 1.4907  data_time: 0.0715  lr: 4.6446e-06  max_mem: 21589M
[01/18 05:23:12] d2.utils.events INFO:  eta: 6:52:47  iter: 22959  total_loss: 35.22  loss_ce: 0.2825  loss_mask: 0.3728  loss_dice: 2.803  loss_ce_0: 0.5697  loss_mask_0: 0.3657  loss_dice_0: 2.931  loss_ce_1: 0.3172  loss_mask_1: 0.3786  loss_dice_1: 2.848  loss_ce_2: 0.3228  loss_mask_2: 0.3774  loss_dice_2: 2.819  loss_ce_3: 0.2981  loss_mask_3: 0.3737  loss_dice_3: 2.807  loss_ce_4: 0.2812  loss_mask_4: 0.3711  loss_dice_4: 2.819  loss_ce_5: 0.2989  loss_mask_5: 0.3708  loss_dice_5: 2.804  loss_ce_6: 0.2808  loss_mask_6: 0.3728  loss_dice_6: 2.808  loss_ce_7: 0.2884  loss_mask_7: 0.3724  loss_dice_7: 2.804  loss_ce_8: 0.2755  loss_mask_8: 0.3735  loss_dice_8: 2.802  time: 1.4907  data_time: 0.0713  lr: 4.6397e-06  max_mem: 21589M
[01/18 05:23:41] d2.utils.events INFO:  eta: 6:52:27  iter: 22979  total_loss: 34.98  loss_ce: 0.2878  loss_mask: 0.3666  loss_dice: 2.769  loss_ce_0: 0.5415  loss_mask_0: 0.3667  loss_dice_0: 2.894  loss_ce_1: 0.3124  loss_mask_1: 0.3802  loss_dice_1: 2.82  loss_ce_2: 0.3225  loss_mask_2: 0.3753  loss_dice_2: 2.793  loss_ce_3: 0.3056  loss_mask_3: 0.3711  loss_dice_3: 2.778  loss_ce_4: 0.2899  loss_mask_4: 0.3689  loss_dice_4: 2.776  loss_ce_5: 0.284  loss_mask_5: 0.3681  loss_dice_5: 2.766  loss_ce_6: 0.2828  loss_mask_6: 0.3682  loss_dice_6: 2.765  loss_ce_7: 0.2823  loss_mask_7: 0.367  loss_dice_7: 2.773  loss_ce_8: 0.2769  loss_mask_8: 0.3677  loss_dice_8: 2.772  time: 1.4906  data_time: 0.0732  lr: 4.6348e-06  max_mem: 21589M
[01/18 05:24:11] d2.utils.events INFO:  eta: 6:52:01  iter: 22999  total_loss: 35.12  loss_ce: 0.2514  loss_mask: 0.3671  loss_dice: 2.805  loss_ce_0: 0.5375  loss_mask_0: 0.3637  loss_dice_0: 2.941  loss_ce_1: 0.3017  loss_mask_1: 0.3777  loss_dice_1: 2.847  loss_ce_2: 0.3082  loss_mask_2: 0.3725  loss_dice_2: 2.818  loss_ce_3: 0.2786  loss_mask_3: 0.3689  loss_dice_3: 2.804  loss_ce_4: 0.2642  loss_mask_4: 0.3683  loss_dice_4: 2.814  loss_ce_5: 0.2459  loss_mask_5: 0.3688  loss_dice_5: 2.816  loss_ce_6: 0.2524  loss_mask_6: 0.37  loss_dice_6: 2.814  loss_ce_7: 0.2659  loss_mask_7: 0.3678  loss_dice_7: 2.807  loss_ce_8: 0.2603  loss_mask_8: 0.3687  loss_dice_8: 2.807  time: 1.4906  data_time: 0.0707  lr: 4.6299e-06  max_mem: 21589M
[01/18 05:24:39] d2.utils.events INFO:  eta: 6:51:34  iter: 23019  total_loss: 34.47  loss_ce: 0.2668  loss_mask: 0.3628  loss_dice: 2.762  loss_ce_0: 0.5449  loss_mask_0: 0.3602  loss_dice_0: 2.896  loss_ce_1: 0.2938  loss_mask_1: 0.3672  loss_dice_1: 2.802  loss_ce_2: 0.3006  loss_mask_2: 0.3657  loss_dice_2: 2.784  loss_ce_3: 0.2825  loss_mask_3: 0.3639  loss_dice_3: 2.771  loss_ce_4: 0.269  loss_mask_4: 0.3622  loss_dice_4: 2.77  loss_ce_5: 0.2766  loss_mask_5: 0.362  loss_dice_5: 2.768  loss_ce_6: 0.266  loss_mask_6: 0.3613  loss_dice_6: 2.758  loss_ce_7: 0.2724  loss_mask_7: 0.3617  loss_dice_7: 2.767  loss_ce_8: 0.2804  loss_mask_8: 0.3631  loss_dice_8: 2.762  time: 1.4906  data_time: 0.0703  lr: 4.625e-06  max_mem: 21589M
[01/18 05:25:09] d2.utils.events INFO:  eta: 6:51:20  iter: 23039  total_loss: 35.24  loss_ce: 0.2856  loss_mask: 0.3563  loss_dice: 2.862  loss_ce_0: 0.5808  loss_mask_0: 0.3522  loss_dice_0: 2.982  loss_ce_1: 0.3035  loss_mask_1: 0.3585  loss_dice_1: 2.904  loss_ce_2: 0.2935  loss_mask_2: 0.3582  loss_dice_2: 2.874  loss_ce_3: 0.297  loss_mask_3: 0.3571  loss_dice_3: 2.876  loss_ce_4: 0.2859  loss_mask_4: 0.3581  loss_dice_4: 2.861  loss_ce_5: 0.2741  loss_mask_5: 0.3568  loss_dice_5: 2.865  loss_ce_6: 0.2851  loss_mask_6: 0.3559  loss_dice_6: 2.861  loss_ce_7: 0.2737  loss_mask_7: 0.3564  loss_dice_7: 2.863  loss_ce_8: 0.2814  loss_mask_8: 0.3568  loss_dice_8: 2.858  time: 1.4905  data_time: 0.0708  lr: 4.6201e-06  max_mem: 21589M
[01/18 05:25:38] d2.utils.events INFO:  eta: 6:50:49  iter: 23059  total_loss: 35.01  loss_ce: 0.2825  loss_mask: 0.3662  loss_dice: 2.833  loss_ce_0: 0.56  loss_mask_0: 0.3664  loss_dice_0: 2.943  loss_ce_1: 0.3034  loss_mask_1: 0.3713  loss_dice_1: 2.865  loss_ce_2: 0.2913  loss_mask_2: 0.3698  loss_dice_2: 2.842  loss_ce_3: 0.2926  loss_mask_3: 0.3678  loss_dice_3: 2.84  loss_ce_4: 0.2867  loss_mask_4: 0.3658  loss_dice_4: 2.829  loss_ce_5: 0.2787  loss_mask_5: 0.367  loss_dice_5: 2.836  loss_ce_6: 0.2913  loss_mask_6: 0.3671  loss_dice_6: 2.824  loss_ce_7: 0.2757  loss_mask_7: 0.3674  loss_dice_7: 2.835  loss_ce_8: 0.2743  loss_mask_8: 0.366  loss_dice_8: 2.827  time: 1.4905  data_time: 0.0665  lr: 4.6152e-06  max_mem: 21589M
[01/18 05:26:06] d2.utils.events INFO:  eta: 6:50:17  iter: 23079  total_loss: 35.52  loss_ce: 0.2712  loss_mask: 0.3635  loss_dice: 2.845  loss_ce_0: 0.5706  loss_mask_0: 0.3637  loss_dice_0: 2.977  loss_ce_1: 0.3318  loss_mask_1: 0.3716  loss_dice_1: 2.886  loss_ce_2: 0.3153  loss_mask_2: 0.3684  loss_dice_2: 2.86  loss_ce_3: 0.2913  loss_mask_3: 0.3622  loss_dice_3: 2.852  loss_ce_4: 0.2868  loss_mask_4: 0.3623  loss_dice_4: 2.847  loss_ce_5: 0.2852  loss_mask_5: 0.3634  loss_dice_5: 2.848  loss_ce_6: 0.2733  loss_mask_6: 0.3634  loss_dice_6: 2.834  loss_ce_7: 0.2763  loss_mask_7: 0.3615  loss_dice_7: 2.834  loss_ce_8: 0.2717  loss_mask_8: 0.361  loss_dice_8: 2.84  time: 1.4904  data_time: 0.0659  lr: 4.6103e-06  max_mem: 21589M
[01/18 05:26:35] d2.utils.events INFO:  eta: 6:49:51  iter: 23099  total_loss: 34.83  loss_ce: 0.2783  loss_mask: 0.361  loss_dice: 2.777  loss_ce_0: 0.5647  loss_mask_0: 0.3612  loss_dice_0: 2.901  loss_ce_1: 0.3126  loss_mask_1: 0.3666  loss_dice_1: 2.813  loss_ce_2: 0.3007  loss_mask_2: 0.3658  loss_dice_2: 2.795  loss_ce_3: 0.2931  loss_mask_3: 0.365  loss_dice_3: 2.777  loss_ce_4: 0.3008  loss_mask_4: 0.3633  loss_dice_4: 2.777  loss_ce_5: 0.2799  loss_mask_5: 0.3627  loss_dice_5: 2.781  loss_ce_6: 0.2795  loss_mask_6: 0.3614  loss_dice_6: 2.78  loss_ce_7: 0.2746  loss_mask_7: 0.3619  loss_dice_7: 2.776  loss_ce_8: 0.28  loss_mask_8: 0.3602  loss_dice_8: 2.769  time: 1.4904  data_time: 0.0715  lr: 4.6054e-06  max_mem: 21589M
[01/18 05:27:04] d2.utils.events INFO:  eta: 6:49:28  iter: 23119  total_loss: 34.97  loss_ce: 0.2731  loss_mask: 0.3672  loss_dice: 2.822  loss_ce_0: 0.5499  loss_mask_0: 0.3656  loss_dice_0: 2.959  loss_ce_1: 0.3084  loss_mask_1: 0.3743  loss_dice_1: 2.868  loss_ce_2: 0.3079  loss_mask_2: 0.3674  loss_dice_2: 2.856  loss_ce_3: 0.2766  loss_mask_3: 0.3671  loss_dice_3: 2.836  loss_ce_4: 0.2916  loss_mask_4: 0.3673  loss_dice_4: 2.834  loss_ce_5: 0.2745  loss_mask_5: 0.3672  loss_dice_5: 2.834  loss_ce_6: 0.2779  loss_mask_6: 0.368  loss_dice_6: 2.827  loss_ce_7: 0.2667  loss_mask_7: 0.3685  loss_dice_7: 2.826  loss_ce_8: 0.2616  loss_mask_8: 0.3683  loss_dice_8: 2.828  time: 1.4904  data_time: 0.0738  lr: 4.6005e-06  max_mem: 21589M
[01/18 05:27:34] d2.utils.events INFO:  eta: 6:49:18  iter: 23139  total_loss: 35.47  loss_ce: 0.254  loss_mask: 0.3601  loss_dice: 2.839  loss_ce_0: 0.5524  loss_mask_0: 0.3549  loss_dice_0: 2.983  loss_ce_1: 0.2854  loss_mask_1: 0.3658  loss_dice_1: 2.883  loss_ce_2: 0.2664  loss_mask_2: 0.3624  loss_dice_2: 2.862  loss_ce_3: 0.2712  loss_mask_3: 0.3598  loss_dice_3: 2.853  loss_ce_4: 0.2677  loss_mask_4: 0.3606  loss_dice_4: 2.843  loss_ce_5: 0.2734  loss_mask_5: 0.3591  loss_dice_5: 2.852  loss_ce_6: 0.2588  loss_mask_6: 0.3596  loss_dice_6: 2.846  loss_ce_7: 0.254  loss_mask_7: 0.36  loss_dice_7: 2.851  loss_ce_8: 0.2598  loss_mask_8: 0.3588  loss_dice_8: 2.845  time: 1.4904  data_time: 0.0690  lr: 4.5956e-06  max_mem: 21589M
[01/18 05:28:03] d2.utils.events INFO:  eta: 6:48:35  iter: 23159  total_loss: 35.41  loss_ce: 0.2665  loss_mask: 0.3582  loss_dice: 2.849  loss_ce_0: 0.5445  loss_mask_0: 0.362  loss_dice_0: 2.983  loss_ce_1: 0.3008  loss_mask_1: 0.3623  loss_dice_1: 2.904  loss_ce_2: 0.2876  loss_mask_2: 0.36  loss_dice_2: 2.874  loss_ce_3: 0.2807  loss_mask_3: 0.3603  loss_dice_3: 2.864  loss_ce_4: 0.2714  loss_mask_4: 0.3598  loss_dice_4: 2.859  loss_ce_5: 0.2766  loss_mask_5: 0.3588  loss_dice_5: 2.851  loss_ce_6: 0.2739  loss_mask_6: 0.3591  loss_dice_6: 2.851  loss_ce_7: 0.2579  loss_mask_7: 0.3592  loss_dice_7: 2.852  loss_ce_8: 0.2572  loss_mask_8: 0.3578  loss_dice_8: 2.845  time: 1.4903  data_time: 0.0725  lr: 4.5907e-06  max_mem: 21589M
[01/18 05:28:32] d2.utils.events INFO:  eta: 6:47:57  iter: 23179  total_loss: 34.93  loss_ce: 0.2943  loss_mask: 0.3619  loss_dice: 2.777  loss_ce_0: 0.5633  loss_mask_0: 0.3646  loss_dice_0: 2.909  loss_ce_1: 0.3112  loss_mask_1: 0.3666  loss_dice_1: 2.83  loss_ce_2: 0.3227  loss_mask_2: 0.3638  loss_dice_2: 2.803  loss_ce_3: 0.3125  loss_mask_3: 0.3623  loss_dice_3: 2.784  loss_ce_4: 0.2936  loss_mask_4: 0.3606  loss_dice_4: 2.784  loss_ce_5: 0.2825  loss_mask_5: 0.3625  loss_dice_5: 2.774  loss_ce_6: 0.2762  loss_mask_6: 0.3626  loss_dice_6: 2.774  loss_ce_7: 0.2761  loss_mask_7: 0.3632  loss_dice_7: 2.775  loss_ce_8: 0.2809  loss_mask_8: 0.3624  loss_dice_8: 2.781  time: 1.4903  data_time: 0.0726  lr: 4.5858e-06  max_mem: 21589M
[01/18 05:29:01] d2.utils.events INFO:  eta: 6:47:23  iter: 23199  total_loss: 35.45  loss_ce: 0.2722  loss_mask: 0.3772  loss_dice: 2.813  loss_ce_0: 0.5542  loss_mask_0: 0.3731  loss_dice_0: 2.936  loss_ce_1: 0.3073  loss_mask_1: 0.3834  loss_dice_1: 2.86  loss_ce_2: 0.2921  loss_mask_2: 0.3819  loss_dice_2: 2.833  loss_ce_3: 0.2883  loss_mask_3: 0.3767  loss_dice_3: 2.819  loss_ce_4: 0.2776  loss_mask_4: 0.3773  loss_dice_4: 2.814  loss_ce_5: 0.2765  loss_mask_5: 0.3767  loss_dice_5: 2.819  loss_ce_6: 0.2818  loss_mask_6: 0.3771  loss_dice_6: 2.829  loss_ce_7: 0.2778  loss_mask_7: 0.3775  loss_dice_7: 2.816  loss_ce_8: 0.2716  loss_mask_8: 0.3769  loss_dice_8: 2.825  time: 1.4903  data_time: 0.0691  lr: 4.5809e-06  max_mem: 21589M
[01/18 05:29:30] d2.utils.events INFO:  eta: 6:46:55  iter: 23219  total_loss: 34.86  loss_ce: 0.2636  loss_mask: 0.3623  loss_dice: 2.792  loss_ce_0: 0.5314  loss_mask_0: 0.3597  loss_dice_0: 2.922  loss_ce_1: 0.3013  loss_mask_1: 0.3701  loss_dice_1: 2.836  loss_ce_2: 0.308  loss_mask_2: 0.364  loss_dice_2: 2.799  loss_ce_3: 0.2784  loss_mask_3: 0.3653  loss_dice_3: 2.793  loss_ce_4: 0.2661  loss_mask_4: 0.3639  loss_dice_4: 2.791  loss_ce_5: 0.2538  loss_mask_5: 0.3638  loss_dice_5: 2.792  loss_ce_6: 0.2619  loss_mask_6: 0.3632  loss_dice_6: 2.795  loss_ce_7: 0.257  loss_mask_7: 0.3624  loss_dice_7: 2.786  loss_ce_8: 0.2496  loss_mask_8: 0.3633  loss_dice_8: 2.791  time: 1.4902  data_time: 0.0722  lr: 4.576e-06  max_mem: 21589M
[01/18 05:29:59] d2.utils.events INFO:  eta: 6:46:36  iter: 23239  total_loss: 34.5  loss_ce: 0.2958  loss_mask: 0.3691  loss_dice: 2.766  loss_ce_0: 0.5788  loss_mask_0: 0.3675  loss_dice_0: 2.901  loss_ce_1: 0.3281  loss_mask_1: 0.3728  loss_dice_1: 2.807  loss_ce_2: 0.3173  loss_mask_2: 0.3708  loss_dice_2: 2.776  loss_ce_3: 0.3105  loss_mask_3: 0.3701  loss_dice_3: 2.767  loss_ce_4: 0.3014  loss_mask_4: 0.371  loss_dice_4: 2.773  loss_ce_5: 0.295  loss_mask_5: 0.3692  loss_dice_5: 2.761  loss_ce_6: 0.2848  loss_mask_6: 0.3696  loss_dice_6: 2.762  loss_ce_7: 0.3044  loss_mask_7: 0.3693  loss_dice_7: 2.758  loss_ce_8: 0.2865  loss_mask_8: 0.3695  loss_dice_8: 2.765  time: 1.4902  data_time: 0.0725  lr: 4.571e-06  max_mem: 21589M
[01/18 05:30:28] d2.utils.events INFO:  eta: 6:45:52  iter: 23259  total_loss: 34.75  loss_ce: 0.2605  loss_mask: 0.3562  loss_dice: 2.768  loss_ce_0: 0.5715  loss_mask_0: 0.3527  loss_dice_0: 2.917  loss_ce_1: 0.2949  loss_mask_1: 0.3556  loss_dice_1: 2.835  loss_ce_2: 0.2983  loss_mask_2: 0.3546  loss_dice_2: 2.799  loss_ce_3: 0.2758  loss_mask_3: 0.3576  loss_dice_3: 2.778  loss_ce_4: 0.2691  loss_mask_4: 0.3576  loss_dice_4: 2.778  loss_ce_5: 0.246  loss_mask_5: 0.3578  loss_dice_5: 2.77  loss_ce_6: 0.2643  loss_mask_6: 0.3575  loss_dice_6: 2.778  loss_ce_7: 0.256  loss_mask_7: 0.3557  loss_dice_7: 2.775  loss_ce_8: 0.2602  loss_mask_8: 0.3552  loss_dice_8: 2.772  time: 1.4901  data_time: 0.0718  lr: 4.5661e-06  max_mem: 21589M
[01/18 05:30:57] d2.utils.events INFO:  eta: 6:45:27  iter: 23279  total_loss: 35.07  loss_ce: 0.2642  loss_mask: 0.3619  loss_dice: 2.798  loss_ce_0: 0.5558  loss_mask_0: 0.3563  loss_dice_0: 2.934  loss_ce_1: 0.3051  loss_mask_1: 0.3663  loss_dice_1: 2.857  loss_ce_2: 0.3209  loss_mask_2: 0.3644  loss_dice_2: 2.83  loss_ce_3: 0.2946  loss_mask_3: 0.3617  loss_dice_3: 2.806  loss_ce_4: 0.2802  loss_mask_4: 0.3634  loss_dice_4: 2.802  loss_ce_5: 0.2726  loss_mask_5: 0.3624  loss_dice_5: 2.795  loss_ce_6: 0.2786  loss_mask_6: 0.3628  loss_dice_6: 2.797  loss_ce_7: 0.2656  loss_mask_7: 0.3639  loss_dice_7: 2.802  loss_ce_8: 0.2578  loss_mask_8: 0.3619  loss_dice_8: 2.801  time: 1.4901  data_time: 0.0746  lr: 4.5612e-06  max_mem: 21589M
[01/18 05:31:27] d2.utils.events INFO:  eta: 6:45:08  iter: 23299  total_loss: 35.91  loss_ce: 0.2651  loss_mask: 0.3677  loss_dice: 2.9  loss_ce_0: 0.5494  loss_mask_0: 0.3708  loss_dice_0: 3.027  loss_ce_1: 0.306  loss_mask_1: 0.374  loss_dice_1: 2.937  loss_ce_2: 0.2939  loss_mask_2: 0.3718  loss_dice_2: 2.916  loss_ce_3: 0.2698  loss_mask_3: 0.3697  loss_dice_3: 2.903  loss_ce_4: 0.2788  loss_mask_4: 0.3696  loss_dice_4: 2.905  loss_ce_5: 0.266  loss_mask_5: 0.3688  loss_dice_5: 2.897  loss_ce_6: 0.27  loss_mask_6: 0.3699  loss_dice_6: 2.898  loss_ce_7: 0.2601  loss_mask_7: 0.3693  loss_dice_7: 2.905  loss_ce_8: 0.268  loss_mask_8: 0.3687  loss_dice_8: 2.896  time: 1.4901  data_time: 0.0721  lr: 4.5563e-06  max_mem: 21589M
[01/18 05:31:56] d2.utils.events INFO:  eta: 6:44:44  iter: 23319  total_loss: 35.17  loss_ce: 0.2822  loss_mask: 0.3674  loss_dice: 2.801  loss_ce_0: 0.5856  loss_mask_0: 0.3658  loss_dice_0: 2.918  loss_ce_1: 0.3158  loss_mask_1: 0.3715  loss_dice_1: 2.842  loss_ce_2: 0.3093  loss_mask_2: 0.3679  loss_dice_2: 2.81  loss_ce_3: 0.2834  loss_mask_3: 0.3656  loss_dice_3: 2.791  loss_ce_4: 0.275  loss_mask_4: 0.3636  loss_dice_4: 2.801  loss_ce_5: 0.2668  loss_mask_5: 0.363  loss_dice_5: 2.802  loss_ce_6: 0.2699  loss_mask_6: 0.3651  loss_dice_6: 2.801  loss_ce_7: 0.2562  loss_mask_7: 0.3667  loss_dice_7: 2.795  loss_ce_8: 0.2798  loss_mask_8: 0.3665  loss_dice_8: 2.804  time: 1.4901  data_time: 0.0731  lr: 4.5514e-06  max_mem: 21589M
[01/18 05:32:25] d2.utils.events INFO:  eta: 6:44:34  iter: 23339  total_loss: 35.83  loss_ce: 0.2875  loss_mask: 0.3727  loss_dice: 2.891  loss_ce_0: 0.5677  loss_mask_0: 0.3677  loss_dice_0: 3.015  loss_ce_1: 0.3151  loss_mask_1: 0.3782  loss_dice_1: 2.94  loss_ce_2: 0.3138  loss_mask_2: 0.3749  loss_dice_2: 2.917  loss_ce_3: 0.3055  loss_mask_3: 0.373  loss_dice_3: 2.905  loss_ce_4: 0.2989  loss_mask_4: 0.3732  loss_dice_4: 2.901  loss_ce_5: 0.2816  loss_mask_5: 0.3726  loss_dice_5: 2.891  loss_ce_6: 0.2924  loss_mask_6: 0.373  loss_dice_6: 2.889  loss_ce_7: 0.2887  loss_mask_7: 0.3743  loss_dice_7: 2.888  loss_ce_8: 0.284  loss_mask_8: 0.3734  loss_dice_8: 2.884  time: 1.4900  data_time: 0.0728  lr: 4.5465e-06  max_mem: 21589M
[01/18 05:32:55] d2.utils.events INFO:  eta: 6:44:05  iter: 23359  total_loss: 35.32  loss_ce: 0.2967  loss_mask: 0.3716  loss_dice: 2.807  loss_ce_0: 0.5758  loss_mask_0: 0.3651  loss_dice_0: 2.927  loss_ce_1: 0.3239  loss_mask_1: 0.3773  loss_dice_1: 2.845  loss_ce_2: 0.3174  loss_mask_2: 0.3732  loss_dice_2: 2.823  loss_ce_3: 0.3102  loss_mask_3: 0.3722  loss_dice_3: 2.808  loss_ce_4: 0.3119  loss_mask_4: 0.374  loss_dice_4: 2.8  loss_ce_5: 0.289  loss_mask_5: 0.372  loss_dice_5: 2.804  loss_ce_6: 0.2925  loss_mask_6: 0.3741  loss_dice_6: 2.806  loss_ce_7: 0.2925  loss_mask_7: 0.3735  loss_dice_7: 2.809  loss_ce_8: 0.2947  loss_mask_8: 0.3716  loss_dice_8: 2.798  time: 1.4900  data_time: 0.0672  lr: 4.5416e-06  max_mem: 21589M
[01/18 05:33:23] d2.utils.events INFO:  eta: 6:43:13  iter: 23379  total_loss: 35.15  loss_ce: 0.2714  loss_mask: 0.3796  loss_dice: 2.796  loss_ce_0: 0.5574  loss_mask_0: 0.3687  loss_dice_0: 2.918  loss_ce_1: 0.2964  loss_mask_1: 0.3838  loss_dice_1: 2.833  loss_ce_2: 0.2874  loss_mask_2: 0.3803  loss_dice_2: 2.813  loss_ce_3: 0.2841  loss_mask_3: 0.3785  loss_dice_3: 2.797  loss_ce_4: 0.2645  loss_mask_4: 0.3825  loss_dice_4: 2.805  loss_ce_5: 0.2695  loss_mask_5: 0.3824  loss_dice_5: 2.806  loss_ce_6: 0.2757  loss_mask_6: 0.3806  loss_dice_6: 2.792  loss_ce_7: 0.2655  loss_mask_7: 0.3814  loss_dice_7: 2.795  loss_ce_8: 0.2658  loss_mask_8: 0.3797  loss_dice_8: 2.797  time: 1.4900  data_time: 0.0676  lr: 4.5367e-06  max_mem: 21589M
[01/18 05:33:52] d2.utils.events INFO:  eta: 6:42:36  iter: 23399  total_loss: 35.27  loss_ce: 0.2836  loss_mask: 0.3802  loss_dice: 2.787  loss_ce_0: 0.5481  loss_mask_0: 0.3736  loss_dice_0: 2.923  loss_ce_1: 0.3194  loss_mask_1: 0.3842  loss_dice_1: 2.823  loss_ce_2: 0.3116  loss_mask_2: 0.3799  loss_dice_2: 2.803  loss_ce_3: 0.2807  loss_mask_3: 0.3783  loss_dice_3: 2.786  loss_ce_4: 0.2848  loss_mask_4: 0.378  loss_dice_4: 2.785  loss_ce_5: 0.2706  loss_mask_5: 0.3774  loss_dice_5: 2.785  loss_ce_6: 0.2665  loss_mask_6: 0.377  loss_dice_6: 2.785  loss_ce_7: 0.2609  loss_mask_7: 0.3765  loss_dice_7: 2.787  loss_ce_8: 0.2681  loss_mask_8: 0.3778  loss_dice_8: 2.783  time: 1.4899  data_time: 0.0704  lr: 4.5318e-06  max_mem: 21589M
[01/18 05:34:21] d2.utils.events INFO:  eta: 6:41:48  iter: 23419  total_loss: 35.22  loss_ce: 0.2688  loss_mask: 0.3784  loss_dice: 2.806  loss_ce_0: 0.5602  loss_mask_0: 0.3792  loss_dice_0: 2.934  loss_ce_1: 0.3006  loss_mask_1: 0.3841  loss_dice_1: 2.848  loss_ce_2: 0.2927  loss_mask_2: 0.3802  loss_dice_2: 2.825  loss_ce_3: 0.2799  loss_mask_3: 0.3796  loss_dice_3: 2.805  loss_ce_4: 0.2667  loss_mask_4: 0.3792  loss_dice_4: 2.804  loss_ce_5: 0.2805  loss_mask_5: 0.381  loss_dice_5: 2.802  loss_ce_6: 0.2589  loss_mask_6: 0.3802  loss_dice_6: 2.804  loss_ce_7: 0.2762  loss_mask_7: 0.3799  loss_dice_7: 2.807  loss_ce_8: 0.2727  loss_mask_8: 0.3793  loss_dice_8: 2.803  time: 1.4899  data_time: 0.0695  lr: 4.5268e-06  max_mem: 21589M
[01/18 05:34:50] d2.utils.events INFO:  eta: 6:41:13  iter: 23439  total_loss: 35.55  loss_ce: 0.2854  loss_mask: 0.3547  loss_dice: 2.845  loss_ce_0: 0.581  loss_mask_0: 0.357  loss_dice_0: 3.005  loss_ce_1: 0.3225  loss_mask_1: 0.3602  loss_dice_1: 2.894  loss_ce_2: 0.3011  loss_mask_2: 0.3584  loss_dice_2: 2.872  loss_ce_3: 0.2823  loss_mask_3: 0.3556  loss_dice_3: 2.849  loss_ce_4: 0.2989  loss_mask_4: 0.3535  loss_dice_4: 2.854  loss_ce_5: 0.2909  loss_mask_5: 0.3533  loss_dice_5: 2.846  loss_ce_6: 0.2813  loss_mask_6: 0.3544  loss_dice_6: 2.85  loss_ce_7: 0.2745  loss_mask_7: 0.3546  loss_dice_7: 2.844  loss_ce_8: 0.2789  loss_mask_8: 0.3537  loss_dice_8: 2.842  time: 1.4899  data_time: 0.0678  lr: 4.5219e-06  max_mem: 21589M
[01/18 05:35:19] d2.utils.events INFO:  eta: 6:40:25  iter: 23459  total_loss: 35.31  loss_ce: 0.2708  loss_mask: 0.3697  loss_dice: 2.815  loss_ce_0: 0.5588  loss_mask_0: 0.3609  loss_dice_0: 2.949  loss_ce_1: 0.3161  loss_mask_1: 0.3717  loss_dice_1: 2.859  loss_ce_2: 0.2998  loss_mask_2: 0.372  loss_dice_2: 2.84  loss_ce_3: 0.2987  loss_mask_3: 0.3677  loss_dice_3: 2.816  loss_ce_4: 0.2692  loss_mask_4: 0.3676  loss_dice_4: 2.815  loss_ce_5: 0.2774  loss_mask_5: 0.3697  loss_dice_5: 2.809  loss_ce_6: 0.2849  loss_mask_6: 0.3678  loss_dice_6: 2.813  loss_ce_7: 0.2664  loss_mask_7: 0.3693  loss_dice_7: 2.815  loss_ce_8: 0.2709  loss_mask_8: 0.367  loss_dice_8: 2.812  time: 1.4898  data_time: 0.0692  lr: 4.517e-06  max_mem: 21589M
[01/18 05:35:48] d2.utils.events INFO:  eta: 6:39:35  iter: 23479  total_loss: 35.69  loss_ce: 0.2875  loss_mask: 0.3612  loss_dice: 2.85  loss_ce_0: 0.5429  loss_mask_0: 0.3565  loss_dice_0: 2.97  loss_ce_1: 0.3073  loss_mask_1: 0.3651  loss_dice_1: 2.896  loss_ce_2: 0.3028  loss_mask_2: 0.3609  loss_dice_2: 2.865  loss_ce_3: 0.2918  loss_mask_3: 0.3615  loss_dice_3: 2.85  loss_ce_4: 0.2999  loss_mask_4: 0.3621  loss_dice_4: 2.851  loss_ce_5: 0.2797  loss_mask_5: 0.3632  loss_dice_5: 2.847  loss_ce_6: 0.2792  loss_mask_6: 0.3628  loss_dice_6: 2.847  loss_ce_7: 0.2968  loss_mask_7: 0.3601  loss_dice_7: 2.849  loss_ce_8: 0.2781  loss_mask_8: 0.3619  loss_dice_8: 2.849  time: 1.4898  data_time: 0.0745  lr: 4.5121e-06  max_mem: 21589M
[01/18 05:36:17] d2.utils.events INFO:  eta: 6:39:15  iter: 23499  total_loss: 34.67  loss_ce: 0.253  loss_mask: 0.3707  loss_dice: 2.79  loss_ce_0: 0.5473  loss_mask_0: 0.3646  loss_dice_0: 2.916  loss_ce_1: 0.2968  loss_mask_1: 0.3756  loss_dice_1: 2.817  loss_ce_2: 0.3007  loss_mask_2: 0.373  loss_dice_2: 2.811  loss_ce_3: 0.2791  loss_mask_3: 0.3743  loss_dice_3: 2.8  loss_ce_4: 0.2574  loss_mask_4: 0.3733  loss_dice_4: 2.793  loss_ce_5: 0.2596  loss_mask_5: 0.3737  loss_dice_5: 2.789  loss_ce_6: 0.2625  loss_mask_6: 0.3721  loss_dice_6: 2.782  loss_ce_7: 0.2608  loss_mask_7: 0.372  loss_dice_7: 2.79  loss_ce_8: 0.2688  loss_mask_8: 0.3713  loss_dice_8: 2.792  time: 1.4898  data_time: 0.0689  lr: 4.5072e-06  max_mem: 21589M
[01/18 05:36:46] d2.utils.events INFO:  eta: 6:38:43  iter: 23519  total_loss: 34.71  loss_ce: 0.285  loss_mask: 0.357  loss_dice: 2.773  loss_ce_0: 0.5488  loss_mask_0: 0.3571  loss_dice_0: 2.921  loss_ce_1: 0.3125  loss_mask_1: 0.3654  loss_dice_1: 2.832  loss_ce_2: 0.3124  loss_mask_2: 0.3593  loss_dice_2: 2.811  loss_ce_3: 0.2971  loss_mask_3: 0.3579  loss_dice_3: 2.782  loss_ce_4: 0.2807  loss_mask_4: 0.3596  loss_dice_4: 2.784  loss_ce_5: 0.2772  loss_mask_5: 0.3605  loss_dice_5: 2.779  loss_ce_6: 0.2751  loss_mask_6: 0.3587  loss_dice_6: 2.778  loss_ce_7: 0.2719  loss_mask_7: 0.3568  loss_dice_7: 2.773  loss_ce_8: 0.2667  loss_mask_8: 0.357  loss_dice_8: 2.775  time: 1.4897  data_time: 0.0717  lr: 4.5023e-06  max_mem: 21589M
[01/18 05:37:16] d2.utils.events INFO:  eta: 6:38:09  iter: 23539  total_loss: 35.37  loss_ce: 0.2888  loss_mask: 0.3691  loss_dice: 2.845  loss_ce_0: 0.5579  loss_mask_0: 0.3656  loss_dice_0: 2.971  loss_ce_1: 0.3154  loss_mask_1: 0.3701  loss_dice_1: 2.873  loss_ce_2: 0.3307  loss_mask_2: 0.3658  loss_dice_2: 2.855  loss_ce_3: 0.287  loss_mask_3: 0.3653  loss_dice_3: 2.853  loss_ce_4: 0.2903  loss_mask_4: 0.3647  loss_dice_4: 2.847  loss_ce_5: 0.2865  loss_mask_5: 0.3637  loss_dice_5: 2.854  loss_ce_6: 0.2808  loss_mask_6: 0.3637  loss_dice_6: 2.846  loss_ce_7: 0.2907  loss_mask_7: 0.3652  loss_dice_7: 2.854  loss_ce_8: 0.2778  loss_mask_8: 0.3668  loss_dice_8: 2.853  time: 1.4897  data_time: 0.0697  lr: 4.4973e-06  max_mem: 21589M
[01/18 05:37:44] d2.utils.events INFO:  eta: 6:37:31  iter: 23559  total_loss: 34.73  loss_ce: 0.2812  loss_mask: 0.3625  loss_dice: 2.781  loss_ce_0: 0.5552  loss_mask_0: 0.3657  loss_dice_0: 2.909  loss_ce_1: 0.301  loss_mask_1: 0.3727  loss_dice_1: 2.816  loss_ce_2: 0.3108  loss_mask_2: 0.3683  loss_dice_2: 2.784  loss_ce_3: 0.2807  loss_mask_3: 0.365  loss_dice_3: 2.786  loss_ce_4: 0.2961  loss_mask_4: 0.3648  loss_dice_4: 2.775  loss_ce_5: 0.2813  loss_mask_5: 0.3657  loss_dice_5: 2.786  loss_ce_6: 0.2736  loss_mask_6: 0.3641  loss_dice_6: 2.775  loss_ce_7: 0.2707  loss_mask_7: 0.3655  loss_dice_7: 2.783  loss_ce_8: 0.2653  loss_mask_8: 0.3634  loss_dice_8: 2.785  time: 1.4897  data_time: 0.0716  lr: 4.4924e-06  max_mem: 21589M
[01/18 05:38:14] d2.utils.events INFO:  eta: 6:37:00  iter: 23579  total_loss: 34.55  loss_ce: 0.2797  loss_mask: 0.3612  loss_dice: 2.765  loss_ce_0: 0.5541  loss_mask_0: 0.3594  loss_dice_0: 2.909  loss_ce_1: 0.3096  loss_mask_1: 0.3647  loss_dice_1: 2.816  loss_ce_2: 0.3104  loss_mask_2: 0.3612  loss_dice_2: 2.795  loss_ce_3: 0.2762  loss_mask_3: 0.3622  loss_dice_3: 2.78  loss_ce_4: 0.2843  loss_mask_4: 0.36  loss_dice_4: 2.776  loss_ce_5: 0.2762  loss_mask_5: 0.36  loss_dice_5: 2.77  loss_ce_6: 0.2734  loss_mask_6: 0.3623  loss_dice_6: 2.768  loss_ce_7: 0.2868  loss_mask_7: 0.3608  loss_dice_7: 2.772  loss_ce_8: 0.2731  loss_mask_8: 0.363  loss_dice_8: 2.777  time: 1.4896  data_time: 0.0711  lr: 4.4875e-06  max_mem: 21589M
[01/18 05:38:43] d2.utils.events INFO:  eta: 6:36:31  iter: 23599  total_loss: 34.95  loss_ce: 0.2828  loss_mask: 0.3638  loss_dice: 2.744  loss_ce_0: 0.5845  loss_mask_0: 0.3655  loss_dice_0: 2.877  loss_ce_1: 0.328  loss_mask_1: 0.3689  loss_dice_1: 2.791  loss_ce_2: 0.3279  loss_mask_2: 0.3671  loss_dice_2: 2.764  loss_ce_3: 0.3023  loss_mask_3: 0.3643  loss_dice_3: 2.76  loss_ce_4: 0.2942  loss_mask_4: 0.3633  loss_dice_4: 2.755  loss_ce_5: 0.2923  loss_mask_5: 0.364  loss_dice_5: 2.747  loss_ce_6: 0.2857  loss_mask_6: 0.3655  loss_dice_6: 2.752  loss_ce_7: 0.2783  loss_mask_7: 0.3653  loss_dice_7: 2.744  loss_ce_8: 0.2787  loss_mask_8: 0.3652  loss_dice_8: 2.757  time: 1.4896  data_time: 0.0730  lr: 4.4826e-06  max_mem: 21589M
[01/18 05:39:12] d2.utils.events INFO:  eta: 6:35:57  iter: 23619  total_loss: 35  loss_ce: 0.2847  loss_mask: 0.3727  loss_dice: 2.78  loss_ce_0: 0.5586  loss_mask_0: 0.3593  loss_dice_0: 2.914  loss_ce_1: 0.3291  loss_mask_1: 0.3746  loss_dice_1: 2.819  loss_ce_2: 0.3207  loss_mask_2: 0.3743  loss_dice_2: 2.794  loss_ce_3: 0.297  loss_mask_3: 0.3735  loss_dice_3: 2.784  loss_ce_4: 0.3016  loss_mask_4: 0.3749  loss_dice_4: 2.776  loss_ce_5: 0.2916  loss_mask_5: 0.3749  loss_dice_5: 2.787  loss_ce_6: 0.2816  loss_mask_6: 0.3731  loss_dice_6: 2.777  loss_ce_7: 0.2815  loss_mask_7: 0.3726  loss_dice_7: 2.774  loss_ce_8: 0.2841  loss_mask_8: 0.3742  loss_dice_8: 2.783  time: 1.4896  data_time: 0.0710  lr: 4.4777e-06  max_mem: 21589M
[01/18 05:39:41] d2.utils.events INFO:  eta: 6:35:17  iter: 23639  total_loss: 34.7  loss_ce: 0.2879  loss_mask: 0.3682  loss_dice: 2.777  loss_ce_0: 0.5369  loss_mask_0: 0.3615  loss_dice_0: 2.906  loss_ce_1: 0.3259  loss_mask_1: 0.3711  loss_dice_1: 2.798  loss_ce_2: 0.309  loss_mask_2: 0.3687  loss_dice_2: 2.793  loss_ce_3: 0.2843  loss_mask_3: 0.3672  loss_dice_3: 2.78  loss_ce_4: 0.2864  loss_mask_4: 0.3672  loss_dice_4: 2.781  loss_ce_5: 0.2717  loss_mask_5: 0.3668  loss_dice_5: 2.778  loss_ce_6: 0.2815  loss_mask_6: 0.3667  loss_dice_6: 2.776  loss_ce_7: 0.2805  loss_mask_7: 0.3674  loss_dice_7: 2.769  loss_ce_8: 0.2754  loss_mask_8: 0.3687  loss_dice_8: 2.77  time: 1.4895  data_time: 0.0703  lr: 4.4728e-06  max_mem: 21589M
[01/18 05:40:10] d2.utils.events INFO:  eta: 6:34:46  iter: 23659  total_loss: 35.4  loss_ce: 0.2693  loss_mask: 0.3651  loss_dice: 2.798  loss_ce_0: 0.5298  loss_mask_0: 0.3634  loss_dice_0: 2.922  loss_ce_1: 0.321  loss_mask_1: 0.3745  loss_dice_1: 2.853  loss_ce_2: 0.3142  loss_mask_2: 0.3715  loss_dice_2: 2.819  loss_ce_3: 0.2929  loss_mask_3: 0.3692  loss_dice_3: 2.801  loss_ce_4: 0.2851  loss_mask_4: 0.3663  loss_dice_4: 2.805  loss_ce_5: 0.2874  loss_mask_5: 0.3628  loss_dice_5: 2.812  loss_ce_6: 0.2632  loss_mask_6: 0.3648  loss_dice_6: 2.801  loss_ce_7: 0.2712  loss_mask_7: 0.3659  loss_dice_7: 2.804  loss_ce_8: 0.2862  loss_mask_8: 0.365  loss_dice_8: 2.801  time: 1.4895  data_time: 0.0682  lr: 4.4678e-06  max_mem: 21589M
[01/18 05:40:39] d2.utils.events INFO:  eta: 6:34:23  iter: 23679  total_loss: 35.41  loss_ce: 0.2711  loss_mask: 0.3751  loss_dice: 2.844  loss_ce_0: 0.5502  loss_mask_0: 0.3683  loss_dice_0: 2.962  loss_ce_1: 0.3385  loss_mask_1: 0.3765  loss_dice_1: 2.893  loss_ce_2: 0.2954  loss_mask_2: 0.3738  loss_dice_2: 2.868  loss_ce_3: 0.2944  loss_mask_3: 0.3727  loss_dice_3: 2.86  loss_ce_4: 0.2807  loss_mask_4: 0.3733  loss_dice_4: 2.86  loss_ce_5: 0.2843  loss_mask_5: 0.3741  loss_dice_5: 2.855  loss_ce_6: 0.2651  loss_mask_6: 0.3741  loss_dice_6: 2.856  loss_ce_7: 0.2715  loss_mask_7: 0.3745  loss_dice_7: 2.849  loss_ce_8: 0.2745  loss_mask_8: 0.3754  loss_dice_8: 2.855  time: 1.4895  data_time: 0.0680  lr: 4.4629e-06  max_mem: 21589M
[01/18 05:41:08] d2.utils.events INFO:  eta: 6:33:54  iter: 23699  total_loss: 35.17  loss_ce: 0.2641  loss_mask: 0.3579  loss_dice: 2.782  loss_ce_0: 0.5636  loss_mask_0: 0.3526  loss_dice_0: 2.929  loss_ce_1: 0.3173  loss_mask_1: 0.3617  loss_dice_1: 2.84  loss_ce_2: 0.2929  loss_mask_2: 0.36  loss_dice_2: 2.812  loss_ce_3: 0.2884  loss_mask_3: 0.3575  loss_dice_3: 2.795  loss_ce_4: 0.2744  loss_mask_4: 0.3593  loss_dice_4: 2.795  loss_ce_5: 0.2787  loss_mask_5: 0.3588  loss_dice_5: 2.791  loss_ce_6: 0.2702  loss_mask_6: 0.3591  loss_dice_6: 2.784  loss_ce_7: 0.2751  loss_mask_7: 0.36  loss_dice_7: 2.795  loss_ce_8: 0.2699  loss_mask_8: 0.3591  loss_dice_8: 2.797  time: 1.4894  data_time: 0.0744  lr: 4.458e-06  max_mem: 21589M
[01/18 05:41:37] d2.utils.events INFO:  eta: 6:33:19  iter: 23719  total_loss: 34.55  loss_ce: 0.2708  loss_mask: 0.3657  loss_dice: 2.766  loss_ce_0: 0.548  loss_mask_0: 0.3597  loss_dice_0: 2.902  loss_ce_1: 0.3004  loss_mask_1: 0.373  loss_dice_1: 2.804  loss_ce_2: 0.3053  loss_mask_2: 0.3677  loss_dice_2: 2.785  loss_ce_3: 0.2895  loss_mask_3: 0.3635  loss_dice_3: 2.778  loss_ce_4: 0.2708  loss_mask_4: 0.3631  loss_dice_4: 2.765  loss_ce_5: 0.2791  loss_mask_5: 0.364  loss_dice_5: 2.767  loss_ce_6: 0.2923  loss_mask_6: 0.3635  loss_dice_6: 2.762  loss_ce_7: 0.2643  loss_mask_7: 0.3636  loss_dice_7: 2.766  loss_ce_8: 0.2604  loss_mask_8: 0.3653  loss_dice_8: 2.765  time: 1.4894  data_time: 0.0677  lr: 4.4531e-06  max_mem: 21589M
[01/18 05:42:06] d2.utils.events INFO:  eta: 6:32:50  iter: 23739  total_loss: 35  loss_ce: 0.2731  loss_mask: 0.3721  loss_dice: 2.78  loss_ce_0: 0.5733  loss_mask_0: 0.365  loss_dice_0: 2.917  loss_ce_1: 0.3143  loss_mask_1: 0.3738  loss_dice_1: 2.818  loss_ce_2: 0.3136  loss_mask_2: 0.3716  loss_dice_2: 2.798  loss_ce_3: 0.2987  loss_mask_3: 0.3702  loss_dice_3: 2.789  loss_ce_4: 0.287  loss_mask_4: 0.37  loss_dice_4: 2.789  loss_ce_5: 0.2786  loss_mask_5: 0.3716  loss_dice_5: 2.786  loss_ce_6: 0.2784  loss_mask_6: 0.3729  loss_dice_6: 2.781  loss_ce_7: 0.2778  loss_mask_7: 0.3717  loss_dice_7: 2.782  loss_ce_8: 0.2809  loss_mask_8: 0.3729  loss_dice_8: 2.779  time: 1.4893  data_time: 0.0704  lr: 4.4481e-06  max_mem: 21589M
[01/18 05:42:35] d2.utils.events INFO:  eta: 6:32:21  iter: 23759  total_loss: 35.53  loss_ce: 0.2869  loss_mask: 0.3644  loss_dice: 2.88  loss_ce_0: 0.5545  loss_mask_0: 0.3656  loss_dice_0: 2.989  loss_ce_1: 0.3186  loss_mask_1: 0.3678  loss_dice_1: 2.916  loss_ce_2: 0.3219  loss_mask_2: 0.365  loss_dice_2: 2.896  loss_ce_3: 0.3001  loss_mask_3: 0.3613  loss_dice_3: 2.888  loss_ce_4: 0.3125  loss_mask_4: 0.3628  loss_dice_4: 2.882  loss_ce_5: 0.3011  loss_mask_5: 0.3631  loss_dice_5: 2.874  loss_ce_6: 0.2923  loss_mask_6: 0.3646  loss_dice_6: 2.882  loss_ce_7: 0.2808  loss_mask_7: 0.3633  loss_dice_7: 2.876  loss_ce_8: 0.2887  loss_mask_8: 0.3638  loss_dice_8: 2.885  time: 1.4893  data_time: 0.0731  lr: 4.4432e-06  max_mem: 21589M
[01/18 05:43:04] d2.utils.events INFO:  eta: 6:31:37  iter: 23779  total_loss: 35.22  loss_ce: 0.2881  loss_mask: 0.3593  loss_dice: 2.799  loss_ce_0: 0.5256  loss_mask_0: 0.3608  loss_dice_0: 2.928  loss_ce_1: 0.3054  loss_mask_1: 0.3656  loss_dice_1: 2.838  loss_ce_2: 0.3061  loss_mask_2: 0.364  loss_dice_2: 2.82  loss_ce_3: 0.2847  loss_mask_3: 0.3621  loss_dice_3: 2.804  loss_ce_4: 0.2856  loss_mask_4: 0.3608  loss_dice_4: 2.804  loss_ce_5: 0.2945  loss_mask_5: 0.3613  loss_dice_5: 2.796  loss_ce_6: 0.2831  loss_mask_6: 0.3603  loss_dice_6: 2.801  loss_ce_7: 0.2832  loss_mask_7: 0.3597  loss_dice_7: 2.796  loss_ce_8: 0.2818  loss_mask_8: 0.3597  loss_dice_8: 2.797  time: 1.4893  data_time: 0.0703  lr: 4.4383e-06  max_mem: 21589M
[01/18 05:43:34] d2.utils.events INFO:  eta: 6:31:13  iter: 23799  total_loss: 35.09  loss_ce: 0.2627  loss_mask: 0.3617  loss_dice: 2.781  loss_ce_0: 0.5565  loss_mask_0: 0.3599  loss_dice_0: 2.913  loss_ce_1: 0.3162  loss_mask_1: 0.3695  loss_dice_1: 2.828  loss_ce_2: 0.2984  loss_mask_2: 0.3661  loss_dice_2: 2.809  loss_ce_3: 0.2803  loss_mask_3: 0.3615  loss_dice_3: 2.796  loss_ce_4: 0.28  loss_mask_4: 0.3629  loss_dice_4: 2.785  loss_ce_5: 0.2711  loss_mask_5: 0.3631  loss_dice_5: 2.784  loss_ce_6: 0.2663  loss_mask_6: 0.362  loss_dice_6: 2.782  loss_ce_7: 0.2613  loss_mask_7: 0.3612  loss_dice_7: 2.785  loss_ce_8: 0.2642  loss_mask_8: 0.3606  loss_dice_8: 2.781  time: 1.4893  data_time: 0.0739  lr: 4.4334e-06  max_mem: 21589M
[01/18 05:44:03] d2.utils.events INFO:  eta: 6:30:44  iter: 23819  total_loss: 34.7  loss_ce: 0.2749  loss_mask: 0.3597  loss_dice: 2.768  loss_ce_0: 0.5701  loss_mask_0: 0.3611  loss_dice_0: 2.898  loss_ce_1: 0.3203  loss_mask_1: 0.3618  loss_dice_1: 2.811  loss_ce_2: 0.3248  loss_mask_2: 0.3592  loss_dice_2: 2.79  loss_ce_3: 0.2992  loss_mask_3: 0.3583  loss_dice_3: 2.767  loss_ce_4: 0.2978  loss_mask_4: 0.3589  loss_dice_4: 2.77  loss_ce_5: 0.2798  loss_mask_5: 0.3579  loss_dice_5: 2.773  loss_ce_6: 0.2898  loss_mask_6: 0.357  loss_dice_6: 2.772  loss_ce_7: 0.2816  loss_mask_7: 0.3592  loss_dice_7: 2.768  loss_ce_8: 0.2739  loss_mask_8: 0.3588  loss_dice_8: 2.769  time: 1.4892  data_time: 0.0676  lr: 4.4284e-06  max_mem: 21589M
[01/18 05:44:32] d2.utils.events INFO:  eta: 6:30:16  iter: 23839  total_loss: 34.97  loss_ce: 0.2767  loss_mask: 0.3554  loss_dice: 2.83  loss_ce_0: 0.5621  loss_mask_0: 0.3545  loss_dice_0: 2.973  loss_ce_1: 0.3025  loss_mask_1: 0.362  loss_dice_1: 2.877  loss_ce_2: 0.2923  loss_mask_2: 0.3586  loss_dice_2: 2.857  loss_ce_3: 0.2874  loss_mask_3: 0.3571  loss_dice_3: 2.839  loss_ce_4: 0.2927  loss_mask_4: 0.3566  loss_dice_4: 2.842  loss_ce_5: 0.2884  loss_mask_5: 0.3561  loss_dice_5: 2.836  loss_ce_6: 0.2743  loss_mask_6: 0.3559  loss_dice_6: 2.839  loss_ce_7: 0.2613  loss_mask_7: 0.3563  loss_dice_7: 2.841  loss_ce_8: 0.2626  loss_mask_8: 0.3558  loss_dice_8: 2.841  time: 1.4892  data_time: 0.0721  lr: 4.4235e-06  max_mem: 21589M
[01/18 05:45:01] d2.utils.events INFO:  eta: 6:29:43  iter: 23859  total_loss: 33.97  loss_ce: 0.2534  loss_mask: 0.3616  loss_dice: 2.713  loss_ce_0: 0.5472  loss_mask_0: 0.3625  loss_dice_0: 2.847  loss_ce_1: 0.3024  loss_mask_1: 0.3719  loss_dice_1: 2.753  loss_ce_2: 0.3041  loss_mask_2: 0.3677  loss_dice_2: 2.732  loss_ce_3: 0.2749  loss_mask_3: 0.3669  loss_dice_3: 2.716  loss_ce_4: 0.2753  loss_mask_4: 0.3664  loss_dice_4: 2.712  loss_ce_5: 0.2727  loss_mask_5: 0.363  loss_dice_5: 2.717  loss_ce_6: 0.2516  loss_mask_6: 0.3614  loss_dice_6: 2.715  loss_ce_7: 0.2648  loss_mask_7: 0.362  loss_dice_7: 2.716  loss_ce_8: 0.2663  loss_mask_8: 0.3617  loss_dice_8: 2.714  time: 1.4892  data_time: 0.0682  lr: 4.4186e-06  max_mem: 21589M
[01/18 05:45:30] d2.utils.events INFO:  eta: 6:29:07  iter: 23879  total_loss: 34.78  loss_ce: 0.2695  loss_mask: 0.3726  loss_dice: 2.789  loss_ce_0: 0.5604  loss_mask_0: 0.3695  loss_dice_0: 2.934  loss_ce_1: 0.3057  loss_mask_1: 0.375  loss_dice_1: 2.839  loss_ce_2: 0.3086  loss_mask_2: 0.3716  loss_dice_2: 2.807  loss_ce_3: 0.2819  loss_mask_3: 0.3711  loss_dice_3: 2.784  loss_ce_4: 0.2798  loss_mask_4: 0.3714  loss_dice_4: 2.791  loss_ce_5: 0.28  loss_mask_5: 0.3698  loss_dice_5: 2.798  loss_ce_6: 0.2703  loss_mask_6: 0.3704  loss_dice_6: 2.805  loss_ce_7: 0.2711  loss_mask_7: 0.3716  loss_dice_7: 2.798  loss_ce_8: 0.2616  loss_mask_8: 0.3734  loss_dice_8: 2.794  time: 1.4891  data_time: 0.0702  lr: 4.4137e-06  max_mem: 21589M
[01/18 05:45:59] d2.utils.events INFO:  eta: 6:28:29  iter: 23899  total_loss: 34.97  loss_ce: 0.2719  loss_mask: 0.3669  loss_dice: 2.77  loss_ce_0: 0.6051  loss_mask_0: 0.3592  loss_dice_0: 2.92  loss_ce_1: 0.3247  loss_mask_1: 0.3669  loss_dice_1: 2.825  loss_ce_2: 0.3115  loss_mask_2: 0.3651  loss_dice_2: 2.795  loss_ce_3: 0.2921  loss_mask_3: 0.3658  loss_dice_3: 2.781  loss_ce_4: 0.2757  loss_mask_4: 0.3656  loss_dice_4: 2.784  loss_ce_5: 0.2775  loss_mask_5: 0.366  loss_dice_5: 2.785  loss_ce_6: 0.2678  loss_mask_6: 0.3656  loss_dice_6: 2.784  loss_ce_7: 0.2696  loss_mask_7: 0.367  loss_dice_7: 2.773  loss_ce_8: 0.2707  loss_mask_8: 0.3661  loss_dice_8: 2.775  time: 1.4891  data_time: 0.0712  lr: 4.4087e-06  max_mem: 21589M
[01/18 05:46:28] d2.utils.events INFO:  eta: 6:28:04  iter: 23919  total_loss: 34.53  loss_ce: 0.2817  loss_mask: 0.3562  loss_dice: 2.768  loss_ce_0: 0.5446  loss_mask_0: 0.3551  loss_dice_0: 2.893  loss_ce_1: 0.3205  loss_mask_1: 0.3601  loss_dice_1: 2.81  loss_ce_2: 0.3051  loss_mask_2: 0.358  loss_dice_2: 2.791  loss_ce_3: 0.2891  loss_mask_3: 0.3561  loss_dice_3: 2.778  loss_ce_4: 0.2872  loss_mask_4: 0.3571  loss_dice_4: 2.782  loss_ce_5: 0.278  loss_mask_5: 0.356  loss_dice_5: 2.774  loss_ce_6: 0.2724  loss_mask_6: 0.3547  loss_dice_6: 2.772  loss_ce_7: 0.2722  loss_mask_7: 0.355  loss_dice_7: 2.767  loss_ce_8: 0.2701  loss_mask_8: 0.3559  loss_dice_8: 2.766  time: 1.4891  data_time: 0.0720  lr: 4.4038e-06  max_mem: 21589M
[01/18 05:46:57] d2.utils.events INFO:  eta: 6:27:29  iter: 23939  total_loss: 35.15  loss_ce: 0.235  loss_mask: 0.3594  loss_dice: 2.843  loss_ce_0: 0.5628  loss_mask_0: 0.3545  loss_dice_0: 2.998  loss_ce_1: 0.2977  loss_mask_1: 0.364  loss_dice_1: 2.894  loss_ce_2: 0.3116  loss_mask_2: 0.36  loss_dice_2: 2.877  loss_ce_3: 0.2698  loss_mask_3: 0.3607  loss_dice_3: 2.856  loss_ce_4: 0.2687  loss_mask_4: 0.3587  loss_dice_4: 2.858  loss_ce_5: 0.2609  loss_mask_5: 0.3578  loss_dice_5: 2.846  loss_ce_6: 0.2654  loss_mask_6: 0.356  loss_dice_6: 2.847  loss_ce_7: 0.2615  loss_mask_7: 0.3579  loss_dice_7: 2.845  loss_ce_8: 0.2602  loss_mask_8: 0.3586  loss_dice_8: 2.849  time: 1.4890  data_time: 0.0675  lr: 4.3989e-06  max_mem: 21589M
[01/18 05:47:26] d2.utils.events INFO:  eta: 6:27:06  iter: 23959  total_loss: 34.37  loss_ce: 0.2501  loss_mask: 0.365  loss_dice: 2.75  loss_ce_0: 0.5373  loss_mask_0: 0.3563  loss_dice_0: 2.881  loss_ce_1: 0.3123  loss_mask_1: 0.3671  loss_dice_1: 2.798  loss_ce_2: 0.2856  loss_mask_2: 0.3624  loss_dice_2: 2.772  loss_ce_3: 0.2769  loss_mask_3: 0.3591  loss_dice_3: 2.765  loss_ce_4: 0.2597  loss_mask_4: 0.3586  loss_dice_4: 2.762  loss_ce_5: 0.2575  loss_mask_5: 0.361  loss_dice_5: 2.758  loss_ce_6: 0.2442  loss_mask_6: 0.3614  loss_dice_6: 2.753  loss_ce_7: 0.2551  loss_mask_7: 0.3612  loss_dice_7: 2.746  loss_ce_8: 0.2432  loss_mask_8: 0.3624  loss_dice_8: 2.748  time: 1.4890  data_time: 0.0646  lr: 4.3939e-06  max_mem: 21589M
[01/18 05:47:55] d2.utils.events INFO:  eta: 6:26:40  iter: 23979  total_loss: 34.36  loss_ce: 0.2516  loss_mask: 0.362  loss_dice: 2.744  loss_ce_0: 0.5711  loss_mask_0: 0.3621  loss_dice_0: 2.878  loss_ce_1: 0.3036  loss_mask_1: 0.3701  loss_dice_1: 2.783  loss_ce_2: 0.3089  loss_mask_2: 0.3667  loss_dice_2: 2.761  loss_ce_3: 0.2783  loss_mask_3: 0.3644  loss_dice_3: 2.746  loss_ce_4: 0.275  loss_mask_4: 0.3634  loss_dice_4: 2.737  loss_ce_5: 0.2775  loss_mask_5: 0.3626  loss_dice_5: 2.754  loss_ce_6: 0.2649  loss_mask_6: 0.3623  loss_dice_6: 2.742  loss_ce_7: 0.2574  loss_mask_7: 0.3632  loss_dice_7: 2.745  loss_ce_8: 0.2594  loss_mask_8: 0.3634  loss_dice_8: 2.74  time: 1.4890  data_time: 0.0707  lr: 4.389e-06  max_mem: 21589M
[01/18 05:48:23] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 05:48:24] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 05:48:24] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 05:48:25] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 05:48:40] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0101 s/iter. Inference: 0.1705 s/iter. Eval: 0.2429 s/iter. Total: 0.4234 s/iter. ETA=0:07:38
[01/18 05:48:45] d2.evaluation.evaluator INFO: Inference done 23/1093. Dataloading: 0.0111 s/iter. Inference: 0.1763 s/iter. Eval: 0.2514 s/iter. Total: 0.4389 s/iter. ETA=0:07:49
[01/18 05:48:50] d2.evaluation.evaluator INFO: Inference done 36/1093. Dataloading: 0.0106 s/iter. Inference: 0.1718 s/iter. Eval: 0.2444 s/iter. Total: 0.4270 s/iter. ETA=0:07:31
[01/18 05:48:55] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0114 s/iter. Inference: 0.1676 s/iter. Eval: 0.2307 s/iter. Total: 0.4098 s/iter. ETA=0:07:07
[01/18 05:49:00] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0118 s/iter. Inference: 0.1659 s/iter. Eval: 0.2347 s/iter. Total: 0.4125 s/iter. ETA=0:07:05
[01/18 05:49:06] d2.evaluation.evaluator INFO: Inference done 76/1093. Dataloading: 0.0119 s/iter. Inference: 0.1632 s/iter. Eval: 0.2297 s/iter. Total: 0.4049 s/iter. ETA=0:06:51
[01/18 05:49:11] d2.evaluation.evaluator INFO: Inference done 88/1093. Dataloading: 0.0120 s/iter. Inference: 0.1629 s/iter. Eval: 0.2351 s/iter. Total: 0.4101 s/iter. ETA=0:06:52
[01/18 05:49:16] d2.evaluation.evaluator INFO: Inference done 101/1093. Dataloading: 0.0120 s/iter. Inference: 0.1638 s/iter. Eval: 0.2327 s/iter. Total: 0.4087 s/iter. ETA=0:06:45
[01/18 05:49:21] d2.evaluation.evaluator INFO: Inference done 114/1093. Dataloading: 0.0119 s/iter. Inference: 0.1640 s/iter. Eval: 0.2307 s/iter. Total: 0.4067 s/iter. ETA=0:06:38
[01/18 05:49:26] d2.evaluation.evaluator INFO: Inference done 128/1093. Dataloading: 0.0118 s/iter. Inference: 0.1633 s/iter. Eval: 0.2270 s/iter. Total: 0.4022 s/iter. ETA=0:06:28
[01/18 05:49:32] d2.evaluation.evaluator INFO: Inference done 143/1093. Dataloading: 0.0116 s/iter. Inference: 0.1629 s/iter. Eval: 0.2225 s/iter. Total: 0.3970 s/iter. ETA=0:06:17
[01/18 05:49:37] d2.evaluation.evaluator INFO: Inference done 158/1093. Dataloading: 0.0117 s/iter. Inference: 0.1634 s/iter. Eval: 0.2183 s/iter. Total: 0.3935 s/iter. ETA=0:06:07
[01/18 05:49:42] d2.evaluation.evaluator INFO: Inference done 170/1093. Dataloading: 0.0118 s/iter. Inference: 0.1627 s/iter. Eval: 0.2218 s/iter. Total: 0.3964 s/iter. ETA=0:06:05
[01/18 05:49:48] d2.evaluation.evaluator INFO: Inference done 184/1093. Dataloading: 0.0117 s/iter. Inference: 0.1628 s/iter. Eval: 0.2199 s/iter. Total: 0.3945 s/iter. ETA=0:05:58
[01/18 05:49:53] d2.evaluation.evaluator INFO: Inference done 199/1093. Dataloading: 0.0116 s/iter. Inference: 0.1620 s/iter. Eval: 0.2179 s/iter. Total: 0.3916 s/iter. ETA=0:05:50
[01/18 05:49:58] d2.evaluation.evaluator INFO: Inference done 213/1093. Dataloading: 0.0117 s/iter. Inference: 0.1618 s/iter. Eval: 0.2178 s/iter. Total: 0.3914 s/iter. ETA=0:05:44
[01/18 05:50:04] d2.evaluation.evaluator INFO: Inference done 228/1093. Dataloading: 0.0117 s/iter. Inference: 0.1607 s/iter. Eval: 0.2157 s/iter. Total: 0.3883 s/iter. ETA=0:05:35
[01/18 05:50:09] d2.evaluation.evaluator INFO: Inference done 242/1093. Dataloading: 0.0117 s/iter. Inference: 0.1599 s/iter. Eval: 0.2158 s/iter. Total: 0.3875 s/iter. ETA=0:05:29
[01/18 05:50:14] d2.evaluation.evaluator INFO: Inference done 255/1093. Dataloading: 0.0117 s/iter. Inference: 0.1596 s/iter. Eval: 0.2159 s/iter. Total: 0.3873 s/iter. ETA=0:05:24
[01/18 05:50:19] d2.evaluation.evaluator INFO: Inference done 269/1093. Dataloading: 0.0118 s/iter. Inference: 0.1596 s/iter. Eval: 0.2151 s/iter. Total: 0.3866 s/iter. ETA=0:05:18
[01/18 05:50:24] d2.evaluation.evaluator INFO: Inference done 283/1093. Dataloading: 0.0118 s/iter. Inference: 0.1592 s/iter. Eval: 0.2144 s/iter. Total: 0.3855 s/iter. ETA=0:05:12
[01/18 05:50:30] d2.evaluation.evaluator INFO: Inference done 298/1093. Dataloading: 0.0117 s/iter. Inference: 0.1588 s/iter. Eval: 0.2135 s/iter. Total: 0.3841 s/iter. ETA=0:05:05
[01/18 05:50:35] d2.evaluation.evaluator INFO: Inference done 309/1093. Dataloading: 0.0118 s/iter. Inference: 0.1600 s/iter. Eval: 0.2158 s/iter. Total: 0.3877 s/iter. ETA=0:05:03
[01/18 05:50:40] d2.evaluation.evaluator INFO: Inference done 322/1093. Dataloading: 0.0119 s/iter. Inference: 0.1601 s/iter. Eval: 0.2164 s/iter. Total: 0.3885 s/iter. ETA=0:04:59
[01/18 05:50:45] d2.evaluation.evaluator INFO: Inference done 338/1093. Dataloading: 0.0119 s/iter. Inference: 0.1594 s/iter. Eval: 0.2142 s/iter. Total: 0.3855 s/iter. ETA=0:04:51
[01/18 05:50:50] d2.evaluation.evaluator INFO: Inference done 355/1093. Dataloading: 0.0117 s/iter. Inference: 0.1584 s/iter. Eval: 0.2112 s/iter. Total: 0.3814 s/iter. ETA=0:04:41
[01/18 05:50:55] d2.evaluation.evaluator INFO: Inference done 367/1093. Dataloading: 0.0117 s/iter. Inference: 0.1586 s/iter. Eval: 0.2123 s/iter. Total: 0.3827 s/iter. ETA=0:04:37
[01/18 05:51:01] d2.evaluation.evaluator INFO: Inference done 381/1093. Dataloading: 0.0117 s/iter. Inference: 0.1588 s/iter. Eval: 0.2116 s/iter. Total: 0.3822 s/iter. ETA=0:04:32
[01/18 05:51:06] d2.evaluation.evaluator INFO: Inference done 394/1093. Dataloading: 0.0117 s/iter. Inference: 0.1585 s/iter. Eval: 0.2132 s/iter. Total: 0.3835 s/iter. ETA=0:04:28
[01/18 05:51:11] d2.evaluation.evaluator INFO: Inference done 406/1093. Dataloading: 0.0117 s/iter. Inference: 0.1592 s/iter. Eval: 0.2135 s/iter. Total: 0.3845 s/iter. ETA=0:04:24
[01/18 05:51:16] d2.evaluation.evaluator INFO: Inference done 418/1093. Dataloading: 0.0118 s/iter. Inference: 0.1598 s/iter. Eval: 0.2140 s/iter. Total: 0.3857 s/iter. ETA=0:04:20
[01/18 05:51:21] d2.evaluation.evaluator INFO: Inference done 431/1093. Dataloading: 0.0118 s/iter. Inference: 0.1598 s/iter. Eval: 0.2144 s/iter. Total: 0.3862 s/iter. ETA=0:04:15
[01/18 05:51:27] d2.evaluation.evaluator INFO: Inference done 445/1093. Dataloading: 0.0118 s/iter. Inference: 0.1603 s/iter. Eval: 0.2133 s/iter. Total: 0.3855 s/iter. ETA=0:04:09
[01/18 05:51:32] d2.evaluation.evaluator INFO: Inference done 458/1093. Dataloading: 0.0118 s/iter. Inference: 0.1606 s/iter. Eval: 0.2136 s/iter. Total: 0.3861 s/iter. ETA=0:04:05
[01/18 05:51:37] d2.evaluation.evaluator INFO: Inference done 472/1093. Dataloading: 0.0118 s/iter. Inference: 0.1609 s/iter. Eval: 0.2126 s/iter. Total: 0.3853 s/iter. ETA=0:03:59
[01/18 05:51:42] d2.evaluation.evaluator INFO: Inference done 487/1093. Dataloading: 0.0118 s/iter. Inference: 0.1605 s/iter. Eval: 0.2118 s/iter. Total: 0.3841 s/iter. ETA=0:03:52
[01/18 05:51:47] d2.evaluation.evaluator INFO: Inference done 503/1093. Dataloading: 0.0117 s/iter. Inference: 0.1601 s/iter. Eval: 0.2101 s/iter. Total: 0.3820 s/iter. ETA=0:03:45
[01/18 05:51:52] d2.evaluation.evaluator INFO: Inference done 517/1093. Dataloading: 0.0117 s/iter. Inference: 0.1605 s/iter. Eval: 0.2096 s/iter. Total: 0.3818 s/iter. ETA=0:03:39
[01/18 05:51:58] d2.evaluation.evaluator INFO: Inference done 529/1093. Dataloading: 0.0117 s/iter. Inference: 0.1607 s/iter. Eval: 0.2109 s/iter. Total: 0.3834 s/iter. ETA=0:03:36
[01/18 05:52:03] d2.evaluation.evaluator INFO: Inference done 544/1093. Dataloading: 0.0117 s/iter. Inference: 0.1606 s/iter. Eval: 0.2105 s/iter. Total: 0.3829 s/iter. ETA=0:03:30
[01/18 05:52:09] d2.evaluation.evaluator INFO: Inference done 557/1093. Dataloading: 0.0117 s/iter. Inference: 0.1609 s/iter. Eval: 0.2109 s/iter. Total: 0.3836 s/iter. ETA=0:03:25
[01/18 05:52:14] d2.evaluation.evaluator INFO: Inference done 570/1093. Dataloading: 0.0117 s/iter. Inference: 0.1612 s/iter. Eval: 0.2112 s/iter. Total: 0.3842 s/iter. ETA=0:03:20
[01/18 05:52:19] d2.evaluation.evaluator INFO: Inference done 585/1093. Dataloading: 0.0118 s/iter. Inference: 0.1617 s/iter. Eval: 0.2097 s/iter. Total: 0.3832 s/iter. ETA=0:03:14
[01/18 05:52:24] d2.evaluation.evaluator INFO: Inference done 598/1093. Dataloading: 0.0118 s/iter. Inference: 0.1617 s/iter. Eval: 0.2098 s/iter. Total: 0.3834 s/iter. ETA=0:03:09
[01/18 05:52:30] d2.evaluation.evaluator INFO: Inference done 609/1093. Dataloading: 0.0119 s/iter. Inference: 0.1620 s/iter. Eval: 0.2112 s/iter. Total: 0.3851 s/iter. ETA=0:03:06
[01/18 05:52:35] d2.evaluation.evaluator INFO: Inference done 623/1093. Dataloading: 0.0118 s/iter. Inference: 0.1623 s/iter. Eval: 0.2110 s/iter. Total: 0.3852 s/iter. ETA=0:03:01
[01/18 05:52:40] d2.evaluation.evaluator INFO: Inference done 636/1093. Dataloading: 0.0118 s/iter. Inference: 0.1625 s/iter. Eval: 0.2112 s/iter. Total: 0.3857 s/iter. ETA=0:02:56
[01/18 05:52:45] d2.evaluation.evaluator INFO: Inference done 649/1093. Dataloading: 0.0118 s/iter. Inference: 0.1629 s/iter. Eval: 0.2109 s/iter. Total: 0.3857 s/iter. ETA=0:02:51
[01/18 05:52:51] d2.evaluation.evaluator INFO: Inference done 663/1093. Dataloading: 0.0118 s/iter. Inference: 0.1628 s/iter. Eval: 0.2106 s/iter. Total: 0.3854 s/iter. ETA=0:02:45
[01/18 05:52:56] d2.evaluation.evaluator INFO: Inference done 676/1093. Dataloading: 0.0118 s/iter. Inference: 0.1631 s/iter. Eval: 0.2105 s/iter. Total: 0.3854 s/iter. ETA=0:02:40
[01/18 05:53:01] d2.evaluation.evaluator INFO: Inference done 691/1093. Dataloading: 0.0118 s/iter. Inference: 0.1632 s/iter. Eval: 0.2096 s/iter. Total: 0.3847 s/iter. ETA=0:02:34
[01/18 05:53:06] d2.evaluation.evaluator INFO: Inference done 703/1093. Dataloading: 0.0118 s/iter. Inference: 0.1631 s/iter. Eval: 0.2103 s/iter. Total: 0.3853 s/iter. ETA=0:02:30
[01/18 05:53:11] d2.evaluation.evaluator INFO: Inference done 715/1093. Dataloading: 0.0118 s/iter. Inference: 0.1630 s/iter. Eval: 0.2114 s/iter. Total: 0.3863 s/iter. ETA=0:02:26
[01/18 05:53:17] d2.evaluation.evaluator INFO: Inference done 729/1093. Dataloading: 0.0118 s/iter. Inference: 0.1632 s/iter. Eval: 0.2110 s/iter. Total: 0.3861 s/iter. ETA=0:02:20
[01/18 05:53:22] d2.evaluation.evaluator INFO: Inference done 743/1093. Dataloading: 0.0118 s/iter. Inference: 0.1633 s/iter. Eval: 0.2106 s/iter. Total: 0.3858 s/iter. ETA=0:02:15
[01/18 05:53:27] d2.evaluation.evaluator INFO: Inference done 756/1093. Dataloading: 0.0118 s/iter. Inference: 0.1636 s/iter. Eval: 0.2106 s/iter. Total: 0.3861 s/iter. ETA=0:02:10
[01/18 05:53:32] d2.evaluation.evaluator INFO: Inference done 768/1093. Dataloading: 0.0118 s/iter. Inference: 0.1636 s/iter. Eval: 0.2112 s/iter. Total: 0.3867 s/iter. ETA=0:02:05
[01/18 05:53:37] d2.evaluation.evaluator INFO: Inference done 782/1093. Dataloading: 0.0118 s/iter. Inference: 0.1636 s/iter. Eval: 0.2109 s/iter. Total: 0.3864 s/iter. ETA=0:02:00
[01/18 05:53:43] d2.evaluation.evaluator INFO: Inference done 798/1093. Dataloading: 0.0118 s/iter. Inference: 0.1634 s/iter. Eval: 0.2100 s/iter. Total: 0.3854 s/iter. ETA=0:01:53
[01/18 05:53:48] d2.evaluation.evaluator INFO: Inference done 811/1093. Dataloading: 0.0118 s/iter. Inference: 0.1634 s/iter. Eval: 0.2101 s/iter. Total: 0.3855 s/iter. ETA=0:01:48
[01/18 05:53:53] d2.evaluation.evaluator INFO: Inference done 826/1093. Dataloading: 0.0117 s/iter. Inference: 0.1634 s/iter. Eval: 0.2095 s/iter. Total: 0.3848 s/iter. ETA=0:01:42
[01/18 05:53:58] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0117 s/iter. Inference: 0.1636 s/iter. Eval: 0.2095 s/iter. Total: 0.3850 s/iter. ETA=0:01:37
[01/18 05:54:03] d2.evaluation.evaluator INFO: Inference done 852/1093. Dataloading: 0.0117 s/iter. Inference: 0.1638 s/iter. Eval: 0.2096 s/iter. Total: 0.3851 s/iter. ETA=0:01:32
[01/18 05:54:08] d2.evaluation.evaluator INFO: Inference done 865/1093. Dataloading: 0.0117 s/iter. Inference: 0.1636 s/iter. Eval: 0.2100 s/iter. Total: 0.3854 s/iter. ETA=0:01:27
[01/18 05:54:14] d2.evaluation.evaluator INFO: Inference done 877/1093. Dataloading: 0.0117 s/iter. Inference: 0.1642 s/iter. Eval: 0.2101 s/iter. Total: 0.3861 s/iter. ETA=0:01:23
[01/18 05:54:19] d2.evaluation.evaluator INFO: Inference done 888/1093. Dataloading: 0.0118 s/iter. Inference: 0.1649 s/iter. Eval: 0.2108 s/iter. Total: 0.3875 s/iter. ETA=0:01:19
[01/18 05:54:24] d2.evaluation.evaluator INFO: Inference done 902/1093. Dataloading: 0.0118 s/iter. Inference: 0.1649 s/iter. Eval: 0.2105 s/iter. Total: 0.3873 s/iter. ETA=0:01:13
[01/18 05:54:30] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0117 s/iter. Inference: 0.1646 s/iter. Eval: 0.2101 s/iter. Total: 0.3866 s/iter. ETA=0:01:08
[01/18 05:54:35] d2.evaluation.evaluator INFO: Inference done 931/1093. Dataloading: 0.0117 s/iter. Inference: 0.1644 s/iter. Eval: 0.2100 s/iter. Total: 0.3863 s/iter. ETA=0:01:02
[01/18 05:54:40] d2.evaluation.evaluator INFO: Inference done 944/1093. Dataloading: 0.0117 s/iter. Inference: 0.1642 s/iter. Eval: 0.2103 s/iter. Total: 0.3864 s/iter. ETA=0:00:57
[01/18 05:54:45] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0117 s/iter. Inference: 0.1642 s/iter. Eval: 0.2104 s/iter. Total: 0.3863 s/iter. ETA=0:00:52
[01/18 05:54:50] d2.evaluation.evaluator INFO: Inference done 971/1093. Dataloading: 0.0117 s/iter. Inference: 0.1641 s/iter. Eval: 0.2106 s/iter. Total: 0.3865 s/iter. ETA=0:00:47
[01/18 05:54:56] d2.evaluation.evaluator INFO: Inference done 986/1093. Dataloading: 0.0117 s/iter. Inference: 0.1641 s/iter. Eval: 0.2101 s/iter. Total: 0.3859 s/iter. ETA=0:00:41
[01/18 05:55:01] d2.evaluation.evaluator INFO: Inference done 1000/1093. Dataloading: 0.0117 s/iter. Inference: 0.1640 s/iter. Eval: 0.2100 s/iter. Total: 0.3858 s/iter. ETA=0:00:35
[01/18 05:55:06] d2.evaluation.evaluator INFO: Inference done 1014/1093. Dataloading: 0.0116 s/iter. Inference: 0.1642 s/iter. Eval: 0.2097 s/iter. Total: 0.3856 s/iter. ETA=0:00:30
[01/18 05:55:11] d2.evaluation.evaluator INFO: Inference done 1025/1093. Dataloading: 0.0117 s/iter. Inference: 0.1647 s/iter. Eval: 0.2100 s/iter. Total: 0.3865 s/iter. ETA=0:00:26
[01/18 05:55:16] d2.evaluation.evaluator INFO: Inference done 1041/1093. Dataloading: 0.0116 s/iter. Inference: 0.1644 s/iter. Eval: 0.2095 s/iter. Total: 0.3856 s/iter. ETA=0:00:20
[01/18 05:55:22] d2.evaluation.evaluator INFO: Inference done 1057/1093. Dataloading: 0.0116 s/iter. Inference: 0.1640 s/iter. Eval: 0.2089 s/iter. Total: 0.3847 s/iter. ETA=0:00:13
[01/18 05:55:27] d2.evaluation.evaluator INFO: Inference done 1075/1093. Dataloading: 0.0116 s/iter. Inference: 0.1636 s/iter. Eval: 0.2077 s/iter. Total: 0.3830 s/iter. ETA=0:00:06
[01/18 05:55:32] d2.evaluation.evaluator INFO: Inference done 1091/1093. Dataloading: 0.0115 s/iter. Inference: 0.1634 s/iter. Eval: 0.2070 s/iter. Total: 0.3820 s/iter. ETA=0:00:00
[01/18 05:55:33] d2.evaluation.evaluator INFO: Total inference time: 0:06:55.938129 (0.382296 s / iter per device, on 4 devices)
[01/18 05:55:33] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:57 (0.163293 s / iter per device, on 4 devices)
[01/18 05:55:56] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.442923006672551, 'mIoU': 19.265800436092196, 'fwIoU': 41.2113328086954, 'IoU-0': nan, 'IoU-1': 95.00353995949202, 'IoU-2': 46.13848485483897, 'IoU-3': 57.83297587546794, 'IoU-4': 52.6695045491162, 'IoU-5': 46.24110814716016, 'IoU-6': 41.28138351332965, 'IoU-7': 33.95703125, 'IoU-8': 22.62517582330233, 'IoU-9': 35.54171228117526, 'IoU-10': 38.543098791176476, 'IoU-11': 47.49804867912049, 'IoU-12': 48.904673116002265, 'IoU-13': 47.39503655572599, 'IoU-14': 46.79683935481675, 'IoU-15': 45.9799079712746, 'IoU-16': 48.42269539573954, 'IoU-17': 45.292462235537165, 'IoU-18': 46.139761735506866, 'IoU-19': 45.52367856718251, 'IoU-20': 45.68524283988705, 'IoU-21': 44.07627861516518, 'IoU-22': 46.08685630984785, 'IoU-23': 43.295561741212765, 'IoU-24': 43.75519547722998, 'IoU-25': 42.66998828866913, 'IoU-26': 43.63784055389107, 'IoU-27': 44.68613520601727, 'IoU-28': 43.1947072936333, 'IoU-29': 44.390579079504136, 'IoU-30': 41.85148216157849, 'IoU-31': 43.39562943335726, 'IoU-32': 41.46951195823664, 'IoU-33': 41.56584078288182, 'IoU-34': 41.3239979115661, 'IoU-35': 42.633326839796716, 'IoU-36': 41.43736688009481, 'IoU-37': 39.98657785937791, 'IoU-38': 38.95035476116398, 'IoU-39': 39.11171633836347, 'IoU-40': 39.88410381583465, 'IoU-41': 37.735441110955904, 'IoU-42': 37.17843977520896, 'IoU-43': 36.15507281612035, 'IoU-44': 35.78754587384916, 'IoU-45': 34.20078154392533, 'IoU-46': 34.70000254370883, 'IoU-47': 33.622014451830125, 'IoU-48': 34.45731487084649, 'IoU-49': 33.33531307747698, 'IoU-50': 34.26506550677208, 'IoU-51': 32.74085834728629, 'IoU-52': 31.76662184432771, 'IoU-53': 31.7687252491828, 'IoU-54': 31.25987326999572, 'IoU-55': 30.465594493784742, 'IoU-56': 29.67707078369842, 'IoU-57': 29.227059716529258, 'IoU-58': 27.037662423642168, 'IoU-59': 25.537490274916063, 'IoU-60': 24.903014293393714, 'IoU-61': 24.55549646604873, 'IoU-62': 25.410920778498163, 'IoU-63': 23.93479631675943, 'IoU-64': 22.51583099170065, 'IoU-65': 22.530823298568396, 'IoU-66': 21.02324013904538, 'IoU-67': 20.582507553586442, 'IoU-68': 19.815564377764623, 'IoU-69': 20.063357670302278, 'IoU-70': 20.073864199118287, 'IoU-71': 18.810281587455492, 'IoU-72': 19.14520436028434, 'IoU-73': 19.572268051028612, 'IoU-74': 19.063785079514023, 'IoU-75': 19.3135708907513, 'IoU-76': 19.330456613809073, 'IoU-77': 17.653229025955305, 'IoU-78': 18.157305311596264, 'IoU-79': 17.82304273305988, 'IoU-80': 17.030940497757978, 'IoU-81': 16.623620115568894, 'IoU-82': 16.85690567845855, 'IoU-83': 16.965135467815486, 'IoU-84': 16.7957291766683, 'IoU-85': 16.614115300667176, 'IoU-86': 16.50068419702809, 'IoU-87': 16.327148051929253, 'IoU-88': 16.655922254468106, 'IoU-89': 16.3302839428976, 'IoU-90': 16.60688864863503, 'IoU-91': 15.867371249652443, 'IoU-92': 15.496998643045815, 'IoU-93': 16.4740509577994, 'IoU-94': 15.916581742277863, 'IoU-95': 16.168014881543748, 'IoU-96': 14.969959599123364, 'IoU-97': 15.420736119389513, 'IoU-98': 14.622452198620941, 'IoU-99': 14.992788839286023, 'IoU-100': 12.99486888330492, 'IoU-101': 12.906514208385023, 'IoU-102': 12.229813712117998, 'IoU-103': 12.257352696573978, 'IoU-104': 12.527856788240507, 'IoU-105': 12.61475414631556, 'IoU-106': 12.812720439163028, 'IoU-107': 14.499195455041008, 'IoU-108': 12.31358139647204, 'IoU-109': 11.40652596455698, 'IoU-110': 13.318988832886514, 'IoU-111': 12.126043224304631, 'IoU-112': 11.439233396850913, 'IoU-113': 9.931790246198984, 'IoU-114': 13.17808328821549, 'IoU-115': 11.25179182406148, 'IoU-116': 10.95298147649778, 'IoU-117': 11.621351003305529, 'IoU-118': 9.079378177030424, 'IoU-119': 9.75459820140603, 'IoU-120': 10.591447170389527, 'IoU-121': 8.953950374007286, 'IoU-122': 9.280562872161207, 'IoU-123': 8.568380796048558, 'IoU-124': 9.155429599835449, 'IoU-125': 6.5408318085487, 'IoU-126': 9.171163063181623, 'IoU-127': 6.848680034063637, 'IoU-128': 7.251753854852659, 'IoU-129': 7.621643438737034, 'IoU-130': 6.848234958563561, 'IoU-131': 7.761466854483166, 'IoU-132': 7.43647027794866, 'IoU-133': 7.3794162522345825, 'IoU-134': 7.726414835851159, 'IoU-135': 6.290258021193688, 'IoU-136': 6.409575122751486, 'IoU-137': 5.139328301320409, 'IoU-138': 7.352442074383091, 'IoU-139': 4.792040451735337, 'IoU-140': 7.354524368718181, 'IoU-141': 4.885768145670813, 'IoU-142': 6.610658283538282, 'IoU-143': 4.39407487349142, 'IoU-144': 5.82511235555696, 'IoU-145': 4.35964568186172, 'IoU-146': 4.770647323308422, 'IoU-147': 5.08137106627269, 'IoU-148': 4.678648019520595, 'IoU-149': 4.468112033175994, 'IoU-150': 3.321518285568165, 'IoU-151': 4.717186902380806, 'IoU-152': 3.8558741671175323, 'IoU-153': 3.218225399542421, 'IoU-154': 2.7761526914479546, 'IoU-155': 2.953323059021588, 'IoU-156': 2.754942832245468, 'IoU-157': 3.7401340604634625, 'IoU-158': 2.227210406119022, 'IoU-159': 2.297180764943932, 'IoU-160': 2.296588662263735, 'IoU-161': 3.947521017200597, 'IoU-162': 2.435956972220742, 'IoU-163': 2.516690332020746, 'IoU-164': 2.7000711296585687, 'IoU-165': 1.7915921657161407, 'IoU-166': 2.225325825535596, 'IoU-167': 1.72023534906504, 'IoU-168': 2.196585206478475, 'IoU-169': 1.2438940108655425, 'IoU-170': 1.5913240299719515, 'IoU-171': 2.0507223302272575, 'IoU-172': 0.6520934005855173, 'IoU-173': 2.5497408826650063, 'IoU-174': 2.5815343389155188, 'IoU-175': 1.7554129456876875, 'IoU-176': 1.8783644485167794, 'IoU-177': 1.332358215690317, 'IoU-178': 1.1048842264813226, 'IoU-179': 0.5815113839827355, 'IoU-180': 2.1919627636004213, 'IoU-181': 2.2269679919144725, 'IoU-182': 0.5901845141778786, 'IoU-183': 2.376486280258834, 'IoU-184': 1.081178435374892, 'IoU-185': 0.9724703139474582, 'IoU-186': 2.8841623075684284, 'IoU-187': 2.577531171510289, 'IoU-188': 1.5772849047942061, 'IoU-189': 1.9189756123798098, 'IoU-190': 2.2059740953018716, 'IoU-191': 2.0768257076400394, 'mACC': 29.287193270336964, 'pACC': 55.53680519943547, 'ACC-0': nan, 'ACC-1': 98.76943896993966, 'ACC-2': 58.58460689647011, 'ACC-3': 73.3083227999473, 'ACC-4': 70.31513791055023, 'ACC-5': 63.03076022302779, 'ACC-6': 57.873333367825786, 'ACC-7': 49.13072331550295, 'ACC-8': 29.07795866381957, 'ACC-9': 46.69402649004168, 'ACC-10': 54.03271195253051, 'ACC-11': 64.9076088695983, 'ACC-12': 69.0609444722187, 'ACC-13': 64.87359284000776, 'ACC-14': 66.45279692694186, 'ACC-15': 61.825902998785544, 'ACC-16': 61.654186789510256, 'ACC-17': 63.92691126982527, 'ACC-18': 62.539067635851566, 'ACC-19': 63.339981533315346, 'ACC-20': 59.80097848621123, 'ACC-21': 60.06962066310536, 'ACC-22': 62.00431729852428, 'ACC-23': 59.695667376511764, 'ACC-24': 62.70317113712416, 'ACC-25': 62.508223309842016, 'ACC-26': 59.3532754587482, 'ACC-27': 60.898059545831714, 'ACC-28': 61.3251227028174, 'ACC-29': 62.21777888010036, 'ACC-30': 59.2234675817114, 'ACC-31': 59.85751883657714, 'ACC-32': 60.61987943624744, 'ACC-33': 58.17974430414191, 'ACC-34': 59.312248718423646, 'ACC-35': 58.260035759503246, 'ACC-36': 58.125101526809765, 'ACC-37': 56.211607502603634, 'ACC-38': 56.268059521527746, 'ACC-39': 57.18496936796983, 'ACC-40': 58.06311791578969, 'ACC-41': 54.61769237939889, 'ACC-42': 53.075620221864874, 'ACC-43': 53.069487805905446, 'ACC-44': 51.732951237378266, 'ACC-45': 51.980232769297885, 'ACC-46': 51.92741167199244, 'ACC-47': 48.8335732224375, 'ACC-48': 51.23780987717371, 'ACC-49': 49.89627990156523, 'ACC-50': 52.82674805531975, 'ACC-51': 49.80186215087939, 'ACC-52': 47.95148912863972, 'ACC-53': 47.95163827308932, 'ACC-54': 49.22809215750595, 'ACC-55': 47.12784431127094, 'ACC-56': 47.701153882782314, 'ACC-57': 46.04271272947512, 'ACC-58': 42.573498995779474, 'ACC-59': 40.475611156188634, 'ACC-60': 39.987707869773516, 'ACC-61': 41.46294312791145, 'ACC-62': 40.17003230932835, 'ACC-63': 38.00399938317787, 'ACC-64': 35.529490993294196, 'ACC-65': 36.32842982817536, 'ACC-66': 34.633310043903776, 'ACC-67': 33.61436497922126, 'ACC-68': 33.58575646196701, 'ACC-69': 31.183686456543608, 'ACC-70': 32.38882659460769, 'ACC-71': 31.75589487222884, 'ACC-72': 32.025910800242684, 'ACC-73': 30.89857149107939, 'ACC-74': 31.69952094090305, 'ACC-75': 32.10319752680622, 'ACC-76': 31.53060355619156, 'ACC-77': 30.29485372152585, 'ACC-78': 30.734899472533332, 'ACC-79': 29.709740321289512, 'ACC-80': 28.41443506198172, 'ACC-81': 28.759181267499674, 'ACC-82': 28.48348634526323, 'ACC-83': 28.782729678837548, 'ACC-84': 26.823279040191956, 'ACC-85': 27.274249597157397, 'ACC-86': 28.419678320774437, 'ACC-87': 26.49599095223693, 'ACC-88': 30.34554624011776, 'ACC-89': 27.670060337284536, 'ACC-90': 26.10476186009867, 'ACC-91': 29.15951470729515, 'ACC-92': 25.072525037330788, 'ACC-93': 26.556933348124335, 'ACC-94': 26.970193479641214, 'ACC-95': 28.739458367042424, 'ACC-96': 27.53931850969605, 'ACC-97': 24.924612584960116, 'ACC-98': 23.731680338619356, 'ACC-99': 29.38265143585309, 'ACC-100': 20.268345947401244, 'ACC-101': 22.40139209142134, 'ACC-102': 19.91737652922519, 'ACC-103': 23.49241994899405, 'ACC-104': 24.1847716485084, 'ACC-105': 21.47621113861114, 'ACC-106': 22.995450155590465, 'ACC-107': 26.15060223925086, 'ACC-108': 21.70144651253155, 'ACC-109': 17.851231000605257, 'ACC-110': 24.028346645413105, 'ACC-111': 20.742797992153264, 'ACC-112': 20.438740666742742, 'ACC-113': 15.640677531162167, 'ACC-114': 27.240736779231174, 'ACC-115': 20.57556417542435, 'ACC-116': 19.562924172799175, 'ACC-117': 23.374603299614975, 'ACC-118': 15.412078026717923, 'ACC-119': 16.96679057634513, 'ACC-120': 18.809540124465435, 'ACC-121': 18.098206462765944, 'ACC-122': 17.082283676299923, 'ACC-123': 13.681515940387118, 'ACC-124': 17.523617535763925, 'ACC-125': 11.155441321871775, 'ACC-126': 19.675497955354786, 'ACC-127': 12.917196217140614, 'ACC-128': 12.920924854645314, 'ACC-129': 14.966311256752965, 'ACC-130': 14.46279179554384, 'ACC-131': 14.442156078933325, 'ACC-132': 14.494140670815279, 'ACC-133': 13.18996781590425, 'ACC-134': 16.523222748815165, 'ACC-135': 10.907659198449347, 'ACC-136': 12.889605851658592, 'ACC-137': 9.930950369289974, 'ACC-138': 16.015337462243316, 'ACC-139': 8.19387203802737, 'ACC-140': 14.61485836203198, 'ACC-141': 8.362437072806195, 'ACC-142': 12.844119221596054, 'ACC-143': 8.385653810521246, 'ACC-144': 10.649819494584838, 'ACC-145': 7.694447550258279, 'ACC-146': 9.059148751456442, 'ACC-147': 8.831708578501239, 'ACC-148': 9.028934909049086, 'ACC-149': 9.881965663660454, 'ACC-150': 6.44448822139463, 'ACC-151': 8.136457763809, 'ACC-152': 6.668310405286938, 'ACC-153': 5.270456585866358, 'ACC-154': 5.878236446874189, 'ACC-155': 4.70501376758541, 'ACC-156': 4.467154039571578, 'ACC-157': 7.6588366113948885, 'ACC-158': 4.587531455039934, 'ACC-159': 4.131567083561345, 'ACC-160': 3.4749242184318874, 'ACC-161': 7.121633803924432, 'ACC-162': 4.760176256541932, 'ACC-163': 5.68707990467266, 'ACC-164': 4.796755443795801, 'ACC-165': 2.5509247936168116, 'ACC-166': 4.715984924159119, 'ACC-167': 2.6232179367585777, 'ACC-168': 4.338209438075815, 'ACC-169': 2.1349621771104403, 'ACC-170': 3.4323225009583034, 'ACC-171': 3.596193303196678, 'ACC-172': 0.8217579731081232, 'ACC-173': 6.564930593445793, 'ACC-174': 4.514418963579326, 'ACC-175': 2.6889563100064895, 'ACC-176': 2.766571859763176, 'ACC-177': 1.8962147677759753, 'ACC-178': 1.498133128328353, 'ACC-179': 0.8216993554225057, 'ACC-180': 3.9052710894375022, 'ACC-181': 4.89553058996023, 'ACC-182': 0.6861112229885167, 'ACC-183': 4.656139714797611, 'ACC-184': 1.8687929965072156, 'ACC-185': 1.5201696160082037, 'ACC-186': 6.848939952267753, 'ACC-187': 4.718346440522354, 'ACC-188': 2.902929663855373, 'ACC-189': 7.877110605076491, 'ACC-190': 7.319188404581993, 'ACC-191': 9.622707993474714})])
[01/18 05:55:56] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 05:55:56] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 05:55:56] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 05:55:56] d2.evaluation.testing INFO: copypaste: 2.4429,19.2658,41.2113,29.2872,55.5368
[01/18 05:55:57] d2.utils.events INFO:  eta: 6:26:04  iter: 23999  total_loss: 33.95  loss_ce: 0.2423  loss_mask: 0.3548  loss_dice: 2.748  loss_ce_0: 0.5271  loss_mask_0: 0.3524  loss_dice_0: 2.877  loss_ce_1: 0.2829  loss_mask_1: 0.3636  loss_dice_1: 2.778  loss_ce_2: 0.2842  loss_mask_2: 0.3585  loss_dice_2: 2.754  loss_ce_3: 0.2656  loss_mask_3: 0.3579  loss_dice_3: 2.75  loss_ce_4: 0.2634  loss_mask_4: 0.3552  loss_dice_4: 2.748  loss_ce_5: 0.2562  loss_mask_5: 0.3543  loss_dice_5: 2.745  loss_ce_6: 0.2453  loss_mask_6: 0.3542  loss_dice_6: 2.745  loss_ce_7: 0.2371  loss_mask_7: 0.3547  loss_dice_7: 2.733  loss_ce_8: 0.2449  loss_mask_8: 0.3558  loss_dice_8: 2.741  time: 1.4889  data_time: 0.0744  lr: 4.3841e-06  max_mem: 21589M
[01/18 05:56:25] d2.utils.events INFO:  eta: 6:25:35  iter: 24019  total_loss: 34.64  loss_ce: 0.2685  loss_mask: 0.3518  loss_dice: 2.765  loss_ce_0: 0.5528  loss_mask_0: 0.3521  loss_dice_0: 2.907  loss_ce_1: 0.306  loss_mask_1: 0.3559  loss_dice_1: 2.814  loss_ce_2: 0.2985  loss_mask_2: 0.3526  loss_dice_2: 2.794  loss_ce_3: 0.2692  loss_mask_3: 0.3524  loss_dice_3: 2.779  loss_ce_4: 0.2703  loss_mask_4: 0.353  loss_dice_4: 2.774  loss_ce_5: 0.2774  loss_mask_5: 0.3509  loss_dice_5: 2.765  loss_ce_6: 0.2642  loss_mask_6: 0.351  loss_dice_6: 2.763  loss_ce_7: 0.2629  loss_mask_7: 0.352  loss_dice_7: 2.767  loss_ce_8: 0.2677  loss_mask_8: 0.3519  loss_dice_8: 2.774  time: 1.4889  data_time: 0.0694  lr: 4.3791e-06  max_mem: 21589M
[01/18 05:56:55] d2.utils.events INFO:  eta: 6:25:04  iter: 24039  total_loss: 34.56  loss_ce: 0.2514  loss_mask: 0.3674  loss_dice: 2.774  loss_ce_0: 0.5302  loss_mask_0: 0.3597  loss_dice_0: 2.899  loss_ce_1: 0.2956  loss_mask_1: 0.3673  loss_dice_1: 2.815  loss_ce_2: 0.2954  loss_mask_2: 0.3672  loss_dice_2: 2.794  loss_ce_3: 0.272  loss_mask_3: 0.365  loss_dice_3: 2.783  loss_ce_4: 0.2657  loss_mask_4: 0.3674  loss_dice_4: 2.774  loss_ce_5: 0.2499  loss_mask_5: 0.3664  loss_dice_5: 2.779  loss_ce_6: 0.2418  loss_mask_6: 0.3679  loss_dice_6: 2.771  loss_ce_7: 0.255  loss_mask_7: 0.3663  loss_dice_7: 2.776  loss_ce_8: 0.2494  loss_mask_8: 0.3665  loss_dice_8: 2.776  time: 1.4888  data_time: 0.0707  lr: 4.3742e-06  max_mem: 21589M
[01/18 05:57:24] d2.utils.events INFO:  eta: 6:24:51  iter: 24059  total_loss: 34.66  loss_ce: 0.2673  loss_mask: 0.3595  loss_dice: 2.759  loss_ce_0: 0.5484  loss_mask_0: 0.3543  loss_dice_0: 2.899  loss_ce_1: 0.2894  loss_mask_1: 0.3633  loss_dice_1: 2.8  loss_ce_2: 0.3156  loss_mask_2: 0.3627  loss_dice_2: 2.785  loss_ce_3: 0.2807  loss_mask_3: 0.3614  loss_dice_3: 2.766  loss_ce_4: 0.265  loss_mask_4: 0.36  loss_dice_4: 2.764  loss_ce_5: 0.2586  loss_mask_5: 0.3588  loss_dice_5: 2.769  loss_ce_6: 0.2587  loss_mask_6: 0.3598  loss_dice_6: 2.761  loss_ce_7: 0.266  loss_mask_7: 0.3592  loss_dice_7: 2.761  loss_ce_8: 0.269  loss_mask_8: 0.3602  loss_dice_8: 2.76  time: 1.4888  data_time: 0.0731  lr: 4.3693e-06  max_mem: 21589M
[01/18 05:57:53] d2.utils.events INFO:  eta: 6:24:25  iter: 24079  total_loss: 34.32  loss_ce: 0.2624  loss_mask: 0.3635  loss_dice: 2.75  loss_ce_0: 0.5522  loss_mask_0: 0.3535  loss_dice_0: 2.885  loss_ce_1: 0.3045  loss_mask_1: 0.3682  loss_dice_1: 2.79  loss_ce_2: 0.2912  loss_mask_2: 0.3645  loss_dice_2: 2.774  loss_ce_3: 0.2819  loss_mask_3: 0.3619  loss_dice_3: 2.751  loss_ce_4: 0.2722  loss_mask_4: 0.3602  loss_dice_4: 2.755  loss_ce_5: 0.2697  loss_mask_5: 0.3606  loss_dice_5: 2.756  loss_ce_6: 0.2633  loss_mask_6: 0.3618  loss_dice_6: 2.746  loss_ce_7: 0.2585  loss_mask_7: 0.3627  loss_dice_7: 2.745  loss_ce_8: 0.2548  loss_mask_8: 0.3625  loss_dice_8: 2.748  time: 1.4888  data_time: 0.0716  lr: 4.3643e-06  max_mem: 21589M
[01/18 05:58:22] d2.utils.events INFO:  eta: 6:23:56  iter: 24099  total_loss: 34.09  loss_ce: 0.2687  loss_mask: 0.3702  loss_dice: 2.701  loss_ce_0: 0.5661  loss_mask_0: 0.3661  loss_dice_0: 2.815  loss_ce_1: 0.3094  loss_mask_1: 0.3757  loss_dice_1: 2.735  loss_ce_2: 0.3092  loss_mask_2: 0.3708  loss_dice_2: 2.707  loss_ce_3: 0.2732  loss_mask_3: 0.3696  loss_dice_3: 2.694  loss_ce_4: 0.2682  loss_mask_4: 0.3696  loss_dice_4: 2.702  loss_ce_5: 0.2638  loss_mask_5: 0.3699  loss_dice_5: 2.71  loss_ce_6: 0.2713  loss_mask_6: 0.3699  loss_dice_6: 2.695  loss_ce_7: 0.2595  loss_mask_7: 0.3704  loss_dice_7: 2.7  loss_ce_8: 0.2715  loss_mask_8: 0.3687  loss_dice_8: 2.694  time: 1.4887  data_time: 0.0660  lr: 4.3594e-06  max_mem: 21589M
[01/18 05:58:51] d2.utils.events INFO:  eta: 6:23:35  iter: 24119  total_loss: 35.19  loss_ce: 0.2777  loss_mask: 0.3632  loss_dice: 2.831  loss_ce_0: 0.5367  loss_mask_0: 0.3597  loss_dice_0: 2.956  loss_ce_1: 0.2991  loss_mask_1: 0.3688  loss_dice_1: 2.868  loss_ce_2: 0.2962  loss_mask_2: 0.3642  loss_dice_2: 2.851  loss_ce_3: 0.2853  loss_mask_3: 0.3645  loss_dice_3: 2.837  loss_ce_4: 0.2756  loss_mask_4: 0.363  loss_dice_4: 2.84  loss_ce_5: 0.2659  loss_mask_5: 0.3628  loss_dice_5: 2.838  loss_ce_6: 0.2707  loss_mask_6: 0.3639  loss_dice_6: 2.824  loss_ce_7: 0.2772  loss_mask_7: 0.3637  loss_dice_7: 2.833  loss_ce_8: 0.2717  loss_mask_8: 0.3641  loss_dice_8: 2.83  time: 1.4887  data_time: 0.0749  lr: 4.3545e-06  max_mem: 21589M
[01/18 05:59:20] d2.utils.events INFO:  eta: 6:23:00  iter: 24139  total_loss: 34.52  loss_ce: 0.2961  loss_mask: 0.3594  loss_dice: 2.708  loss_ce_0: 0.5691  loss_mask_0: 0.3534  loss_dice_0: 2.828  loss_ce_1: 0.3564  loss_mask_1: 0.3607  loss_dice_1: 2.742  loss_ce_2: 0.3337  loss_mask_2: 0.3584  loss_dice_2: 2.726  loss_ce_3: 0.3143  loss_mask_3: 0.3602  loss_dice_3: 2.708  loss_ce_4: 0.3008  loss_mask_4: 0.3612  loss_dice_4: 2.71  loss_ce_5: 0.3048  loss_mask_5: 0.3599  loss_dice_5: 2.705  loss_ce_6: 0.2937  loss_mask_6: 0.3609  loss_dice_6: 2.707  loss_ce_7: 0.2991  loss_mask_7: 0.3601  loss_dice_7: 2.707  loss_ce_8: 0.3017  loss_mask_8: 0.3588  loss_dice_8: 2.701  time: 1.4887  data_time: 0.0679  lr: 4.3495e-06  max_mem: 21589M
[01/18 05:59:49] d2.utils.events INFO:  eta: 6:22:35  iter: 24159  total_loss: 34.4  loss_ce: 0.2454  loss_mask: 0.3583  loss_dice: 2.755  loss_ce_0: 0.5646  loss_mask_0: 0.358  loss_dice_0: 2.878  loss_ce_1: 0.3054  loss_mask_1: 0.3674  loss_dice_1: 2.779  loss_ce_2: 0.295  loss_mask_2: 0.3634  loss_dice_2: 2.765  loss_ce_3: 0.2621  loss_mask_3: 0.3597  loss_dice_3: 2.756  loss_ce_4: 0.2671  loss_mask_4: 0.3597  loss_dice_4: 2.745  loss_ce_5: 0.2593  loss_mask_5: 0.3599  loss_dice_5: 2.753  loss_ce_6: 0.2458  loss_mask_6: 0.3574  loss_dice_6: 2.75  loss_ce_7: 0.2535  loss_mask_7: 0.3582  loss_dice_7: 2.754  loss_ce_8: 0.2525  loss_mask_8: 0.3571  loss_dice_8: 2.75  time: 1.4887  data_time: 0.0722  lr: 4.3446e-06  max_mem: 21589M
[01/18 06:00:18] d2.utils.events INFO:  eta: 6:22:04  iter: 24179  total_loss: 34.81  loss_ce: 0.2741  loss_mask: 0.3605  loss_dice: 2.762  loss_ce_0: 0.5915  loss_mask_0: 0.3566  loss_dice_0: 2.903  loss_ce_1: 0.3385  loss_mask_1: 0.3623  loss_dice_1: 2.803  loss_ce_2: 0.3237  loss_mask_2: 0.3602  loss_dice_2: 2.777  loss_ce_3: 0.2835  loss_mask_3: 0.358  loss_dice_3: 2.773  loss_ce_4: 0.3001  loss_mask_4: 0.3596  loss_dice_4: 2.76  loss_ce_5: 0.278  loss_mask_5: 0.3595  loss_dice_5: 2.77  loss_ce_6: 0.2975  loss_mask_6: 0.359  loss_dice_6: 2.759  loss_ce_7: 0.2773  loss_mask_7: 0.36  loss_dice_7: 2.763  loss_ce_8: 0.2853  loss_mask_8: 0.3608  loss_dice_8: 2.766  time: 1.4886  data_time: 0.0673  lr: 4.3397e-06  max_mem: 21589M
[01/18 06:00:48] d2.utils.events INFO:  eta: 6:21:39  iter: 24199  total_loss: 35.76  loss_ce: 0.2666  loss_mask: 0.3604  loss_dice: 2.837  loss_ce_0: 0.5664  loss_mask_0: 0.3576  loss_dice_0: 2.973  loss_ce_1: 0.3145  loss_mask_1: 0.3644  loss_dice_1: 2.873  loss_ce_2: 0.2982  loss_mask_2: 0.3619  loss_dice_2: 2.85  loss_ce_3: 0.2863  loss_mask_3: 0.3618  loss_dice_3: 2.838  loss_ce_4: 0.2787  loss_mask_4: 0.3607  loss_dice_4: 2.841  loss_ce_5: 0.2664  loss_mask_5: 0.36  loss_dice_5: 2.831  loss_ce_6: 0.2746  loss_mask_6: 0.362  loss_dice_6: 2.838  loss_ce_7: 0.2652  loss_mask_7: 0.3604  loss_dice_7: 2.842  loss_ce_8: 0.2652  loss_mask_8: 0.3613  loss_dice_8: 2.825  time: 1.4886  data_time: 0.0711  lr: 4.3347e-06  max_mem: 21589M
[01/18 06:01:17] d2.utils.events INFO:  eta: 6:21:16  iter: 24219  total_loss: 35.6  loss_ce: 0.284  loss_mask: 0.3615  loss_dice: 2.867  loss_ce_0: 0.5738  loss_mask_0: 0.3614  loss_dice_0: 2.992  loss_ce_1: 0.3243  loss_mask_1: 0.3679  loss_dice_1: 2.923  loss_ce_2: 0.3353  loss_mask_2: 0.3637  loss_dice_2: 2.901  loss_ce_3: 0.2941  loss_mask_3: 0.3625  loss_dice_3: 2.883  loss_ce_4: 0.2935  loss_mask_4: 0.3612  loss_dice_4: 2.881  loss_ce_5: 0.2787  loss_mask_5: 0.3615  loss_dice_5: 2.882  loss_ce_6: 0.2748  loss_mask_6: 0.3611  loss_dice_6: 2.88  loss_ce_7: 0.2872  loss_mask_7: 0.3607  loss_dice_7: 2.889  loss_ce_8: 0.2856  loss_mask_8: 0.3607  loss_dice_8: 2.875  time: 1.4886  data_time: 0.0763  lr: 4.3298e-06  max_mem: 21589M
[01/18 06:01:46] d2.utils.events INFO:  eta: 6:20:43  iter: 24239  total_loss: 34.53  loss_ce: 0.2666  loss_mask: 0.3541  loss_dice: 2.745  loss_ce_0: 0.5809  loss_mask_0: 0.3543  loss_dice_0: 2.889  loss_ce_1: 0.3035  loss_mask_1: 0.3624  loss_dice_1: 2.794  loss_ce_2: 0.299  loss_mask_2: 0.3586  loss_dice_2: 2.776  loss_ce_3: 0.2894  loss_mask_3: 0.357  loss_dice_3: 2.76  loss_ce_4: 0.2814  loss_mask_4: 0.3555  loss_dice_4: 2.744  loss_ce_5: 0.265  loss_mask_5: 0.3554  loss_dice_5: 2.752  loss_ce_6: 0.2648  loss_mask_6: 0.3563  loss_dice_6: 2.744  loss_ce_7: 0.2635  loss_mask_7: 0.3552  loss_dice_7: 2.748  loss_ce_8: 0.2548  loss_mask_8: 0.3549  loss_dice_8: 2.749  time: 1.4886  data_time: 0.0656  lr: 4.3249e-06  max_mem: 21589M
[01/18 06:02:16] d2.utils.events INFO:  eta: 6:20:25  iter: 24259  total_loss: 35.64  loss_ce: 0.2648  loss_mask: 0.3624  loss_dice: 2.848  loss_ce_0: 0.5584  loss_mask_0: 0.3632  loss_dice_0: 2.967  loss_ce_1: 0.3009  loss_mask_1: 0.3728  loss_dice_1: 2.883  loss_ce_2: 0.294  loss_mask_2: 0.3678  loss_dice_2: 2.872  loss_ce_3: 0.2814  loss_mask_3: 0.365  loss_dice_3: 2.853  loss_ce_4: 0.2713  loss_mask_4: 0.3633  loss_dice_4: 2.854  loss_ce_5: 0.2676  loss_mask_5: 0.3621  loss_dice_5: 2.857  loss_ce_6: 0.2654  loss_mask_6: 0.3616  loss_dice_6: 2.857  loss_ce_7: 0.2596  loss_mask_7: 0.3618  loss_dice_7: 2.843  loss_ce_8: 0.2648  loss_mask_8: 0.3628  loss_dice_8: 2.851  time: 1.4886  data_time: 0.0734  lr: 4.3199e-06  max_mem: 21589M
[01/18 06:02:45] d2.utils.events INFO:  eta: 6:19:51  iter: 24279  total_loss: 34.38  loss_ce: 0.2459  loss_mask: 0.3611  loss_dice: 2.738  loss_ce_0: 0.5615  loss_mask_0: 0.365  loss_dice_0: 2.867  loss_ce_1: 0.3051  loss_mask_1: 0.3679  loss_dice_1: 2.777  loss_ce_2: 0.2935  loss_mask_2: 0.3634  loss_dice_2: 2.759  loss_ce_3: 0.2727  loss_mask_3: 0.3582  loss_dice_3: 2.746  loss_ce_4: 0.2665  loss_mask_4: 0.3613  loss_dice_4: 2.737  loss_ce_5: 0.2628  loss_mask_5: 0.3614  loss_dice_5: 2.746  loss_ce_6: 0.2598  loss_mask_6: 0.3602  loss_dice_6: 2.743  loss_ce_7: 0.2496  loss_mask_7: 0.3616  loss_dice_7: 2.739  loss_ce_8: 0.2495  loss_mask_8: 0.3605  loss_dice_8: 2.74  time: 1.4885  data_time: 0.0695  lr: 4.315e-06  max_mem: 21589M
[01/18 06:03:14] d2.utils.events INFO:  eta: 6:19:12  iter: 24299  total_loss: 34.19  loss_ce: 0.2481  loss_mask: 0.3661  loss_dice: 2.727  loss_ce_0: 0.5765  loss_mask_0: 0.3644  loss_dice_0: 2.837  loss_ce_1: 0.2959  loss_mask_1: 0.3747  loss_dice_1: 2.758  loss_ce_2: 0.2856  loss_mask_2: 0.3721  loss_dice_2: 2.736  loss_ce_3: 0.2887  loss_mask_3: 0.3666  loss_dice_3: 2.726  loss_ce_4: 0.2729  loss_mask_4: 0.3672  loss_dice_4: 2.726  loss_ce_5: 0.2676  loss_mask_5: 0.3679  loss_dice_5: 2.723  loss_ce_6: 0.2644  loss_mask_6: 0.3658  loss_dice_6: 2.73  loss_ce_7: 0.2509  loss_mask_7: 0.3662  loss_dice_7: 2.732  loss_ce_8: 0.2567  loss_mask_8: 0.366  loss_dice_8: 2.728  time: 1.4885  data_time: 0.0700  lr: 4.31e-06  max_mem: 21589M
[01/18 06:03:43] d2.utils.events INFO:  eta: 6:18:36  iter: 24319  total_loss: 34.69  loss_ce: 0.2717  loss_mask: 0.3626  loss_dice: 2.772  loss_ce_0: 0.5785  loss_mask_0: 0.3568  loss_dice_0: 2.914  loss_ce_1: 0.3169  loss_mask_1: 0.3673  loss_dice_1: 2.81  loss_ce_2: 0.3004  loss_mask_2: 0.366  loss_dice_2: 2.789  loss_ce_3: 0.2807  loss_mask_3: 0.3653  loss_dice_3: 2.778  loss_ce_4: 0.29  loss_mask_4: 0.3643  loss_dice_4: 2.784  loss_ce_5: 0.2797  loss_mask_5: 0.3629  loss_dice_5: 2.779  loss_ce_6: 0.2784  loss_mask_6: 0.3629  loss_dice_6: 2.781  loss_ce_7: 0.2646  loss_mask_7: 0.3647  loss_dice_7: 2.776  loss_ce_8: 0.269  loss_mask_8: 0.3628  loss_dice_8: 2.766  time: 1.4884  data_time: 0.0719  lr: 4.3051e-06  max_mem: 21589M
[01/18 06:04:12] d2.utils.events INFO:  eta: 6:18:07  iter: 24339  total_loss: 34.71  loss_ce: 0.2409  loss_mask: 0.3567  loss_dice: 2.757  loss_ce_0: 0.5278  loss_mask_0: 0.3576  loss_dice_0: 2.914  loss_ce_1: 0.2744  loss_mask_1: 0.364  loss_dice_1: 2.801  loss_ce_2: 0.274  loss_mask_2: 0.3613  loss_dice_2: 2.78  loss_ce_3: 0.2572  loss_mask_3: 0.3586  loss_dice_3: 2.774  loss_ce_4: 0.2533  loss_mask_4: 0.3589  loss_dice_4: 2.767  loss_ce_5: 0.2449  loss_mask_5: 0.3595  loss_dice_5: 2.774  loss_ce_6: 0.232  loss_mask_6: 0.3583  loss_dice_6: 2.765  loss_ce_7: 0.2347  loss_mask_7: 0.358  loss_dice_7: 2.763  loss_ce_8: 0.2475  loss_mask_8: 0.3583  loss_dice_8: 2.758  time: 1.4884  data_time: 0.0687  lr: 4.3001e-06  max_mem: 21589M
[01/18 06:04:42] d2.utils.events INFO:  eta: 6:17:38  iter: 24359  total_loss: 34.89  loss_ce: 0.2593  loss_mask: 0.3547  loss_dice: 2.759  loss_ce_0: 0.5599  loss_mask_0: 0.3502  loss_dice_0: 2.896  loss_ce_1: 0.3053  loss_mask_1: 0.3544  loss_dice_1: 2.808  loss_ce_2: 0.3129  loss_mask_2: 0.3546  loss_dice_2: 2.783  loss_ce_3: 0.2874  loss_mask_3: 0.3541  loss_dice_3: 2.767  loss_ce_4: 0.2839  loss_mask_4: 0.3548  loss_dice_4: 2.762  loss_ce_5: 0.2791  loss_mask_5: 0.3535  loss_dice_5: 2.779  loss_ce_6: 0.2824  loss_mask_6: 0.3537  loss_dice_6: 2.771  loss_ce_7: 0.2668  loss_mask_7: 0.3533  loss_dice_7: 2.766  loss_ce_8: 0.2774  loss_mask_8: 0.3543  loss_dice_8: 2.77  time: 1.4884  data_time: 0.0702  lr: 4.2952e-06  max_mem: 21589M
[01/18 06:05:11] d2.utils.events INFO:  eta: 6:17:09  iter: 24379  total_loss: 34.51  loss_ce: 0.2659  loss_mask: 0.3531  loss_dice: 2.769  loss_ce_0: 0.5368  loss_mask_0: 0.3505  loss_dice_0: 2.897  loss_ce_1: 0.2959  loss_mask_1: 0.355  loss_dice_1: 2.802  loss_ce_2: 0.2966  loss_mask_2: 0.3537  loss_dice_2: 2.795  loss_ce_3: 0.2825  loss_mask_3: 0.3535  loss_dice_3: 2.777  loss_ce_4: 0.2867  loss_mask_4: 0.353  loss_dice_4: 2.782  loss_ce_5: 0.2863  loss_mask_5: 0.3525  loss_dice_5: 2.789  loss_ce_6: 0.26  loss_mask_6: 0.3532  loss_dice_6: 2.774  loss_ce_7: 0.2594  loss_mask_7: 0.3533  loss_dice_7: 2.775  loss_ce_8: 0.2644  loss_mask_8: 0.353  loss_dice_8: 2.779  time: 1.4884  data_time: 0.0698  lr: 4.2903e-06  max_mem: 21589M
[01/18 06:05:40] d2.utils.events INFO:  eta: 6:16:40  iter: 24399  total_loss: 34.98  loss_ce: 0.2911  loss_mask: 0.3585  loss_dice: 2.755  loss_ce_0: 0.5659  loss_mask_0: 0.361  loss_dice_0: 2.882  loss_ce_1: 0.3375  loss_mask_1: 0.3637  loss_dice_1: 2.798  loss_ce_2: 0.3229  loss_mask_2: 0.3613  loss_dice_2: 2.772  loss_ce_3: 0.306  loss_mask_3: 0.3609  loss_dice_3: 2.763  loss_ce_4: 0.291  loss_mask_4: 0.361  loss_dice_4: 2.756  loss_ce_5: 0.2929  loss_mask_5: 0.3618  loss_dice_5: 2.758  loss_ce_6: 0.2812  loss_mask_6: 0.3601  loss_dice_6: 2.761  loss_ce_7: 0.2778  loss_mask_7: 0.3606  loss_dice_7: 2.76  loss_ce_8: 0.3029  loss_mask_8: 0.3593  loss_dice_8: 2.747  time: 1.4884  data_time: 0.0681  lr: 4.2853e-06  max_mem: 21589M
[01/18 06:06:09] d2.utils.events INFO:  eta: 6:16:13  iter: 24419  total_loss: 34.41  loss_ce: 0.2513  loss_mask: 0.3606  loss_dice: 2.784  loss_ce_0: 0.5395  loss_mask_0: 0.3602  loss_dice_0: 2.924  loss_ce_1: 0.298  loss_mask_1: 0.3647  loss_dice_1: 2.834  loss_ce_2: 0.285  loss_mask_2: 0.3648  loss_dice_2: 2.802  loss_ce_3: 0.2757  loss_mask_3: 0.3616  loss_dice_3: 2.791  loss_ce_4: 0.2506  loss_mask_4: 0.362  loss_dice_4: 2.786  loss_ce_5: 0.2616  loss_mask_5: 0.3621  loss_dice_5: 2.788  loss_ce_6: 0.2496  loss_mask_6: 0.3617  loss_dice_6: 2.786  loss_ce_7: 0.2601  loss_mask_7: 0.3608  loss_dice_7: 2.787  loss_ce_8: 0.2521  loss_mask_8: 0.3619  loss_dice_8: 2.784  time: 1.4883  data_time: 0.0704  lr: 4.2804e-06  max_mem: 21589M
[01/18 06:06:38] d2.utils.events INFO:  eta: 6:15:44  iter: 24439  total_loss: 34.19  loss_ce: 0.279  loss_mask: 0.3697  loss_dice: 2.698  loss_ce_0: 0.5884  loss_mask_0: 0.361  loss_dice_0: 2.842  loss_ce_1: 0.3259  loss_mask_1: 0.3716  loss_dice_1: 2.75  loss_ce_2: 0.3163  loss_mask_2: 0.3711  loss_dice_2: 2.725  loss_ce_3: 0.282  loss_mask_3: 0.3694  loss_dice_3: 2.715  loss_ce_4: 0.2931  loss_mask_4: 0.3682  loss_dice_4: 2.711  loss_ce_5: 0.2887  loss_mask_5: 0.3694  loss_dice_5: 2.71  loss_ce_6: 0.2754  loss_mask_6: 0.369  loss_dice_6: 2.702  loss_ce_7: 0.2755  loss_mask_7: 0.3701  loss_dice_7: 2.708  loss_ce_8: 0.2692  loss_mask_8: 0.3698  loss_dice_8: 2.707  time: 1.4883  data_time: 0.0705  lr: 4.2754e-06  max_mem: 21589M
[01/18 06:07:07] d2.utils.events INFO:  eta: 6:15:19  iter: 24459  total_loss: 34.14  loss_ce: 0.264  loss_mask: 0.3566  loss_dice: 2.736  loss_ce_0: 0.554  loss_mask_0: 0.3538  loss_dice_0: 2.868  loss_ce_1: 0.306  loss_mask_1: 0.3623  loss_dice_1: 2.782  loss_ce_2: 0.29  loss_mask_2: 0.3611  loss_dice_2: 2.769  loss_ce_3: 0.2771  loss_mask_3: 0.3592  loss_dice_3: 2.742  loss_ce_4: 0.2704  loss_mask_4: 0.3566  loss_dice_4: 2.748  loss_ce_5: 0.2719  loss_mask_5: 0.3569  loss_dice_5: 2.74  loss_ce_6: 0.2679  loss_mask_6: 0.3569  loss_dice_6: 2.738  loss_ce_7: 0.2623  loss_mask_7: 0.3562  loss_dice_7: 2.735  loss_ce_8: 0.2566  loss_mask_8: 0.3562  loss_dice_8: 2.734  time: 1.4882  data_time: 0.0718  lr: 4.2705e-06  max_mem: 21589M
[01/18 06:07:36] d2.utils.events INFO:  eta: 6:14:51  iter: 24479  total_loss: 34.6  loss_ce: 0.2641  loss_mask: 0.3566  loss_dice: 2.773  loss_ce_0: 0.5679  loss_mask_0: 0.3658  loss_dice_0: 2.92  loss_ce_1: 0.3038  loss_mask_1: 0.3649  loss_dice_1: 2.826  loss_ce_2: 0.285  loss_mask_2: 0.3635  loss_dice_2: 2.801  loss_ce_3: 0.2726  loss_mask_3: 0.3604  loss_dice_3: 2.782  loss_ce_4: 0.26  loss_mask_4: 0.3595  loss_dice_4: 2.782  loss_ce_5: 0.265  loss_mask_5: 0.3586  loss_dice_5: 2.788  loss_ce_6: 0.2547  loss_mask_6: 0.3593  loss_dice_6: 2.781  loss_ce_7: 0.256  loss_mask_7: 0.3588  loss_dice_7: 2.778  loss_ce_8: 0.2513  loss_mask_8: 0.3577  loss_dice_8: 2.778  time: 1.4882  data_time: 0.0740  lr: 4.2655e-06  max_mem: 21589M
[01/18 06:08:05] d2.utils.events INFO:  eta: 6:14:27  iter: 24499  total_loss: 34.42  loss_ce: 0.2428  loss_mask: 0.357  loss_dice: 2.78  loss_ce_0: 0.5509  loss_mask_0: 0.3533  loss_dice_0: 2.918  loss_ce_1: 0.3034  loss_mask_1: 0.3616  loss_dice_1: 2.834  loss_ce_2: 0.2794  loss_mask_2: 0.362  loss_dice_2: 2.799  loss_ce_3: 0.2692  loss_mask_3: 0.3586  loss_dice_3: 2.782  loss_ce_4: 0.2681  loss_mask_4: 0.3583  loss_dice_4: 2.78  loss_ce_5: 0.2635  loss_mask_5: 0.3568  loss_dice_5: 2.792  loss_ce_6: 0.244  loss_mask_6: 0.3573  loss_dice_6: 2.778  loss_ce_7: 0.2505  loss_mask_7: 0.3568  loss_dice_7: 2.778  loss_ce_8: 0.2583  loss_mask_8: 0.3578  loss_dice_8: 2.777  time: 1.4882  data_time: 0.0730  lr: 4.2606e-06  max_mem: 21589M
[01/18 06:08:34] d2.utils.events INFO:  eta: 6:14:08  iter: 24519  total_loss: 34.9  loss_ce: 0.2791  loss_mask: 0.3748  loss_dice: 2.763  loss_ce_0: 0.5841  loss_mask_0: 0.3761  loss_dice_0: 2.889  loss_ce_1: 0.3408  loss_mask_1: 0.3848  loss_dice_1: 2.808  loss_ce_2: 0.3238  loss_mask_2: 0.3821  loss_dice_2: 2.779  loss_ce_3: 0.2941  loss_mask_3: 0.3783  loss_dice_3: 2.772  loss_ce_4: 0.2901  loss_mask_4: 0.3769  loss_dice_4: 2.768  loss_ce_5: 0.2931  loss_mask_5: 0.3743  loss_dice_5: 2.768  loss_ce_6: 0.2842  loss_mask_6: 0.3739  loss_dice_6: 2.766  loss_ce_7: 0.2704  loss_mask_7: 0.3729  loss_dice_7: 2.765  loss_ce_8: 0.2705  loss_mask_8: 0.3761  loss_dice_8: 2.764  time: 1.4882  data_time: 0.0743  lr: 4.2556e-06  max_mem: 21589M
[01/18 06:09:03] d2.utils.events INFO:  eta: 6:13:39  iter: 24539  total_loss: 34.69  loss_ce: 0.2543  loss_mask: 0.357  loss_dice: 2.781  loss_ce_0: 0.5697  loss_mask_0: 0.361  loss_dice_0: 2.905  loss_ce_1: 0.3042  loss_mask_1: 0.3652  loss_dice_1: 2.816  loss_ce_2: 0.2928  loss_mask_2: 0.3608  loss_dice_2: 2.808  loss_ce_3: 0.2653  loss_mask_3: 0.3546  loss_dice_3: 2.8  loss_ce_4: 0.2604  loss_mask_4: 0.354  loss_dice_4: 2.79  loss_ce_5: 0.2615  loss_mask_5: 0.3545  loss_dice_5: 2.794  loss_ce_6: 0.2449  loss_mask_6: 0.3538  loss_dice_6: 2.783  loss_ce_7: 0.2504  loss_mask_7: 0.356  loss_dice_7: 2.785  loss_ce_8: 0.2565  loss_mask_8: 0.3567  loss_dice_8: 2.791  time: 1.4881  data_time: 0.0767  lr: 4.2507e-06  max_mem: 21589M
[01/18 06:09:32] d2.utils.events INFO:  eta: 6:13:07  iter: 24559  total_loss: 34.52  loss_ce: 0.2587  loss_mask: 0.3567  loss_dice: 2.766  loss_ce_0: 0.5719  loss_mask_0: 0.3544  loss_dice_0: 2.899  loss_ce_1: 0.3116  loss_mask_1: 0.3617  loss_dice_1: 2.821  loss_ce_2: 0.2929  loss_mask_2: 0.3602  loss_dice_2: 2.789  loss_ce_3: 0.2828  loss_mask_3: 0.3623  loss_dice_3: 2.779  loss_ce_4: 0.2797  loss_mask_4: 0.3599  loss_dice_4: 2.765  loss_ce_5: 0.2634  loss_mask_5: 0.36  loss_dice_5: 2.771  loss_ce_6: 0.2679  loss_mask_6: 0.3577  loss_dice_6: 2.77  loss_ce_7: 0.2687  loss_mask_7: 0.3558  loss_dice_7: 2.767  loss_ce_8: 0.2553  loss_mask_8: 0.3575  loss_dice_8: 2.763  time: 1.4881  data_time: 0.0725  lr: 4.2457e-06  max_mem: 21589M
[01/18 06:10:01] d2.utils.events INFO:  eta: 6:12:32  iter: 24579  total_loss: 34.77  loss_ce: 0.2678  loss_mask: 0.3598  loss_dice: 2.773  loss_ce_0: 0.5593  loss_mask_0: 0.3525  loss_dice_0: 2.89  loss_ce_1: 0.3122  loss_mask_1: 0.3651  loss_dice_1: 2.809  loss_ce_2: 0.2984  loss_mask_2: 0.3627  loss_dice_2: 2.788  loss_ce_3: 0.2916  loss_mask_3: 0.3611  loss_dice_3: 2.778  loss_ce_4: 0.2622  loss_mask_4: 0.3628  loss_dice_4: 2.77  loss_ce_5: 0.2592  loss_mask_5: 0.3614  loss_dice_5: 2.778  loss_ce_6: 0.2661  loss_mask_6: 0.3609  loss_dice_6: 2.766  loss_ce_7: 0.2721  loss_mask_7: 0.3608  loss_dice_7: 2.776  loss_ce_8: 0.2615  loss_mask_8: 0.3605  loss_dice_8: 2.778  time: 1.4881  data_time: 0.0672  lr: 4.2408e-06  max_mem: 21589M
[01/18 06:10:29] d2.utils.events INFO:  eta: 6:12:01  iter: 24599  total_loss: 35.18  loss_ce: 0.2774  loss_mask: 0.3676  loss_dice: 2.787  loss_ce_0: 0.5735  loss_mask_0: 0.3622  loss_dice_0: 2.916  loss_ce_1: 0.3196  loss_mask_1: 0.3742  loss_dice_1: 2.818  loss_ce_2: 0.3353  loss_mask_2: 0.3706  loss_dice_2: 2.796  loss_ce_3: 0.3031  loss_mask_3: 0.3673  loss_dice_3: 2.788  loss_ce_4: 0.2958  loss_mask_4: 0.3641  loss_dice_4: 2.787  loss_ce_5: 0.2904  loss_mask_5: 0.3659  loss_dice_5: 2.784  loss_ce_6: 0.2839  loss_mask_6: 0.366  loss_dice_6: 2.782  loss_ce_7: 0.2905  loss_mask_7: 0.3677  loss_dice_7: 2.781  loss_ce_8: 0.2812  loss_mask_8: 0.3663  loss_dice_8: 2.787  time: 1.4880  data_time: 0.0690  lr: 4.2358e-06  max_mem: 21589M
[01/18 06:10:58] d2.utils.events INFO:  eta: 6:11:27  iter: 24619  total_loss: 35.03  loss_ce: 0.2471  loss_mask: 0.3592  loss_dice: 2.799  loss_ce_0: 0.6061  loss_mask_0: 0.3582  loss_dice_0: 2.952  loss_ce_1: 0.316  loss_mask_1: 0.3671  loss_dice_1: 2.859  loss_ce_2: 0.2719  loss_mask_2: 0.3641  loss_dice_2: 2.819  loss_ce_3: 0.2669  loss_mask_3: 0.3587  loss_dice_3: 2.807  loss_ce_4: 0.2666  loss_mask_4: 0.3594  loss_dice_4: 2.793  loss_ce_5: 0.2594  loss_mask_5: 0.3601  loss_dice_5: 2.805  loss_ce_6: 0.2491  loss_mask_6: 0.3591  loss_dice_6: 2.8  loss_ce_7: 0.2452  loss_mask_7: 0.3605  loss_dice_7: 2.808  loss_ce_8: 0.2592  loss_mask_8: 0.3602  loss_dice_8: 2.81  time: 1.4880  data_time: 0.0674  lr: 4.2309e-06  max_mem: 21589M
[01/18 06:11:27] d2.utils.events INFO:  eta: 6:10:57  iter: 24639  total_loss: 34.11  loss_ce: 0.2695  loss_mask: 0.3609  loss_dice: 2.72  loss_ce_0: 0.5607  loss_mask_0: 0.3555  loss_dice_0: 2.85  loss_ce_1: 0.3215  loss_mask_1: 0.3653  loss_dice_1: 2.763  loss_ce_2: 0.3022  loss_mask_2: 0.3614  loss_dice_2: 2.741  loss_ce_3: 0.2825  loss_mask_3: 0.3616  loss_dice_3: 2.725  loss_ce_4: 0.2772  loss_mask_4: 0.3616  loss_dice_4: 2.729  loss_ce_5: 0.2741  loss_mask_5: 0.3592  loss_dice_5: 2.712  loss_ce_6: 0.2731  loss_mask_6: 0.3587  loss_dice_6: 2.715  loss_ce_7: 0.2758  loss_mask_7: 0.3612  loss_dice_7: 2.714  loss_ce_8: 0.2722  loss_mask_8: 0.3612  loss_dice_8: 2.719  time: 1.4879  data_time: 0.0688  lr: 4.2259e-06  max_mem: 21589M
[01/18 06:11:55] d2.utils.events INFO:  eta: 6:10:22  iter: 24659  total_loss: 35.17  loss_ce: 0.2767  loss_mask: 0.3645  loss_dice: 2.793  loss_ce_0: 0.5504  loss_mask_0: 0.3645  loss_dice_0: 2.923  loss_ce_1: 0.3336  loss_mask_1: 0.3672  loss_dice_1: 2.836  loss_ce_2: 0.3282  loss_mask_2: 0.3648  loss_dice_2: 2.805  loss_ce_3: 0.2819  loss_mask_3: 0.3609  loss_dice_3: 2.801  loss_ce_4: 0.29  loss_mask_4: 0.3609  loss_dice_4: 2.803  loss_ce_5: 0.2826  loss_mask_5: 0.3619  loss_dice_5: 2.801  loss_ce_6: 0.2858  loss_mask_6: 0.3619  loss_dice_6: 2.798  loss_ce_7: 0.2811  loss_mask_7: 0.3632  loss_dice_7: 2.797  loss_ce_8: 0.2835  loss_mask_8: 0.3624  loss_dice_8: 2.791  time: 1.4879  data_time: 0.0666  lr: 4.221e-06  max_mem: 21589M
[01/18 06:12:25] d2.utils.events INFO:  eta: 6:09:56  iter: 24679  total_loss: 35.07  loss_ce: 0.2798  loss_mask: 0.3635  loss_dice: 2.781  loss_ce_0: 0.5524  loss_mask_0: 0.3709  loss_dice_0: 2.915  loss_ce_1: 0.3158  loss_mask_1: 0.3719  loss_dice_1: 2.835  loss_ce_2: 0.2998  loss_mask_2: 0.37  loss_dice_2: 2.796  loss_ce_3: 0.2939  loss_mask_3: 0.3672  loss_dice_3: 2.793  loss_ce_4: 0.3021  loss_mask_4: 0.3658  loss_dice_4: 2.795  loss_ce_5: 0.2689  loss_mask_5: 0.3654  loss_dice_5: 2.791  loss_ce_6: 0.2737  loss_mask_6: 0.3648  loss_dice_6: 2.796  loss_ce_7: 0.2614  loss_mask_7: 0.3652  loss_dice_7: 2.793  loss_ce_8: 0.2633  loss_mask_8: 0.3661  loss_dice_8: 2.793  time: 1.4878  data_time: 0.0704  lr: 4.216e-06  max_mem: 21589M
[01/18 06:12:53] d2.utils.events INFO:  eta: 6:09:16  iter: 24699  total_loss: 34.75  loss_ce: 0.2649  loss_mask: 0.3579  loss_dice: 2.813  loss_ce_0: 0.5799  loss_mask_0: 0.3639  loss_dice_0: 2.923  loss_ce_1: 0.3157  loss_mask_1: 0.3622  loss_dice_1: 2.857  loss_ce_2: 0.2974  loss_mask_2: 0.3611  loss_dice_2: 2.832  loss_ce_3: 0.2734  loss_mask_3: 0.3595  loss_dice_3: 2.811  loss_ce_4: 0.2676  loss_mask_4: 0.3582  loss_dice_4: 2.81  loss_ce_5: 0.2642  loss_mask_5: 0.357  loss_dice_5: 2.813  loss_ce_6: 0.27  loss_mask_6: 0.3577  loss_dice_6: 2.81  loss_ce_7: 0.2594  loss_mask_7: 0.3576  loss_dice_7: 2.81  loss_ce_8: 0.2589  loss_mask_8: 0.3578  loss_dice_8: 2.807  time: 1.4878  data_time: 0.0685  lr: 4.2111e-06  max_mem: 21589M
[01/18 06:13:22] d2.utils.events INFO:  eta: 6:08:58  iter: 24719  total_loss: 34.53  loss_ce: 0.2644  loss_mask: 0.3517  loss_dice: 2.765  loss_ce_0: 0.5462  loss_mask_0: 0.3437  loss_dice_0: 2.892  loss_ce_1: 0.2993  loss_mask_1: 0.3561  loss_dice_1: 2.813  loss_ce_2: 0.2909  loss_mask_2: 0.3555  loss_dice_2: 2.787  loss_ce_3: 0.2772  loss_mask_3: 0.3526  loss_dice_3: 2.781  loss_ce_4: 0.2758  loss_mask_4: 0.3509  loss_dice_4: 2.773  loss_ce_5: 0.2851  loss_mask_5: 0.3535  loss_dice_5: 2.772  loss_ce_6: 0.2692  loss_mask_6: 0.3528  loss_dice_6: 2.775  loss_ce_7: 0.2609  loss_mask_7: 0.3518  loss_dice_7: 2.766  loss_ce_8: 0.2674  loss_mask_8: 0.3525  loss_dice_8: 2.766  time: 1.4878  data_time: 0.0708  lr: 4.2061e-06  max_mem: 21589M
[01/18 06:13:51] d2.utils.events INFO:  eta: 6:08:29  iter: 24739  total_loss: 34.24  loss_ce: 0.2472  loss_mask: 0.3709  loss_dice: 2.701  loss_ce_0: 0.5644  loss_mask_0: 0.3686  loss_dice_0: 2.833  loss_ce_1: 0.2961  loss_mask_1: 0.3772  loss_dice_1: 2.743  loss_ce_2: 0.2613  loss_mask_2: 0.3762  loss_dice_2: 2.718  loss_ce_3: 0.2476  loss_mask_3: 0.3741  loss_dice_3: 2.701  loss_ce_4: 0.2594  loss_mask_4: 0.3726  loss_dice_4: 2.701  loss_ce_5: 0.2467  loss_mask_5: 0.3726  loss_dice_5: 2.701  loss_ce_6: 0.2482  loss_mask_6: 0.3724  loss_dice_6: 2.709  loss_ce_7: 0.2473  loss_mask_7: 0.371  loss_dice_7: 2.702  loss_ce_8: 0.2426  loss_mask_8: 0.3727  loss_dice_8: 2.692  time: 1.4877  data_time: 0.0741  lr: 4.2012e-06  max_mem: 21589M
[01/18 06:14:20] d2.utils.events INFO:  eta: 6:08:00  iter: 24759  total_loss: 33.97  loss_ce: 0.2792  loss_mask: 0.374  loss_dice: 2.681  loss_ce_0: 0.5762  loss_mask_0: 0.3645  loss_dice_0: 2.805  loss_ce_1: 0.3107  loss_mask_1: 0.3783  loss_dice_1: 2.719  loss_ce_2: 0.2984  loss_mask_2: 0.3759  loss_dice_2: 2.7  loss_ce_3: 0.2903  loss_mask_3: 0.374  loss_dice_3: 2.681  loss_ce_4: 0.2862  loss_mask_4: 0.3718  loss_dice_4: 2.688  loss_ce_5: 0.2797  loss_mask_5: 0.3717  loss_dice_5: 2.677  loss_ce_6: 0.2863  loss_mask_6: 0.3737  loss_dice_6: 2.686  loss_ce_7: 0.2638  loss_mask_7: 0.373  loss_dice_7: 2.687  loss_ce_8: 0.2731  loss_mask_8: 0.3737  loss_dice_8: 2.689  time: 1.4877  data_time: 0.0676  lr: 4.1962e-06  max_mem: 21589M
[01/18 06:14:49] d2.utils.events INFO:  eta: 6:07:37  iter: 24779  total_loss: 34.43  loss_ce: 0.2549  loss_mask: 0.3531  loss_dice: 2.753  loss_ce_0: 0.5403  loss_mask_0: 0.3509  loss_dice_0: 2.883  loss_ce_1: 0.2994  loss_mask_1: 0.3565  loss_dice_1: 2.803  loss_ce_2: 0.2986  loss_mask_2: 0.3525  loss_dice_2: 2.782  loss_ce_3: 0.2661  loss_mask_3: 0.3539  loss_dice_3: 2.77  loss_ce_4: 0.2633  loss_mask_4: 0.3525  loss_dice_4: 2.762  loss_ce_5: 0.2594  loss_mask_5: 0.3536  loss_dice_5: 2.766  loss_ce_6: 0.2628  loss_mask_6: 0.3522  loss_dice_6: 2.753  loss_ce_7: 0.2411  loss_mask_7: 0.3537  loss_dice_7: 2.763  loss_ce_8: 0.2595  loss_mask_8: 0.3529  loss_dice_8: 2.756  time: 1.4876  data_time: 0.0692  lr: 4.1913e-06  max_mem: 21589M
[01/18 06:15:18] d2.utils.events INFO:  eta: 6:06:56  iter: 24799  total_loss: 33.91  loss_ce: 0.2607  loss_mask: 0.3597  loss_dice: 2.721  loss_ce_0: 0.5662  loss_mask_0: 0.3569  loss_dice_0: 2.847  loss_ce_1: 0.2939  loss_mask_1: 0.3639  loss_dice_1: 2.774  loss_ce_2: 0.273  loss_mask_2: 0.3629  loss_dice_2: 2.745  loss_ce_3: 0.2704  loss_mask_3: 0.3606  loss_dice_3: 2.729  loss_ce_4: 0.2596  loss_mask_4: 0.3608  loss_dice_4: 2.732  loss_ce_5: 0.2669  loss_mask_5: 0.3598  loss_dice_5: 2.729  loss_ce_6: 0.2592  loss_mask_6: 0.3592  loss_dice_6: 2.723  loss_ce_7: 0.2573  loss_mask_7: 0.3593  loss_dice_7: 2.726  loss_ce_8: 0.2585  loss_mask_8: 0.3583  loss_dice_8: 2.73  time: 1.4876  data_time: 0.0674  lr: 4.1863e-06  max_mem: 21589M
[01/18 06:15:47] d2.utils.events INFO:  eta: 6:06:30  iter: 24819  total_loss: 33.71  loss_ce: 0.2451  loss_mask: 0.3512  loss_dice: 2.72  loss_ce_0: 0.537  loss_mask_0: 0.3466  loss_dice_0: 2.842  loss_ce_1: 0.2906  loss_mask_1: 0.358  loss_dice_1: 2.754  loss_ce_2: 0.2767  loss_mask_2: 0.3523  loss_dice_2: 2.747  loss_ce_3: 0.2601  loss_mask_3: 0.3521  loss_dice_3: 2.726  loss_ce_4: 0.2573  loss_mask_4: 0.3537  loss_dice_4: 2.717  loss_ce_5: 0.2438  loss_mask_5: 0.3519  loss_dice_5: 2.717  loss_ce_6: 0.25  loss_mask_6: 0.3516  loss_dice_6: 2.72  loss_ce_7: 0.2465  loss_mask_7: 0.3523  loss_dice_7: 2.72  loss_ce_8: 0.2466  loss_mask_8: 0.351  loss_dice_8: 2.72  time: 1.4876  data_time: 0.0713  lr: 4.1813e-06  max_mem: 21589M
[01/18 06:16:16] d2.utils.events INFO:  eta: 6:05:42  iter: 24839  total_loss: 34.55  loss_ce: 0.268  loss_mask: 0.3511  loss_dice: 2.729  loss_ce_0: 0.5648  loss_mask_0: 0.3546  loss_dice_0: 2.855  loss_ce_1: 0.3157  loss_mask_1: 0.3608  loss_dice_1: 2.753  loss_ce_2: 0.301  loss_mask_2: 0.3562  loss_dice_2: 2.736  loss_ce_3: 0.2897  loss_mask_3: 0.3535  loss_dice_3: 2.735  loss_ce_4: 0.2845  loss_mask_4: 0.3522  loss_dice_4: 2.726  loss_ce_5: 0.2839  loss_mask_5: 0.3511  loss_dice_5: 2.725  loss_ce_6: 0.28  loss_mask_6: 0.3503  loss_dice_6: 2.725  loss_ce_7: 0.2554  loss_mask_7: 0.3516  loss_dice_7: 2.73  loss_ce_8: 0.27  loss_mask_8: 0.3529  loss_dice_8: 2.724  time: 1.4875  data_time: 0.0664  lr: 4.1764e-06  max_mem: 21589M
[01/18 06:16:45] d2.utils.events INFO:  eta: 6:05:24  iter: 24859  total_loss: 35.02  loss_ce: 0.2745  loss_mask: 0.356  loss_dice: 2.821  loss_ce_0: 0.5588  loss_mask_0: 0.3501  loss_dice_0: 2.936  loss_ce_1: 0.325  loss_mask_1: 0.3619  loss_dice_1: 2.859  loss_ce_2: 0.3169  loss_mask_2: 0.3612  loss_dice_2: 2.835  loss_ce_3: 0.2981  loss_mask_3: 0.3567  loss_dice_3: 2.821  loss_ce_4: 0.2862  loss_mask_4: 0.3565  loss_dice_4: 2.825  loss_ce_5: 0.2932  loss_mask_5: 0.3579  loss_dice_5: 2.824  loss_ce_6: 0.2895  loss_mask_6: 0.3566  loss_dice_6: 2.813  loss_ce_7: 0.2853  loss_mask_7: 0.3583  loss_dice_7: 2.816  loss_ce_8: 0.2786  loss_mask_8: 0.3575  loss_dice_8: 2.818  time: 1.4875  data_time: 0.0715  lr: 4.1714e-06  max_mem: 21589M
[01/18 06:17:14] d2.utils.events INFO:  eta: 6:05:03  iter: 24879  total_loss: 34.97  loss_ce: 0.2711  loss_mask: 0.3647  loss_dice: 2.8  loss_ce_0: 0.567  loss_mask_0: 0.3586  loss_dice_0: 2.921  loss_ce_1: 0.2983  loss_mask_1: 0.3688  loss_dice_1: 2.844  loss_ce_2: 0.2989  loss_mask_2: 0.3656  loss_dice_2: 2.818  loss_ce_3: 0.291  loss_mask_3: 0.3643  loss_dice_3: 2.803  loss_ce_4: 0.2826  loss_mask_4: 0.3633  loss_dice_4: 2.797  loss_ce_5: 0.2888  loss_mask_5: 0.364  loss_dice_5: 2.811  loss_ce_6: 0.2774  loss_mask_6: 0.3642  loss_dice_6: 2.801  loss_ce_7: 0.2784  loss_mask_7: 0.3645  loss_dice_7: 2.797  loss_ce_8: 0.2719  loss_mask_8: 0.3627  loss_dice_8: 2.794  time: 1.4875  data_time: 0.0693  lr: 4.1665e-06  max_mem: 21589M
[01/18 06:17:43] d2.utils.events INFO:  eta: 6:04:41  iter: 24899  total_loss: 34.19  loss_ce: 0.2501  loss_mask: 0.3538  loss_dice: 2.737  loss_ce_0: 0.5502  loss_mask_0: 0.3555  loss_dice_0: 2.862  loss_ce_1: 0.2944  loss_mask_1: 0.36  loss_dice_1: 2.778  loss_ce_2: 0.2943  loss_mask_2: 0.3562  loss_dice_2: 2.753  loss_ce_3: 0.2728  loss_mask_3: 0.355  loss_dice_3: 2.749  loss_ce_4: 0.2762  loss_mask_4: 0.3563  loss_dice_4: 2.744  loss_ce_5: 0.2666  loss_mask_5: 0.3564  loss_dice_5: 2.739  loss_ce_6: 0.263  loss_mask_6: 0.3568  loss_dice_6: 2.73  loss_ce_7: 0.262  loss_mask_7: 0.3549  loss_dice_7: 2.735  loss_ce_8: 0.2566  loss_mask_8: 0.3539  loss_dice_8: 2.742  time: 1.4875  data_time: 0.0695  lr: 4.1615e-06  max_mem: 21589M
[01/18 06:18:12] d2.utils.events INFO:  eta: 6:04:07  iter: 24919  total_loss: 35.01  loss_ce: 0.2733  loss_mask: 0.361  loss_dice: 2.795  loss_ce_0: 0.5891  loss_mask_0: 0.3514  loss_dice_0: 2.936  loss_ce_1: 0.3206  loss_mask_1: 0.3641  loss_dice_1: 2.834  loss_ce_2: 0.3073  loss_mask_2: 0.3611  loss_dice_2: 2.81  loss_ce_3: 0.2937  loss_mask_3: 0.3598  loss_dice_3: 2.795  loss_ce_4: 0.2802  loss_mask_4: 0.3623  loss_dice_4: 2.799  loss_ce_5: 0.2695  loss_mask_5: 0.3609  loss_dice_5: 2.793  loss_ce_6: 0.2583  loss_mask_6: 0.3619  loss_dice_6: 2.796  loss_ce_7: 0.2687  loss_mask_7: 0.3611  loss_dice_7: 2.787  loss_ce_8: 0.2617  loss_mask_8: 0.36  loss_dice_8: 2.79  time: 1.4874  data_time: 0.0715  lr: 4.1566e-06  max_mem: 21589M
[01/18 06:18:41] d2.utils.events INFO:  eta: 6:03:33  iter: 24939  total_loss: 34.45  loss_ce: 0.2507  loss_mask: 0.3684  loss_dice: 2.772  loss_ce_0: 0.5634  loss_mask_0: 0.369  loss_dice_0: 2.875  loss_ce_1: 0.2813  loss_mask_1: 0.3754  loss_dice_1: 2.798  loss_ce_2: 0.2852  loss_mask_2: 0.3722  loss_dice_2: 2.781  loss_ce_3: 0.2718  loss_mask_3: 0.368  loss_dice_3: 2.782  loss_ce_4: 0.2606  loss_mask_4: 0.3689  loss_dice_4: 2.775  loss_ce_5: 0.2516  loss_mask_5: 0.3678  loss_dice_5: 2.787  loss_ce_6: 0.2599  loss_mask_6: 0.3664  loss_dice_6: 2.773  loss_ce_7: 0.2529  loss_mask_7: 0.3682  loss_dice_7: 2.783  loss_ce_8: 0.2512  loss_mask_8: 0.3676  loss_dice_8: 2.776  time: 1.4874  data_time: 0.0719  lr: 4.1516e-06  max_mem: 21589M
[01/18 06:19:10] d2.utils.events INFO:  eta: 6:03:07  iter: 24959  total_loss: 34.65  loss_ce: 0.2496  loss_mask: 0.3594  loss_dice: 2.788  loss_ce_0: 0.569  loss_mask_0: 0.3665  loss_dice_0: 2.936  loss_ce_1: 0.2957  loss_mask_1: 0.3673  loss_dice_1: 2.837  loss_ce_2: 0.2802  loss_mask_2: 0.3643  loss_dice_2: 2.82  loss_ce_3: 0.2671  loss_mask_3: 0.3626  loss_dice_3: 2.805  loss_ce_4: 0.2673  loss_mask_4: 0.362  loss_dice_4: 2.792  loss_ce_5: 0.2526  loss_mask_5: 0.359  loss_dice_5: 2.799  loss_ce_6: 0.2437  loss_mask_6: 0.3624  loss_dice_6: 2.794  loss_ce_7: 0.2604  loss_mask_7: 0.3586  loss_dice_7: 2.795  loss_ce_8: 0.2459  loss_mask_8: 0.3597  loss_dice_8: 2.795  time: 1.4874  data_time: 0.0690  lr: 4.1466e-06  max_mem: 21589M
[01/18 06:19:39] d2.utils.events INFO:  eta: 6:02:35  iter: 24979  total_loss: 34.57  loss_ce: 0.2568  loss_mask: 0.3527  loss_dice: 2.788  loss_ce_0: 0.5491  loss_mask_0: 0.3524  loss_dice_0: 2.9  loss_ce_1: 0.289  loss_mask_1: 0.3527  loss_dice_1: 2.826  loss_ce_2: 0.2869  loss_mask_2: 0.3488  loss_dice_2: 2.798  loss_ce_3: 0.2688  loss_mask_3: 0.3509  loss_dice_3: 2.786  loss_ce_4: 0.2676  loss_mask_4: 0.3516  loss_dice_4: 2.786  loss_ce_5: 0.2531  loss_mask_5: 0.3523  loss_dice_5: 2.792  loss_ce_6: 0.2534  loss_mask_6: 0.3539  loss_dice_6: 2.792  loss_ce_7: 0.2549  loss_mask_7: 0.3534  loss_dice_7: 2.783  loss_ce_8: 0.2513  loss_mask_8: 0.3529  loss_dice_8: 2.795  time: 1.4873  data_time: 0.0703  lr: 4.1417e-06  max_mem: 21589M
[01/18 06:20:09] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0024999.pth
[01/18 06:20:10] d2.utils.events INFO:  eta: 6:02:11  iter: 24999  total_loss: 35.32  loss_ce: 0.2676  loss_mask: 0.3572  loss_dice: 2.839  loss_ce_0: 0.5768  loss_mask_0: 0.3528  loss_dice_0: 2.971  loss_ce_1: 0.3232  loss_mask_1: 0.3655  loss_dice_1: 2.878  loss_ce_2: 0.313  loss_mask_2: 0.36  loss_dice_2: 2.856  loss_ce_3: 0.2982  loss_mask_3: 0.3571  loss_dice_3: 2.845  loss_ce_4: 0.2776  loss_mask_4: 0.3542  loss_dice_4: 2.843  loss_ce_5: 0.2858  loss_mask_5: 0.3546  loss_dice_5: 2.85  loss_ce_6: 0.2865  loss_mask_6: 0.3555  loss_dice_6: 2.843  loss_ce_7: 0.2794  loss_mask_7: 0.3561  loss_dice_7: 2.843  loss_ce_8: 0.2806  loss_mask_8: 0.3558  loss_dice_8: 2.846  time: 1.4873  data_time: 0.0721  lr: 4.1367e-06  max_mem: 21589M
[01/18 06:20:39] d2.utils.events INFO:  eta: 6:01:49  iter: 25019  total_loss: 34.81  loss_ce: 0.2526  loss_mask: 0.3598  loss_dice: 2.815  loss_ce_0: 0.5381  loss_mask_0: 0.3599  loss_dice_0: 2.929  loss_ce_1: 0.2926  loss_mask_1: 0.3652  loss_dice_1: 2.85  loss_ce_2: 0.274  loss_mask_2: 0.3617  loss_dice_2: 2.834  loss_ce_3: 0.2599  loss_mask_3: 0.3618  loss_dice_3: 2.823  loss_ce_4: 0.2683  loss_mask_4: 0.362  loss_dice_4: 2.824  loss_ce_5: 0.2452  loss_mask_5: 0.3615  loss_dice_5: 2.821  loss_ce_6: 0.249  loss_mask_6: 0.3618  loss_dice_6: 2.814  loss_ce_7: 0.2447  loss_mask_7: 0.36  loss_dice_7: 2.815  loss_ce_8: 0.2363  loss_mask_8: 0.3602  loss_dice_8: 2.816  time: 1.4873  data_time: 0.0696  lr: 4.1317e-06  max_mem: 21589M
[01/18 06:21:09] d2.utils.events INFO:  eta: 6:01:20  iter: 25039  total_loss: 34.16  loss_ce: 0.2518  loss_mask: 0.3601  loss_dice: 2.73  loss_ce_0: 0.5573  loss_mask_0: 0.3573  loss_dice_0: 2.849  loss_ce_1: 0.3081  loss_mask_1: 0.3682  loss_dice_1: 2.777  loss_ce_2: 0.3008  loss_mask_2: 0.3628  loss_dice_2: 2.762  loss_ce_3: 0.293  loss_mask_3: 0.3613  loss_dice_3: 2.749  loss_ce_4: 0.2627  loss_mask_4: 0.3605  loss_dice_4: 2.745  loss_ce_5: 0.2589  loss_mask_5: 0.3606  loss_dice_5: 2.752  loss_ce_6: 0.2715  loss_mask_6: 0.3582  loss_dice_6: 2.741  loss_ce_7: 0.2615  loss_mask_7: 0.3586  loss_dice_7: 2.739  loss_ce_8: 0.2511  loss_mask_8: 0.3598  loss_dice_8: 2.745  time: 1.4873  data_time: 0.0755  lr: 4.1268e-06  max_mem: 21589M
[01/18 06:21:38] d2.utils.events INFO:  eta: 6:00:50  iter: 25059  total_loss: 34.08  loss_ce: 0.2621  loss_mask: 0.3633  loss_dice: 2.711  loss_ce_0: 0.5434  loss_mask_0: 0.3552  loss_dice_0: 2.831  loss_ce_1: 0.3018  loss_mask_1: 0.3659  loss_dice_1: 2.749  loss_ce_2: 0.3037  loss_mask_2: 0.3616  loss_dice_2: 2.734  loss_ce_3: 0.2845  loss_mask_3: 0.3623  loss_dice_3: 2.71  loss_ce_4: 0.2591  loss_mask_4: 0.3622  loss_dice_4: 2.706  loss_ce_5: 0.2689  loss_mask_5: 0.3625  loss_dice_5: 2.712  loss_ce_6: 0.2655  loss_mask_6: 0.3624  loss_dice_6: 2.703  loss_ce_7: 0.2458  loss_mask_7: 0.3633  loss_dice_7: 2.704  loss_ce_8: 0.2463  loss_mask_8: 0.3634  loss_dice_8: 2.707  time: 1.4873  data_time: 0.0698  lr: 4.1218e-06  max_mem: 21589M
[01/18 06:22:07] d2.utils.events INFO:  eta: 6:00:23  iter: 25079  total_loss: 34.59  loss_ce: 0.2603  loss_mask: 0.3536  loss_dice: 2.772  loss_ce_0: 0.5741  loss_mask_0: 0.3537  loss_dice_0: 2.894  loss_ce_1: 0.3084  loss_mask_1: 0.3604  loss_dice_1: 2.812  loss_ce_2: 0.3052  loss_mask_2: 0.3575  loss_dice_2: 2.789  loss_ce_3: 0.2794  loss_mask_3: 0.3556  loss_dice_3: 2.77  loss_ce_4: 0.2866  loss_mask_4: 0.3548  loss_dice_4: 2.776  loss_ce_5: 0.2743  loss_mask_5: 0.3549  loss_dice_5: 2.773  loss_ce_6: 0.2621  loss_mask_6: 0.3552  loss_dice_6: 2.77  loss_ce_7: 0.2607  loss_mask_7: 0.3549  loss_dice_7: 2.764  loss_ce_8: 0.2643  loss_mask_8: 0.3549  loss_dice_8: 2.775  time: 1.4872  data_time: 0.0706  lr: 4.1168e-06  max_mem: 21589M
[01/18 06:22:36] d2.utils.events INFO:  eta: 5:59:53  iter: 25099  total_loss: 34.41  loss_ce: 0.2579  loss_mask: 0.3542  loss_dice: 2.733  loss_ce_0: 0.5803  loss_mask_0: 0.3504  loss_dice_0: 2.871  loss_ce_1: 0.2996  loss_mask_1: 0.3576  loss_dice_1: 2.786  loss_ce_2: 0.2887  loss_mask_2: 0.3574  loss_dice_2: 2.758  loss_ce_3: 0.2702  loss_mask_3: 0.354  loss_dice_3: 2.744  loss_ce_4: 0.2635  loss_mask_4: 0.355  loss_dice_4: 2.735  loss_ce_5: 0.2594  loss_mask_5: 0.3541  loss_dice_5: 2.74  loss_ce_6: 0.2741  loss_mask_6: 0.3555  loss_dice_6: 2.734  loss_ce_7: 0.2523  loss_mask_7: 0.3554  loss_dice_7: 2.741  loss_ce_8: 0.2695  loss_mask_8: 0.3555  loss_dice_8: 2.736  time: 1.4872  data_time: 0.0693  lr: 4.1119e-06  max_mem: 21589M
[01/18 06:23:05] d2.utils.events INFO:  eta: 5:59:20  iter: 25119  total_loss: 34.44  loss_ce: 0.2577  loss_mask: 0.3602  loss_dice: 2.768  loss_ce_0: 0.5571  loss_mask_0: 0.3546  loss_dice_0: 2.891  loss_ce_1: 0.2791  loss_mask_1: 0.363  loss_dice_1: 2.801  loss_ce_2: 0.2613  loss_mask_2: 0.3626  loss_dice_2: 2.777  loss_ce_3: 0.257  loss_mask_3: 0.3609  loss_dice_3: 2.777  loss_ce_4: 0.2577  loss_mask_4: 0.3611  loss_dice_4: 2.777  loss_ce_5: 0.2453  loss_mask_5: 0.3604  loss_dice_5: 2.775  loss_ce_6: 0.2578  loss_mask_6: 0.3601  loss_dice_6: 2.766  loss_ce_7: 0.2562  loss_mask_7: 0.3615  loss_dice_7: 2.766  loss_ce_8: 0.2577  loss_mask_8: 0.3586  loss_dice_8: 2.77  time: 1.4872  data_time: 0.0708  lr: 4.1069e-06  max_mem: 21589M
[01/18 06:23:34] d2.utils.events INFO:  eta: 5:58:47  iter: 25139  total_loss: 34.49  loss_ce: 0.2584  loss_mask: 0.3555  loss_dice: 2.787  loss_ce_0: 0.5372  loss_mask_0: 0.3546  loss_dice_0: 2.917  loss_ce_1: 0.296  loss_mask_1: 0.3584  loss_dice_1: 2.83  loss_ce_2: 0.2856  loss_mask_2: 0.3563  loss_dice_2: 2.8  loss_ce_3: 0.2712  loss_mask_3: 0.3563  loss_dice_3: 2.804  loss_ce_4: 0.2664  loss_mask_4: 0.3559  loss_dice_4: 2.788  loss_ce_5: 0.2751  loss_mask_5: 0.3558  loss_dice_5: 2.799  loss_ce_6: 0.2577  loss_mask_6: 0.3557  loss_dice_6: 2.793  loss_ce_7: 0.2569  loss_mask_7: 0.3557  loss_dice_7: 2.788  loss_ce_8: 0.2546  loss_mask_8: 0.3546  loss_dice_8: 2.789  time: 1.4871  data_time: 0.0677  lr: 4.1019e-06  max_mem: 21589M
[01/18 06:24:03] d2.utils.events INFO:  eta: 5:58:11  iter: 25159  total_loss: 34.58  loss_ce: 0.2707  loss_mask: 0.3586  loss_dice: 2.757  loss_ce_0: 0.5589  loss_mask_0: 0.3594  loss_dice_0: 2.884  loss_ce_1: 0.3277  loss_mask_1: 0.3602  loss_dice_1: 2.793  loss_ce_2: 0.3143  loss_mask_2: 0.3583  loss_dice_2: 2.783  loss_ce_3: 0.2913  loss_mask_3: 0.3586  loss_dice_3: 2.765  loss_ce_4: 0.2806  loss_mask_4: 0.3599  loss_dice_4: 2.763  loss_ce_5: 0.2806  loss_mask_5: 0.3573  loss_dice_5: 2.764  loss_ce_6: 0.2779  loss_mask_6: 0.358  loss_dice_6: 2.757  loss_ce_7: 0.2766  loss_mask_7: 0.3572  loss_dice_7: 2.756  loss_ce_8: 0.2795  loss_mask_8: 0.3586  loss_dice_8: 2.76  time: 1.4871  data_time: 0.0709  lr: 4.097e-06  max_mem: 21589M
[01/18 06:24:32] d2.utils.events INFO:  eta: 5:57:39  iter: 25179  total_loss: 34.08  loss_ce: 0.2558  loss_mask: 0.3575  loss_dice: 2.725  loss_ce_0: 0.5589  loss_mask_0: 0.3563  loss_dice_0: 2.867  loss_ce_1: 0.3189  loss_mask_1: 0.3672  loss_dice_1: 2.766  loss_ce_2: 0.3088  loss_mask_2: 0.3636  loss_dice_2: 2.745  loss_ce_3: 0.29  loss_mask_3: 0.3602  loss_dice_3: 2.741  loss_ce_4: 0.2609  loss_mask_4: 0.3588  loss_dice_4: 2.737  loss_ce_5: 0.2667  loss_mask_5: 0.3589  loss_dice_5: 2.728  loss_ce_6: 0.2699  loss_mask_6: 0.3575  loss_dice_6: 2.724  loss_ce_7: 0.2628  loss_mask_7: 0.3575  loss_dice_7: 2.725  loss_ce_8: 0.2584  loss_mask_8: 0.3568  loss_dice_8: 2.732  time: 1.4871  data_time: 0.0731  lr: 4.092e-06  max_mem: 21589M
[01/18 06:25:01] d2.utils.events INFO:  eta: 5:57:04  iter: 25199  total_loss: 34.16  loss_ce: 0.2569  loss_mask: 0.3532  loss_dice: 2.704  loss_ce_0: 0.5792  loss_mask_0: 0.3517  loss_dice_0: 2.831  loss_ce_1: 0.3125  loss_mask_1: 0.3588  loss_dice_1: 2.743  loss_ce_2: 0.2984  loss_mask_2: 0.356  loss_dice_2: 2.734  loss_ce_3: 0.2754  loss_mask_3: 0.3525  loss_dice_3: 2.712  loss_ce_4: 0.2693  loss_mask_4: 0.3519  loss_dice_4: 2.707  loss_ce_5: 0.2667  loss_mask_5: 0.3517  loss_dice_5: 2.706  loss_ce_6: 0.2558  loss_mask_6: 0.3533  loss_dice_6: 2.707  loss_ce_7: 0.26  loss_mask_7: 0.3535  loss_dice_7: 2.7  loss_ce_8: 0.249  loss_mask_8: 0.3529  loss_dice_8: 2.717  time: 1.4871  data_time: 0.0708  lr: 4.087e-06  max_mem: 21589M
[01/18 06:25:30] d2.utils.events INFO:  eta: 5:56:26  iter: 25219  total_loss: 34.52  loss_ce: 0.2552  loss_mask: 0.3646  loss_dice: 2.769  loss_ce_0: 0.5709  loss_mask_0: 0.3592  loss_dice_0: 2.907  loss_ce_1: 0.2929  loss_mask_1: 0.3667  loss_dice_1: 2.824  loss_ce_2: 0.2924  loss_mask_2: 0.3652  loss_dice_2: 2.798  loss_ce_3: 0.2664  loss_mask_3: 0.3644  loss_dice_3: 2.786  loss_ce_4: 0.2572  loss_mask_4: 0.3638  loss_dice_4: 2.777  loss_ce_5: 0.2678  loss_mask_5: 0.363  loss_dice_5: 2.775  loss_ce_6: 0.2538  loss_mask_6: 0.3647  loss_dice_6: 2.773  loss_ce_7: 0.2549  loss_mask_7: 0.364  loss_dice_7: 2.774  loss_ce_8: 0.2545  loss_mask_8: 0.3642  loss_dice_8: 2.774  time: 1.4870  data_time: 0.0713  lr: 4.0821e-06  max_mem: 21589M
[01/18 06:25:59] d2.utils.events INFO:  eta: 5:55:59  iter: 25239  total_loss: 34.52  loss_ce: 0.2569  loss_mask: 0.3434  loss_dice: 2.746  loss_ce_0: 0.5604  loss_mask_0: 0.3365  loss_dice_0: 2.897  loss_ce_1: 0.3201  loss_mask_1: 0.3455  loss_dice_1: 2.804  loss_ce_2: 0.3216  loss_mask_2: 0.3452  loss_dice_2: 2.779  loss_ce_3: 0.2906  loss_mask_3: 0.3427  loss_dice_3: 2.766  loss_ce_4: 0.2692  loss_mask_4: 0.342  loss_dice_4: 2.756  loss_ce_5: 0.2694  loss_mask_5: 0.343  loss_dice_5: 2.763  loss_ce_6: 0.2665  loss_mask_6: 0.3425  loss_dice_6: 2.747  loss_ce_7: 0.2564  loss_mask_7: 0.3426  loss_dice_7: 2.757  loss_ce_8: 0.2624  loss_mask_8: 0.3426  loss_dice_8: 2.755  time: 1.4870  data_time: 0.0709  lr: 4.0771e-06  max_mem: 21589M
[01/18 06:26:29] d2.utils.events INFO:  eta: 5:55:29  iter: 25259  total_loss: 34.13  loss_ce: 0.262  loss_mask: 0.3553  loss_dice: 2.762  loss_ce_0: 0.5179  loss_mask_0: 0.3487  loss_dice_0: 2.901  loss_ce_1: 0.2816  loss_mask_1: 0.3602  loss_dice_1: 2.802  loss_ce_2: 0.2893  loss_mask_2: 0.3569  loss_dice_2: 2.787  loss_ce_3: 0.2776  loss_mask_3: 0.3566  loss_dice_3: 2.769  loss_ce_4: 0.2543  loss_mask_4: 0.3565  loss_dice_4: 2.774  loss_ce_5: 0.2746  loss_mask_5: 0.3555  loss_dice_5: 2.776  loss_ce_6: 0.2583  loss_mask_6: 0.356  loss_dice_6: 2.776  loss_ce_7: 0.2665  loss_mask_7: 0.3559  loss_dice_7: 2.767  loss_ce_8: 0.2576  loss_mask_8: 0.3565  loss_dice_8: 2.762  time: 1.4870  data_time: 0.0747  lr: 4.0721e-06  max_mem: 21589M
[01/18 06:26:58] d2.utils.events INFO:  eta: 5:55:05  iter: 25279  total_loss: 34.33  loss_ce: 0.2481  loss_mask: 0.3564  loss_dice: 2.806  loss_ce_0: 0.5436  loss_mask_0: 0.3551  loss_dice_0: 2.944  loss_ce_1: 0.2989  loss_mask_1: 0.3644  loss_dice_1: 2.855  loss_ce_2: 0.273  loss_mask_2: 0.3598  loss_dice_2: 2.821  loss_ce_3: 0.2514  loss_mask_3: 0.3565  loss_dice_3: 2.812  loss_ce_4: 0.2696  loss_mask_4: 0.3569  loss_dice_4: 2.811  loss_ce_5: 0.2487  loss_mask_5: 0.3563  loss_dice_5: 2.808  loss_ce_6: 0.2463  loss_mask_6: 0.3574  loss_dice_6: 2.809  loss_ce_7: 0.2439  loss_mask_7: 0.3576  loss_dice_7: 2.802  loss_ce_8: 0.2343  loss_mask_8: 0.3577  loss_dice_8: 2.8  time: 1.4870  data_time: 0.0697  lr: 4.0671e-06  max_mem: 21589M
[01/18 06:27:27] d2.utils.events INFO:  eta: 5:54:36  iter: 25299  total_loss: 34.51  loss_ce: 0.2473  loss_mask: 0.3517  loss_dice: 2.733  loss_ce_0: 0.5398  loss_mask_0: 0.3505  loss_dice_0: 2.872  loss_ce_1: 0.2916  loss_mask_1: 0.356  loss_dice_1: 2.777  loss_ce_2: 0.2858  loss_mask_2: 0.353  loss_dice_2: 2.758  loss_ce_3: 0.2706  loss_mask_3: 0.3511  loss_dice_3: 2.748  loss_ce_4: 0.2604  loss_mask_4: 0.3523  loss_dice_4: 2.748  loss_ce_5: 0.2514  loss_mask_5: 0.3499  loss_dice_5: 2.747  loss_ce_6: 0.2489  loss_mask_6: 0.3488  loss_dice_6: 2.744  loss_ce_7: 0.2535  loss_mask_7: 0.3498  loss_dice_7: 2.738  loss_ce_8: 0.2481  loss_mask_8: 0.3496  loss_dice_8: 2.735  time: 1.4869  data_time: 0.0749  lr: 4.0622e-06  max_mem: 21589M
[01/18 06:27:56] d2.utils.events INFO:  eta: 5:54:11  iter: 25319  total_loss: 34.29  loss_ce: 0.2671  loss_mask: 0.353  loss_dice: 2.738  loss_ce_0: 0.5572  loss_mask_0: 0.3573  loss_dice_0: 2.862  loss_ce_1: 0.3158  loss_mask_1: 0.3589  loss_dice_1: 2.782  loss_ce_2: 0.299  loss_mask_2: 0.356  loss_dice_2: 2.766  loss_ce_3: 0.2976  loss_mask_3: 0.3544  loss_dice_3: 2.746  loss_ce_4: 0.2709  loss_mask_4: 0.3514  loss_dice_4: 2.745  loss_ce_5: 0.2761  loss_mask_5: 0.3509  loss_dice_5: 2.75  loss_ce_6: 0.2811  loss_mask_6: 0.3517  loss_dice_6: 2.739  loss_ce_7: 0.2776  loss_mask_7: 0.3505  loss_dice_7: 2.735  loss_ce_8: 0.2746  loss_mask_8: 0.3515  loss_dice_8: 2.735  time: 1.4869  data_time: 0.0687  lr: 4.0572e-06  max_mem: 21589M
[01/18 06:28:25] d2.utils.events INFO:  eta: 5:53:37  iter: 25339  total_loss: 33.86  loss_ce: 0.2673  loss_mask: 0.3587  loss_dice: 2.709  loss_ce_0: 0.572  loss_mask_0: 0.3513  loss_dice_0: 2.838  loss_ce_1: 0.3086  loss_mask_1: 0.3621  loss_dice_1: 2.744  loss_ce_2: 0.2957  loss_mask_2: 0.3594  loss_dice_2: 2.73  loss_ce_3: 0.2815  loss_mask_3: 0.3582  loss_dice_3: 2.71  loss_ce_4: 0.275  loss_mask_4: 0.3577  loss_dice_4: 2.71  loss_ce_5: 0.2716  loss_mask_5: 0.3573  loss_dice_5: 2.709  loss_ce_6: 0.2755  loss_mask_6: 0.3582  loss_dice_6: 2.702  loss_ce_7: 0.27  loss_mask_7: 0.3583  loss_dice_7: 2.714  loss_ce_8: 0.2762  loss_mask_8: 0.359  loss_dice_8: 2.709  time: 1.4869  data_time: 0.0737  lr: 4.0522e-06  max_mem: 21589M
[01/18 06:28:54] d2.utils.events INFO:  eta: 5:53:08  iter: 25359  total_loss: 35.06  loss_ce: 0.2447  loss_mask: 0.3589  loss_dice: 2.791  loss_ce_0: 0.5627  loss_mask_0: 0.3588  loss_dice_0: 2.92  loss_ce_1: 0.2738  loss_mask_1: 0.3703  loss_dice_1: 2.842  loss_ce_2: 0.2864  loss_mask_2: 0.3676  loss_dice_2: 2.815  loss_ce_3: 0.2667  loss_mask_3: 0.3645  loss_dice_3: 2.8  loss_ce_4: 0.2542  loss_mask_4: 0.3593  loss_dice_4: 2.798  loss_ce_5: 0.2593  loss_mask_5: 0.3595  loss_dice_5: 2.797  loss_ce_6: 0.2413  loss_mask_6: 0.3597  loss_dice_6: 2.796  loss_ce_7: 0.2503  loss_mask_7: 0.3607  loss_dice_7: 2.792  loss_ce_8: 0.2486  loss_mask_8: 0.3601  loss_dice_8: 2.788  time: 1.4868  data_time: 0.0698  lr: 4.0472e-06  max_mem: 21589M
[01/18 06:29:23] d2.utils.events INFO:  eta: 5:52:42  iter: 25379  total_loss: 34.1  loss_ce: 0.2459  loss_mask: 0.3488  loss_dice: 2.743  loss_ce_0: 0.57  loss_mask_0: 0.3514  loss_dice_0: 2.872  loss_ce_1: 0.2868  loss_mask_1: 0.3554  loss_dice_1: 2.79  loss_ce_2: 0.2693  loss_mask_2: 0.3515  loss_dice_2: 2.757  loss_ce_3: 0.261  loss_mask_3: 0.3478  loss_dice_3: 2.759  loss_ce_4: 0.2479  loss_mask_4: 0.3462  loss_dice_4: 2.75  loss_ce_5: 0.2407  loss_mask_5: 0.3467  loss_dice_5: 2.75  loss_ce_6: 0.2488  loss_mask_6: 0.3469  loss_dice_6: 2.748  loss_ce_7: 0.2496  loss_mask_7: 0.3491  loss_dice_7: 2.748  loss_ce_8: 0.2523  loss_mask_8: 0.3476  loss_dice_8: 2.752  time: 1.4868  data_time: 0.0728  lr: 4.0423e-06  max_mem: 21589M
[01/18 06:29:53] d2.utils.events INFO:  eta: 5:52:21  iter: 25399  total_loss: 34.04  loss_ce: 0.2619  loss_mask: 0.3524  loss_dice: 2.736  loss_ce_0: 0.5911  loss_mask_0: 0.3426  loss_dice_0: 2.874  loss_ce_1: 0.3099  loss_mask_1: 0.3552  loss_dice_1: 2.781  loss_ce_2: 0.2937  loss_mask_2: 0.3523  loss_dice_2: 2.759  loss_ce_3: 0.2878  loss_mask_3: 0.3538  loss_dice_3: 2.746  loss_ce_4: 0.2742  loss_mask_4: 0.3536  loss_dice_4: 2.746  loss_ce_5: 0.2715  loss_mask_5: 0.3527  loss_dice_5: 2.74  loss_ce_6: 0.2591  loss_mask_6: 0.3518  loss_dice_6: 2.743  loss_ce_7: 0.2571  loss_mask_7: 0.3518  loss_dice_7: 2.73  loss_ce_8: 0.2554  loss_mask_8: 0.3516  loss_dice_8: 2.742  time: 1.4868  data_time: 0.0707  lr: 4.0373e-06  max_mem: 21589M
[01/18 06:30:22] d2.utils.events INFO:  eta: 5:52:02  iter: 25419  total_loss: 34  loss_ce: 0.2499  loss_mask: 0.3578  loss_dice: 2.726  loss_ce_0: 0.5703  loss_mask_0: 0.3551  loss_dice_0: 2.846  loss_ce_1: 0.2836  loss_mask_1: 0.3659  loss_dice_1: 2.773  loss_ce_2: 0.2831  loss_mask_2: 0.3612  loss_dice_2: 2.747  loss_ce_3: 0.2627  loss_mask_3: 0.3577  loss_dice_3: 2.718  loss_ce_4: 0.2571  loss_mask_4: 0.3597  loss_dice_4: 2.729  loss_ce_5: 0.2494  loss_mask_5: 0.3586  loss_dice_5: 2.734  loss_ce_6: 0.2532  loss_mask_6: 0.3572  loss_dice_6: 2.723  loss_ce_7: 0.2413  loss_mask_7: 0.3577  loss_dice_7: 2.725  loss_ce_8: 0.2407  loss_mask_8: 0.3577  loss_dice_8: 2.729  time: 1.4868  data_time: 0.0754  lr: 4.0323e-06  max_mem: 21589M
[01/18 06:30:51] d2.utils.events INFO:  eta: 5:51:37  iter: 25439  total_loss: 34.26  loss_ce: 0.2521  loss_mask: 0.3504  loss_dice: 2.777  loss_ce_0: 0.5616  loss_mask_0: 0.3474  loss_dice_0: 2.898  loss_ce_1: 0.2865  loss_mask_1: 0.3578  loss_dice_1: 2.818  loss_ce_2: 0.2804  loss_mask_2: 0.3514  loss_dice_2: 2.794  loss_ce_3: 0.2822  loss_mask_3: 0.3481  loss_dice_3: 2.787  loss_ce_4: 0.2592  loss_mask_4: 0.349  loss_dice_4: 2.782  loss_ce_5: 0.2618  loss_mask_5: 0.3483  loss_dice_5: 2.785  loss_ce_6: 0.2545  loss_mask_6: 0.3477  loss_dice_6: 2.781  loss_ce_7: 0.2491  loss_mask_7: 0.3492  loss_dice_7: 2.774  loss_ce_8: 0.2579  loss_mask_8: 0.35  loss_dice_8: 2.779  time: 1.4867  data_time: 0.0695  lr: 4.0273e-06  max_mem: 21589M
[01/18 06:31:19] d2.utils.events INFO:  eta: 5:51:11  iter: 25459  total_loss: 33.58  loss_ce: 0.2591  loss_mask: 0.3522  loss_dice: 2.651  loss_ce_0: 0.5416  loss_mask_0: 0.3528  loss_dice_0: 2.77  loss_ce_1: 0.2818  loss_mask_1: 0.3586  loss_dice_1: 2.688  loss_ce_2: 0.2845  loss_mask_2: 0.3578  loss_dice_2: 2.682  loss_ce_3: 0.2683  loss_mask_3: 0.356  loss_dice_3: 2.67  loss_ce_4: 0.2769  loss_mask_4: 0.3547  loss_dice_4: 2.659  loss_ce_5: 0.259  loss_mask_5: 0.353  loss_dice_5: 2.657  loss_ce_6: 0.2724  loss_mask_6: 0.3531  loss_dice_6: 2.655  loss_ce_7: 0.2436  loss_mask_7: 0.352  loss_dice_7: 2.657  loss_ce_8: 0.2463  loss_mask_8: 0.3524  loss_dice_8: 2.662  time: 1.4867  data_time: 0.0694  lr: 4.0224e-06  max_mem: 21589M
[01/18 06:31:48] d2.utils.events INFO:  eta: 5:50:35  iter: 25479  total_loss: 32.99  loss_ce: 0.2571  loss_mask: 0.3573  loss_dice: 2.631  loss_ce_0: 0.5626  loss_mask_0: 0.3507  loss_dice_0: 2.773  loss_ce_1: 0.3116  loss_mask_1: 0.3589  loss_dice_1: 2.683  loss_ce_2: 0.3074  loss_mask_2: 0.3571  loss_dice_2: 2.66  loss_ce_3: 0.2887  loss_mask_3: 0.3563  loss_dice_3: 2.633  loss_ce_4: 0.2683  loss_mask_4: 0.3571  loss_dice_4: 2.64  loss_ce_5: 0.2674  loss_mask_5: 0.358  loss_dice_5: 2.637  loss_ce_6: 0.2535  loss_mask_6: 0.3585  loss_dice_6: 2.642  loss_ce_7: 0.2648  loss_mask_7: 0.3579  loss_dice_7: 2.635  loss_ce_8: 0.257  loss_mask_8: 0.3578  loss_dice_8: 2.63  time: 1.4866  data_time: 0.0665  lr: 4.0174e-06  max_mem: 21589M
[01/18 06:32:18] d2.utils.events INFO:  eta: 5:50:06  iter: 25499  total_loss: 34.25  loss_ce: 0.2421  loss_mask: 0.3532  loss_dice: 2.793  loss_ce_0: 0.5489  loss_mask_0: 0.3556  loss_dice_0: 2.923  loss_ce_1: 0.2983  loss_mask_1: 0.3617  loss_dice_1: 2.839  loss_ce_2: 0.2853  loss_mask_2: 0.3588  loss_dice_2: 2.811  loss_ce_3: 0.2649  loss_mask_3: 0.3551  loss_dice_3: 2.803  loss_ce_4: 0.2642  loss_mask_4: 0.3547  loss_dice_4: 2.798  loss_ce_5: 0.2514  loss_mask_5: 0.3533  loss_dice_5: 2.798  loss_ce_6: 0.2515  loss_mask_6: 0.3543  loss_dice_6: 2.796  loss_ce_7: 0.2435  loss_mask_7: 0.3538  loss_dice_7: 2.791  loss_ce_8: 0.2319  loss_mask_8: 0.3544  loss_dice_8: 2.794  time: 1.4866  data_time: 0.0741  lr: 4.0124e-06  max_mem: 21589M
[01/18 06:32:47] d2.utils.events INFO:  eta: 5:49:21  iter: 25519  total_loss: 33.87  loss_ce: 0.2627  loss_mask: 0.361  loss_dice: 2.71  loss_ce_0: 0.5725  loss_mask_0: 0.3578  loss_dice_0: 2.836  loss_ce_1: 0.3021  loss_mask_1: 0.3681  loss_dice_1: 2.744  loss_ce_2: 0.2946  loss_mask_2: 0.3635  loss_dice_2: 2.726  loss_ce_3: 0.2757  loss_mask_3: 0.3625  loss_dice_3: 2.709  loss_ce_4: 0.2914  loss_mask_4: 0.3624  loss_dice_4: 2.709  loss_ce_5: 0.2724  loss_mask_5: 0.3614  loss_dice_5: 2.711  loss_ce_6: 0.2633  loss_mask_6: 0.3622  loss_dice_6: 2.704  loss_ce_7: 0.2662  loss_mask_7: 0.3608  loss_dice_7: 2.709  loss_ce_8: 0.2521  loss_mask_8: 0.3621  loss_dice_8: 2.709  time: 1.4866  data_time: 0.0677  lr: 4.0074e-06  max_mem: 21589M
[01/18 06:33:16] d2.utils.events INFO:  eta: 5:48:58  iter: 25539  total_loss: 33.89  loss_ce: 0.2792  loss_mask: 0.3501  loss_dice: 2.692  loss_ce_0: 0.5844  loss_mask_0: 0.3473  loss_dice_0: 2.838  loss_ce_1: 0.3055  loss_mask_1: 0.3534  loss_dice_1: 2.751  loss_ce_2: 0.3136  loss_mask_2: 0.351  loss_dice_2: 2.738  loss_ce_3: 0.287  loss_mask_3: 0.3489  loss_dice_3: 2.712  loss_ce_4: 0.2703  loss_mask_4: 0.3484  loss_dice_4: 2.703  loss_ce_5: 0.271  loss_mask_5: 0.3483  loss_dice_5: 2.71  loss_ce_6: 0.2704  loss_mask_6: 0.3493  loss_dice_6: 2.697  loss_ce_7: 0.2612  loss_mask_7: 0.3499  loss_dice_7: 2.699  loss_ce_8: 0.2694  loss_mask_8: 0.3509  loss_dice_8: 2.698  time: 1.4866  data_time: 0.0715  lr: 4.0024e-06  max_mem: 21589M
[01/18 06:33:45] d2.utils.events INFO:  eta: 5:48:35  iter: 25559  total_loss: 33.93  loss_ce: 0.2782  loss_mask: 0.3493  loss_dice: 2.708  loss_ce_0: 0.5535  loss_mask_0: 0.3404  loss_dice_0: 2.852  loss_ce_1: 0.2997  loss_mask_1: 0.3533  loss_dice_1: 2.761  loss_ce_2: 0.3169  loss_mask_2: 0.351  loss_dice_2: 2.732  loss_ce_3: 0.2746  loss_mask_3: 0.3511  loss_dice_3: 2.715  loss_ce_4: 0.2831  loss_mask_4: 0.3496  loss_dice_4: 2.719  loss_ce_5: 0.2877  loss_mask_5: 0.3505  loss_dice_5: 2.719  loss_ce_6: 0.2855  loss_mask_6: 0.3493  loss_dice_6: 2.712  loss_ce_7: 0.2873  loss_mask_7: 0.3492  loss_dice_7: 2.709  loss_ce_8: 0.2788  loss_mask_8: 0.3493  loss_dice_8: 2.714  time: 1.4866  data_time: 0.0707  lr: 3.9975e-06  max_mem: 21589M
[01/18 06:34:15] d2.utils.events INFO:  eta: 5:48:17  iter: 25579  total_loss: 33.74  loss_ce: 0.2517  loss_mask: 0.3521  loss_dice: 2.714  loss_ce_0: 0.5598  loss_mask_0: 0.3584  loss_dice_0: 2.86  loss_ce_1: 0.2985  loss_mask_1: 0.3646  loss_dice_1: 2.751  loss_ce_2: 0.3056  loss_mask_2: 0.3569  loss_dice_2: 2.722  loss_ce_3: 0.2741  loss_mask_3: 0.3552  loss_dice_3: 2.713  loss_ce_4: 0.2604  loss_mask_4: 0.353  loss_dice_4: 2.717  loss_ce_5: 0.2534  loss_mask_5: 0.3531  loss_dice_5: 2.718  loss_ce_6: 0.2596  loss_mask_6: 0.354  loss_dice_6: 2.711  loss_ce_7: 0.2412  loss_mask_7: 0.3507  loss_dice_7: 2.708  loss_ce_8: 0.2509  loss_mask_8: 0.3529  loss_dice_8: 2.714  time: 1.4865  data_time: 0.0673  lr: 3.9925e-06  max_mem: 21589M
[01/18 06:34:43] d2.utils.events INFO:  eta: 5:47:43  iter: 25599  total_loss: 33.31  loss_ce: 0.2691  loss_mask: 0.3514  loss_dice: 2.682  loss_ce_0: 0.5657  loss_mask_0: 0.3525  loss_dice_0: 2.812  loss_ce_1: 0.3057  loss_mask_1: 0.3591  loss_dice_1: 2.722  loss_ce_2: 0.2874  loss_mask_2: 0.3547  loss_dice_2: 2.698  loss_ce_3: 0.2884  loss_mask_3: 0.3539  loss_dice_3: 2.677  loss_ce_4: 0.2839  loss_mask_4: 0.353  loss_dice_4: 2.683  loss_ce_5: 0.2673  loss_mask_5: 0.3521  loss_dice_5: 2.677  loss_ce_6: 0.2638  loss_mask_6: 0.3533  loss_dice_6: 2.668  loss_ce_7: 0.2662  loss_mask_7: 0.3536  loss_dice_7: 2.665  loss_ce_8: 0.2494  loss_mask_8: 0.3526  loss_dice_8: 2.671  time: 1.4865  data_time: 0.0705  lr: 3.9875e-06  max_mem: 21589M
[01/18 06:35:13] d2.utils.events INFO:  eta: 5:47:38  iter: 25619  total_loss: 34.59  loss_ce: 0.2408  loss_mask: 0.3626  loss_dice: 2.753  loss_ce_0: 0.5707  loss_mask_0: 0.3628  loss_dice_0: 2.877  loss_ce_1: 0.2852  loss_mask_1: 0.369  loss_dice_1: 2.786  loss_ce_2: 0.2928  loss_mask_2: 0.3634  loss_dice_2: 2.778  loss_ce_3: 0.2578  loss_mask_3: 0.3633  loss_dice_3: 2.753  loss_ce_4: 0.2568  loss_mask_4: 0.3632  loss_dice_4: 2.758  loss_ce_5: 0.2541  loss_mask_5: 0.3636  loss_dice_5: 2.748  loss_ce_6: 0.2577  loss_mask_6: 0.3619  loss_dice_6: 2.747  loss_ce_7: 0.2566  loss_mask_7: 0.3599  loss_dice_7: 2.753  loss_ce_8: 0.2478  loss_mask_8: 0.3628  loss_dice_8: 2.75  time: 1.4865  data_time: 0.0764  lr: 3.9825e-06  max_mem: 21589M
[01/18 06:35:43] d2.utils.events INFO:  eta: 5:47:19  iter: 25639  total_loss: 34.05  loss_ce: 0.2609  loss_mask: 0.3435  loss_dice: 2.715  loss_ce_0: 0.5746  loss_mask_0: 0.3419  loss_dice_0: 2.844  loss_ce_1: 0.3168  loss_mask_1: 0.3483  loss_dice_1: 2.76  loss_ce_2: 0.2941  loss_mask_2: 0.3443  loss_dice_2: 2.741  loss_ce_3: 0.2756  loss_mask_3: 0.345  loss_dice_3: 2.724  loss_ce_4: 0.2694  loss_mask_4: 0.3448  loss_dice_4: 2.713  loss_ce_5: 0.2581  loss_mask_5: 0.3437  loss_dice_5: 2.71  loss_ce_6: 0.2475  loss_mask_6: 0.3435  loss_dice_6: 2.717  loss_ce_7: 0.2618  loss_mask_7: 0.3449  loss_dice_7: 2.712  loss_ce_8: 0.2655  loss_mask_8: 0.3432  loss_dice_8: 2.717  time: 1.4865  data_time: 0.0740  lr: 3.9775e-06  max_mem: 21589M
[01/18 06:36:13] d2.utils.events INFO:  eta: 5:47:06  iter: 25659  total_loss: 33.67  loss_ce: 0.239  loss_mask: 0.3469  loss_dice: 2.728  loss_ce_0: 0.53  loss_mask_0: 0.3454  loss_dice_0: 2.875  loss_ce_1: 0.287  loss_mask_1: 0.3554  loss_dice_1: 2.763  loss_ce_2: 0.2791  loss_mask_2: 0.3513  loss_dice_2: 2.75  loss_ce_3: 0.257  loss_mask_3: 0.3503  loss_dice_3: 2.726  loss_ce_4: 0.255  loss_mask_4: 0.348  loss_dice_4: 2.732  loss_ce_5: 0.2473  loss_mask_5: 0.3465  loss_dice_5: 2.734  loss_ce_6: 0.2397  loss_mask_6: 0.3473  loss_dice_6: 2.732  loss_ce_7: 0.2425  loss_mask_7: 0.3472  loss_dice_7: 2.737  loss_ce_8: 0.2421  loss_mask_8: 0.3472  loss_dice_8: 2.735  time: 1.4865  data_time: 0.0743  lr: 3.9725e-06  max_mem: 21589M
[01/18 06:36:42] d2.utils.events INFO:  eta: 5:46:51  iter: 25679  total_loss: 33.71  loss_ce: 0.2554  loss_mask: 0.3491  loss_dice: 2.692  loss_ce_0: 0.5587  loss_mask_0: 0.3409  loss_dice_0: 2.836  loss_ce_1: 0.2982  loss_mask_1: 0.3495  loss_dice_1: 2.74  loss_ce_2: 0.2912  loss_mask_2: 0.3473  loss_dice_2: 2.721  loss_ce_3: 0.2597  loss_mask_3: 0.3474  loss_dice_3: 2.699  loss_ce_4: 0.2665  loss_mask_4: 0.3483  loss_dice_4: 2.703  loss_ce_5: 0.2572  loss_mask_5: 0.3498  loss_dice_5: 2.707  loss_ce_6: 0.2506  loss_mask_6: 0.3506  loss_dice_6: 2.698  loss_ce_7: 0.2403  loss_mask_7: 0.3512  loss_dice_7: 2.694  loss_ce_8: 0.2575  loss_mask_8: 0.3493  loss_dice_8: 2.69  time: 1.4865  data_time: 0.0778  lr: 3.9675e-06  max_mem: 21589M
[01/18 06:37:12] d2.utils.events INFO:  eta: 5:46:34  iter: 25699  total_loss: 34.38  loss_ce: 0.2655  loss_mask: 0.3578  loss_dice: 2.743  loss_ce_0: 0.5665  loss_mask_0: 0.3551  loss_dice_0: 2.879  loss_ce_1: 0.3102  loss_mask_1: 0.3605  loss_dice_1: 2.79  loss_ce_2: 0.3107  loss_mask_2: 0.3592  loss_dice_2: 2.755  loss_ce_3: 0.2868  loss_mask_3: 0.3572  loss_dice_3: 2.749  loss_ce_4: 0.2729  loss_mask_4: 0.3577  loss_dice_4: 2.754  loss_ce_5: 0.2636  loss_mask_5: 0.3569  loss_dice_5: 2.741  loss_ce_6: 0.2521  loss_mask_6: 0.3587  loss_dice_6: 2.739  loss_ce_7: 0.2611  loss_mask_7: 0.3582  loss_dice_7: 2.744  loss_ce_8: 0.2616  loss_mask_8: 0.3587  loss_dice_8: 2.74  time: 1.4865  data_time: 0.0760  lr: 3.9626e-06  max_mem: 21589M
[01/18 06:37:41] d2.utils.events INFO:  eta: 5:46:07  iter: 25719  total_loss: 34.25  loss_ce: 0.2639  loss_mask: 0.3564  loss_dice: 2.754  loss_ce_0: 0.5643  loss_mask_0: 0.3496  loss_dice_0: 2.885  loss_ce_1: 0.3079  loss_mask_1: 0.3612  loss_dice_1: 2.788  loss_ce_2: 0.2828  loss_mask_2: 0.36  loss_dice_2: 2.773  loss_ce_3: 0.2876  loss_mask_3: 0.3575  loss_dice_3: 2.761  loss_ce_4: 0.2834  loss_mask_4: 0.3571  loss_dice_4: 2.755  loss_ce_5: 0.2739  loss_mask_5: 0.3549  loss_dice_5: 2.756  loss_ce_6: 0.2649  loss_mask_6: 0.3563  loss_dice_6: 2.758  loss_ce_7: 0.2712  loss_mask_7: 0.3555  loss_dice_7: 2.749  loss_ce_8: 0.2534  loss_mask_8: 0.3565  loss_dice_8: 2.76  time: 1.4865  data_time: 0.0713  lr: 3.9576e-06  max_mem: 21589M
[01/18 06:38:11] d2.utils.events INFO:  eta: 5:45:45  iter: 25739  total_loss: 34.3  loss_ce: 0.2624  loss_mask: 0.3517  loss_dice: 2.732  loss_ce_0: 0.5571  loss_mask_0: 0.3489  loss_dice_0: 2.854  loss_ce_1: 0.3218  loss_mask_1: 0.3571  loss_dice_1: 2.771  loss_ce_2: 0.2978  loss_mask_2: 0.3546  loss_dice_2: 2.751  loss_ce_3: 0.2894  loss_mask_3: 0.3517  loss_dice_3: 2.742  loss_ce_4: 0.2724  loss_mask_4: 0.3531  loss_dice_4: 2.742  loss_ce_5: 0.2836  loss_mask_5: 0.3522  loss_dice_5: 2.747  loss_ce_6: 0.2765  loss_mask_6: 0.3526  loss_dice_6: 2.734  loss_ce_7: 0.2674  loss_mask_7: 0.3527  loss_dice_7: 2.731  loss_ce_8: 0.2723  loss_mask_8: 0.352  loss_dice_8: 2.742  time: 1.4865  data_time: 0.0739  lr: 3.9526e-06  max_mem: 21589M
[01/18 06:38:40] d2.utils.events INFO:  eta: 5:45:10  iter: 25759  total_loss: 34.83  loss_ce: 0.2538  loss_mask: 0.3605  loss_dice: 2.773  loss_ce_0: 0.5665  loss_mask_0: 0.3583  loss_dice_0: 2.909  loss_ce_1: 0.2965  loss_mask_1: 0.3671  loss_dice_1: 2.817  loss_ce_2: 0.2931  loss_mask_2: 0.3646  loss_dice_2: 2.789  loss_ce_3: 0.2814  loss_mask_3: 0.363  loss_dice_3: 2.777  loss_ce_4: 0.2668  loss_mask_4: 0.3611  loss_dice_4: 2.773  loss_ce_5: 0.2599  loss_mask_5: 0.3604  loss_dice_5: 2.776  loss_ce_6: 0.2628  loss_mask_6: 0.3606  loss_dice_6: 2.773  loss_ce_7: 0.2568  loss_mask_7: 0.3599  loss_dice_7: 2.769  loss_ce_8: 0.2539  loss_mask_8: 0.3608  loss_dice_8: 2.776  time: 1.4864  data_time: 0.0727  lr: 3.9476e-06  max_mem: 21589M
[01/18 06:39:08] d2.utils.events INFO:  eta: 5:44:37  iter: 25779  total_loss: 34.39  loss_ce: 0.275  loss_mask: 0.352  loss_dice: 2.748  loss_ce_0: 0.5447  loss_mask_0: 0.3514  loss_dice_0: 2.874  loss_ce_1: 0.3046  loss_mask_1: 0.3608  loss_dice_1: 2.788  loss_ce_2: 0.2744  loss_mask_2: 0.3581  loss_dice_2: 2.767  loss_ce_3: 0.269  loss_mask_3: 0.3527  loss_dice_3: 2.767  loss_ce_4: 0.2717  loss_mask_4: 0.3536  loss_dice_4: 2.762  loss_ce_5: 0.2545  loss_mask_5: 0.3516  loss_dice_5: 2.761  loss_ce_6: 0.2437  loss_mask_6: 0.3518  loss_dice_6: 2.76  loss_ce_7: 0.2376  loss_mask_7: 0.3522  loss_dice_7: 2.76  loss_ce_8: 0.2558  loss_mask_8: 0.3534  loss_dice_8: 2.755  time: 1.4864  data_time: 0.0680  lr: 3.9426e-06  max_mem: 21589M
[01/18 06:39:37] d2.utils.events INFO:  eta: 5:44:19  iter: 25799  total_loss: 34.54  loss_ce: 0.2524  loss_mask: 0.3531  loss_dice: 2.762  loss_ce_0: 0.5978  loss_mask_0: 0.356  loss_dice_0: 2.891  loss_ce_1: 0.311  loss_mask_1: 0.364  loss_dice_1: 2.805  loss_ce_2: 0.29  loss_mask_2: 0.359  loss_dice_2: 2.779  loss_ce_3: 0.2859  loss_mask_3: 0.3543  loss_dice_3: 2.765  loss_ce_4: 0.2715  loss_mask_4: 0.3549  loss_dice_4: 2.766  loss_ce_5: 0.2536  loss_mask_5: 0.3542  loss_dice_5: 2.762  loss_ce_6: 0.2584  loss_mask_6: 0.3529  loss_dice_6: 2.767  loss_ce_7: 0.2574  loss_mask_7: 0.3524  loss_dice_7: 2.765  loss_ce_8: 0.2612  loss_mask_8: 0.3515  loss_dice_8: 2.756  time: 1.4863  data_time: 0.0715  lr: 3.9376e-06  max_mem: 21589M
[01/18 06:40:06] d2.utils.events INFO:  eta: 5:43:58  iter: 25819  total_loss: 34.07  loss_ce: 0.271  loss_mask: 0.3447  loss_dice: 2.719  loss_ce_0: 0.537  loss_mask_0: 0.342  loss_dice_0: 2.852  loss_ce_1: 0.315  loss_mask_1: 0.3509  loss_dice_1: 2.765  loss_ce_2: 0.3065  loss_mask_2: 0.3468  loss_dice_2: 2.743  loss_ce_3: 0.3024  loss_mask_3: 0.3447  loss_dice_3: 2.726  loss_ce_4: 0.2875  loss_mask_4: 0.3447  loss_dice_4: 2.723  loss_ce_5: 0.2719  loss_mask_5: 0.3446  loss_dice_5: 2.723  loss_ce_6: 0.2758  loss_mask_6: 0.3455  loss_dice_6: 2.717  loss_ce_7: 0.2768  loss_mask_7: 0.345  loss_dice_7: 2.716  loss_ce_8: 0.2738  loss_mask_8: 0.3448  loss_dice_8: 2.721  time: 1.4863  data_time: 0.0694  lr: 3.9326e-06  max_mem: 21589M
[01/18 06:40:35] d2.utils.events INFO:  eta: 5:43:29  iter: 25839  total_loss: 33.84  loss_ce: 0.2338  loss_mask: 0.3579  loss_dice: 2.695  loss_ce_0: 0.5524  loss_mask_0: 0.3539  loss_dice_0: 2.83  loss_ce_1: 0.3025  loss_mask_1: 0.3631  loss_dice_1: 2.747  loss_ce_2: 0.2883  loss_mask_2: 0.3619  loss_dice_2: 2.723  loss_ce_3: 0.2476  loss_mask_3: 0.3611  loss_dice_3: 2.708  loss_ce_4: 0.2537  loss_mask_4: 0.3613  loss_dice_4: 2.698  loss_ce_5: 0.2534  loss_mask_5: 0.3617  loss_dice_5: 2.695  loss_ce_6: 0.2347  loss_mask_6: 0.3613  loss_dice_6: 2.693  loss_ce_7: 0.2318  loss_mask_7: 0.3588  loss_dice_7: 2.694  loss_ce_8: 0.239  loss_mask_8: 0.3576  loss_dice_8: 2.697  time: 1.4863  data_time: 0.0654  lr: 3.9276e-06  max_mem: 21589M
[01/18 06:41:04] d2.utils.events INFO:  eta: 5:42:54  iter: 25859  total_loss: 34.35  loss_ce: 0.2476  loss_mask: 0.3584  loss_dice: 2.773  loss_ce_0: 0.5501  loss_mask_0: 0.3544  loss_dice_0: 2.896  loss_ce_1: 0.3113  loss_mask_1: 0.3612  loss_dice_1: 2.817  loss_ce_2: 0.2941  loss_mask_2: 0.358  loss_dice_2: 2.792  loss_ce_3: 0.2734  loss_mask_3: 0.3587  loss_dice_3: 2.784  loss_ce_4: 0.2666  loss_mask_4: 0.3604  loss_dice_4: 2.785  loss_ce_5: 0.2602  loss_mask_5: 0.3593  loss_dice_5: 2.79  loss_ce_6: 0.2469  loss_mask_6: 0.3592  loss_dice_6: 2.777  loss_ce_7: 0.2529  loss_mask_7: 0.3598  loss_dice_7: 2.779  loss_ce_8: 0.2617  loss_mask_8: 0.3599  loss_dice_8: 2.782  time: 1.4863  data_time: 0.0726  lr: 3.9226e-06  max_mem: 21589M
[01/18 06:41:33] d2.utils.events INFO:  eta: 5:42:27  iter: 25879  total_loss: 34.58  loss_ce: 0.2688  loss_mask: 0.3533  loss_dice: 2.742  loss_ce_0: 0.5629  loss_mask_0: 0.3459  loss_dice_0: 2.878  loss_ce_1: 0.3143  loss_mask_1: 0.3602  loss_dice_1: 2.786  loss_ce_2: 0.2934  loss_mask_2: 0.3573  loss_dice_2: 2.77  loss_ce_3: 0.298  loss_mask_3: 0.3543  loss_dice_3: 2.754  loss_ce_4: 0.2831  loss_mask_4: 0.3539  loss_dice_4: 2.74  loss_ce_5: 0.2696  loss_mask_5: 0.3518  loss_dice_5: 2.749  loss_ce_6: 0.2662  loss_mask_6: 0.3522  loss_dice_6: 2.742  loss_ce_7: 0.2735  loss_mask_7: 0.3527  loss_dice_7: 2.743  loss_ce_8: 0.2703  loss_mask_8: 0.3532  loss_dice_8: 2.747  time: 1.4862  data_time: 0.0684  lr: 3.9176e-06  max_mem: 21589M
[01/18 06:42:02] d2.utils.events INFO:  eta: 5:41:48  iter: 25899  total_loss: 33.78  loss_ce: 0.2618  loss_mask: 0.3535  loss_dice: 2.686  loss_ce_0: 0.5389  loss_mask_0: 0.3501  loss_dice_0: 2.813  loss_ce_1: 0.2806  loss_mask_1: 0.3605  loss_dice_1: 2.738  loss_ce_2: 0.2861  loss_mask_2: 0.3557  loss_dice_2: 2.703  loss_ce_3: 0.265  loss_mask_3: 0.3546  loss_dice_3: 2.687  loss_ce_4: 0.2627  loss_mask_4: 0.3548  loss_dice_4: 2.689  loss_ce_5: 0.2579  loss_mask_5: 0.3557  loss_dice_5: 2.68  loss_ce_6: 0.2504  loss_mask_6: 0.3555  loss_dice_6: 2.686  loss_ce_7: 0.2504  loss_mask_7: 0.3572  loss_dice_7: 2.691  loss_ce_8: 0.2524  loss_mask_8: 0.3545  loss_dice_8: 2.684  time: 1.4862  data_time: 0.0684  lr: 3.9126e-06  max_mem: 21589M
[01/18 06:42:31] d2.utils.events INFO:  eta: 5:41:23  iter: 25919  total_loss: 34.66  loss_ce: 0.2556  loss_mask: 0.3507  loss_dice: 2.743  loss_ce_0: 0.559  loss_mask_0: 0.3527  loss_dice_0: 2.88  loss_ce_1: 0.3032  loss_mask_1: 0.3576  loss_dice_1: 2.771  loss_ce_2: 0.2899  loss_mask_2: 0.3544  loss_dice_2: 2.759  loss_ce_3: 0.2759  loss_mask_3: 0.3531  loss_dice_3: 2.746  loss_ce_4: 0.2732  loss_mask_4: 0.3516  loss_dice_4: 2.735  loss_ce_5: 0.2598  loss_mask_5: 0.3516  loss_dice_5: 2.743  loss_ce_6: 0.2596  loss_mask_6: 0.3503  loss_dice_6: 2.734  loss_ce_7: 0.2605  loss_mask_7: 0.3503  loss_dice_7: 2.737  loss_ce_8: 0.2524  loss_mask_8: 0.3512  loss_dice_8: 2.732  time: 1.4862  data_time: 0.0731  lr: 3.9077e-06  max_mem: 21589M
[01/18 06:43:00] d2.utils.events INFO:  eta: 5:40:55  iter: 25939  total_loss: 34.39  loss_ce: 0.28  loss_mask: 0.3586  loss_dice: 2.756  loss_ce_0: 0.5939  loss_mask_0: 0.3599  loss_dice_0: 2.885  loss_ce_1: 0.3191  loss_mask_1: 0.363  loss_dice_1: 2.804  loss_ce_2: 0.3022  loss_mask_2: 0.36  loss_dice_2: 2.786  loss_ce_3: 0.2797  loss_mask_3: 0.3581  loss_dice_3: 2.765  loss_ce_4: 0.2817  loss_mask_4: 0.3584  loss_dice_4: 2.765  loss_ce_5: 0.275  loss_mask_5: 0.3593  loss_dice_5: 2.767  loss_ce_6: 0.2828  loss_mask_6: 0.3572  loss_dice_6: 2.762  loss_ce_7: 0.2816  loss_mask_7: 0.3576  loss_dice_7: 2.759  loss_ce_8: 0.2648  loss_mask_8: 0.3589  loss_dice_8: 2.754  time: 1.4861  data_time: 0.0720  lr: 3.9027e-06  max_mem: 21589M
[01/18 06:43:29] d2.utils.events INFO:  eta: 5:40:19  iter: 25959  total_loss: 34.18  loss_ce: 0.2573  loss_mask: 0.3562  loss_dice: 2.729  loss_ce_0: 0.5338  loss_mask_0: 0.3573  loss_dice_0: 2.866  loss_ce_1: 0.2889  loss_mask_1: 0.3619  loss_dice_1: 2.77  loss_ce_2: 0.2782  loss_mask_2: 0.3572  loss_dice_2: 2.755  loss_ce_3: 0.2631  loss_mask_3: 0.3581  loss_dice_3: 2.735  loss_ce_4: 0.2486  loss_mask_4: 0.3573  loss_dice_4: 2.74  loss_ce_5: 0.2479  loss_mask_5: 0.3574  loss_dice_5: 2.728  loss_ce_6: 0.2475  loss_mask_6: 0.3567  loss_dice_6: 2.732  loss_ce_7: 0.2551  loss_mask_7: 0.3568  loss_dice_7: 2.733  loss_ce_8: 0.2493  loss_mask_8: 0.3558  loss_dice_8: 2.731  time: 1.4861  data_time: 0.0673  lr: 3.8977e-06  max_mem: 21589M
[01/18 06:43:58] d2.utils.events INFO:  eta: 5:39:51  iter: 25979  total_loss: 33.86  loss_ce: 0.2461  loss_mask: 0.3521  loss_dice: 2.715  loss_ce_0: 0.5279  loss_mask_0: 0.3563  loss_dice_0: 2.861  loss_ce_1: 0.2881  loss_mask_1: 0.3569  loss_dice_1: 2.765  loss_ce_2: 0.2842  loss_mask_2: 0.3555  loss_dice_2: 2.74  loss_ce_3: 0.2578  loss_mask_3: 0.3552  loss_dice_3: 2.726  loss_ce_4: 0.2578  loss_mask_4: 0.3563  loss_dice_4: 2.723  loss_ce_5: 0.2456  loss_mask_5: 0.3561  loss_dice_5: 2.726  loss_ce_6: 0.2565  loss_mask_6: 0.355  loss_dice_6: 2.72  loss_ce_7: 0.2421  loss_mask_7: 0.3542  loss_dice_7: 2.724  loss_ce_8: 0.2397  loss_mask_8: 0.3542  loss_dice_8: 2.712  time: 1.4861  data_time: 0.0667  lr: 3.8927e-06  max_mem: 21589M
[01/18 06:44:27] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 06:44:28] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 06:44:28] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 06:44:28] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 06:44:43] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0074 s/iter. Inference: 0.1529 s/iter. Eval: 0.2046 s/iter. Total: 0.3649 s/iter. ETA=0:06:34
[01/18 06:44:48] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0102 s/iter. Inference: 0.1573 s/iter. Eval: 0.2257 s/iter. Total: 0.3933 s/iter. ETA=0:07:00
[01/18 06:44:53] d2.evaluation.evaluator INFO: Inference done 37/1093. Dataloading: 0.0105 s/iter. Inference: 0.1596 s/iter. Eval: 0.2202 s/iter. Total: 0.3904 s/iter. ETA=0:06:52
[01/18 06:44:59] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0113 s/iter. Inference: 0.1614 s/iter. Eval: 0.2248 s/iter. Total: 0.3976 s/iter. ETA=0:06:54
[01/18 06:45:04] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0117 s/iter. Inference: 0.1611 s/iter. Eval: 0.2333 s/iter. Total: 0.4061 s/iter. ETA=0:06:58
[01/18 06:45:09] d2.evaluation.evaluator INFO: Inference done 76/1093. Dataloading: 0.0117 s/iter. Inference: 0.1588 s/iter. Eval: 0.2269 s/iter. Total: 0.3974 s/iter. ETA=0:06:44
[01/18 06:45:14] d2.evaluation.evaluator INFO: Inference done 90/1093. Dataloading: 0.0117 s/iter. Inference: 0.1569 s/iter. Eval: 0.2255 s/iter. Total: 0.3941 s/iter. ETA=0:06:35
[01/18 06:45:19] d2.evaluation.evaluator INFO: Inference done 104/1093. Dataloading: 0.0115 s/iter. Inference: 0.1566 s/iter. Eval: 0.2221 s/iter. Total: 0.3903 s/iter. ETA=0:06:25
[01/18 06:45:25] d2.evaluation.evaluator INFO: Inference done 116/1093. Dataloading: 0.0116 s/iter. Inference: 0.1590 s/iter. Eval: 0.2234 s/iter. Total: 0.3942 s/iter. ETA=0:06:25
[01/18 06:45:30] d2.evaluation.evaluator INFO: Inference done 129/1093. Dataloading: 0.0117 s/iter. Inference: 0.1585 s/iter. Eval: 0.2249 s/iter. Total: 0.3951 s/iter. ETA=0:06:20
[01/18 06:45:35] d2.evaluation.evaluator INFO: Inference done 144/1093. Dataloading: 0.0115 s/iter. Inference: 0.1584 s/iter. Eval: 0.2212 s/iter. Total: 0.3912 s/iter. ETA=0:06:11
[01/18 06:45:40] d2.evaluation.evaluator INFO: Inference done 158/1093. Dataloading: 0.0115 s/iter. Inference: 0.1591 s/iter. Eval: 0.2188 s/iter. Total: 0.3894 s/iter. ETA=0:06:04
[01/18 06:45:46] d2.evaluation.evaluator INFO: Inference done 171/1093. Dataloading: 0.0116 s/iter. Inference: 0.1597 s/iter. Eval: 0.2202 s/iter. Total: 0.3916 s/iter. ETA=0:06:01
[01/18 06:45:51] d2.evaluation.evaluator INFO: Inference done 185/1093. Dataloading: 0.0115 s/iter. Inference: 0.1593 s/iter. Eval: 0.2183 s/iter. Total: 0.3893 s/iter. ETA=0:05:53
[01/18 06:45:56] d2.evaluation.evaluator INFO: Inference done 200/1093. Dataloading: 0.0114 s/iter. Inference: 0.1588 s/iter. Eval: 0.2161 s/iter. Total: 0.3865 s/iter. ETA=0:05:45
[01/18 06:46:01] d2.evaluation.evaluator INFO: Inference done 213/1093. Dataloading: 0.0115 s/iter. Inference: 0.1585 s/iter. Eval: 0.2173 s/iter. Total: 0.3874 s/iter. ETA=0:05:40
[01/18 06:46:07] d2.evaluation.evaluator INFO: Inference done 228/1093. Dataloading: 0.0115 s/iter. Inference: 0.1582 s/iter. Eval: 0.2149 s/iter. Total: 0.3846 s/iter. ETA=0:05:32
[01/18 06:46:12] d2.evaluation.evaluator INFO: Inference done 240/1093. Dataloading: 0.0117 s/iter. Inference: 0.1585 s/iter. Eval: 0.2163 s/iter. Total: 0.3866 s/iter. ETA=0:05:29
[01/18 06:46:17] d2.evaluation.evaluator INFO: Inference done 253/1093. Dataloading: 0.0118 s/iter. Inference: 0.1588 s/iter. Eval: 0.2170 s/iter. Total: 0.3877 s/iter. ETA=0:05:25
[01/18 06:46:22] d2.evaluation.evaluator INFO: Inference done 266/1093. Dataloading: 0.0118 s/iter. Inference: 0.1595 s/iter. Eval: 0.2167 s/iter. Total: 0.3880 s/iter. ETA=0:05:20
[01/18 06:46:27] d2.evaluation.evaluator INFO: Inference done 281/1093. Dataloading: 0.0117 s/iter. Inference: 0.1585 s/iter. Eval: 0.2155 s/iter. Total: 0.3858 s/iter. ETA=0:05:13
[01/18 06:46:32] d2.evaluation.evaluator INFO: Inference done 295/1093. Dataloading: 0.0117 s/iter. Inference: 0.1583 s/iter. Eval: 0.2147 s/iter. Total: 0.3848 s/iter. ETA=0:05:07
[01/18 06:46:37] d2.evaluation.evaluator INFO: Inference done 306/1093. Dataloading: 0.0118 s/iter. Inference: 0.1586 s/iter. Eval: 0.2172 s/iter. Total: 0.3876 s/iter. ETA=0:05:05
[01/18 06:46:43] d2.evaluation.evaluator INFO: Inference done 317/1093. Dataloading: 0.0119 s/iter. Inference: 0.1591 s/iter. Eval: 0.2191 s/iter. Total: 0.3901 s/iter. ETA=0:05:02
[01/18 06:46:48] d2.evaluation.evaluator INFO: Inference done 331/1093. Dataloading: 0.0119 s/iter. Inference: 0.1591 s/iter. Eval: 0.2184 s/iter. Total: 0.3895 s/iter. ETA=0:04:56
[01/18 06:46:53] d2.evaluation.evaluator INFO: Inference done 347/1093. Dataloading: 0.0118 s/iter. Inference: 0.1587 s/iter. Eval: 0.2160 s/iter. Total: 0.3866 s/iter. ETA=0:04:48
[01/18 06:46:58] d2.evaluation.evaluator INFO: Inference done 361/1093. Dataloading: 0.0118 s/iter. Inference: 0.1584 s/iter. Eval: 0.2153 s/iter. Total: 0.3856 s/iter. ETA=0:04:42
[01/18 06:47:03] d2.evaluation.evaluator INFO: Inference done 376/1093. Dataloading: 0.0118 s/iter. Inference: 0.1581 s/iter. Eval: 0.2140 s/iter. Total: 0.3840 s/iter. ETA=0:04:35
[01/18 06:47:08] d2.evaluation.evaluator INFO: Inference done 390/1093. Dataloading: 0.0118 s/iter. Inference: 0.1576 s/iter. Eval: 0.2137 s/iter. Total: 0.3831 s/iter. ETA=0:04:29
[01/18 06:47:13] d2.evaluation.evaluator INFO: Inference done 404/1093. Dataloading: 0.0118 s/iter. Inference: 0.1571 s/iter. Eval: 0.2135 s/iter. Total: 0.3824 s/iter. ETA=0:04:23
[01/18 06:47:18] d2.evaluation.evaluator INFO: Inference done 418/1093. Dataloading: 0.0118 s/iter. Inference: 0.1565 s/iter. Eval: 0.2132 s/iter. Total: 0.3816 s/iter. ETA=0:04:17
[01/18 06:47:23] d2.evaluation.evaluator INFO: Inference done 429/1093. Dataloading: 0.0119 s/iter. Inference: 0.1567 s/iter. Eval: 0.2149 s/iter. Total: 0.3836 s/iter. ETA=0:04:14
[01/18 06:47:29] d2.evaluation.evaluator INFO: Inference done 444/1093. Dataloading: 0.0118 s/iter. Inference: 0.1568 s/iter. Eval: 0.2138 s/iter. Total: 0.3825 s/iter. ETA=0:04:08
[01/18 06:47:34] d2.evaluation.evaluator INFO: Inference done 458/1093. Dataloading: 0.0118 s/iter. Inference: 0.1571 s/iter. Eval: 0.2132 s/iter. Total: 0.3822 s/iter. ETA=0:04:02
[01/18 06:47:39] d2.evaluation.evaluator INFO: Inference done 475/1093. Dataloading: 0.0117 s/iter. Inference: 0.1566 s/iter. Eval: 0.2110 s/iter. Total: 0.3793 s/iter. ETA=0:03:54
[01/18 06:47:44] d2.evaluation.evaluator INFO: Inference done 490/1093. Dataloading: 0.0117 s/iter. Inference: 0.1565 s/iter. Eval: 0.2101 s/iter. Total: 0.3784 s/iter. ETA=0:03:48
[01/18 06:47:49] d2.evaluation.evaluator INFO: Inference done 506/1093. Dataloading: 0.0116 s/iter. Inference: 0.1560 s/iter. Eval: 0.2086 s/iter. Total: 0.3763 s/iter. ETA=0:03:40
[01/18 06:47:55] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0116 s/iter. Inference: 0.1558 s/iter. Eval: 0.2081 s/iter. Total: 0.3755 s/iter. ETA=0:03:34
[01/18 06:48:00] d2.evaluation.evaluator INFO: Inference done 534/1093. Dataloading: 0.0116 s/iter. Inference: 0.1557 s/iter. Eval: 0.2088 s/iter. Total: 0.3762 s/iter. ETA=0:03:30
[01/18 06:48:05] d2.evaluation.evaluator INFO: Inference done 548/1093. Dataloading: 0.0116 s/iter. Inference: 0.1557 s/iter. Eval: 0.2087 s/iter. Total: 0.3761 s/iter. ETA=0:03:24
[01/18 06:48:10] d2.evaluation.evaluator INFO: Inference done 562/1093. Dataloading: 0.0116 s/iter. Inference: 0.1556 s/iter. Eval: 0.2086 s/iter. Total: 0.3758 s/iter. ETA=0:03:19
[01/18 06:48:15] d2.evaluation.evaluator INFO: Inference done 577/1093. Dataloading: 0.0116 s/iter. Inference: 0.1560 s/iter. Eval: 0.2073 s/iter. Total: 0.3750 s/iter. ETA=0:03:13
[01/18 06:48:21] d2.evaluation.evaluator INFO: Inference done 592/1093. Dataloading: 0.0116 s/iter. Inference: 0.1560 s/iter. Eval: 0.2069 s/iter. Total: 0.3746 s/iter. ETA=0:03:07
[01/18 06:48:26] d2.evaluation.evaluator INFO: Inference done 605/1093. Dataloading: 0.0117 s/iter. Inference: 0.1559 s/iter. Eval: 0.2077 s/iter. Total: 0.3754 s/iter. ETA=0:03:03
[01/18 06:48:31] d2.evaluation.evaluator INFO: Inference done 619/1093. Dataloading: 0.0117 s/iter. Inference: 0.1558 s/iter. Eval: 0.2074 s/iter. Total: 0.3751 s/iter. ETA=0:02:57
[01/18 06:48:36] d2.evaluation.evaluator INFO: Inference done 632/1093. Dataloading: 0.0117 s/iter. Inference: 0.1559 s/iter. Eval: 0.2080 s/iter. Total: 0.3757 s/iter. ETA=0:02:53
[01/18 06:48:42] d2.evaluation.evaluator INFO: Inference done 647/1093. Dataloading: 0.0117 s/iter. Inference: 0.1558 s/iter. Eval: 0.2074 s/iter. Total: 0.3750 s/iter. ETA=0:02:47
[01/18 06:48:47] d2.evaluation.evaluator INFO: Inference done 661/1093. Dataloading: 0.0117 s/iter. Inference: 0.1560 s/iter. Eval: 0.2073 s/iter. Total: 0.3751 s/iter. ETA=0:02:42
[01/18 06:48:52] d2.evaluation.evaluator INFO: Inference done 676/1093. Dataloading: 0.0117 s/iter. Inference: 0.1560 s/iter. Eval: 0.2068 s/iter. Total: 0.3746 s/iter. ETA=0:02:36
[01/18 06:48:57] d2.evaluation.evaluator INFO: Inference done 692/1093. Dataloading: 0.0116 s/iter. Inference: 0.1559 s/iter. Eval: 0.2057 s/iter. Total: 0.3734 s/iter. ETA=0:02:29
[01/18 06:49:03] d2.evaluation.evaluator INFO: Inference done 705/1093. Dataloading: 0.0116 s/iter. Inference: 0.1561 s/iter. Eval: 0.2063 s/iter. Total: 0.3741 s/iter. ETA=0:02:25
[01/18 06:49:08] d2.evaluation.evaluator INFO: Inference done 717/1093. Dataloading: 0.0117 s/iter. Inference: 0.1561 s/iter. Eval: 0.2070 s/iter. Total: 0.3750 s/iter. ETA=0:02:20
[01/18 06:49:13] d2.evaluation.evaluator INFO: Inference done 732/1093. Dataloading: 0.0116 s/iter. Inference: 0.1563 s/iter. Eval: 0.2062 s/iter. Total: 0.3742 s/iter. ETA=0:02:15
[01/18 06:49:18] d2.evaluation.evaluator INFO: Inference done 747/1093. Dataloading: 0.0116 s/iter. Inference: 0.1561 s/iter. Eval: 0.2057 s/iter. Total: 0.3736 s/iter. ETA=0:02:09
[01/18 06:49:23] d2.evaluation.evaluator INFO: Inference done 762/1093. Dataloading: 0.0116 s/iter. Inference: 0.1561 s/iter. Eval: 0.2056 s/iter. Total: 0.3734 s/iter. ETA=0:02:03
[01/18 06:49:29] d2.evaluation.evaluator INFO: Inference done 775/1093. Dataloading: 0.0116 s/iter. Inference: 0.1561 s/iter. Eval: 0.2059 s/iter. Total: 0.3737 s/iter. ETA=0:01:58
[01/18 06:49:34] d2.evaluation.evaluator INFO: Inference done 790/1093. Dataloading: 0.0116 s/iter. Inference: 0.1560 s/iter. Eval: 0.2054 s/iter. Total: 0.3730 s/iter. ETA=0:01:53
[01/18 06:49:39] d2.evaluation.evaluator INFO: Inference done 805/1093. Dataloading: 0.0115 s/iter. Inference: 0.1560 s/iter. Eval: 0.2049 s/iter. Total: 0.3725 s/iter. ETA=0:01:47
[01/18 06:49:44] d2.evaluation.evaluator INFO: Inference done 820/1093. Dataloading: 0.0115 s/iter. Inference: 0.1560 s/iter. Eval: 0.2046 s/iter. Total: 0.3722 s/iter. ETA=0:01:41
[01/18 06:49:49] d2.evaluation.evaluator INFO: Inference done 836/1093. Dataloading: 0.0115 s/iter. Inference: 0.1560 s/iter. Eval: 0.2038 s/iter. Total: 0.3713 s/iter. ETA=0:01:35
[01/18 06:49:55] d2.evaluation.evaluator INFO: Inference done 852/1093. Dataloading: 0.0115 s/iter. Inference: 0.1558 s/iter. Eval: 0.2035 s/iter. Total: 0.3708 s/iter. ETA=0:01:29
[01/18 06:50:00] d2.evaluation.evaluator INFO: Inference done 865/1093. Dataloading: 0.0115 s/iter. Inference: 0.1557 s/iter. Eval: 0.2041 s/iter. Total: 0.3714 s/iter. ETA=0:01:24
[01/18 06:50:05] d2.evaluation.evaluator INFO: Inference done 879/1093. Dataloading: 0.0115 s/iter. Inference: 0.1557 s/iter. Eval: 0.2040 s/iter. Total: 0.3714 s/iter. ETA=0:01:19
[01/18 06:50:11] d2.evaluation.evaluator INFO: Inference done 891/1093. Dataloading: 0.0116 s/iter. Inference: 0.1558 s/iter. Eval: 0.2049 s/iter. Total: 0.3724 s/iter. ETA=0:01:15
[01/18 06:50:16] d2.evaluation.evaluator INFO: Inference done 907/1093. Dataloading: 0.0116 s/iter. Inference: 0.1556 s/iter. Eval: 0.2044 s/iter. Total: 0.3716 s/iter. ETA=0:01:09
[01/18 06:50:21] d2.evaluation.evaluator INFO: Inference done 919/1093. Dataloading: 0.0116 s/iter. Inference: 0.1558 s/iter. Eval: 0.2049 s/iter. Total: 0.3724 s/iter. ETA=0:01:04
[01/18 06:50:26] d2.evaluation.evaluator INFO: Inference done 934/1093. Dataloading: 0.0115 s/iter. Inference: 0.1557 s/iter. Eval: 0.2045 s/iter. Total: 0.3719 s/iter. ETA=0:00:59
[01/18 06:50:32] d2.evaluation.evaluator INFO: Inference done 948/1093. Dataloading: 0.0116 s/iter. Inference: 0.1558 s/iter. Eval: 0.2048 s/iter. Total: 0.3722 s/iter. ETA=0:00:53
[01/18 06:50:37] d2.evaluation.evaluator INFO: Inference done 962/1093. Dataloading: 0.0115 s/iter. Inference: 0.1557 s/iter. Eval: 0.2048 s/iter. Total: 0.3721 s/iter. ETA=0:00:48
[01/18 06:50:42] d2.evaluation.evaluator INFO: Inference done 976/1093. Dataloading: 0.0115 s/iter. Inference: 0.1556 s/iter. Eval: 0.2047 s/iter. Total: 0.3720 s/iter. ETA=0:00:43
[01/18 06:50:47] d2.evaluation.evaluator INFO: Inference done 991/1093. Dataloading: 0.0115 s/iter. Inference: 0.1557 s/iter. Eval: 0.2042 s/iter. Total: 0.3715 s/iter. ETA=0:00:37
[01/18 06:50:52] d2.evaluation.evaluator INFO: Inference done 1005/1093. Dataloading: 0.0115 s/iter. Inference: 0.1556 s/iter. Eval: 0.2043 s/iter. Total: 0.3715 s/iter. ETA=0:00:32
[01/18 06:50:57] d2.evaluation.evaluator INFO: Inference done 1019/1093. Dataloading: 0.0115 s/iter. Inference: 0.1556 s/iter. Eval: 0.2042 s/iter. Total: 0.3714 s/iter. ETA=0:00:27
[01/18 06:51:03] d2.evaluation.evaluator INFO: Inference done 1034/1093. Dataloading: 0.0115 s/iter. Inference: 0.1555 s/iter. Eval: 0.2041 s/iter. Total: 0.3712 s/iter. ETA=0:00:21
[01/18 06:51:08] d2.evaluation.evaluator INFO: Inference done 1048/1093. Dataloading: 0.0115 s/iter. Inference: 0.1554 s/iter. Eval: 0.2040 s/iter. Total: 0.3710 s/iter. ETA=0:00:16
[01/18 06:51:13] d2.evaluation.evaluator INFO: Inference done 1064/1093. Dataloading: 0.0114 s/iter. Inference: 0.1552 s/iter. Eval: 0.2034 s/iter. Total: 0.3701 s/iter. ETA=0:00:10
[01/18 06:51:18] d2.evaluation.evaluator INFO: Inference done 1079/1093. Dataloading: 0.0114 s/iter. Inference: 0.1554 s/iter. Eval: 0.2027 s/iter. Total: 0.3696 s/iter. ETA=0:00:05
[01/18 06:51:23] d2.evaluation.evaluator INFO: Inference done 1092/1093. Dataloading: 0.0114 s/iter. Inference: 0.1558 s/iter. Eval: 0.2028 s/iter. Total: 0.3701 s/iter. ETA=0:00:00
[01/18 06:51:24] d2.evaluation.evaluator INFO: Total inference time: 0:06:42.984048 (0.370390 s / iter per device, on 4 devices)
[01/18 06:51:24] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:49 (0.155775 s / iter per device, on 4 devices)
[01/18 06:51:49] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.378503590590393, 'mIoU': 19.42339536314705, 'fwIoU': 41.80144282846937, 'IoU-0': nan, 'IoU-1': 95.0205349397033, 'IoU-2': 47.248514250826915, 'IoU-3': 58.9057194722967, 'IoU-4': 53.302284672006465, 'IoU-5': 46.42561612225984, 'IoU-6': 40.8279036880093, 'IoU-7': 32.82466801710556, 'IoU-8': 20.73542702727252, 'IoU-9': 29.728498990794073, 'IoU-10': 36.31772405370741, 'IoU-11': 47.21546542381069, 'IoU-12': 49.506265242135875, 'IoU-13': 48.681655374851864, 'IoU-14': 48.760828075232176, 'IoU-15': 49.11207764787528, 'IoU-16': 49.010928206697294, 'IoU-17': 46.21721851504429, 'IoU-18': 45.855330704226624, 'IoU-19': 46.97216547012762, 'IoU-20': 46.0159918592958, 'IoU-21': 45.82487581333453, 'IoU-22': 47.686395006134866, 'IoU-23': 45.68329807494331, 'IoU-24': 45.74023090821775, 'IoU-25': 44.71105559269009, 'IoU-26': 43.30630976447725, 'IoU-27': 45.218280649906994, 'IoU-28': 43.03620339752211, 'IoU-29': 43.98897409416502, 'IoU-30': 43.050775376944, 'IoU-31': 44.669990636598996, 'IoU-32': 44.1241243296333, 'IoU-33': 41.95729547939165, 'IoU-34': 41.301071435901235, 'IoU-35': 42.395649310253546, 'IoU-36': 41.17945920727391, 'IoU-37': 39.68299266393957, 'IoU-38': 40.16448522107533, 'IoU-39': 39.4800833475949, 'IoU-40': 39.79004142814451, 'IoU-41': 38.948626546365865, 'IoU-42': 38.66296966454277, 'IoU-43': 37.937084686431554, 'IoU-44': 37.3860157071175, 'IoU-45': 36.41766045265265, 'IoU-46': 34.72219236177102, 'IoU-47': 34.95870424944583, 'IoU-48': 35.1230088505283, 'IoU-49': 34.515215264989195, 'IoU-50': 34.48932818647665, 'IoU-51': 33.388820599743, 'IoU-52': 32.86776082823805, 'IoU-53': 32.52766372832977, 'IoU-54': 31.962980876868784, 'IoU-55': 30.385912200706635, 'IoU-56': 29.200027286849274, 'IoU-57': 28.31733453185433, 'IoU-58': 26.459978864366278, 'IoU-59': 25.367747514317706, 'IoU-60': 25.06478366167831, 'IoU-61': 25.49536398333256, 'IoU-62': 24.74711673884977, 'IoU-63': 25.61877159204304, 'IoU-64': 24.126384361451677, 'IoU-65': 21.934478141775536, 'IoU-66': 22.263233742607937, 'IoU-67': 22.076920471533494, 'IoU-68': 21.283457776975904, 'IoU-69': 20.810987690580383, 'IoU-70': 20.521311294704017, 'IoU-71': 19.208279247150934, 'IoU-72': 20.595168393104196, 'IoU-73': 20.330560314101344, 'IoU-74': 19.398441577843727, 'IoU-75': 19.7801702454185, 'IoU-76': 19.58505126539365, 'IoU-77': 19.093205295856407, 'IoU-78': 18.593471484581393, 'IoU-79': 17.448207389802242, 'IoU-80': 18.23383321669791, 'IoU-81': 17.721130628105392, 'IoU-82': 17.24319741002632, 'IoU-83': 17.817716642546664, 'IoU-84': 16.433288365558806, 'IoU-85': 16.636008889056633, 'IoU-86': 16.13333553640238, 'IoU-87': 16.23013895798949, 'IoU-88': 15.270219801507912, 'IoU-89': 14.783012114482469, 'IoU-90': 15.57341497536874, 'IoU-91': 15.180609465968645, 'IoU-92': 14.474600054149764, 'IoU-93': 15.491668161857108, 'IoU-94': 16.057253183924157, 'IoU-95': 16.459843532416194, 'IoU-96': 15.109475586254916, 'IoU-97': 16.070007826570144, 'IoU-98': 15.021064528754579, 'IoU-99': 14.25399439953074, 'IoU-100': 13.494950217090368, 'IoU-101': 13.343983657927646, 'IoU-102': 12.848221658336564, 'IoU-103': 12.270533843194507, 'IoU-104': 12.709652503758019, 'IoU-105': 12.975610275088151, 'IoU-106': 12.340372758895006, 'IoU-107': 12.086535677759743, 'IoU-108': 13.455043777093497, 'IoU-109': 13.555242911127424, 'IoU-110': 12.542847065098634, 'IoU-111': 11.101225589399563, 'IoU-112': 11.441828620985477, 'IoU-113': 10.048170535696297, 'IoU-114': 12.269365209433095, 'IoU-115': 10.371907661221826, 'IoU-116': 11.195055063552122, 'IoU-117': 10.371004299397221, 'IoU-118': 10.240742562044675, 'IoU-119': 12.112019787303147, 'IoU-120': 10.517274576108015, 'IoU-121': 9.71299819199571, 'IoU-122': 10.411335464041887, 'IoU-123': 8.329394714132613, 'IoU-124': 8.321233894175489, 'IoU-125': 8.005530908378654, 'IoU-126': 9.66619665788799, 'IoU-127': 7.089145859419116, 'IoU-128': 9.337437237088164, 'IoU-129': 7.552858059231042, 'IoU-130': 7.212089337681007, 'IoU-131': 8.682848092155684, 'IoU-132': 6.47263413954457, 'IoU-133': 7.889979632844786, 'IoU-134': 5.796447897877064, 'IoU-135': 5.606435670861488, 'IoU-136': 5.862767170311231, 'IoU-137': 4.282680796157037, 'IoU-138': 7.447828286062045, 'IoU-139': 5.139179199102429, 'IoU-140': 4.591510209333202, 'IoU-141': 5.975142219099456, 'IoU-142': 7.020628348132746, 'IoU-143': 4.13018552106877, 'IoU-144': 5.663680310884939, 'IoU-145': 3.677226827900019, 'IoU-146': 5.196984891203716, 'IoU-147': 4.752835899558834, 'IoU-148': 3.0950543548664324, 'IoU-149': 3.826813633405843, 'IoU-150': 4.2991760905979755, 'IoU-151': 2.1072330427364805, 'IoU-152': 2.8702112134612467, 'IoU-153': 2.7029022607193482, 'IoU-154': 2.4401377914819813, 'IoU-155': 2.8297107194416715, 'IoU-156': 2.4485748518317374, 'IoU-157': 2.543018466153849, 'IoU-158': 2.2133118676492383, 'IoU-159': 2.2036735519317663, 'IoU-160': 1.720596357417251, 'IoU-161': 3.8383781933509837, 'IoU-162': 2.5584671835165973, 'IoU-163': 1.875234503398868, 'IoU-164': 1.9061155485766028, 'IoU-165': 1.7683731522444617, 'IoU-166': 2.0421040608108356, 'IoU-167': 1.8672598099991788, 'IoU-168': 1.7111098874694646, 'IoU-169': 2.173564411246313, 'IoU-170': 2.170531265839297, 'IoU-171': 1.08918942052555, 'IoU-172': 1.9156813239798316, 'IoU-173': 0.9984302359029431, 'IoU-174': 1.5374608089297097, 'IoU-175': 1.6097512426160212, 'IoU-176': 2.1476484742812807, 'IoU-177': 1.850764508475709, 'IoU-178': 1.3680563289341474, 'IoU-179': 1.3340943302696384, 'IoU-180': 1.564768843897092, 'IoU-181': 2.9217547339929943, 'IoU-182': 0.6717082731178113, 'IoU-183': 1.7012234385061171, 'IoU-184': 0.6977979492351656, 'IoU-185': 3.1139945534977143, 'IoU-186': 1.8486773937953314, 'IoU-187': 0.6757566220290974, 'IoU-188': 1.4786816237445233, 'IoU-189': 1.381403345060802, 'IoU-190': 1.2980226350005664, 'IoU-191': 2.848672531534757, 'mACC': 29.391662687879705, 'pACC': 56.119689223581496, 'ACC-0': nan, 'ACC-1': 98.08408791636373, 'ACC-2': 72.34629971481652, 'ACC-3': 72.62224697343275, 'ACC-4': 69.39953361573853, 'ACC-5': 61.97985300871311, 'ACC-6': 56.56296510915999, 'ACC-7': 46.530368166387724, 'ACC-8': 25.34864146698691, 'ACC-9': 37.67697555653983, 'ACC-10': 49.16091148596501, 'ACC-11': 62.46157544783023, 'ACC-12': 69.28834393316109, 'ACC-13': 68.3561069231266, 'ACC-14': 64.97100167487835, 'ACC-15': 71.0454231670879, 'ACC-16': 62.65352767361219, 'ACC-17': 62.856332952825646, 'ACC-18': 63.099249621423034, 'ACC-19': 65.24895948776569, 'ACC-20': 65.29895288525586, 'ACC-21': 64.05911237395914, 'ACC-22': 63.78056588572299, 'ACC-23': 62.57463340282232, 'ACC-24': 62.62413755503891, 'ACC-25': 62.4644806306503, 'ACC-26': 60.30967337781755, 'ACC-27': 62.932910072243665, 'ACC-28': 62.007437603318664, 'ACC-29': 58.66195608360463, 'ACC-30': 58.55068420712616, 'ACC-31': 60.444777908334515, 'ACC-32': 61.20089730675339, 'ACC-33': 59.302315317891065, 'ACC-34': 61.021723675274394, 'ACC-35': 59.62795555765399, 'ACC-36': 58.484062510481515, 'ACC-37': 58.11461610082848, 'ACC-38': 58.899579333157284, 'ACC-39': 54.70665704503411, 'ACC-40': 54.95420759844147, 'ACC-41': 55.356363879298875, 'ACC-42': 56.8406720136778, 'ACC-43': 55.377685025219435, 'ACC-44': 54.58844258923906, 'ACC-45': 52.84612378048364, 'ACC-46': 50.97608441134894, 'ACC-47': 50.51762071984112, 'ACC-48': 50.99321994787107, 'ACC-49': 51.281996216778246, 'ACC-50': 51.896884452080386, 'ACC-51': 48.42018135226656, 'ACC-52': 48.53274264320072, 'ACC-53': 50.72164662342126, 'ACC-54': 49.62223009441251, 'ACC-55': 46.513491894141154, 'ACC-56': 45.77160340385949, 'ACC-57': 42.31916214507509, 'ACC-58': 41.200124568229505, 'ACC-59': 40.41645031688201, 'ACC-60': 38.803686502369544, 'ACC-61': 39.438509324650624, 'ACC-62': 39.361755821672695, 'ACC-63': 40.44935602589472, 'ACC-64': 37.263649511430515, 'ACC-65': 34.52863587332032, 'ACC-66': 35.82171485152886, 'ACC-67': 34.550958441878876, 'ACC-68': 35.604724387534745, 'ACC-69': 32.775587517433195, 'ACC-70': 32.9250433883081, 'ACC-71': 34.61377016572514, 'ACC-72': 33.51229501259302, 'ACC-73': 33.706331166032946, 'ACC-74': 34.2848431644893, 'ACC-75': 35.60371459553432, 'ACC-76': 32.1515158062137, 'ACC-77': 33.26541987394383, 'ACC-78': 33.01428443712671, 'ACC-79': 29.891626688978658, 'ACC-80': 31.13995429288492, 'ACC-81': 29.042887834799785, 'ACC-82': 28.586715673688662, 'ACC-83': 31.04998854211521, 'ACC-84': 27.511675439289927, 'ACC-85': 27.953098983322946, 'ACC-86': 28.59914134284208, 'ACC-87': 27.040114457530617, 'ACC-88': 24.856952463449133, 'ACC-89': 27.509822058120136, 'ACC-90': 25.09406635527459, 'ACC-91': 28.05551163305257, 'ACC-92': 25.93093765015324, 'ACC-93': 25.795992316340033, 'ACC-94': 27.856807914346394, 'ACC-95': 27.62000698840617, 'ACC-96': 26.134824922908393, 'ACC-97': 28.871672154757483, 'ACC-98': 26.411598500071943, 'ACC-99': 24.494217479541533, 'ACC-100': 22.65484163688514, 'ACC-101': 23.12714182791095, 'ACC-102': 21.53479836882646, 'ACC-103': 20.748299801643526, 'ACC-104': 22.112269742505102, 'ACC-105': 22.42611576250699, 'ACC-106': 21.082973627894525, 'ACC-107': 20.039712326323166, 'ACC-108': 23.31126204839838, 'ACC-109': 22.616108396658742, 'ACC-110': 21.94962075100665, 'ACC-111': 19.39154108222776, 'ACC-112': 21.200282408399115, 'ACC-113': 15.912480645558405, 'ACC-114': 24.745981865140525, 'ACC-115': 17.441515192518757, 'ACC-116': 20.401758553414528, 'ACC-117': 17.384015616473807, 'ACC-118': 18.318199166392702, 'ACC-119': 22.456216650218288, 'ACC-120': 18.426822968827416, 'ACC-121': 18.701033372137967, 'ACC-122': 19.397095870248236, 'ACC-123': 14.530492293785487, 'ACC-124': 14.264496156573353, 'ACC-125': 13.470304982906642, 'ACC-126': 18.88667018825308, 'ACC-127': 12.64852722668187, 'ACC-128': 19.408860446083686, 'ACC-129': 14.400721906123337, 'ACC-130': 13.265814164063327, 'ACC-131': 16.31626762046982, 'ACC-132': 14.539379986796822, 'ACC-133': 16.505882272891192, 'ACC-134': 11.058405859543301, 'ACC-135': 10.868394048229861, 'ACC-136': 13.034688897689154, 'ACC-137': 7.807485324949989, 'ACC-138': 19.28367007640176, 'ACC-139': 8.772349811806162, 'ACC-140': 6.857195864744478, 'ACC-141': 10.756405487604752, 'ACC-142': 15.525461088972554, 'ACC-143': 7.532081955118961, 'ACC-144': 12.243501805054152, 'ACC-145': 6.754354972159485, 'ACC-146': 10.854675470060085, 'ACC-147': 9.445088733991158, 'ACC-148': 5.836504123849509, 'ACC-149': 6.85266446871365, 'ACC-150': 8.945715539462315, 'ACC-151': 3.0434314593784606, 'ACC-152': 4.940488232517365, 'ACC-153': 4.443444365048829, 'ACC-154': 4.7574932455995125, 'ACC-155': 6.312752210578731, 'ACC-156': 4.552088787191271, 'ACC-157': 3.8164884386573084, 'ACC-158': 3.709824698574979, 'ACC-159': 4.232789980917925, 'ACC-160': 2.96424693480523, 'ACC-161': 7.715466036840346, 'ACC-162': 4.438546746364932, 'ACC-163': 3.2546735008033116, 'ACC-164': 3.0639683662112067, 'ACC-165': 2.7303884355534946, 'ACC-166': 3.7217852616027733, 'ACC-167': 3.2388516277639674, 'ACC-168': 4.808003375764821, 'ACC-169': 4.946718068128578, 'ACC-170': 5.935268395871488, 'ACC-171': 1.5894300057493538, 'ACC-172': 3.3627810497462685, 'ACC-173': 1.3713862120088955, 'ACC-174': 2.202873913521634, 'ACC-175': 2.541788459156299, 'ACC-176': 4.955595542140701, 'ACC-177': 4.292919506375157, 'ACC-178': 2.1117959633849774, 'ACC-179': 2.380696354698193, 'ACC-180': 2.5682380033187715, 'ACC-181': 7.8215267859046245, 'ACC-182': 0.8038589017955929, 'ACC-183': 2.4880459375022075, 'ACC-184': 1.223771688399286, 'ACC-185': 5.836555206038182, 'ACC-186': 3.4368633634945107, 'ACC-187': 0.80406866475797, 'ACC-188': 2.2998970985835925, 'ACC-189': 4.218514534518036, 'ACC-190': 1.8613035412976722, 'ACC-191': 7.932920065252855})])
[01/18 06:51:49] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 06:51:49] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 06:51:49] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 06:51:49] d2.evaluation.testing INFO: copypaste: 2.3785,19.4234,41.8014,29.3917,56.1197
[01/18 06:51:49] d2.utils.events INFO:  eta: 5:39:19  iter: 25999  total_loss: 34.1  loss_ce: 0.2634  loss_mask: 0.3578  loss_dice: 2.731  loss_ce_0: 0.553  loss_mask_0: 0.3534  loss_dice_0: 2.85  loss_ce_1: 0.3007  loss_mask_1: 0.3624  loss_dice_1: 2.776  loss_ce_2: 0.2884  loss_mask_2: 0.3598  loss_dice_2: 2.745  loss_ce_3: 0.262  loss_mask_3: 0.356  loss_dice_3: 2.735  loss_ce_4: 0.2628  loss_mask_4: 0.3556  loss_dice_4: 2.732  loss_ce_5: 0.2648  loss_mask_5: 0.356  loss_dice_5: 2.729  loss_ce_6: 0.2522  loss_mask_6: 0.3566  loss_dice_6: 2.729  loss_ce_7: 0.2493  loss_mask_7: 0.3568  loss_dice_7: 2.724  loss_ce_8: 0.2543  loss_mask_8: 0.3572  loss_dice_8: 2.721  time: 1.4860  data_time: 0.0705  lr: 3.8877e-06  max_mem: 21589M
[01/18 06:52:18] d2.utils.events INFO:  eta: 5:38:40  iter: 26019  total_loss: 34.1  loss_ce: 0.2627  loss_mask: 0.3579  loss_dice: 2.765  loss_ce_0: 0.5504  loss_mask_0: 0.3589  loss_dice_0: 2.888  loss_ce_1: 0.287  loss_mask_1: 0.3652  loss_dice_1: 2.801  loss_ce_2: 0.2874  loss_mask_2: 0.3616  loss_dice_2: 2.779  loss_ce_3: 0.2631  loss_mask_3: 0.3589  loss_dice_3: 2.769  loss_ce_4: 0.2594  loss_mask_4: 0.3577  loss_dice_4: 2.77  loss_ce_5: 0.2613  loss_mask_5: 0.3582  loss_dice_5: 2.766  loss_ce_6: 0.2532  loss_mask_6: 0.358  loss_dice_6: 2.763  loss_ce_7: 0.2432  loss_mask_7: 0.3585  loss_dice_7: 2.76  loss_ce_8: 0.252  loss_mask_8: 0.3583  loss_dice_8: 2.77  time: 1.4860  data_time: 0.0661  lr: 3.8827e-06  max_mem: 21589M
[01/18 06:52:47] d2.utils.events INFO:  eta: 5:38:05  iter: 26039  total_loss: 34.08  loss_ce: 0.267  loss_mask: 0.3486  loss_dice: 2.743  loss_ce_0: 0.5588  loss_mask_0: 0.3476  loss_dice_0: 2.879  loss_ce_1: 0.3202  loss_mask_1: 0.3511  loss_dice_1: 2.794  loss_ce_2: 0.3094  loss_mask_2: 0.3468  loss_dice_2: 2.772  loss_ce_3: 0.2866  loss_mask_3: 0.3481  loss_dice_3: 2.748  loss_ce_4: 0.2822  loss_mask_4: 0.3483  loss_dice_4: 2.747  loss_ce_5: 0.2707  loss_mask_5: 0.3469  loss_dice_5: 2.749  loss_ce_6: 0.2757  loss_mask_6: 0.3466  loss_dice_6: 2.739  loss_ce_7: 0.2656  loss_mask_7: 0.3484  loss_dice_7: 2.753  loss_ce_8: 0.2752  loss_mask_8: 0.3468  loss_dice_8: 2.74  time: 1.4860  data_time: 0.0740  lr: 3.8777e-06  max_mem: 21589M
[01/18 06:53:16] d2.utils.events INFO:  eta: 5:37:37  iter: 26059  total_loss: 34.41  loss_ce: 0.2745  loss_mask: 0.3566  loss_dice: 2.735  loss_ce_0: 0.5656  loss_mask_0: 0.3539  loss_dice_0: 2.876  loss_ce_1: 0.3131  loss_mask_1: 0.3623  loss_dice_1: 2.782  loss_ce_2: 0.3104  loss_mask_2: 0.3589  loss_dice_2: 2.756  loss_ce_3: 0.291  loss_mask_3: 0.3561  loss_dice_3: 2.745  loss_ce_4: 0.2966  loss_mask_4: 0.3544  loss_dice_4: 2.738  loss_ce_5: 0.2845  loss_mask_5: 0.3548  loss_dice_5: 2.732  loss_ce_6: 0.2681  loss_mask_6: 0.3563  loss_dice_6: 2.741  loss_ce_7: 0.2619  loss_mask_7: 0.3567  loss_dice_7: 2.746  loss_ce_8: 0.267  loss_mask_8: 0.356  loss_dice_8: 2.734  time: 1.4860  data_time: 0.0712  lr: 3.8727e-06  max_mem: 21589M
[01/18 06:53:45] d2.utils.events INFO:  eta: 5:37:07  iter: 26079  total_loss: 34.63  loss_ce: 0.2613  loss_mask: 0.3549  loss_dice: 2.768  loss_ce_0: 0.5782  loss_mask_0: 0.3535  loss_dice_0: 2.882  loss_ce_1: 0.3123  loss_mask_1: 0.3613  loss_dice_1: 2.806  loss_ce_2: 0.3002  loss_mask_2: 0.3592  loss_dice_2: 2.782  loss_ce_3: 0.2807  loss_mask_3: 0.3555  loss_dice_3: 2.769  loss_ce_4: 0.2835  loss_mask_4: 0.3562  loss_dice_4: 2.763  loss_ce_5: 0.2803  loss_mask_5: 0.3544  loss_dice_5: 2.773  loss_ce_6: 0.2796  loss_mask_6: 0.3531  loss_dice_6: 2.772  loss_ce_7: 0.2732  loss_mask_7: 0.3528  loss_dice_7: 2.774  loss_ce_8: 0.2637  loss_mask_8: 0.3543  loss_dice_8: 2.765  time: 1.4859  data_time: 0.0690  lr: 3.8677e-06  max_mem: 21589M
[01/18 06:54:14] d2.utils.events INFO:  eta: 5:36:37  iter: 26099  total_loss: 33.19  loss_ce: 0.2463  loss_mask: 0.3458  loss_dice: 2.665  loss_ce_0: 0.5731  loss_mask_0: 0.3508  loss_dice_0: 2.792  loss_ce_1: 0.3046  loss_mask_1: 0.3539  loss_dice_1: 2.698  loss_ce_2: 0.2952  loss_mask_2: 0.3518  loss_dice_2: 2.684  loss_ce_3: 0.2725  loss_mask_3: 0.3486  loss_dice_3: 2.665  loss_ce_4: 0.2663  loss_mask_4: 0.3488  loss_dice_4: 2.66  loss_ce_5: 0.2549  loss_mask_5: 0.3479  loss_dice_5: 2.662  loss_ce_6: 0.2601  loss_mask_6: 0.3463  loss_dice_6: 2.656  loss_ce_7: 0.2548  loss_mask_7: 0.3475  loss_dice_7: 2.657  loss_ce_8: 0.253  loss_mask_8: 0.3465  loss_dice_8: 2.664  time: 1.4859  data_time: 0.0727  lr: 3.8627e-06  max_mem: 21589M
[01/18 06:54:43] d2.utils.events INFO:  eta: 5:36:09  iter: 26119  total_loss: 34.09  loss_ce: 0.2516  loss_mask: 0.3578  loss_dice: 2.711  loss_ce_0: 0.5458  loss_mask_0: 0.3535  loss_dice_0: 2.853  loss_ce_1: 0.2941  loss_mask_1: 0.3621  loss_dice_1: 2.752  loss_ce_2: 0.2892  loss_mask_2: 0.3606  loss_dice_2: 2.736  loss_ce_3: 0.2744  loss_mask_3: 0.3591  loss_dice_3: 2.721  loss_ce_4: 0.2776  loss_mask_4: 0.3583  loss_dice_4: 2.728  loss_ce_5: 0.2666  loss_mask_5: 0.3577  loss_dice_5: 2.722  loss_ce_6: 0.261  loss_mask_6: 0.3575  loss_dice_6: 2.717  loss_ce_7: 0.2531  loss_mask_7: 0.3577  loss_dice_7: 2.717  loss_ce_8: 0.2589  loss_mask_8: 0.3579  loss_dice_8: 2.71  time: 1.4859  data_time: 0.0684  lr: 3.8577e-06  max_mem: 21589M
[01/18 06:55:13] d2.utils.events INFO:  eta: 5:35:41  iter: 26139  total_loss: 34.81  loss_ce: 0.2764  loss_mask: 0.3576  loss_dice: 2.805  loss_ce_0: 0.577  loss_mask_0: 0.3508  loss_dice_0: 2.943  loss_ce_1: 0.3102  loss_mask_1: 0.3582  loss_dice_1: 2.844  loss_ce_2: 0.3129  loss_mask_2: 0.3559  loss_dice_2: 2.821  loss_ce_3: 0.3066  loss_mask_3: 0.3554  loss_dice_3: 2.81  loss_ce_4: 0.2936  loss_mask_4: 0.3567  loss_dice_4: 2.807  loss_ce_5: 0.2818  loss_mask_5: 0.3563  loss_dice_5: 2.804  loss_ce_6: 0.2856  loss_mask_6: 0.3567  loss_dice_6: 2.806  loss_ce_7: 0.2848  loss_mask_7: 0.3561  loss_dice_7: 2.802  loss_ce_8: 0.2771  loss_mask_8: 0.3577  loss_dice_8: 2.805  time: 1.4859  data_time: 0.0739  lr: 3.8527e-06  max_mem: 21589M
[01/18 06:55:42] d2.utils.events INFO:  eta: 5:35:14  iter: 26159  total_loss: 34.83  loss_ce: 0.257  loss_mask: 0.3565  loss_dice: 2.801  loss_ce_0: 0.5737  loss_mask_0: 0.349  loss_dice_0: 2.917  loss_ce_1: 0.3164  loss_mask_1: 0.3582  loss_dice_1: 2.839  loss_ce_2: 0.2977  loss_mask_2: 0.3586  loss_dice_2: 2.817  loss_ce_3: 0.2762  loss_mask_3: 0.3563  loss_dice_3: 2.801  loss_ce_4: 0.2906  loss_mask_4: 0.3554  loss_dice_4: 2.795  loss_ce_5: 0.2704  loss_mask_5: 0.3557  loss_dice_5: 2.806  loss_ce_6: 0.267  loss_mask_6: 0.3555  loss_dice_6: 2.8  loss_ce_7: 0.265  loss_mask_7: 0.3551  loss_dice_7: 2.791  loss_ce_8: 0.2712  loss_mask_8: 0.3568  loss_dice_8: 2.793  time: 1.4858  data_time: 0.0714  lr: 3.8477e-06  max_mem: 21589M
[01/18 06:56:11] d2.utils.events INFO:  eta: 5:34:45  iter: 26179  total_loss: 33.64  loss_ce: 0.2488  loss_mask: 0.358  loss_dice: 2.697  loss_ce_0: 0.5427  loss_mask_0: 0.3541  loss_dice_0: 2.816  loss_ce_1: 0.298  loss_mask_1: 0.3614  loss_dice_1: 2.736  loss_ce_2: 0.2864  loss_mask_2: 0.3604  loss_dice_2: 2.716  loss_ce_3: 0.2779  loss_mask_3: 0.3594  loss_dice_3: 2.701  loss_ce_4: 0.2572  loss_mask_4: 0.3596  loss_dice_4: 2.709  loss_ce_5: 0.261  loss_mask_5: 0.3595  loss_dice_5: 2.699  loss_ce_6: 0.2506  loss_mask_6: 0.3591  loss_dice_6: 2.694  loss_ce_7: 0.2446  loss_mask_7: 0.3584  loss_dice_7: 2.698  loss_ce_8: 0.252  loss_mask_8: 0.3581  loss_dice_8: 2.691  time: 1.4858  data_time: 0.0699  lr: 3.8427e-06  max_mem: 21589M
[01/18 06:56:40] d2.utils.events INFO:  eta: 5:34:22  iter: 26199  total_loss: 33.77  loss_ce: 0.2654  loss_mask: 0.3471  loss_dice: 2.686  loss_ce_0: 0.5576  loss_mask_0: 0.3471  loss_dice_0: 2.814  loss_ce_1: 0.3156  loss_mask_1: 0.3492  loss_dice_1: 2.729  loss_ce_2: 0.3035  loss_mask_2: 0.348  loss_dice_2: 2.715  loss_ce_3: 0.2747  loss_mask_3: 0.3481  loss_dice_3: 2.692  loss_ce_4: 0.2819  loss_mask_4: 0.348  loss_dice_4: 2.686  loss_ce_5: 0.2647  loss_mask_5: 0.3482  loss_dice_5: 2.681  loss_ce_6: 0.2493  loss_mask_6: 0.3457  loss_dice_6: 2.684  loss_ce_7: 0.2437  loss_mask_7: 0.3468  loss_dice_7: 2.684  loss_ce_8: 0.2667  loss_mask_8: 0.3485  loss_dice_8: 2.687  time: 1.4858  data_time: 0.0680  lr: 3.8377e-06  max_mem: 21589M
[01/18 06:57:09] d2.utils.events INFO:  eta: 5:33:58  iter: 26219  total_loss: 34.29  loss_ce: 0.2595  loss_mask: 0.34  loss_dice: 2.748  loss_ce_0: 0.5605  loss_mask_0: 0.3369  loss_dice_0: 2.872  loss_ce_1: 0.3048  loss_mask_1: 0.3458  loss_dice_1: 2.788  loss_ce_2: 0.2898  loss_mask_2: 0.3402  loss_dice_2: 2.771  loss_ce_3: 0.2745  loss_mask_3: 0.3391  loss_dice_3: 2.761  loss_ce_4: 0.26  loss_mask_4: 0.3385  loss_dice_4: 2.754  loss_ce_5: 0.2591  loss_mask_5: 0.3402  loss_dice_5: 2.763  loss_ce_6: 0.2572  loss_mask_6: 0.3391  loss_dice_6: 2.75  loss_ce_7: 0.2615  loss_mask_7: 0.3389  loss_dice_7: 2.762  loss_ce_8: 0.2622  loss_mask_8: 0.3393  loss_dice_8: 2.765  time: 1.4858  data_time: 0.0713  lr: 3.8326e-06  max_mem: 21589M
[01/18 06:57:38] d2.utils.events INFO:  eta: 5:33:28  iter: 26239  total_loss: 34.11  loss_ce: 0.2514  loss_mask: 0.3506  loss_dice: 2.741  loss_ce_0: 0.5506  loss_mask_0: 0.3492  loss_dice_0: 2.878  loss_ce_1: 0.2917  loss_mask_1: 0.3585  loss_dice_1: 2.794  loss_ce_2: 0.2909  loss_mask_2: 0.355  loss_dice_2: 2.765  loss_ce_3: 0.2653  loss_mask_3: 0.3517  loss_dice_3: 2.758  loss_ce_4: 0.2588  loss_mask_4: 0.3508  loss_dice_4: 2.752  loss_ce_5: 0.2629  loss_mask_5: 0.3511  loss_dice_5: 2.75  loss_ce_6: 0.2554  loss_mask_6: 0.3499  loss_dice_6: 2.746  loss_ce_7: 0.2629  loss_mask_7: 0.3505  loss_dice_7: 2.743  loss_ce_8: 0.2541  loss_mask_8: 0.349  loss_dice_8: 2.748  time: 1.4857  data_time: 0.0694  lr: 3.8276e-06  max_mem: 21589M
[01/18 06:58:07] d2.utils.events INFO:  eta: 5:32:51  iter: 26259  total_loss: 33.59  loss_ce: 0.2469  loss_mask: 0.3623  loss_dice: 2.669  loss_ce_0: 0.5586  loss_mask_0: 0.3603  loss_dice_0: 2.807  loss_ce_1: 0.2849  loss_mask_1: 0.3676  loss_dice_1: 2.708  loss_ce_2: 0.2833  loss_mask_2: 0.3656  loss_dice_2: 2.689  loss_ce_3: 0.2797  loss_mask_3: 0.3649  loss_dice_3: 2.671  loss_ce_4: 0.2463  loss_mask_4: 0.3639  loss_dice_4: 2.667  loss_ce_5: 0.2573  loss_mask_5: 0.3618  loss_dice_5: 2.663  loss_ce_6: 0.2422  loss_mask_6: 0.3612  loss_dice_6: 2.665  loss_ce_7: 0.2472  loss_mask_7: 0.3626  loss_dice_7: 2.668  loss_ce_8: 0.2417  loss_mask_8: 0.3612  loss_dice_8: 2.671  time: 1.4857  data_time: 0.0686  lr: 3.8226e-06  max_mem: 21589M
[01/18 06:58:36] d2.utils.events INFO:  eta: 5:32:17  iter: 26279  total_loss: 34.06  loss_ce: 0.2585  loss_mask: 0.3527  loss_dice: 2.718  loss_ce_0: 0.5652  loss_mask_0: 0.355  loss_dice_0: 2.833  loss_ce_1: 0.2941  loss_mask_1: 0.3589  loss_dice_1: 2.738  loss_ce_2: 0.2996  loss_mask_2: 0.3556  loss_dice_2: 2.723  loss_ce_3: 0.2727  loss_mask_3: 0.3536  loss_dice_3: 2.715  loss_ce_4: 0.266  loss_mask_4: 0.3541  loss_dice_4: 2.718  loss_ce_5: 0.2639  loss_mask_5: 0.3514  loss_dice_5: 2.708  loss_ce_6: 0.2652  loss_mask_6: 0.352  loss_dice_6: 2.72  loss_ce_7: 0.2618  loss_mask_7: 0.3526  loss_dice_7: 2.71  loss_ce_8: 0.2408  loss_mask_8: 0.3527  loss_dice_8: 2.715  time: 1.4857  data_time: 0.0699  lr: 3.8176e-06  max_mem: 21589M
[01/18 06:59:05] d2.utils.events INFO:  eta: 5:31:55  iter: 26299  total_loss: 34.24  loss_ce: 0.2438  loss_mask: 0.3503  loss_dice: 2.764  loss_ce_0: 0.5604  loss_mask_0: 0.3498  loss_dice_0: 2.886  loss_ce_1: 0.2965  loss_mask_1: 0.3586  loss_dice_1: 2.803  loss_ce_2: 0.2737  loss_mask_2: 0.3544  loss_dice_2: 2.786  loss_ce_3: 0.2803  loss_mask_3: 0.3516  loss_dice_3: 2.767  loss_ce_4: 0.2528  loss_mask_4: 0.3501  loss_dice_4: 2.756  loss_ce_5: 0.2568  loss_mask_5: 0.35  loss_dice_5: 2.767  loss_ce_6: 0.2555  loss_mask_6: 0.3512  loss_dice_6: 2.765  loss_ce_7: 0.249  loss_mask_7: 0.3508  loss_dice_7: 2.754  loss_ce_8: 0.2525  loss_mask_8: 0.3518  loss_dice_8: 2.766  time: 1.4857  data_time: 0.0657  lr: 3.8126e-06  max_mem: 21589M
[01/18 06:59:34] d2.utils.events INFO:  eta: 5:31:18  iter: 26319  total_loss: 34.13  loss_ce: 0.2646  loss_mask: 0.3489  loss_dice: 2.682  loss_ce_0: 0.5574  loss_mask_0: 0.346  loss_dice_0: 2.823  loss_ce_1: 0.3143  loss_mask_1: 0.3531  loss_dice_1: 2.728  loss_ce_2: 0.3199  loss_mask_2: 0.3519  loss_dice_2: 2.698  loss_ce_3: 0.2844  loss_mask_3: 0.3517  loss_dice_3: 2.697  loss_ce_4: 0.2772  loss_mask_4: 0.3511  loss_dice_4: 2.686  loss_ce_5: 0.2648  loss_mask_5: 0.3502  loss_dice_5: 2.687  loss_ce_6: 0.2777  loss_mask_6: 0.3505  loss_dice_6: 2.684  loss_ce_7: 0.2631  loss_mask_7: 0.35  loss_dice_7: 2.679  loss_ce_8: 0.2626  loss_mask_8: 0.3495  loss_dice_8: 2.687  time: 1.4856  data_time: 0.0704  lr: 3.8076e-06  max_mem: 21589M
[01/18 07:00:04] d2.utils.events INFO:  eta: 5:30:53  iter: 26339  total_loss: 34.56  loss_ce: 0.2837  loss_mask: 0.3525  loss_dice: 2.741  loss_ce_0: 0.5832  loss_mask_0: 0.3483  loss_dice_0: 2.893  loss_ce_1: 0.3241  loss_mask_1: 0.3548  loss_dice_1: 2.791  loss_ce_2: 0.3222  loss_mask_2: 0.3543  loss_dice_2: 2.766  loss_ce_3: 0.2981  loss_mask_3: 0.3513  loss_dice_3: 2.757  loss_ce_4: 0.2826  loss_mask_4: 0.3505  loss_dice_4: 2.757  loss_ce_5: 0.3007  loss_mask_5: 0.3516  loss_dice_5: 2.749  loss_ce_6: 0.2867  loss_mask_6: 0.3506  loss_dice_6: 2.748  loss_ce_7: 0.2836  loss_mask_7: 0.3522  loss_dice_7: 2.756  loss_ce_8: 0.2679  loss_mask_8: 0.3525  loss_dice_8: 2.746  time: 1.4856  data_time: 0.0738  lr: 3.8026e-06  max_mem: 21589M
[01/18 07:00:33] d2.utils.events INFO:  eta: 5:30:19  iter: 26359  total_loss: 34.06  loss_ce: 0.2516  loss_mask: 0.3611  loss_dice: 2.727  loss_ce_0: 0.5803  loss_mask_0: 0.3627  loss_dice_0: 2.846  loss_ce_1: 0.2925  loss_mask_1: 0.3696  loss_dice_1: 2.771  loss_ce_2: 0.3069  loss_mask_2: 0.3662  loss_dice_2: 2.75  loss_ce_3: 0.2809  loss_mask_3: 0.3649  loss_dice_3: 2.736  loss_ce_4: 0.2735  loss_mask_4: 0.3636  loss_dice_4: 2.74  loss_ce_5: 0.2655  loss_mask_5: 0.3631  loss_dice_5: 2.731  loss_ce_6: 0.2662  loss_mask_6: 0.3634  loss_dice_6: 2.739  loss_ce_7: 0.2736  loss_mask_7: 0.3628  loss_dice_7: 2.726  loss_ce_8: 0.2549  loss_mask_8: 0.3631  loss_dice_8: 2.733  time: 1.4856  data_time: 0.0697  lr: 3.7976e-06  max_mem: 21589M
[01/18 07:01:01] d2.utils.events INFO:  eta: 5:29:46  iter: 26379  total_loss: 34.31  loss_ce: 0.2601  loss_mask: 0.3526  loss_dice: 2.719  loss_ce_0: 0.5654  loss_mask_0: 0.346  loss_dice_0: 2.86  loss_ce_1: 0.2872  loss_mask_1: 0.3556  loss_dice_1: 2.763  loss_ce_2: 0.2841  loss_mask_2: 0.3536  loss_dice_2: 2.741  loss_ce_3: 0.2728  loss_mask_3: 0.3524  loss_dice_3: 2.719  loss_ce_4: 0.2593  loss_mask_4: 0.353  loss_dice_4: 2.723  loss_ce_5: 0.2504  loss_mask_5: 0.3525  loss_dice_5: 2.727  loss_ce_6: 0.2601  loss_mask_6: 0.3542  loss_dice_6: 2.728  loss_ce_7: 0.2456  loss_mask_7: 0.3523  loss_dice_7: 2.719  loss_ce_8: 0.2484  loss_mask_8: 0.353  loss_dice_8: 2.718  time: 1.4855  data_time: 0.0698  lr: 3.7926e-06  max_mem: 21589M
[01/18 07:01:31] d2.utils.events INFO:  eta: 5:29:14  iter: 26399  total_loss: 34.28  loss_ce: 0.2729  loss_mask: 0.3595  loss_dice: 2.725  loss_ce_0: 0.5771  loss_mask_0: 0.3564  loss_dice_0: 2.864  loss_ce_1: 0.3093  loss_mask_1: 0.3623  loss_dice_1: 2.785  loss_ce_2: 0.2948  loss_mask_2: 0.36  loss_dice_2: 2.752  loss_ce_3: 0.2917  loss_mask_3: 0.3604  loss_dice_3: 2.741  loss_ce_4: 0.2791  loss_mask_4: 0.3591  loss_dice_4: 2.738  loss_ce_5: 0.2681  loss_mask_5: 0.3592  loss_dice_5: 2.728  loss_ce_6: 0.2643  loss_mask_6: 0.3579  loss_dice_6: 2.724  loss_ce_7: 0.2637  loss_mask_7: 0.3579  loss_dice_7: 2.734  loss_ce_8: 0.2584  loss_mask_8: 0.3582  loss_dice_8: 2.734  time: 1.4855  data_time: 0.0743  lr: 3.7876e-06  max_mem: 21589M
[01/18 07:01:59] d2.utils.events INFO:  eta: 5:28:40  iter: 26419  total_loss: 33.98  loss_ce: 0.2532  loss_mask: 0.3553  loss_dice: 2.704  loss_ce_0: 0.5367  loss_mask_0: 0.3479  loss_dice_0: 2.848  loss_ce_1: 0.3031  loss_mask_1: 0.3584  loss_dice_1: 2.76  loss_ce_2: 0.2902  loss_mask_2: 0.359  loss_dice_2: 2.73  loss_ce_3: 0.2639  loss_mask_3: 0.3571  loss_dice_3: 2.716  loss_ce_4: 0.2727  loss_mask_4: 0.3572  loss_dice_4: 2.717  loss_ce_5: 0.2536  loss_mask_5: 0.357  loss_dice_5: 2.713  loss_ce_6: 0.2553  loss_mask_6: 0.3577  loss_dice_6: 2.702  loss_ce_7: 0.2572  loss_mask_7: 0.3554  loss_dice_7: 2.704  loss_ce_8: 0.2582  loss_mask_8: 0.3564  loss_dice_8: 2.702  time: 1.4855  data_time: 0.0679  lr: 3.7825e-06  max_mem: 21589M
[01/18 07:02:28] d2.utils.events INFO:  eta: 5:28:07  iter: 26439  total_loss: 33.84  loss_ce: 0.2527  loss_mask: 0.359  loss_dice: 2.72  loss_ce_0: 0.5553  loss_mask_0: 0.354  loss_dice_0: 2.847  loss_ce_1: 0.2923  loss_mask_1: 0.3629  loss_dice_1: 2.762  loss_ce_2: 0.2798  loss_mask_2: 0.3592  loss_dice_2: 2.738  loss_ce_3: 0.2796  loss_mask_3: 0.3573  loss_dice_3: 2.722  loss_ce_4: 0.2637  loss_mask_4: 0.3553  loss_dice_4: 2.726  loss_ce_5: 0.2648  loss_mask_5: 0.3574  loss_dice_5: 2.726  loss_ce_6: 0.2509  loss_mask_6: 0.3588  loss_dice_6: 2.719  loss_ce_7: 0.2551  loss_mask_7: 0.3561  loss_dice_7: 2.723  loss_ce_8: 0.2583  loss_mask_8: 0.357  loss_dice_8: 2.719  time: 1.4854  data_time: 0.0656  lr: 3.7775e-06  max_mem: 21589M
[01/18 07:02:57] d2.utils.events INFO:  eta: 5:27:37  iter: 26459  total_loss: 34.77  loss_ce: 0.2638  loss_mask: 0.3615  loss_dice: 2.732  loss_ce_0: 0.6217  loss_mask_0: 0.3601  loss_dice_0: 2.863  loss_ce_1: 0.312  loss_mask_1: 0.3655  loss_dice_1: 2.771  loss_ce_2: 0.301  loss_mask_2: 0.3627  loss_dice_2: 2.766  loss_ce_3: 0.3045  loss_mask_3: 0.3607  loss_dice_3: 2.74  loss_ce_4: 0.2708  loss_mask_4: 0.3597  loss_dice_4: 2.739  loss_ce_5: 0.2693  loss_mask_5: 0.359  loss_dice_5: 2.747  loss_ce_6: 0.2855  loss_mask_6: 0.3612  loss_dice_6: 2.751  loss_ce_7: 0.282  loss_mask_7: 0.3619  loss_dice_7: 2.734  loss_ce_8: 0.2671  loss_mask_8: 0.3618  loss_dice_8: 2.754  time: 1.4854  data_time: 0.0704  lr: 3.7725e-06  max_mem: 21589M
[01/18 07:03:25] d2.utils.events INFO:  eta: 5:27:06  iter: 26479  total_loss: 33.48  loss_ce: 0.2638  loss_mask: 0.3599  loss_dice: 2.649  loss_ce_0: 0.5623  loss_mask_0: 0.3592  loss_dice_0: 2.759  loss_ce_1: 0.3082  loss_mask_1: 0.3649  loss_dice_1: 2.69  loss_ce_2: 0.2988  loss_mask_2: 0.3626  loss_dice_2: 2.67  loss_ce_3: 0.2855  loss_mask_3: 0.36  loss_dice_3: 2.663  loss_ce_4: 0.2757  loss_mask_4: 0.3579  loss_dice_4: 2.653  loss_ce_5: 0.2719  loss_mask_5: 0.3601  loss_dice_5: 2.645  loss_ce_6: 0.2572  loss_mask_6: 0.3603  loss_dice_6: 2.647  loss_ce_7: 0.2449  loss_mask_7: 0.358  loss_dice_7: 2.643  loss_ce_8: 0.256  loss_mask_8: 0.3596  loss_dice_8: 2.644  time: 1.4854  data_time: 0.0660  lr: 3.7675e-06  max_mem: 21589M
[01/18 07:03:55] d2.utils.events INFO:  eta: 5:26:38  iter: 26499  total_loss: 34.2  loss_ce: 0.2594  loss_mask: 0.3526  loss_dice: 2.733  loss_ce_0: 0.5741  loss_mask_0: 0.3499  loss_dice_0: 2.855  loss_ce_1: 0.3048  loss_mask_1: 0.3557  loss_dice_1: 2.772  loss_ce_2: 0.3021  loss_mask_2: 0.3543  loss_dice_2: 2.757  loss_ce_3: 0.2814  loss_mask_3: 0.3548  loss_dice_3: 2.739  loss_ce_4: 0.2854  loss_mask_4: 0.353  loss_dice_4: 2.742  loss_ce_5: 0.2779  loss_mask_5: 0.3546  loss_dice_5: 2.738  loss_ce_6: 0.2635  loss_mask_6: 0.3541  loss_dice_6: 2.734  loss_ce_7: 0.2664  loss_mask_7: 0.3527  loss_dice_7: 2.734  loss_ce_8: 0.2608  loss_mask_8: 0.3523  loss_dice_8: 2.732  time: 1.4854  data_time: 0.0701  lr: 3.7625e-06  max_mem: 21589M
[01/18 07:04:24] d2.utils.events INFO:  eta: 5:26:15  iter: 26519  total_loss: 33.85  loss_ce: 0.2637  loss_mask: 0.3544  loss_dice: 2.697  loss_ce_0: 0.5631  loss_mask_0: 0.3581  loss_dice_0: 2.83  loss_ce_1: 0.2977  loss_mask_1: 0.3618  loss_dice_1: 2.743  loss_ce_2: 0.3074  loss_mask_2: 0.357  loss_dice_2: 2.713  loss_ce_3: 0.2809  loss_mask_3: 0.3563  loss_dice_3: 2.71  loss_ce_4: 0.2731  loss_mask_4: 0.3545  loss_dice_4: 2.709  loss_ce_5: 0.2683  loss_mask_5: 0.3541  loss_dice_5: 2.706  loss_ce_6: 0.2783  loss_mask_6: 0.3539  loss_dice_6: 2.703  loss_ce_7: 0.2648  loss_mask_7: 0.3538  loss_dice_7: 2.697  loss_ce_8: 0.271  loss_mask_8: 0.3529  loss_dice_8: 2.697  time: 1.4853  data_time: 0.0675  lr: 3.7575e-06  max_mem: 21589M
[01/18 07:04:53] d2.utils.events INFO:  eta: 5:25:40  iter: 26539  total_loss: 34.15  loss_ce: 0.2799  loss_mask: 0.3543  loss_dice: 2.731  loss_ce_0: 0.5649  loss_mask_0: 0.3568  loss_dice_0: 2.841  loss_ce_1: 0.3187  loss_mask_1: 0.3618  loss_dice_1: 2.763  loss_ce_2: 0.3014  loss_mask_2: 0.3583  loss_dice_2: 2.746  loss_ce_3: 0.2811  loss_mask_3: 0.3564  loss_dice_3: 2.732  loss_ce_4: 0.2788  loss_mask_4: 0.3539  loss_dice_4: 2.728  loss_ce_5: 0.2835  loss_mask_5: 0.3541  loss_dice_5: 2.728  loss_ce_6: 0.2698  loss_mask_6: 0.3545  loss_dice_6: 2.721  loss_ce_7: 0.2717  loss_mask_7: 0.3537  loss_dice_7: 2.722  loss_ce_8: 0.2815  loss_mask_8: 0.354  loss_dice_8: 2.724  time: 1.4853  data_time: 0.0645  lr: 3.7525e-06  max_mem: 21589M
[01/18 07:05:22] d2.utils.events INFO:  eta: 5:25:15  iter: 26559  total_loss: 34.54  loss_ce: 0.2567  loss_mask: 0.3482  loss_dice: 2.801  loss_ce_0: 0.5755  loss_mask_0: 0.3459  loss_dice_0: 2.932  loss_ce_1: 0.3008  loss_mask_1: 0.3547  loss_dice_1: 2.842  loss_ce_2: 0.2844  loss_mask_2: 0.3513  loss_dice_2: 2.817  loss_ce_3: 0.2571  loss_mask_3: 0.35  loss_dice_3: 2.805  loss_ce_4: 0.2663  loss_mask_4: 0.3491  loss_dice_4: 2.804  loss_ce_5: 0.2543  loss_mask_5: 0.3491  loss_dice_5: 2.805  loss_ce_6: 0.2577  loss_mask_6: 0.3485  loss_dice_6: 2.807  loss_ce_7: 0.2494  loss_mask_7: 0.3482  loss_dice_7: 2.81  loss_ce_8: 0.2493  loss_mask_8: 0.3482  loss_dice_8: 2.808  time: 1.4853  data_time: 0.0709  lr: 3.7474e-06  max_mem: 21589M
[01/18 07:05:52] d2.utils.events INFO:  eta: 5:24:42  iter: 26579  total_loss: 34.36  loss_ce: 0.28  loss_mask: 0.3595  loss_dice: 2.729  loss_ce_0: 0.584  loss_mask_0: 0.3491  loss_dice_0: 2.866  loss_ce_1: 0.3082  loss_mask_1: 0.3565  loss_dice_1: 2.778  loss_ce_2: 0.3249  loss_mask_2: 0.3539  loss_dice_2: 2.753  loss_ce_3: 0.2998  loss_mask_3: 0.358  loss_dice_3: 2.742  loss_ce_4: 0.2737  loss_mask_4: 0.3568  loss_dice_4: 2.74  loss_ce_5: 0.264  loss_mask_5: 0.3579  loss_dice_5: 2.75  loss_ce_6: 0.2687  loss_mask_6: 0.3578  loss_dice_6: 2.737  loss_ce_7: 0.2793  loss_mask_7: 0.361  loss_dice_7: 2.731  loss_ce_8: 0.2791  loss_mask_8: 0.3618  loss_dice_8: 2.734  time: 1.4853  data_time: 0.0735  lr: 3.7424e-06  max_mem: 21589M
[01/18 07:06:20] d2.utils.events INFO:  eta: 5:24:14  iter: 26599  total_loss: 33.66  loss_ce: 0.2513  loss_mask: 0.3603  loss_dice: 2.69  loss_ce_0: 0.551  loss_mask_0: 0.3614  loss_dice_0: 2.828  loss_ce_1: 0.2957  loss_mask_1: 0.3689  loss_dice_1: 2.724  loss_ce_2: 0.2954  loss_mask_2: 0.364  loss_dice_2: 2.708  loss_ce_3: 0.292  loss_mask_3: 0.3613  loss_dice_3: 2.697  loss_ce_4: 0.259  loss_mask_4: 0.3629  loss_dice_4: 2.697  loss_ce_5: 0.2633  loss_mask_5: 0.3624  loss_dice_5: 2.692  loss_ce_6: 0.2584  loss_mask_6: 0.3601  loss_dice_6: 2.684  loss_ce_7: 0.246  loss_mask_7: 0.3601  loss_dice_7: 2.684  loss_ce_8: 0.2382  loss_mask_8: 0.3598  loss_dice_8: 2.688  time: 1.4852  data_time: 0.0658  lr: 3.7374e-06  max_mem: 21589M
[01/18 07:06:50] d2.utils.events INFO:  eta: 5:23:43  iter: 26619  total_loss: 33.69  loss_ce: 0.2524  loss_mask: 0.3442  loss_dice: 2.707  loss_ce_0: 0.5504  loss_mask_0: 0.3376  loss_dice_0: 2.832  loss_ce_1: 0.2948  loss_mask_1: 0.3471  loss_dice_1: 2.743  loss_ce_2: 0.3023  loss_mask_2: 0.3431  loss_dice_2: 2.732  loss_ce_3: 0.2721  loss_mask_3: 0.3434  loss_dice_3: 2.724  loss_ce_4: 0.2677  loss_mask_4: 0.345  loss_dice_4: 2.716  loss_ce_5: 0.263  loss_mask_5: 0.3433  loss_dice_5: 2.719  loss_ce_6: 0.258  loss_mask_6: 0.3441  loss_dice_6: 2.715  loss_ce_7: 0.2633  loss_mask_7: 0.3438  loss_dice_7: 2.71  loss_ce_8: 0.2624  loss_mask_8: 0.3438  loss_dice_8: 2.722  time: 1.4852  data_time: 0.0714  lr: 3.7324e-06  max_mem: 21589M
[01/18 07:07:19] d2.utils.events INFO:  eta: 5:22:49  iter: 26639  total_loss: 33.63  loss_ce: 0.2588  loss_mask: 0.3407  loss_dice: 2.691  loss_ce_0: 0.5662  loss_mask_0: 0.3348  loss_dice_0: 2.831  loss_ce_1: 0.313  loss_mask_1: 0.3449  loss_dice_1: 2.747  loss_ce_2: 0.3036  loss_mask_2: 0.3424  loss_dice_2: 2.72  loss_ce_3: 0.2648  loss_mask_3: 0.3414  loss_dice_3: 2.703  loss_ce_4: 0.2662  loss_mask_4: 0.3415  loss_dice_4: 2.696  loss_ce_5: 0.2606  loss_mask_5: 0.3414  loss_dice_5: 2.69  loss_ce_6: 0.2564  loss_mask_6: 0.3417  loss_dice_6: 2.684  loss_ce_7: 0.2369  loss_mask_7: 0.343  loss_dice_7: 2.695  loss_ce_8: 0.25  loss_mask_8: 0.3425  loss_dice_8: 2.689  time: 1.4852  data_time: 0.0704  lr: 3.7274e-06  max_mem: 21589M
[01/18 07:07:48] d2.utils.events INFO:  eta: 5:22:15  iter: 26659  total_loss: 33.32  loss_ce: 0.2546  loss_mask: 0.3539  loss_dice: 2.662  loss_ce_0: 0.5801  loss_mask_0: 0.3458  loss_dice_0: 2.809  loss_ce_1: 0.293  loss_mask_1: 0.3586  loss_dice_1: 2.703  loss_ce_2: 0.2804  loss_mask_2: 0.3565  loss_dice_2: 2.682  loss_ce_3: 0.2851  loss_mask_3: 0.3549  loss_dice_3: 2.677  loss_ce_4: 0.2546  loss_mask_4: 0.3544  loss_dice_4: 2.666  loss_ce_5: 0.2631  loss_mask_5: 0.3536  loss_dice_5: 2.666  loss_ce_6: 0.2563  loss_mask_6: 0.3547  loss_dice_6: 2.662  loss_ce_7: 0.2447  loss_mask_7: 0.3543  loss_dice_7: 2.66  loss_ce_8: 0.2354  loss_mask_8: 0.354  loss_dice_8: 2.664  time: 1.4851  data_time: 0.0673  lr: 3.7223e-06  max_mem: 21589M
[01/18 07:08:17] d2.utils.events INFO:  eta: 5:21:36  iter: 26679  total_loss: 33.45  loss_ce: 0.2362  loss_mask: 0.3515  loss_dice: 2.697  loss_ce_0: 0.5323  loss_mask_0: 0.3474  loss_dice_0: 2.832  loss_ce_1: 0.2754  loss_mask_1: 0.3586  loss_dice_1: 2.735  loss_ce_2: 0.2716  loss_mask_2: 0.3555  loss_dice_2: 2.715  loss_ce_3: 0.2672  loss_mask_3: 0.3508  loss_dice_3: 2.699  loss_ce_4: 0.2606  loss_mask_4: 0.3515  loss_dice_4: 2.704  loss_ce_5: 0.2446  loss_mask_5: 0.3508  loss_dice_5: 2.703  loss_ce_6: 0.2357  loss_mask_6: 0.3511  loss_dice_6: 2.697  loss_ce_7: 0.23  loss_mask_7: 0.3519  loss_dice_7: 2.7  loss_ce_8: 0.2343  loss_mask_8: 0.3509  loss_dice_8: 2.695  time: 1.4851  data_time: 0.0695  lr: 3.7173e-06  max_mem: 21589M
[01/18 07:08:46] d2.utils.events INFO:  eta: 5:21:04  iter: 26699  total_loss: 33.43  loss_ce: 0.2554  loss_mask: 0.3483  loss_dice: 2.668  loss_ce_0: 0.5557  loss_mask_0: 0.3443  loss_dice_0: 2.793  loss_ce_1: 0.295  loss_mask_1: 0.353  loss_dice_1: 2.709  loss_ce_2: 0.2828  loss_mask_2: 0.3509  loss_dice_2: 2.688  loss_ce_3: 0.2845  loss_mask_3: 0.3521  loss_dice_3: 2.67  loss_ce_4: 0.2627  loss_mask_4: 0.3498  loss_dice_4: 2.666  loss_ce_5: 0.2546  loss_mask_5: 0.3501  loss_dice_5: 2.668  loss_ce_6: 0.2616  loss_mask_6: 0.3505  loss_dice_6: 2.669  loss_ce_7: 0.2458  loss_mask_7: 0.3483  loss_dice_7: 2.674  loss_ce_8: 0.2538  loss_mask_8: 0.3486  loss_dice_8: 2.671  time: 1.4851  data_time: 0.0726  lr: 3.7123e-06  max_mem: 21589M
[01/18 07:09:15] d2.utils.events INFO:  eta: 5:20:29  iter: 26719  total_loss: 32.99  loss_ce: 0.2397  loss_mask: 0.3528  loss_dice: 2.645  loss_ce_0: 0.5276  loss_mask_0: 0.3483  loss_dice_0: 2.783  loss_ce_1: 0.2525  loss_mask_1: 0.3572  loss_dice_1: 2.69  loss_ce_2: 0.2757  loss_mask_2: 0.3566  loss_dice_2: 2.667  loss_ce_3: 0.2585  loss_mask_3: 0.3521  loss_dice_3: 2.648  loss_ce_4: 0.249  loss_mask_4: 0.3545  loss_dice_4: 2.652  loss_ce_5: 0.2523  loss_mask_5: 0.3533  loss_dice_5: 2.643  loss_ce_6: 0.2522  loss_mask_6: 0.3535  loss_dice_6: 2.65  loss_ce_7: 0.2386  loss_mask_7: 0.3521  loss_dice_7: 2.651  loss_ce_8: 0.235  loss_mask_8: 0.3519  loss_dice_8: 2.647  time: 1.4851  data_time: 0.0646  lr: 3.7073e-06  max_mem: 21589M
[01/18 07:09:44] d2.utils.events INFO:  eta: 5:19:36  iter: 26739  total_loss: 33.42  loss_ce: 0.2547  loss_mask: 0.3601  loss_dice: 2.673  loss_ce_0: 0.5741  loss_mask_0: 0.3511  loss_dice_0: 2.808  loss_ce_1: 0.297  loss_mask_1: 0.3621  loss_dice_1: 2.717  loss_ce_2: 0.2952  loss_mask_2: 0.3571  loss_dice_2: 2.7  loss_ce_3: 0.2791  loss_mask_3: 0.3598  loss_dice_3: 2.68  loss_ce_4: 0.2725  loss_mask_4: 0.3577  loss_dice_4: 2.685  loss_ce_5: 0.2591  loss_mask_5: 0.3577  loss_dice_5: 2.686  loss_ce_6: 0.257  loss_mask_6: 0.3589  loss_dice_6: 2.683  loss_ce_7: 0.2532  loss_mask_7: 0.3591  loss_dice_7: 2.677  loss_ce_8: 0.2515  loss_mask_8: 0.3595  loss_dice_8: 2.674  time: 1.4850  data_time: 0.0711  lr: 3.7022e-06  max_mem: 21589M
[01/18 07:10:13] d2.utils.events INFO:  eta: 5:19:13  iter: 26759  total_loss: 33.54  loss_ce: 0.246  loss_mask: 0.3459  loss_dice: 2.703  loss_ce_0: 0.5569  loss_mask_0: 0.3386  loss_dice_0: 2.834  loss_ce_1: 0.2797  loss_mask_1: 0.3502  loss_dice_1: 2.738  loss_ce_2: 0.281  loss_mask_2: 0.3457  loss_dice_2: 2.721  loss_ce_3: 0.262  loss_mask_3: 0.3476  loss_dice_3: 2.703  loss_ce_4: 0.2458  loss_mask_4: 0.3475  loss_dice_4: 2.7  loss_ce_5: 0.252  loss_mask_5: 0.3469  loss_dice_5: 2.711  loss_ce_6: 0.2477  loss_mask_6: 0.3457  loss_dice_6: 2.71  loss_ce_7: 0.234  loss_mask_7: 0.3453  loss_dice_7: 2.697  loss_ce_8: 0.2498  loss_mask_8: 0.3448  loss_dice_8: 2.702  time: 1.4850  data_time: 0.0680  lr: 3.6972e-06  max_mem: 21589M
[01/18 07:10:42] d2.utils.events INFO:  eta: 5:19:08  iter: 26779  total_loss: 33.73  loss_ce: 0.2621  loss_mask: 0.351  loss_dice: 2.746  loss_ce_0: 0.5319  loss_mask_0: 0.3488  loss_dice_0: 2.867  loss_ce_1: 0.2728  loss_mask_1: 0.3561  loss_dice_1: 2.786  loss_ce_2: 0.281  loss_mask_2: 0.3522  loss_dice_2: 2.764  loss_ce_3: 0.2738  loss_mask_3: 0.3513  loss_dice_3: 2.748  loss_ce_4: 0.2653  loss_mask_4: 0.3505  loss_dice_4: 2.75  loss_ce_5: 0.25  loss_mask_5: 0.3511  loss_dice_5: 2.754  loss_ce_6: 0.2506  loss_mask_6: 0.3499  loss_dice_6: 2.733  loss_ce_7: 0.2407  loss_mask_7: 0.3508  loss_dice_7: 2.749  loss_ce_8: 0.2573  loss_mask_8: 0.3515  loss_dice_8: 2.744  time: 1.4850  data_time: 0.0684  lr: 3.6922e-06  max_mem: 21589M
[01/18 07:11:11] d2.utils.events INFO:  eta: 5:18:39  iter: 26799  total_loss: 34.03  loss_ce: 0.2682  loss_mask: 0.3567  loss_dice: 2.701  loss_ce_0: 0.5609  loss_mask_0: 0.3511  loss_dice_0: 2.832  loss_ce_1: 0.2881  loss_mask_1: 0.3575  loss_dice_1: 2.75  loss_ce_2: 0.3225  loss_mask_2: 0.3552  loss_dice_2: 2.727  loss_ce_3: 0.2885  loss_mask_3: 0.3546  loss_dice_3: 2.708  loss_ce_4: 0.2673  loss_mask_4: 0.3554  loss_dice_4: 2.708  loss_ce_5: 0.2757  loss_mask_5: 0.3551  loss_dice_5: 2.708  loss_ce_6: 0.2751  loss_mask_6: 0.3542  loss_dice_6: 2.697  loss_ce_7: 0.2596  loss_mask_7: 0.3567  loss_dice_7: 2.707  loss_ce_8: 0.2712  loss_mask_8: 0.3561  loss_dice_8: 2.702  time: 1.4850  data_time: 0.0710  lr: 3.6872e-06  max_mem: 21589M
[01/18 07:11:40] d2.utils.events INFO:  eta: 5:18:05  iter: 26819  total_loss: 34.28  loss_ce: 0.2508  loss_mask: 0.3549  loss_dice: 2.744  loss_ce_0: 0.5756  loss_mask_0: 0.3538  loss_dice_0: 2.894  loss_ce_1: 0.2944  loss_mask_1: 0.3592  loss_dice_1: 2.789  loss_ce_2: 0.2736  loss_mask_2: 0.3578  loss_dice_2: 2.767  loss_ce_3: 0.2692  loss_mask_3: 0.3548  loss_dice_3: 2.76  loss_ce_4: 0.2593  loss_mask_4: 0.3545  loss_dice_4: 2.752  loss_ce_5: 0.2549  loss_mask_5: 0.3545  loss_dice_5: 2.75  loss_ce_6: 0.2575  loss_mask_6: 0.3549  loss_dice_6: 2.748  loss_ce_7: 0.2474  loss_mask_7: 0.3551  loss_dice_7: 2.756  loss_ce_8: 0.2387  loss_mask_8: 0.3556  loss_dice_8: 2.748  time: 1.4849  data_time: 0.0657  lr: 3.6821e-06  max_mem: 21589M
[01/18 07:12:09] d2.utils.events INFO:  eta: 5:17:36  iter: 26839  total_loss: 33.23  loss_ce: 0.2441  loss_mask: 0.3537  loss_dice: 2.626  loss_ce_0: 0.5501  loss_mask_0: 0.3469  loss_dice_0: 2.761  loss_ce_1: 0.2842  loss_mask_1: 0.3589  loss_dice_1: 2.666  loss_ce_2: 0.2905  loss_mask_2: 0.3551  loss_dice_2: 2.647  loss_ce_3: 0.2713  loss_mask_3: 0.3538  loss_dice_3: 2.633  loss_ce_4: 0.2698  loss_mask_4: 0.3537  loss_dice_4: 2.637  loss_ce_5: 0.2541  loss_mask_5: 0.3536  loss_dice_5: 2.636  loss_ce_6: 0.267  loss_mask_6: 0.3537  loss_dice_6: 2.628  loss_ce_7: 0.2745  loss_mask_7: 0.3534  loss_dice_7: 2.628  loss_ce_8: 0.2534  loss_mask_8: 0.3526  loss_dice_8: 2.624  time: 1.4849  data_time: 0.0670  lr: 3.6771e-06  max_mem: 21589M
[01/18 07:12:38] d2.utils.events INFO:  eta: 5:17:04  iter: 26859  total_loss: 33.67  loss_ce: 0.2332  loss_mask: 0.3592  loss_dice: 2.735  loss_ce_0: 0.5465  loss_mask_0: 0.3597  loss_dice_0: 2.873  loss_ce_1: 0.2974  loss_mask_1: 0.3666  loss_dice_1: 2.784  loss_ce_2: 0.2822  loss_mask_2: 0.3634  loss_dice_2: 2.758  loss_ce_3: 0.2613  loss_mask_3: 0.3603  loss_dice_3: 2.747  loss_ce_4: 0.2506  loss_mask_4: 0.3608  loss_dice_4: 2.744  loss_ce_5: 0.2558  loss_mask_5: 0.3603  loss_dice_5: 2.747  loss_ce_6: 0.2499  loss_mask_6: 0.3603  loss_dice_6: 2.739  loss_ce_7: 0.241  loss_mask_7: 0.3606  loss_dice_7: 2.743  loss_ce_8: 0.2359  loss_mask_8: 0.3588  loss_dice_8: 2.742  time: 1.4849  data_time: 0.0724  lr: 3.6721e-06  max_mem: 21589M
[01/18 07:13:08] d2.utils.events INFO:  eta: 5:16:38  iter: 26879  total_loss: 33.51  loss_ce: 0.2458  loss_mask: 0.3489  loss_dice: 2.708  loss_ce_0: 0.5717  loss_mask_0: 0.3467  loss_dice_0: 2.85  loss_ce_1: 0.2884  loss_mask_1: 0.3493  loss_dice_1: 2.759  loss_ce_2: 0.2873  loss_mask_2: 0.3496  loss_dice_2: 2.729  loss_ce_3: 0.2761  loss_mask_3: 0.3486  loss_dice_3: 2.717  loss_ce_4: 0.2612  loss_mask_4: 0.3492  loss_dice_4: 2.705  loss_ce_5: 0.2483  loss_mask_5: 0.3495  loss_dice_5: 2.72  loss_ce_6: 0.2558  loss_mask_6: 0.3482  loss_dice_6: 2.707  loss_ce_7: 0.2583  loss_mask_7: 0.3474  loss_dice_7: 2.706  loss_ce_8: 0.2475  loss_mask_8: 0.3478  loss_dice_8: 2.709  time: 1.4849  data_time: 0.0688  lr: 3.667e-06  max_mem: 21589M
[01/18 07:13:37] d2.utils.events INFO:  eta: 5:16:17  iter: 26899  total_loss: 33.6  loss_ce: 0.2619  loss_mask: 0.347  loss_dice: 2.656  loss_ce_0: 0.5942  loss_mask_0: 0.3478  loss_dice_0: 2.798  loss_ce_1: 0.3352  loss_mask_1: 0.3512  loss_dice_1: 2.697  loss_ce_2: 0.3111  loss_mask_2: 0.3485  loss_dice_2: 2.688  loss_ce_3: 0.2898  loss_mask_3: 0.3478  loss_dice_3: 2.664  loss_ce_4: 0.2958  loss_mask_4: 0.3462  loss_dice_4: 2.659  loss_ce_5: 0.2784  loss_mask_5: 0.3467  loss_dice_5: 2.663  loss_ce_6: 0.2811  loss_mask_6: 0.3461  loss_dice_6: 2.654  loss_ce_7: 0.2625  loss_mask_7: 0.3475  loss_dice_7: 2.665  loss_ce_8: 0.2507  loss_mask_8: 0.3475  loss_dice_8: 2.66  time: 1.4848  data_time: 0.0710  lr: 3.662e-06  max_mem: 21589M
[01/18 07:14:06] d2.utils.events INFO:  eta: 5:15:40  iter: 26919  total_loss: 33.4  loss_ce: 0.2451  loss_mask: 0.3479  loss_dice: 2.678  loss_ce_0: 0.5438  loss_mask_0: 0.3461  loss_dice_0: 2.802  loss_ce_1: 0.3009  loss_mask_1: 0.3543  loss_dice_1: 2.724  loss_ce_2: 0.2728  loss_mask_2: 0.3505  loss_dice_2: 2.711  loss_ce_3: 0.2704  loss_mask_3: 0.347  loss_dice_3: 2.693  loss_ce_4: 0.2491  loss_mask_4: 0.3467  loss_dice_4: 2.691  loss_ce_5: 0.2401  loss_mask_5: 0.3457  loss_dice_5: 2.685  loss_ce_6: 0.2476  loss_mask_6: 0.3467  loss_dice_6: 2.684  loss_ce_7: 0.2452  loss_mask_7: 0.3462  loss_dice_7: 2.687  loss_ce_8: 0.2489  loss_mask_8: 0.3487  loss_dice_8: 2.684  time: 1.4848  data_time: 0.0703  lr: 3.657e-06  max_mem: 21589M
[01/18 07:14:35] d2.utils.events INFO:  eta: 5:15:11  iter: 26939  total_loss: 33.85  loss_ce: 0.235  loss_mask: 0.3433  loss_dice: 2.746  loss_ce_0: 0.5444  loss_mask_0: 0.3407  loss_dice_0: 2.889  loss_ce_1: 0.2889  loss_mask_1: 0.3515  loss_dice_1: 2.807  loss_ce_2: 0.2839  loss_mask_2: 0.3457  loss_dice_2: 2.787  loss_ce_3: 0.2469  loss_mask_3: 0.3478  loss_dice_3: 2.761  loss_ce_4: 0.2491  loss_mask_4: 0.3452  loss_dice_4: 2.753  loss_ce_5: 0.243  loss_mask_5: 0.345  loss_dice_5: 2.761  loss_ce_6: 0.2426  loss_mask_6: 0.3431  loss_dice_6: 2.757  loss_ce_7: 0.2245  loss_mask_7: 0.3432  loss_dice_7: 2.749  loss_ce_8: 0.2252  loss_mask_8: 0.343  loss_dice_8: 2.755  time: 1.4848  data_time: 0.0675  lr: 3.6519e-06  max_mem: 21589M
[01/18 07:15:04] d2.utils.events INFO:  eta: 5:14:47  iter: 26959  total_loss: 33.55  loss_ce: 0.2493  loss_mask: 0.35  loss_dice: 2.709  loss_ce_0: 0.5881  loss_mask_0: 0.3498  loss_dice_0: 2.829  loss_ce_1: 0.2887  loss_mask_1: 0.3562  loss_dice_1: 2.745  loss_ce_2: 0.29  loss_mask_2: 0.3506  loss_dice_2: 2.728  loss_ce_3: 0.2765  loss_mask_3: 0.3499  loss_dice_3: 2.713  loss_ce_4: 0.25  loss_mask_4: 0.3501  loss_dice_4: 2.707  loss_ce_5: 0.2448  loss_mask_5: 0.3509  loss_dice_5: 2.715  loss_ce_6: 0.2399  loss_mask_6: 0.3495  loss_dice_6: 2.703  loss_ce_7: 0.2503  loss_mask_7: 0.3505  loss_dice_7: 2.714  loss_ce_8: 0.2456  loss_mask_8: 0.3498  loss_dice_8: 2.699  time: 1.4848  data_time: 0.0666  lr: 3.6469e-06  max_mem: 21589M
[01/18 07:15:33] d2.utils.events INFO:  eta: 5:14:24  iter: 26979  total_loss: 33.55  loss_ce: 0.2451  loss_mask: 0.3493  loss_dice: 2.675  loss_ce_0: 0.5688  loss_mask_0: 0.3437  loss_dice_0: 2.816  loss_ce_1: 0.2831  loss_mask_1: 0.3563  loss_dice_1: 2.716  loss_ce_2: 0.2864  loss_mask_2: 0.3516  loss_dice_2: 2.698  loss_ce_3: 0.2607  loss_mask_3: 0.3476  loss_dice_3: 2.689  loss_ce_4: 0.2653  loss_mask_4: 0.3477  loss_dice_4: 2.685  loss_ce_5: 0.2459  loss_mask_5: 0.3477  loss_dice_5: 2.69  loss_ce_6: 0.2514  loss_mask_6: 0.3484  loss_dice_6: 2.675  loss_ce_7: 0.2416  loss_mask_7: 0.3499  loss_dice_7: 2.682  loss_ce_8: 0.2425  loss_mask_8: 0.3488  loss_dice_8: 2.68  time: 1.4848  data_time: 0.0683  lr: 3.6419e-06  max_mem: 21589M
[01/18 07:16:02] d2.utils.events INFO:  eta: 5:13:52  iter: 26999  total_loss: 33.84  loss_ce: 0.2549  loss_mask: 0.3496  loss_dice: 2.739  loss_ce_0: 0.5232  loss_mask_0: 0.3493  loss_dice_0: 2.866  loss_ce_1: 0.2846  loss_mask_1: 0.3571  loss_dice_1: 2.774  loss_ce_2: 0.278  loss_mask_2: 0.3534  loss_dice_2: 2.76  loss_ce_3: 0.2542  loss_mask_3: 0.3501  loss_dice_3: 2.746  loss_ce_4: 0.2574  loss_mask_4: 0.3501  loss_dice_4: 2.741  loss_ce_5: 0.2542  loss_mask_5: 0.3504  loss_dice_5: 2.746  loss_ce_6: 0.2718  loss_mask_6: 0.351  loss_dice_6: 2.736  loss_ce_7: 0.2629  loss_mask_7: 0.3495  loss_dice_7: 2.74  loss_ce_8: 0.2608  loss_mask_8: 0.3488  loss_dice_8: 2.737  time: 1.4847  data_time: 0.0691  lr: 3.6368e-06  max_mem: 21589M
[01/18 07:16:31] d2.utils.events INFO:  eta: 5:13:23  iter: 27019  total_loss: 33.49  loss_ce: 0.2486  loss_mask: 0.3474  loss_dice: 2.664  loss_ce_0: 0.5282  loss_mask_0: 0.3438  loss_dice_0: 2.822  loss_ce_1: 0.2995  loss_mask_1: 0.352  loss_dice_1: 2.724  loss_ce_2: 0.2822  loss_mask_2: 0.352  loss_dice_2: 2.701  loss_ce_3: 0.2772  loss_mask_3: 0.3493  loss_dice_3: 2.678  loss_ce_4: 0.2639  loss_mask_4: 0.3474  loss_dice_4: 2.681  loss_ce_5: 0.2525  loss_mask_5: 0.3478  loss_dice_5: 2.675  loss_ce_6: 0.2464  loss_mask_6: 0.349  loss_dice_6: 2.672  loss_ce_7: 0.238  loss_mask_7: 0.3476  loss_dice_7: 2.674  loss_ce_8: 0.2471  loss_mask_8: 0.3478  loss_dice_8: 2.675  time: 1.4847  data_time: 0.0675  lr: 3.6318e-06  max_mem: 21589M
[01/18 07:17:00] d2.utils.events INFO:  eta: 5:12:54  iter: 27039  total_loss: 33.73  loss_ce: 0.2563  loss_mask: 0.3451  loss_dice: 2.68  loss_ce_0: 0.5445  loss_mask_0: 0.3438  loss_dice_0: 2.822  loss_ce_1: 0.3133  loss_mask_1: 0.3526  loss_dice_1: 2.721  loss_ce_2: 0.2993  loss_mask_2: 0.35  loss_dice_2: 2.694  loss_ce_3: 0.2843  loss_mask_3: 0.3466  loss_dice_3: 2.679  loss_ce_4: 0.2649  loss_mask_4: 0.3476  loss_dice_4: 2.686  loss_ce_5: 0.273  loss_mask_5: 0.346  loss_dice_5: 2.678  loss_ce_6: 0.2667  loss_mask_6: 0.3459  loss_dice_6: 2.676  loss_ce_7: 0.2716  loss_mask_7: 0.3449  loss_dice_7: 2.678  loss_ce_8: 0.2598  loss_mask_8: 0.3442  loss_dice_8: 2.674  time: 1.4847  data_time: 0.0698  lr: 3.6268e-06  max_mem: 21589M
[01/18 07:17:29] d2.utils.events INFO:  eta: 5:12:25  iter: 27059  total_loss: 32.93  loss_ce: 0.2427  loss_mask: 0.3391  loss_dice: 2.643  loss_ce_0: 0.5495  loss_mask_0: 0.3413  loss_dice_0: 2.768  loss_ce_1: 0.2709  loss_mask_1: 0.3465  loss_dice_1: 2.676  loss_ce_2: 0.2821  loss_mask_2: 0.3427  loss_dice_2: 2.66  loss_ce_3: 0.2686  loss_mask_3: 0.3414  loss_dice_3: 2.649  loss_ce_4: 0.2543  loss_mask_4: 0.3399  loss_dice_4: 2.642  loss_ce_5: 0.2463  loss_mask_5: 0.338  loss_dice_5: 2.648  loss_ce_6: 0.2486  loss_mask_6: 0.336  loss_dice_6: 2.647  loss_ce_7: 0.2441  loss_mask_7: 0.3394  loss_dice_7: 2.635  loss_ce_8: 0.2422  loss_mask_8: 0.3398  loss_dice_8: 2.644  time: 1.4846  data_time: 0.0704  lr: 3.6217e-06  max_mem: 21589M
[01/18 07:17:58] d2.utils.events INFO:  eta: 5:11:56  iter: 27079  total_loss: 33.7  loss_ce: 0.2478  loss_mask: 0.3582  loss_dice: 2.687  loss_ce_0: 0.5381  loss_mask_0: 0.3577  loss_dice_0: 2.822  loss_ce_1: 0.2844  loss_mask_1: 0.366  loss_dice_1: 2.736  loss_ce_2: 0.2823  loss_mask_2: 0.3619  loss_dice_2: 2.709  loss_ce_3: 0.2651  loss_mask_3: 0.3592  loss_dice_3: 2.693  loss_ce_4: 0.2593  loss_mask_4: 0.358  loss_dice_4: 2.691  loss_ce_5: 0.2427  loss_mask_5: 0.3571  loss_dice_5: 2.692  loss_ce_6: 0.2375  loss_mask_6: 0.3578  loss_dice_6: 2.689  loss_ce_7: 0.2455  loss_mask_7: 0.3565  loss_dice_7: 2.69  loss_ce_8: 0.2366  loss_mask_8: 0.3572  loss_dice_8: 2.693  time: 1.4846  data_time: 0.0692  lr: 3.6167e-06  max_mem: 21589M
[01/18 07:18:28] d2.utils.events INFO:  eta: 5:11:34  iter: 27099  total_loss: 33.32  loss_ce: 0.2707  loss_mask: 0.3459  loss_dice: 2.694  loss_ce_0: 0.5603  loss_mask_0: 0.3438  loss_dice_0: 2.818  loss_ce_1: 0.3215  loss_mask_1: 0.3538  loss_dice_1: 2.724  loss_ce_2: 0.2931  loss_mask_2: 0.3504  loss_dice_2: 2.711  loss_ce_3: 0.2932  loss_mask_3: 0.3462  loss_dice_3: 2.703  loss_ce_4: 0.271  loss_mask_4: 0.3467  loss_dice_4: 2.696  loss_ce_5: 0.2734  loss_mask_5: 0.3451  loss_dice_5: 2.695  loss_ce_6: 0.268  loss_mask_6: 0.3458  loss_dice_6: 2.692  loss_ce_7: 0.273  loss_mask_7: 0.3466  loss_dice_7: 2.696  loss_ce_8: 0.2627  loss_mask_8: 0.3449  loss_dice_8: 2.696  time: 1.4846  data_time: 0.0764  lr: 3.6117e-06  max_mem: 21589M
[01/18 07:18:57] d2.utils.events INFO:  eta: 5:11:10  iter: 27119  total_loss: 33.26  loss_ce: 0.2507  loss_mask: 0.3413  loss_dice: 2.644  loss_ce_0: 0.5643  loss_mask_0: 0.3399  loss_dice_0: 2.777  loss_ce_1: 0.2966  loss_mask_1: 0.3496  loss_dice_1: 2.696  loss_ce_2: 0.2782  loss_mask_2: 0.3455  loss_dice_2: 2.668  loss_ce_3: 0.2691  loss_mask_3: 0.3421  loss_dice_3: 2.654  loss_ce_4: 0.2454  loss_mask_4: 0.3423  loss_dice_4: 2.645  loss_ce_5: 0.2523  loss_mask_5: 0.342  loss_dice_5: 2.65  loss_ce_6: 0.2554  loss_mask_6: 0.3418  loss_dice_6: 2.643  loss_ce_7: 0.2402  loss_mask_7: 0.3415  loss_dice_7: 2.641  loss_ce_8: 0.2523  loss_mask_8: 0.3405  loss_dice_8: 2.649  time: 1.4846  data_time: 0.0701  lr: 3.6066e-06  max_mem: 21589M
[01/18 07:19:27] d2.utils.events INFO:  eta: 5:10:46  iter: 27139  total_loss: 33.88  loss_ce: 0.2549  loss_mask: 0.352  loss_dice: 2.706  loss_ce_0: 0.5808  loss_mask_0: 0.3531  loss_dice_0: 2.85  loss_ce_1: 0.2997  loss_mask_1: 0.3566  loss_dice_1: 2.753  loss_ce_2: 0.2955  loss_mask_2: 0.3559  loss_dice_2: 2.734  loss_ce_3: 0.2767  loss_mask_3: 0.3529  loss_dice_3: 2.723  loss_ce_4: 0.2696  loss_mask_4: 0.351  loss_dice_4: 2.722  loss_ce_5: 0.2763  loss_mask_5: 0.3506  loss_dice_5: 2.725  loss_ce_6: 0.2572  loss_mask_6: 0.3523  loss_dice_6: 2.712  loss_ce_7: 0.2509  loss_mask_7: 0.3521  loss_dice_7: 2.714  loss_ce_8: 0.2438  loss_mask_8: 0.3525  loss_dice_8: 2.716  time: 1.4846  data_time: 0.0771  lr: 3.6016e-06  max_mem: 21589M
[01/18 07:19:57] d2.utils.events INFO:  eta: 5:10:19  iter: 27159  total_loss: 33.57  loss_ce: 0.2429  loss_mask: 0.3544  loss_dice: 2.703  loss_ce_0: 0.5585  loss_mask_0: 0.3463  loss_dice_0: 2.836  loss_ce_1: 0.2876  loss_mask_1: 0.3547  loss_dice_1: 2.75  loss_ce_2: 0.277  loss_mask_2: 0.3526  loss_dice_2: 2.721  loss_ce_3: 0.2665  loss_mask_3: 0.3524  loss_dice_3: 2.714  loss_ce_4: 0.2598  loss_mask_4: 0.3525  loss_dice_4: 2.706  loss_ce_5: 0.2657  loss_mask_5: 0.3526  loss_dice_5: 2.712  loss_ce_6: 0.2503  loss_mask_6: 0.3521  loss_dice_6: 2.71  loss_ce_7: 0.2421  loss_mask_7: 0.3527  loss_dice_7: 2.709  loss_ce_8: 0.2364  loss_mask_8: 0.3533  loss_dice_8: 2.709  time: 1.4846  data_time: 0.0746  lr: 3.5965e-06  max_mem: 21589M
[01/18 07:20:26] d2.utils.events INFO:  eta: 5:09:49  iter: 27179  total_loss: 33.48  loss_ce: 0.2541  loss_mask: 0.3513  loss_dice: 2.676  loss_ce_0: 0.5303  loss_mask_0: 0.3508  loss_dice_0: 2.807  loss_ce_1: 0.296  loss_mask_1: 0.3619  loss_dice_1: 2.716  loss_ce_2: 0.2896  loss_mask_2: 0.3564  loss_dice_2: 2.689  loss_ce_3: 0.2633  loss_mask_3: 0.3557  loss_dice_3: 2.681  loss_ce_4: 0.2518  loss_mask_4: 0.3534  loss_dice_4: 2.681  loss_ce_5: 0.2541  loss_mask_5: 0.3532  loss_dice_5: 2.672  loss_ce_6: 0.2483  loss_mask_6: 0.3522  loss_dice_6: 2.679  loss_ce_7: 0.2374  loss_mask_7: 0.3541  loss_dice_7: 2.678  loss_ce_8: 0.2384  loss_mask_8: 0.3528  loss_dice_8: 2.672  time: 1.4846  data_time: 0.0695  lr: 3.5915e-06  max_mem: 21589M
[01/18 07:20:56] d2.utils.events INFO:  eta: 5:09:29  iter: 27199  total_loss: 33.96  loss_ce: 0.2384  loss_mask: 0.3524  loss_dice: 2.701  loss_ce_0: 0.5752  loss_mask_0: 0.3558  loss_dice_0: 2.843  loss_ce_1: 0.3015  loss_mask_1: 0.3573  loss_dice_1: 2.75  loss_ce_2: 0.2877  loss_mask_2: 0.355  loss_dice_2: 2.729  loss_ce_3: 0.2716  loss_mask_3: 0.3536  loss_dice_3: 2.711  loss_ce_4: 0.2524  loss_mask_4: 0.352  loss_dice_4: 2.714  loss_ce_5: 0.2649  loss_mask_5: 0.3529  loss_dice_5: 2.706  loss_ce_6: 0.259  loss_mask_6: 0.3537  loss_dice_6: 2.702  loss_ce_7: 0.2445  loss_mask_7: 0.3538  loss_dice_7: 2.709  loss_ce_8: 0.258  loss_mask_8: 0.3527  loss_dice_8: 2.71  time: 1.4846  data_time: 0.0723  lr: 3.5865e-06  max_mem: 21589M
[01/18 07:21:26] d2.utils.events INFO:  eta: 5:09:09  iter: 27219  total_loss: 33.86  loss_ce: 0.2366  loss_mask: 0.3425  loss_dice: 2.687  loss_ce_0: 0.5528  loss_mask_0: 0.341  loss_dice_0: 2.819  loss_ce_1: 0.2817  loss_mask_1: 0.345  loss_dice_1: 2.733  loss_ce_2: 0.2732  loss_mask_2: 0.344  loss_dice_2: 2.707  loss_ce_3: 0.252  loss_mask_3: 0.342  loss_dice_3: 2.697  loss_ce_4: 0.2416  loss_mask_4: 0.3419  loss_dice_4: 2.695  loss_ce_5: 0.2473  loss_mask_5: 0.3436  loss_dice_5: 2.684  loss_ce_6: 0.2339  loss_mask_6: 0.3432  loss_dice_6: 2.689  loss_ce_7: 0.2352  loss_mask_7: 0.3425  loss_dice_7: 2.685  loss_ce_8: 0.2376  loss_mask_8: 0.3429  loss_dice_8: 2.694  time: 1.4846  data_time: 0.0769  lr: 3.5814e-06  max_mem: 21589M
[01/18 07:21:54] d2.utils.events INFO:  eta: 5:08:31  iter: 27239  total_loss: 33.35  loss_ce: 0.2487  loss_mask: 0.3477  loss_dice: 2.698  loss_ce_0: 0.5638  loss_mask_0: 0.3483  loss_dice_0: 2.819  loss_ce_1: 0.2899  loss_mask_1: 0.3558  loss_dice_1: 2.74  loss_ce_2: 0.2789  loss_mask_2: 0.3521  loss_dice_2: 2.719  loss_ce_3: 0.262  loss_mask_3: 0.3491  loss_dice_3: 2.706  loss_ce_4: 0.2592  loss_mask_4: 0.3474  loss_dice_4: 2.705  loss_ce_5: 0.2648  loss_mask_5: 0.3446  loss_dice_5: 2.701  loss_ce_6: 0.2546  loss_mask_6: 0.3447  loss_dice_6: 2.703  loss_ce_7: 0.2363  loss_mask_7: 0.3465  loss_dice_7: 2.7  loss_ce_8: 0.2544  loss_mask_8: 0.3471  loss_dice_8: 2.704  time: 1.4845  data_time: 0.0674  lr: 3.5764e-06  max_mem: 21589M
[01/18 07:22:23] d2.utils.events INFO:  eta: 5:08:02  iter: 27259  total_loss: 33.58  loss_ce: 0.2584  loss_mask: 0.3464  loss_dice: 2.722  loss_ce_0: 0.569  loss_mask_0: 0.3431  loss_dice_0: 2.846  loss_ce_1: 0.3252  loss_mask_1: 0.3462  loss_dice_1: 2.765  loss_ce_2: 0.3032  loss_mask_2: 0.3452  loss_dice_2: 2.742  loss_ce_3: 0.2707  loss_mask_3: 0.3431  loss_dice_3: 2.723  loss_ce_4: 0.2726  loss_mask_4: 0.3434  loss_dice_4: 2.718  loss_ce_5: 0.2756  loss_mask_5: 0.3433  loss_dice_5: 2.718  loss_ce_6: 0.2604  loss_mask_6: 0.3443  loss_dice_6: 2.713  loss_ce_7: 0.2608  loss_mask_7: 0.3428  loss_dice_7: 2.72  loss_ce_8: 0.2536  loss_mask_8: 0.3456  loss_dice_8: 2.715  time: 1.4845  data_time: 0.0654  lr: 3.5713e-06  max_mem: 21589M
[01/18 07:22:52] d2.utils.events INFO:  eta: 5:07:33  iter: 27279  total_loss: 33.52  loss_ce: 0.2392  loss_mask: 0.3438  loss_dice: 2.706  loss_ce_0: 0.5253  loss_mask_0: 0.3402  loss_dice_0: 2.832  loss_ce_1: 0.2951  loss_mask_1: 0.3486  loss_dice_1: 2.763  loss_ce_2: 0.2792  loss_mask_2: 0.3461  loss_dice_2: 2.722  loss_ce_3: 0.2575  loss_mask_3: 0.3435  loss_dice_3: 2.716  loss_ce_4: 0.2651  loss_mask_4: 0.3403  loss_dice_4: 2.714  loss_ce_5: 0.2421  loss_mask_5: 0.3416  loss_dice_5: 2.714  loss_ce_6: 0.2386  loss_mask_6: 0.3413  loss_dice_6: 2.709  loss_ce_7: 0.2425  loss_mask_7: 0.3406  loss_dice_7: 2.701  loss_ce_8: 0.2381  loss_mask_8: 0.3424  loss_dice_8: 2.706  time: 1.4845  data_time: 0.0662  lr: 3.5663e-06  max_mem: 21589M
[01/18 07:23:21] d2.utils.events INFO:  eta: 5:07:04  iter: 27299  total_loss: 33.18  loss_ce: 0.2645  loss_mask: 0.3499  loss_dice: 2.669  loss_ce_0: 0.5363  loss_mask_0: 0.3403  loss_dice_0: 2.795  loss_ce_1: 0.289  loss_mask_1: 0.3543  loss_dice_1: 2.705  loss_ce_2: 0.2929  loss_mask_2: 0.3522  loss_dice_2: 2.677  loss_ce_3: 0.2667  loss_mask_3: 0.3506  loss_dice_3: 2.668  loss_ce_4: 0.2702  loss_mask_4: 0.3505  loss_dice_4: 2.668  loss_ce_5: 0.2718  loss_mask_5: 0.3515  loss_dice_5: 2.654  loss_ce_6: 0.2538  loss_mask_6: 0.3504  loss_dice_6: 2.657  loss_ce_7: 0.2559  loss_mask_7: 0.351  loss_dice_7: 2.666  loss_ce_8: 0.2545  loss_mask_8: 0.3504  loss_dice_8: 2.663  time: 1.4845  data_time: 0.0657  lr: 3.5612e-06  max_mem: 21589M
[01/18 07:23:51] d2.utils.events INFO:  eta: 5:06:35  iter: 27319  total_loss: 34.8  loss_ce: 0.2565  loss_mask: 0.3517  loss_dice: 2.754  loss_ce_0: 0.5795  loss_mask_0: 0.3519  loss_dice_0: 2.884  loss_ce_1: 0.302  loss_mask_1: 0.3553  loss_dice_1: 2.805  loss_ce_2: 0.2974  loss_mask_2: 0.3535  loss_dice_2: 2.78  loss_ce_3: 0.2749  loss_mask_3: 0.3509  loss_dice_3: 2.764  loss_ce_4: 0.2603  loss_mask_4: 0.3507  loss_dice_4: 2.765  loss_ce_5: 0.272  loss_mask_5: 0.35  loss_dice_5: 2.759  loss_ce_6: 0.2738  loss_mask_6: 0.3499  loss_dice_6: 2.754  loss_ce_7: 0.267  loss_mask_7: 0.3496  loss_dice_7: 2.771  loss_ce_8: 0.2582  loss_mask_8: 0.3522  loss_dice_8: 2.763  time: 1.4844  data_time: 0.0682  lr: 3.5562e-06  max_mem: 21589M
[01/18 07:24:19] d2.utils.events INFO:  eta: 5:05:57  iter: 27339  total_loss: 34.29  loss_ce: 0.2706  loss_mask: 0.3561  loss_dice: 2.72  loss_ce_0: 0.5695  loss_mask_0: 0.3625  loss_dice_0: 2.841  loss_ce_1: 0.3008  loss_mask_1: 0.3687  loss_dice_1: 2.761  loss_ce_2: 0.3071  loss_mask_2: 0.3624  loss_dice_2: 2.746  loss_ce_3: 0.2784  loss_mask_3: 0.3587  loss_dice_3: 2.722  loss_ce_4: 0.2695  loss_mask_4: 0.3576  loss_dice_4: 2.724  loss_ce_5: 0.2707  loss_mask_5: 0.357  loss_dice_5: 2.72  loss_ce_6: 0.2637  loss_mask_6: 0.3573  loss_dice_6: 2.717  loss_ce_7: 0.2568  loss_mask_7: 0.3585  loss_dice_7: 2.719  loss_ce_8: 0.2629  loss_mask_8: 0.3568  loss_dice_8: 2.722  time: 1.4844  data_time: 0.0730  lr: 3.5511e-06  max_mem: 21589M
[01/18 07:24:49] d2.utils.events INFO:  eta: 5:05:33  iter: 27359  total_loss: 33.67  loss_ce: 0.2623  loss_mask: 0.3494  loss_dice: 2.688  loss_ce_0: 0.5725  loss_mask_0: 0.3487  loss_dice_0: 2.815  loss_ce_1: 0.307  loss_mask_1: 0.353  loss_dice_1: 2.722  loss_ce_2: 0.2981  loss_mask_2: 0.3528  loss_dice_2: 2.7  loss_ce_3: 0.2889  loss_mask_3: 0.3529  loss_dice_3: 2.692  loss_ce_4: 0.2773  loss_mask_4: 0.3507  loss_dice_4: 2.695  loss_ce_5: 0.2619  loss_mask_5: 0.3508  loss_dice_5: 2.699  loss_ce_6: 0.2729  loss_mask_6: 0.3496  loss_dice_6: 2.691  loss_ce_7: 0.2588  loss_mask_7: 0.35  loss_dice_7: 2.693  loss_ce_8: 0.2527  loss_mask_8: 0.3503  loss_dice_8: 2.687  time: 1.4844  data_time: 0.0705  lr: 3.5461e-06  max_mem: 21589M
[01/18 07:25:18] d2.utils.events INFO:  eta: 5:05:13  iter: 27379  total_loss: 33.39  loss_ce: 0.2598  loss_mask: 0.3444  loss_dice: 2.714  loss_ce_0: 0.5575  loss_mask_0: 0.3492  loss_dice_0: 2.837  loss_ce_1: 0.2873  loss_mask_1: 0.3565  loss_dice_1: 2.756  loss_ce_2: 0.2964  loss_mask_2: 0.3526  loss_dice_2: 2.73  loss_ce_3: 0.2731  loss_mask_3: 0.3474  loss_dice_3: 2.716  loss_ce_4: 0.2593  loss_mask_4: 0.3466  loss_dice_4: 2.726  loss_ce_5: 0.2549  loss_mask_5: 0.3457  loss_dice_5: 2.72  loss_ce_6: 0.249  loss_mask_6: 0.3438  loss_dice_6: 2.712  loss_ce_7: 0.2567  loss_mask_7: 0.3442  loss_dice_7: 2.714  loss_ce_8: 0.2486  loss_mask_8: 0.3446  loss_dice_8: 2.712  time: 1.4844  data_time: 0.0720  lr: 3.541e-06  max_mem: 21589M
[01/18 07:25:47] d2.utils.events INFO:  eta: 5:04:35  iter: 27399  total_loss: 33.56  loss_ce: 0.2723  loss_mask: 0.3449  loss_dice: 2.682  loss_ce_0: 0.5782  loss_mask_0: 0.3426  loss_dice_0: 2.839  loss_ce_1: 0.3068  loss_mask_1: 0.3518  loss_dice_1: 2.742  loss_ce_2: 0.2919  loss_mask_2: 0.3475  loss_dice_2: 2.716  loss_ce_3: 0.2824  loss_mask_3: 0.3457  loss_dice_3: 2.696  loss_ce_4: 0.2628  loss_mask_4: 0.3448  loss_dice_4: 2.69  loss_ce_5: 0.2748  loss_mask_5: 0.3455  loss_dice_5: 2.69  loss_ce_6: 0.2589  loss_mask_6: 0.3454  loss_dice_6: 2.678  loss_ce_7: 0.2584  loss_mask_7: 0.3453  loss_dice_7: 2.691  loss_ce_8: 0.2695  loss_mask_8: 0.345  loss_dice_8: 2.684  time: 1.4843  data_time: 0.0698  lr: 3.536e-06  max_mem: 21589M
[01/18 07:26:16] d2.utils.events INFO:  eta: 5:04:10  iter: 27419  total_loss: 33.61  loss_ce: 0.2426  loss_mask: 0.3511  loss_dice: 2.739  loss_ce_0: 0.5683  loss_mask_0: 0.3496  loss_dice_0: 2.876  loss_ce_1: 0.2943  loss_mask_1: 0.3586  loss_dice_1: 2.784  loss_ce_2: 0.2916  loss_mask_2: 0.3553  loss_dice_2: 2.759  loss_ce_3: 0.2624  loss_mask_3: 0.3522  loss_dice_3: 2.741  loss_ce_4: 0.2501  loss_mask_4: 0.3544  loss_dice_4: 2.733  loss_ce_5: 0.2575  loss_mask_5: 0.3519  loss_dice_5: 2.733  loss_ce_6: 0.2352  loss_mask_6: 0.352  loss_dice_6: 2.741  loss_ce_7: 0.2367  loss_mask_7: 0.3511  loss_dice_7: 2.748  loss_ce_8: 0.2427  loss_mask_8: 0.3506  loss_dice_8: 2.737  time: 1.4843  data_time: 0.0706  lr: 3.5309e-06  max_mem: 21589M
[01/18 07:26:45] d2.utils.events INFO:  eta: 5:03:53  iter: 27439  total_loss: 33.17  loss_ce: 0.2615  loss_mask: 0.3552  loss_dice: 2.653  loss_ce_0: 0.5847  loss_mask_0: 0.3511  loss_dice_0: 2.78  loss_ce_1: 0.3085  loss_mask_1: 0.3598  loss_dice_1: 2.7  loss_ce_2: 0.292  loss_mask_2: 0.3565  loss_dice_2: 2.675  loss_ce_3: 0.2725  loss_mask_3: 0.3559  loss_dice_3: 2.665  loss_ce_4: 0.2688  loss_mask_4: 0.3555  loss_dice_4: 2.665  loss_ce_5: 0.2804  loss_mask_5: 0.3547  loss_dice_5: 2.662  loss_ce_6: 0.2706  loss_mask_6: 0.3557  loss_dice_6: 2.657  loss_ce_7: 0.2709  loss_mask_7: 0.3557  loss_dice_7: 2.659  loss_ce_8: 0.2512  loss_mask_8: 0.356  loss_dice_8: 2.662  time: 1.4843  data_time: 0.0717  lr: 3.5259e-06  max_mem: 21589M
[01/18 07:27:14] d2.utils.events INFO:  eta: 5:03:29  iter: 27459  total_loss: 33.34  loss_ce: 0.2484  loss_mask: 0.3446  loss_dice: 2.676  loss_ce_0: 0.5602  loss_mask_0: 0.3447  loss_dice_0: 2.803  loss_ce_1: 0.284  loss_mask_1: 0.3517  loss_dice_1: 2.717  loss_ce_2: 0.2782  loss_mask_2: 0.3466  loss_dice_2: 2.693  loss_ce_3: 0.2675  loss_mask_3: 0.3468  loss_dice_3: 2.687  loss_ce_4: 0.2547  loss_mask_4: 0.345  loss_dice_4: 2.685  loss_ce_5: 0.2552  loss_mask_5: 0.3453  loss_dice_5: 2.68  loss_ce_6: 0.2499  loss_mask_6: 0.3447  loss_dice_6: 2.67  loss_ce_7: 0.244  loss_mask_7: 0.3446  loss_dice_7: 2.673  loss_ce_8: 0.2468  loss_mask_8: 0.3451  loss_dice_8: 2.668  time: 1.4843  data_time: 0.0679  lr: 3.5208e-06  max_mem: 21589M
[01/18 07:27:44] d2.utils.events INFO:  eta: 5:03:05  iter: 27479  total_loss: 33.03  loss_ce: 0.2366  loss_mask: 0.3443  loss_dice: 2.644  loss_ce_0: 0.5473  loss_mask_0: 0.3423  loss_dice_0: 2.793  loss_ce_1: 0.2795  loss_mask_1: 0.3492  loss_dice_1: 2.696  loss_ce_2: 0.2636  loss_mask_2: 0.3478  loss_dice_2: 2.682  loss_ce_3: 0.2583  loss_mask_3: 0.3436  loss_dice_3: 2.664  loss_ce_4: 0.2546  loss_mask_4: 0.3454  loss_dice_4: 2.655  loss_ce_5: 0.2417  loss_mask_5: 0.3453  loss_dice_5: 2.645  loss_ce_6: 0.2304  loss_mask_6: 0.3437  loss_dice_6: 2.654  loss_ce_7: 0.238  loss_mask_7: 0.3435  loss_dice_7: 2.656  loss_ce_8: 0.2323  loss_mask_8: 0.3444  loss_dice_8: 2.647  time: 1.4843  data_time: 0.0684  lr: 3.5158e-06  max_mem: 21589M
[01/18 07:28:12] d2.utils.events INFO:  eta: 5:02:19  iter: 27499  total_loss: 33.24  loss_ce: 0.2493  loss_mask: 0.3414  loss_dice: 2.647  loss_ce_0: 0.5601  loss_mask_0: 0.3442  loss_dice_0: 2.788  loss_ce_1: 0.2925  loss_mask_1: 0.3469  loss_dice_1: 2.702  loss_ce_2: 0.2928  loss_mask_2: 0.3431  loss_dice_2: 2.67  loss_ce_3: 0.2684  loss_mask_3: 0.3427  loss_dice_3: 2.662  loss_ce_4: 0.268  loss_mask_4: 0.3419  loss_dice_4: 2.658  loss_ce_5: 0.2621  loss_mask_5: 0.3428  loss_dice_5: 2.667  loss_ce_6: 0.2668  loss_mask_6: 0.342  loss_dice_6: 2.659  loss_ce_7: 0.2576  loss_mask_7: 0.342  loss_dice_7: 2.656  loss_ce_8: 0.2525  loss_mask_8: 0.3412  loss_dice_8: 2.649  time: 1.4842  data_time: 0.0676  lr: 3.5107e-06  max_mem: 21589M
[01/18 07:28:42] d2.utils.events INFO:  eta: 5:02:00  iter: 27519  total_loss: 34.62  loss_ce: 0.2463  loss_mask: 0.3411  loss_dice: 2.737  loss_ce_0: 0.5871  loss_mask_0: 0.3352  loss_dice_0: 2.88  loss_ce_1: 0.3105  loss_mask_1: 0.3429  loss_dice_1: 2.791  loss_ce_2: 0.272  loss_mask_2: 0.3441  loss_dice_2: 2.759  loss_ce_3: 0.281  loss_mask_3: 0.3396  loss_dice_3: 2.734  loss_ce_4: 0.2699  loss_mask_4: 0.3384  loss_dice_4: 2.744  loss_ce_5: 0.2565  loss_mask_5: 0.3385  loss_dice_5: 2.739  loss_ce_6: 0.2591  loss_mask_6: 0.3399  loss_dice_6: 2.728  loss_ce_7: 0.2597  loss_mask_7: 0.3395  loss_dice_7: 2.733  loss_ce_8: 0.2365  loss_mask_8: 0.34  loss_dice_8: 2.728  time: 1.4842  data_time: 0.0728  lr: 3.5057e-06  max_mem: 21589M
[01/18 07:29:11] d2.utils.events INFO:  eta: 5:01:35  iter: 27539  total_loss: 33.43  loss_ce: 0.2466  loss_mask: 0.3464  loss_dice: 2.666  loss_ce_0: 0.5301  loss_mask_0: 0.3448  loss_dice_0: 2.798  loss_ce_1: 0.2885  loss_mask_1: 0.3527  loss_dice_1: 2.709  loss_ce_2: 0.2889  loss_mask_2: 0.3522  loss_dice_2: 2.688  loss_ce_3: 0.2584  loss_mask_3: 0.3484  loss_dice_3: 2.679  loss_ce_4: 0.2538  loss_mask_4: 0.3478  loss_dice_4: 2.672  loss_ce_5: 0.2394  loss_mask_5: 0.3467  loss_dice_5: 2.675  loss_ce_6: 0.2496  loss_mask_6: 0.3461  loss_dice_6: 2.678  loss_ce_7: 0.2342  loss_mask_7: 0.3461  loss_dice_7: 2.682  loss_ce_8: 0.2475  loss_mask_8: 0.3459  loss_dice_8: 2.68  time: 1.4842  data_time: 0.0672  lr: 3.5006e-06  max_mem: 21589M
[01/18 07:29:40] d2.utils.events INFO:  eta: 5:01:02  iter: 27559  total_loss: 33.49  loss_ce: 0.2524  loss_mask: 0.3558  loss_dice: 2.695  loss_ce_0: 0.55  loss_mask_0: 0.3552  loss_dice_0: 2.813  loss_ce_1: 0.2825  loss_mask_1: 0.3614  loss_dice_1: 2.732  loss_ce_2: 0.2676  loss_mask_2: 0.3598  loss_dice_2: 2.714  loss_ce_3: 0.2581  loss_mask_3: 0.356  loss_dice_3: 2.703  loss_ce_4: 0.2488  loss_mask_4: 0.3543  loss_dice_4: 2.699  loss_ce_5: 0.2482  loss_mask_5: 0.3541  loss_dice_5: 2.706  loss_ce_6: 0.2468  loss_mask_6: 0.3527  loss_dice_6: 2.691  loss_ce_7: 0.2478  loss_mask_7: 0.3549  loss_dice_7: 2.691  loss_ce_8: 0.2425  loss_mask_8: 0.3535  loss_dice_8: 2.687  time: 1.4842  data_time: 0.0697  lr: 3.4956e-06  max_mem: 21589M
[01/18 07:30:09] d2.utils.events INFO:  eta: 5:00:33  iter: 27579  total_loss: 33.8  loss_ce: 0.2716  loss_mask: 0.3495  loss_dice: 2.686  loss_ce_0: 0.601  loss_mask_0: 0.347  loss_dice_0: 2.81  loss_ce_1: 0.3138  loss_mask_1: 0.3555  loss_dice_1: 2.726  loss_ce_2: 0.3042  loss_mask_2: 0.354  loss_dice_2: 2.707  loss_ce_3: 0.2888  loss_mask_3: 0.3521  loss_dice_3: 2.691  loss_ce_4: 0.2853  loss_mask_4: 0.3506  loss_dice_4: 2.692  loss_ce_5: 0.2652  loss_mask_5: 0.3498  loss_dice_5: 2.683  loss_ce_6: 0.2644  loss_mask_6: 0.3502  loss_dice_6: 2.695  loss_ce_7: 0.2628  loss_mask_7: 0.3487  loss_dice_7: 2.69  loss_ce_8: 0.2562  loss_mask_8: 0.3487  loss_dice_8: 2.682  time: 1.4841  data_time: 0.0694  lr: 3.4905e-06  max_mem: 21589M
[01/18 07:30:38] d2.utils.events INFO:  eta: 5:00:06  iter: 27599  total_loss: 33.58  loss_ce: 0.2587  loss_mask: 0.35  loss_dice: 2.703  loss_ce_0: 0.5736  loss_mask_0: 0.351  loss_dice_0: 2.84  loss_ce_1: 0.3096  loss_mask_1: 0.3572  loss_dice_1: 2.742  loss_ce_2: 0.3153  loss_mask_2: 0.3524  loss_dice_2: 2.721  loss_ce_3: 0.2761  loss_mask_3: 0.3516  loss_dice_3: 2.704  loss_ce_4: 0.2732  loss_mask_4: 0.3516  loss_dice_4: 2.698  loss_ce_5: 0.2752  loss_mask_5: 0.35  loss_dice_5: 2.704  loss_ce_6: 0.275  loss_mask_6: 0.3515  loss_dice_6: 2.697  loss_ce_7: 0.2614  loss_mask_7: 0.3507  loss_dice_7: 2.699  loss_ce_8: 0.2539  loss_mask_8: 0.3507  loss_dice_8: 2.699  time: 1.4841  data_time: 0.0670  lr: 3.4854e-06  max_mem: 21589M
[01/18 07:31:07] d2.utils.events INFO:  eta: 4:59:40  iter: 27619  total_loss: 34.11  loss_ce: 0.2744  loss_mask: 0.3462  loss_dice: 2.751  loss_ce_0: 0.577  loss_mask_0: 0.3464  loss_dice_0: 2.892  loss_ce_1: 0.313  loss_mask_1: 0.3514  loss_dice_1: 2.804  loss_ce_2: 0.2933  loss_mask_2: 0.3468  loss_dice_2: 2.781  loss_ce_3: 0.2725  loss_mask_3: 0.3457  loss_dice_3: 2.76  loss_ce_4: 0.267  loss_mask_4: 0.3447  loss_dice_4: 2.757  loss_ce_5: 0.2678  loss_mask_5: 0.3421  loss_dice_5: 2.754  loss_ce_6: 0.261  loss_mask_6: 0.3438  loss_dice_6: 2.753  loss_ce_7: 0.2579  loss_mask_7: 0.3448  loss_dice_7: 2.753  loss_ce_8: 0.2591  loss_mask_8: 0.345  loss_dice_8: 2.754  time: 1.4841  data_time: 0.0715  lr: 3.4804e-06  max_mem: 21589M
[01/18 07:31:37] d2.utils.events INFO:  eta: 4:59:23  iter: 27639  total_loss: 34.29  loss_ce: 0.2616  loss_mask: 0.3647  loss_dice: 2.733  loss_ce_0: 0.5725  loss_mask_0: 0.3641  loss_dice_0: 2.863  loss_ce_1: 0.3118  loss_mask_1: 0.3737  loss_dice_1: 2.775  loss_ce_2: 0.3113  loss_mask_2: 0.3701  loss_dice_2: 2.754  loss_ce_3: 0.2857  loss_mask_3: 0.3655  loss_dice_3: 2.735  loss_ce_4: 0.2662  loss_mask_4: 0.3649  loss_dice_4: 2.744  loss_ce_5: 0.2631  loss_mask_5: 0.3649  loss_dice_5: 2.739  loss_ce_6: 0.2671  loss_mask_6: 0.3651  loss_dice_6: 2.737  loss_ce_7: 0.2649  loss_mask_7: 0.3659  loss_dice_7: 2.729  loss_ce_8: 0.2643  loss_mask_8: 0.366  loss_dice_8: 2.738  time: 1.4841  data_time: 0.0739  lr: 3.4753e-06  max_mem: 21589M
[01/18 07:32:05] d2.utils.events INFO:  eta: 4:58:52  iter: 27659  total_loss: 32.98  loss_ce: 0.2491  loss_mask: 0.3383  loss_dice: 2.681  loss_ce_0: 0.5475  loss_mask_0: 0.3345  loss_dice_0: 2.815  loss_ce_1: 0.2964  loss_mask_1: 0.3412  loss_dice_1: 2.716  loss_ce_2: 0.2852  loss_mask_2: 0.3385  loss_dice_2: 2.699  loss_ce_3: 0.2627  loss_mask_3: 0.3382  loss_dice_3: 2.693  loss_ce_4: 0.2514  loss_mask_4: 0.338  loss_dice_4: 2.68  loss_ce_5: 0.242  loss_mask_5: 0.3389  loss_dice_5: 2.678  loss_ce_6: 0.253  loss_mask_6: 0.3401  loss_dice_6: 2.672  loss_ce_7: 0.238  loss_mask_7: 0.3375  loss_dice_7: 2.674  loss_ce_8: 0.2309  loss_mask_8: 0.3375  loss_dice_8: 2.679  time: 1.4840  data_time: 0.0720  lr: 3.4703e-06  max_mem: 21589M
[01/18 07:32:34] d2.utils.events INFO:  eta: 4:58:23  iter: 27679  total_loss: 33.92  loss_ce: 0.2505  loss_mask: 0.3553  loss_dice: 2.667  loss_ce_0: 0.5791  loss_mask_0: 0.3515  loss_dice_0: 2.796  loss_ce_1: 0.3217  loss_mask_1: 0.3587  loss_dice_1: 2.701  loss_ce_2: 0.2925  loss_mask_2: 0.358  loss_dice_2: 2.682  loss_ce_3: 0.2768  loss_mask_3: 0.3567  loss_dice_3: 2.669  loss_ce_4: 0.2737  loss_mask_4: 0.3549  loss_dice_4: 2.657  loss_ce_5: 0.2725  loss_mask_5: 0.3552  loss_dice_5: 2.67  loss_ce_6: 0.2544  loss_mask_6: 0.3559  loss_dice_6: 2.666  loss_ce_7: 0.2626  loss_mask_7: 0.3547  loss_dice_7: 2.662  loss_ce_8: 0.2543  loss_mask_8: 0.355  loss_dice_8: 2.668  time: 1.4840  data_time: 0.0684  lr: 3.4652e-06  max_mem: 21589M
[01/18 07:33:03] d2.utils.events INFO:  eta: 4:57:41  iter: 27699  total_loss: 33.63  loss_ce: 0.2506  loss_mask: 0.3521  loss_dice: 2.697  loss_ce_0: 0.575  loss_mask_0: 0.3623  loss_dice_0: 2.824  loss_ce_1: 0.2807  loss_mask_1: 0.3582  loss_dice_1: 2.746  loss_ce_2: 0.2768  loss_mask_2: 0.3547  loss_dice_2: 2.724  loss_ce_3: 0.2665  loss_mask_3: 0.3529  loss_dice_3: 2.694  loss_ce_4: 0.259  loss_mask_4: 0.3518  loss_dice_4: 2.697  loss_ce_5: 0.263  loss_mask_5: 0.3514  loss_dice_5: 2.693  loss_ce_6: 0.2624  loss_mask_6: 0.3504  loss_dice_6: 2.696  loss_ce_7: 0.2602  loss_mask_7: 0.3526  loss_dice_7: 2.687  loss_ce_8: 0.2524  loss_mask_8: 0.3533  loss_dice_8: 2.696  time: 1.4840  data_time: 0.0719  lr: 3.4601e-06  max_mem: 21589M
[01/18 07:33:32] d2.utils.events INFO:  eta: 4:57:13  iter: 27719  total_loss: 33.18  loss_ce: 0.236  loss_mask: 0.3406  loss_dice: 2.645  loss_ce_0: 0.5512  loss_mask_0: 0.3416  loss_dice_0: 2.787  loss_ce_1: 0.2785  loss_mask_1: 0.3477  loss_dice_1: 2.696  loss_ce_2: 0.2835  loss_mask_2: 0.3447  loss_dice_2: 2.678  loss_ce_3: 0.259  loss_mask_3: 0.3443  loss_dice_3: 2.659  loss_ce_4: 0.248  loss_mask_4: 0.3422  loss_dice_4: 2.658  loss_ce_5: 0.2608  loss_mask_5: 0.3407  loss_dice_5: 2.645  loss_ce_6: 0.2375  loss_mask_6: 0.3416  loss_dice_6: 2.648  loss_ce_7: 0.2361  loss_mask_7: 0.342  loss_dice_7: 2.642  loss_ce_8: 0.2395  loss_mask_8: 0.3416  loss_dice_8: 2.642  time: 1.4840  data_time: 0.0734  lr: 3.4551e-06  max_mem: 21589M
[01/18 07:34:02] d2.utils.events INFO:  eta: 4:56:55  iter: 27739  total_loss: 34.16  loss_ce: 0.2343  loss_mask: 0.3462  loss_dice: 2.753  loss_ce_0: 0.5539  loss_mask_0: 0.3465  loss_dice_0: 2.861  loss_ce_1: 0.2823  loss_mask_1: 0.348  loss_dice_1: 2.796  loss_ce_2: 0.2611  loss_mask_2: 0.3476  loss_dice_2: 2.766  loss_ce_3: 0.2631  loss_mask_3: 0.3452  loss_dice_3: 2.743  loss_ce_4: 0.244  loss_mask_4: 0.3454  loss_dice_4: 2.748  loss_ce_5: 0.2421  loss_mask_5: 0.3471  loss_dice_5: 2.756  loss_ce_6: 0.2556  loss_mask_6: 0.3464  loss_dice_6: 2.741  loss_ce_7: 0.2427  loss_mask_7: 0.346  loss_dice_7: 2.739  loss_ce_8: 0.234  loss_mask_8: 0.3463  loss_dice_8: 2.739  time: 1.4840  data_time: 0.0750  lr: 3.45e-06  max_mem: 21589M
[01/18 07:34:31] d2.utils.events INFO:  eta: 4:56:26  iter: 27759  total_loss: 34.24  loss_ce: 0.2544  loss_mask: 0.3507  loss_dice: 2.74  loss_ce_0: 0.5613  loss_mask_0: 0.3523  loss_dice_0: 2.874  loss_ce_1: 0.2901  loss_mask_1: 0.3579  loss_dice_1: 2.79  loss_ce_2: 0.2988  loss_mask_2: 0.3552  loss_dice_2: 2.774  loss_ce_3: 0.2834  loss_mask_3: 0.3521  loss_dice_3: 2.747  loss_ce_4: 0.266  loss_mask_4: 0.3531  loss_dice_4: 2.749  loss_ce_5: 0.2651  loss_mask_5: 0.3514  loss_dice_5: 2.751  loss_ce_6: 0.256  loss_mask_6: 0.3511  loss_dice_6: 2.748  loss_ce_7: 0.2655  loss_mask_7: 0.3514  loss_dice_7: 2.734  loss_ce_8: 0.2755  loss_mask_8: 0.3519  loss_dice_8: 2.744  time: 1.4839  data_time: 0.0714  lr: 3.4449e-06  max_mem: 21589M
[01/18 07:35:00] d2.utils.events INFO:  eta: 4:55:54  iter: 27779  total_loss: 34.2  loss_ce: 0.2621  loss_mask: 0.3456  loss_dice: 2.703  loss_ce_0: 0.5384  loss_mask_0: 0.3401  loss_dice_0: 2.868  loss_ce_1: 0.309  loss_mask_1: 0.3462  loss_dice_1: 2.766  loss_ce_2: 0.3072  loss_mask_2: 0.346  loss_dice_2: 2.739  loss_ce_3: 0.2802  loss_mask_3: 0.345  loss_dice_3: 2.719  loss_ce_4: 0.2846  loss_mask_4: 0.3445  loss_dice_4: 2.719  loss_ce_5: 0.278  loss_mask_5: 0.3436  loss_dice_5: 2.721  loss_ce_6: 0.2547  loss_mask_6: 0.3447  loss_dice_6: 2.708  loss_ce_7: 0.2511  loss_mask_7: 0.3447  loss_dice_7: 2.711  loss_ce_8: 0.2581  loss_mask_8: 0.3461  loss_dice_8: 2.707  time: 1.4839  data_time: 0.0703  lr: 3.4399e-06  max_mem: 21589M
[01/18 07:35:29] d2.utils.events INFO:  eta: 4:55:15  iter: 27799  total_loss: 34.14  loss_ce: 0.235  loss_mask: 0.3497  loss_dice: 2.745  loss_ce_0: 0.5326  loss_mask_0: 0.3513  loss_dice_0: 2.869  loss_ce_1: 0.2787  loss_mask_1: 0.3575  loss_dice_1: 2.783  loss_ce_2: 0.2838  loss_mask_2: 0.3529  loss_dice_2: 2.76  loss_ce_3: 0.2578  loss_mask_3: 0.3531  loss_dice_3: 2.75  loss_ce_4: 0.2632  loss_mask_4: 0.3508  loss_dice_4: 2.744  loss_ce_5: 0.2466  loss_mask_5: 0.3504  loss_dice_5: 2.747  loss_ce_6: 0.2385  loss_mask_6: 0.3501  loss_dice_6: 2.741  loss_ce_7: 0.2409  loss_mask_7: 0.3495  loss_dice_7: 2.739  loss_ce_8: 0.2472  loss_mask_8: 0.349  loss_dice_8: 2.74  time: 1.4839  data_time: 0.0667  lr: 3.4348e-06  max_mem: 21589M
[01/18 07:35:58] d2.utils.events INFO:  eta: 4:54:48  iter: 27819  total_loss: 33.84  loss_ce: 0.2544  loss_mask: 0.3536  loss_dice: 2.682  loss_ce_0: 0.5799  loss_mask_0: 0.3523  loss_dice_0: 2.812  loss_ce_1: 0.3069  loss_mask_1: 0.3578  loss_dice_1: 2.743  loss_ce_2: 0.294  loss_mask_2: 0.3536  loss_dice_2: 2.717  loss_ce_3: 0.2838  loss_mask_3: 0.3528  loss_dice_3: 2.694  loss_ce_4: 0.2649  loss_mask_4: 0.3531  loss_dice_4: 2.69  loss_ce_5: 0.2621  loss_mask_5: 0.3532  loss_dice_5: 2.699  loss_ce_6: 0.2524  loss_mask_6: 0.353  loss_dice_6: 2.694  loss_ce_7: 0.2652  loss_mask_7: 0.3531  loss_dice_7: 2.696  loss_ce_8: 0.261  loss_mask_8: 0.3542  loss_dice_8: 2.688  time: 1.4838  data_time: 0.0704  lr: 3.4297e-06  max_mem: 21589M
[01/18 07:36:26] d2.utils.events INFO:  eta: 4:54:22  iter: 27839  total_loss: 33.46  loss_ce: 0.247  loss_mask: 0.3479  loss_dice: 2.654  loss_ce_0: 0.5321  loss_mask_0: 0.3497  loss_dice_0: 2.773  loss_ce_1: 0.3056  loss_mask_1: 0.3546  loss_dice_1: 2.694  loss_ce_2: 0.2893  loss_mask_2: 0.3507  loss_dice_2: 2.676  loss_ce_3: 0.2774  loss_mask_3: 0.3477  loss_dice_3: 2.653  loss_ce_4: 0.2568  loss_mask_4: 0.3471  loss_dice_4: 2.667  loss_ce_5: 0.2667  loss_mask_5: 0.3478  loss_dice_5: 2.652  loss_ce_6: 0.2446  loss_mask_6: 0.3465  loss_dice_6: 2.658  loss_ce_7: 0.2355  loss_mask_7: 0.3487  loss_dice_7: 2.659  loss_ce_8: 0.2462  loss_mask_8: 0.348  loss_dice_8: 2.655  time: 1.4838  data_time: 0.0700  lr: 3.4247e-06  max_mem: 21589M
[01/18 07:36:56] d2.utils.events INFO:  eta: 4:53:58  iter: 27859  total_loss: 33.45  loss_ce: 0.2554  loss_mask: 0.3405  loss_dice: 2.677  loss_ce_0: 0.6099  loss_mask_0: 0.3399  loss_dice_0: 2.802  loss_ce_1: 0.3192  loss_mask_1: 0.3448  loss_dice_1: 2.721  loss_ce_2: 0.2934  loss_mask_2: 0.3442  loss_dice_2: 2.704  loss_ce_3: 0.2852  loss_mask_3: 0.3414  loss_dice_3: 2.69  loss_ce_4: 0.2805  loss_mask_4: 0.3419  loss_dice_4: 2.68  loss_ce_5: 0.2586  loss_mask_5: 0.3407  loss_dice_5: 2.674  loss_ce_6: 0.2592  loss_mask_6: 0.3417  loss_dice_6: 2.67  loss_ce_7: 0.2473  loss_mask_7: 0.341  loss_dice_7: 2.665  loss_ce_8: 0.2487  loss_mask_8: 0.3409  loss_dice_8: 2.672  time: 1.4838  data_time: 0.0685  lr: 3.4196e-06  max_mem: 21589M
[01/18 07:37:24] d2.utils.events INFO:  eta: 4:53:17  iter: 27879  total_loss: 33.23  loss_ce: 0.2483  loss_mask: 0.3464  loss_dice: 2.679  loss_ce_0: 0.5501  loss_mask_0: 0.3428  loss_dice_0: 2.803  loss_ce_1: 0.2984  loss_mask_1: 0.3501  loss_dice_1: 2.717  loss_ce_2: 0.2894  loss_mask_2: 0.346  loss_dice_2: 2.692  loss_ce_3: 0.2761  loss_mask_3: 0.3443  loss_dice_3: 2.678  loss_ce_4: 0.2618  loss_mask_4: 0.344  loss_dice_4: 2.679  loss_ce_5: 0.2601  loss_mask_5: 0.3462  loss_dice_5: 2.678  loss_ce_6: 0.2613  loss_mask_6: 0.3455  loss_dice_6: 2.674  loss_ce_7: 0.252  loss_mask_7: 0.3439  loss_dice_7: 2.677  loss_ce_8: 0.2454  loss_mask_8: 0.3449  loss_dice_8: 2.677  time: 1.4838  data_time: 0.0725  lr: 3.4145e-06  max_mem: 21589M
[01/18 07:37:54] d2.utils.events INFO:  eta: 4:52:55  iter: 27899  total_loss: 33.56  loss_ce: 0.261  loss_mask: 0.3499  loss_dice: 2.702  loss_ce_0: 0.5458  loss_mask_0: 0.3459  loss_dice_0: 2.827  loss_ce_1: 0.3003  loss_mask_1: 0.3559  loss_dice_1: 2.738  loss_ce_2: 0.2993  loss_mask_2: 0.3533  loss_dice_2: 2.728  loss_ce_3: 0.2877  loss_mask_3: 0.3514  loss_dice_3: 2.717  loss_ce_4: 0.2694  loss_mask_4: 0.3507  loss_dice_4: 2.717  loss_ce_5: 0.2715  loss_mask_5: 0.3487  loss_dice_5: 2.715  loss_ce_6: 0.2556  loss_mask_6: 0.35  loss_dice_6: 2.714  loss_ce_7: 0.2542  loss_mask_7: 0.3488  loss_dice_7: 2.707  loss_ce_8: 0.2554  loss_mask_8: 0.3506  loss_dice_8: 2.707  time: 1.4837  data_time: 0.0701  lr: 3.4095e-06  max_mem: 21589M
[01/18 07:38:23] d2.utils.events INFO:  eta: 4:52:36  iter: 27919  total_loss: 33.66  loss_ce: 0.2494  loss_mask: 0.3454  loss_dice: 2.685  loss_ce_0: 0.5644  loss_mask_0: 0.3467  loss_dice_0: 2.804  loss_ce_1: 0.2931  loss_mask_1: 0.3534  loss_dice_1: 2.724  loss_ce_2: 0.2748  loss_mask_2: 0.3499  loss_dice_2: 2.706  loss_ce_3: 0.2856  loss_mask_3: 0.3464  loss_dice_3: 2.69  loss_ce_4: 0.2636  loss_mask_4: 0.3446  loss_dice_4: 2.685  loss_ce_5: 0.2591  loss_mask_5: 0.346  loss_dice_5: 2.691  loss_ce_6: 0.2496  loss_mask_6: 0.3466  loss_dice_6: 2.692  loss_ce_7: 0.2547  loss_mask_7: 0.3456  loss_dice_7: 2.685  loss_ce_8: 0.2517  loss_mask_8: 0.3449  loss_dice_8: 2.691  time: 1.4837  data_time: 0.0670  lr: 3.4044e-06  max_mem: 21589M
[01/18 07:38:52] d2.utils.events INFO:  eta: 4:52:08  iter: 27939  total_loss: 33.54  loss_ce: 0.2565  loss_mask: 0.349  loss_dice: 2.654  loss_ce_0: 0.5576  loss_mask_0: 0.3472  loss_dice_0: 2.779  loss_ce_1: 0.3062  loss_mask_1: 0.3496  loss_dice_1: 2.703  loss_ce_2: 0.3007  loss_mask_2: 0.3506  loss_dice_2: 2.673  loss_ce_3: 0.2814  loss_mask_3: 0.3489  loss_dice_3: 2.672  loss_ce_4: 0.2789  loss_mask_4: 0.3484  loss_dice_4: 2.666  loss_ce_5: 0.2668  loss_mask_5: 0.3482  loss_dice_5: 2.661  loss_ce_6: 0.2504  loss_mask_6: 0.348  loss_dice_6: 2.664  loss_ce_7: 0.2552  loss_mask_7: 0.3489  loss_dice_7: 2.664  loss_ce_8: 0.2425  loss_mask_8: 0.3488  loss_dice_8: 2.665  time: 1.4837  data_time: 0.0695  lr: 3.3993e-06  max_mem: 21589M
[01/18 07:39:21] d2.utils.events INFO:  eta: 4:51:39  iter: 27959  total_loss: 32.83  loss_ce: 0.2268  loss_mask: 0.3482  loss_dice: 2.624  loss_ce_0: 0.5492  loss_mask_0: 0.3429  loss_dice_0: 2.742  loss_ce_1: 0.2561  loss_mask_1: 0.351  loss_dice_1: 2.661  loss_ce_2: 0.269  loss_mask_2: 0.351  loss_dice_2: 2.642  loss_ce_3: 0.2474  loss_mask_3: 0.349  loss_dice_3: 2.635  loss_ce_4: 0.2447  loss_mask_4: 0.3472  loss_dice_4: 2.626  loss_ce_5: 0.2319  loss_mask_5: 0.3481  loss_dice_5: 2.635  loss_ce_6: 0.2397  loss_mask_6: 0.347  loss_dice_6: 2.626  loss_ce_7: 0.2313  loss_mask_7: 0.3477  loss_dice_7: 2.629  loss_ce_8: 0.2192  loss_mask_8: 0.3484  loss_dice_8: 2.627  time: 1.4837  data_time: 0.0668  lr: 3.3942e-06  max_mem: 21589M
[01/18 07:39:51] d2.utils.events INFO:  eta: 4:51:09  iter: 27979  total_loss: 34.22  loss_ce: 0.2371  loss_mask: 0.3528  loss_dice: 2.769  loss_ce_0: 0.5692  loss_mask_0: 0.3578  loss_dice_0: 2.879  loss_ce_1: 0.2814  loss_mask_1: 0.3602  loss_dice_1: 2.813  loss_ce_2: 0.2742  loss_mask_2: 0.3566  loss_dice_2: 2.785  loss_ce_3: 0.2625  loss_mask_3: 0.3532  loss_dice_3: 2.775  loss_ce_4: 0.2552  loss_mask_4: 0.3552  loss_dice_4: 2.77  loss_ce_5: 0.2544  loss_mask_5: 0.3541  loss_dice_5: 2.77  loss_ce_6: 0.2488  loss_mask_6: 0.3528  loss_dice_6: 2.768  loss_ce_7: 0.2434  loss_mask_7: 0.3529  loss_dice_7: 2.765  loss_ce_8: 0.2363  loss_mask_8: 0.3528  loss_dice_8: 2.756  time: 1.4837  data_time: 0.0748  lr: 3.3892e-06  max_mem: 21589M
[01/18 07:40:20] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 07:40:20] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 07:40:20] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 07:40:21] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 07:40:36] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0087 s/iter. Inference: 0.1771 s/iter. Eval: 0.2209 s/iter. Total: 0.4067 s/iter. ETA=0:07:20
[01/18 07:40:41] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0109 s/iter. Inference: 0.1666 s/iter. Eval: 0.2231 s/iter. Total: 0.4006 s/iter. ETA=0:07:08
[01/18 07:40:47] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0112 s/iter. Inference: 0.1622 s/iter. Eval: 0.2220 s/iter. Total: 0.3955 s/iter. ETA=0:06:57
[01/18 07:40:52] d2.evaluation.evaluator INFO: Inference done 52/1093. Dataloading: 0.0123 s/iter. Inference: 0.1594 s/iter. Eval: 0.2205 s/iter. Total: 0.3923 s/iter. ETA=0:06:48
[01/18 07:40:57] d2.evaluation.evaluator INFO: Inference done 64/1093. Dataloading: 0.0128 s/iter. Inference: 0.1593 s/iter. Eval: 0.2285 s/iter. Total: 0.4007 s/iter. ETA=0:06:52
[01/18 07:41:02] d2.evaluation.evaluator INFO: Inference done 79/1093. Dataloading: 0.0125 s/iter. Inference: 0.1576 s/iter. Eval: 0.2205 s/iter. Total: 0.3906 s/iter. ETA=0:06:36
[01/18 07:41:08] d2.evaluation.evaluator INFO: Inference done 91/1093. Dataloading: 0.0127 s/iter. Inference: 0.1565 s/iter. Eval: 0.2276 s/iter. Total: 0.3969 s/iter. ETA=0:06:37
[01/18 07:41:13] d2.evaluation.evaluator INFO: Inference done 105/1093. Dataloading: 0.0126 s/iter. Inference: 0.1576 s/iter. Eval: 0.2236 s/iter. Total: 0.3939 s/iter. ETA=0:06:29
[01/18 07:41:18] d2.evaluation.evaluator INFO: Inference done 119/1093. Dataloading: 0.0123 s/iter. Inference: 0.1568 s/iter. Eval: 0.2204 s/iter. Total: 0.3896 s/iter. ETA=0:06:19
[01/18 07:41:23] d2.evaluation.evaluator INFO: Inference done 133/1093. Dataloading: 0.0121 s/iter. Inference: 0.1557 s/iter. Eval: 0.2195 s/iter. Total: 0.3874 s/iter. ETA=0:06:11
[01/18 07:41:28] d2.evaluation.evaluator INFO: Inference done 148/1093. Dataloading: 0.0117 s/iter. Inference: 0.1558 s/iter. Eval: 0.2150 s/iter. Total: 0.3826 s/iter. ETA=0:06:01
[01/18 07:41:33] d2.evaluation.evaluator INFO: Inference done 162/1093. Dataloading: 0.0117 s/iter. Inference: 0.1566 s/iter. Eval: 0.2126 s/iter. Total: 0.3811 s/iter. ETA=0:05:54
[01/18 07:41:38] d2.evaluation.evaluator INFO: Inference done 174/1093. Dataloading: 0.0119 s/iter. Inference: 0.1569 s/iter. Eval: 0.2151 s/iter. Total: 0.3840 s/iter. ETA=0:05:52
[01/18 07:41:44] d2.evaluation.evaluator INFO: Inference done 188/1093. Dataloading: 0.0118 s/iter. Inference: 0.1572 s/iter. Eval: 0.2141 s/iter. Total: 0.3833 s/iter. ETA=0:05:46
[01/18 07:41:49] d2.evaluation.evaluator INFO: Inference done 203/1093. Dataloading: 0.0118 s/iter. Inference: 0.1565 s/iter. Eval: 0.2124 s/iter. Total: 0.3807 s/iter. ETA=0:05:38
[01/18 07:41:54] d2.evaluation.evaluator INFO: Inference done 216/1093. Dataloading: 0.0118 s/iter. Inference: 0.1565 s/iter. Eval: 0.2143 s/iter. Total: 0.3827 s/iter. ETA=0:05:35
[01/18 07:42:00] d2.evaluation.evaluator INFO: Inference done 232/1093. Dataloading: 0.0118 s/iter. Inference: 0.1556 s/iter. Eval: 0.2120 s/iter. Total: 0.3795 s/iter. ETA=0:05:26
[01/18 07:42:05] d2.evaluation.evaluator INFO: Inference done 246/1093. Dataloading: 0.0118 s/iter. Inference: 0.1551 s/iter. Eval: 0.2131 s/iter. Total: 0.3801 s/iter. ETA=0:05:21
[01/18 07:42:10] d2.evaluation.evaluator INFO: Inference done 260/1093. Dataloading: 0.0119 s/iter. Inference: 0.1541 s/iter. Eval: 0.2131 s/iter. Total: 0.3792 s/iter. ETA=0:05:15
[01/18 07:42:15] d2.evaluation.evaluator INFO: Inference done 273/1093. Dataloading: 0.0118 s/iter. Inference: 0.1540 s/iter. Eval: 0.2136 s/iter. Total: 0.3796 s/iter. ETA=0:05:11
[01/18 07:42:20] d2.evaluation.evaluator INFO: Inference done 287/1093. Dataloading: 0.0118 s/iter. Inference: 0.1541 s/iter. Eval: 0.2131 s/iter. Total: 0.3792 s/iter. ETA=0:05:05
[01/18 07:42:26] d2.evaluation.evaluator INFO: Inference done 300/1093. Dataloading: 0.0119 s/iter. Inference: 0.1549 s/iter. Eval: 0.2141 s/iter. Total: 0.3809 s/iter. ETA=0:05:02
[01/18 07:42:31] d2.evaluation.evaluator INFO: Inference done 311/1093. Dataloading: 0.0120 s/iter. Inference: 0.1552 s/iter. Eval: 0.2173 s/iter. Total: 0.3846 s/iter. ETA=0:05:00
[01/18 07:42:37] d2.evaluation.evaluator INFO: Inference done 324/1093. Dataloading: 0.0120 s/iter. Inference: 0.1558 s/iter. Eval: 0.2180 s/iter. Total: 0.3859 s/iter. ETA=0:04:56
[01/18 07:42:42] d2.evaluation.evaluator INFO: Inference done 339/1093. Dataloading: 0.0119 s/iter. Inference: 0.1561 s/iter. Eval: 0.2161 s/iter. Total: 0.3842 s/iter. ETA=0:04:49
[01/18 07:42:47] d2.evaluation.evaluator INFO: Inference done 354/1093. Dataloading: 0.0119 s/iter. Inference: 0.1564 s/iter. Eval: 0.2143 s/iter. Total: 0.3827 s/iter. ETA=0:04:42
[01/18 07:42:52] d2.evaluation.evaluator INFO: Inference done 366/1093. Dataloading: 0.0119 s/iter. Inference: 0.1571 s/iter. Eval: 0.2157 s/iter. Total: 0.3848 s/iter. ETA=0:04:39
[01/18 07:42:58] d2.evaluation.evaluator INFO: Inference done 381/1093. Dataloading: 0.0118 s/iter. Inference: 0.1571 s/iter. Eval: 0.2146 s/iter. Total: 0.3836 s/iter. ETA=0:04:33
[01/18 07:43:03] d2.evaluation.evaluator INFO: Inference done 393/1093. Dataloading: 0.0119 s/iter. Inference: 0.1572 s/iter. Eval: 0.2161 s/iter. Total: 0.3853 s/iter. ETA=0:04:29
[01/18 07:43:08] d2.evaluation.evaluator INFO: Inference done 406/1093. Dataloading: 0.0119 s/iter. Inference: 0.1570 s/iter. Eval: 0.2164 s/iter. Total: 0.3854 s/iter. ETA=0:04:24
[01/18 07:43:13] d2.evaluation.evaluator INFO: Inference done 419/1093. Dataloading: 0.0119 s/iter. Inference: 0.1569 s/iter. Eval: 0.2165 s/iter. Total: 0.3854 s/iter. ETA=0:04:19
[01/18 07:43:18] d2.evaluation.evaluator INFO: Inference done 431/1093. Dataloading: 0.0120 s/iter. Inference: 0.1568 s/iter. Eval: 0.2181 s/iter. Total: 0.3869 s/iter. ETA=0:04:16
[01/18 07:43:24] d2.evaluation.evaluator INFO: Inference done 446/1093. Dataloading: 0.0119 s/iter. Inference: 0.1567 s/iter. Eval: 0.2169 s/iter. Total: 0.3856 s/iter. ETA=0:04:09
[01/18 07:43:29] d2.evaluation.evaluator INFO: Inference done 461/1093. Dataloading: 0.0119 s/iter. Inference: 0.1566 s/iter. Eval: 0.2160 s/iter. Total: 0.3846 s/iter. ETA=0:04:03
[01/18 07:43:34] d2.evaluation.evaluator INFO: Inference done 477/1093. Dataloading: 0.0118 s/iter. Inference: 0.1563 s/iter. Eval: 0.2141 s/iter. Total: 0.3823 s/iter. ETA=0:03:55
[01/18 07:43:39] d2.evaluation.evaluator INFO: Inference done 492/1093. Dataloading: 0.0117 s/iter. Inference: 0.1562 s/iter. Eval: 0.2134 s/iter. Total: 0.3814 s/iter. ETA=0:03:49
[01/18 07:43:44] d2.evaluation.evaluator INFO: Inference done 508/1093. Dataloading: 0.0117 s/iter. Inference: 0.1559 s/iter. Eval: 0.2116 s/iter. Total: 0.3793 s/iter. ETA=0:03:41
[01/18 07:43:50] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0117 s/iter. Inference: 0.1561 s/iter. Eval: 0.2119 s/iter. Total: 0.3797 s/iter. ETA=0:03:37
[01/18 07:43:55] d2.evaluation.evaluator INFO: Inference done 534/1093. Dataloading: 0.0117 s/iter. Inference: 0.1561 s/iter. Eval: 0.2124 s/iter. Total: 0.3803 s/iter. ETA=0:03:32
[01/18 07:44:00] d2.evaluation.evaluator INFO: Inference done 548/1093. Dataloading: 0.0117 s/iter. Inference: 0.1561 s/iter. Eval: 0.2124 s/iter. Total: 0.3803 s/iter. ETA=0:03:27
[01/18 07:44:05] d2.evaluation.evaluator INFO: Inference done 562/1093. Dataloading: 0.0116 s/iter. Inference: 0.1560 s/iter. Eval: 0.2121 s/iter. Total: 0.3798 s/iter. ETA=0:03:21
[01/18 07:44:10] d2.evaluation.evaluator INFO: Inference done 578/1093. Dataloading: 0.0117 s/iter. Inference: 0.1559 s/iter. Eval: 0.2105 s/iter. Total: 0.3782 s/iter. ETA=0:03:14
[01/18 07:44:16] d2.evaluation.evaluator INFO: Inference done 594/1093. Dataloading: 0.0116 s/iter. Inference: 0.1558 s/iter. Eval: 0.2094 s/iter. Total: 0.3769 s/iter. ETA=0:03:08
[01/18 07:44:21] d2.evaluation.evaluator INFO: Inference done 607/1093. Dataloading: 0.0117 s/iter. Inference: 0.1556 s/iter. Eval: 0.2099 s/iter. Total: 0.3773 s/iter. ETA=0:03:03
[01/18 07:44:26] d2.evaluation.evaluator INFO: Inference done 620/1093. Dataloading: 0.0118 s/iter. Inference: 0.1558 s/iter. Eval: 0.2101 s/iter. Total: 0.3777 s/iter. ETA=0:02:58
[01/18 07:44:31] d2.evaluation.evaluator INFO: Inference done 634/1093. Dataloading: 0.0117 s/iter. Inference: 0.1558 s/iter. Eval: 0.2101 s/iter. Total: 0.3778 s/iter. ETA=0:02:53
[01/18 07:44:36] d2.evaluation.evaluator INFO: Inference done 650/1093. Dataloading: 0.0117 s/iter. Inference: 0.1556 s/iter. Eval: 0.2091 s/iter. Total: 0.3765 s/iter. ETA=0:02:46
[01/18 07:44:42] d2.evaluation.evaluator INFO: Inference done 663/1093. Dataloading: 0.0117 s/iter. Inference: 0.1559 s/iter. Eval: 0.2094 s/iter. Total: 0.3771 s/iter. ETA=0:02:42
[01/18 07:44:47] d2.evaluation.evaluator INFO: Inference done 677/1093. Dataloading: 0.0117 s/iter. Inference: 0.1559 s/iter. Eval: 0.2092 s/iter. Total: 0.3769 s/iter. ETA=0:02:36
[01/18 07:44:52] d2.evaluation.evaluator INFO: Inference done 693/1093. Dataloading: 0.0116 s/iter. Inference: 0.1559 s/iter. Eval: 0.2082 s/iter. Total: 0.3759 s/iter. ETA=0:02:30
[01/18 07:44:57] d2.evaluation.evaluator INFO: Inference done 705/1093. Dataloading: 0.0117 s/iter. Inference: 0.1561 s/iter. Eval: 0.2090 s/iter. Total: 0.3768 s/iter. ETA=0:02:26
[01/18 07:45:02] d2.evaluation.evaluator INFO: Inference done 716/1093. Dataloading: 0.0117 s/iter. Inference: 0.1560 s/iter. Eval: 0.2103 s/iter. Total: 0.3782 s/iter. ETA=0:02:22
[01/18 07:45:08] d2.evaluation.evaluator INFO: Inference done 731/1093. Dataloading: 0.0117 s/iter. Inference: 0.1561 s/iter. Eval: 0.2097 s/iter. Total: 0.3776 s/iter. ETA=0:02:16
[01/18 07:45:13] d2.evaluation.evaluator INFO: Inference done 746/1093. Dataloading: 0.0117 s/iter. Inference: 0.1561 s/iter. Eval: 0.2090 s/iter. Total: 0.3768 s/iter. ETA=0:02:10
[01/18 07:45:18] d2.evaluation.evaluator INFO: Inference done 761/1093. Dataloading: 0.0116 s/iter. Inference: 0.1558 s/iter. Eval: 0.2085 s/iter. Total: 0.3760 s/iter. ETA=0:02:04
[01/18 07:45:23] d2.evaluation.evaluator INFO: Inference done 776/1093. Dataloading: 0.0116 s/iter. Inference: 0.1554 s/iter. Eval: 0.2083 s/iter. Total: 0.3755 s/iter. ETA=0:01:59
[01/18 07:45:28] d2.evaluation.evaluator INFO: Inference done 792/1093. Dataloading: 0.0116 s/iter. Inference: 0.1552 s/iter. Eval: 0.2075 s/iter. Total: 0.3743 s/iter. ETA=0:01:52
[01/18 07:45:33] d2.evaluation.evaluator INFO: Inference done 805/1093. Dataloading: 0.0116 s/iter. Inference: 0.1552 s/iter. Eval: 0.2076 s/iter. Total: 0.3745 s/iter. ETA=0:01:47
[01/18 07:45:38] d2.evaluation.evaluator INFO: Inference done 819/1093. Dataloading: 0.0116 s/iter. Inference: 0.1553 s/iter. Eval: 0.2073 s/iter. Total: 0.3743 s/iter. ETA=0:01:42
[01/18 07:45:43] d2.evaluation.evaluator INFO: Inference done 834/1093. Dataloading: 0.0116 s/iter. Inference: 0.1553 s/iter. Eval: 0.2069 s/iter. Total: 0.3738 s/iter. ETA=0:01:36
[01/18 07:45:49] d2.evaluation.evaluator INFO: Inference done 849/1093. Dataloading: 0.0115 s/iter. Inference: 0.1552 s/iter. Eval: 0.2065 s/iter. Total: 0.3733 s/iter. ETA=0:01:31
[01/18 07:45:54] d2.evaluation.evaluator INFO: Inference done 861/1093. Dataloading: 0.0116 s/iter. Inference: 0.1552 s/iter. Eval: 0.2073 s/iter. Total: 0.3742 s/iter. ETA=0:01:26
[01/18 07:45:59] d2.evaluation.evaluator INFO: Inference done 877/1093. Dataloading: 0.0115 s/iter. Inference: 0.1549 s/iter. Eval: 0.2068 s/iter. Total: 0.3733 s/iter. ETA=0:01:20
[01/18 07:46:04] d2.evaluation.evaluator INFO: Inference done 888/1093. Dataloading: 0.0116 s/iter. Inference: 0.1550 s/iter. Eval: 0.2077 s/iter. Total: 0.3744 s/iter. ETA=0:01:16
[01/18 07:46:09] d2.evaluation.evaluator INFO: Inference done 902/1093. Dataloading: 0.0116 s/iter. Inference: 0.1550 s/iter. Eval: 0.2076 s/iter. Total: 0.3742 s/iter. ETA=0:01:11
[01/18 07:46:14] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0116 s/iter. Inference: 0.1549 s/iter. Eval: 0.2072 s/iter. Total: 0.3737 s/iter. ETA=0:01:05
[01/18 07:46:20] d2.evaluation.evaluator INFO: Inference done 931/1093. Dataloading: 0.0116 s/iter. Inference: 0.1549 s/iter. Eval: 0.2072 s/iter. Total: 0.3738 s/iter. ETA=0:01:00
[01/18 07:46:25] d2.evaluation.evaluator INFO: Inference done 945/1093. Dataloading: 0.0116 s/iter. Inference: 0.1549 s/iter. Eval: 0.2074 s/iter. Total: 0.3740 s/iter. ETA=0:00:55
[01/18 07:46:30] d2.evaluation.evaluator INFO: Inference done 960/1093. Dataloading: 0.0116 s/iter. Inference: 0.1547 s/iter. Eval: 0.2072 s/iter. Total: 0.3736 s/iter. ETA=0:00:49
[01/18 07:46:36] d2.evaluation.evaluator INFO: Inference done 975/1093. Dataloading: 0.0115 s/iter. Inference: 0.1546 s/iter. Eval: 0.2070 s/iter. Total: 0.3733 s/iter. ETA=0:00:44
[01/18 07:46:41] d2.evaluation.evaluator INFO: Inference done 991/1093. Dataloading: 0.0115 s/iter. Inference: 0.1546 s/iter. Eval: 0.2064 s/iter. Total: 0.3727 s/iter. ETA=0:00:38
[01/18 07:46:46] d2.evaluation.evaluator INFO: Inference done 1006/1093. Dataloading: 0.0115 s/iter. Inference: 0.1545 s/iter. Eval: 0.2062 s/iter. Total: 0.3723 s/iter. ETA=0:00:32
[01/18 07:46:51] d2.evaluation.evaluator INFO: Inference done 1020/1093. Dataloading: 0.0117 s/iter. Inference: 0.1545 s/iter. Eval: 0.2060 s/iter. Total: 0.3723 s/iter. ETA=0:00:27
[01/18 07:46:56] d2.evaluation.evaluator INFO: Inference done 1033/1093. Dataloading: 0.0117 s/iter. Inference: 0.1545 s/iter. Eval: 0.2062 s/iter. Total: 0.3725 s/iter. ETA=0:00:22
[01/18 07:47:02] d2.evaluation.evaluator INFO: Inference done 1048/1093. Dataloading: 0.0117 s/iter. Inference: 0.1543 s/iter. Eval: 0.2058 s/iter. Total: 0.3720 s/iter. ETA=0:00:16
[01/18 07:47:07] d2.evaluation.evaluator INFO: Inference done 1065/1093. Dataloading: 0.0117 s/iter. Inference: 0.1541 s/iter. Eval: 0.2051 s/iter. Total: 0.3709 s/iter. ETA=0:00:10
[01/18 07:47:12] d2.evaluation.evaluator INFO: Inference done 1082/1093. Dataloading: 0.0117 s/iter. Inference: 0.1539 s/iter. Eval: 0.2041 s/iter. Total: 0.3698 s/iter. ETA=0:00:04
[01/18 07:47:16] d2.evaluation.evaluator INFO: Total inference time: 0:06:42.387355 (0.369841 s / iter per device, on 4 devices)
[01/18 07:47:16] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:47 (0.153812 s / iter per device, on 4 devices)
[01/18 07:47:41] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.349082799905785, 'mIoU': 19.975401369514547, 'fwIoU': 42.911981928215695, 'IoU-0': nan, 'IoU-1': 95.40971090595163, 'IoU-2': 47.10300764423505, 'IoU-3': 58.217404648307806, 'IoU-4': 52.73583400661792, 'IoU-5': 46.799932709543064, 'IoU-6': 41.58559164309495, 'IoU-7': 33.896196820158686, 'IoU-8': 20.425793240890457, 'IoU-9': 31.59915252595036, 'IoU-10': 37.21156573349683, 'IoU-11': 47.162649640737364, 'IoU-12': 49.196763611473564, 'IoU-13': 49.51654204517494, 'IoU-14': 49.61378685768694, 'IoU-15': 50.303392502716314, 'IoU-16': 51.39511871865034, 'IoU-17': 46.0844929091732, 'IoU-18': 47.00115577090877, 'IoU-19': 46.87427841214396, 'IoU-20': 47.084464826503364, 'IoU-21': 47.5278262315034, 'IoU-22': 47.363092578988415, 'IoU-23': 47.38146147095658, 'IoU-24': 46.051080627038345, 'IoU-25': 46.573641700066055, 'IoU-26': 45.74721451450284, 'IoU-27': 46.35039540671967, 'IoU-28': 43.51049639019988, 'IoU-29': 46.628454313362944, 'IoU-30': 45.009774012352146, 'IoU-31': 45.84678021573758, 'IoU-32': 44.98083684190334, 'IoU-33': 43.59785239235773, 'IoU-34': 43.14671887597003, 'IoU-35': 44.57089077182184, 'IoU-36': 44.63477456265865, 'IoU-37': 41.86060124927968, 'IoU-38': 42.67869116414056, 'IoU-39': 42.057685792635326, 'IoU-40': 42.36237961959888, 'IoU-41': 40.23607869624707, 'IoU-42': 39.33080194212774, 'IoU-43': 39.223031306028886, 'IoU-44': 39.88420985654517, 'IoU-45': 38.160371742416125, 'IoU-46': 37.23051566359168, 'IoU-47': 36.59108934326564, 'IoU-48': 37.21833804842579, 'IoU-49': 35.83813505562743, 'IoU-50': 35.738129095775825, 'IoU-51': 34.486726741581535, 'IoU-52': 33.60211926444871, 'IoU-53': 32.794751438660185, 'IoU-54': 32.58627116100315, 'IoU-55': 32.10320407898009, 'IoU-56': 31.08165067459796, 'IoU-57': 29.831318256245893, 'IoU-58': 28.28245301082479, 'IoU-59': 27.455538019639363, 'IoU-60': 27.14241798491635, 'IoU-61': 25.45496598808385, 'IoU-62': 26.36529918996603, 'IoU-63': 25.925017894398255, 'IoU-64': 25.40006410480663, 'IoU-65': 24.246224511920072, 'IoU-66': 23.3787186003862, 'IoU-67': 22.743140483843057, 'IoU-68': 22.003630820198865, 'IoU-69': 22.294039475040268, 'IoU-70': 21.840777636003384, 'IoU-71': 20.073140713871023, 'IoU-72': 19.95549190947067, 'IoU-73': 20.446278429913107, 'IoU-74': 20.457300151568578, 'IoU-75': 20.038451262822118, 'IoU-76': 19.026478449348243, 'IoU-77': 19.62177256331784, 'IoU-78': 18.616577829701857, 'IoU-79': 18.291942366186625, 'IoU-80': 18.455534408484233, 'IoU-81': 18.460928290510548, 'IoU-82': 17.89973425746962, 'IoU-83': 17.766901496200386, 'IoU-84': 16.808290507885438, 'IoU-85': 16.98900775374709, 'IoU-86': 16.11304338666107, 'IoU-87': 16.44371263373762, 'IoU-88': 16.167509102094925, 'IoU-89': 16.012487892983085, 'IoU-90': 16.2538549270073, 'IoU-91': 16.121643392389, 'IoU-92': 15.426555196451272, 'IoU-93': 16.56598453989681, 'IoU-94': 15.948792271836426, 'IoU-95': 17.461794551960296, 'IoU-96': 16.55813040169435, 'IoU-97': 16.41986876122619, 'IoU-98': 15.967207158349659, 'IoU-99': 14.845822575621458, 'IoU-100': 16.22711460783162, 'IoU-101': 14.544177290608513, 'IoU-102': 14.508572176767123, 'IoU-103': 14.31579427964255, 'IoU-104': 13.598066943018583, 'IoU-105': 13.653562897532021, 'IoU-106': 13.400562347675143, 'IoU-107': 12.831837352449766, 'IoU-108': 12.727556328474652, 'IoU-109': 13.385874527967854, 'IoU-110': 12.625680467364583, 'IoU-111': 12.515814098334296, 'IoU-112': 11.869367300062644, 'IoU-113': 12.024404706504898, 'IoU-114': 12.122465549288282, 'IoU-115': 9.960094512757134, 'IoU-116': 11.217214018073616, 'IoU-117': 10.946438897191099, 'IoU-118': 9.015051820601311, 'IoU-119': 12.795792572036605, 'IoU-120': 9.611373008312132, 'IoU-121': 10.399976721390953, 'IoU-122': 9.800943123282067, 'IoU-123': 8.959445558946898, 'IoU-124': 10.779558416380194, 'IoU-125': 8.082852894669596, 'IoU-126': 9.209534314051197, 'IoU-127': 7.444886591364691, 'IoU-128': 7.588291481061586, 'IoU-129': 6.928308096721072, 'IoU-130': 7.8178314354267435, 'IoU-131': 6.718841750487779, 'IoU-132': 6.371456549125785, 'IoU-133': 7.355089728674683, 'IoU-134': 6.145015637431575, 'IoU-135': 5.488323237018259, 'IoU-136': 5.766654660391027, 'IoU-137': 4.740847225152986, 'IoU-138': 6.60864620917134, 'IoU-139': 3.966095776147759, 'IoU-140': 5.313908143274444, 'IoU-141': 3.8992073656837647, 'IoU-142': 4.4239847194894395, 'IoU-143': 3.998373300464073, 'IoU-144': 4.912695189929492, 'IoU-145': 3.063044780986228, 'IoU-146': 3.912904729216266, 'IoU-147': 4.722541706642449, 'IoU-148': 3.0719616166404506, 'IoU-149': 2.2535334244259957, 'IoU-150': 3.8651356064779168, 'IoU-151': 2.1186453551431113, 'IoU-152': 3.2838820333014587, 'IoU-153': 2.736083576315142, 'IoU-154': 2.674408856946455, 'IoU-155': 2.738419999542368, 'IoU-156': 2.0123344322526004, 'IoU-157': 2.4902749552771284, 'IoU-158': 1.9327405947207437, 'IoU-159': 1.711505191122141, 'IoU-160': 2.982538520310709, 'IoU-161': 2.5541688318172655, 'IoU-162': 2.123204016079706, 'IoU-163': 1.7536402673124556, 'IoU-164': 1.8244177740400935, 'IoU-165': 1.3310864434443763, 'IoU-166': 2.232051829538066, 'IoU-167': 2.1593448986247323, 'IoU-168': 1.9972111045432177, 'IoU-169': 2.6032810338660957, 'IoU-170': 1.7868141510787119, 'IoU-171': 2.256320680391807, 'IoU-172': 1.2236356541029247, 'IoU-173': 1.9807279994802358, 'IoU-174': 1.992924094352418, 'IoU-175': 1.0486998587280423, 'IoU-176': 1.1795297741530217, 'IoU-177': 1.1907916360787858, 'IoU-178': 1.6559737800562473, 'IoU-179': 2.0407894216444964, 'IoU-180': 2.0659213756719965, 'IoU-181': 2.0797560296190354, 'IoU-182': 0.7576336086050596, 'IoU-183': 2.873384333003889, 'IoU-184': 1.3395169869221857, 'IoU-185': 0.6384041854860043, 'IoU-186': 1.9257307740559528, 'IoU-187': 3.2656905899037345, 'IoU-188': 2.723534490999384, 'IoU-189': 2.194278024003765, 'IoU-190': 1.835417813027067, 'IoU-191': 1.699489735882158, 'mACC': 29.97076820012819, 'pACC': 57.28268564561269, 'ACC-0': nan, 'ACC-1': 98.73388325894551, 'ACC-2': 59.0937363177015, 'ACC-3': 72.32461623427791, 'ACC-4': 68.70840524206255, 'ACC-5': 63.661241424587665, 'ACC-6': 59.30545998271928, 'ACC-7': 48.53892966661926, 'ACC-8': 24.85578529970563, 'ACC-9': 41.04768849954627, 'ACC-10': 50.195322874053495, 'ACC-11': 62.1297375265474, 'ACC-12': 66.731711904755, 'ACC-13': 68.88495286373139, 'ACC-14': 69.46584590562576, 'ACC-15': 67.57522339456872, 'ACC-16': 66.46347391232834, 'ACC-17': 64.0275465692192, 'ACC-18': 64.18639045086141, 'ACC-19': 63.937612019055436, 'ACC-20': 63.39889583440078, 'ACC-21': 66.51125901805544, 'ACC-22': 65.73844275012712, 'ACC-23': 64.23608118202559, 'ACC-24': 63.06418376849717, 'ACC-25': 64.08160183286083, 'ACC-26': 61.78426347507513, 'ACC-27': 61.82145498396135, 'ACC-28': 62.977399891368876, 'ACC-29': 63.891649026488516, 'ACC-30': 62.540959487415435, 'ACC-31': 63.15040868680127, 'ACC-32': 61.759356369663664, 'ACC-33': 60.737471286011626, 'ACC-34': 60.356210496439786, 'ACC-35': 61.471992256682164, 'ACC-36': 61.84726345102807, 'ACC-37': 58.74017236368828, 'ACC-38': 59.66925971084076, 'ACC-39': 59.2226322764723, 'ACC-40': 59.32884356964636, 'ACC-41': 58.59325862282579, 'ACC-42': 56.12372483343307, 'ACC-43': 55.339126177358246, 'ACC-44': 56.33882461827212, 'ACC-45': 55.89932359111502, 'ACC-46': 53.98055148034675, 'ACC-47': 52.38808690842791, 'ACC-48': 54.25155667441576, 'ACC-49': 53.09852471786449, 'ACC-50': 53.858090027155406, 'ACC-51': 51.063747204078815, 'ACC-52': 50.930211532150004, 'ACC-53': 48.95105519191207, 'ACC-54': 48.04200366213694, 'ACC-55': 48.12946816954921, 'ACC-56': 47.10971123749926, 'ACC-57': 45.561672314849716, 'ACC-58': 43.022302035288604, 'ACC-59': 42.017054046322954, 'ACC-60': 43.198044120948495, 'ACC-61': 40.04320263419377, 'ACC-62': 40.33459327329458, 'ACC-63': 40.35603954449604, 'ACC-64': 40.87321651002286, 'ACC-65': 39.09228359448377, 'ACC-66': 38.45023836574434, 'ACC-67': 35.98729342666328, 'ACC-68': 36.1380523416712, 'ACC-69': 34.78150021411838, 'ACC-70': 34.95317409248858, 'ACC-71': 33.993650170598606, 'ACC-72': 34.54904775013876, 'ACC-73': 32.37401820673757, 'ACC-74': 33.29170427896209, 'ACC-75': 34.6212294114371, 'ACC-76': 29.748610392707832, 'ACC-77': 31.601361820598868, 'ACC-78': 31.892555370467857, 'ACC-79': 29.396969280807046, 'ACC-80': 32.04631176622603, 'ACC-81': 30.25863224737041, 'ACC-82': 30.955848450569974, 'ACC-83': 28.850263865237025, 'ACC-84': 28.02550024594497, 'ACC-85': 28.862440071231738, 'ACC-86': 27.389455747020463, 'ACC-87': 27.02438070684915, 'ACC-88': 26.201761439718187, 'ACC-89': 28.13474622169644, 'ACC-90': 27.847431953328705, 'ACC-91': 27.977866324334226, 'ACC-92': 28.842305449392164, 'ACC-93': 29.59031136383491, 'ACC-94': 28.725875864552087, 'ACC-95': 30.68679978216319, 'ACC-96': 28.45026261189045, 'ACC-97': 28.314268402653113, 'ACC-98': 26.767823289973876, 'ACC-99': 27.804346937342977, 'ACC-100': 28.728813353152542, 'ACC-101': 25.90322161868456, 'ACC-102': 24.55149524241051, 'ACC-103': 24.43390478889204, 'ACC-104': 24.253682890238803, 'ACC-105': 23.66918474564627, 'ACC-106': 24.881101192934537, 'ACC-107': 22.07558777071394, 'ACC-108': 21.152285233334815, 'ACC-109': 22.39951919465104, 'ACC-110': 23.287523158261443, 'ACC-111': 22.784647876244836, 'ACC-112': 21.336681928198065, 'ACC-113': 21.370424505974086, 'ACC-114': 23.60403455251591, 'ACC-115': 16.490498098116156, 'ACC-116': 20.802133656976753, 'ACC-117': 21.23039201110319, 'ACC-118': 14.111198863667376, 'ACC-119': 24.38065533594363, 'ACC-120': 15.95789540265025, 'ACC-121': 20.12997660366629, 'ACC-122': 16.092917055865342, 'ACC-123': 14.427318426823035, 'ACC-124': 22.316327144889915, 'ACC-125': 15.465382009064973, 'ACC-126': 17.75186932485724, 'ACC-127': 13.789027091179246, 'ACC-128': 12.959771481818924, 'ACC-129': 11.666904080740029, 'ACC-130': 16.54815126288879, 'ACC-131': 12.911897484769522, 'ACC-132': 12.358643892859895, 'ACC-133': 13.989398980723564, 'ACC-134': 11.553709607927617, 'ACC-135': 10.23188529730346, 'ACC-136': 11.410020848863214, 'ACC-137': 8.230773986293874, 'ACC-138': 14.087049775363605, 'ACC-139': 7.523490411583329, 'ACC-140': 11.163069603950392, 'ACC-141': 6.663928811271437, 'ACC-142': 8.594563957728015, 'ACC-143': 7.43009184501457, 'ACC-144': 10.809747292418773, 'ACC-145': 6.318567052036047, 'ACC-146': 7.787530556761325, 'ACC-147': 9.84017389318123, 'ACC-148': 4.878470720525907, 'ACC-149': 3.5073610467421648, 'ACC-150': 8.637296502325912, 'ACC-151': 2.9728770530852633, 'ACC-152': 5.699136826488637, 'ACC-153': 5.539332893603767, 'ACC-154': 5.618862771482281, 'ACC-155': 5.310319156541267, 'ACC-156': 4.143574628168086, 'ACC-157': 3.984440606315492, 'ACC-158': 3.3486650925621446, 'ACC-159': 2.588981164602041, 'ACC-160': 5.298347509387866, 'ACC-161': 3.878936297486218, 'ACC-162': 3.9412260005177018, 'ACC-163': 2.5227503068788244, 'ACC-164': 2.854841624941733, 'ACC-165': 1.8352936692215092, 'ACC-166': 3.8960472085351423, 'ACC-167': 3.7726671843270507, 'ACC-168': 5.39401505028483, 'ACC-169': 5.300293212936463, 'ACC-170': 4.509881170407598, 'ACC-171': 3.877048322125461, 'ACC-172': 1.7305626551098268, 'ACC-173': 3.843042223417986, 'ACC-174': 2.8265679492843176, 'ACC-175': 1.368328221284243, 'ACC-176': 1.537830856744834, 'ACC-177': 1.5177100000184547, 'ACC-178': 2.228077054572918, 'ACC-179': 3.1407175362815773, 'ACC-180': 3.877332452842968, 'ACC-181': 4.73943640286986, 'ACC-182': 0.9872325504810597, 'ACC-183': 4.216352639546464, 'ACC-184': 3.1953907508425368, 'ACC-185': 0.7524631736485456, 'ACC-186': 3.2612313597552482, 'ACC-187': 7.505285156863456, 'ACC-188': 8.766544933618498, 'ACC-189': 9.192058964728474, 'ACC-190': 3.4870620539316897, 'ACC-191': 4.146688417618271})])
[01/18 07:47:41] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 07:47:41] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 07:47:41] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 07:47:41] d2.evaluation.testing INFO: copypaste: 2.3491,19.9754,42.9120,29.9708,57.2827
[01/18 07:47:42] d2.utils.events INFO:  eta: 4:50:41  iter: 27999  total_loss: 33.76  loss_ce: 0.2567  loss_mask: 0.3411  loss_dice: 2.696  loss_ce_0: 0.5893  loss_mask_0: 0.3395  loss_dice_0: 2.837  loss_ce_1: 0.3121  loss_mask_1: 0.342  loss_dice_1: 2.744  loss_ce_2: 0.2804  loss_mask_2: 0.3406  loss_dice_2: 2.722  loss_ce_3: 0.27  loss_mask_3: 0.3401  loss_dice_3: 2.708  loss_ce_4: 0.2809  loss_mask_4: 0.3387  loss_dice_4: 2.695  loss_ce_5: 0.2565  loss_mask_5: 0.3414  loss_dice_5: 2.699  loss_ce_6: 0.2497  loss_mask_6: 0.3428  loss_dice_6: 2.708  loss_ce_7: 0.2487  loss_mask_7: 0.3414  loss_dice_7: 2.696  loss_ce_8: 0.2478  loss_mask_8: 0.3417  loss_dice_8: 2.693  time: 1.4836  data_time: 0.0716  lr: 3.3841e-06  max_mem: 21589M
[01/18 07:48:11] d2.utils.events INFO:  eta: 4:50:14  iter: 28019  total_loss: 33.74  loss_ce: 0.2487  loss_mask: 0.3363  loss_dice: 2.681  loss_ce_0: 0.5645  loss_mask_0: 0.3282  loss_dice_0: 2.832  loss_ce_1: 0.318  loss_mask_1: 0.3422  loss_dice_1: 2.718  loss_ce_2: 0.2932  loss_mask_2: 0.3387  loss_dice_2: 2.706  loss_ce_3: 0.2754  loss_mask_3: 0.3355  loss_dice_3: 2.691  loss_ce_4: 0.2712  loss_mask_4: 0.3375  loss_dice_4: 2.693  loss_ce_5: 0.2575  loss_mask_5: 0.3378  loss_dice_5: 2.689  loss_ce_6: 0.2645  loss_mask_6: 0.3385  loss_dice_6: 2.687  loss_ce_7: 0.2525  loss_mask_7: 0.3355  loss_dice_7: 2.691  loss_ce_8: 0.2459  loss_mask_8: 0.3363  loss_dice_8: 2.684  time: 1.4836  data_time: 0.0727  lr: 3.379e-06  max_mem: 21589M
[01/18 07:48:40] d2.utils.events INFO:  eta: 4:49:43  iter: 28039  total_loss: 34.16  loss_ce: 0.2593  loss_mask: 0.3423  loss_dice: 2.727  loss_ce_0: 0.5507  loss_mask_0: 0.3407  loss_dice_0: 2.863  loss_ce_1: 0.2784  loss_mask_1: 0.3525  loss_dice_1: 2.784  loss_ce_2: 0.2993  loss_mask_2: 0.3475  loss_dice_2: 2.759  loss_ce_3: 0.2811  loss_mask_3: 0.3427  loss_dice_3: 2.739  loss_ce_4: 0.2579  loss_mask_4: 0.3435  loss_dice_4: 2.742  loss_ce_5: 0.2708  loss_mask_5: 0.3424  loss_dice_5: 2.748  loss_ce_6: 0.2583  loss_mask_6: 0.3417  loss_dice_6: 2.736  loss_ce_7: 0.2654  loss_mask_7: 0.3423  loss_dice_7: 2.745  loss_ce_8: 0.2624  loss_mask_8: 0.3419  loss_dice_8: 2.735  time: 1.4836  data_time: 0.0699  lr: 3.3739e-06  max_mem: 21589M
[01/18 07:49:09] d2.utils.events INFO:  eta: 4:49:11  iter: 28059  total_loss: 32.68  loss_ce: 0.2273  loss_mask: 0.3495  loss_dice: 2.603  loss_ce_0: 0.5353  loss_mask_0: 0.3444  loss_dice_0: 2.737  loss_ce_1: 0.2859  loss_mask_1: 0.3533  loss_dice_1: 2.65  loss_ce_2: 0.2749  loss_mask_2: 0.3526  loss_dice_2: 2.619  loss_ce_3: 0.2608  loss_mask_3: 0.3504  loss_dice_3: 2.61  loss_ce_4: 0.2362  loss_mask_4: 0.3501  loss_dice_4: 2.597  loss_ce_5: 0.2495  loss_mask_5: 0.3486  loss_dice_5: 2.608  loss_ce_6: 0.2345  loss_mask_6: 0.3487  loss_dice_6: 2.606  loss_ce_7: 0.2282  loss_mask_7: 0.3496  loss_dice_7: 2.609  loss_ce_8: 0.2379  loss_mask_8: 0.349  loss_dice_8: 2.602  time: 1.4836  data_time: 0.0723  lr: 3.3689e-06  max_mem: 21589M
[01/18 07:49:38] d2.utils.events INFO:  eta: 4:48:44  iter: 28079  total_loss: 32.75  loss_ce: 0.2418  loss_mask: 0.3493  loss_dice: 2.599  loss_ce_0: 0.5364  loss_mask_0: 0.3414  loss_dice_0: 2.732  loss_ce_1: 0.276  loss_mask_1: 0.3499  loss_dice_1: 2.641  loss_ce_2: 0.2667  loss_mask_2: 0.3469  loss_dice_2: 2.617  loss_ce_3: 0.2551  loss_mask_3: 0.346  loss_dice_3: 2.607  loss_ce_4: 0.2534  loss_mask_4: 0.3464  loss_dice_4: 2.603  loss_ce_5: 0.2296  loss_mask_5: 0.3459  loss_dice_5: 2.595  loss_ce_6: 0.2325  loss_mask_6: 0.3466  loss_dice_6: 2.6  loss_ce_7: 0.2408  loss_mask_7: 0.3466  loss_dice_7: 2.597  loss_ce_8: 0.2279  loss_mask_8: 0.3478  loss_dice_8: 2.593  time: 1.4835  data_time: 0.0723  lr: 3.3638e-06  max_mem: 21589M
[01/18 07:50:07] d2.utils.events INFO:  eta: 4:48:15  iter: 28099  total_loss: 33.76  loss_ce: 0.2553  loss_mask: 0.3482  loss_dice: 2.713  loss_ce_0: 0.5617  loss_mask_0: 0.3461  loss_dice_0: 2.86  loss_ce_1: 0.2935  loss_mask_1: 0.3501  loss_dice_1: 2.758  loss_ce_2: 0.2912  loss_mask_2: 0.3467  loss_dice_2: 2.736  loss_ce_3: 0.2712  loss_mask_3: 0.3455  loss_dice_3: 2.729  loss_ce_4: 0.2608  loss_mask_4: 0.343  loss_dice_4: 2.722  loss_ce_5: 0.2483  loss_mask_5: 0.3449  loss_dice_5: 2.718  loss_ce_6: 0.2533  loss_mask_6: 0.3452  loss_dice_6: 2.721  loss_ce_7: 0.2503  loss_mask_7: 0.347  loss_dice_7: 2.716  loss_ce_8: 0.2467  loss_mask_8: 0.3474  loss_dice_8: 2.715  time: 1.4835  data_time: 0.0694  lr: 3.3587e-06  max_mem: 21589M
[01/18 07:50:36] d2.utils.events INFO:  eta: 4:47:40  iter: 28119  total_loss: 33.48  loss_ce: 0.2456  loss_mask: 0.3459  loss_dice: 2.715  loss_ce_0: 0.5621  loss_mask_0: 0.3398  loss_dice_0: 2.857  loss_ce_1: 0.2941  loss_mask_1: 0.3517  loss_dice_1: 2.766  loss_ce_2: 0.2923  loss_mask_2: 0.347  loss_dice_2: 2.73  loss_ce_3: 0.2606  loss_mask_3: 0.3465  loss_dice_3: 2.723  loss_ce_4: 0.2535  loss_mask_4: 0.3474  loss_dice_4: 2.718  loss_ce_5: 0.258  loss_mask_5: 0.3464  loss_dice_5: 2.715  loss_ce_6: 0.243  loss_mask_6: 0.3455  loss_dice_6: 2.72  loss_ce_7: 0.2515  loss_mask_7: 0.3471  loss_dice_7: 2.722  loss_ce_8: 0.2334  loss_mask_8: 0.3455  loss_dice_8: 2.714  time: 1.4835  data_time: 0.0709  lr: 3.3536e-06  max_mem: 21589M
[01/18 07:51:06] d2.utils.events INFO:  eta: 4:46:58  iter: 28139  total_loss: 33.81  loss_ce: 0.249  loss_mask: 0.354  loss_dice: 2.711  loss_ce_0: 0.5594  loss_mask_0: 0.3561  loss_dice_0: 2.831  loss_ce_1: 0.2721  loss_mask_1: 0.3591  loss_dice_1: 2.748  loss_ce_2: 0.2857  loss_mask_2: 0.3582  loss_dice_2: 2.72  loss_ce_3: 0.2525  loss_mask_3: 0.3539  loss_dice_3: 2.718  loss_ce_4: 0.2513  loss_mask_4: 0.3545  loss_dice_4: 2.721  loss_ce_5: 0.2656  loss_mask_5: 0.3541  loss_dice_5: 2.72  loss_ce_6: 0.246  loss_mask_6: 0.3534  loss_dice_6: 2.712  loss_ce_7: 0.2349  loss_mask_7: 0.354  loss_dice_7: 2.711  loss_ce_8: 0.2324  loss_mask_8: 0.3535  loss_dice_8: 2.713  time: 1.4835  data_time: 0.0729  lr: 3.3485e-06  max_mem: 21589M
[01/18 07:51:35] d2.utils.events INFO:  eta: 4:46:21  iter: 28159  total_loss: 32.59  loss_ce: 0.242  loss_mask: 0.3391  loss_dice: 2.602  loss_ce_0: 0.5563  loss_mask_0: 0.3322  loss_dice_0: 2.755  loss_ce_1: 0.2876  loss_mask_1: 0.3425  loss_dice_1: 2.648  loss_ce_2: 0.2782  loss_mask_2: 0.3418  loss_dice_2: 2.631  loss_ce_3: 0.2774  loss_mask_3: 0.3396  loss_dice_3: 2.607  loss_ce_4: 0.2538  loss_mask_4: 0.3389  loss_dice_4: 2.614  loss_ce_5: 0.2534  loss_mask_5: 0.3387  loss_dice_5: 2.62  loss_ce_6: 0.2629  loss_mask_6: 0.3384  loss_dice_6: 2.607  loss_ce_7: 0.2401  loss_mask_7: 0.3388  loss_dice_7: 2.602  loss_ce_8: 0.2524  loss_mask_8: 0.3385  loss_dice_8: 2.612  time: 1.4835  data_time: 0.0697  lr: 3.3435e-06  max_mem: 21589M
[01/18 07:52:04] d2.utils.events INFO:  eta: 4:46:03  iter: 28179  total_loss: 33.63  loss_ce: 0.2415  loss_mask: 0.3465  loss_dice: 2.688  loss_ce_0: 0.5593  loss_mask_0: 0.3411  loss_dice_0: 2.833  loss_ce_1: 0.2856  loss_mask_1: 0.3513  loss_dice_1: 2.74  loss_ce_2: 0.2841  loss_mask_2: 0.3477  loss_dice_2: 2.716  loss_ce_3: 0.2619  loss_mask_3: 0.3476  loss_dice_3: 2.703  loss_ce_4: 0.2617  loss_mask_4: 0.3476  loss_dice_4: 2.699  loss_ce_5: 0.2549  loss_mask_5: 0.3471  loss_dice_5: 2.691  loss_ce_6: 0.2455  loss_mask_6: 0.3472  loss_dice_6: 2.699  loss_ce_7: 0.2526  loss_mask_7: 0.3484  loss_dice_7: 2.696  loss_ce_8: 0.2414  loss_mask_8: 0.3465  loss_dice_8: 2.694  time: 1.4835  data_time: 0.0746  lr: 3.3384e-06  max_mem: 21589M
[01/18 07:52:33] d2.utils.events INFO:  eta: 4:45:16  iter: 28199  total_loss: 32.7  loss_ce: 0.2282  loss_mask: 0.3392  loss_dice: 2.605  loss_ce_0: 0.5588  loss_mask_0: 0.3364  loss_dice_0: 2.741  loss_ce_1: 0.2923  loss_mask_1: 0.3463  loss_dice_1: 2.657  loss_ce_2: 0.2752  loss_mask_2: 0.3436  loss_dice_2: 2.638  loss_ce_3: 0.2664  loss_mask_3: 0.3413  loss_dice_3: 2.617  loss_ce_4: 0.2378  loss_mask_4: 0.3396  loss_dice_4: 2.618  loss_ce_5: 0.2463  loss_mask_5: 0.3378  loss_dice_5: 2.607  loss_ce_6: 0.2355  loss_mask_6: 0.3377  loss_dice_6: 2.604  loss_ce_7: 0.237  loss_mask_7: 0.3379  loss_dice_7: 2.612  loss_ce_8: 0.2285  loss_mask_8: 0.3387  loss_dice_8: 2.616  time: 1.4834  data_time: 0.0698  lr: 3.3333e-06  max_mem: 21589M
[01/18 07:53:02] d2.utils.events INFO:  eta: 4:44:38  iter: 28219  total_loss: 33.26  loss_ce: 0.2649  loss_mask: 0.3446  loss_dice: 2.636  loss_ce_0: 0.6052  loss_mask_0: 0.3395  loss_dice_0: 2.774  loss_ce_1: 0.3107  loss_mask_1: 0.3472  loss_dice_1: 2.684  loss_ce_2: 0.3084  loss_mask_2: 0.3464  loss_dice_2: 2.658  loss_ce_3: 0.2724  loss_mask_3: 0.3453  loss_dice_3: 2.641  loss_ce_4: 0.2824  loss_mask_4: 0.3445  loss_dice_4: 2.644  loss_ce_5: 0.2778  loss_mask_5: 0.3433  loss_dice_5: 2.647  loss_ce_6: 0.2669  loss_mask_6: 0.3446  loss_dice_6: 2.644  loss_ce_7: 0.2544  loss_mask_7: 0.3443  loss_dice_7: 2.64  loss_ce_8: 0.2461  loss_mask_8: 0.3461  loss_dice_8: 2.639  time: 1.4834  data_time: 0.0703  lr: 3.3282e-06  max_mem: 21589M
[01/18 07:53:32] d2.utils.events INFO:  eta: 4:44:16  iter: 28239  total_loss: 33.14  loss_ce: 0.2473  loss_mask: 0.3356  loss_dice: 2.623  loss_ce_0: 0.5524  loss_mask_0: 0.328  loss_dice_0: 2.771  loss_ce_1: 0.2894  loss_mask_1: 0.3381  loss_dice_1: 2.676  loss_ce_2: 0.2799  loss_mask_2: 0.3368  loss_dice_2: 2.65  loss_ce_3: 0.2693  loss_mask_3: 0.3381  loss_dice_3: 2.627  loss_ce_4: 0.2519  loss_mask_4: 0.3381  loss_dice_4: 2.616  loss_ce_5: 0.2523  loss_mask_5: 0.3373  loss_dice_5: 2.625  loss_ce_6: 0.2411  loss_mask_6: 0.3376  loss_dice_6: 2.622  loss_ce_7: 0.2544  loss_mask_7: 0.3359  loss_dice_7: 2.625  loss_ce_8: 0.2475  loss_mask_8: 0.3369  loss_dice_8: 2.631  time: 1.4834  data_time: 0.0690  lr: 3.3231e-06  max_mem: 21589M
[01/18 07:54:00] d2.utils.events INFO:  eta: 4:43:50  iter: 28259  total_loss: 32.99  loss_ce: 0.2382  loss_mask: 0.3468  loss_dice: 2.655  loss_ce_0: 0.5314  loss_mask_0: 0.3453  loss_dice_0: 2.781  loss_ce_1: 0.2874  loss_mask_1: 0.35  loss_dice_1: 2.703  loss_ce_2: 0.2763  loss_mask_2: 0.3479  loss_dice_2: 2.674  loss_ce_3: 0.2543  loss_mask_3: 0.3479  loss_dice_3: 2.651  loss_ce_4: 0.25  loss_mask_4: 0.3463  loss_dice_4: 2.66  loss_ce_5: 0.242  loss_mask_5: 0.3465  loss_dice_5: 2.654  loss_ce_6: 0.2441  loss_mask_6: 0.3452  loss_dice_6: 2.659  loss_ce_7: 0.2458  loss_mask_7: 0.3464  loss_dice_7: 2.659  loss_ce_8: 0.2536  loss_mask_8: 0.3465  loss_dice_8: 2.652  time: 1.4834  data_time: 0.0668  lr: 3.318e-06  max_mem: 21589M
[01/18 07:54:29] d2.utils.events INFO:  eta: 4:43:21  iter: 28279  total_loss: 33.48  loss_ce: 0.2515  loss_mask: 0.3505  loss_dice: 2.621  loss_ce_0: 0.5502  loss_mask_0: 0.3496  loss_dice_0: 2.772  loss_ce_1: 0.2737  loss_mask_1: 0.3566  loss_dice_1: 2.673  loss_ce_2: 0.2861  loss_mask_2: 0.3549  loss_dice_2: 2.651  loss_ce_3: 0.2669  loss_mask_3: 0.3535  loss_dice_3: 2.641  loss_ce_4: 0.2645  loss_mask_4: 0.3506  loss_dice_4: 2.636  loss_ce_5: 0.2574  loss_mask_5: 0.3518  loss_dice_5: 2.634  loss_ce_6: 0.2378  loss_mask_6: 0.3497  loss_dice_6: 2.632  loss_ce_7: 0.2503  loss_mask_7: 0.3505  loss_dice_7: 2.635  loss_ce_8: 0.2352  loss_mask_8: 0.3491  loss_dice_8: 2.629  time: 1.4833  data_time: 0.0671  lr: 3.3129e-06  max_mem: 21589M
[01/18 07:54:58] d2.utils.events INFO:  eta: 4:42:45  iter: 28299  total_loss: 33.79  loss_ce: 0.2452  loss_mask: 0.3498  loss_dice: 2.673  loss_ce_0: 0.562  loss_mask_0: 0.3454  loss_dice_0: 2.812  loss_ce_1: 0.2932  loss_mask_1: 0.3571  loss_dice_1: 2.728  loss_ce_2: 0.2864  loss_mask_2: 0.3535  loss_dice_2: 2.709  loss_ce_3: 0.2591  loss_mask_3: 0.3508  loss_dice_3: 2.686  loss_ce_4: 0.2462  loss_mask_4: 0.3499  loss_dice_4: 2.69  loss_ce_5: 0.2448  loss_mask_5: 0.3495  loss_dice_5: 2.688  loss_ce_6: 0.2437  loss_mask_6: 0.3505  loss_dice_6: 2.675  loss_ce_7: 0.258  loss_mask_7: 0.3486  loss_dice_7: 2.673  loss_ce_8: 0.2373  loss_mask_8: 0.3494  loss_dice_8: 2.678  time: 1.4833  data_time: 0.0664  lr: 3.3079e-06  max_mem: 21589M
[01/18 07:55:27] d2.utils.events INFO:  eta: 4:42:16  iter: 28319  total_loss: 33.58  loss_ce: 0.2329  loss_mask: 0.3417  loss_dice: 2.686  loss_ce_0: 0.5762  loss_mask_0: 0.337  loss_dice_0: 2.815  loss_ce_1: 0.2709  loss_mask_1: 0.3461  loss_dice_1: 2.729  loss_ce_2: 0.2502  loss_mask_2: 0.3426  loss_dice_2: 2.705  loss_ce_3: 0.2425  loss_mask_3: 0.3422  loss_dice_3: 2.69  loss_ce_4: 0.2505  loss_mask_4: 0.3438  loss_dice_4: 2.683  loss_ce_5: 0.2216  loss_mask_5: 0.3425  loss_dice_5: 2.699  loss_ce_6: 0.2393  loss_mask_6: 0.342  loss_dice_6: 2.691  loss_ce_7: 0.2321  loss_mask_7: 0.3429  loss_dice_7: 2.691  loss_ce_8: 0.2386  loss_mask_8: 0.342  loss_dice_8: 2.69  time: 1.4833  data_time: 0.0694  lr: 3.3028e-06  max_mem: 21589M
[01/18 07:55:56] d2.utils.events INFO:  eta: 4:41:48  iter: 28339  total_loss: 33.66  loss_ce: 0.2236  loss_mask: 0.3457  loss_dice: 2.728  loss_ce_0: 0.5649  loss_mask_0: 0.3398  loss_dice_0: 2.858  loss_ce_1: 0.289  loss_mask_1: 0.3489  loss_dice_1: 2.776  loss_ce_2: 0.2684  loss_mask_2: 0.3481  loss_dice_2: 2.755  loss_ce_3: 0.2514  loss_mask_3: 0.3478  loss_dice_3: 2.744  loss_ce_4: 0.2443  loss_mask_4: 0.3478  loss_dice_4: 2.736  loss_ce_5: 0.2391  loss_mask_5: 0.3453  loss_dice_5: 2.731  loss_ce_6: 0.2285  loss_mask_6: 0.3467  loss_dice_6: 2.728  loss_ce_7: 0.2285  loss_mask_7: 0.3458  loss_dice_7: 2.728  loss_ce_8: 0.2318  loss_mask_8: 0.3467  loss_dice_8: 2.726  time: 1.4833  data_time: 0.0680  lr: 3.2977e-06  max_mem: 21589M
[01/18 07:56:26] d2.utils.events INFO:  eta: 4:41:18  iter: 28359  total_loss: 33.17  loss_ce: 0.2357  loss_mask: 0.3545  loss_dice: 2.681  loss_ce_0: 0.5584  loss_mask_0: 0.3494  loss_dice_0: 2.791  loss_ce_1: 0.2674  loss_mask_1: 0.3558  loss_dice_1: 2.708  loss_ce_2: 0.2504  loss_mask_2: 0.3537  loss_dice_2: 2.695  loss_ce_3: 0.2389  loss_mask_3: 0.354  loss_dice_3: 2.673  loss_ce_4: 0.2208  loss_mask_4: 0.3535  loss_dice_4: 2.684  loss_ce_5: 0.2239  loss_mask_5: 0.3543  loss_dice_5: 2.673  loss_ce_6: 0.2342  loss_mask_6: 0.3549  loss_dice_6: 2.677  loss_ce_7: 0.2371  loss_mask_7: 0.3549  loss_dice_7: 2.679  loss_ce_8: 0.2347  loss_mask_8: 0.3557  loss_dice_8: 2.68  time: 1.4832  data_time: 0.0677  lr: 3.2926e-06  max_mem: 21589M
[01/18 07:56:54] d2.utils.events INFO:  eta: 4:40:50  iter: 28379  total_loss: 33.33  loss_ce: 0.2397  loss_mask: 0.3487  loss_dice: 2.664  loss_ce_0: 0.5517  loss_mask_0: 0.3503  loss_dice_0: 2.799  loss_ce_1: 0.2892  loss_mask_1: 0.3564  loss_dice_1: 2.711  loss_ce_2: 0.2847  loss_mask_2: 0.351  loss_dice_2: 2.69  loss_ce_3: 0.2649  loss_mask_3: 0.3487  loss_dice_3: 2.672  loss_ce_4: 0.2411  loss_mask_4: 0.3484  loss_dice_4: 2.68  loss_ce_5: 0.25  loss_mask_5: 0.348  loss_dice_5: 2.672  loss_ce_6: 0.2444  loss_mask_6: 0.3479  loss_dice_6: 2.67  loss_ce_7: 0.2326  loss_mask_7: 0.3482  loss_dice_7: 2.671  loss_ce_8: 0.2365  loss_mask_8: 0.3484  loss_dice_8: 2.673  time: 1.4832  data_time: 0.0665  lr: 3.2875e-06  max_mem: 21589M
[01/18 07:57:24] d2.utils.events INFO:  eta: 4:40:21  iter: 28399  total_loss: 33.19  loss_ce: 0.2351  loss_mask: 0.3384  loss_dice: 2.658  loss_ce_0: 0.5561  loss_mask_0: 0.3401  loss_dice_0: 2.8  loss_ce_1: 0.2945  loss_mask_1: 0.3472  loss_dice_1: 2.705  loss_ce_2: 0.2696  loss_mask_2: 0.3423  loss_dice_2: 2.676  loss_ce_3: 0.268  loss_mask_3: 0.3411  loss_dice_3: 2.669  loss_ce_4: 0.2546  loss_mask_4: 0.3382  loss_dice_4: 2.659  loss_ce_5: 0.246  loss_mask_5: 0.3397  loss_dice_5: 2.657  loss_ce_6: 0.2299  loss_mask_6: 0.34  loss_dice_6: 2.66  loss_ce_7: 0.2325  loss_mask_7: 0.3378  loss_dice_7: 2.655  loss_ce_8: 0.2398  loss_mask_8: 0.3361  loss_dice_8: 2.662  time: 1.4832  data_time: 0.0696  lr: 3.2824e-06  max_mem: 21589M
[01/18 07:57:53] d2.utils.events INFO:  eta: 4:39:52  iter: 28419  total_loss: 33.49  loss_ce: 0.2574  loss_mask: 0.337  loss_dice: 2.657  loss_ce_0: 0.5699  loss_mask_0: 0.3348  loss_dice_0: 2.798  loss_ce_1: 0.3073  loss_mask_1: 0.341  loss_dice_1: 2.7  loss_ce_2: 0.2917  loss_mask_2: 0.3396  loss_dice_2: 2.675  loss_ce_3: 0.2664  loss_mask_3: 0.3361  loss_dice_3: 2.666  loss_ce_4: 0.2752  loss_mask_4: 0.3379  loss_dice_4: 2.654  loss_ce_5: 0.2598  loss_mask_5: 0.336  loss_dice_5: 2.651  loss_ce_6: 0.241  loss_mask_6: 0.3355  loss_dice_6: 2.658  loss_ce_7: 0.2374  loss_mask_7: 0.3371  loss_dice_7: 2.653  loss_ce_8: 0.2486  loss_mask_8: 0.3352  loss_dice_8: 2.655  time: 1.4832  data_time: 0.0721  lr: 3.2773e-06  max_mem: 21589M
[01/18 07:58:22] d2.utils.events INFO:  eta: 4:39:22  iter: 28439  total_loss: 33.03  loss_ce: 0.2606  loss_mask: 0.3474  loss_dice: 2.622  loss_ce_0: 0.5506  loss_mask_0: 0.343  loss_dice_0: 2.765  loss_ce_1: 0.3021  loss_mask_1: 0.3514  loss_dice_1: 2.669  loss_ce_2: 0.3115  loss_mask_2: 0.3468  loss_dice_2: 2.647  loss_ce_3: 0.2914  loss_mask_3: 0.3484  loss_dice_3: 2.627  loss_ce_4: 0.2723  loss_mask_4: 0.3483  loss_dice_4: 2.63  loss_ce_5: 0.2738  loss_mask_5: 0.3471  loss_dice_5: 2.629  loss_ce_6: 0.2633  loss_mask_6: 0.3482  loss_dice_6: 2.628  loss_ce_7: 0.2559  loss_mask_7: 0.3476  loss_dice_7: 2.629  loss_ce_8: 0.2544  loss_mask_8: 0.3474  loss_dice_8: 2.638  time: 1.4831  data_time: 0.0660  lr: 3.2722e-06  max_mem: 21589M
[01/18 07:58:50] d2.utils.events INFO:  eta: 4:38:41  iter: 28459  total_loss: 32.7  loss_ce: 0.2341  loss_mask: 0.3458  loss_dice: 2.641  loss_ce_0: 0.5436  loss_mask_0: 0.3489  loss_dice_0: 2.766  loss_ce_1: 0.2628  loss_mask_1: 0.355  loss_dice_1: 2.689  loss_ce_2: 0.2477  loss_mask_2: 0.351  loss_dice_2: 2.664  loss_ce_3: 0.2423  loss_mask_3: 0.3465  loss_dice_3: 2.64  loss_ce_4: 0.2562  loss_mask_4: 0.3452  loss_dice_4: 2.647  loss_ce_5: 0.2406  loss_mask_5: 0.3453  loss_dice_5: 2.652  loss_ce_6: 0.2245  loss_mask_6: 0.3458  loss_dice_6: 2.646  loss_ce_7: 0.2371  loss_mask_7: 0.3427  loss_dice_7: 2.64  loss_ce_8: 0.2313  loss_mask_8: 0.3446  loss_dice_8: 2.641  time: 1.4831  data_time: 0.0673  lr: 3.2671e-06  max_mem: 21589M
[01/18 07:59:19] d2.utils.events INFO:  eta: 4:38:03  iter: 28479  total_loss: 33.11  loss_ce: 0.2313  loss_mask: 0.3487  loss_dice: 2.699  loss_ce_0: 0.546  loss_mask_0: 0.3517  loss_dice_0: 2.826  loss_ce_1: 0.2759  loss_mask_1: 0.3579  loss_dice_1: 2.739  loss_ce_2: 0.2721  loss_mask_2: 0.3548  loss_dice_2: 2.715  loss_ce_3: 0.2554  loss_mask_3: 0.3499  loss_dice_3: 2.711  loss_ce_4: 0.2417  loss_mask_4: 0.3507  loss_dice_4: 2.716  loss_ce_5: 0.2361  loss_mask_5: 0.3513  loss_dice_5: 2.707  loss_ce_6: 0.2342  loss_mask_6: 0.35  loss_dice_6: 2.703  loss_ce_7: 0.2358  loss_mask_7: 0.3503  loss_dice_7: 2.704  loss_ce_8: 0.2304  loss_mask_8: 0.3506  loss_dice_8: 2.701  time: 1.4831  data_time: 0.0698  lr: 3.262e-06  max_mem: 21589M
[01/18 07:59:49] d2.utils.events INFO:  eta: 4:37:44  iter: 28499  total_loss: 33.17  loss_ce: 0.242  loss_mask: 0.3382  loss_dice: 2.665  loss_ce_0: 0.5674  loss_mask_0: 0.3381  loss_dice_0: 2.8  loss_ce_1: 0.2882  loss_mask_1: 0.3404  loss_dice_1: 2.711  loss_ce_2: 0.2714  loss_mask_2: 0.3394  loss_dice_2: 2.68  loss_ce_3: 0.2623  loss_mask_3: 0.3368  loss_dice_3: 2.674  loss_ce_4: 0.2576  loss_mask_4: 0.3394  loss_dice_4: 2.674  loss_ce_5: 0.2469  loss_mask_5: 0.3381  loss_dice_5: 2.666  loss_ce_6: 0.2424  loss_mask_6: 0.3376  loss_dice_6: 2.661  loss_ce_7: 0.2331  loss_mask_7: 0.3388  loss_dice_7: 2.672  loss_ce_8: 0.2335  loss_mask_8: 0.3385  loss_dice_8: 2.664  time: 1.4831  data_time: 0.0696  lr: 3.2569e-06  max_mem: 21589M
[01/18 08:00:17] d2.utils.events INFO:  eta: 4:36:53  iter: 28519  total_loss: 32.79  loss_ce: 0.2367  loss_mask: 0.3443  loss_dice: 2.596  loss_ce_0: 0.5507  loss_mask_0: 0.3406  loss_dice_0: 2.732  loss_ce_1: 0.2996  loss_mask_1: 0.3491  loss_dice_1: 2.633  loss_ce_2: 0.2944  loss_mask_2: 0.3447  loss_dice_2: 2.611  loss_ce_3: 0.2698  loss_mask_3: 0.343  loss_dice_3: 2.61  loss_ce_4: 0.247  loss_mask_4: 0.344  loss_dice_4: 2.602  loss_ce_5: 0.2469  loss_mask_5: 0.3443  loss_dice_5: 2.593  loss_ce_6: 0.2485  loss_mask_6: 0.3435  loss_dice_6: 2.606  loss_ce_7: 0.2397  loss_mask_7: 0.3436  loss_dice_7: 2.593  loss_ce_8: 0.2366  loss_mask_8: 0.3437  loss_dice_8: 2.596  time: 1.4830  data_time: 0.0664  lr: 3.2518e-06  max_mem: 21589M
[01/18 08:00:47] d2.utils.events INFO:  eta: 4:36:25  iter: 28539  total_loss: 33.02  loss_ce: 0.2495  loss_mask: 0.35  loss_dice: 2.635  loss_ce_0: 0.5448  loss_mask_0: 0.355  loss_dice_0: 2.795  loss_ce_1: 0.3214  loss_mask_1: 0.3529  loss_dice_1: 2.682  loss_ce_2: 0.2774  loss_mask_2: 0.3525  loss_dice_2: 2.654  loss_ce_3: 0.2712  loss_mask_3: 0.3512  loss_dice_3: 2.646  loss_ce_4: 0.2595  loss_mask_4: 0.3502  loss_dice_4: 2.65  loss_ce_5: 0.2528  loss_mask_5: 0.3483  loss_dice_5: 2.638  loss_ce_6: 0.2375  loss_mask_6: 0.3485  loss_dice_6: 2.64  loss_ce_7: 0.2477  loss_mask_7: 0.3504  loss_dice_7: 2.63  loss_ce_8: 0.2365  loss_mask_8: 0.3487  loss_dice_8: 2.635  time: 1.4830  data_time: 0.0718  lr: 3.2467e-06  max_mem: 21589M
[01/18 08:01:16] d2.utils.events INFO:  eta: 4:36:02  iter: 28559  total_loss: 32.62  loss_ce: 0.2371  loss_mask: 0.3375  loss_dice: 2.593  loss_ce_0: 0.5517  loss_mask_0: 0.3334  loss_dice_0: 2.755  loss_ce_1: 0.2796  loss_mask_1: 0.3429  loss_dice_1: 2.649  loss_ce_2: 0.2904  loss_mask_2: 0.3411  loss_dice_2: 2.627  loss_ce_3: 0.2722  loss_mask_3: 0.3388  loss_dice_3: 2.611  loss_ce_4: 0.2538  loss_mask_4: 0.3375  loss_dice_4: 2.611  loss_ce_5: 0.2602  loss_mask_5: 0.3359  loss_dice_5: 2.605  loss_ce_6: 0.2461  loss_mask_6: 0.3355  loss_dice_6: 2.608  loss_ce_7: 0.2427  loss_mask_7: 0.3374  loss_dice_7: 2.602  loss_ce_8: 0.242  loss_mask_8: 0.3383  loss_dice_8: 2.602  time: 1.4830  data_time: 0.0711  lr: 3.2416e-06  max_mem: 21589M
[01/18 08:01:46] d2.utils.events INFO:  eta: 4:35:41  iter: 28579  total_loss: 32.85  loss_ce: 0.2394  loss_mask: 0.3437  loss_dice: 2.627  loss_ce_0: 0.5688  loss_mask_0: 0.3437  loss_dice_0: 2.753  loss_ce_1: 0.3089  loss_mask_1: 0.3506  loss_dice_1: 2.664  loss_ce_2: 0.2947  loss_mask_2: 0.3464  loss_dice_2: 2.641  loss_ce_3: 0.2789  loss_mask_3: 0.3444  loss_dice_3: 2.639  loss_ce_4: 0.2503  loss_mask_4: 0.3444  loss_dice_4: 2.625  loss_ce_5: 0.248  loss_mask_5: 0.3442  loss_dice_5: 2.638  loss_ce_6: 0.2509  loss_mask_6: 0.3432  loss_dice_6: 2.635  loss_ce_7: 0.2379  loss_mask_7: 0.3433  loss_dice_7: 2.627  loss_ce_8: 0.2391  loss_mask_8: 0.3433  loss_dice_8: 2.623  time: 1.4830  data_time: 0.0715  lr: 3.2365e-06  max_mem: 21589M
[01/18 08:02:15] d2.utils.events INFO:  eta: 4:35:19  iter: 28599  total_loss: 32.67  loss_ce: 0.2273  loss_mask: 0.3451  loss_dice: 2.619  loss_ce_0: 0.5347  loss_mask_0: 0.34  loss_dice_0: 2.754  loss_ce_1: 0.2924  loss_mask_1: 0.3486  loss_dice_1: 2.668  loss_ce_2: 0.2627  loss_mask_2: 0.3446  loss_dice_2: 2.641  loss_ce_3: 0.2553  loss_mask_3: 0.3443  loss_dice_3: 2.626  loss_ce_4: 0.239  loss_mask_4: 0.3454  loss_dice_4: 2.626  loss_ce_5: 0.2251  loss_mask_5: 0.3435  loss_dice_5: 2.618  loss_ce_6: 0.222  loss_mask_6: 0.3434  loss_dice_6: 2.623  loss_ce_7: 0.2282  loss_mask_7: 0.3441  loss_dice_7: 2.624  loss_ce_8: 0.2242  loss_mask_8: 0.3458  loss_dice_8: 2.623  time: 1.4830  data_time: 0.0742  lr: 3.2314e-06  max_mem: 21589M
[01/18 08:02:45] d2.utils.events INFO:  eta: 4:34:51  iter: 28619  total_loss: 33.47  loss_ce: 0.2509  loss_mask: 0.3491  loss_dice: 2.67  loss_ce_0: 0.5433  loss_mask_0: 0.3463  loss_dice_0: 2.796  loss_ce_1: 0.2783  loss_mask_1: 0.3545  loss_dice_1: 2.705  loss_ce_2: 0.2887  loss_mask_2: 0.3531  loss_dice_2: 2.689  loss_ce_3: 0.2658  loss_mask_3: 0.3517  loss_dice_3: 2.671  loss_ce_4: 0.2697  loss_mask_4: 0.3507  loss_dice_4: 2.671  loss_ce_5: 0.2562  loss_mask_5: 0.3503  loss_dice_5: 2.674  loss_ce_6: 0.2665  loss_mask_6: 0.3486  loss_dice_6: 2.668  loss_ce_7: 0.2442  loss_mask_7: 0.3491  loss_dice_7: 2.662  loss_ce_8: 0.2703  loss_mask_8: 0.3489  loss_dice_8: 2.664  time: 1.4830  data_time: 0.0730  lr: 3.2263e-06  max_mem: 21589M
[01/18 08:03:15] d2.utils.events INFO:  eta: 4:34:22  iter: 28639  total_loss: 32.3  loss_ce: 0.2289  loss_mask: 0.3425  loss_dice: 2.598  loss_ce_0: 0.5001  loss_mask_0: 0.344  loss_dice_0: 2.745  loss_ce_1: 0.2728  loss_mask_1: 0.3485  loss_dice_1: 2.646  loss_ce_2: 0.2633  loss_mask_2: 0.3453  loss_dice_2: 2.619  loss_ce_3: 0.2541  loss_mask_3: 0.3423  loss_dice_3: 2.605  loss_ce_4: 0.2293  loss_mask_4: 0.3431  loss_dice_4: 2.604  loss_ce_5: 0.2266  loss_mask_5: 0.3422  loss_dice_5: 2.602  loss_ce_6: 0.2261  loss_mask_6: 0.3413  loss_dice_6: 2.598  loss_ce_7: 0.2206  loss_mask_7: 0.3425  loss_dice_7: 2.609  loss_ce_8: 0.2229  loss_mask_8: 0.3432  loss_dice_8: 2.6  time: 1.4830  data_time: 0.0776  lr: 3.2212e-06  max_mem: 21589M
[01/18 08:03:44] d2.utils.events INFO:  eta: 4:33:53  iter: 28659  total_loss: 32.82  loss_ce: 0.2241  loss_mask: 0.3447  loss_dice: 2.642  loss_ce_0: 0.5265  loss_mask_0: 0.3438  loss_dice_0: 2.783  loss_ce_1: 0.2772  loss_mask_1: 0.3521  loss_dice_1: 2.687  loss_ce_2: 0.2728  loss_mask_2: 0.3495  loss_dice_2: 2.661  loss_ce_3: 0.26  loss_mask_3: 0.3505  loss_dice_3: 2.645  loss_ce_4: 0.2498  loss_mask_4: 0.3469  loss_dice_4: 2.643  loss_ce_5: 0.2356  loss_mask_5: 0.3469  loss_dice_5: 2.644  loss_ce_6: 0.233  loss_mask_6: 0.3451  loss_dice_6: 2.64  loss_ce_7: 0.2254  loss_mask_7: 0.3462  loss_dice_7: 2.649  loss_ce_8: 0.2296  loss_mask_8: 0.3457  loss_dice_8: 2.636  time: 1.4830  data_time: 0.0688  lr: 3.2161e-06  max_mem: 21589M
[01/18 08:04:13] d2.utils.events INFO:  eta: 4:33:27  iter: 28679  total_loss: 33.72  loss_ce: 0.2615  loss_mask: 0.3481  loss_dice: 2.656  loss_ce_0: 0.5559  loss_mask_0: 0.3451  loss_dice_0: 2.785  loss_ce_1: 0.2993  loss_mask_1: 0.3499  loss_dice_1: 2.711  loss_ce_2: 0.3006  loss_mask_2: 0.3482  loss_dice_2: 2.674  loss_ce_3: 0.2948  loss_mask_3: 0.3499  loss_dice_3: 2.661  loss_ce_4: 0.2767  loss_mask_4: 0.3487  loss_dice_4: 2.65  loss_ce_5: 0.2889  loss_mask_5: 0.348  loss_dice_5: 2.664  loss_ce_6: 0.2709  loss_mask_6: 0.3477  loss_dice_6: 2.658  loss_ce_7: 0.267  loss_mask_7: 0.3481  loss_dice_7: 2.648  loss_ce_8: 0.2703  loss_mask_8: 0.3479  loss_dice_8: 2.65  time: 1.4830  data_time: 0.0681  lr: 3.211e-06  max_mem: 21589M
[01/18 08:04:43] d2.utils.events INFO:  eta: 4:33:06  iter: 28699  total_loss: 32.95  loss_ce: 0.223  loss_mask: 0.3429  loss_dice: 2.642  loss_ce_0: 0.5458  loss_mask_0: 0.3417  loss_dice_0: 2.783  loss_ce_1: 0.2655  loss_mask_1: 0.3537  loss_dice_1: 2.691  loss_ce_2: 0.281  loss_mask_2: 0.3484  loss_dice_2: 2.67  loss_ce_3: 0.2539  loss_mask_3: 0.3455  loss_dice_3: 2.647  loss_ce_4: 0.2559  loss_mask_4: 0.3458  loss_dice_4: 2.647  loss_ce_5: 0.2404  loss_mask_5: 0.3443  loss_dice_5: 2.643  loss_ce_6: 0.2404  loss_mask_6: 0.3435  loss_dice_6: 2.639  loss_ce_7: 0.2248  loss_mask_7: 0.3432  loss_dice_7: 2.637  loss_ce_8: 0.2293  loss_mask_8: 0.3424  loss_dice_8: 2.64  time: 1.4830  data_time: 0.0718  lr: 3.2059e-06  max_mem: 21589M
[01/18 08:05:12] d2.utils.events INFO:  eta: 4:32:37  iter: 28719  total_loss: 34.18  loss_ce: 0.2417  loss_mask: 0.343  loss_dice: 2.732  loss_ce_0: 0.586  loss_mask_0: 0.3432  loss_dice_0: 2.868  loss_ce_1: 0.3002  loss_mask_1: 0.3507  loss_dice_1: 2.78  loss_ce_2: 0.2936  loss_mask_2: 0.3465  loss_dice_2: 2.762  loss_ce_3: 0.2867  loss_mask_3: 0.3429  loss_dice_3: 2.744  loss_ce_4: 0.2767  loss_mask_4: 0.3421  loss_dice_4: 2.748  loss_ce_5: 0.2625  loss_mask_5: 0.3399  loss_dice_5: 2.745  loss_ce_6: 0.2505  loss_mask_6: 0.3404  loss_dice_6: 2.742  loss_ce_7: 0.2554  loss_mask_7: 0.3403  loss_dice_7: 2.739  loss_ce_8: 0.2485  loss_mask_8: 0.3403  loss_dice_8: 2.741  time: 1.4829  data_time: 0.0743  lr: 3.2008e-06  max_mem: 21589M
[01/18 08:05:41] d2.utils.events INFO:  eta: 4:32:03  iter: 28739  total_loss: 33.57  loss_ce: 0.257  loss_mask: 0.3441  loss_dice: 2.696  loss_ce_0: 0.5833  loss_mask_0: 0.3423  loss_dice_0: 2.815  loss_ce_1: 0.3068  loss_mask_1: 0.3517  loss_dice_1: 2.736  loss_ce_2: 0.2903  loss_mask_2: 0.3448  loss_dice_2: 2.713  loss_ce_3: 0.2827  loss_mask_3: 0.3455  loss_dice_3: 2.695  loss_ce_4: 0.2653  loss_mask_4: 0.3445  loss_dice_4: 2.7  loss_ce_5: 0.2658  loss_mask_5: 0.344  loss_dice_5: 2.69  loss_ce_6: 0.264  loss_mask_6: 0.346  loss_dice_6: 2.694  loss_ce_7: 0.2627  loss_mask_7: 0.3447  loss_dice_7: 2.689  loss_ce_8: 0.2457  loss_mask_8: 0.3463  loss_dice_8: 2.689  time: 1.4829  data_time: 0.0726  lr: 3.1957e-06  max_mem: 21589M
[01/18 08:06:10] d2.utils.events INFO:  eta: 4:31:38  iter: 28759  total_loss: 32.64  loss_ce: 0.2553  loss_mask: 0.3435  loss_dice: 2.629  loss_ce_0: 0.5739  loss_mask_0: 0.3436  loss_dice_0: 2.755  loss_ce_1: 0.3041  loss_mask_1: 0.35  loss_dice_1: 2.674  loss_ce_2: 0.3043  loss_mask_2: 0.3456  loss_dice_2: 2.655  loss_ce_3: 0.29  loss_mask_3: 0.3435  loss_dice_3: 2.638  loss_ce_4: 0.2713  loss_mask_4: 0.3433  loss_dice_4: 2.629  loss_ce_5: 0.272  loss_mask_5: 0.3425  loss_dice_5: 2.642  loss_ce_6: 0.2623  loss_mask_6: 0.3424  loss_dice_6: 2.635  loss_ce_7: 0.2652  loss_mask_7: 0.3434  loss_dice_7: 2.639  loss_ce_8: 0.2551  loss_mask_8: 0.343  loss_dice_8: 2.628  time: 1.4829  data_time: 0.0703  lr: 3.1906e-06  max_mem: 21589M
[01/18 08:06:39] d2.utils.events INFO:  eta: 4:31:06  iter: 28779  total_loss: 33.19  loss_ce: 0.2319  loss_mask: 0.3457  loss_dice: 2.656  loss_ce_0: 0.5503  loss_mask_0: 0.3399  loss_dice_0: 2.789  loss_ce_1: 0.2779  loss_mask_1: 0.3508  loss_dice_1: 2.713  loss_ce_2: 0.2786  loss_mask_2: 0.3493  loss_dice_2: 2.672  loss_ce_3: 0.2504  loss_mask_3: 0.3476  loss_dice_3: 2.675  loss_ce_4: 0.2399  loss_mask_4: 0.3462  loss_dice_4: 2.67  loss_ce_5: 0.2391  loss_mask_5: 0.3443  loss_dice_5: 2.665  loss_ce_6: 0.2317  loss_mask_6: 0.3443  loss_dice_6: 2.671  loss_ce_7: 0.2366  loss_mask_7: 0.3451  loss_dice_7: 2.658  loss_ce_8: 0.2298  loss_mask_8: 0.3462  loss_dice_8: 2.663  time: 1.4829  data_time: 0.0731  lr: 3.1855e-06  max_mem: 21589M
[01/18 08:07:08] d2.utils.events INFO:  eta: 4:30:34  iter: 28799  total_loss: 32.65  loss_ce: 0.2584  loss_mask: 0.3386  loss_dice: 2.602  loss_ce_0: 0.5422  loss_mask_0: 0.3448  loss_dice_0: 2.741  loss_ce_1: 0.3078  loss_mask_1: 0.3459  loss_dice_1: 2.654  loss_ce_2: 0.2998  loss_mask_2: 0.3442  loss_dice_2: 2.636  loss_ce_3: 0.2857  loss_mask_3: 0.3385  loss_dice_3: 2.616  loss_ce_4: 0.2716  loss_mask_4: 0.3401  loss_dice_4: 2.611  loss_ce_5: 0.2607  loss_mask_5: 0.3375  loss_dice_5: 2.608  loss_ce_6: 0.2578  loss_mask_6: 0.337  loss_dice_6: 2.6  loss_ce_7: 0.2532  loss_mask_7: 0.3371  loss_dice_7: 2.612  loss_ce_8: 0.2556  loss_mask_8: 0.3382  loss_dice_8: 2.613  time: 1.4828  data_time: 0.0711  lr: 3.1804e-06  max_mem: 21589M
[01/18 08:07:37] d2.utils.events INFO:  eta: 4:30:11  iter: 28819  total_loss: 33.01  loss_ce: 0.2577  loss_mask: 0.3482  loss_dice: 2.638  loss_ce_0: 0.5829  loss_mask_0: 0.3481  loss_dice_0: 2.761  loss_ce_1: 0.2897  loss_mask_1: 0.3527  loss_dice_1: 2.683  loss_ce_2: 0.2964  loss_mask_2: 0.3493  loss_dice_2: 2.669  loss_ce_3: 0.2804  loss_mask_3: 0.3478  loss_dice_3: 2.651  loss_ce_4: 0.2799  loss_mask_4: 0.3483  loss_dice_4: 2.648  loss_ce_5: 0.2718  loss_mask_5: 0.3469  loss_dice_5: 2.652  loss_ce_6: 0.2662  loss_mask_6: 0.3468  loss_dice_6: 2.644  loss_ce_7: 0.2558  loss_mask_7: 0.3468  loss_dice_7: 2.644  loss_ce_8: 0.2538  loss_mask_8: 0.3481  loss_dice_8: 2.64  time: 1.4828  data_time: 0.0710  lr: 3.1753e-06  max_mem: 21589M
[01/18 08:08:06] d2.utils.events INFO:  eta: 4:29:46  iter: 28839  total_loss: 33.12  loss_ce: 0.2559  loss_mask: 0.3429  loss_dice: 2.643  loss_ce_0: 0.56  loss_mask_0: 0.3421  loss_dice_0: 2.766  loss_ce_1: 0.3  loss_mask_1: 0.3487  loss_dice_1: 2.69  loss_ce_2: 0.2886  loss_mask_2: 0.3447  loss_dice_2: 2.665  loss_ce_3: 0.2782  loss_mask_3: 0.3444  loss_dice_3: 2.647  loss_ce_4: 0.2711  loss_mask_4: 0.3439  loss_dice_4: 2.651  loss_ce_5: 0.2707  loss_mask_5: 0.3422  loss_dice_5: 2.655  loss_ce_6: 0.2655  loss_mask_6: 0.3416  loss_dice_6: 2.633  loss_ce_7: 0.2461  loss_mask_7: 0.3422  loss_dice_7: 2.643  loss_ce_8: 0.2475  loss_mask_8: 0.3408  loss_dice_8: 2.645  time: 1.4828  data_time: 0.0681  lr: 3.1701e-06  max_mem: 21589M
[01/18 08:08:35] d2.utils.events INFO:  eta: 4:29:13  iter: 28859  total_loss: 33.51  loss_ce: 0.2465  loss_mask: 0.3368  loss_dice: 2.673  loss_ce_0: 0.5743  loss_mask_0: 0.3407  loss_dice_0: 2.801  loss_ce_1: 0.2897  loss_mask_1: 0.3434  loss_dice_1: 2.713  loss_ce_2: 0.2802  loss_mask_2: 0.3395  loss_dice_2: 2.695  loss_ce_3: 0.2665  loss_mask_3: 0.337  loss_dice_3: 2.685  loss_ce_4: 0.2724  loss_mask_4: 0.3382  loss_dice_4: 2.682  loss_ce_5: 0.2612  loss_mask_5: 0.3382  loss_dice_5: 2.674  loss_ce_6: 0.2616  loss_mask_6: 0.3378  loss_dice_6: 2.671  loss_ce_7: 0.2496  loss_mask_7: 0.3389  loss_dice_7: 2.673  loss_ce_8: 0.246  loss_mask_8: 0.3376  loss_dice_8: 2.67  time: 1.4828  data_time: 0.0730  lr: 3.165e-06  max_mem: 21589M
[01/18 08:09:04] d2.utils.events INFO:  eta: 4:28:50  iter: 28879  total_loss: 33.11  loss_ce: 0.2382  loss_mask: 0.3483  loss_dice: 2.643  loss_ce_0: 0.5468  loss_mask_0: 0.3459  loss_dice_0: 2.781  loss_ce_1: 0.3014  loss_mask_1: 0.3515  loss_dice_1: 2.689  loss_ce_2: 0.2875  loss_mask_2: 0.3499  loss_dice_2: 2.658  loss_ce_3: 0.2538  loss_mask_3: 0.3453  loss_dice_3: 2.649  loss_ce_4: 0.2601  loss_mask_4: 0.3465  loss_dice_4: 2.654  loss_ce_5: 0.2581  loss_mask_5: 0.3469  loss_dice_5: 2.644  loss_ce_6: 0.2518  loss_mask_6: 0.3463  loss_dice_6: 2.638  loss_ce_7: 0.2445  loss_mask_7: 0.3472  loss_dice_7: 2.639  loss_ce_8: 0.2385  loss_mask_8: 0.3468  loss_dice_8: 2.644  time: 1.4827  data_time: 0.0685  lr: 3.1599e-06  max_mem: 21589M
[01/18 08:09:32] d2.utils.events INFO:  eta: 4:28:09  iter: 28899  total_loss: 32.81  loss_ce: 0.2384  loss_mask: 0.3368  loss_dice: 2.607  loss_ce_0: 0.5404  loss_mask_0: 0.3339  loss_dice_0: 2.725  loss_ce_1: 0.2814  loss_mask_1: 0.3416  loss_dice_1: 2.661  loss_ce_2: 0.2788  loss_mask_2: 0.3375  loss_dice_2: 2.633  loss_ce_3: 0.2605  loss_mask_3: 0.3379  loss_dice_3: 2.615  loss_ce_4: 0.2535  loss_mask_4: 0.3369  loss_dice_4: 2.607  loss_ce_5: 0.2437  loss_mask_5: 0.3362  loss_dice_5: 2.604  loss_ce_6: 0.2443  loss_mask_6: 0.3356  loss_dice_6: 2.609  loss_ce_7: 0.2425  loss_mask_7: 0.3361  loss_dice_7: 2.606  loss_ce_8: 0.2428  loss_mask_8: 0.3367  loss_dice_8: 2.606  time: 1.4827  data_time: 0.0660  lr: 3.1548e-06  max_mem: 21589M
[01/18 08:10:02] d2.utils.events INFO:  eta: 4:27:39  iter: 28919  total_loss: 33.02  loss_ce: 0.2498  loss_mask: 0.3499  loss_dice: 2.68  loss_ce_0: 0.5577  loss_mask_0: 0.3519  loss_dice_0: 2.807  loss_ce_1: 0.3022  loss_mask_1: 0.3589  loss_dice_1: 2.722  loss_ce_2: 0.2693  loss_mask_2: 0.3575  loss_dice_2: 2.705  loss_ce_3: 0.2729  loss_mask_3: 0.3548  loss_dice_3: 2.691  loss_ce_4: 0.244  loss_mask_4: 0.3553  loss_dice_4: 2.68  loss_ce_5: 0.263  loss_mask_5: 0.3539  loss_dice_5: 2.687  loss_ce_6: 0.2504  loss_mask_6: 0.3529  loss_dice_6: 2.677  loss_ce_7: 0.2454  loss_mask_7: 0.353  loss_dice_7: 2.685  loss_ce_8: 0.2384  loss_mask_8: 0.3512  loss_dice_8: 2.686  time: 1.4827  data_time: 0.0690  lr: 3.1497e-06  max_mem: 21589M
[01/18 08:10:30] d2.utils.events INFO:  eta: 4:27:11  iter: 28939  total_loss: 33.81  loss_ce: 0.2748  loss_mask: 0.3533  loss_dice: 2.692  loss_ce_0: 0.5758  loss_mask_0: 0.3491  loss_dice_0: 2.813  loss_ce_1: 0.321  loss_mask_1: 0.3564  loss_dice_1: 2.734  loss_ce_2: 0.3129  loss_mask_2: 0.3535  loss_dice_2: 2.713  loss_ce_3: 0.2767  loss_mask_3: 0.3525  loss_dice_3: 2.687  loss_ce_4: 0.2814  loss_mask_4: 0.353  loss_dice_4: 2.691  loss_ce_5: 0.2661  loss_mask_5: 0.3527  loss_dice_5: 2.697  loss_ce_6: 0.2788  loss_mask_6: 0.3514  loss_dice_6: 2.684  loss_ce_7: 0.2614  loss_mask_7: 0.3506  loss_dice_7: 2.689  loss_ce_8: 0.2512  loss_mask_8: 0.3516  loss_dice_8: 2.68  time: 1.4826  data_time: 0.0715  lr: 3.1446e-06  max_mem: 21589M
[01/18 08:11:00] d2.utils.events INFO:  eta: 4:26:41  iter: 28959  total_loss: 33.25  loss_ce: 0.2339  loss_mask: 0.3444  loss_dice: 2.672  loss_ce_0: 0.5376  loss_mask_0: 0.3419  loss_dice_0: 2.8  loss_ce_1: 0.2606  loss_mask_1: 0.3474  loss_dice_1: 2.718  loss_ce_2: 0.2635  loss_mask_2: 0.3458  loss_dice_2: 2.693  loss_ce_3: 0.2494  loss_mask_3: 0.3421  loss_dice_3: 2.681  loss_ce_4: 0.24  loss_mask_4: 0.3428  loss_dice_4: 2.682  loss_ce_5: 0.2437  loss_mask_5: 0.3426  loss_dice_5: 2.681  loss_ce_6: 0.2401  loss_mask_6: 0.3439  loss_dice_6: 2.672  loss_ce_7: 0.2207  loss_mask_7: 0.3442  loss_dice_7: 2.678  loss_ce_8: 0.2297  loss_mask_8: 0.3442  loss_dice_8: 2.663  time: 1.4826  data_time: 0.0699  lr: 3.1395e-06  max_mem: 21589M
[01/18 08:11:29] d2.utils.events INFO:  eta: 4:26:08  iter: 28979  total_loss: 32.93  loss_ce: 0.271  loss_mask: 0.3485  loss_dice: 2.602  loss_ce_0: 0.5876  loss_mask_0: 0.3429  loss_dice_0: 2.723  loss_ce_1: 0.3188  loss_mask_1: 0.3518  loss_dice_1: 2.656  loss_ce_2: 0.3081  loss_mask_2: 0.3498  loss_dice_2: 2.616  loss_ce_3: 0.2732  loss_mask_3: 0.3481  loss_dice_3: 2.618  loss_ce_4: 0.2719  loss_mask_4: 0.3476  loss_dice_4: 2.613  loss_ce_5: 0.2646  loss_mask_5: 0.3484  loss_dice_5: 2.605  loss_ce_6: 0.2716  loss_mask_6: 0.3471  loss_dice_6: 2.608  loss_ce_7: 0.2572  loss_mask_7: 0.3472  loss_dice_7: 2.612  loss_ce_8: 0.2633  loss_mask_8: 0.3481  loss_dice_8: 2.596  time: 1.4826  data_time: 0.0691  lr: 3.1343e-06  max_mem: 21589M
[01/18 08:11:58] d2.utils.events INFO:  eta: 4:25:39  iter: 28999  total_loss: 33.56  loss_ce: 0.2455  loss_mask: 0.3437  loss_dice: 2.686  loss_ce_0: 0.5668  loss_mask_0: 0.3428  loss_dice_0: 2.808  loss_ce_1: 0.3015  loss_mask_1: 0.3469  loss_dice_1: 2.723  loss_ce_2: 0.2908  loss_mask_2: 0.3443  loss_dice_2: 2.716  loss_ce_3: 0.2616  loss_mask_3: 0.3439  loss_dice_3: 2.698  loss_ce_4: 0.2755  loss_mask_4: 0.3436  loss_dice_4: 2.688  loss_ce_5: 0.2519  loss_mask_5: 0.3419  loss_dice_5: 2.684  loss_ce_6: 0.2424  loss_mask_6: 0.3432  loss_dice_6: 2.691  loss_ce_7: 0.2517  loss_mask_7: 0.345  loss_dice_7: 2.688  loss_ce_8: 0.2433  loss_mask_8: 0.3432  loss_dice_8: 2.69  time: 1.4826  data_time: 0.0673  lr: 3.1292e-06  max_mem: 21589M
[01/18 08:12:26] d2.utils.events INFO:  eta: 4:25:01  iter: 29019  total_loss: 33.25  loss_ce: 0.2416  loss_mask: 0.343  loss_dice: 2.674  loss_ce_0: 0.5788  loss_mask_0: 0.3416  loss_dice_0: 2.814  loss_ce_1: 0.2798  loss_mask_1: 0.3485  loss_dice_1: 2.715  loss_ce_2: 0.2886  loss_mask_2: 0.3437  loss_dice_2: 2.689  loss_ce_3: 0.2793  loss_mask_3: 0.3448  loss_dice_3: 2.681  loss_ce_4: 0.274  loss_mask_4: 0.3445  loss_dice_4: 2.684  loss_ce_5: 0.2609  loss_mask_5: 0.342  loss_dice_5: 2.681  loss_ce_6: 0.2536  loss_mask_6: 0.3428  loss_dice_6: 2.678  loss_ce_7: 0.2397  loss_mask_7: 0.3425  loss_dice_7: 2.675  loss_ce_8: 0.2401  loss_mask_8: 0.3425  loss_dice_8: 2.68  time: 1.4825  data_time: 0.0675  lr: 3.1241e-06  max_mem: 21589M
[01/18 08:12:56] d2.utils.events INFO:  eta: 4:24:34  iter: 29039  total_loss: 33.52  loss_ce: 0.2458  loss_mask: 0.3408  loss_dice: 2.674  loss_ce_0: 0.5529  loss_mask_0: 0.3428  loss_dice_0: 2.784  loss_ce_1: 0.3212  loss_mask_1: 0.3456  loss_dice_1: 2.713  loss_ce_2: 0.3047  loss_mask_2: 0.3408  loss_dice_2: 2.688  loss_ce_3: 0.2759  loss_mask_3: 0.3393  loss_dice_3: 2.678  loss_ce_4: 0.2894  loss_mask_4: 0.3395  loss_dice_4: 2.673  loss_ce_5: 0.2744  loss_mask_5: 0.3399  loss_dice_5: 2.681  loss_ce_6: 0.2744  loss_mask_6: 0.3399  loss_dice_6: 2.674  loss_ce_7: 0.2449  loss_mask_7: 0.3411  loss_dice_7: 2.671  loss_ce_8: 0.2606  loss_mask_8: 0.3399  loss_dice_8: 2.675  time: 1.4825  data_time: 0.0744  lr: 3.119e-06  max_mem: 21589M
[01/18 08:13:25] d2.utils.events INFO:  eta: 4:24:05  iter: 29059  total_loss: 32.89  loss_ce: 0.2582  loss_mask: 0.3461  loss_dice: 2.66  loss_ce_0: 0.56  loss_mask_0: 0.3421  loss_dice_0: 2.786  loss_ce_1: 0.2964  loss_mask_1: 0.3534  loss_dice_1: 2.689  loss_ce_2: 0.2956  loss_mask_2: 0.3479  loss_dice_2: 2.678  loss_ce_3: 0.2668  loss_mask_3: 0.3459  loss_dice_3: 2.659  loss_ce_4: 0.2611  loss_mask_4: 0.3452  loss_dice_4: 2.666  loss_ce_5: 0.254  loss_mask_5: 0.346  loss_dice_5: 2.652  loss_ce_6: 0.2481  loss_mask_6: 0.3444  loss_dice_6: 2.663  loss_ce_7: 0.2483  loss_mask_7: 0.3455  loss_dice_7: 2.658  loss_ce_8: 0.2486  loss_mask_8: 0.3446  loss_dice_8: 2.658  time: 1.4825  data_time: 0.0695  lr: 3.1138e-06  max_mem: 21589M
[01/18 08:13:54] d2.utils.events INFO:  eta: 4:23:34  iter: 29079  total_loss: 33.55  loss_ce: 0.2416  loss_mask: 0.3401  loss_dice: 2.711  loss_ce_0: 0.5689  loss_mask_0: 0.3395  loss_dice_0: 2.827  loss_ce_1: 0.2961  loss_mask_1: 0.3469  loss_dice_1: 2.755  loss_ce_2: 0.2851  loss_mask_2: 0.3417  loss_dice_2: 2.727  loss_ce_3: 0.2849  loss_mask_3: 0.3383  loss_dice_3: 2.713  loss_ce_4: 0.2641  loss_mask_4: 0.338  loss_dice_4: 2.709  loss_ce_5: 0.2451  loss_mask_5: 0.3376  loss_dice_5: 2.712  loss_ce_6: 0.2579  loss_mask_6: 0.3402  loss_dice_6: 2.71  loss_ce_7: 0.2445  loss_mask_7: 0.3394  loss_dice_7: 2.713  loss_ce_8: 0.2374  loss_mask_8: 0.3399  loss_dice_8: 2.706  time: 1.4825  data_time: 0.0716  lr: 3.1087e-06  max_mem: 21589M
[01/18 08:14:23] d2.utils.events INFO:  eta: 4:23:00  iter: 29099  total_loss: 34.01  loss_ce: 0.2837  loss_mask: 0.3549  loss_dice: 2.699  loss_ce_0: 0.5778  loss_mask_0: 0.3546  loss_dice_0: 2.827  loss_ce_1: 0.308  loss_mask_1: 0.3594  loss_dice_1: 2.744  loss_ce_2: 0.3069  loss_mask_2: 0.3565  loss_dice_2: 2.719  loss_ce_3: 0.3024  loss_mask_3: 0.3552  loss_dice_3: 2.704  loss_ce_4: 0.2833  loss_mask_4: 0.3558  loss_dice_4: 2.707  loss_ce_5: 0.281  loss_mask_5: 0.3552  loss_dice_5: 2.705  loss_ce_6: 0.2801  loss_mask_6: 0.3543  loss_dice_6: 2.705  loss_ce_7: 0.2771  loss_mask_7: 0.3552  loss_dice_7: 2.704  loss_ce_8: 0.2644  loss_mask_8: 0.3551  loss_dice_8: 2.703  time: 1.4825  data_time: 0.0672  lr: 3.1036e-06  max_mem: 21589M
[01/18 08:14:51] d2.utils.events INFO:  eta: 4:22:26  iter: 29119  total_loss: 32.43  loss_ce: 0.2236  loss_mask: 0.3471  loss_dice: 2.609  loss_ce_0: 0.5413  loss_mask_0: 0.3477  loss_dice_0: 2.729  loss_ce_1: 0.2729  loss_mask_1: 0.3548  loss_dice_1: 2.634  loss_ce_2: 0.2584  loss_mask_2: 0.3519  loss_dice_2: 2.624  loss_ce_3: 0.245  loss_mask_3: 0.3493  loss_dice_3: 2.609  loss_ce_4: 0.2422  loss_mask_4: 0.3494  loss_dice_4: 2.604  loss_ce_5: 0.252  loss_mask_5: 0.3487  loss_dice_5: 2.604  loss_ce_6: 0.2457  loss_mask_6: 0.3489  loss_dice_6: 2.611  loss_ce_7: 0.2212  loss_mask_7: 0.3472  loss_dice_7: 2.606  loss_ce_8: 0.2308  loss_mask_8: 0.3471  loss_dice_8: 2.612  time: 1.4824  data_time: 0.0675  lr: 3.0985e-06  max_mem: 21589M
[01/18 08:15:21] d2.utils.events INFO:  eta: 4:21:57  iter: 29139  total_loss: 32.86  loss_ce: 0.2436  loss_mask: 0.3417  loss_dice: 2.63  loss_ce_0: 0.5747  loss_mask_0: 0.3428  loss_dice_0: 2.758  loss_ce_1: 0.3038  loss_mask_1: 0.3468  loss_dice_1: 2.676  loss_ce_2: 0.2871  loss_mask_2: 0.3438  loss_dice_2: 2.651  loss_ce_3: 0.2492  loss_mask_3: 0.342  loss_dice_3: 2.64  loss_ce_4: 0.2607  loss_mask_4: 0.3414  loss_dice_4: 2.64  loss_ce_5: 0.2451  loss_mask_5: 0.3421  loss_dice_5: 2.632  loss_ce_6: 0.2387  loss_mask_6: 0.3421  loss_dice_6: 2.637  loss_ce_7: 0.2337  loss_mask_7: 0.3412  loss_dice_7: 2.636  loss_ce_8: 0.2229  loss_mask_8: 0.3428  loss_dice_8: 2.637  time: 1.4824  data_time: 0.0717  lr: 3.0933e-06  max_mem: 21589M
[01/18 08:15:49] d2.utils.events INFO:  eta: 4:21:19  iter: 29159  total_loss: 32.19  loss_ce: 0.2415  loss_mask: 0.3393  loss_dice: 2.592  loss_ce_0: 0.539  loss_mask_0: 0.3366  loss_dice_0: 2.732  loss_ce_1: 0.2625  loss_mask_1: 0.3423  loss_dice_1: 2.634  loss_ce_2: 0.2656  loss_mask_2: 0.34  loss_dice_2: 2.622  loss_ce_3: 0.2476  loss_mask_3: 0.3381  loss_dice_3: 2.607  loss_ce_4: 0.2478  loss_mask_4: 0.3383  loss_dice_4: 2.6  loss_ce_5: 0.2373  loss_mask_5: 0.3385  loss_dice_5: 2.599  loss_ce_6: 0.2283  loss_mask_6: 0.3387  loss_dice_6: 2.599  loss_ce_7: 0.2355  loss_mask_7: 0.3404  loss_dice_7: 2.595  loss_ce_8: 0.2446  loss_mask_8: 0.3389  loss_dice_8: 2.599  time: 1.4824  data_time: 0.0703  lr: 3.0882e-06  max_mem: 21589M
[01/18 08:16:18] d2.utils.events INFO:  eta: 4:20:45  iter: 29179  total_loss: 32.42  loss_ce: 0.2369  loss_mask: 0.3427  loss_dice: 2.609  loss_ce_0: 0.5438  loss_mask_0: 0.3408  loss_dice_0: 2.735  loss_ce_1: 0.2996  loss_mask_1: 0.3489  loss_dice_1: 2.648  loss_ce_2: 0.2684  loss_mask_2: 0.3431  loss_dice_2: 2.632  loss_ce_3: 0.2428  loss_mask_3: 0.3431  loss_dice_3: 2.615  loss_ce_4: 0.2427  loss_mask_4: 0.3417  loss_dice_4: 2.619  loss_ce_5: 0.2458  loss_mask_5: 0.3426  loss_dice_5: 2.615  loss_ce_6: 0.2321  loss_mask_6: 0.344  loss_dice_6: 2.617  loss_ce_7: 0.2329  loss_mask_7: 0.3427  loss_dice_7: 2.612  loss_ce_8: 0.2311  loss_mask_8: 0.3428  loss_dice_8: 2.611  time: 1.4823  data_time: 0.0717  lr: 3.0831e-06  max_mem: 21589M
[01/18 08:16:47] d2.utils.events INFO:  eta: 4:20:16  iter: 29199  total_loss: 33.11  loss_ce: 0.2445  loss_mask: 0.3446  loss_dice: 2.635  loss_ce_0: 0.5546  loss_mask_0: 0.3436  loss_dice_0: 2.77  loss_ce_1: 0.2928  loss_mask_1: 0.3526  loss_dice_1: 2.676  loss_ce_2: 0.2841  loss_mask_2: 0.3481  loss_dice_2: 2.663  loss_ce_3: 0.2583  loss_mask_3: 0.3457  loss_dice_3: 2.645  loss_ce_4: 0.2533  loss_mask_4: 0.3451  loss_dice_4: 2.652  loss_ce_5: 0.25  loss_mask_5: 0.3466  loss_dice_5: 2.647  loss_ce_6: 0.2532  loss_mask_6: 0.3448  loss_dice_6: 2.643  loss_ce_7: 0.249  loss_mask_7: 0.3435  loss_dice_7: 2.645  loss_ce_8: 0.2363  loss_mask_8: 0.3426  loss_dice_8: 2.637  time: 1.4823  data_time: 0.0715  lr: 3.078e-06  max_mem: 21589M
[01/18 08:17:16] d2.utils.events INFO:  eta: 4:19:53  iter: 29219  total_loss: 33.6  loss_ce: 0.2604  loss_mask: 0.3297  loss_dice: 2.733  loss_ce_0: 0.5548  loss_mask_0: 0.3301  loss_dice_0: 2.866  loss_ce_1: 0.3012  loss_mask_1: 0.3317  loss_dice_1: 2.78  loss_ce_2: 0.2899  loss_mask_2: 0.33  loss_dice_2: 2.745  loss_ce_3: 0.2767  loss_mask_3: 0.3298  loss_dice_3: 2.73  loss_ce_4: 0.264  loss_mask_4: 0.3297  loss_dice_4: 2.732  loss_ce_5: 0.2576  loss_mask_5: 0.3281  loss_dice_5: 2.734  loss_ce_6: 0.2492  loss_mask_6: 0.3294  loss_dice_6: 2.725  loss_ce_7: 0.2494  loss_mask_7: 0.3307  loss_dice_7: 2.725  loss_ce_8: 0.2544  loss_mask_8: 0.3283  loss_dice_8: 2.726  time: 1.4823  data_time: 0.0696  lr: 3.0728e-06  max_mem: 21589M
[01/18 08:17:45] d2.utils.events INFO:  eta: 4:19:25  iter: 29239  total_loss: 33.2  loss_ce: 0.2441  loss_mask: 0.3475  loss_dice: 2.725  loss_ce_0: 0.5467  loss_mask_0: 0.351  loss_dice_0: 2.853  loss_ce_1: 0.2945  loss_mask_1: 0.354  loss_dice_1: 2.765  loss_ce_2: 0.271  loss_mask_2: 0.3508  loss_dice_2: 2.745  loss_ce_3: 0.2648  loss_mask_3: 0.3486  loss_dice_3: 2.734  loss_ce_4: 0.2602  loss_mask_4: 0.3487  loss_dice_4: 2.727  loss_ce_5: 0.2571  loss_mask_5: 0.3482  loss_dice_5: 2.734  loss_ce_6: 0.24  loss_mask_6: 0.3461  loss_dice_6: 2.725  loss_ce_7: 0.2489  loss_mask_7: 0.347  loss_dice_7: 2.724  loss_ce_8: 0.2453  loss_mask_8: 0.3474  loss_dice_8: 2.726  time: 1.4823  data_time: 0.0707  lr: 3.0677e-06  max_mem: 21589M
[01/18 08:18:13] d2.utils.events INFO:  eta: 4:18:45  iter: 29259  total_loss: 33.62  loss_ce: 0.23  loss_mask: 0.3462  loss_dice: 2.714  loss_ce_0: 0.5276  loss_mask_0: 0.3475  loss_dice_0: 2.835  loss_ce_1: 0.2822  loss_mask_1: 0.3536  loss_dice_1: 2.759  loss_ce_2: 0.2555  loss_mask_2: 0.3486  loss_dice_2: 2.724  loss_ce_3: 0.246  loss_mask_3: 0.3469  loss_dice_3: 2.721  loss_ce_4: 0.2421  loss_mask_4: 0.3439  loss_dice_4: 2.722  loss_ce_5: 0.2368  loss_mask_5: 0.3443  loss_dice_5: 2.717  loss_ce_6: 0.2487  loss_mask_6: 0.3456  loss_dice_6: 2.71  loss_ce_7: 0.2401  loss_mask_7: 0.3451  loss_dice_7: 2.707  loss_ce_8: 0.2308  loss_mask_8: 0.3457  loss_dice_8: 2.703  time: 1.4822  data_time: 0.0715  lr: 3.0626e-06  max_mem: 21589M
[01/18 08:18:43] d2.utils.events INFO:  eta: 4:18:16  iter: 29279  total_loss: 33.46  loss_ce: 0.2484  loss_mask: 0.3409  loss_dice: 2.683  loss_ce_0: 0.576  loss_mask_0: 0.3364  loss_dice_0: 2.813  loss_ce_1: 0.3027  loss_mask_1: 0.3448  loss_dice_1: 2.723  loss_ce_2: 0.2816  loss_mask_2: 0.3441  loss_dice_2: 2.702  loss_ce_3: 0.2708  loss_mask_3: 0.3414  loss_dice_3: 2.688  loss_ce_4: 0.2584  loss_mask_4: 0.3407  loss_dice_4: 2.685  loss_ce_5: 0.2476  loss_mask_5: 0.3406  loss_dice_5: 2.675  loss_ce_6: 0.2546  loss_mask_6: 0.3414  loss_dice_6: 2.682  loss_ce_7: 0.249  loss_mask_7: 0.3409  loss_dice_7: 2.688  loss_ce_8: 0.2484  loss_mask_8: 0.3402  loss_dice_8: 2.685  time: 1.4822  data_time: 0.0711  lr: 3.0574e-06  max_mem: 21589M
[01/18 08:19:12] d2.utils.events INFO:  eta: 4:18:01  iter: 29299  total_loss: 32.43  loss_ce: 0.2344  loss_mask: 0.3453  loss_dice: 2.61  loss_ce_0: 0.5389  loss_mask_0: 0.3422  loss_dice_0: 2.753  loss_ce_1: 0.2755  loss_mask_1: 0.3528  loss_dice_1: 2.653  loss_ce_2: 0.2814  loss_mask_2: 0.3487  loss_dice_2: 2.631  loss_ce_3: 0.2642  loss_mask_3: 0.3451  loss_dice_3: 2.632  loss_ce_4: 0.2605  loss_mask_4: 0.3453  loss_dice_4: 2.624  loss_ce_5: 0.2454  loss_mask_5: 0.3449  loss_dice_5: 2.622  loss_ce_6: 0.2375  loss_mask_6: 0.3452  loss_dice_6: 2.618  loss_ce_7: 0.2371  loss_mask_7: 0.346  loss_dice_7: 2.619  loss_ce_8: 0.2377  loss_mask_8: 0.3469  loss_dice_8: 2.62  time: 1.4822  data_time: 0.0707  lr: 3.0523e-06  max_mem: 21589M
[01/18 08:19:41] d2.utils.events INFO:  eta: 4:17:34  iter: 29319  total_loss: 32.68  loss_ce: 0.2409  loss_mask: 0.3403  loss_dice: 2.614  loss_ce_0: 0.5625  loss_mask_0: 0.3427  loss_dice_0: 2.737  loss_ce_1: 0.2987  loss_mask_1: 0.3482  loss_dice_1: 2.656  loss_ce_2: 0.2866  loss_mask_2: 0.3438  loss_dice_2: 2.637  loss_ce_3: 0.2711  loss_mask_3: 0.3405  loss_dice_3: 2.62  loss_ce_4: 0.2633  loss_mask_4: 0.3413  loss_dice_4: 2.615  loss_ce_5: 0.2598  loss_mask_5: 0.3427  loss_dice_5: 2.612  loss_ce_6: 0.2434  loss_mask_6: 0.3405  loss_dice_6: 2.616  loss_ce_7: 0.2489  loss_mask_7: 0.3406  loss_dice_7: 2.611  loss_ce_8: 0.2428  loss_mask_8: 0.3406  loss_dice_8: 2.626  time: 1.4822  data_time: 0.0709  lr: 3.0472e-06  max_mem: 21589M
[01/18 08:20:10] d2.utils.events INFO:  eta: 4:17:07  iter: 29339  total_loss: 33.69  loss_ce: 0.2643  loss_mask: 0.3375  loss_dice: 2.692  loss_ce_0: 0.5819  loss_mask_0: 0.339  loss_dice_0: 2.825  loss_ce_1: 0.3067  loss_mask_1: 0.342  loss_dice_1: 2.742  loss_ce_2: 0.2955  loss_mask_2: 0.3384  loss_dice_2: 2.706  loss_ce_3: 0.2779  loss_mask_3: 0.3376  loss_dice_3: 2.692  loss_ce_4: 0.2548  loss_mask_4: 0.338  loss_dice_4: 2.701  loss_ce_5: 0.2678  loss_mask_5: 0.3386  loss_dice_5: 2.692  loss_ce_6: 0.2572  loss_mask_6: 0.3358  loss_dice_6: 2.699  loss_ce_7: 0.2491  loss_mask_7: 0.3367  loss_dice_7: 2.697  loss_ce_8: 0.2581  loss_mask_8: 0.3355  loss_dice_8: 2.697  time: 1.4822  data_time: 0.0729  lr: 3.042e-06  max_mem: 21589M
[01/18 08:20:40] d2.utils.events INFO:  eta: 4:16:42  iter: 29359  total_loss: 33.47  loss_ce: 0.2518  loss_mask: 0.3413  loss_dice: 2.689  loss_ce_0: 0.5638  loss_mask_0: 0.3396  loss_dice_0: 2.834  loss_ce_1: 0.2764  loss_mask_1: 0.344  loss_dice_1: 2.73  loss_ce_2: 0.2696  loss_mask_2: 0.3414  loss_dice_2: 2.71  loss_ce_3: 0.274  loss_mask_3: 0.3411  loss_dice_3: 2.695  loss_ce_4: 0.2599  loss_mask_4: 0.3403  loss_dice_4: 2.693  loss_ce_5: 0.2509  loss_mask_5: 0.3414  loss_dice_5: 2.698  loss_ce_6: 0.2401  loss_mask_6: 0.3406  loss_dice_6: 2.685  loss_ce_7: 0.2453  loss_mask_7: 0.3411  loss_dice_7: 2.689  loss_ce_8: 0.2445  loss_mask_8: 0.3418  loss_dice_8: 2.694  time: 1.4821  data_time: 0.0732  lr: 3.0369e-06  max_mem: 21589M
[01/18 08:21:09] d2.utils.events INFO:  eta: 4:16:07  iter: 29379  total_loss: 33.33  loss_ce: 0.2655  loss_mask: 0.3478  loss_dice: 2.69  loss_ce_0: 0.5481  loss_mask_0: 0.3561  loss_dice_0: 2.819  loss_ce_1: 0.297  loss_mask_1: 0.3544  loss_dice_1: 2.735  loss_ce_2: 0.2808  loss_mask_2: 0.3514  loss_dice_2: 2.718  loss_ce_3: 0.2637  loss_mask_3: 0.3514  loss_dice_3: 2.703  loss_ce_4: 0.261  loss_mask_4: 0.3501  loss_dice_4: 2.7  loss_ce_5: 0.2648  loss_mask_5: 0.3482  loss_dice_5: 2.703  loss_ce_6: 0.2542  loss_mask_6: 0.3486  loss_dice_6: 2.696  loss_ce_7: 0.2502  loss_mask_7: 0.3493  loss_dice_7: 2.705  loss_ce_8: 0.2433  loss_mask_8: 0.3482  loss_dice_8: 2.703  time: 1.4821  data_time: 0.0683  lr: 3.0318e-06  max_mem: 21589M
[01/18 08:21:37] d2.utils.events INFO:  eta: 4:15:45  iter: 29399  total_loss: 32.87  loss_ce: 0.2347  loss_mask: 0.3376  loss_dice: 2.634  loss_ce_0: 0.563  loss_mask_0: 0.3387  loss_dice_0: 2.768  loss_ce_1: 0.2711  loss_mask_1: 0.3436  loss_dice_1: 2.683  loss_ce_2: 0.2525  loss_mask_2: 0.339  loss_dice_2: 2.657  loss_ce_3: 0.2472  loss_mask_3: 0.3383  loss_dice_3: 2.639  loss_ce_4: 0.2246  loss_mask_4: 0.3382  loss_dice_4: 2.638  loss_ce_5: 0.2338  loss_mask_5: 0.336  loss_dice_5: 2.638  loss_ce_6: 0.2361  loss_mask_6: 0.3372  loss_dice_6: 2.642  loss_ce_7: 0.2246  loss_mask_7: 0.3389  loss_dice_7: 2.638  loss_ce_8: 0.2302  loss_mask_8: 0.3373  loss_dice_8: 2.641  time: 1.4821  data_time: 0.0687  lr: 3.0266e-06  max_mem: 21589M
[01/18 08:22:06] d2.utils.events INFO:  eta: 4:15:13  iter: 29419  total_loss: 33.05  loss_ce: 0.2347  loss_mask: 0.3329  loss_dice: 2.658  loss_ce_0: 0.5437  loss_mask_0: 0.3328  loss_dice_0: 2.791  loss_ce_1: 0.2652  loss_mask_1: 0.3387  loss_dice_1: 2.714  loss_ce_2: 0.2538  loss_mask_2: 0.3364  loss_dice_2: 2.698  loss_ce_3: 0.2571  loss_mask_3: 0.3357  loss_dice_3: 2.676  loss_ce_4: 0.2464  loss_mask_4: 0.3342  loss_dice_4: 2.674  loss_ce_5: 0.233  loss_mask_5: 0.3338  loss_dice_5: 2.677  loss_ce_6: 0.2369  loss_mask_6: 0.3338  loss_dice_6: 2.675  loss_ce_7: 0.2335  loss_mask_7: 0.3327  loss_dice_7: 2.66  loss_ce_8: 0.2352  loss_mask_8: 0.3317  loss_dice_8: 2.664  time: 1.4821  data_time: 0.0695  lr: 3.0215e-06  max_mem: 21589M
[01/18 08:22:35] d2.utils.events INFO:  eta: 4:14:40  iter: 29439  total_loss: 32.74  loss_ce: 0.2233  loss_mask: 0.3374  loss_dice: 2.628  loss_ce_0: 0.5562  loss_mask_0: 0.3387  loss_dice_0: 2.765  loss_ce_1: 0.2687  loss_mask_1: 0.3453  loss_dice_1: 2.681  loss_ce_2: 0.2707  loss_mask_2: 0.3437  loss_dice_2: 2.654  loss_ce_3: 0.2494  loss_mask_3: 0.3391  loss_dice_3: 2.644  loss_ce_4: 0.2422  loss_mask_4: 0.3371  loss_dice_4: 2.639  loss_ce_5: 0.2263  loss_mask_5: 0.3375  loss_dice_5: 2.639  loss_ce_6: 0.2242  loss_mask_6: 0.3369  loss_dice_6: 2.636  loss_ce_7: 0.2326  loss_mask_7: 0.3367  loss_dice_7: 2.641  loss_ce_8: 0.2253  loss_mask_8: 0.3373  loss_dice_8: 2.644  time: 1.4820  data_time: 0.0740  lr: 3.0163e-06  max_mem: 21589M
[01/18 08:23:04] d2.utils.events INFO:  eta: 4:14:18  iter: 29459  total_loss: 32.95  loss_ce: 0.2183  loss_mask: 0.3401  loss_dice: 2.633  loss_ce_0: 0.553  loss_mask_0: 0.3363  loss_dice_0: 2.77  loss_ce_1: 0.2915  loss_mask_1: 0.3471  loss_dice_1: 2.682  loss_ce_2: 0.2604  loss_mask_2: 0.3429  loss_dice_2: 2.66  loss_ce_3: 0.2493  loss_mask_3: 0.3408  loss_dice_3: 2.64  loss_ce_4: 0.2486  loss_mask_4: 0.3396  loss_dice_4: 2.641  loss_ce_5: 0.2306  loss_mask_5: 0.3399  loss_dice_5: 2.643  loss_ce_6: 0.2308  loss_mask_6: 0.3407  loss_dice_6: 2.633  loss_ce_7: 0.2317  loss_mask_7: 0.3402  loss_dice_7: 2.639  loss_ce_8: 0.2214  loss_mask_8: 0.3398  loss_dice_8: 2.63  time: 1.4820  data_time: 0.0707  lr: 3.0112e-06  max_mem: 21589M
[01/18 08:23:33] d2.utils.events INFO:  eta: 4:13:54  iter: 29479  total_loss: 33.22  loss_ce: 0.2407  loss_mask: 0.3445  loss_dice: 2.648  loss_ce_0: 0.5799  loss_mask_0: 0.3512  loss_dice_0: 2.781  loss_ce_1: 0.2813  loss_mask_1: 0.351  loss_dice_1: 2.699  loss_ce_2: 0.2708  loss_mask_2: 0.3469  loss_dice_2: 2.666  loss_ce_3: 0.2712  loss_mask_3: 0.343  loss_dice_3: 2.654  loss_ce_4: 0.2697  loss_mask_4: 0.3441  loss_dice_4: 2.658  loss_ce_5: 0.2609  loss_mask_5: 0.3437  loss_dice_5: 2.641  loss_ce_6: 0.2418  loss_mask_6: 0.3443  loss_dice_6: 2.644  loss_ce_7: 0.2476  loss_mask_7: 0.3449  loss_dice_7: 2.647  loss_ce_8: 0.2378  loss_mask_8: 0.3447  loss_dice_8: 2.649  time: 1.4820  data_time: 0.0737  lr: 3.0061e-06  max_mem: 21589M
[01/18 08:24:02] d2.utils.events INFO:  eta: 4:13:23  iter: 29499  total_loss: 33.05  loss_ce: 0.2519  loss_mask: 0.3422  loss_dice: 2.594  loss_ce_0: 0.5607  loss_mask_0: 0.3376  loss_dice_0: 2.745  loss_ce_1: 0.2822  loss_mask_1: 0.3465  loss_dice_1: 2.653  loss_ce_2: 0.2962  loss_mask_2: 0.3478  loss_dice_2: 2.622  loss_ce_3: 0.2776  loss_mask_3: 0.3445  loss_dice_3: 2.602  loss_ce_4: 0.2605  loss_mask_4: 0.3421  loss_dice_4: 2.6  loss_ce_5: 0.2592  loss_mask_5: 0.3422  loss_dice_5: 2.607  loss_ce_6: 0.2486  loss_mask_6: 0.3426  loss_dice_6: 2.607  loss_ce_7: 0.2429  loss_mask_7: 0.3428  loss_dice_7: 2.601  loss_ce_8: 0.2408  loss_mask_8: 0.3417  loss_dice_8: 2.596  time: 1.4820  data_time: 0.0722  lr: 3.0009e-06  max_mem: 21589M
[01/18 08:24:31] d2.utils.events INFO:  eta: 4:12:55  iter: 29519  total_loss: 32.84  loss_ce: 0.2343  loss_mask: 0.3415  loss_dice: 2.633  loss_ce_0: 0.5509  loss_mask_0: 0.3396  loss_dice_0: 2.786  loss_ce_1: 0.2864  loss_mask_1: 0.3442  loss_dice_1: 2.683  loss_ce_2: 0.2758  loss_mask_2: 0.3416  loss_dice_2: 2.663  loss_ce_3: 0.2556  loss_mask_3: 0.3402  loss_dice_3: 2.641  loss_ce_4: 0.2428  loss_mask_4: 0.3407  loss_dice_4: 2.641  loss_ce_5: 0.2316  loss_mask_5: 0.3415  loss_dice_5: 2.638  loss_ce_6: 0.2344  loss_mask_6: 0.3403  loss_dice_6: 2.637  loss_ce_7: 0.248  loss_mask_7: 0.3419  loss_dice_7: 2.638  loss_ce_8: 0.2414  loss_mask_8: 0.3422  loss_dice_8: 2.642  time: 1.4819  data_time: 0.0717  lr: 2.9958e-06  max_mem: 21589M
[01/18 08:25:01] d2.utils.events INFO:  eta: 4:12:25  iter: 29539  total_loss: 33.7  loss_ce: 0.2364  loss_mask: 0.3387  loss_dice: 2.672  loss_ce_0: 0.5634  loss_mask_0: 0.3429  loss_dice_0: 2.797  loss_ce_1: 0.297  loss_mask_1: 0.3427  loss_dice_1: 2.712  loss_ce_2: 0.284  loss_mask_2: 0.3415  loss_dice_2: 2.683  loss_ce_3: 0.2624  loss_mask_3: 0.3409  loss_dice_3: 2.674  loss_ce_4: 0.2641  loss_mask_4: 0.3433  loss_dice_4: 2.676  loss_ce_5: 0.2421  loss_mask_5: 0.34  loss_dice_5: 2.672  loss_ce_6: 0.2551  loss_mask_6: 0.3405  loss_dice_6: 2.67  loss_ce_7: 0.2501  loss_mask_7: 0.3394  loss_dice_7: 2.667  loss_ce_8: 0.2378  loss_mask_8: 0.3399  loss_dice_8: 2.675  time: 1.4819  data_time: 0.0706  lr: 2.9906e-06  max_mem: 21589M
[01/18 08:25:29] d2.utils.events INFO:  eta: 4:11:55  iter: 29559  total_loss: 32.71  loss_ce: 0.2458  loss_mask: 0.3469  loss_dice: 2.611  loss_ce_0: 0.5784  loss_mask_0: 0.3431  loss_dice_0: 2.75  loss_ce_1: 0.3083  loss_mask_1: 0.3526  loss_dice_1: 2.662  loss_ce_2: 0.2879  loss_mask_2: 0.3491  loss_dice_2: 2.633  loss_ce_3: 0.2633  loss_mask_3: 0.3494  loss_dice_3: 2.617  loss_ce_4: 0.2538  loss_mask_4: 0.3471  loss_dice_4: 2.606  loss_ce_5: 0.2472  loss_mask_5: 0.3466  loss_dice_5: 2.607  loss_ce_6: 0.2417  loss_mask_6: 0.3453  loss_dice_6: 2.608  loss_ce_7: 0.2444  loss_mask_7: 0.347  loss_dice_7: 2.607  loss_ce_8: 0.2454  loss_mask_8: 0.347  loss_dice_8: 2.606  time: 1.4819  data_time: 0.0702  lr: 2.9855e-06  max_mem: 21589M
[01/18 08:25:59] d2.utils.events INFO:  eta: 4:11:21  iter: 29579  total_loss: 32.83  loss_ce: 0.236  loss_mask: 0.3329  loss_dice: 2.63  loss_ce_0: 0.5503  loss_mask_0: 0.33  loss_dice_0: 2.776  loss_ce_1: 0.2805  loss_mask_1: 0.3363  loss_dice_1: 2.695  loss_ce_2: 0.2685  loss_mask_2: 0.334  loss_dice_2: 2.661  loss_ce_3: 0.2654  loss_mask_3: 0.3326  loss_dice_3: 2.648  loss_ce_4: 0.2518  loss_mask_4: 0.3319  loss_dice_4: 2.647  loss_ce_5: 0.2508  loss_mask_5: 0.3311  loss_dice_5: 2.644  loss_ce_6: 0.2394  loss_mask_6: 0.3325  loss_dice_6: 2.641  loss_ce_7: 0.2362  loss_mask_7: 0.3333  loss_dice_7: 2.635  loss_ce_8: 0.2399  loss_mask_8: 0.3329  loss_dice_8: 2.634  time: 1.4819  data_time: 0.0730  lr: 2.9803e-06  max_mem: 21589M
[01/18 08:26:28] d2.utils.events INFO:  eta: 4:10:38  iter: 29599  total_loss: 32.21  loss_ce: 0.2288  loss_mask: 0.3318  loss_dice: 2.596  loss_ce_0: 0.5379  loss_mask_0: 0.3311  loss_dice_0: 2.739  loss_ce_1: 0.2674  loss_mask_1: 0.3377  loss_dice_1: 2.644  loss_ce_2: 0.2656  loss_mask_2: 0.3352  loss_dice_2: 2.629  loss_ce_3: 0.248  loss_mask_3: 0.3348  loss_dice_3: 2.601  loss_ce_4: 0.2503  loss_mask_4: 0.3322  loss_dice_4: 2.605  loss_ce_5: 0.2363  loss_mask_5: 0.3314  loss_dice_5: 2.601  loss_ce_6: 0.2365  loss_mask_6: 0.3323  loss_dice_6: 2.597  loss_ce_7: 0.225  loss_mask_7: 0.3332  loss_dice_7: 2.601  loss_ce_8: 0.2151  loss_mask_8: 0.3323  loss_dice_8: 2.596  time: 1.4819  data_time: 0.0684  lr: 2.9752e-06  max_mem: 21589M
[01/18 08:26:57] d2.utils.events INFO:  eta: 4:09:58  iter: 29619  total_loss: 32.85  loss_ce: 0.2508  loss_mask: 0.3379  loss_dice: 2.633  loss_ce_0: 0.5542  loss_mask_0: 0.3388  loss_dice_0: 2.763  loss_ce_1: 0.2825  loss_mask_1: 0.3463  loss_dice_1: 2.683  loss_ce_2: 0.2849  loss_mask_2: 0.3393  loss_dice_2: 2.652  loss_ce_3: 0.2785  loss_mask_3: 0.3375  loss_dice_3: 2.638  loss_ce_4: 0.2552  loss_mask_4: 0.3381  loss_dice_4: 2.64  loss_ce_5: 0.2592  loss_mask_5: 0.3374  loss_dice_5: 2.634  loss_ce_6: 0.2444  loss_mask_6: 0.3388  loss_dice_6: 2.629  loss_ce_7: 0.2446  loss_mask_7: 0.3378  loss_dice_7: 2.644  loss_ce_8: 0.2343  loss_mask_8: 0.3384  loss_dice_8: 2.632  time: 1.4818  data_time: 0.0688  lr: 2.97e-06  max_mem: 21589M
[01/18 08:27:26] d2.utils.events INFO:  eta: 4:09:20  iter: 29639  total_loss: 32.6  loss_ce: 0.2528  loss_mask: 0.3408  loss_dice: 2.613  loss_ce_0: 0.5454  loss_mask_0: 0.3359  loss_dice_0: 2.741  loss_ce_1: 0.2945  loss_mask_1: 0.3443  loss_dice_1: 2.651  loss_ce_2: 0.2763  loss_mask_2: 0.3419  loss_dice_2: 2.621  loss_ce_3: 0.2689  loss_mask_3: 0.3425  loss_dice_3: 2.608  loss_ce_4: 0.2454  loss_mask_4: 0.3432  loss_dice_4: 2.606  loss_ce_5: 0.2487  loss_mask_5: 0.3427  loss_dice_5: 2.609  loss_ce_6: 0.2481  loss_mask_6: 0.3434  loss_dice_6: 2.608  loss_ce_7: 0.2376  loss_mask_7: 0.3417  loss_dice_7: 2.607  loss_ce_8: 0.254  loss_mask_8: 0.3393  loss_dice_8: 2.611  time: 1.4818  data_time: 0.0668  lr: 2.9649e-06  max_mem: 21589M
[01/18 08:27:54] d2.utils.events INFO:  eta: 4:08:48  iter: 29659  total_loss: 32.43  loss_ce: 0.2471  loss_mask: 0.342  loss_dice: 2.575  loss_ce_0: 0.5495  loss_mask_0: 0.3403  loss_dice_0: 2.709  loss_ce_1: 0.2958  loss_mask_1: 0.3439  loss_dice_1: 2.606  loss_ce_2: 0.2918  loss_mask_2: 0.3427  loss_dice_2: 2.592  loss_ce_3: 0.2584  loss_mask_3: 0.3406  loss_dice_3: 2.58  loss_ce_4: 0.2578  loss_mask_4: 0.34  loss_dice_4: 2.574  loss_ce_5: 0.2431  loss_mask_5: 0.3402  loss_dice_5: 2.574  loss_ce_6: 0.2617  loss_mask_6: 0.3404  loss_dice_6: 2.577  loss_ce_7: 0.2503  loss_mask_7: 0.3411  loss_dice_7: 2.572  loss_ce_8: 0.2481  loss_mask_8: 0.3418  loss_dice_8: 2.571  time: 1.4818  data_time: 0.0678  lr: 2.9597e-06  max_mem: 21589M
[01/18 08:28:23] d2.utils.events INFO:  eta: 4:08:21  iter: 29679  total_loss: 33  loss_ce: 0.2474  loss_mask: 0.3424  loss_dice: 2.64  loss_ce_0: 0.5687  loss_mask_0: 0.3398  loss_dice_0: 2.776  loss_ce_1: 0.2969  loss_mask_1: 0.3499  loss_dice_1: 2.68  loss_ce_2: 0.2813  loss_mask_2: 0.3455  loss_dice_2: 2.664  loss_ce_3: 0.2517  loss_mask_3: 0.345  loss_dice_3: 2.648  loss_ce_4: 0.2556  loss_mask_4: 0.3437  loss_dice_4: 2.645  loss_ce_5: 0.2612  loss_mask_5: 0.3428  loss_dice_5: 2.639  loss_ce_6: 0.2511  loss_mask_6: 0.3433  loss_dice_6: 2.638  loss_ce_7: 0.2477  loss_mask_7: 0.342  loss_dice_7: 2.642  loss_ce_8: 0.2506  loss_mask_8: 0.3424  loss_dice_8: 2.642  time: 1.4818  data_time: 0.0701  lr: 2.9546e-06  max_mem: 21589M
[01/18 08:28:52] d2.utils.events INFO:  eta: 4:07:43  iter: 29699  total_loss: 33.46  loss_ce: 0.2473  loss_mask: 0.3396  loss_dice: 2.684  loss_ce_0: 0.575  loss_mask_0: 0.3443  loss_dice_0: 2.824  loss_ce_1: 0.3124  loss_mask_1: 0.3444  loss_dice_1: 2.731  loss_ce_2: 0.2886  loss_mask_2: 0.3422  loss_dice_2: 2.717  loss_ce_3: 0.2593  loss_mask_3: 0.3415  loss_dice_3: 2.7  loss_ce_4: 0.2586  loss_mask_4: 0.3385  loss_dice_4: 2.7  loss_ce_5: 0.2493  loss_mask_5: 0.3385  loss_dice_5: 2.701  loss_ce_6: 0.2491  loss_mask_6: 0.3384  loss_dice_6: 2.691  loss_ce_7: 0.2378  loss_mask_7: 0.3382  loss_dice_7: 2.696  loss_ce_8: 0.252  loss_mask_8: 0.3392  loss_dice_8: 2.693  time: 1.4817  data_time: 0.0667  lr: 2.9494e-06  max_mem: 21589M
[01/18 08:29:21] d2.utils.events INFO:  eta: 4:07:14  iter: 29719  total_loss: 33.37  loss_ce: 0.2425  loss_mask: 0.3402  loss_dice: 2.703  loss_ce_0: 0.5499  loss_mask_0: 0.3361  loss_dice_0: 2.836  loss_ce_1: 0.286  loss_mask_1: 0.3456  loss_dice_1: 2.75  loss_ce_2: 0.2635  loss_mask_2: 0.3425  loss_dice_2: 2.729  loss_ce_3: 0.2587  loss_mask_3: 0.3407  loss_dice_3: 2.709  loss_ce_4: 0.2542  loss_mask_4: 0.3409  loss_dice_4: 2.707  loss_ce_5: 0.2605  loss_mask_5: 0.3413  loss_dice_5: 2.707  loss_ce_6: 0.2436  loss_mask_6: 0.3404  loss_dice_6: 2.704  loss_ce_7: 0.2427  loss_mask_7: 0.3406  loss_dice_7: 2.707  loss_ce_8: 0.2398  loss_mask_8: 0.3401  loss_dice_8: 2.7  time: 1.4817  data_time: 0.0683  lr: 2.9443e-06  max_mem: 21589M
[01/18 08:29:51] d2.utils.events INFO:  eta: 4:06:47  iter: 29739  total_loss: 32.89  loss_ce: 0.2423  loss_mask: 0.339  loss_dice: 2.65  loss_ce_0: 0.5701  loss_mask_0: 0.3327  loss_dice_0: 2.795  loss_ce_1: 0.2971  loss_mask_1: 0.342  loss_dice_1: 2.693  loss_ce_2: 0.2851  loss_mask_2: 0.3399  loss_dice_2: 2.67  loss_ce_3: 0.2649  loss_mask_3: 0.3395  loss_dice_3: 2.657  loss_ce_4: 0.2719  loss_mask_4: 0.3379  loss_dice_4: 2.651  loss_ce_5: 0.2602  loss_mask_5: 0.3397  loss_dice_5: 2.649  loss_ce_6: 0.2508  loss_mask_6: 0.3386  loss_dice_6: 2.65  loss_ce_7: 0.2469  loss_mask_7: 0.3392  loss_dice_7: 2.645  loss_ce_8: 0.2425  loss_mask_8: 0.3375  loss_dice_8: 2.647  time: 1.4817  data_time: 0.0725  lr: 2.9391e-06  max_mem: 21589M
[01/18 08:30:20] d2.utils.events INFO:  eta: 4:06:16  iter: 29759  total_loss: 33.34  loss_ce: 0.2419  loss_mask: 0.3458  loss_dice: 2.676  loss_ce_0: 0.5375  loss_mask_0: 0.3476  loss_dice_0: 2.827  loss_ce_1: 0.2808  loss_mask_1: 0.3482  loss_dice_1: 2.72  loss_ce_2: 0.2764  loss_mask_2: 0.3461  loss_dice_2: 2.699  loss_ce_3: 0.2524  loss_mask_3: 0.3454  loss_dice_3: 2.682  loss_ce_4: 0.2527  loss_mask_4: 0.3453  loss_dice_4: 2.681  loss_ce_5: 0.2501  loss_mask_5: 0.3452  loss_dice_5: 2.679  loss_ce_6: 0.2476  loss_mask_6: 0.3454  loss_dice_6: 2.675  loss_ce_7: 0.2369  loss_mask_7: 0.3459  loss_dice_7: 2.679  loss_ce_8: 0.2461  loss_mask_8: 0.3446  loss_dice_8: 2.678  time: 1.4817  data_time: 0.0747  lr: 2.934e-06  max_mem: 21589M
[01/18 08:30:49] d2.utils.events INFO:  eta: 4:05:48  iter: 29779  total_loss: 32.72  loss_ce: 0.2296  loss_mask: 0.3361  loss_dice: 2.638  loss_ce_0: 0.5424  loss_mask_0: 0.3352  loss_dice_0: 2.763  loss_ce_1: 0.3023  loss_mask_1: 0.3396  loss_dice_1: 2.671  loss_ce_2: 0.2646  loss_mask_2: 0.3377  loss_dice_2: 2.653  loss_ce_3: 0.2607  loss_mask_3: 0.3375  loss_dice_3: 2.642  loss_ce_4: 0.2405  loss_mask_4: 0.3359  loss_dice_4: 2.632  loss_ce_5: 0.23  loss_mask_5: 0.3362  loss_dice_5: 2.644  loss_ce_6: 0.2219  loss_mask_6: 0.3358  loss_dice_6: 2.644  loss_ce_7: 0.2254  loss_mask_7: 0.3371  loss_dice_7: 2.642  loss_ce_8: 0.2233  loss_mask_8: 0.337  loss_dice_8: 2.639  time: 1.4817  data_time: 0.0725  lr: 2.9288e-06  max_mem: 21589M
[01/18 08:31:18] d2.utils.events INFO:  eta: 4:05:20  iter: 29799  total_loss: 33.36  loss_ce: 0.2524  loss_mask: 0.3479  loss_dice: 2.644  loss_ce_0: 0.5731  loss_mask_0: 0.347  loss_dice_0: 2.775  loss_ce_1: 0.3077  loss_mask_1: 0.3536  loss_dice_1: 2.685  loss_ce_2: 0.3069  loss_mask_2: 0.349  loss_dice_2: 2.671  loss_ce_3: 0.2728  loss_mask_3: 0.3465  loss_dice_3: 2.655  loss_ce_4: 0.2667  loss_mask_4: 0.3456  loss_dice_4: 2.651  loss_ce_5: 0.2549  loss_mask_5: 0.3454  loss_dice_5: 2.649  loss_ce_6: 0.2466  loss_mask_6: 0.3443  loss_dice_6: 2.646  loss_ce_7: 0.2477  loss_mask_7: 0.3454  loss_dice_7: 2.651  loss_ce_8: 0.2485  loss_mask_8: 0.3459  loss_dice_8: 2.651  time: 1.4816  data_time: 0.0666  lr: 2.9236e-06  max_mem: 21589M
[01/18 08:31:47] d2.utils.events INFO:  eta: 4:04:55  iter: 29819  total_loss: 32.8  loss_ce: 0.2389  loss_mask: 0.3427  loss_dice: 2.596  loss_ce_0: 0.5648  loss_mask_0: 0.3432  loss_dice_0: 2.744  loss_ce_1: 0.2961  loss_mask_1: 0.3525  loss_dice_1: 2.638  loss_ce_2: 0.2745  loss_mask_2: 0.3485  loss_dice_2: 2.621  loss_ce_3: 0.2612  loss_mask_3: 0.3445  loss_dice_3: 2.606  loss_ce_4: 0.2509  loss_mask_4: 0.3425  loss_dice_4: 2.603  loss_ce_5: 0.2356  loss_mask_5: 0.3441  loss_dice_5: 2.607  loss_ce_6: 0.2369  loss_mask_6: 0.3431  loss_dice_6: 2.599  loss_ce_7: 0.2531  loss_mask_7: 0.3437  loss_dice_7: 2.596  loss_ce_8: 0.2444  loss_mask_8: 0.3426  loss_dice_8: 2.592  time: 1.4816  data_time: 0.0653  lr: 2.9185e-06  max_mem: 21589M
[01/18 08:32:16] d2.utils.events INFO:  eta: 4:04:21  iter: 29839  total_loss: 32.71  loss_ce: 0.2362  loss_mask: 0.345  loss_dice: 2.602  loss_ce_0: 0.5466  loss_mask_0: 0.3387  loss_dice_0: 2.739  loss_ce_1: 0.283  loss_mask_1: 0.3467  loss_dice_1: 2.645  loss_ce_2: 0.2781  loss_mask_2: 0.3453  loss_dice_2: 2.622  loss_ce_3: 0.2378  loss_mask_3: 0.3434  loss_dice_3: 2.6  loss_ce_4: 0.2314  loss_mask_4: 0.3429  loss_dice_4: 2.605  loss_ce_5: 0.2467  loss_mask_5: 0.341  loss_dice_5: 2.61  loss_ce_6: 0.2228  loss_mask_6: 0.343  loss_dice_6: 2.61  loss_ce_7: 0.2226  loss_mask_7: 0.3424  loss_dice_7: 2.601  loss_ce_8: 0.2342  loss_mask_8: 0.3431  loss_dice_8: 2.6  time: 1.4816  data_time: 0.0689  lr: 2.9133e-06  max_mem: 21589M
[01/18 08:32:44] d2.utils.events INFO:  eta: 4:03:57  iter: 29859  total_loss: 32.58  loss_ce: 0.2323  loss_mask: 0.3532  loss_dice: 2.608  loss_ce_0: 0.5577  loss_mask_0: 0.3534  loss_dice_0: 2.745  loss_ce_1: 0.2708  loss_mask_1: 0.3611  loss_dice_1: 2.666  loss_ce_2: 0.2716  loss_mask_2: 0.3555  loss_dice_2: 2.647  loss_ce_3: 0.2491  loss_mask_3: 0.3539  loss_dice_3: 2.627  loss_ce_4: 0.2584  loss_mask_4: 0.3517  loss_dice_4: 2.623  loss_ce_5: 0.2471  loss_mask_5: 0.3524  loss_dice_5: 2.615  loss_ce_6: 0.2372  loss_mask_6: 0.3529  loss_dice_6: 2.616  loss_ce_7: 0.2474  loss_mask_7: 0.3536  loss_dice_7: 2.605  loss_ce_8: 0.233  loss_mask_8: 0.3528  loss_dice_8: 2.606  time: 1.4816  data_time: 0.0669  lr: 2.9082e-06  max_mem: 21589M
[01/18 08:33:13] d2.utils.events INFO:  eta: 4:03:23  iter: 29879  total_loss: 33.47  loss_ce: 0.2543  loss_mask: 0.3431  loss_dice: 2.669  loss_ce_0: 0.5633  loss_mask_0: 0.3428  loss_dice_0: 2.807  loss_ce_1: 0.3223  loss_mask_1: 0.3503  loss_dice_1: 2.706  loss_ce_2: 0.3086  loss_mask_2: 0.3455  loss_dice_2: 2.689  loss_ce_3: 0.2909  loss_mask_3: 0.3453  loss_dice_3: 2.669  loss_ce_4: 0.2806  loss_mask_4: 0.3446  loss_dice_4: 2.678  loss_ce_5: 0.2787  loss_mask_5: 0.3449  loss_dice_5: 2.673  loss_ce_6: 0.2544  loss_mask_6: 0.3441  loss_dice_6: 2.677  loss_ce_7: 0.2578  loss_mask_7: 0.3443  loss_dice_7: 2.669  loss_ce_8: 0.2474  loss_mask_8: 0.3435  loss_dice_8: 2.673  time: 1.4815  data_time: 0.0704  lr: 2.903e-06  max_mem: 21589M
[01/18 08:33:42] d2.utils.events INFO:  eta: 4:02:53  iter: 29899  total_loss: 32.42  loss_ce: 0.2254  loss_mask: 0.336  loss_dice: 2.607  loss_ce_0: 0.5427  loss_mask_0: 0.3401  loss_dice_0: 2.738  loss_ce_1: 0.2752  loss_mask_1: 0.34  loss_dice_1: 2.646  loss_ce_2: 0.2726  loss_mask_2: 0.338  loss_dice_2: 2.629  loss_ce_3: 0.2494  loss_mask_3: 0.3377  loss_dice_3: 2.608  loss_ce_4: 0.249  loss_mask_4: 0.3355  loss_dice_4: 2.601  loss_ce_5: 0.2387  loss_mask_5: 0.3364  loss_dice_5: 2.603  loss_ce_6: 0.2299  loss_mask_6: 0.3366  loss_dice_6: 2.606  loss_ce_7: 0.2317  loss_mask_7: 0.3358  loss_dice_7: 2.603  loss_ce_8: 0.2222  loss_mask_8: 0.3358  loss_dice_8: 2.608  time: 1.4815  data_time: 0.0677  lr: 2.8978e-06  max_mem: 21589M
[01/18 08:34:11] d2.utils.events INFO:  eta: 4:02:24  iter: 29919  total_loss: 33.08  loss_ce: 0.2614  loss_mask: 0.3387  loss_dice: 2.673  loss_ce_0: 0.5828  loss_mask_0: 0.3428  loss_dice_0: 2.804  loss_ce_1: 0.3047  loss_mask_1: 0.3452  loss_dice_1: 2.727  loss_ce_2: 0.2966  loss_mask_2: 0.3408  loss_dice_2: 2.695  loss_ce_3: 0.3038  loss_mask_3: 0.3408  loss_dice_3: 2.686  loss_ce_4: 0.2801  loss_mask_4: 0.3382  loss_dice_4: 2.683  loss_ce_5: 0.2663  loss_mask_5: 0.3389  loss_dice_5: 2.684  loss_ce_6: 0.2583  loss_mask_6: 0.3388  loss_dice_6: 2.671  loss_ce_7: 0.254  loss_mask_7: 0.3387  loss_dice_7: 2.675  loss_ce_8: 0.2641  loss_mask_8: 0.3385  loss_dice_8: 2.679  time: 1.4815  data_time: 0.0686  lr: 2.8927e-06  max_mem: 21589M
[01/18 08:34:40] d2.utils.events INFO:  eta: 4:01:55  iter: 29939  total_loss: 33.31  loss_ce: 0.2288  loss_mask: 0.3423  loss_dice: 2.663  loss_ce_0: 0.5714  loss_mask_0: 0.3396  loss_dice_0: 2.804  loss_ce_1: 0.282  loss_mask_1: 0.3463  loss_dice_1: 2.72  loss_ce_2: 0.2749  loss_mask_2: 0.3452  loss_dice_2: 2.695  loss_ce_3: 0.2538  loss_mask_3: 0.3415  loss_dice_3: 2.678  loss_ce_4: 0.2561  loss_mask_4: 0.3429  loss_dice_4: 2.667  loss_ce_5: 0.2462  loss_mask_5: 0.3425  loss_dice_5: 2.673  loss_ce_6: 0.2392  loss_mask_6: 0.3422  loss_dice_6: 2.678  loss_ce_7: 0.2356  loss_mask_7: 0.3414  loss_dice_7: 2.664  loss_ce_8: 0.2373  loss_mask_8: 0.3415  loss_dice_8: 2.672  time: 1.4815  data_time: 0.0742  lr: 2.8875e-06  max_mem: 21589M
[01/18 08:35:09] d2.utils.events INFO:  eta: 4:01:25  iter: 29959  total_loss: 32.23  loss_ce: 0.2478  loss_mask: 0.3285  loss_dice: 2.582  loss_ce_0: 0.5646  loss_mask_0: 0.3349  loss_dice_0: 2.726  loss_ce_1: 0.2889  loss_mask_1: 0.3361  loss_dice_1: 2.63  loss_ce_2: 0.2707  loss_mask_2: 0.3317  loss_dice_2: 2.604  loss_ce_3: 0.2672  loss_mask_3: 0.329  loss_dice_3: 2.582  loss_ce_4: 0.2498  loss_mask_4: 0.3287  loss_dice_4: 2.583  loss_ce_5: 0.2505  loss_mask_5: 0.3285  loss_dice_5: 2.583  loss_ce_6: 0.2559  loss_mask_6: 0.3312  loss_dice_6: 2.583  loss_ce_7: 0.2511  loss_mask_7: 0.3299  loss_dice_7: 2.584  loss_ce_8: 0.2443  loss_mask_8: 0.3283  loss_dice_8: 2.585  time: 1.4814  data_time: 0.0666  lr: 2.8823e-06  max_mem: 21589M
[01/18 08:35:39] d2.utils.events INFO:  eta: 4:01:03  iter: 29979  total_loss: 32.98  loss_ce: 0.2504  loss_mask: 0.336  loss_dice: 2.643  loss_ce_0: 0.5476  loss_mask_0: 0.3358  loss_dice_0: 2.778  loss_ce_1: 0.2939  loss_mask_1: 0.3391  loss_dice_1: 2.69  loss_ce_2: 0.2824  loss_mask_2: 0.3388  loss_dice_2: 2.668  loss_ce_3: 0.2726  loss_mask_3: 0.338  loss_dice_3: 2.659  loss_ce_4: 0.2678  loss_mask_4: 0.3364  loss_dice_4: 2.65  loss_ce_5: 0.2606  loss_mask_5: 0.3359  loss_dice_5: 2.652  loss_ce_6: 0.2496  loss_mask_6: 0.336  loss_dice_6: 2.648  loss_ce_7: 0.2459  loss_mask_7: 0.3388  loss_dice_7: 2.641  loss_ce_8: 0.2336  loss_mask_8: 0.3373  loss_dice_8: 2.638  time: 1.4814  data_time: 0.0708  lr: 2.8772e-06  max_mem: 21589M
[01/18 08:36:08] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0029999.pth
[01/18 08:36:10] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 08:36:10] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 08:36:10] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 08:36:11] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 08:36:25] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0097 s/iter. Inference: 0.1536 s/iter. Eval: 0.2239 s/iter. Total: 0.3872 s/iter. ETA=0:06:58
[01/18 08:36:30] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0113 s/iter. Inference: 0.1496 s/iter. Eval: 0.2263 s/iter. Total: 0.3872 s/iter. ETA=0:06:53
[01/18 08:36:35] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0113 s/iter. Inference: 0.1533 s/iter. Eval: 0.2186 s/iter. Total: 0.3833 s/iter. ETA=0:06:44
[01/18 08:36:40] d2.evaluation.evaluator INFO: Inference done 51/1093. Dataloading: 0.0121 s/iter. Inference: 0.1538 s/iter. Eval: 0.2200 s/iter. Total: 0.3860 s/iter. ETA=0:06:42
[01/18 08:36:46] d2.evaluation.evaluator INFO: Inference done 62/1093. Dataloading: 0.0128 s/iter. Inference: 0.1559 s/iter. Eval: 0.2341 s/iter. Total: 0.4029 s/iter. ETA=0:06:55
[01/18 08:36:51] d2.evaluation.evaluator INFO: Inference done 75/1093. Dataloading: 0.0129 s/iter. Inference: 0.1542 s/iter. Eval: 0.2362 s/iter. Total: 0.4034 s/iter. ETA=0:06:50
[01/18 08:36:56] d2.evaluation.evaluator INFO: Inference done 88/1093. Dataloading: 0.0128 s/iter. Inference: 0.1532 s/iter. Eval: 0.2404 s/iter. Total: 0.4064 s/iter. ETA=0:06:48
[01/18 08:37:02] d2.evaluation.evaluator INFO: Inference done 102/1093. Dataloading: 0.0127 s/iter. Inference: 0.1520 s/iter. Eval: 0.2357 s/iter. Total: 0.4004 s/iter. ETA=0:06:36
[01/18 08:37:07] d2.evaluation.evaluator INFO: Inference done 116/1093. Dataloading: 0.0125 s/iter. Inference: 0.1524 s/iter. Eval: 0.2339 s/iter. Total: 0.3988 s/iter. ETA=0:06:29
[01/18 08:37:12] d2.evaluation.evaluator INFO: Inference done 130/1093. Dataloading: 0.0125 s/iter. Inference: 0.1518 s/iter. Eval: 0.2315 s/iter. Total: 0.3959 s/iter. ETA=0:06:21
[01/18 08:37:18] d2.evaluation.evaluator INFO: Inference done 144/1093. Dataloading: 0.0123 s/iter. Inference: 0.1533 s/iter. Eval: 0.2287 s/iter. Total: 0.3944 s/iter. ETA=0:06:14
[01/18 08:37:23] d2.evaluation.evaluator INFO: Inference done 159/1093. Dataloading: 0.0122 s/iter. Inference: 0.1539 s/iter. Eval: 0.2236 s/iter. Total: 0.3898 s/iter. ETA=0:06:04
[01/18 08:37:28] d2.evaluation.evaluator INFO: Inference done 172/1093. Dataloading: 0.0122 s/iter. Inference: 0.1539 s/iter. Eval: 0.2249 s/iter. Total: 0.3912 s/iter. ETA=0:06:00
[01/18 08:37:33] d2.evaluation.evaluator INFO: Inference done 186/1093. Dataloading: 0.0121 s/iter. Inference: 0.1537 s/iter. Eval: 0.2232 s/iter. Total: 0.3891 s/iter. ETA=0:05:52
[01/18 08:37:38] d2.evaluation.evaluator INFO: Inference done 201/1093. Dataloading: 0.0121 s/iter. Inference: 0.1534 s/iter. Eval: 0.2203 s/iter. Total: 0.3859 s/iter. ETA=0:05:44
[01/18 08:37:43] d2.evaluation.evaluator INFO: Inference done 213/1093. Dataloading: 0.0121 s/iter. Inference: 0.1535 s/iter. Eval: 0.2224 s/iter. Total: 0.3881 s/iter. ETA=0:05:41
[01/18 08:37:49] d2.evaluation.evaluator INFO: Inference done 227/1093. Dataloading: 0.0121 s/iter. Inference: 0.1539 s/iter. Eval: 0.2217 s/iter. Total: 0.3878 s/iter. ETA=0:05:35
[01/18 08:37:54] d2.evaluation.evaluator INFO: Inference done 240/1093. Dataloading: 0.0122 s/iter. Inference: 0.1541 s/iter. Eval: 0.2229 s/iter. Total: 0.3893 s/iter. ETA=0:05:32
[01/18 08:38:00] d2.evaluation.evaluator INFO: Inference done 253/1093. Dataloading: 0.0123 s/iter. Inference: 0.1540 s/iter. Eval: 0.2242 s/iter. Total: 0.3906 s/iter. ETA=0:05:28
[01/18 08:38:05] d2.evaluation.evaluator INFO: Inference done 266/1093. Dataloading: 0.0123 s/iter. Inference: 0.1542 s/iter. Eval: 0.2245 s/iter. Total: 0.3910 s/iter. ETA=0:05:23
[01/18 08:38:10] d2.evaluation.evaluator INFO: Inference done 278/1093. Dataloading: 0.0124 s/iter. Inference: 0.1548 s/iter. Eval: 0.2254 s/iter. Total: 0.3927 s/iter. ETA=0:05:20
[01/18 08:38:15] d2.evaluation.evaluator INFO: Inference done 292/1093. Dataloading: 0.0123 s/iter. Inference: 0.1552 s/iter. Eval: 0.2236 s/iter. Total: 0.3912 s/iter. ETA=0:05:13
[01/18 08:38:20] d2.evaluation.evaluator INFO: Inference done 305/1093. Dataloading: 0.0124 s/iter. Inference: 0.1554 s/iter. Eval: 0.2241 s/iter. Total: 0.3919 s/iter. ETA=0:05:08
[01/18 08:38:25] d2.evaluation.evaluator INFO: Inference done 318/1093. Dataloading: 0.0124 s/iter. Inference: 0.1549 s/iter. Eval: 0.2243 s/iter. Total: 0.3917 s/iter. ETA=0:05:03
[01/18 08:38:31] d2.evaluation.evaluator INFO: Inference done 331/1093. Dataloading: 0.0125 s/iter. Inference: 0.1552 s/iter. Eval: 0.2243 s/iter. Total: 0.3921 s/iter. ETA=0:04:58
[01/18 08:38:36] d2.evaluation.evaluator INFO: Inference done 347/1093. Dataloading: 0.0123 s/iter. Inference: 0.1553 s/iter. Eval: 0.2216 s/iter. Total: 0.3894 s/iter. ETA=0:04:50
[01/18 08:38:41] d2.evaluation.evaluator INFO: Inference done 360/1093. Dataloading: 0.0123 s/iter. Inference: 0.1555 s/iter. Eval: 0.2218 s/iter. Total: 0.3896 s/iter. ETA=0:04:45
[01/18 08:38:46] d2.evaluation.evaluator INFO: Inference done 372/1093. Dataloading: 0.0123 s/iter. Inference: 0.1557 s/iter. Eval: 0.2228 s/iter. Total: 0.3909 s/iter. ETA=0:04:41
[01/18 08:38:51] d2.evaluation.evaluator INFO: Inference done 385/1093. Dataloading: 0.0123 s/iter. Inference: 0.1558 s/iter. Eval: 0.2227 s/iter. Total: 0.3909 s/iter. ETA=0:04:36
[01/18 08:38:56] d2.evaluation.evaluator INFO: Inference done 397/1093. Dataloading: 0.0123 s/iter. Inference: 0.1560 s/iter. Eval: 0.2233 s/iter. Total: 0.3917 s/iter. ETA=0:04:32
[01/18 08:39:01] d2.evaluation.evaluator INFO: Inference done 411/1093. Dataloading: 0.0122 s/iter. Inference: 0.1560 s/iter. Eval: 0.2223 s/iter. Total: 0.3906 s/iter. ETA=0:04:26
[01/18 08:39:06] d2.evaluation.evaluator INFO: Inference done 424/1093. Dataloading: 0.0123 s/iter. Inference: 0.1555 s/iter. Eval: 0.2225 s/iter. Total: 0.3904 s/iter. ETA=0:04:21
[01/18 08:39:12] d2.evaluation.evaluator INFO: Inference done 438/1093. Dataloading: 0.0123 s/iter. Inference: 0.1555 s/iter. Eval: 0.2221 s/iter. Total: 0.3900 s/iter. ETA=0:04:15
[01/18 08:39:17] d2.evaluation.evaluator INFO: Inference done 453/1093. Dataloading: 0.0122 s/iter. Inference: 0.1554 s/iter. Eval: 0.2211 s/iter. Total: 0.3888 s/iter. ETA=0:04:08
[01/18 08:39:22] d2.evaluation.evaluator INFO: Inference done 467/1093. Dataloading: 0.0122 s/iter. Inference: 0.1555 s/iter. Eval: 0.2206 s/iter. Total: 0.3884 s/iter. ETA=0:04:03
[01/18 08:39:27] d2.evaluation.evaluator INFO: Inference done 482/1093. Dataloading: 0.0121 s/iter. Inference: 0.1559 s/iter. Eval: 0.2190 s/iter. Total: 0.3872 s/iter. ETA=0:03:56
[01/18 08:39:33] d2.evaluation.evaluator INFO: Inference done 497/1093. Dataloading: 0.0121 s/iter. Inference: 0.1557 s/iter. Eval: 0.2180 s/iter. Total: 0.3859 s/iter. ETA=0:03:49
[01/18 08:39:38] d2.evaluation.evaluator INFO: Inference done 513/1093. Dataloading: 0.0120 s/iter. Inference: 0.1555 s/iter. Eval: 0.2161 s/iter. Total: 0.3837 s/iter. ETA=0:03:42
[01/18 08:39:43] d2.evaluation.evaluator INFO: Inference done 526/1093. Dataloading: 0.0120 s/iter. Inference: 0.1555 s/iter. Eval: 0.2166 s/iter. Total: 0.3842 s/iter. ETA=0:03:37
[01/18 08:39:48] d2.evaluation.evaluator INFO: Inference done 541/1093. Dataloading: 0.0120 s/iter. Inference: 0.1553 s/iter. Eval: 0.2161 s/iter. Total: 0.3835 s/iter. ETA=0:03:31
[01/18 08:39:53] d2.evaluation.evaluator INFO: Inference done 553/1093. Dataloading: 0.0121 s/iter. Inference: 0.1555 s/iter. Eval: 0.2167 s/iter. Total: 0.3843 s/iter. ETA=0:03:27
[01/18 08:39:59] d2.evaluation.evaluator INFO: Inference done 568/1093. Dataloading: 0.0121 s/iter. Inference: 0.1552 s/iter. Eval: 0.2161 s/iter. Total: 0.3835 s/iter. ETA=0:03:21
[01/18 08:40:04] d2.evaluation.evaluator INFO: Inference done 584/1093. Dataloading: 0.0121 s/iter. Inference: 0.1552 s/iter. Eval: 0.2141 s/iter. Total: 0.3815 s/iter. ETA=0:03:14
[01/18 08:40:09] d2.evaluation.evaluator INFO: Inference done 597/1093. Dataloading: 0.0121 s/iter. Inference: 0.1554 s/iter. Eval: 0.2141 s/iter. Total: 0.3817 s/iter. ETA=0:03:09
[01/18 08:40:14] d2.evaluation.evaluator INFO: Inference done 609/1093. Dataloading: 0.0122 s/iter. Inference: 0.1557 s/iter. Eval: 0.2152 s/iter. Total: 0.3831 s/iter. ETA=0:03:05
[01/18 08:40:19] d2.evaluation.evaluator INFO: Inference done 624/1093. Dataloading: 0.0121 s/iter. Inference: 0.1556 s/iter. Eval: 0.2145 s/iter. Total: 0.3823 s/iter. ETA=0:02:59
[01/18 08:40:24] d2.evaluation.evaluator INFO: Inference done 638/1093. Dataloading: 0.0121 s/iter. Inference: 0.1556 s/iter. Eval: 0.2140 s/iter. Total: 0.3818 s/iter. ETA=0:02:53
[01/18 08:40:29] d2.evaluation.evaluator INFO: Inference done 652/1093. Dataloading: 0.0121 s/iter. Inference: 0.1556 s/iter. Eval: 0.2136 s/iter. Total: 0.3814 s/iter. ETA=0:02:48
[01/18 08:40:35] d2.evaluation.evaluator INFO: Inference done 667/1093. Dataloading: 0.0121 s/iter. Inference: 0.1555 s/iter. Eval: 0.2131 s/iter. Total: 0.3808 s/iter. ETA=0:02:42
[01/18 08:40:40] d2.evaluation.evaluator INFO: Inference done 681/1093. Dataloading: 0.0120 s/iter. Inference: 0.1556 s/iter. Eval: 0.2129 s/iter. Total: 0.3807 s/iter. ETA=0:02:36
[01/18 08:40:45] d2.evaluation.evaluator INFO: Inference done 696/1093. Dataloading: 0.0120 s/iter. Inference: 0.1557 s/iter. Eval: 0.2121 s/iter. Total: 0.3800 s/iter. ETA=0:02:30
[01/18 08:40:50] d2.evaluation.evaluator INFO: Inference done 707/1093. Dataloading: 0.0121 s/iter. Inference: 0.1559 s/iter. Eval: 0.2132 s/iter. Total: 0.3812 s/iter. ETA=0:02:27
[01/18 08:40:55] d2.evaluation.evaluator INFO: Inference done 718/1093. Dataloading: 0.0122 s/iter. Inference: 0.1560 s/iter. Eval: 0.2141 s/iter. Total: 0.3824 s/iter. ETA=0:02:23
[01/18 08:41:01] d2.evaluation.evaluator INFO: Inference done 734/1093. Dataloading: 0.0121 s/iter. Inference: 0.1559 s/iter. Eval: 0.2130 s/iter. Total: 0.3810 s/iter. ETA=0:02:16
[01/18 08:41:06] d2.evaluation.evaluator INFO: Inference done 749/1093. Dataloading: 0.0121 s/iter. Inference: 0.1558 s/iter. Eval: 0.2123 s/iter. Total: 0.3802 s/iter. ETA=0:02:10
[01/18 08:41:11] d2.evaluation.evaluator INFO: Inference done 762/1093. Dataloading: 0.0121 s/iter. Inference: 0.1559 s/iter. Eval: 0.2124 s/iter. Total: 0.3804 s/iter. ETA=0:02:05
[01/18 08:41:16] d2.evaluation.evaluator INFO: Inference done 774/1093. Dataloading: 0.0121 s/iter. Inference: 0.1559 s/iter. Eval: 0.2129 s/iter. Total: 0.3810 s/iter. ETA=0:02:01
[01/18 08:41:21] d2.evaluation.evaluator INFO: Inference done 788/1093. Dataloading: 0.0121 s/iter. Inference: 0.1562 s/iter. Eval: 0.2126 s/iter. Total: 0.3810 s/iter. ETA=0:01:56
[01/18 08:41:26] d2.evaluation.evaluator INFO: Inference done 802/1093. Dataloading: 0.0120 s/iter. Inference: 0.1562 s/iter. Eval: 0.2125 s/iter. Total: 0.3808 s/iter. ETA=0:01:50
[01/18 08:41:31] d2.evaluation.evaluator INFO: Inference done 817/1093. Dataloading: 0.0120 s/iter. Inference: 0.1561 s/iter. Eval: 0.2119 s/iter. Total: 0.3801 s/iter. ETA=0:01:44
[01/18 08:41:37] d2.evaluation.evaluator INFO: Inference done 834/1093. Dataloading: 0.0120 s/iter. Inference: 0.1558 s/iter. Eval: 0.2108 s/iter. Total: 0.3787 s/iter. ETA=0:01:38
[01/18 08:41:42] d2.evaluation.evaluator INFO: Inference done 849/1093. Dataloading: 0.0119 s/iter. Inference: 0.1557 s/iter. Eval: 0.2102 s/iter. Total: 0.3779 s/iter. ETA=0:01:32
[01/18 08:41:47] d2.evaluation.evaluator INFO: Inference done 862/1093. Dataloading: 0.0120 s/iter. Inference: 0.1556 s/iter. Eval: 0.2107 s/iter. Total: 0.3784 s/iter. ETA=0:01:27
[01/18 08:41:52] d2.evaluation.evaluator INFO: Inference done 877/1093. Dataloading: 0.0119 s/iter. Inference: 0.1555 s/iter. Eval: 0.2104 s/iter. Total: 0.3779 s/iter. ETA=0:01:21
[01/18 08:41:57] d2.evaluation.evaluator INFO: Inference done 889/1093. Dataloading: 0.0120 s/iter. Inference: 0.1554 s/iter. Eval: 0.2110 s/iter. Total: 0.3785 s/iter. ETA=0:01:17
[01/18 08:42:02] d2.evaluation.evaluator INFO: Inference done 904/1093. Dataloading: 0.0119 s/iter. Inference: 0.1553 s/iter. Eval: 0.2104 s/iter. Total: 0.3778 s/iter. ETA=0:01:11
[01/18 08:42:07] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0119 s/iter. Inference: 0.1553 s/iter. Eval: 0.2106 s/iter. Total: 0.3780 s/iter. ETA=0:01:06
[01/18 08:42:13] d2.evaluation.evaluator INFO: Inference done 930/1093. Dataloading: 0.0119 s/iter. Inference: 0.1555 s/iter. Eval: 0.2106 s/iter. Total: 0.3782 s/iter. ETA=0:01:01
[01/18 08:42:18] d2.evaluation.evaluator INFO: Inference done 944/1093. Dataloading: 0.0119 s/iter. Inference: 0.1554 s/iter. Eval: 0.2108 s/iter. Total: 0.3782 s/iter. ETA=0:00:56
[01/18 08:42:23] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0119 s/iter. Inference: 0.1553 s/iter. Eval: 0.2106 s/iter. Total: 0.3780 s/iter. ETA=0:00:51
[01/18 08:42:28] d2.evaluation.evaluator INFO: Inference done 972/1093. Dataloading: 0.0119 s/iter. Inference: 0.1553 s/iter. Eval: 0.2106 s/iter. Total: 0.3779 s/iter. ETA=0:00:45
[01/18 08:42:34] d2.evaluation.evaluator INFO: Inference done 987/1093. Dataloading: 0.0119 s/iter. Inference: 0.1555 s/iter. Eval: 0.2101 s/iter. Total: 0.3776 s/iter. ETA=0:00:40
[01/18 08:42:39] d2.evaluation.evaluator INFO: Inference done 1001/1093. Dataloading: 0.0119 s/iter. Inference: 0.1555 s/iter. Eval: 0.2098 s/iter. Total: 0.3774 s/iter. ETA=0:00:34
[01/18 08:42:44] d2.evaluation.evaluator INFO: Inference done 1013/1093. Dataloading: 0.0119 s/iter. Inference: 0.1557 s/iter. Eval: 0.2102 s/iter. Total: 0.3779 s/iter. ETA=0:00:30
[01/18 08:42:49] d2.evaluation.evaluator INFO: Inference done 1026/1093. Dataloading: 0.0119 s/iter. Inference: 0.1558 s/iter. Eval: 0.2106 s/iter. Total: 0.3783 s/iter. ETA=0:00:25
[01/18 08:42:54] d2.evaluation.evaluator INFO: Inference done 1040/1093. Dataloading: 0.0119 s/iter. Inference: 0.1557 s/iter. Eval: 0.2105 s/iter. Total: 0.3781 s/iter. ETA=0:00:20
[01/18 08:42:59] d2.evaluation.evaluator INFO: Inference done 1054/1093. Dataloading: 0.0119 s/iter. Inference: 0.1557 s/iter. Eval: 0.2104 s/iter. Total: 0.3781 s/iter. ETA=0:00:14
[01/18 08:43:05] d2.evaluation.evaluator INFO: Inference done 1070/1093. Dataloading: 0.0118 s/iter. Inference: 0.1556 s/iter. Eval: 0.2098 s/iter. Total: 0.3773 s/iter. ETA=0:00:08
[01/18 08:43:10] d2.evaluation.evaluator INFO: Inference done 1087/1093. Dataloading: 0.0118 s/iter. Inference: 0.1554 s/iter. Eval: 0.2088 s/iter. Total: 0.3761 s/iter. ETA=0:00:02
[01/18 08:43:13] d2.evaluation.evaluator INFO: Total inference time: 0:06:49.786426 (0.376642 s / iter per device, on 4 devices)
[01/18 08:43:13] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:49 (0.155393 s / iter per device, on 4 devices)
[01/18 08:43:38] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.3456461834934608, 'mIoU': 20.250060540870592, 'fwIoU': 43.22359884510656, 'IoU-0': nan, 'IoU-1': 95.630509669165, 'IoU-2': 46.33011112026721, 'IoU-3': 57.93319232017817, 'IoU-4': 51.449387169124975, 'IoU-5': 44.991325324718794, 'IoU-6': 40.670858048626194, 'IoU-7': 33.04287537426986, 'IoU-8': 21.10210384518503, 'IoU-9': 35.40755405734806, 'IoU-10': 40.313720139930616, 'IoU-11': 48.828366469186044, 'IoU-12': 51.273813353574226, 'IoU-13': 49.871438625496104, 'IoU-14': 51.2134884674293, 'IoU-15': 51.6011536242157, 'IoU-16': 50.86944629837854, 'IoU-17': 47.41523243546105, 'IoU-18': 48.679991070836344, 'IoU-19': 47.920409537935626, 'IoU-20': 48.322604168454376, 'IoU-21': 48.18379154316971, 'IoU-22': 48.14341397885059, 'IoU-23': 47.12611282241925, 'IoU-24': 46.53884295801134, 'IoU-25': 46.949596651070756, 'IoU-26': 44.51265176867883, 'IoU-27': 47.03293235072, 'IoU-28': 45.186466513623515, 'IoU-29': 46.518888427118924, 'IoU-30': 45.02899583658832, 'IoU-31': 46.38703178353257, 'IoU-32': 45.14787640847132, 'IoU-33': 42.60856525362326, 'IoU-34': 41.998211726106675, 'IoU-35': 44.605087704420654, 'IoU-36': 44.618626559423106, 'IoU-37': 42.8499723868582, 'IoU-38': 42.97675586550792, 'IoU-39': 42.675722348409785, 'IoU-40': 43.23458015487886, 'IoU-41': 40.74326747045769, 'IoU-42': 40.086812655719314, 'IoU-43': 40.12642113876769, 'IoU-44': 38.891980305308934, 'IoU-45': 37.69180047659457, 'IoU-46': 37.60620615720816, 'IoU-47': 36.5644298187986, 'IoU-48': 36.455441908091956, 'IoU-49': 36.18343270576824, 'IoU-50': 34.92855408477627, 'IoU-51': 33.665849603883316, 'IoU-52': 32.543658310598204, 'IoU-53': 31.590532672582018, 'IoU-54': 31.63486181447084, 'IoU-55': 31.15887189824139, 'IoU-56': 30.04827598249074, 'IoU-57': 29.469562200410028, 'IoU-58': 28.729043614360606, 'IoU-59': 27.4043694201931, 'IoU-60': 27.058819609682143, 'IoU-61': 26.99942229394917, 'IoU-62': 27.44339195671504, 'IoU-63': 26.1131022975634, 'IoU-64': 25.081901772808123, 'IoU-65': 24.615632198980183, 'IoU-66': 23.403751985025576, 'IoU-67': 22.06555006711472, 'IoU-68': 21.550262848554258, 'IoU-69': 21.70336255651282, 'IoU-70': 21.53112745563014, 'IoU-71': 19.018104450317043, 'IoU-72': 19.77418573293762, 'IoU-73': 20.175774123553065, 'IoU-74': 20.430481559357073, 'IoU-75': 19.972579345906123, 'IoU-76': 19.833559028853216, 'IoU-77': 19.822656169947923, 'IoU-78': 19.161292411127157, 'IoU-79': 18.396915977914947, 'IoU-80': 18.14918187384187, 'IoU-81': 18.97620239585565, 'IoU-82': 17.66859736681962, 'IoU-83': 18.439235938241165, 'IoU-84': 17.67721040169572, 'IoU-85': 18.371428330227022, 'IoU-86': 17.04861111836687, 'IoU-87': 17.401376067245828, 'IoU-88': 17.37575358345566, 'IoU-89': 16.92458183744873, 'IoU-90': 15.917875619295039, 'IoU-91': 16.134589748613706, 'IoU-92': 16.24865885538189, 'IoU-93': 16.47524204127627, 'IoU-94': 16.34326319821873, 'IoU-95': 16.262054466398006, 'IoU-96': 16.062525904766925, 'IoU-97': 15.918329334276471, 'IoU-98': 15.698999730716395, 'IoU-99': 14.918549889577761, 'IoU-100': 15.20883515496012, 'IoU-101': 13.252617643857494, 'IoU-102': 13.611479486574094, 'IoU-103': 12.468792889857202, 'IoU-104': 12.394149004755137, 'IoU-105': 11.650684608441189, 'IoU-106': 12.07433803696281, 'IoU-107': 13.24081946439682, 'IoU-108': 12.685410542099909, 'IoU-109': 13.59714855385553, 'IoU-110': 12.741678131965523, 'IoU-111': 12.19008050835253, 'IoU-112': 11.97898476432968, 'IoU-113': 11.389932028642686, 'IoU-114': 11.81897003508221, 'IoU-115': 11.016307366082698, 'IoU-116': 11.403106886443569, 'IoU-117': 10.613906869476478, 'IoU-118': 10.418419126690333, 'IoU-119': 11.2215532481145, 'IoU-120': 9.865525504360392, 'IoU-121': 10.213057960159762, 'IoU-122': 10.770417095062106, 'IoU-123': 9.216205709556265, 'IoU-124': 10.648412432826126, 'IoU-125': 9.446833440542699, 'IoU-126': 8.196105566910326, 'IoU-127': 9.953172310295027, 'IoU-128': 7.838895238809046, 'IoU-129': 7.295660827067066, 'IoU-130': 6.916393095202869, 'IoU-131': 7.912492151116693, 'IoU-132': 6.313165189142012, 'IoU-133': 8.100553192975212, 'IoU-134': 7.399820378013462, 'IoU-135': 6.144370724362948, 'IoU-136': 5.636870471462299, 'IoU-137': 7.222858316355094, 'IoU-138': 7.288458747843472, 'IoU-139': 6.272577802935056, 'IoU-140': 6.733008054101543, 'IoU-141': 5.593461668026165, 'IoU-142': 7.302789528810055, 'IoU-143': 4.578067879800006, 'IoU-144': 6.1629102282538035, 'IoU-145': 5.168862956960848, 'IoU-146': 5.390310688588329, 'IoU-147': 6.054272652359675, 'IoU-148': 3.072999164369723, 'IoU-149': 3.0535766093936365, 'IoU-150': 4.935410755216988, 'IoU-151': 2.957678202588206, 'IoU-152': 3.5304585253344576, 'IoU-153': 4.900808325622742, 'IoU-154': 2.058344793012747, 'IoU-155': 3.6359981044559646, 'IoU-156': 3.4007249272775124, 'IoU-157': 2.9015017928133346, 'IoU-158': 3.419714655721474, 'IoU-159': 1.8767800033305009, 'IoU-160': 4.460018551736001, 'IoU-161': 3.115648669913228, 'IoU-162': 3.723033601463699, 'IoU-163': 2.65568290901303, 'IoU-164': 1.6872708538283372, 'IoU-165': 2.8092433167195288, 'IoU-166': 3.3056734727262866, 'IoU-167': 1.651382294378925, 'IoU-168': 2.6231654384273484, 'IoU-169': 3.0879255323957584, 'IoU-170': 2.4283763491666894, 'IoU-171': 1.5483351270691237, 'IoU-172': 1.7470863567870358, 'IoU-173': 2.5775139616926652, 'IoU-174': 1.7867376129728636, 'IoU-175': 1.3888754709689886, 'IoU-176': 1.670259935727942, 'IoU-177': 1.834957414508805, 'IoU-178': 1.7668486045389822, 'IoU-179': 1.0901272550480325, 'IoU-180': 2.9134159179729746, 'IoU-181': 1.2461790143975389, 'IoU-182': 0.5393059641441444, 'IoU-183': 1.7462812657063724, 'IoU-184': 1.6521280602636534, 'IoU-185': 2.9899405752530535, 'IoU-186': 1.463240363731672, 'IoU-187': 2.1242052427866587, 'IoU-188': 2.8258781289027812, 'IoU-189': 1.8002122798611366, 'IoU-190': 2.1251274637615687, 'IoU-191': 2.14152833428735, 'mACC': 30.453055575823708, 'pACC': 57.51756409066762, 'ACC-0': nan, 'ACC-1': 98.54467883968576, 'ACC-2': 59.6691139491403, 'ACC-3': 72.37268082569422, 'ACC-4': 67.51870967914962, 'ACC-5': 62.57608779707186, 'ACC-6': 59.207673880763046, 'ACC-7': 49.09243721991898, 'ACC-8': 26.74442878764156, 'ACC-9': 45.987745838421986, 'ACC-10': 56.51601995434091, 'ACC-11': 62.566941218291284, 'ACC-12': 69.89500776873128, 'ACC-13': 70.2529115346841, 'ACC-14': 69.58915703071025, 'ACC-15': 66.66560568272777, 'ACC-16': 68.02324471297766, 'ACC-17': 65.17376700271669, 'ACC-18': 65.93494669478093, 'ACC-19': 66.3094604659775, 'ACC-20': 64.33103746339484, 'ACC-21': 64.72879416810365, 'ACC-22': 65.80476355284692, 'ACC-23': 63.551388708891054, 'ACC-24': 64.16437725375654, 'ACC-25': 63.235497788550745, 'ACC-26': 61.22070925889462, 'ACC-27': 62.260530565050175, 'ACC-28': 64.0378688991792, 'ACC-29': 63.55854443166188, 'ACC-30': 63.43033620769341, 'ACC-31': 62.95067260580465, 'ACC-32': 62.42164712210627, 'ACC-33': 60.328993314997014, 'ACC-34': 59.04407777184946, 'ACC-35': 60.0079851856259, 'ACC-36': 60.84379219322681, 'ACC-37': 60.6381796962432, 'ACC-38': 59.79451722165054, 'ACC-39': 58.79777457499148, 'ACC-40': 58.468991378225525, 'ACC-41': 58.37151834392389, 'ACC-42': 56.207273870980615, 'ACC-43': 56.525832880761705, 'ACC-44': 56.22079571599755, 'ACC-45': 54.798738718209464, 'ACC-46': 54.974059724156945, 'ACC-47': 52.1620883255678, 'ACC-48': 52.316059188931355, 'ACC-49': 52.211601985966674, 'ACC-50': 52.93566258244646, 'ACC-51': 48.775853095332614, 'ACC-52': 48.94621702583999, 'ACC-53': 47.28834058035487, 'ACC-54': 48.54058442722422, 'ACC-55': 49.187982099569105, 'ACC-56': 46.38661297112706, 'ACC-57': 45.62665676328725, 'ACC-58': 45.83193762057503, 'ACC-59': 43.371826842939065, 'ACC-60': 43.593875658456234, 'ACC-61': 44.218277557267406, 'ACC-62': 44.238683848770755, 'ACC-63': 42.15888851013846, 'ACC-64': 39.59562419902646, 'ACC-65': 40.034898092722294, 'ACC-66': 39.11071650359296, 'ACC-67': 36.0968000164207, 'ACC-68': 34.74452392543441, 'ACC-69': 36.63466489228569, 'ACC-70': 34.37557055042681, 'ACC-71': 32.97728053312546, 'ACC-72': 31.639611937133783, 'ACC-73': 31.61809650887995, 'ACC-74': 33.51163204374765, 'ACC-75': 33.30014338146568, 'ACC-76': 32.145870956836916, 'ACC-77': 33.09181918630914, 'ACC-78': 31.644916458748035, 'ACC-79': 30.60150701710478, 'ACC-80': 32.15630563899119, 'ACC-81': 30.529186707840406, 'ACC-82': 32.46755336253083, 'ACC-83': 32.11045779341506, 'ACC-84': 30.057617018371186, 'ACC-85': 31.228263742591793, 'ACC-86': 29.023916347036945, 'ACC-87': 30.203038791647945, 'ACC-88': 29.71464897769683, 'ACC-89': 29.672833820245348, 'ACC-90': 26.411816016057326, 'ACC-91': 26.937968936977757, 'ACC-92': 27.950504514888873, 'ACC-93': 27.55693172759631, 'ACC-94': 29.385571984984832, 'ACC-95': 26.869729183521212, 'ACC-96': 28.61582825537077, 'ACC-97': 27.039620574408186, 'ACC-98': 26.60677112340611, 'ACC-99': 25.572968211371894, 'ACC-100': 27.435536882464135, 'ACC-101': 24.311713445133957, 'ACC-102': 23.757634798368827, 'ACC-103': 21.01745064701993, 'ACC-104': 23.40385135633654, 'ACC-105': 18.42034492718905, 'ACC-106': 20.97399343444157, 'ACC-107': 22.10901226670844, 'ACC-108': 19.900885108363386, 'ACC-109': 22.6698588059528, 'ACC-110': 23.74398893883449, 'ACC-111': 20.69099411729864, 'ACC-112': 23.4146725577379, 'ACC-113': 19.16555264841584, 'ACC-114': 21.50031505136485, 'ACC-115': 20.421496549546706, 'ACC-116': 20.386116137470896, 'ACC-117': 20.53258508414589, 'ACC-118': 17.28570007016006, 'ACC-119': 18.67935186102558, 'ACC-120': 16.71147000636032, 'ACC-121': 19.58419841326437, 'ACC-122': 18.28502552199798, 'ACC-123': 14.505179910355675, 'ACC-124': 19.596494807684827, 'ACC-125': 18.258348443433118, 'ACC-126': 13.7004321532983, 'ACC-127': 18.926679064377762, 'ACC-128': 12.756390788531647, 'ACC-129': 11.409743850086503, 'ACC-130': 13.386285835162765, 'ACC-131': 14.789464834822322, 'ACC-132': 10.471527714945395, 'ACC-133': 14.364585247028245, 'ACC-134': 13.648668677294271, 'ACC-135': 10.574226098854114, 'ACC-136': 11.82779887253075, 'ACC-137': 14.603077200188977, 'ACC-138': 14.342383303297204, 'ACC-139': 12.201240743002886, 'ACC-140': 13.93917709011458, 'ACC-141': 10.046891384002734, 'ACC-142': 15.003480163938864, 'ACC-143': 8.079773339335922, 'ACC-144': 12.569494584837546, 'ACC-145': 10.302019275922985, 'ACC-146': 10.38888761965685, 'ACC-147': 12.402145623468524, 'ACC-148': 4.683993456590983, 'ACC-149': 4.995289376301253, 'ACC-150': 12.074204015180179, 'ACC-151': 4.486812614104699, 'ACC-152': 6.14178720749882, 'ACC-153': 11.123783872918155, 'ACC-154': 3.8471609208763518, 'ACC-155': 6.86579747228611, 'ACC-156': 6.31438784282547, 'ACC-157': 4.355868050026336, 'ACC-158': 6.600230611487387, 'ACC-159': 3.1156521221366256, 'ACC-160': 8.211979143102747, 'ACC-161': 4.665283070796486, 'ACC-162': 6.1996245176301175, 'ACC-163': 4.246010575281235, 'ACC-164': 2.4093176348225356, 'ACC-165': 4.834718750297783, 'ACC-166': 6.038562590761431, 'ACC-167': 2.9966977436608317, 'ACC-168': 5.84095224699346, 'ACC-169': 7.192402784382332, 'ACC-170': 6.96810625097605, 'ACC-171': 2.584453967513395, 'ACC-172': 3.6783716709263987, 'ACC-173': 6.365873574334062, 'ACC-174': 2.894231109769032, 'ACC-175': 2.126353583703564, 'ACC-176': 2.6067666008822843, 'ACC-177': 3.040402754171211, 'ACC-178': 3.233271072084619, 'ACC-179': 1.3033571998109077, 'ACC-180': 6.841579464255478, 'ACC-181': 1.8115452850791487, 'ACC-182': 0.6875327241411172, 'ACC-183': 3.844605427650553, 'ACC-184': 3.8793255930776356, 'ACC-185': 6.165902193644943, 'ACC-186': 2.0691055279228916, 'ACC-187': 3.2433346621727734, 'ACC-188': 7.070689237722448, 'ACC-189': 8.651908369510245, 'ACC-190': 4.313696720710146, 'ACC-191': 7.0308646003262645})])
[01/18 08:43:38] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 08:43:38] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 08:43:38] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 08:43:38] d2.evaluation.testing INFO: copypaste: 2.3456,20.2501,43.2236,30.4531,57.5176
[01/18 08:43:38] d2.utils.events INFO:  eta: 4:00:45  iter: 29999  total_loss: 34  loss_ce: 0.2312  loss_mask: 0.3445  loss_dice: 2.705  loss_ce_0: 0.5744  loss_mask_0: 0.3394  loss_dice_0: 2.835  loss_ce_1: 0.2917  loss_mask_1: 0.3488  loss_dice_1: 2.759  loss_ce_2: 0.2746  loss_mask_2: 0.3484  loss_dice_2: 2.728  loss_ce_3: 0.2722  loss_mask_3: 0.3453  loss_dice_3: 2.723  loss_ce_4: 0.2591  loss_mask_4: 0.3455  loss_dice_4: 2.718  loss_ce_5: 0.2493  loss_mask_5: 0.3458  loss_dice_5: 2.723  loss_ce_6: 0.2413  loss_mask_6: 0.3436  loss_dice_6: 2.716  loss_ce_7: 0.239  loss_mask_7: 0.3439  loss_dice_7: 2.709  loss_ce_8: 0.2413  loss_mask_8: 0.3429  loss_dice_8: 2.714  time: 1.4814  data_time: 0.0722  lr: 2.872e-06  max_mem: 21589M
[01/18 08:44:08] d2.utils.events INFO:  eta: 4:00:22  iter: 30019  total_loss: 32.95  loss_ce: 0.2462  loss_mask: 0.3489  loss_dice: 2.632  loss_ce_0: 0.5485  loss_mask_0: 0.3425  loss_dice_0: 2.763  loss_ce_1: 0.273  loss_mask_1: 0.3524  loss_dice_1: 2.677  loss_ce_2: 0.2821  loss_mask_2: 0.3504  loss_dice_2: 2.649  loss_ce_3: 0.2635  loss_mask_3: 0.3491  loss_dice_3: 2.626  loss_ce_4: 0.2549  loss_mask_4: 0.3481  loss_dice_4: 2.651  loss_ce_5: 0.2662  loss_mask_5: 0.3462  loss_dice_5: 2.636  loss_ce_6: 0.2341  loss_mask_6: 0.3478  loss_dice_6: 2.638  loss_ce_7: 0.2422  loss_mask_7: 0.3484  loss_dice_7: 2.635  loss_ce_8: 0.2412  loss_mask_8: 0.3491  loss_dice_8: 2.624  time: 1.4814  data_time: 0.0717  lr: 2.8668e-06  max_mem: 21589M
[01/18 08:44:36] d2.utils.events INFO:  eta: 3:59:52  iter: 30039  total_loss: 32.25  loss_ce: 0.2317  loss_mask: 0.3398  loss_dice: 2.565  loss_ce_0: 0.5397  loss_mask_0: 0.3359  loss_dice_0: 2.694  loss_ce_1: 0.2608  loss_mask_1: 0.3447  loss_dice_1: 2.609  loss_ce_2: 0.2549  loss_mask_2: 0.3416  loss_dice_2: 2.595  loss_ce_3: 0.2315  loss_mask_3: 0.3408  loss_dice_3: 2.588  loss_ce_4: 0.2229  loss_mask_4: 0.3409  loss_dice_4: 2.586  loss_ce_5: 0.2251  loss_mask_5: 0.3397  loss_dice_5: 2.58  loss_ce_6: 0.2256  loss_mask_6: 0.3401  loss_dice_6: 2.573  loss_ce_7: 0.2145  loss_mask_7: 0.3401  loss_dice_7: 2.571  loss_ce_8: 0.2186  loss_mask_8: 0.3403  loss_dice_8: 2.573  time: 1.4814  data_time: 0.0684  lr: 2.8617e-06  max_mem: 21589M
[01/18 08:45:06] d2.utils.events INFO:  eta: 3:59:29  iter: 30059  total_loss: 32.48  loss_ce: 0.2306  loss_mask: 0.3407  loss_dice: 2.574  loss_ce_0: 0.5488  loss_mask_0: 0.343  loss_dice_0: 2.688  loss_ce_1: 0.286  loss_mask_1: 0.3452  loss_dice_1: 2.604  loss_ce_2: 0.2675  loss_mask_2: 0.342  loss_dice_2: 2.593  loss_ce_3: 0.2504  loss_mask_3: 0.3413  loss_dice_3: 2.577  loss_ce_4: 0.2363  loss_mask_4: 0.3388  loss_dice_4: 2.583  loss_ce_5: 0.2381  loss_mask_5: 0.3384  loss_dice_5: 2.576  loss_ce_6: 0.233  loss_mask_6: 0.3408  loss_dice_6: 2.573  loss_ce_7: 0.227  loss_mask_7: 0.3391  loss_dice_7: 2.577  loss_ce_8: 0.2424  loss_mask_8: 0.3396  loss_dice_8: 2.575  time: 1.4814  data_time: 0.0733  lr: 2.8565e-06  max_mem: 21589M
[01/18 08:45:35] d2.utils.events INFO:  eta: 3:59:11  iter: 30079  total_loss: 32.79  loss_ce: 0.2651  loss_mask: 0.3364  loss_dice: 2.622  loss_ce_0: 0.5717  loss_mask_0: 0.3345  loss_dice_0: 2.755  loss_ce_1: 0.289  loss_mask_1: 0.3398  loss_dice_1: 2.672  loss_ce_2: 0.2907  loss_mask_2: 0.3389  loss_dice_2: 2.642  loss_ce_3: 0.2589  loss_mask_3: 0.3367  loss_dice_3: 2.628  loss_ce_4: 0.2468  loss_mask_4: 0.3359  loss_dice_4: 2.635  loss_ce_5: 0.2516  loss_mask_5: 0.3361  loss_dice_5: 2.622  loss_ce_6: 0.2444  loss_mask_6: 0.3352  loss_dice_6: 2.626  loss_ce_7: 0.2422  loss_mask_7: 0.3359  loss_dice_7: 2.621  loss_ce_8: 0.2475  loss_mask_8: 0.3355  loss_dice_8: 2.632  time: 1.4814  data_time: 0.0798  lr: 2.8513e-06  max_mem: 21589M
[01/18 08:46:05] d2.utils.events INFO:  eta: 3:58:47  iter: 30099  total_loss: 32.25  loss_ce: 0.2287  loss_mask: 0.3401  loss_dice: 2.6  loss_ce_0: 0.5561  loss_mask_0: 0.3379  loss_dice_0: 2.739  loss_ce_1: 0.2877  loss_mask_1: 0.3432  loss_dice_1: 2.64  loss_ce_2: 0.2859  loss_mask_2: 0.3414  loss_dice_2: 2.627  loss_ce_3: 0.2654  loss_mask_3: 0.3428  loss_dice_3: 2.611  loss_ce_4: 0.2482  loss_mask_4: 0.3407  loss_dice_4: 2.603  loss_ce_5: 0.2507  loss_mask_5: 0.3405  loss_dice_5: 2.601  loss_ce_6: 0.2289  loss_mask_6: 0.3411  loss_dice_6: 2.604  loss_ce_7: 0.2248  loss_mask_7: 0.3414  loss_dice_7: 2.598  loss_ce_8: 0.2344  loss_mask_8: 0.3416  loss_dice_8: 2.601  time: 1.4814  data_time: 0.0784  lr: 2.8461e-06  max_mem: 21589M
[01/18 08:46:34] d2.utils.events INFO:  eta: 3:58:21  iter: 30119  total_loss: 32.8  loss_ce: 0.2265  loss_mask: 0.3417  loss_dice: 2.625  loss_ce_0: 0.5465  loss_mask_0: 0.3464  loss_dice_0: 2.756  loss_ce_1: 0.2854  loss_mask_1: 0.3505  loss_dice_1: 2.673  loss_ce_2: 0.271  loss_mask_2: 0.3452  loss_dice_2: 2.653  loss_ce_3: 0.2499  loss_mask_3: 0.3432  loss_dice_3: 2.63  loss_ce_4: 0.251  loss_mask_4: 0.3418  loss_dice_4: 2.635  loss_ce_5: 0.2455  loss_mask_5: 0.3415  loss_dice_5: 2.627  loss_ce_6: 0.2474  loss_mask_6: 0.3416  loss_dice_6: 2.63  loss_ce_7: 0.2338  loss_mask_7: 0.3437  loss_dice_7: 2.623  loss_ce_8: 0.2264  loss_mask_8: 0.3428  loss_dice_8: 2.622  time: 1.4814  data_time: 0.0705  lr: 2.841e-06  max_mem: 21589M
[01/18 08:47:03] d2.utils.events INFO:  eta: 3:57:52  iter: 30139  total_loss: 32.7  loss_ce: 0.2116  loss_mask: 0.3419  loss_dice: 2.659  loss_ce_0: 0.5576  loss_mask_0: 0.3434  loss_dice_0: 2.776  loss_ce_1: 0.2821  loss_mask_1: 0.3507  loss_dice_1: 2.691  loss_ce_2: 0.2534  loss_mask_2: 0.3468  loss_dice_2: 2.678  loss_ce_3: 0.238  loss_mask_3: 0.344  loss_dice_3: 2.663  loss_ce_4: 0.2305  loss_mask_4: 0.3432  loss_dice_4: 2.668  loss_ce_5: 0.2256  loss_mask_5: 0.3431  loss_dice_5: 2.667  loss_ce_6: 0.2156  loss_mask_6: 0.3431  loss_dice_6: 2.653  loss_ce_7: 0.2206  loss_mask_7: 0.342  loss_dice_7: 2.66  loss_ce_8: 0.2124  loss_mask_8: 0.3426  loss_dice_8: 2.66  time: 1.4813  data_time: 0.0697  lr: 2.8358e-06  max_mem: 21589M
[01/18 08:47:32] d2.utils.events INFO:  eta: 3:57:27  iter: 30159  total_loss: 32.28  loss_ce: 0.2502  loss_mask: 0.3455  loss_dice: 2.562  loss_ce_0: 0.5484  loss_mask_0: 0.3434  loss_dice_0: 2.688  loss_ce_1: 0.2863  loss_mask_1: 0.3462  loss_dice_1: 2.594  loss_ce_2: 0.2718  loss_mask_2: 0.344  loss_dice_2: 2.569  loss_ce_3: 0.2701  loss_mask_3: 0.3429  loss_dice_3: 2.567  loss_ce_4: 0.256  loss_mask_4: 0.3432  loss_dice_4: 2.556  loss_ce_5: 0.244  loss_mask_5: 0.3437  loss_dice_5: 2.561  loss_ce_6: 0.2431  loss_mask_6: 0.3445  loss_dice_6: 2.557  loss_ce_7: 0.2408  loss_mask_7: 0.3447  loss_dice_7: 2.558  loss_ce_8: 0.2427  loss_mask_8: 0.3465  loss_dice_8: 2.56  time: 1.4813  data_time: 0.0721  lr: 2.8306e-06  max_mem: 21589M
[01/18 08:48:02] d2.utils.events INFO:  eta: 3:57:03  iter: 30179  total_loss: 32.34  loss_ce: 0.2151  loss_mask: 0.3477  loss_dice: 2.638  loss_ce_0: 0.5423  loss_mask_0: 0.3492  loss_dice_0: 2.754  loss_ce_1: 0.2644  loss_mask_1: 0.3552  loss_dice_1: 2.673  loss_ce_2: 0.2466  loss_mask_2: 0.3521  loss_dice_2: 2.656  loss_ce_3: 0.2372  loss_mask_3: 0.3514  loss_dice_3: 2.643  loss_ce_4: 0.2194  loss_mask_4: 0.3487  loss_dice_4: 2.646  loss_ce_5: 0.2271  loss_mask_5: 0.3483  loss_dice_5: 2.643  loss_ce_6: 0.2124  loss_mask_6: 0.3469  loss_dice_6: 2.642  loss_ce_7: 0.2118  loss_mask_7: 0.3484  loss_dice_7: 2.645  loss_ce_8: 0.216  loss_mask_8: 0.3488  loss_dice_8: 2.635  time: 1.4813  data_time: 0.0718  lr: 2.8254e-06  max_mem: 21589M
[01/18 08:48:30] d2.utils.events INFO:  eta: 3:56:29  iter: 30199  total_loss: 32.73  loss_ce: 0.2342  loss_mask: 0.3409  loss_dice: 2.622  loss_ce_0: 0.5557  loss_mask_0: 0.3394  loss_dice_0: 2.782  loss_ce_1: 0.2813  loss_mask_1: 0.3447  loss_dice_1: 2.676  loss_ce_2: 0.2837  loss_mask_2: 0.3404  loss_dice_2: 2.652  loss_ce_3: 0.2496  loss_mask_3: 0.3414  loss_dice_3: 2.645  loss_ce_4: 0.2501  loss_mask_4: 0.339  loss_dice_4: 2.633  loss_ce_5: 0.2422  loss_mask_5: 0.3392  loss_dice_5: 2.635  loss_ce_6: 0.2455  loss_mask_6: 0.3398  loss_dice_6: 2.623  loss_ce_7: 0.24  loss_mask_7: 0.3398  loss_dice_7: 2.626  loss_ce_8: 0.2284  loss_mask_8: 0.3401  loss_dice_8: 2.626  time: 1.4813  data_time: 0.0646  lr: 2.8203e-06  max_mem: 21589M
[01/18 08:48:59] d2.utils.events INFO:  eta: 3:55:55  iter: 30219  total_loss: 32.17  loss_ce: 0.2375  loss_mask: 0.3345  loss_dice: 2.607  loss_ce_0: 0.5378  loss_mask_0: 0.3342  loss_dice_0: 2.743  loss_ce_1: 0.2762  loss_mask_1: 0.3417  loss_dice_1: 2.644  loss_ce_2: 0.2585  loss_mask_2: 0.3398  loss_dice_2: 2.629  loss_ce_3: 0.2505  loss_mask_3: 0.3387  loss_dice_3: 2.619  loss_ce_4: 0.2231  loss_mask_4: 0.3356  loss_dice_4: 2.61  loss_ce_5: 0.2446  loss_mask_5: 0.3332  loss_dice_5: 2.609  loss_ce_6: 0.2391  loss_mask_6: 0.3342  loss_dice_6: 2.606  loss_ce_7: 0.2342  loss_mask_7: 0.3339  loss_dice_7: 2.617  loss_ce_8: 0.2206  loss_mask_8: 0.3336  loss_dice_8: 2.606  time: 1.4812  data_time: 0.0650  lr: 2.8151e-06  max_mem: 21589M
[01/18 08:49:28] d2.utils.events INFO:  eta: 3:55:25  iter: 30239  total_loss: 32.41  loss_ce: 0.2321  loss_mask: 0.3436  loss_dice: 2.597  loss_ce_0: 0.5599  loss_mask_0: 0.3411  loss_dice_0: 2.744  loss_ce_1: 0.2878  loss_mask_1: 0.3499  loss_dice_1: 2.639  loss_ce_2: 0.2743  loss_mask_2: 0.3444  loss_dice_2: 2.624  loss_ce_3: 0.2521  loss_mask_3: 0.3415  loss_dice_3: 2.607  loss_ce_4: 0.2613  loss_mask_4: 0.3409  loss_dice_4: 2.605  loss_ce_5: 0.2442  loss_mask_5: 0.34  loss_dice_5: 2.606  loss_ce_6: 0.2333  loss_mask_6: 0.3414  loss_dice_6: 2.598  loss_ce_7: 0.2484  loss_mask_7: 0.3413  loss_dice_7: 2.594  loss_ce_8: 0.2434  loss_mask_8: 0.3421  loss_dice_8: 2.597  time: 1.4812  data_time: 0.0690  lr: 2.8099e-06  max_mem: 21589M
[01/18 08:49:57] d2.utils.events INFO:  eta: 3:55:02  iter: 30259  total_loss: 32.65  loss_ce: 0.2334  loss_mask: 0.3475  loss_dice: 2.623  loss_ce_0: 0.5725  loss_mask_0: 0.3432  loss_dice_0: 2.762  loss_ce_1: 0.2968  loss_mask_1: 0.3549  loss_dice_1: 2.665  loss_ce_2: 0.2782  loss_mask_2: 0.3527  loss_dice_2: 2.64  loss_ce_3: 0.2552  loss_mask_3: 0.3495  loss_dice_3: 2.632  loss_ce_4: 0.2497  loss_mask_4: 0.3488  loss_dice_4: 2.627  loss_ce_5: 0.239  loss_mask_5: 0.3465  loss_dice_5: 2.631  loss_ce_6: 0.2294  loss_mask_6: 0.3479  loss_dice_6: 2.627  loss_ce_7: 0.228  loss_mask_7: 0.3469  loss_dice_7: 2.632  loss_ce_8: 0.2341  loss_mask_8: 0.3462  loss_dice_8: 2.629  time: 1.4812  data_time: 0.0724  lr: 2.8047e-06  max_mem: 21589M
[01/18 08:50:26] d2.utils.events INFO:  eta: 3:54:29  iter: 30279  total_loss: 32.4  loss_ce: 0.2311  loss_mask: 0.3352  loss_dice: 2.602  loss_ce_0: 0.5702  loss_mask_0: 0.3391  loss_dice_0: 2.736  loss_ce_1: 0.273  loss_mask_1: 0.3413  loss_dice_1: 2.643  loss_ce_2: 0.2782  loss_mask_2: 0.3389  loss_dice_2: 2.618  loss_ce_3: 0.2543  loss_mask_3: 0.3358  loss_dice_3: 2.611  loss_ce_4: 0.2343  loss_mask_4: 0.3355  loss_dice_4: 2.61  loss_ce_5: 0.2324  loss_mask_5: 0.3344  loss_dice_5: 2.607  loss_ce_6: 0.2335  loss_mask_6: 0.3336  loss_dice_6: 2.599  loss_ce_7: 0.2318  loss_mask_7: 0.3335  loss_dice_7: 2.6  loss_ce_8: 0.2195  loss_mask_8: 0.3338  loss_dice_8: 2.599  time: 1.4812  data_time: 0.0681  lr: 2.7995e-06  max_mem: 21589M
[01/18 08:50:55] d2.utils.events INFO:  eta: 3:53:59  iter: 30299  total_loss: 32.58  loss_ce: 0.2423  loss_mask: 0.3416  loss_dice: 2.607  loss_ce_0: 0.5314  loss_mask_0: 0.3381  loss_dice_0: 2.753  loss_ce_1: 0.2764  loss_mask_1: 0.3453  loss_dice_1: 2.67  loss_ce_2: 0.2777  loss_mask_2: 0.3419  loss_dice_2: 2.646  loss_ce_3: 0.2683  loss_mask_3: 0.3402  loss_dice_3: 2.628  loss_ce_4: 0.2555  loss_mask_4: 0.3402  loss_dice_4: 2.626  loss_ce_5: 0.2449  loss_mask_5: 0.3395  loss_dice_5: 2.615  loss_ce_6: 0.2493  loss_mask_6: 0.3399  loss_dice_6: 2.608  loss_ce_7: 0.2441  loss_mask_7: 0.3406  loss_dice_7: 2.619  loss_ce_8: 0.2448  loss_mask_8: 0.3402  loss_dice_8: 2.608  time: 1.4811  data_time: 0.0718  lr: 2.7944e-06  max_mem: 21589M
[01/18 08:51:24] d2.utils.events INFO:  eta: 3:53:20  iter: 30319  total_loss: 32.39  loss_ce: 0.2509  loss_mask: 0.339  loss_dice: 2.586  loss_ce_0: 0.5291  loss_mask_0: 0.3347  loss_dice_0: 2.716  loss_ce_1: 0.2907  loss_mask_1: 0.3408  loss_dice_1: 2.628  loss_ce_2: 0.2773  loss_mask_2: 0.3395  loss_dice_2: 2.613  loss_ce_3: 0.2586  loss_mask_3: 0.3374  loss_dice_3: 2.589  loss_ce_4: 0.2618  loss_mask_4: 0.3379  loss_dice_4: 2.592  loss_ce_5: 0.2443  loss_mask_5: 0.3373  loss_dice_5: 2.585  loss_ce_6: 0.2432  loss_mask_6: 0.3389  loss_dice_6: 2.583  loss_ce_7: 0.2402  loss_mask_7: 0.3377  loss_dice_7: 2.589  loss_ce_8: 0.2501  loss_mask_8: 0.338  loss_dice_8: 2.579  time: 1.4811  data_time: 0.0679  lr: 2.7892e-06  max_mem: 21589M
[01/18 08:51:53] d2.utils.events INFO:  eta: 3:52:53  iter: 30339  total_loss: 32.61  loss_ce: 0.2614  loss_mask: 0.3388  loss_dice: 2.589  loss_ce_0: 0.5842  loss_mask_0: 0.3309  loss_dice_0: 2.731  loss_ce_1: 0.3165  loss_mask_1: 0.3413  loss_dice_1: 2.639  loss_ce_2: 0.3106  loss_mask_2: 0.3424  loss_dice_2: 2.616  loss_ce_3: 0.2828  loss_mask_3: 0.3403  loss_dice_3: 2.601  loss_ce_4: 0.2843  loss_mask_4: 0.3393  loss_dice_4: 2.595  loss_ce_5: 0.2704  loss_mask_5: 0.3404  loss_dice_5: 2.595  loss_ce_6: 0.2685  loss_mask_6: 0.3407  loss_dice_6: 2.589  loss_ce_7: 0.265  loss_mask_7: 0.3402  loss_dice_7: 2.59  loss_ce_8: 0.259  loss_mask_8: 0.3391  loss_dice_8: 2.59  time: 1.4811  data_time: 0.0711  lr: 2.784e-06  max_mem: 21589M
[01/18 08:52:22] d2.utils.events INFO:  eta: 3:52:22  iter: 30359  total_loss: 32.73  loss_ce: 0.2673  loss_mask: 0.3325  loss_dice: 2.652  loss_ce_0: 0.5393  loss_mask_0: 0.3332  loss_dice_0: 2.753  loss_ce_1: 0.3062  loss_mask_1: 0.3388  loss_dice_1: 2.682  loss_ce_2: 0.2886  loss_mask_2: 0.3361  loss_dice_2: 2.662  loss_ce_3: 0.2897  loss_mask_3: 0.3345  loss_dice_3: 2.655  loss_ce_4: 0.2674  loss_mask_4: 0.3339  loss_dice_4: 2.655  loss_ce_5: 0.2731  loss_mask_5: 0.3342  loss_dice_5: 2.648  loss_ce_6: 0.262  loss_mask_6: 0.3338  loss_dice_6: 2.651  loss_ce_7: 0.2587  loss_mask_7: 0.3334  loss_dice_7: 2.649  loss_ce_8: 0.2595  loss_mask_8: 0.3326  loss_dice_8: 2.644  time: 1.4811  data_time: 0.0688  lr: 2.7788e-06  max_mem: 21589M
[01/18 08:52:51] d2.utils.events INFO:  eta: 3:51:57  iter: 30379  total_loss: 32.51  loss_ce: 0.2272  loss_mask: 0.3441  loss_dice: 2.592  loss_ce_0: 0.5758  loss_mask_0: 0.3457  loss_dice_0: 2.722  loss_ce_1: 0.2988  loss_mask_1: 0.35  loss_dice_1: 2.638  loss_ce_2: 0.2761  loss_mask_2: 0.3489  loss_dice_2: 2.599  loss_ce_3: 0.2656  loss_mask_3: 0.3444  loss_dice_3: 2.589  loss_ce_4: 0.2634  loss_mask_4: 0.3432  loss_dice_4: 2.596  loss_ce_5: 0.2627  loss_mask_5: 0.3426  loss_dice_5: 2.588  loss_ce_6: 0.2459  loss_mask_6: 0.3431  loss_dice_6: 2.587  loss_ce_7: 0.2431  loss_mask_7: 0.3432  loss_dice_7: 2.587  loss_ce_8: 0.2353  loss_mask_8: 0.3424  loss_dice_8: 2.585  time: 1.4811  data_time: 0.0724  lr: 2.7736e-06  max_mem: 21589M
[01/18 08:53:20] d2.utils.events INFO:  eta: 3:51:22  iter: 30399  total_loss: 32.64  loss_ce: 0.244  loss_mask: 0.3312  loss_dice: 2.634  loss_ce_0: 0.5505  loss_mask_0: 0.3368  loss_dice_0: 2.767  loss_ce_1: 0.2958  loss_mask_1: 0.339  loss_dice_1: 2.673  loss_ce_2: 0.2842  loss_mask_2: 0.3339  loss_dice_2: 2.658  loss_ce_3: 0.2664  loss_mask_3: 0.3324  loss_dice_3: 2.649  loss_ce_4: 0.2575  loss_mask_4: 0.3327  loss_dice_4: 2.63  loss_ce_5: 0.2524  loss_mask_5: 0.3312  loss_dice_5: 2.633  loss_ce_6: 0.2533  loss_mask_6: 0.3328  loss_dice_6: 2.631  loss_ce_7: 0.2536  loss_mask_7: 0.3314  loss_dice_7: 2.635  loss_ce_8: 0.2498  loss_mask_8: 0.332  loss_dice_8: 2.628  time: 1.4810  data_time: 0.0690  lr: 2.7684e-06  max_mem: 21589M
[01/18 08:53:49] d2.utils.events INFO:  eta: 3:50:53  iter: 30419  total_loss: 32.69  loss_ce: 0.231  loss_mask: 0.3416  loss_dice: 2.617  loss_ce_0: 0.59  loss_mask_0: 0.3456  loss_dice_0: 2.748  loss_ce_1: 0.2984  loss_mask_1: 0.3516  loss_dice_1: 2.655  loss_ce_2: 0.2809  loss_mask_2: 0.3474  loss_dice_2: 2.637  loss_ce_3: 0.2707  loss_mask_3: 0.348  loss_dice_3: 2.617  loss_ce_4: 0.2572  loss_mask_4: 0.3426  loss_dice_4: 2.617  loss_ce_5: 0.2664  loss_mask_5: 0.342  loss_dice_5: 2.614  loss_ce_6: 0.2423  loss_mask_6: 0.3417  loss_dice_6: 2.614  loss_ce_7: 0.2373  loss_mask_7: 0.3423  loss_dice_7: 2.612  loss_ce_8: 0.2478  loss_mask_8: 0.3421  loss_dice_8: 2.624  time: 1.4810  data_time: 0.0710  lr: 2.7632e-06  max_mem: 21589M
[01/18 08:54:18] d2.utils.events INFO:  eta: 3:50:24  iter: 30439  total_loss: 32.94  loss_ce: 0.248  loss_mask: 0.3357  loss_dice: 2.619  loss_ce_0: 0.5602  loss_mask_0: 0.3326  loss_dice_0: 2.758  loss_ce_1: 0.2811  loss_mask_1: 0.3408  loss_dice_1: 2.66  loss_ce_2: 0.2775  loss_mask_2: 0.3387  loss_dice_2: 2.64  loss_ce_3: 0.2625  loss_mask_3: 0.3373  loss_dice_3: 2.624  loss_ce_4: 0.2501  loss_mask_4: 0.3354  loss_dice_4: 2.624  loss_ce_5: 0.2508  loss_mask_5: 0.3359  loss_dice_5: 2.624  loss_ce_6: 0.2597  loss_mask_6: 0.3357  loss_dice_6: 2.621  loss_ce_7: 0.2476  loss_mask_7: 0.3361  loss_dice_7: 2.617  loss_ce_8: 0.244  loss_mask_8: 0.3371  loss_dice_8: 2.62  time: 1.4810  data_time: 0.0684  lr: 2.758e-06  max_mem: 21589M
[01/18 08:54:46] d2.utils.events INFO:  eta: 3:49:51  iter: 30459  total_loss: 32.45  loss_ce: 0.2342  loss_mask: 0.3415  loss_dice: 2.61  loss_ce_0: 0.5662  loss_mask_0: 0.3375  loss_dice_0: 2.737  loss_ce_1: 0.2843  loss_mask_1: 0.3439  loss_dice_1: 2.652  loss_ce_2: 0.287  loss_mask_2: 0.3443  loss_dice_2: 2.632  loss_ce_3: 0.278  loss_mask_3: 0.3418  loss_dice_3: 2.616  loss_ce_4: 0.2629  loss_mask_4: 0.3427  loss_dice_4: 2.615  loss_ce_5: 0.2479  loss_mask_5: 0.3433  loss_dice_5: 2.613  loss_ce_6: 0.2482  loss_mask_6: 0.3407  loss_dice_6: 2.603  loss_ce_7: 0.2488  loss_mask_7: 0.3405  loss_dice_7: 2.615  loss_ce_8: 0.2406  loss_mask_8: 0.3416  loss_dice_8: 2.603  time: 1.4809  data_time: 0.0710  lr: 2.7528e-06  max_mem: 21589M
[01/18 08:55:15] d2.utils.events INFO:  eta: 3:49:13  iter: 30479  total_loss: 32.79  loss_ce: 0.2206  loss_mask: 0.3468  loss_dice: 2.659  loss_ce_0: 0.56  loss_mask_0: 0.3513  loss_dice_0: 2.784  loss_ce_1: 0.2728  loss_mask_1: 0.3563  loss_dice_1: 2.698  loss_ce_2: 0.2667  loss_mask_2: 0.3531  loss_dice_2: 2.674  loss_ce_3: 0.2304  loss_mask_3: 0.3518  loss_dice_3: 2.664  loss_ce_4: 0.2231  loss_mask_4: 0.3488  loss_dice_4: 2.665  loss_ce_5: 0.2339  loss_mask_5: 0.3495  loss_dice_5: 2.658  loss_ce_6: 0.2152  loss_mask_6: 0.3497  loss_dice_6: 2.66  loss_ce_7: 0.2192  loss_mask_7: 0.3493  loss_dice_7: 2.659  loss_ce_8: 0.2161  loss_mask_8: 0.3484  loss_dice_8: 2.666  time: 1.4809  data_time: 0.0670  lr: 2.7476e-06  max_mem: 21589M
[01/18 08:55:43] d2.utils.events INFO:  eta: 3:48:44  iter: 30499  total_loss: 32.34  loss_ce: 0.2434  loss_mask: 0.3422  loss_dice: 2.583  loss_ce_0: 0.5484  loss_mask_0: 0.3367  loss_dice_0: 2.705  loss_ce_1: 0.282  loss_mask_1: 0.3469  loss_dice_1: 2.622  loss_ce_2: 0.2809  loss_mask_2: 0.3432  loss_dice_2: 2.598  loss_ce_3: 0.2677  loss_mask_3: 0.3426  loss_dice_3: 2.593  loss_ce_4: 0.2585  loss_mask_4: 0.3431  loss_dice_4: 2.582  loss_ce_5: 0.2396  loss_mask_5: 0.3432  loss_dice_5: 2.59  loss_ce_6: 0.2274  loss_mask_6: 0.3436  loss_dice_6: 2.594  loss_ce_7: 0.2365  loss_mask_7: 0.3427  loss_dice_7: 2.573  loss_ce_8: 0.2327  loss_mask_8: 0.3429  loss_dice_8: 2.581  time: 1.4809  data_time: 0.0659  lr: 2.7424e-06  max_mem: 21589M
[01/18 08:56:12] d2.utils.events INFO:  eta: 3:48:13  iter: 30519  total_loss: 32.77  loss_ce: 0.2441  loss_mask: 0.341  loss_dice: 2.6  loss_ce_0: 0.6118  loss_mask_0: 0.3408  loss_dice_0: 2.739  loss_ce_1: 0.2947  loss_mask_1: 0.3444  loss_dice_1: 2.64  loss_ce_2: 0.2877  loss_mask_2: 0.3409  loss_dice_2: 2.622  loss_ce_3: 0.278  loss_mask_3: 0.3402  loss_dice_3: 2.609  loss_ce_4: 0.2701  loss_mask_4: 0.3397  loss_dice_4: 2.602  loss_ce_5: 0.2552  loss_mask_5: 0.3404  loss_dice_5: 2.603  loss_ce_6: 0.2548  loss_mask_6: 0.3399  loss_dice_6: 2.608  loss_ce_7: 0.2396  loss_mask_7: 0.339  loss_dice_7: 2.607  loss_ce_8: 0.2564  loss_mask_8: 0.3391  loss_dice_8: 2.61  time: 1.4808  data_time: 0.0709  lr: 2.7373e-06  max_mem: 21589M
[01/18 08:56:41] d2.utils.events INFO:  eta: 3:47:45  iter: 30539  total_loss: 32.94  loss_ce: 0.2319  loss_mask: 0.3375  loss_dice: 2.639  loss_ce_0: 0.5542  loss_mask_0: 0.3405  loss_dice_0: 2.77  loss_ce_1: 0.2794  loss_mask_1: 0.3428  loss_dice_1: 2.677  loss_ce_2: 0.2732  loss_mask_2: 0.3432  loss_dice_2: 2.655  loss_ce_3: 0.2523  loss_mask_3: 0.3425  loss_dice_3: 2.644  loss_ce_4: 0.2495  loss_mask_4: 0.3395  loss_dice_4: 2.646  loss_ce_5: 0.2435  loss_mask_5: 0.3398  loss_dice_5: 2.649  loss_ce_6: 0.2306  loss_mask_6: 0.3383  loss_dice_6: 2.646  loss_ce_7: 0.2468  loss_mask_7: 0.3398  loss_dice_7: 2.654  loss_ce_8: 0.2219  loss_mask_8: 0.337  loss_dice_8: 2.641  time: 1.4808  data_time: 0.0738  lr: 2.7321e-06  max_mem: 21589M
[01/18 08:57:10] d2.utils.events INFO:  eta: 3:47:16  iter: 30559  total_loss: 32.46  loss_ce: 0.2415  loss_mask: 0.3403  loss_dice: 2.58  loss_ce_0: 0.5573  loss_mask_0: 0.3336  loss_dice_0: 2.736  loss_ce_1: 0.2799  loss_mask_1: 0.3402  loss_dice_1: 2.63  loss_ce_2: 0.2811  loss_mask_2: 0.3387  loss_dice_2: 2.602  loss_ce_3: 0.2646  loss_mask_3: 0.338  loss_dice_3: 2.594  loss_ce_4: 0.2474  loss_mask_4: 0.3376  loss_dice_4: 2.596  loss_ce_5: 0.2591  loss_mask_5: 0.3383  loss_dice_5: 2.589  loss_ce_6: 0.2469  loss_mask_6: 0.3395  loss_dice_6: 2.596  loss_ce_7: 0.2389  loss_mask_7: 0.3389  loss_dice_7: 2.583  loss_ce_8: 0.2479  loss_mask_8: 0.3398  loss_dice_8: 2.593  time: 1.4808  data_time: 0.0707  lr: 2.7269e-06  max_mem: 21589M
[01/18 08:57:39] d2.utils.events INFO:  eta: 3:46:47  iter: 30579  total_loss: 33.72  loss_ce: 0.2404  loss_mask: 0.3436  loss_dice: 2.687  loss_ce_0: 0.5544  loss_mask_0: 0.3525  loss_dice_0: 2.81  loss_ce_1: 0.2779  loss_mask_1: 0.357  loss_dice_1: 2.741  loss_ce_2: 0.29  loss_mask_2: 0.3521  loss_dice_2: 2.722  loss_ce_3: 0.2725  loss_mask_3: 0.3468  loss_dice_3: 2.704  loss_ce_4: 0.2612  loss_mask_4: 0.3462  loss_dice_4: 2.704  loss_ce_5: 0.2591  loss_mask_5: 0.3428  loss_dice_5: 2.701  loss_ce_6: 0.2476  loss_mask_6: 0.3439  loss_dice_6: 2.694  loss_ce_7: 0.2509  loss_mask_7: 0.3439  loss_dice_7: 2.701  loss_ce_8: 0.2383  loss_mask_8: 0.3444  loss_dice_8: 2.693  time: 1.4808  data_time: 0.0670  lr: 2.7217e-06  max_mem: 21589M
[01/18 08:58:08] d2.utils.events INFO:  eta: 3:46:17  iter: 30599  total_loss: 33.12  loss_ce: 0.237  loss_mask: 0.3398  loss_dice: 2.646  loss_ce_0: 0.5861  loss_mask_0: 0.3339  loss_dice_0: 2.789  loss_ce_1: 0.2781  loss_mask_1: 0.3431  loss_dice_1: 2.699  loss_ce_2: 0.2646  loss_mask_2: 0.3392  loss_dice_2: 2.672  loss_ce_3: 0.2506  loss_mask_3: 0.3394  loss_dice_3: 2.657  loss_ce_4: 0.2541  loss_mask_4: 0.3382  loss_dice_4: 2.65  loss_ce_5: 0.2529  loss_mask_5: 0.3385  loss_dice_5: 2.653  loss_ce_6: 0.2432  loss_mask_6: 0.339  loss_dice_6: 2.652  loss_ce_7: 0.2434  loss_mask_7: 0.3389  loss_dice_7: 2.656  loss_ce_8: 0.2459  loss_mask_8: 0.3388  loss_dice_8: 2.649  time: 1.4808  data_time: 0.0721  lr: 2.7165e-06  max_mem: 21589M
[01/18 08:58:37] d2.utils.events INFO:  eta: 3:45:50  iter: 30619  total_loss: 32.82  loss_ce: 0.2297  loss_mask: 0.3426  loss_dice: 2.632  loss_ce_0: 0.55  loss_mask_0: 0.3418  loss_dice_0: 2.767  loss_ce_1: 0.2878  loss_mask_1: 0.3471  loss_dice_1: 2.682  loss_ce_2: 0.2661  loss_mask_2: 0.3436  loss_dice_2: 2.661  loss_ce_3: 0.2513  loss_mask_3: 0.3436  loss_dice_3: 2.642  loss_ce_4: 0.2506  loss_mask_4: 0.3439  loss_dice_4: 2.64  loss_ce_5: 0.2351  loss_mask_5: 0.3423  loss_dice_5: 2.645  loss_ce_6: 0.2262  loss_mask_6: 0.3435  loss_dice_6: 2.636  loss_ce_7: 0.229  loss_mask_7: 0.3449  loss_dice_7: 2.633  loss_ce_8: 0.2308  loss_mask_8: 0.3436  loss_dice_8: 2.638  time: 1.4807  data_time: 0.0718  lr: 2.7113e-06  max_mem: 21589M
[01/18 08:59:07] d2.utils.events INFO:  eta: 3:45:25  iter: 30639  total_loss: 33.18  loss_ce: 0.24  loss_mask: 0.3236  loss_dice: 2.676  loss_ce_0: 0.5453  loss_mask_0: 0.3286  loss_dice_0: 2.83  loss_ce_1: 0.2811  loss_mask_1: 0.3345  loss_dice_1: 2.729  loss_ce_2: 0.2709  loss_mask_2: 0.3312  loss_dice_2: 2.703  loss_ce_3: 0.2567  loss_mask_3: 0.3278  loss_dice_3: 2.676  loss_ce_4: 0.2525  loss_mask_4: 0.3277  loss_dice_4: 2.682  loss_ce_5: 0.2438  loss_mask_5: 0.3268  loss_dice_5: 2.681  loss_ce_6: 0.2362  loss_mask_6: 0.3257  loss_dice_6: 2.677  loss_ce_7: 0.2415  loss_mask_7: 0.326  loss_dice_7: 2.686  loss_ce_8: 0.2386  loss_mask_8: 0.3253  loss_dice_8: 2.678  time: 1.4807  data_time: 0.0691  lr: 2.7061e-06  max_mem: 21589M
[01/18 08:59:36] d2.utils.events INFO:  eta: 3:45:05  iter: 30659  total_loss: 33.5  loss_ce: 0.2678  loss_mask: 0.3429  loss_dice: 2.657  loss_ce_0: 0.5979  loss_mask_0: 0.3389  loss_dice_0: 2.783  loss_ce_1: 0.289  loss_mask_1: 0.3478  loss_dice_1: 2.701  loss_ce_2: 0.2927  loss_mask_2: 0.3431  loss_dice_2: 2.675  loss_ce_3: 0.2743  loss_mask_3: 0.3433  loss_dice_3: 2.661  loss_ce_4: 0.2583  loss_mask_4: 0.3421  loss_dice_4: 2.673  loss_ce_5: 0.259  loss_mask_5: 0.3419  loss_dice_5: 2.659  loss_ce_6: 0.2592  loss_mask_6: 0.3413  loss_dice_6: 2.663  loss_ce_7: 0.2681  loss_mask_7: 0.3424  loss_dice_7: 2.666  loss_ce_8: 0.2586  loss_mask_8: 0.3441  loss_dice_8: 2.665  time: 1.4807  data_time: 0.0725  lr: 2.7008e-06  max_mem: 21589M
[01/18 09:00:04] d2.utils.events INFO:  eta: 3:44:29  iter: 30679  total_loss: 32.59  loss_ce: 0.2604  loss_mask: 0.3389  loss_dice: 2.612  loss_ce_0: 0.5699  loss_mask_0: 0.3397  loss_dice_0: 2.733  loss_ce_1: 0.308  loss_mask_1: 0.3461  loss_dice_1: 2.661  loss_ce_2: 0.2938  loss_mask_2: 0.3455  loss_dice_2: 2.635  loss_ce_3: 0.2732  loss_mask_3: 0.343  loss_dice_3: 2.61  loss_ce_4: 0.2683  loss_mask_4: 0.3411  loss_dice_4: 2.615  loss_ce_5: 0.2585  loss_mask_5: 0.3391  loss_dice_5: 2.614  loss_ce_6: 0.2543  loss_mask_6: 0.3384  loss_dice_6: 2.608  loss_ce_7: 0.2457  loss_mask_7: 0.339  loss_dice_7: 2.602  loss_ce_8: 0.2562  loss_mask_8: 0.3389  loss_dice_8: 2.611  time: 1.4807  data_time: 0.0721  lr: 2.6956e-06  max_mem: 21589M
[01/18 09:00:33] d2.utils.events INFO:  eta: 3:43:55  iter: 30699  total_loss: 32.83  loss_ce: 0.2363  loss_mask: 0.3464  loss_dice: 2.618  loss_ce_0: 0.5589  loss_mask_0: 0.3472  loss_dice_0: 2.755  loss_ce_1: 0.296  loss_mask_1: 0.3517  loss_dice_1: 2.661  loss_ce_2: 0.2698  loss_mask_2: 0.3482  loss_dice_2: 2.641  loss_ce_3: 0.2503  loss_mask_3: 0.3448  loss_dice_3: 2.628  loss_ce_4: 0.2525  loss_mask_4: 0.3443  loss_dice_4: 2.625  loss_ce_5: 0.2491  loss_mask_5: 0.3451  loss_dice_5: 2.628  loss_ce_6: 0.2449  loss_mask_6: 0.3455  loss_dice_6: 2.623  loss_ce_7: 0.2443  loss_mask_7: 0.3452  loss_dice_7: 2.625  loss_ce_8: 0.2396  loss_mask_8: 0.3453  loss_dice_8: 2.623  time: 1.4806  data_time: 0.0698  lr: 2.6904e-06  max_mem: 21589M
[01/18 09:01:02] d2.utils.events INFO:  eta: 3:43:26  iter: 30719  total_loss: 32.83  loss_ce: 0.2568  loss_mask: 0.3345  loss_dice: 2.638  loss_ce_0: 0.577  loss_mask_0: 0.3341  loss_dice_0: 2.771  loss_ce_1: 0.3081  loss_mask_1: 0.338  loss_dice_1: 2.688  loss_ce_2: 0.2874  loss_mask_2: 0.3358  loss_dice_2: 2.669  loss_ce_3: 0.2619  loss_mask_3: 0.3351  loss_dice_3: 2.65  loss_ce_4: 0.2433  loss_mask_4: 0.3337  loss_dice_4: 2.649  loss_ce_5: 0.2434  loss_mask_5: 0.3344  loss_dice_5: 2.641  loss_ce_6: 0.2457  loss_mask_6: 0.3345  loss_dice_6: 2.641  loss_ce_7: 0.2575  loss_mask_7: 0.3342  loss_dice_7: 2.64  loss_ce_8: 0.2558  loss_mask_8: 0.3354  loss_dice_8: 2.641  time: 1.4806  data_time: 0.0728  lr: 2.6852e-06  max_mem: 21589M
[01/18 09:01:31] d2.utils.events INFO:  eta: 3:42:59  iter: 30739  total_loss: 32.31  loss_ce: 0.235  loss_mask: 0.3401  loss_dice: 2.603  loss_ce_0: 0.5611  loss_mask_0: 0.3371  loss_dice_0: 2.722  loss_ce_1: 0.3029  loss_mask_1: 0.3459  loss_dice_1: 2.636  loss_ce_2: 0.2802  loss_mask_2: 0.3422  loss_dice_2: 2.618  loss_ce_3: 0.2645  loss_mask_3: 0.3439  loss_dice_3: 2.602  loss_ce_4: 0.2432  loss_mask_4: 0.3427  loss_dice_4: 2.612  loss_ce_5: 0.258  loss_mask_5: 0.3406  loss_dice_5: 2.601  loss_ce_6: 0.2442  loss_mask_6: 0.3415  loss_dice_6: 2.604  loss_ce_7: 0.2491  loss_mask_7: 0.3392  loss_dice_7: 2.608  loss_ce_8: 0.2445  loss_mask_8: 0.3396  loss_dice_8: 2.593  time: 1.4806  data_time: 0.0696  lr: 2.68e-06  max_mem: 21589M
[01/18 09:02:00] d2.utils.events INFO:  eta: 3:42:33  iter: 30759  total_loss: 32.78  loss_ce: 0.2416  loss_mask: 0.3404  loss_dice: 2.618  loss_ce_0: 0.5678  loss_mask_0: 0.347  loss_dice_0: 2.741  loss_ce_1: 0.3082  loss_mask_1: 0.3458  loss_dice_1: 2.653  loss_ce_2: 0.283  loss_mask_2: 0.3423  loss_dice_2: 2.631  loss_ce_3: 0.2742  loss_mask_3: 0.342  loss_dice_3: 2.623  loss_ce_4: 0.2636  loss_mask_4: 0.3425  loss_dice_4: 2.619  loss_ce_5: 0.262  loss_mask_5: 0.3412  loss_dice_5: 2.617  loss_ce_6: 0.2517  loss_mask_6: 0.3422  loss_dice_6: 2.616  loss_ce_7: 0.2666  loss_mask_7: 0.3423  loss_dice_7: 2.614  loss_ce_8: 0.2489  loss_mask_8: 0.3418  loss_dice_8: 2.622  time: 1.4806  data_time: 0.0709  lr: 2.6748e-06  max_mem: 21589M
[01/18 09:02:30] d2.utils.events INFO:  eta: 3:42:05  iter: 30779  total_loss: 33.51  loss_ce: 0.2278  loss_mask: 0.342  loss_dice: 2.695  loss_ce_0: 0.5358  loss_mask_0: 0.3394  loss_dice_0: 2.818  loss_ce_1: 0.262  loss_mask_1: 0.3495  loss_dice_1: 2.737  loss_ce_2: 0.2534  loss_mask_2: 0.3456  loss_dice_2: 2.72  loss_ce_3: 0.2533  loss_mask_3: 0.3434  loss_dice_3: 2.702  loss_ce_4: 0.2343  loss_mask_4: 0.3422  loss_dice_4: 2.711  loss_ce_5: 0.2366  loss_mask_5: 0.3421  loss_dice_5: 2.7  loss_ce_6: 0.2317  loss_mask_6: 0.3425  loss_dice_6: 2.701  loss_ce_7: 0.2216  loss_mask_7: 0.3427  loss_dice_7: 2.703  loss_ce_8: 0.2258  loss_mask_8: 0.3426  loss_dice_8: 2.702  time: 1.4806  data_time: 0.0688  lr: 2.6696e-06  max_mem: 21589M
[01/18 09:02:59] d2.utils.events INFO:  eta: 3:41:36  iter: 30799  total_loss: 32.45  loss_ce: 0.2349  loss_mask: 0.3371  loss_dice: 2.598  loss_ce_0: 0.5073  loss_mask_0: 0.3341  loss_dice_0: 2.733  loss_ce_1: 0.2753  loss_mask_1: 0.3434  loss_dice_1: 2.639  loss_ce_2: 0.2735  loss_mask_2: 0.3408  loss_dice_2: 2.622  loss_ce_3: 0.2612  loss_mask_3: 0.3375  loss_dice_3: 2.596  loss_ce_4: 0.2424  loss_mask_4: 0.337  loss_dice_4: 2.599  loss_ce_5: 0.237  loss_mask_5: 0.3365  loss_dice_5: 2.604  loss_ce_6: 0.231  loss_mask_6: 0.3362  loss_dice_6: 2.589  loss_ce_7: 0.2261  loss_mask_7: 0.3379  loss_dice_7: 2.594  loss_ce_8: 0.2395  loss_mask_8: 0.3377  loss_dice_8: 2.596  time: 1.4805  data_time: 0.0695  lr: 2.6644e-06  max_mem: 21589M
[01/18 09:03:28] d2.utils.events INFO:  eta: 3:41:07  iter: 30819  total_loss: 33  loss_ce: 0.2188  loss_mask: 0.3288  loss_dice: 2.674  loss_ce_0: 0.5705  loss_mask_0: 0.3302  loss_dice_0: 2.806  loss_ce_1: 0.2716  loss_mask_1: 0.3328  loss_dice_1: 2.717  loss_ce_2: 0.2495  loss_mask_2: 0.3311  loss_dice_2: 2.699  loss_ce_3: 0.2508  loss_mask_3: 0.3305  loss_dice_3: 2.683  loss_ce_4: 0.2375  loss_mask_4: 0.3298  loss_dice_4: 2.668  loss_ce_5: 0.2256  loss_mask_5: 0.3291  loss_dice_5: 2.668  loss_ce_6: 0.2287  loss_mask_6: 0.3297  loss_dice_6: 2.677  loss_ce_7: 0.229  loss_mask_7: 0.3296  loss_dice_7: 2.67  loss_ce_8: 0.2208  loss_mask_8: 0.3286  loss_dice_8: 2.669  time: 1.4805  data_time: 0.0752  lr: 2.6592e-06  max_mem: 21589M
[01/18 09:03:57] d2.utils.events INFO:  eta: 3:40:33  iter: 30839  total_loss: 32.27  loss_ce: 0.2394  loss_mask: 0.3384  loss_dice: 2.566  loss_ce_0: 0.5375  loss_mask_0: 0.3337  loss_dice_0: 2.693  loss_ce_1: 0.2869  loss_mask_1: 0.3408  loss_dice_1: 2.595  loss_ce_2: 0.2602  loss_mask_2: 0.3394  loss_dice_2: 2.58  loss_ce_3: 0.2332  loss_mask_3: 0.3376  loss_dice_3: 2.564  loss_ce_4: 0.239  loss_mask_4: 0.3368  loss_dice_4: 2.568  loss_ce_5: 0.2402  loss_mask_5: 0.337  loss_dice_5: 2.563  loss_ce_6: 0.2345  loss_mask_6: 0.3358  loss_dice_6: 2.555  loss_ce_7: 0.2349  loss_mask_7: 0.3375  loss_dice_7: 2.565  loss_ce_8: 0.2191  loss_mask_8: 0.3372  loss_dice_8: 2.569  time: 1.4805  data_time: 0.0688  lr: 2.654e-06  max_mem: 21589M
[01/18 09:04:26] d2.utils.events INFO:  eta: 3:40:07  iter: 30859  total_loss: 31.75  loss_ce: 0.2185  loss_mask: 0.3325  loss_dice: 2.531  loss_ce_0: 0.5631  loss_mask_0: 0.3305  loss_dice_0: 2.658  loss_ce_1: 0.2821  loss_mask_1: 0.3357  loss_dice_1: 2.57  loss_ce_2: 0.2564  loss_mask_2: 0.3337  loss_dice_2: 2.553  loss_ce_3: 0.2419  loss_mask_3: 0.3335  loss_dice_3: 2.534  loss_ce_4: 0.246  loss_mask_4: 0.3333  loss_dice_4: 2.535  loss_ce_5: 0.2167  loss_mask_5: 0.3341  loss_dice_5: 2.536  loss_ce_6: 0.234  loss_mask_6: 0.3325  loss_dice_6: 2.533  loss_ce_7: 0.227  loss_mask_7: 0.3333  loss_dice_7: 2.53  loss_ce_8: 0.2271  loss_mask_8: 0.3329  loss_dice_8: 2.531  time: 1.4805  data_time: 0.0675  lr: 2.6487e-06  max_mem: 21589M
[01/18 09:04:55] d2.utils.events INFO:  eta: 3:39:44  iter: 30879  total_loss: 32.27  loss_ce: 0.2373  loss_mask: 0.3321  loss_dice: 2.574  loss_ce_0: 0.5653  loss_mask_0: 0.3358  loss_dice_0: 2.713  loss_ce_1: 0.2733  loss_mask_1: 0.3379  loss_dice_1: 2.621  loss_ce_2: 0.2465  loss_mask_2: 0.3334  loss_dice_2: 2.597  loss_ce_3: 0.2578  loss_mask_3: 0.3315  loss_dice_3: 2.589  loss_ce_4: 0.2426  loss_mask_4: 0.3321  loss_dice_4: 2.582  loss_ce_5: 0.2329  loss_mask_5: 0.3309  loss_dice_5: 2.576  loss_ce_6: 0.2339  loss_mask_6: 0.3308  loss_dice_6: 2.58  loss_ce_7: 0.226  loss_mask_7: 0.3311  loss_dice_7: 2.578  loss_ce_8: 0.2387  loss_mask_8: 0.3321  loss_dice_8: 2.578  time: 1.4805  data_time: 0.0683  lr: 2.6435e-06  max_mem: 21589M
[01/18 09:05:23] d2.utils.events INFO:  eta: 3:39:17  iter: 30899  total_loss: 32.47  loss_ce: 0.2407  loss_mask: 0.3305  loss_dice: 2.6  loss_ce_0: 0.5517  loss_mask_0: 0.3276  loss_dice_0: 2.731  loss_ce_1: 0.2869  loss_mask_1: 0.3372  loss_dice_1: 2.637  loss_ce_2: 0.2958  loss_mask_2: 0.3346  loss_dice_2: 2.626  loss_ce_3: 0.2633  loss_mask_3: 0.3314  loss_dice_3: 2.61  loss_ce_4: 0.25  loss_mask_4: 0.3308  loss_dice_4: 2.603  loss_ce_5: 0.2549  loss_mask_5: 0.3295  loss_dice_5: 2.6  loss_ce_6: 0.2486  loss_mask_6: 0.3308  loss_dice_6: 2.594  loss_ce_7: 0.2397  loss_mask_7: 0.3295  loss_dice_7: 2.591  loss_ce_8: 0.2475  loss_mask_8: 0.3312  loss_dice_8: 2.605  time: 1.4804  data_time: 0.0736  lr: 2.6383e-06  max_mem: 21589M
[01/18 09:05:52] d2.utils.events INFO:  eta: 3:38:44  iter: 30919  total_loss: 32.24  loss_ce: 0.236  loss_mask: 0.3402  loss_dice: 2.603  loss_ce_0: 0.5413  loss_mask_0: 0.3386  loss_dice_0: 2.741  loss_ce_1: 0.2869  loss_mask_1: 0.343  loss_dice_1: 2.641  loss_ce_2: 0.2711  loss_mask_2: 0.3409  loss_dice_2: 2.622  loss_ce_3: 0.2663  loss_mask_3: 0.3393  loss_dice_3: 2.623  loss_ce_4: 0.2516  loss_mask_4: 0.3389  loss_dice_4: 2.619  loss_ce_5: 0.2473  loss_mask_5: 0.3398  loss_dice_5: 2.613  loss_ce_6: 0.249  loss_mask_6: 0.3401  loss_dice_6: 2.617  loss_ce_7: 0.2393  loss_mask_7: 0.3395  loss_dice_7: 2.61  loss_ce_8: 0.2443  loss_mask_8: 0.3404  loss_dice_8: 2.605  time: 1.4804  data_time: 0.0694  lr: 2.6331e-06  max_mem: 21589M
[01/18 09:06:21] d2.utils.events INFO:  eta: 3:38:07  iter: 30939  total_loss: 32.46  loss_ce: 0.2318  loss_mask: 0.3352  loss_dice: 2.626  loss_ce_0: 0.5521  loss_mask_0: 0.3378  loss_dice_0: 2.751  loss_ce_1: 0.2805  loss_mask_1: 0.3391  loss_dice_1: 2.659  loss_ce_2: 0.2729  loss_mask_2: 0.3356  loss_dice_2: 2.639  loss_ce_3: 0.2651  loss_mask_3: 0.3354  loss_dice_3: 2.633  loss_ce_4: 0.258  loss_mask_4: 0.3346  loss_dice_4: 2.624  loss_ce_5: 0.2335  loss_mask_5: 0.333  loss_dice_5: 2.628  loss_ce_6: 0.2258  loss_mask_6: 0.3338  loss_dice_6: 2.621  loss_ce_7: 0.2391  loss_mask_7: 0.3349  loss_dice_7: 2.621  loss_ce_8: 0.2332  loss_mask_8: 0.3359  loss_dice_8: 2.614  time: 1.4804  data_time: 0.0698  lr: 2.6279e-06  max_mem: 21589M
[01/18 09:06:50] d2.utils.events INFO:  eta: 3:37:42  iter: 30959  total_loss: 33.07  loss_ce: 0.2247  loss_mask: 0.3367  loss_dice: 2.641  loss_ce_0: 0.5755  loss_mask_0: 0.3493  loss_dice_0: 2.774  loss_ce_1: 0.2673  loss_mask_1: 0.3482  loss_dice_1: 2.692  loss_ce_2: 0.2739  loss_mask_2: 0.3442  loss_dice_2: 2.653  loss_ce_3: 0.2392  loss_mask_3: 0.3397  loss_dice_3: 2.649  loss_ce_4: 0.2404  loss_mask_4: 0.339  loss_dice_4: 2.643  loss_ce_5: 0.2228  loss_mask_5: 0.3365  loss_dice_5: 2.638  loss_ce_6: 0.2239  loss_mask_6: 0.3381  loss_dice_6: 2.636  loss_ce_7: 0.2238  loss_mask_7: 0.3368  loss_dice_7: 2.646  loss_ce_8: 0.2219  loss_mask_8: 0.3358  loss_dice_8: 2.633  time: 1.4804  data_time: 0.0727  lr: 2.6227e-06  max_mem: 21589M
[01/18 09:07:19] d2.utils.events INFO:  eta: 3:37:03  iter: 30979  total_loss: 33.17  loss_ce: 0.253  loss_mask: 0.3473  loss_dice: 2.66  loss_ce_0: 0.5591  loss_mask_0: 0.3434  loss_dice_0: 2.806  loss_ce_1: 0.2904  loss_mask_1: 0.3535  loss_dice_1: 2.717  loss_ce_2: 0.2869  loss_mask_2: 0.3517  loss_dice_2: 2.69  loss_ce_3: 0.2797  loss_mask_3: 0.3473  loss_dice_3: 2.668  loss_ce_4: 0.2669  loss_mask_4: 0.3458  loss_dice_4: 2.673  loss_ce_5: 0.2534  loss_mask_5: 0.3466  loss_dice_5: 2.673  loss_ce_6: 0.253  loss_mask_6: 0.3465  loss_dice_6: 2.665  loss_ce_7: 0.2545  loss_mask_7: 0.346  loss_dice_7: 2.67  loss_ce_8: 0.245  loss_mask_8: 0.348  loss_dice_8: 2.67  time: 1.4803  data_time: 0.0696  lr: 2.6174e-06  max_mem: 21589M
[01/18 09:07:48] d2.utils.events INFO:  eta: 3:36:27  iter: 30999  total_loss: 32.31  loss_ce: 0.2432  loss_mask: 0.334  loss_dice: 2.597  loss_ce_0: 0.5687  loss_mask_0: 0.3293  loss_dice_0: 2.756  loss_ce_1: 0.2883  loss_mask_1: 0.3394  loss_dice_1: 2.658  loss_ce_2: 0.2792  loss_mask_2: 0.3391  loss_dice_2: 2.63  loss_ce_3: 0.2568  loss_mask_3: 0.3394  loss_dice_3: 2.611  loss_ce_4: 0.255  loss_mask_4: 0.3375  loss_dice_4: 2.593  loss_ce_5: 0.2507  loss_mask_5: 0.3365  loss_dice_5: 2.605  loss_ce_6: 0.2426  loss_mask_6: 0.3355  loss_dice_6: 2.602  loss_ce_7: 0.2489  loss_mask_7: 0.3346  loss_dice_7: 2.604  loss_ce_8: 0.2344  loss_mask_8: 0.3351  loss_dice_8: 2.606  time: 1.4803  data_time: 0.0691  lr: 2.6122e-06  max_mem: 21589M
[01/18 09:08:17] d2.utils.events INFO:  eta: 3:35:56  iter: 31019  total_loss: 33.16  loss_ce: 0.2353  loss_mask: 0.3329  loss_dice: 2.619  loss_ce_0: 0.5882  loss_mask_0: 0.331  loss_dice_0: 2.739  loss_ce_1: 0.2753  loss_mask_1: 0.3397  loss_dice_1: 2.651  loss_ce_2: 0.2763  loss_mask_2: 0.3356  loss_dice_2: 2.635  loss_ce_3: 0.2522  loss_mask_3: 0.3348  loss_dice_3: 2.622  loss_ce_4: 0.2337  loss_mask_4: 0.3359  loss_dice_4: 2.618  loss_ce_5: 0.2338  loss_mask_5: 0.3349  loss_dice_5: 2.624  loss_ce_6: 0.2492  loss_mask_6: 0.3368  loss_dice_6: 2.611  loss_ce_7: 0.2251  loss_mask_7: 0.3349  loss_dice_7: 2.609  loss_ce_8: 0.2345  loss_mask_8: 0.3347  loss_dice_8: 2.607  time: 1.4803  data_time: 0.0693  lr: 2.607e-06  max_mem: 21589M
[01/18 09:08:45] d2.utils.events INFO:  eta: 3:35:26  iter: 31039  total_loss: 32.07  loss_ce: 0.2348  loss_mask: 0.3377  loss_dice: 2.582  loss_ce_0: 0.5848  loss_mask_0: 0.3362  loss_dice_0: 2.729  loss_ce_1: 0.2754  loss_mask_1: 0.3411  loss_dice_1: 2.642  loss_ce_2: 0.2679  loss_mask_2: 0.3401  loss_dice_2: 2.608  loss_ce_3: 0.2449  loss_mask_3: 0.3387  loss_dice_3: 2.593  loss_ce_4: 0.2536  loss_mask_4: 0.3382  loss_dice_4: 2.594  loss_ce_5: 0.2339  loss_mask_5: 0.3369  loss_dice_5: 2.588  loss_ce_6: 0.2414  loss_mask_6: 0.3377  loss_dice_6: 2.589  loss_ce_7: 0.2286  loss_mask_7: 0.3382  loss_dice_7: 2.59  loss_ce_8: 0.229  loss_mask_8: 0.3376  loss_dice_8: 2.587  time: 1.4802  data_time: 0.0724  lr: 2.6018e-06  max_mem: 21589M
[01/18 09:09:14] d2.utils.events INFO:  eta: 3:34:53  iter: 31059  total_loss: 32.19  loss_ce: 0.2425  loss_mask: 0.3468  loss_dice: 2.598  loss_ce_0: 0.5433  loss_mask_0: 0.3422  loss_dice_0: 2.725  loss_ce_1: 0.2855  loss_mask_1: 0.3492  loss_dice_1: 2.641  loss_ce_2: 0.2818  loss_mask_2: 0.3486  loss_dice_2: 2.608  loss_ce_3: 0.2574  loss_mask_3: 0.344  loss_dice_3: 2.598  loss_ce_4: 0.2577  loss_mask_4: 0.3436  loss_dice_4: 2.603  loss_ce_5: 0.2385  loss_mask_5: 0.3457  loss_dice_5: 2.601  loss_ce_6: 0.2394  loss_mask_6: 0.3446  loss_dice_6: 2.599  loss_ce_7: 0.2301  loss_mask_7: 0.345  loss_dice_7: 2.6  loss_ce_8: 0.2394  loss_mask_8: 0.3458  loss_dice_8: 2.605  time: 1.4802  data_time: 0.0703  lr: 2.5965e-06  max_mem: 21589M
[01/18 09:09:43] d2.utils.events INFO:  eta: 3:33:54  iter: 31079  total_loss: 31.9  loss_ce: 0.2462  loss_mask: 0.3399  loss_dice: 2.539  loss_ce_0: 0.554  loss_mask_0: 0.3381  loss_dice_0: 2.684  loss_ce_1: 0.2777  loss_mask_1: 0.346  loss_dice_1: 2.59  loss_ce_2: 0.279  loss_mask_2: 0.3427  loss_dice_2: 2.563  loss_ce_3: 0.2624  loss_mask_3: 0.3413  loss_dice_3: 2.552  loss_ce_4: 0.249  loss_mask_4: 0.3386  loss_dice_4: 2.563  loss_ce_5: 0.2473  loss_mask_5: 0.3388  loss_dice_5: 2.55  loss_ce_6: 0.2403  loss_mask_6: 0.3393  loss_dice_6: 2.543  loss_ce_7: 0.2416  loss_mask_7: 0.3387  loss_dice_7: 2.546  loss_ce_8: 0.25  loss_mask_8: 0.34  loss_dice_8: 2.547  time: 1.4802  data_time: 0.0667  lr: 2.5913e-06  max_mem: 21589M
[01/18 09:10:12] d2.utils.events INFO:  eta: 3:33:26  iter: 31099  total_loss: 32.8  loss_ce: 0.2374  loss_mask: 0.3307  loss_dice: 2.637  loss_ce_0: 0.5671  loss_mask_0: 0.3347  loss_dice_0: 2.748  loss_ce_1: 0.2678  loss_mask_1: 0.3383  loss_dice_1: 2.665  loss_ce_2: 0.2653  loss_mask_2: 0.3325  loss_dice_2: 2.657  loss_ce_3: 0.2549  loss_mask_3: 0.3336  loss_dice_3: 2.643  loss_ce_4: 0.2423  loss_mask_4: 0.3351  loss_dice_4: 2.633  loss_ce_5: 0.2306  loss_mask_5: 0.3347  loss_dice_5: 2.64  loss_ce_6: 0.2259  loss_mask_6: 0.3344  loss_dice_6: 2.635  loss_ce_7: 0.2242  loss_mask_7: 0.3328  loss_dice_7: 2.635  loss_ce_8: 0.2332  loss_mask_8: 0.3329  loss_dice_8: 2.626  time: 1.4802  data_time: 0.0727  lr: 2.5861e-06  max_mem: 21589M
[01/18 09:10:41] d2.utils.events INFO:  eta: 3:32:51  iter: 31119  total_loss: 31.53  loss_ce: 0.2288  loss_mask: 0.3373  loss_dice: 2.541  loss_ce_0: 0.5553  loss_mask_0: 0.339  loss_dice_0: 2.671  loss_ce_1: 0.2815  loss_mask_1: 0.344  loss_dice_1: 2.576  loss_ce_2: 0.2604  loss_mask_2: 0.3414  loss_dice_2: 2.562  loss_ce_3: 0.2396  loss_mask_3: 0.3372  loss_dice_3: 2.538  loss_ce_4: 0.2419  loss_mask_4: 0.3365  loss_dice_4: 2.55  loss_ce_5: 0.2222  loss_mask_5: 0.3347  loss_dice_5: 2.533  loss_ce_6: 0.2273  loss_mask_6: 0.336  loss_dice_6: 2.535  loss_ce_7: 0.2184  loss_mask_7: 0.3367  loss_dice_7: 2.532  loss_ce_8: 0.2123  loss_mask_8: 0.3371  loss_dice_8: 2.538  time: 1.4801  data_time: 0.0677  lr: 2.5808e-06  max_mem: 21589M
[01/18 09:11:10] d2.utils.events INFO:  eta: 3:32:22  iter: 31139  total_loss: 32.19  loss_ce: 0.2213  loss_mask: 0.3379  loss_dice: 2.562  loss_ce_0: 0.5346  loss_mask_0: 0.3317  loss_dice_0: 2.702  loss_ce_1: 0.2849  loss_mask_1: 0.3397  loss_dice_1: 2.6  loss_ce_2: 0.2867  loss_mask_2: 0.3384  loss_dice_2: 2.585  loss_ce_3: 0.261  loss_mask_3: 0.338  loss_dice_3: 2.563  loss_ce_4: 0.2417  loss_mask_4: 0.3375  loss_dice_4: 2.57  loss_ce_5: 0.2516  loss_mask_5: 0.3385  loss_dice_5: 2.567  loss_ce_6: 0.2401  loss_mask_6: 0.3385  loss_dice_6: 2.561  loss_ce_7: 0.2308  loss_mask_7: 0.3386  loss_dice_7: 2.557  loss_ce_8: 0.2209  loss_mask_8: 0.3387  loss_dice_8: 2.558  time: 1.4801  data_time: 0.0743  lr: 2.5756e-06  max_mem: 21589M
[01/18 09:11:39] d2.utils.events INFO:  eta: 3:31:56  iter: 31159  total_loss: 32.97  loss_ce: 0.2266  loss_mask: 0.3262  loss_dice: 2.656  loss_ce_0: 0.5602  loss_mask_0: 0.3258  loss_dice_0: 2.811  loss_ce_1: 0.2804  loss_mask_1: 0.3325  loss_dice_1: 2.698  loss_ce_2: 0.2611  loss_mask_2: 0.3304  loss_dice_2: 2.68  loss_ce_3: 0.2558  loss_mask_3: 0.3276  loss_dice_3: 2.669  loss_ce_4: 0.2419  loss_mask_4: 0.3265  loss_dice_4: 2.654  loss_ce_5: 0.2329  loss_mask_5: 0.3268  loss_dice_5: 2.669  loss_ce_6: 0.224  loss_mask_6: 0.3279  loss_dice_6: 2.663  loss_ce_7: 0.2266  loss_mask_7: 0.3253  loss_dice_7: 2.662  loss_ce_8: 0.2371  loss_mask_8: 0.3264  loss_dice_8: 2.661  time: 1.4801  data_time: 0.0722  lr: 2.5704e-06  max_mem: 21589M
[01/18 09:12:08] d2.utils.events INFO:  eta: 3:31:26  iter: 31179  total_loss: 32.47  loss_ce: 0.2213  loss_mask: 0.331  loss_dice: 2.622  loss_ce_0: 0.552  loss_mask_0: 0.3293  loss_dice_0: 2.741  loss_ce_1: 0.2586  loss_mask_1: 0.3356  loss_dice_1: 2.655  loss_ce_2: 0.2587  loss_mask_2: 0.3329  loss_dice_2: 2.638  loss_ce_3: 0.2431  loss_mask_3: 0.3311  loss_dice_3: 2.619  loss_ce_4: 0.2277  loss_mask_4: 0.3305  loss_dice_4: 2.619  loss_ce_5: 0.2284  loss_mask_5: 0.3295  loss_dice_5: 2.621  loss_ce_6: 0.2177  loss_mask_6: 0.3302  loss_dice_6: 2.616  loss_ce_7: 0.2202  loss_mask_7: 0.3293  loss_dice_7: 2.62  loss_ce_8: 0.2118  loss_mask_8: 0.3299  loss_dice_8: 2.618  time: 1.4801  data_time: 0.0717  lr: 2.5651e-06  max_mem: 21589M
[01/18 09:12:37] d2.utils.events INFO:  eta: 3:31:03  iter: 31199  total_loss: 32.06  loss_ce: 0.2233  loss_mask: 0.3366  loss_dice: 2.584  loss_ce_0: 0.54  loss_mask_0: 0.3374  loss_dice_0: 2.712  loss_ce_1: 0.2683  loss_mask_1: 0.3441  loss_dice_1: 2.626  loss_ce_2: 0.276  loss_mask_2: 0.3392  loss_dice_2: 2.599  loss_ce_3: 0.2413  loss_mask_3: 0.3374  loss_dice_3: 2.581  loss_ce_4: 0.2344  loss_mask_4: 0.3368  loss_dice_4: 2.583  loss_ce_5: 0.2445  loss_mask_5: 0.3367  loss_dice_5: 2.587  loss_ce_6: 0.2204  loss_mask_6: 0.3367  loss_dice_6: 2.582  loss_ce_7: 0.2284  loss_mask_7: 0.3373  loss_dice_7: 2.581  loss_ce_8: 0.2183  loss_mask_8: 0.3377  loss_dice_8: 2.581  time: 1.4801  data_time: 0.0687  lr: 2.5599e-06  max_mem: 21589M
[01/18 09:13:06] d2.utils.events INFO:  eta: 3:30:44  iter: 31219  total_loss: 32.7  loss_ce: 0.2348  loss_mask: 0.3277  loss_dice: 2.634  loss_ce_0: 0.5665  loss_mask_0: 0.3268  loss_dice_0: 2.752  loss_ce_1: 0.2851  loss_mask_1: 0.3331  loss_dice_1: 2.675  loss_ce_2: 0.256  loss_mask_2: 0.3304  loss_dice_2: 2.651  loss_ce_3: 0.2473  loss_mask_3: 0.3292  loss_dice_3: 2.634  loss_ce_4: 0.2447  loss_mask_4: 0.3295  loss_dice_4: 2.642  loss_ce_5: 0.2402  loss_mask_5: 0.3281  loss_dice_5: 2.641  loss_ce_6: 0.2302  loss_mask_6: 0.3275  loss_dice_6: 2.628  loss_ce_7: 0.2266  loss_mask_7: 0.3282  loss_dice_7: 2.628  loss_ce_8: 0.2239  loss_mask_8: 0.3267  loss_dice_8: 2.628  time: 1.4801  data_time: 0.0704  lr: 2.5547e-06  max_mem: 21589M
[01/18 09:13:35] d2.utils.events INFO:  eta: 3:30:16  iter: 31239  total_loss: 32.47  loss_ce: 0.2377  loss_mask: 0.3381  loss_dice: 2.613  loss_ce_0: 0.5588  loss_mask_0: 0.3313  loss_dice_0: 2.759  loss_ce_1: 0.2948  loss_mask_1: 0.3383  loss_dice_1: 2.658  loss_ce_2: 0.2789  loss_mask_2: 0.3377  loss_dice_2: 2.635  loss_ce_3: 0.2597  loss_mask_3: 0.3376  loss_dice_3: 2.623  loss_ce_4: 0.2816  loss_mask_4: 0.3379  loss_dice_4: 2.627  loss_ce_5: 0.2403  loss_mask_5: 0.3385  loss_dice_5: 2.62  loss_ce_6: 0.2603  loss_mask_6: 0.3376  loss_dice_6: 2.613  loss_ce_7: 0.2351  loss_mask_7: 0.3373  loss_dice_7: 2.622  loss_ce_8: 0.2455  loss_mask_8: 0.3374  loss_dice_8: 2.609  time: 1.4800  data_time: 0.0715  lr: 2.5494e-06  max_mem: 21589M
[01/18 09:14:04] d2.utils.events INFO:  eta: 3:29:45  iter: 31259  total_loss: 32.78  loss_ce: 0.2501  loss_mask: 0.3436  loss_dice: 2.622  loss_ce_0: 0.5456  loss_mask_0: 0.3423  loss_dice_0: 2.756  loss_ce_1: 0.3077  loss_mask_1: 0.3476  loss_dice_1: 2.675  loss_ce_2: 0.2748  loss_mask_2: 0.3447  loss_dice_2: 2.645  loss_ce_3: 0.2719  loss_mask_3: 0.3456  loss_dice_3: 2.636  loss_ce_4: 0.2568  loss_mask_4: 0.3427  loss_dice_4: 2.628  loss_ce_5: 0.2522  loss_mask_5: 0.3414  loss_dice_5: 2.634  loss_ce_6: 0.2462  loss_mask_6: 0.3396  loss_dice_6: 2.626  loss_ce_7: 0.2422  loss_mask_7: 0.341  loss_dice_7: 2.619  loss_ce_8: 0.2404  loss_mask_8: 0.3415  loss_dice_8: 2.627  time: 1.4800  data_time: 0.0738  lr: 2.5442e-06  max_mem: 21589M
[01/18 09:14:33] d2.utils.events INFO:  eta: 3:29:28  iter: 31279  total_loss: 32.63  loss_ce: 0.2332  loss_mask: 0.3344  loss_dice: 2.617  loss_ce_0: 0.5514  loss_mask_0: 0.3404  loss_dice_0: 2.755  loss_ce_1: 0.2914  loss_mask_1: 0.3485  loss_dice_1: 2.655  loss_ce_2: 0.2701  loss_mask_2: 0.3439  loss_dice_2: 2.642  loss_ce_3: 0.2479  loss_mask_3: 0.3406  loss_dice_3: 2.621  loss_ce_4: 0.2412  loss_mask_4: 0.3407  loss_dice_4: 2.619  loss_ce_5: 0.2607  loss_mask_5: 0.339  loss_dice_5: 2.622  loss_ce_6: 0.2365  loss_mask_6: 0.3369  loss_dice_6: 2.625  loss_ce_7: 0.2439  loss_mask_7: 0.3382  loss_dice_7: 2.619  loss_ce_8: 0.2503  loss_mask_8: 0.3364  loss_dice_8: 2.618  time: 1.4800  data_time: 0.0772  lr: 2.539e-06  max_mem: 21589M
[01/18 09:15:03] d2.utils.events INFO:  eta: 3:29:07  iter: 31299  total_loss: 32.34  loss_ce: 0.2075  loss_mask: 0.3315  loss_dice: 2.601  loss_ce_0: 0.5253  loss_mask_0: 0.3352  loss_dice_0: 2.733  loss_ce_1: 0.2458  loss_mask_1: 0.3351  loss_dice_1: 2.636  loss_ce_2: 0.2484  loss_mask_2: 0.3333  loss_dice_2: 2.615  loss_ce_3: 0.2343  loss_mask_3: 0.3322  loss_dice_3: 2.603  loss_ce_4: 0.2303  loss_mask_4: 0.3323  loss_dice_4: 2.606  loss_ce_5: 0.2177  loss_mask_5: 0.3315  loss_dice_5: 2.596  loss_ce_6: 0.2206  loss_mask_6: 0.3312  loss_dice_6: 2.596  loss_ce_7: 0.225  loss_mask_7: 0.3322  loss_dice_7: 2.594  loss_ce_8: 0.2097  loss_mask_8: 0.3315  loss_dice_8: 2.592  time: 1.4800  data_time: 0.0749  lr: 2.5337e-06  max_mem: 21589M
[01/18 09:15:32] d2.utils.events INFO:  eta: 3:28:40  iter: 31319  total_loss: 32.15  loss_ce: 0.2373  loss_mask: 0.3407  loss_dice: 2.553  loss_ce_0: 0.5581  loss_mask_0: 0.3366  loss_dice_0: 2.705  loss_ce_1: 0.2842  loss_mask_1: 0.346  loss_dice_1: 2.607  loss_ce_2: 0.2611  loss_mask_2: 0.3434  loss_dice_2: 2.581  loss_ce_3: 0.2576  loss_mask_3: 0.3413  loss_dice_3: 2.565  loss_ce_4: 0.2536  loss_mask_4: 0.3407  loss_dice_4: 2.563  loss_ce_5: 0.2423  loss_mask_5: 0.3385  loss_dice_5: 2.552  loss_ce_6: 0.2373  loss_mask_6: 0.3393  loss_dice_6: 2.56  loss_ce_7: 0.2326  loss_mask_7: 0.3412  loss_dice_7: 2.562  loss_ce_8: 0.2376  loss_mask_8: 0.3415  loss_dice_8: 2.55  time: 1.4800  data_time: 0.0704  lr: 2.5285e-06  max_mem: 21589M
[01/18 09:16:01] d2.utils.events INFO:  eta: 3:28:10  iter: 31339  total_loss: 32.24  loss_ce: 0.2181  loss_mask: 0.3407  loss_dice: 2.601  loss_ce_0: 0.5587  loss_mask_0: 0.3414  loss_dice_0: 2.751  loss_ce_1: 0.2621  loss_mask_1: 0.3466  loss_dice_1: 2.65  loss_ce_2: 0.2581  loss_mask_2: 0.3428  loss_dice_2: 2.63  loss_ce_3: 0.2276  loss_mask_3: 0.3409  loss_dice_3: 2.61  loss_ce_4: 0.2356  loss_mask_4: 0.3408  loss_dice_4: 2.612  loss_ce_5: 0.2223  loss_mask_5: 0.3404  loss_dice_5: 2.614  loss_ce_6: 0.2218  loss_mask_6: 0.3414  loss_dice_6: 2.606  loss_ce_7: 0.2149  loss_mask_7: 0.3413  loss_dice_7: 2.613  loss_ce_8: 0.2181  loss_mask_8: 0.341  loss_dice_8: 2.607  time: 1.4799  data_time: 0.0663  lr: 2.5232e-06  max_mem: 21589M
[01/18 09:16:30] d2.utils.events INFO:  eta: 3:27:41  iter: 31359  total_loss: 32.7  loss_ce: 0.2195  loss_mask: 0.3345  loss_dice: 2.614  loss_ce_0: 0.5378  loss_mask_0: 0.3413  loss_dice_0: 2.76  loss_ce_1: 0.2779  loss_mask_1: 0.3442  loss_dice_1: 2.67  loss_ce_2: 0.2722  loss_mask_2: 0.3407  loss_dice_2: 2.648  loss_ce_3: 0.2681  loss_mask_3: 0.3363  loss_dice_3: 2.634  loss_ce_4: 0.2604  loss_mask_4: 0.3362  loss_dice_4: 2.626  loss_ce_5: 0.2312  loss_mask_5: 0.3347  loss_dice_5: 2.634  loss_ce_6: 0.2352  loss_mask_6: 0.3342  loss_dice_6: 2.62  loss_ce_7: 0.2355  loss_mask_7: 0.3336  loss_dice_7: 2.62  loss_ce_8: 0.2199  loss_mask_8: 0.3332  loss_dice_8: 2.633  time: 1.4799  data_time: 0.0705  lr: 2.518e-06  max_mem: 21589M
[01/18 09:16:59] d2.utils.events INFO:  eta: 3:27:12  iter: 31379  total_loss: 32.44  loss_ce: 0.215  loss_mask: 0.3375  loss_dice: 2.61  loss_ce_0: 0.5524  loss_mask_0: 0.3397  loss_dice_0: 2.75  loss_ce_1: 0.276  loss_mask_1: 0.3422  loss_dice_1: 2.654  loss_ce_2: 0.2674  loss_mask_2: 0.3407  loss_dice_2: 2.622  loss_ce_3: 0.2477  loss_mask_3: 0.3393  loss_dice_3: 2.621  loss_ce_4: 0.2266  loss_mask_4: 0.3391  loss_dice_4: 2.618  loss_ce_5: 0.2267  loss_mask_5: 0.3394  loss_dice_5: 2.613  loss_ce_6: 0.2258  loss_mask_6: 0.3397  loss_dice_6: 2.608  loss_ce_7: 0.2123  loss_mask_7: 0.3383  loss_dice_7: 2.617  loss_ce_8: 0.2196  loss_mask_8: 0.338  loss_dice_8: 2.608  time: 1.4799  data_time: 0.0696  lr: 2.5127e-06  max_mem: 21589M
[01/18 09:17:28] d2.utils.events INFO:  eta: 3:26:39  iter: 31399  total_loss: 32.59  loss_ce: 0.2401  loss_mask: 0.329  loss_dice: 2.609  loss_ce_0: 0.5749  loss_mask_0: 0.3275  loss_dice_0: 2.746  loss_ce_1: 0.277  loss_mask_1: 0.3309  loss_dice_1: 2.666  loss_ce_2: 0.2684  loss_mask_2: 0.3291  loss_dice_2: 2.636  loss_ce_3: 0.2509  loss_mask_3: 0.3276  loss_dice_3: 2.62  loss_ce_4: 0.2445  loss_mask_4: 0.3283  loss_dice_4: 2.616  loss_ce_5: 0.2193  loss_mask_5: 0.328  loss_dice_5: 2.616  loss_ce_6: 0.2423  loss_mask_6: 0.3281  loss_dice_6: 2.619  loss_ce_7: 0.2333  loss_mask_7: 0.3271  loss_dice_7: 2.619  loss_ce_8: 0.2369  loss_mask_8: 0.3279  loss_dice_8: 2.609  time: 1.4799  data_time: 0.0679  lr: 2.5075e-06  max_mem: 21589M
[01/18 09:17:57] d2.utils.events INFO:  eta: 3:26:15  iter: 31419  total_loss: 32.93  loss_ce: 0.2388  loss_mask: 0.3314  loss_dice: 2.619  loss_ce_0: 0.5546  loss_mask_0: 0.326  loss_dice_0: 2.769  loss_ce_1: 0.28  loss_mask_1: 0.3334  loss_dice_1: 2.662  loss_ce_2: 0.2897  loss_mask_2: 0.3327  loss_dice_2: 2.648  loss_ce_3: 0.2654  loss_mask_3: 0.3304  loss_dice_3: 2.634  loss_ce_4: 0.2498  loss_mask_4: 0.3331  loss_dice_4: 2.626  loss_ce_5: 0.2508  loss_mask_5: 0.3315  loss_dice_5: 2.632  loss_ce_6: 0.263  loss_mask_6: 0.3305  loss_dice_6: 2.624  loss_ce_7: 0.2322  loss_mask_7: 0.3308  loss_dice_7: 2.626  loss_ce_8: 0.2391  loss_mask_8: 0.3308  loss_dice_8: 2.624  time: 1.4799  data_time: 0.0677  lr: 2.5022e-06  max_mem: 21589M
[01/18 09:18:26] d2.utils.events INFO:  eta: 3:25:51  iter: 31439  total_loss: 32.56  loss_ce: 0.2406  loss_mask: 0.3328  loss_dice: 2.579  loss_ce_0: 0.5453  loss_mask_0: 0.3271  loss_dice_0: 2.717  loss_ce_1: 0.283  loss_mask_1: 0.3359  loss_dice_1: 2.628  loss_ce_2: 0.2694  loss_mask_2: 0.334  loss_dice_2: 2.603  loss_ce_3: 0.2514  loss_mask_3: 0.3313  loss_dice_3: 2.593  loss_ce_4: 0.2544  loss_mask_4: 0.3325  loss_dice_4: 2.592  loss_ce_5: 0.2378  loss_mask_5: 0.3327  loss_dice_5: 2.586  loss_ce_6: 0.2373  loss_mask_6: 0.3314  loss_dice_6: 2.582  loss_ce_7: 0.2424  loss_mask_7: 0.3316  loss_dice_7: 2.577  loss_ce_8: 0.2396  loss_mask_8: 0.3325  loss_dice_8: 2.573  time: 1.4799  data_time: 0.0697  lr: 2.497e-06  max_mem: 21589M
[01/18 09:18:56] d2.utils.events INFO:  eta: 3:25:24  iter: 31459  total_loss: 32.95  loss_ce: 0.2309  loss_mask: 0.3313  loss_dice: 2.636  loss_ce_0: 0.5629  loss_mask_0: 0.3308  loss_dice_0: 2.777  loss_ce_1: 0.283  loss_mask_1: 0.3389  loss_dice_1: 2.67  loss_ce_2: 0.2856  loss_mask_2: 0.3349  loss_dice_2: 2.658  loss_ce_3: 0.2704  loss_mask_3: 0.3319  loss_dice_3: 2.652  loss_ce_4: 0.2501  loss_mask_4: 0.3325  loss_dice_4: 2.652  loss_ce_5: 0.2394  loss_mask_5: 0.3329  loss_dice_5: 2.644  loss_ce_6: 0.2383  loss_mask_6: 0.3309  loss_dice_6: 2.639  loss_ce_7: 0.2345  loss_mask_7: 0.3311  loss_dice_7: 2.642  loss_ce_8: 0.2366  loss_mask_8: 0.331  loss_dice_8: 2.64  time: 1.4799  data_time: 0.0671  lr: 2.4917e-06  max_mem: 21589M
[01/18 09:19:25] d2.utils.events INFO:  eta: 3:24:55  iter: 31479  total_loss: 33.02  loss_ce: 0.2495  loss_mask: 0.3376  loss_dice: 2.657  loss_ce_0: 0.5837  loss_mask_0: 0.3353  loss_dice_0: 2.796  loss_ce_1: 0.2897  loss_mask_1: 0.3418  loss_dice_1: 2.693  loss_ce_2: 0.2795  loss_mask_2: 0.3401  loss_dice_2: 2.677  loss_ce_3: 0.2722  loss_mask_3: 0.3387  loss_dice_3: 2.665  loss_ce_4: 0.2529  loss_mask_4: 0.338  loss_dice_4: 2.668  loss_ce_5: 0.2629  loss_mask_5: 0.3374  loss_dice_5: 2.655  loss_ce_6: 0.245  loss_mask_6: 0.3352  loss_dice_6: 2.655  loss_ce_7: 0.2427  loss_mask_7: 0.3352  loss_dice_7: 2.656  loss_ce_8: 0.2311  loss_mask_8: 0.3365  loss_dice_8: 2.652  time: 1.4798  data_time: 0.0697  lr: 2.4865e-06  max_mem: 21589M
[01/18 09:19:53] d2.utils.events INFO:  eta: 3:24:27  iter: 31499  total_loss: 32.78  loss_ce: 0.2247  loss_mask: 0.3358  loss_dice: 2.64  loss_ce_0: 0.531  loss_mask_0: 0.3384  loss_dice_0: 2.762  loss_ce_1: 0.2705  loss_mask_1: 0.3444  loss_dice_1: 2.688  loss_ce_2: 0.2773  loss_mask_2: 0.3383  loss_dice_2: 2.661  loss_ce_3: 0.241  loss_mask_3: 0.337  loss_dice_3: 2.651  loss_ce_4: 0.2443  loss_mask_4: 0.3368  loss_dice_4: 2.654  loss_ce_5: 0.2332  loss_mask_5: 0.3361  loss_dice_5: 2.652  loss_ce_6: 0.2224  loss_mask_6: 0.3366  loss_dice_6: 2.641  loss_ce_7: 0.2385  loss_mask_7: 0.3367  loss_dice_7: 2.638  loss_ce_8: 0.2302  loss_mask_8: 0.3367  loss_dice_8: 2.644  time: 1.4798  data_time: 0.0667  lr: 2.4812e-06  max_mem: 21589M
[01/18 09:20:22] d2.utils.events INFO:  eta: 3:23:59  iter: 31519  total_loss: 32.4  loss_ce: 0.2331  loss_mask: 0.3402  loss_dice: 2.572  loss_ce_0: 0.5707  loss_mask_0: 0.3398  loss_dice_0: 2.705  loss_ce_1: 0.2902  loss_mask_1: 0.3446  loss_dice_1: 2.609  loss_ce_2: 0.2776  loss_mask_2: 0.3413  loss_dice_2: 2.596  loss_ce_3: 0.261  loss_mask_3: 0.3401  loss_dice_3: 2.578  loss_ce_4: 0.2518  loss_mask_4: 0.3392  loss_dice_4: 2.575  loss_ce_5: 0.2472  loss_mask_5: 0.339  loss_dice_5: 2.58  loss_ce_6: 0.248  loss_mask_6: 0.3408  loss_dice_6: 2.572  loss_ce_7: 0.2366  loss_mask_7: 0.3405  loss_dice_7: 2.576  loss_ce_8: 0.2336  loss_mask_8: 0.3396  loss_dice_8: 2.565  time: 1.4798  data_time: 0.0668  lr: 2.476e-06  max_mem: 21589M
[01/18 09:20:52] d2.utils.events INFO:  eta: 3:23:31  iter: 31539  total_loss: 32.4  loss_ce: 0.2366  loss_mask: 0.3425  loss_dice: 2.602  loss_ce_0: 0.567  loss_mask_0: 0.3448  loss_dice_0: 2.717  loss_ce_1: 0.2825  loss_mask_1: 0.3499  loss_dice_1: 2.642  loss_ce_2: 0.2637  loss_mask_2: 0.3473  loss_dice_2: 2.624  loss_ce_3: 0.2598  loss_mask_3: 0.3442  loss_dice_3: 2.606  loss_ce_4: 0.2431  loss_mask_4: 0.3422  loss_dice_4: 2.601  loss_ce_5: 0.244  loss_mask_5: 0.3428  loss_dice_5: 2.612  loss_ce_6: 0.2326  loss_mask_6: 0.3414  loss_dice_6: 2.6  loss_ce_7: 0.2238  loss_mask_7: 0.3418  loss_dice_7: 2.608  loss_ce_8: 0.2281  loss_mask_8: 0.3425  loss_dice_8: 2.601  time: 1.4798  data_time: 0.0695  lr: 2.4707e-06  max_mem: 21589M
[01/18 09:21:21] d2.utils.events INFO:  eta: 3:23:02  iter: 31559  total_loss: 32.3  loss_ce: 0.2264  loss_mask: 0.34  loss_dice: 2.553  loss_ce_0: 0.5467  loss_mask_0: 0.3372  loss_dice_0: 2.678  loss_ce_1: 0.2741  loss_mask_1: 0.3472  loss_dice_1: 2.6  loss_ce_2: 0.2721  loss_mask_2: 0.3438  loss_dice_2: 2.574  loss_ce_3: 0.2309  loss_mask_3: 0.3404  loss_dice_3: 2.566  loss_ce_4: 0.2241  loss_mask_4: 0.3398  loss_dice_4: 2.56  loss_ce_5: 0.2351  loss_mask_5: 0.3399  loss_dice_5: 2.562  loss_ce_6: 0.2407  loss_mask_6: 0.3393  loss_dice_6: 2.552  loss_ce_7: 0.2329  loss_mask_7: 0.3409  loss_dice_7: 2.551  loss_ce_8: 0.2365  loss_mask_8: 0.3389  loss_dice_8: 2.554  time: 1.4798  data_time: 0.0691  lr: 2.4655e-06  max_mem: 21589M
[01/18 09:21:50] d2.utils.events INFO:  eta: 3:22:31  iter: 31579  total_loss: 32.43  loss_ce: 0.2381  loss_mask: 0.3386  loss_dice: 2.568  loss_ce_0: 0.5457  loss_mask_0: 0.3326  loss_dice_0: 2.701  loss_ce_1: 0.2854  loss_mask_1: 0.3421  loss_dice_1: 2.618  loss_ce_2: 0.2671  loss_mask_2: 0.3396  loss_dice_2: 2.598  loss_ce_3: 0.2663  loss_mask_3: 0.339  loss_dice_3: 2.579  loss_ce_4: 0.2487  loss_mask_4: 0.3387  loss_dice_4: 2.581  loss_ce_5: 0.2442  loss_mask_5: 0.3374  loss_dice_5: 2.579  loss_ce_6: 0.2489  loss_mask_6: 0.3372  loss_dice_6: 2.576  loss_ce_7: 0.2399  loss_mask_7: 0.3376  loss_dice_7: 2.563  loss_ce_8: 0.2409  loss_mask_8: 0.338  loss_dice_8: 2.575  time: 1.4797  data_time: 0.0692  lr: 2.4602e-06  max_mem: 21589M
[01/18 09:22:19] d2.utils.events INFO:  eta: 3:22:02  iter: 31599  total_loss: 32.07  loss_ce: 0.2242  loss_mask: 0.3278  loss_dice: 2.559  loss_ce_0: 0.5688  loss_mask_0: 0.3315  loss_dice_0: 2.697  loss_ce_1: 0.2739  loss_mask_1: 0.3324  loss_dice_1: 2.614  loss_ce_2: 0.2675  loss_mask_2: 0.3283  loss_dice_2: 2.586  loss_ce_3: 0.2593  loss_mask_3: 0.3287  loss_dice_3: 2.566  loss_ce_4: 0.2461  loss_mask_4: 0.3286  loss_dice_4: 2.562  loss_ce_5: 0.2444  loss_mask_5: 0.329  loss_dice_5: 2.563  loss_ce_6: 0.2436  loss_mask_6: 0.3275  loss_dice_6: 2.562  loss_ce_7: 0.2293  loss_mask_7: 0.3275  loss_dice_7: 2.566  loss_ce_8: 0.2279  loss_mask_8: 0.3284  loss_dice_8: 2.566  time: 1.4797  data_time: 0.0696  lr: 2.455e-06  max_mem: 21589M
[01/18 09:22:48] d2.utils.events INFO:  eta: 3:21:31  iter: 31619  total_loss: 32.37  loss_ce: 0.2329  loss_mask: 0.3403  loss_dice: 2.575  loss_ce_0: 0.5548  loss_mask_0: 0.334  loss_dice_0: 2.715  loss_ce_1: 0.2788  loss_mask_1: 0.3437  loss_dice_1: 2.631  loss_ce_2: 0.2761  loss_mask_2: 0.3424  loss_dice_2: 2.594  loss_ce_3: 0.2612  loss_mask_3: 0.3408  loss_dice_3: 2.581  loss_ce_4: 0.2529  loss_mask_4: 0.3405  loss_dice_4: 2.583  loss_ce_5: 0.253  loss_mask_5: 0.3388  loss_dice_5: 2.583  loss_ce_6: 0.2429  loss_mask_6: 0.3401  loss_dice_6: 2.583  loss_ce_7: 0.2313  loss_mask_7: 0.3409  loss_dice_7: 2.575  loss_ce_8: 0.2202  loss_mask_8: 0.3402  loss_dice_8: 2.575  time: 1.4797  data_time: 0.0676  lr: 2.4497e-06  max_mem: 21589M
[01/18 09:23:17] d2.utils.events INFO:  eta: 3:21:01  iter: 31639  total_loss: 32.23  loss_ce: 0.2335  loss_mask: 0.3381  loss_dice: 2.601  loss_ce_0: 0.547  loss_mask_0: 0.3321  loss_dice_0: 2.726  loss_ce_1: 0.2718  loss_mask_1: 0.3419  loss_dice_1: 2.634  loss_ce_2: 0.2765  loss_mask_2: 0.3401  loss_dice_2: 2.615  loss_ce_3: 0.2565  loss_mask_3: 0.339  loss_dice_3: 2.601  loss_ce_4: 0.2401  loss_mask_4: 0.3386  loss_dice_4: 2.597  loss_ce_5: 0.2434  loss_mask_5: 0.3379  loss_dice_5: 2.602  loss_ce_6: 0.227  loss_mask_6: 0.3374  loss_dice_6: 2.598  loss_ce_7: 0.2221  loss_mask_7: 0.3375  loss_dice_7: 2.605  loss_ce_8: 0.2308  loss_mask_8: 0.3376  loss_dice_8: 2.6  time: 1.4797  data_time: 0.0669  lr: 2.4444e-06  max_mem: 21589M
[01/18 09:23:45] d2.utils.events INFO:  eta: 3:20:32  iter: 31659  total_loss: 32.72  loss_ce: 0.2614  loss_mask: 0.3284  loss_dice: 2.595  loss_ce_0: 0.5759  loss_mask_0: 0.3318  loss_dice_0: 2.748  loss_ce_1: 0.3079  loss_mask_1: 0.332  loss_dice_1: 2.664  loss_ce_2: 0.3032  loss_mask_2: 0.3301  loss_dice_2: 2.627  loss_ce_3: 0.2828  loss_mask_3: 0.3299  loss_dice_3: 2.599  loss_ce_4: 0.2759  loss_mask_4: 0.3291  loss_dice_4: 2.615  loss_ce_5: 0.2616  loss_mask_5: 0.3295  loss_dice_5: 2.616  loss_ce_6: 0.2593  loss_mask_6: 0.3313  loss_dice_6: 2.601  loss_ce_7: 0.2643  loss_mask_7: 0.3293  loss_dice_7: 2.6  loss_ce_8: 0.2698  loss_mask_8: 0.3293  loss_dice_8: 2.597  time: 1.4796  data_time: 0.0659  lr: 2.4392e-06  max_mem: 21589M
[01/18 09:24:15] d2.utils.events INFO:  eta: 3:20:04  iter: 31679  total_loss: 32.17  loss_ce: 0.2229  loss_mask: 0.3368  loss_dice: 2.585  loss_ce_0: 0.5575  loss_mask_0: 0.334  loss_dice_0: 2.706  loss_ce_1: 0.2714  loss_mask_1: 0.3414  loss_dice_1: 2.641  loss_ce_2: 0.2623  loss_mask_2: 0.3371  loss_dice_2: 2.609  loss_ce_3: 0.2498  loss_mask_3: 0.3366  loss_dice_3: 2.596  loss_ce_4: 0.241  loss_mask_4: 0.3343  loss_dice_4: 2.597  loss_ce_5: 0.2197  loss_mask_5: 0.3338  loss_dice_5: 2.592  loss_ce_6: 0.238  loss_mask_6: 0.3336  loss_dice_6: 2.591  loss_ce_7: 0.242  loss_mask_7: 0.3357  loss_dice_7: 2.589  loss_ce_8: 0.2323  loss_mask_8: 0.3357  loss_dice_8: 2.581  time: 1.4796  data_time: 0.0682  lr: 2.4339e-06  max_mem: 21589M
[01/18 09:24:43] d2.utils.events INFO:  eta: 3:19:35  iter: 31699  total_loss: 32.19  loss_ce: 0.226  loss_mask: 0.332  loss_dice: 2.565  loss_ce_0: 0.5787  loss_mask_0: 0.3339  loss_dice_0: 2.69  loss_ce_1: 0.2775  loss_mask_1: 0.3392  loss_dice_1: 2.604  loss_ce_2: 0.2751  loss_mask_2: 0.3348  loss_dice_2: 2.581  loss_ce_3: 0.2538  loss_mask_3: 0.3347  loss_dice_3: 2.575  loss_ce_4: 0.2441  loss_mask_4: 0.3335  loss_dice_4: 2.569  loss_ce_5: 0.2316  loss_mask_5: 0.3321  loss_dice_5: 2.569  loss_ce_6: 0.2309  loss_mask_6: 0.3307  loss_dice_6: 2.576  loss_ce_7: 0.2308  loss_mask_7: 0.332  loss_dice_7: 2.567  loss_ce_8: 0.2254  loss_mask_8: 0.3313  loss_dice_8: 2.567  time: 1.4796  data_time: 0.0656  lr: 2.4286e-06  max_mem: 21589M
[01/18 09:25:12] d2.utils.events INFO:  eta: 3:19:02  iter: 31719  total_loss: 32.38  loss_ce: 0.2317  loss_mask: 0.3385  loss_dice: 2.588  loss_ce_0: 0.5675  loss_mask_0: 0.3356  loss_dice_0: 2.704  loss_ce_1: 0.2878  loss_mask_1: 0.3437  loss_dice_1: 2.613  loss_ce_2: 0.2724  loss_mask_2: 0.3405  loss_dice_2: 2.6  loss_ce_3: 0.2508  loss_mask_3: 0.3395  loss_dice_3: 2.591  loss_ce_4: 0.2401  loss_mask_4: 0.3385  loss_dice_4: 2.59  loss_ce_5: 0.2411  loss_mask_5: 0.3384  loss_dice_5: 2.59  loss_ce_6: 0.2359  loss_mask_6: 0.3387  loss_dice_6: 2.583  loss_ce_7: 0.2221  loss_mask_7: 0.3391  loss_dice_7: 2.587  loss_ce_8: 0.2285  loss_mask_8: 0.3387  loss_dice_8: 2.591  time: 1.4796  data_time: 0.0666  lr: 2.4234e-06  max_mem: 21589M
[01/18 09:25:42] d2.utils.events INFO:  eta: 3:18:34  iter: 31739  total_loss: 32.58  loss_ce: 0.2415  loss_mask: 0.3378  loss_dice: 2.625  loss_ce_0: 0.5274  loss_mask_0: 0.3352  loss_dice_0: 2.755  loss_ce_1: 0.2783  loss_mask_1: 0.3427  loss_dice_1: 2.679  loss_ce_2: 0.2804  loss_mask_2: 0.3381  loss_dice_2: 2.65  loss_ce_3: 0.2586  loss_mask_3: 0.3376  loss_dice_3: 2.642  loss_ce_4: 0.2493  loss_mask_4: 0.3373  loss_dice_4: 2.631  loss_ce_5: 0.2414  loss_mask_5: 0.3358  loss_dice_5: 2.626  loss_ce_6: 0.242  loss_mask_6: 0.3358  loss_dice_6: 2.627  loss_ce_7: 0.2425  loss_mask_7: 0.3371  loss_dice_7: 2.623  loss_ce_8: 0.2369  loss_mask_8: 0.3377  loss_dice_8: 2.625  time: 1.4796  data_time: 0.0706  lr: 2.4181e-06  max_mem: 21589M
[01/18 09:26:11] d2.utils.events INFO:  eta: 3:18:05  iter: 31759  total_loss: 32.06  loss_ce: 0.2366  loss_mask: 0.3376  loss_dice: 2.541  loss_ce_0: 0.5437  loss_mask_0: 0.3376  loss_dice_0: 2.691  loss_ce_1: 0.2787  loss_mask_1: 0.3438  loss_dice_1: 2.587  loss_ce_2: 0.2805  loss_mask_2: 0.3406  loss_dice_2: 2.556  loss_ce_3: 0.251  loss_mask_3: 0.339  loss_dice_3: 2.553  loss_ce_4: 0.2439  loss_mask_4: 0.3393  loss_dice_4: 2.546  loss_ce_5: 0.2295  loss_mask_5: 0.3382  loss_dice_5: 2.546  loss_ce_6: 0.2272  loss_mask_6: 0.3378  loss_dice_6: 2.548  loss_ce_7: 0.2243  loss_mask_7: 0.3369  loss_dice_7: 2.546  loss_ce_8: 0.228  loss_mask_8: 0.3367  loss_dice_8: 2.541  time: 1.4795  data_time: 0.0675  lr: 2.4128e-06  max_mem: 21589M
[01/18 09:26:40] d2.utils.events INFO:  eta: 3:17:38  iter: 31779  total_loss: 32.47  loss_ce: 0.2153  loss_mask: 0.328  loss_dice: 2.619  loss_ce_0: 0.5594  loss_mask_0: 0.334  loss_dice_0: 2.753  loss_ce_1: 0.2862  loss_mask_1: 0.3382  loss_dice_1: 2.665  loss_ce_2: 0.2791  loss_mask_2: 0.3333  loss_dice_2: 2.635  loss_ce_3: 0.2467  loss_mask_3: 0.329  loss_dice_3: 2.628  loss_ce_4: 0.2541  loss_mask_4: 0.33  loss_dice_4: 2.625  loss_ce_5: 0.241  loss_mask_5: 0.3294  loss_dice_5: 2.624  loss_ce_6: 0.2399  loss_mask_6: 0.3285  loss_dice_6: 2.622  loss_ce_7: 0.2359  loss_mask_7: 0.3308  loss_dice_7: 2.614  loss_ce_8: 0.2264  loss_mask_8: 0.3313  loss_dice_8: 2.627  time: 1.4795  data_time: 0.0726  lr: 2.4076e-06  max_mem: 21589M
[01/18 09:27:10] d2.utils.events INFO:  eta: 3:17:14  iter: 31799  total_loss: 32.25  loss_ce: 0.2096  loss_mask: 0.3318  loss_dice: 2.625  loss_ce_0: 0.5276  loss_mask_0: 0.3322  loss_dice_0: 2.755  loss_ce_1: 0.265  loss_mask_1: 0.3382  loss_dice_1: 2.663  loss_ce_2: 0.2519  loss_mask_2: 0.3353  loss_dice_2: 2.641  loss_ce_3: 0.2285  loss_mask_3: 0.3336  loss_dice_3: 2.639  loss_ce_4: 0.2358  loss_mask_4: 0.3326  loss_dice_4: 2.628  loss_ce_5: 0.2218  loss_mask_5: 0.3311  loss_dice_5: 2.635  loss_ce_6: 0.2153  loss_mask_6: 0.3321  loss_dice_6: 2.63  loss_ce_7: 0.2182  loss_mask_7: 0.3318  loss_dice_7: 2.624  loss_ce_8: 0.215  loss_mask_8: 0.3321  loss_dice_8: 2.625  time: 1.4795  data_time: 0.0734  lr: 2.4023e-06  max_mem: 21589M
[01/18 09:27:39] d2.utils.events INFO:  eta: 3:16:46  iter: 31819  total_loss: 32.2  loss_ce: 0.2244  loss_mask: 0.3349  loss_dice: 2.564  loss_ce_0: 0.5534  loss_mask_0: 0.3338  loss_dice_0: 2.713  loss_ce_1: 0.2764  loss_mask_1: 0.3375  loss_dice_1: 2.616  loss_ce_2: 0.2735  loss_mask_2: 0.3365  loss_dice_2: 2.59  loss_ce_3: 0.2409  loss_mask_3: 0.3351  loss_dice_3: 2.584  loss_ce_4: 0.2387  loss_mask_4: 0.3339  loss_dice_4: 2.575  loss_ce_5: 0.2249  loss_mask_5: 0.3331  loss_dice_5: 2.572  loss_ce_6: 0.2281  loss_mask_6: 0.3341  loss_dice_6: 2.571  loss_ce_7: 0.2253  loss_mask_7: 0.3351  loss_dice_7: 2.57  loss_ce_8: 0.222  loss_mask_8: 0.334  loss_dice_8: 2.572  time: 1.4795  data_time: 0.0723  lr: 2.397e-06  max_mem: 21589M
[01/18 09:28:09] d2.utils.events INFO:  eta: 3:16:28  iter: 31839  total_loss: 32.17  loss_ce: 0.224  loss_mask: 0.3302  loss_dice: 2.582  loss_ce_0: 0.5344  loss_mask_0: 0.334  loss_dice_0: 2.719  loss_ce_1: 0.2857  loss_mask_1: 0.3369  loss_dice_1: 2.624  loss_ce_2: 0.2715  loss_mask_2: 0.3334  loss_dice_2: 2.608  loss_ce_3: 0.2532  loss_mask_3: 0.3316  loss_dice_3: 2.589  loss_ce_4: 0.2474  loss_mask_4: 0.3298  loss_dice_4: 2.59  loss_ce_5: 0.2269  loss_mask_5: 0.3293  loss_dice_5: 2.591  loss_ce_6: 0.234  loss_mask_6: 0.3294  loss_dice_6: 2.582  loss_ce_7: 0.2272  loss_mask_7: 0.3299  loss_dice_7: 2.574  loss_ce_8: 0.2292  loss_mask_8: 0.3304  loss_dice_8: 2.594  time: 1.4795  data_time: 0.0715  lr: 2.3917e-06  max_mem: 21589M
[01/18 09:28:39] d2.utils.events INFO:  eta: 3:16:06  iter: 31859  total_loss: 32.53  loss_ce: 0.2234  loss_mask: 0.3375  loss_dice: 2.573  loss_ce_0: 0.5572  loss_mask_0: 0.3411  loss_dice_0: 2.715  loss_ce_1: 0.2856  loss_mask_1: 0.3446  loss_dice_1: 2.637  loss_ce_2: 0.2717  loss_mask_2: 0.3397  loss_dice_2: 2.594  loss_ce_3: 0.2456  loss_mask_3: 0.3377  loss_dice_3: 2.592  loss_ce_4: 0.2431  loss_mask_4: 0.3361  loss_dice_4: 2.591  loss_ce_5: 0.2336  loss_mask_5: 0.3369  loss_dice_5: 2.58  loss_ce_6: 0.2286  loss_mask_6: 0.3363  loss_dice_6: 2.581  loss_ce_7: 0.2425  loss_mask_7: 0.3373  loss_dice_7: 2.576  loss_ce_8: 0.2219  loss_mask_8: 0.3374  loss_dice_8: 2.57  time: 1.4795  data_time: 0.0725  lr: 2.3865e-06  max_mem: 21589M
[01/18 09:29:07] d2.utils.events INFO:  eta: 3:15:32  iter: 31879  total_loss: 31.92  loss_ce: 0.2461  loss_mask: 0.3382  loss_dice: 2.557  loss_ce_0: 0.5549  loss_mask_0: 0.3405  loss_dice_0: 2.679  loss_ce_1: 0.287  loss_mask_1: 0.3477  loss_dice_1: 2.603  loss_ce_2: 0.287  loss_mask_2: 0.3446  loss_dice_2: 2.578  loss_ce_3: 0.2706  loss_mask_3: 0.3434  loss_dice_3: 2.565  loss_ce_4: 0.2641  loss_mask_4: 0.3424  loss_dice_4: 2.564  loss_ce_5: 0.2684  loss_mask_5: 0.3397  loss_dice_5: 2.562  loss_ce_6: 0.2415  loss_mask_6: 0.3394  loss_dice_6: 2.556  loss_ce_7: 0.2428  loss_mask_7: 0.3392  loss_dice_7: 2.558  loss_ce_8: 0.2569  loss_mask_8: 0.3383  loss_dice_8: 2.562  time: 1.4795  data_time: 0.0674  lr: 2.3812e-06  max_mem: 21589M
[01/18 09:29:37] d2.utils.events INFO:  eta: 3:15:08  iter: 31899  total_loss: 32.31  loss_ce: 0.242  loss_mask: 0.3361  loss_dice: 2.625  loss_ce_0: 0.5433  loss_mask_0: 0.3392  loss_dice_0: 2.768  loss_ce_1: 0.2925  loss_mask_1: 0.3442  loss_dice_1: 2.673  loss_ce_2: 0.2757  loss_mask_2: 0.3408  loss_dice_2: 2.658  loss_ce_3: 0.2441  loss_mask_3: 0.3411  loss_dice_3: 2.633  loss_ce_4: 0.2551  loss_mask_4: 0.3372  loss_dice_4: 2.629  loss_ce_5: 0.2355  loss_mask_5: 0.3367  loss_dice_5: 2.637  loss_ce_6: 0.2332  loss_mask_6: 0.3353  loss_dice_6: 2.639  loss_ce_7: 0.2331  loss_mask_7: 0.3362  loss_dice_7: 2.63  loss_ce_8: 0.2371  loss_mask_8: 0.3355  loss_dice_8: 2.634  time: 1.4795  data_time: 0.0717  lr: 2.3759e-06  max_mem: 21589M
[01/18 09:30:06] d2.utils.events INFO:  eta: 3:14:48  iter: 31919  total_loss: 32.76  loss_ce: 0.2311  loss_mask: 0.3447  loss_dice: 2.636  loss_ce_0: 0.5592  loss_mask_0: 0.3377  loss_dice_0: 2.763  loss_ce_1: 0.2691  loss_mask_1: 0.3492  loss_dice_1: 2.682  loss_ce_2: 0.2573  loss_mask_2: 0.347  loss_dice_2: 2.654  loss_ce_3: 0.2614  loss_mask_3: 0.3454  loss_dice_3: 2.64  loss_ce_4: 0.2459  loss_mask_4: 0.344  loss_dice_4: 2.639  loss_ce_5: 0.2411  loss_mask_5: 0.3442  loss_dice_5: 2.635  loss_ce_6: 0.2308  loss_mask_6: 0.3443  loss_dice_6: 2.628  loss_ce_7: 0.2249  loss_mask_7: 0.3454  loss_dice_7: 2.636  loss_ce_8: 0.228  loss_mask_8: 0.3456  loss_dice_8: 2.636  time: 1.4795  data_time: 0.0691  lr: 2.3706e-06  max_mem: 21589M
[01/18 09:30:36] d2.utils.events INFO:  eta: 3:14:34  iter: 31939  total_loss: 33.3  loss_ce: 0.246  loss_mask: 0.3432  loss_dice: 2.675  loss_ce_0: 0.5652  loss_mask_0: 0.3374  loss_dice_0: 2.818  loss_ce_1: 0.2775  loss_mask_1: 0.347  loss_dice_1: 2.722  loss_ce_2: 0.2653  loss_mask_2: 0.345  loss_dice_2: 2.692  loss_ce_3: 0.2523  loss_mask_3: 0.3441  loss_dice_3: 2.684  loss_ce_4: 0.2597  loss_mask_4: 0.3421  loss_dice_4: 2.681  loss_ce_5: 0.244  loss_mask_5: 0.3409  loss_dice_5: 2.674  loss_ce_6: 0.2474  loss_mask_6: 0.3423  loss_dice_6: 2.666  loss_ce_7: 0.2305  loss_mask_7: 0.3427  loss_dice_7: 2.674  loss_ce_8: 0.231  loss_mask_8: 0.3436  loss_dice_8: 2.673  time: 1.4795  data_time: 0.0719  lr: 2.3654e-06  max_mem: 21589M
[01/18 09:31:06] d2.utils.events INFO:  eta: 3:14:07  iter: 31959  total_loss: 32.75  loss_ce: 0.2389  loss_mask: 0.3346  loss_dice: 2.633  loss_ce_0: 0.5672  loss_mask_0: 0.3324  loss_dice_0: 2.763  loss_ce_1: 0.3014  loss_mask_1: 0.3433  loss_dice_1: 2.683  loss_ce_2: 0.2807  loss_mask_2: 0.3377  loss_dice_2: 2.655  loss_ce_3: 0.2536  loss_mask_3: 0.3358  loss_dice_3: 2.65  loss_ce_4: 0.2459  loss_mask_4: 0.3348  loss_dice_4: 2.633  loss_ce_5: 0.244  loss_mask_5: 0.3365  loss_dice_5: 2.637  loss_ce_6: 0.2429  loss_mask_6: 0.335  loss_dice_6: 2.635  loss_ce_7: 0.2359  loss_mask_7: 0.3342  loss_dice_7: 2.65  loss_ce_8: 0.2339  loss_mask_8: 0.3337  loss_dice_8: 2.627  time: 1.4795  data_time: 0.0743  lr: 2.3601e-06  max_mem: 21589M
[01/18 09:31:34] d2.utils.events INFO:  eta: 3:13:37  iter: 31979  total_loss: 33.34  loss_ce: 0.2475  loss_mask: 0.3345  loss_dice: 2.672  loss_ce_0: 0.5727  loss_mask_0: 0.3374  loss_dice_0: 2.793  loss_ce_1: 0.2992  loss_mask_1: 0.3407  loss_dice_1: 2.705  loss_ce_2: 0.2847  loss_mask_2: 0.3378  loss_dice_2: 2.683  loss_ce_3: 0.2805  loss_mask_3: 0.335  loss_dice_3: 2.675  loss_ce_4: 0.2597  loss_mask_4: 0.3344  loss_dice_4: 2.667  loss_ce_5: 0.2504  loss_mask_5: 0.3334  loss_dice_5: 2.673  loss_ce_6: 0.2521  loss_mask_6: 0.3347  loss_dice_6: 2.671  loss_ce_7: 0.2473  loss_mask_7: 0.3348  loss_dice_7: 2.665  loss_ce_8: 0.2489  loss_mask_8: 0.3353  loss_dice_8: 2.669  time: 1.4795  data_time: 0.0674  lr: 2.3548e-06  max_mem: 21589M
[01/18 09:32:04] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 09:32:04] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 09:32:04] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 09:32:05] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 09:32:19] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0077 s/iter. Inference: 0.1519 s/iter. Eval: 0.2252 s/iter. Total: 0.3848 s/iter. ETA=0:06:56
[01/18 09:32:24] d2.evaluation.evaluator INFO: Inference done 22/1093. Dataloading: 0.0116 s/iter. Inference: 0.1749 s/iter. Eval: 0.2487 s/iter. Total: 0.4353 s/iter. ETA=0:07:46
[01/18 09:32:29] d2.evaluation.evaluator INFO: Inference done 35/1093. Dataloading: 0.0110 s/iter. Inference: 0.1724 s/iter. Eval: 0.2311 s/iter. Total: 0.4146 s/iter. ETA=0:07:18
[01/18 09:32:35] d2.evaluation.evaluator INFO: Inference done 49/1093. Dataloading: 0.0116 s/iter. Inference: 0.1663 s/iter. Eval: 0.2263 s/iter. Total: 0.4042 s/iter. ETA=0:07:01
[01/18 09:32:40] d2.evaluation.evaluator INFO: Inference done 61/1093. Dataloading: 0.0123 s/iter. Inference: 0.1649 s/iter. Eval: 0.2330 s/iter. Total: 0.4103 s/iter. ETA=0:07:03
[01/18 09:32:45] d2.evaluation.evaluator INFO: Inference done 74/1093. Dataloading: 0.0124 s/iter. Inference: 0.1633 s/iter. Eval: 0.2303 s/iter. Total: 0.4061 s/iter. ETA=0:06:53
[01/18 09:32:50] d2.evaluation.evaluator INFO: Inference done 87/1093. Dataloading: 0.0123 s/iter. Inference: 0.1638 s/iter. Eval: 0.2305 s/iter. Total: 0.4066 s/iter. ETA=0:06:49
[01/18 09:32:55] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0122 s/iter. Inference: 0.1617 s/iter. Eval: 0.2303 s/iter. Total: 0.4042 s/iter. ETA=0:06:41
[01/18 09:33:00] d2.evaluation.evaluator INFO: Inference done 115/1093. Dataloading: 0.0119 s/iter. Inference: 0.1594 s/iter. Eval: 0.2247 s/iter. Total: 0.3960 s/iter. ETA=0:06:27
[01/18 09:33:06] d2.evaluation.evaluator INFO: Inference done 129/1093. Dataloading: 0.0118 s/iter. Inference: 0.1600 s/iter. Eval: 0.2219 s/iter. Total: 0.3938 s/iter. ETA=0:06:19
[01/18 09:33:11] d2.evaluation.evaluator INFO: Inference done 144/1093. Dataloading: 0.0117 s/iter. Inference: 0.1591 s/iter. Eval: 0.2185 s/iter. Total: 0.3893 s/iter. ETA=0:06:09
[01/18 09:33:16] d2.evaluation.evaluator INFO: Inference done 158/1093. Dataloading: 0.0118 s/iter. Inference: 0.1591 s/iter. Eval: 0.2161 s/iter. Total: 0.3871 s/iter. ETA=0:06:01
[01/18 09:33:21] d2.evaluation.evaluator INFO: Inference done 170/1093. Dataloading: 0.0119 s/iter. Inference: 0.1604 s/iter. Eval: 0.2192 s/iter. Total: 0.3916 s/iter. ETA=0:06:01
[01/18 09:33:27] d2.evaluation.evaluator INFO: Inference done 185/1093. Dataloading: 0.0117 s/iter. Inference: 0.1599 s/iter. Eval: 0.2164 s/iter. Total: 0.3881 s/iter. ETA=0:05:52
[01/18 09:33:32] d2.evaluation.evaluator INFO: Inference done 199/1093. Dataloading: 0.0116 s/iter. Inference: 0.1604 s/iter. Eval: 0.2147 s/iter. Total: 0.3868 s/iter. ETA=0:05:45
[01/18 09:33:37] d2.evaluation.evaluator INFO: Inference done 212/1093. Dataloading: 0.0117 s/iter. Inference: 0.1599 s/iter. Eval: 0.2163 s/iter. Total: 0.3880 s/iter. ETA=0:05:41
[01/18 09:33:42] d2.evaluation.evaluator INFO: Inference done 225/1093. Dataloading: 0.0118 s/iter. Inference: 0.1603 s/iter. Eval: 0.2167 s/iter. Total: 0.3889 s/iter. ETA=0:05:37
[01/18 09:33:47] d2.evaluation.evaluator INFO: Inference done 238/1093. Dataloading: 0.0118 s/iter. Inference: 0.1602 s/iter. Eval: 0.2168 s/iter. Total: 0.3889 s/iter. ETA=0:05:32
[01/18 09:33:53] d2.evaluation.evaluator INFO: Inference done 251/1093. Dataloading: 0.0118 s/iter. Inference: 0.1605 s/iter. Eval: 0.2167 s/iter. Total: 0.3890 s/iter. ETA=0:05:27
[01/18 09:33:58] d2.evaluation.evaluator INFO: Inference done 264/1093. Dataloading: 0.0118 s/iter. Inference: 0.1604 s/iter. Eval: 0.2167 s/iter. Total: 0.3889 s/iter. ETA=0:05:22
[01/18 09:34:03] d2.evaluation.evaluator INFO: Inference done 277/1093. Dataloading: 0.0118 s/iter. Inference: 0.1605 s/iter. Eval: 0.2175 s/iter. Total: 0.3898 s/iter. ETA=0:05:18
[01/18 09:34:08] d2.evaluation.evaluator INFO: Inference done 292/1093. Dataloading: 0.0117 s/iter. Inference: 0.1605 s/iter. Eval: 0.2159 s/iter. Total: 0.3882 s/iter. ETA=0:05:10
[01/18 09:34:14] d2.evaluation.evaluator INFO: Inference done 306/1093. Dataloading: 0.0117 s/iter. Inference: 0.1597 s/iter. Eval: 0.2162 s/iter. Total: 0.3877 s/iter. ETA=0:05:05
[01/18 09:34:19] d2.evaluation.evaluator INFO: Inference done 319/1093. Dataloading: 0.0118 s/iter. Inference: 0.1598 s/iter. Eval: 0.2170 s/iter. Total: 0.3887 s/iter. ETA=0:05:00
[01/18 09:34:24] d2.evaluation.evaluator INFO: Inference done 334/1093. Dataloading: 0.0117 s/iter. Inference: 0.1597 s/iter. Eval: 0.2156 s/iter. Total: 0.3871 s/iter. ETA=0:04:53
[01/18 09:34:29] d2.evaluation.evaluator INFO: Inference done 351/1093. Dataloading: 0.0116 s/iter. Inference: 0.1594 s/iter. Eval: 0.2123 s/iter. Total: 0.3834 s/iter. ETA=0:04:44
[01/18 09:34:35] d2.evaluation.evaluator INFO: Inference done 365/1093. Dataloading: 0.0116 s/iter. Inference: 0.1592 s/iter. Eval: 0.2122 s/iter. Total: 0.3831 s/iter. ETA=0:04:38
[01/18 09:34:40] d2.evaluation.evaluator INFO: Inference done 380/1093. Dataloading: 0.0121 s/iter. Inference: 0.1588 s/iter. Eval: 0.2107 s/iter. Total: 0.3817 s/iter. ETA=0:04:32
[01/18 09:34:45] d2.evaluation.evaluator INFO: Inference done 393/1093. Dataloading: 0.0121 s/iter. Inference: 0.1588 s/iter. Eval: 0.2108 s/iter. Total: 0.3818 s/iter. ETA=0:04:27
[01/18 09:34:50] d2.evaluation.evaluator INFO: Inference done 407/1093. Dataloading: 0.0121 s/iter. Inference: 0.1588 s/iter. Eval: 0.2104 s/iter. Total: 0.3814 s/iter. ETA=0:04:21
[01/18 09:34:55] d2.evaluation.evaluator INFO: Inference done 420/1093. Dataloading: 0.0121 s/iter. Inference: 0.1587 s/iter. Eval: 0.2107 s/iter. Total: 0.3815 s/iter. ETA=0:04:16
[01/18 09:35:00] d2.evaluation.evaluator INFO: Inference done 432/1093. Dataloading: 0.0121 s/iter. Inference: 0.1589 s/iter. Eval: 0.2116 s/iter. Total: 0.3827 s/iter. ETA=0:04:12
[01/18 09:35:05] d2.evaluation.evaluator INFO: Inference done 449/1093. Dataloading: 0.0121 s/iter. Inference: 0.1581 s/iter. Eval: 0.2095 s/iter. Total: 0.3798 s/iter. ETA=0:04:04
[01/18 09:35:11] d2.evaluation.evaluator INFO: Inference done 464/1093. Dataloading: 0.0120 s/iter. Inference: 0.1577 s/iter. Eval: 0.2088 s/iter. Total: 0.3786 s/iter. ETA=0:03:58
[01/18 09:35:16] d2.evaluation.evaluator INFO: Inference done 480/1093. Dataloading: 0.0119 s/iter. Inference: 0.1577 s/iter. Eval: 0.2069 s/iter. Total: 0.3766 s/iter. ETA=0:03:50
[01/18 09:35:21] d2.evaluation.evaluator INFO: Inference done 493/1093. Dataloading: 0.0119 s/iter. Inference: 0.1577 s/iter. Eval: 0.2072 s/iter. Total: 0.3769 s/iter. ETA=0:03:46
[01/18 09:35:26] d2.evaluation.evaluator INFO: Inference done 508/1093. Dataloading: 0.0119 s/iter. Inference: 0.1579 s/iter. Eval: 0.2060 s/iter. Total: 0.3758 s/iter. ETA=0:03:39
[01/18 09:35:31] d2.evaluation.evaluator INFO: Inference done 521/1093. Dataloading: 0.0119 s/iter. Inference: 0.1581 s/iter. Eval: 0.2061 s/iter. Total: 0.3762 s/iter. ETA=0:03:35
[01/18 09:35:36] d2.evaluation.evaluator INFO: Inference done 533/1093. Dataloading: 0.0119 s/iter. Inference: 0.1581 s/iter. Eval: 0.2074 s/iter. Total: 0.3775 s/iter. ETA=0:03:31
[01/18 09:35:41] d2.evaluation.evaluator INFO: Inference done 548/1093. Dataloading: 0.0119 s/iter. Inference: 0.1581 s/iter. Eval: 0.2068 s/iter. Total: 0.3769 s/iter. ETA=0:03:25
[01/18 09:35:47] d2.evaluation.evaluator INFO: Inference done 562/1093. Dataloading: 0.0119 s/iter. Inference: 0.1581 s/iter. Eval: 0.2067 s/iter. Total: 0.3767 s/iter. ETA=0:03:20
[01/18 09:35:52] d2.evaluation.evaluator INFO: Inference done 579/1093. Dataloading: 0.0119 s/iter. Inference: 0.1579 s/iter. Eval: 0.2045 s/iter. Total: 0.3745 s/iter. ETA=0:03:12
[01/18 09:35:57] d2.evaluation.evaluator INFO: Inference done 594/1093. Dataloading: 0.0119 s/iter. Inference: 0.1578 s/iter. Eval: 0.2039 s/iter. Total: 0.3736 s/iter. ETA=0:03:06
[01/18 09:36:02] d2.evaluation.evaluator INFO: Inference done 606/1093. Dataloading: 0.0119 s/iter. Inference: 0.1580 s/iter. Eval: 0.2049 s/iter. Total: 0.3749 s/iter. ETA=0:03:02
[01/18 09:36:07] d2.evaluation.evaluator INFO: Inference done 620/1093. Dataloading: 0.0119 s/iter. Inference: 0.1581 s/iter. Eval: 0.2046 s/iter. Total: 0.3747 s/iter. ETA=0:02:57
[01/18 09:36:12] d2.evaluation.evaluator INFO: Inference done 634/1093. Dataloading: 0.0119 s/iter. Inference: 0.1582 s/iter. Eval: 0.2043 s/iter. Total: 0.3745 s/iter. ETA=0:02:51
[01/18 09:36:18] d2.evaluation.evaluator INFO: Inference done 649/1093. Dataloading: 0.0118 s/iter. Inference: 0.1583 s/iter. Eval: 0.2036 s/iter. Total: 0.3738 s/iter. ETA=0:02:45
[01/18 09:36:23] d2.evaluation.evaluator INFO: Inference done 664/1093. Dataloading: 0.0118 s/iter. Inference: 0.1583 s/iter. Eval: 0.2032 s/iter. Total: 0.3734 s/iter. ETA=0:02:40
[01/18 09:36:28] d2.evaluation.evaluator INFO: Inference done 678/1093. Dataloading: 0.0118 s/iter. Inference: 0.1583 s/iter. Eval: 0.2032 s/iter. Total: 0.3733 s/iter. ETA=0:02:34
[01/18 09:36:33] d2.evaluation.evaluator INFO: Inference done 692/1093. Dataloading: 0.0118 s/iter. Inference: 0.1584 s/iter. Eval: 0.2033 s/iter. Total: 0.3735 s/iter. ETA=0:02:29
[01/18 09:36:39] d2.evaluation.evaluator INFO: Inference done 705/1093. Dataloading: 0.0118 s/iter. Inference: 0.1584 s/iter. Eval: 0.2036 s/iter. Total: 0.3739 s/iter. ETA=0:02:25
[01/18 09:36:44] d2.evaluation.evaluator INFO: Inference done 719/1093. Dataloading: 0.0118 s/iter. Inference: 0.1581 s/iter. Eval: 0.2037 s/iter. Total: 0.3737 s/iter. ETA=0:02:19
[01/18 09:36:49] d2.evaluation.evaluator INFO: Inference done 736/1093. Dataloading: 0.0118 s/iter. Inference: 0.1578 s/iter. Eval: 0.2024 s/iter. Total: 0.3721 s/iter. ETA=0:02:12
[01/18 09:36:54] d2.evaluation.evaluator INFO: Inference done 751/1093. Dataloading: 0.0117 s/iter. Inference: 0.1578 s/iter. Eval: 0.2018 s/iter. Total: 0.3714 s/iter. ETA=0:02:07
[01/18 09:36:59] d2.evaluation.evaluator INFO: Inference done 764/1093. Dataloading: 0.0117 s/iter. Inference: 0.1580 s/iter. Eval: 0.2023 s/iter. Total: 0.3721 s/iter. ETA=0:02:02
[01/18 09:37:05] d2.evaluation.evaluator INFO: Inference done 779/1093. Dataloading: 0.0117 s/iter. Inference: 0.1580 s/iter. Eval: 0.2020 s/iter. Total: 0.3718 s/iter. ETA=0:01:56
[01/18 09:37:10] d2.evaluation.evaluator INFO: Inference done 795/1093. Dataloading: 0.0117 s/iter. Inference: 0.1579 s/iter. Eval: 0.2013 s/iter. Total: 0.3709 s/iter. ETA=0:01:50
[01/18 09:37:15] d2.evaluation.evaluator INFO: Inference done 810/1093. Dataloading: 0.0117 s/iter. Inference: 0.1577 s/iter. Eval: 0.2009 s/iter. Total: 0.3703 s/iter. ETA=0:01:44
[01/18 09:37:20] d2.evaluation.evaluator INFO: Inference done 826/1093. Dataloading: 0.0116 s/iter. Inference: 0.1574 s/iter. Eval: 0.2004 s/iter. Total: 0.3695 s/iter. ETA=0:01:38
[01/18 09:37:25] d2.evaluation.evaluator INFO: Inference done 843/1093. Dataloading: 0.0116 s/iter. Inference: 0.1570 s/iter. Eval: 0.1996 s/iter. Total: 0.3683 s/iter. ETA=0:01:32
[01/18 09:37:31] d2.evaluation.evaluator INFO: Inference done 856/1093. Dataloading: 0.0116 s/iter. Inference: 0.1570 s/iter. Eval: 0.2001 s/iter. Total: 0.3687 s/iter. ETA=0:01:27
[01/18 09:37:36] d2.evaluation.evaluator INFO: Inference done 870/1093. Dataloading: 0.0116 s/iter. Inference: 0.1570 s/iter. Eval: 0.2002 s/iter. Total: 0.3689 s/iter. ETA=0:01:22
[01/18 09:37:41] d2.evaluation.evaluator INFO: Inference done 883/1093. Dataloading: 0.0116 s/iter. Inference: 0.1570 s/iter. Eval: 0.2004 s/iter. Total: 0.3691 s/iter. ETA=0:01:17
[01/18 09:37:46] d2.evaluation.evaluator INFO: Inference done 897/1093. Dataloading: 0.0116 s/iter. Inference: 0.1568 s/iter. Eval: 0.2007 s/iter. Total: 0.3691 s/iter. ETA=0:01:12
[01/18 09:37:51] d2.evaluation.evaluator INFO: Inference done 914/1093. Dataloading: 0.0115 s/iter. Inference: 0.1566 s/iter. Eval: 0.1998 s/iter. Total: 0.3680 s/iter. ETA=0:01:05
[01/18 09:37:57] d2.evaluation.evaluator INFO: Inference done 928/1093. Dataloading: 0.0115 s/iter. Inference: 0.1565 s/iter. Eval: 0.2000 s/iter. Total: 0.3681 s/iter. ETA=0:01:00
[01/18 09:38:02] d2.evaluation.evaluator INFO: Inference done 942/1093. Dataloading: 0.0115 s/iter. Inference: 0.1566 s/iter. Eval: 0.2002 s/iter. Total: 0.3684 s/iter. ETA=0:00:55
[01/18 09:38:07] d2.evaluation.evaluator INFO: Inference done 956/1093. Dataloading: 0.0115 s/iter. Inference: 0.1565 s/iter. Eval: 0.2004 s/iter. Total: 0.3685 s/iter. ETA=0:00:50
[01/18 09:38:12] d2.evaluation.evaluator INFO: Inference done 970/1093. Dataloading: 0.0115 s/iter. Inference: 0.1566 s/iter. Eval: 0.2002 s/iter. Total: 0.3684 s/iter. ETA=0:00:45
[01/18 09:38:17] d2.evaluation.evaluator INFO: Inference done 985/1093. Dataloading: 0.0115 s/iter. Inference: 0.1567 s/iter. Eval: 0.1997 s/iter. Total: 0.3680 s/iter. ETA=0:00:39
[01/18 09:38:23] d2.evaluation.evaluator INFO: Inference done 999/1093. Dataloading: 0.0115 s/iter. Inference: 0.1568 s/iter. Eval: 0.1998 s/iter. Total: 0.3682 s/iter. ETA=0:00:34
[01/18 09:38:28] d2.evaluation.evaluator INFO: Inference done 1013/1093. Dataloading: 0.0115 s/iter. Inference: 0.1568 s/iter. Eval: 0.1998 s/iter. Total: 0.3682 s/iter. ETA=0:00:29
[01/18 09:38:33] d2.evaluation.evaluator INFO: Inference done 1026/1093. Dataloading: 0.0117 s/iter. Inference: 0.1568 s/iter. Eval: 0.1998 s/iter. Total: 0.3684 s/iter. ETA=0:00:24
[01/18 09:38:38] d2.evaluation.evaluator INFO: Inference done 1041/1093. Dataloading: 0.0117 s/iter. Inference: 0.1565 s/iter. Eval: 0.1997 s/iter. Total: 0.3680 s/iter. ETA=0:00:19
[01/18 09:38:43] d2.evaluation.evaluator INFO: Inference done 1057/1093. Dataloading: 0.0117 s/iter. Inference: 0.1563 s/iter. Eval: 0.1994 s/iter. Total: 0.3674 s/iter. ETA=0:00:13
[01/18 09:38:48] d2.evaluation.evaluator INFO: Inference done 1076/1093. Dataloading: 0.0116 s/iter. Inference: 0.1558 s/iter. Eval: 0.1982 s/iter. Total: 0.3657 s/iter. ETA=0:00:06
[01/18 09:38:54] d2.evaluation.evaluator INFO: Inference done 1093/1093. Dataloading: 0.0116 s/iter. Inference: 0.1555 s/iter. Eval: 0.1976 s/iter. Total: 0.3648 s/iter. ETA=0:00:00
[01/18 09:38:54] d2.evaluation.evaluator INFO: Total inference time: 0:06:37.382952 (0.365242 s / iter per device, on 4 devices)
[01/18 09:38:54] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:49 (0.155534 s / iter per device, on 4 devices)
[01/18 09:39:18] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.314749597434918, 'mIoU': 20.58623384632419, 'fwIoU': 43.50648546045336, 'IoU-0': nan, 'IoU-1': 95.68391946996002, 'IoU-2': 46.98711349601667, 'IoU-3': 58.51996852271015, 'IoU-4': 52.37026591254859, 'IoU-5': 46.226011272026824, 'IoU-6': 40.98036528074119, 'IoU-7': 33.2611870501798, 'IoU-8': 25.72164373002037, 'IoU-9': 35.93305467367968, 'IoU-10': 40.83503299285204, 'IoU-11': 49.56266698581142, 'IoU-12': 51.00889064725678, 'IoU-13': 50.89811014176577, 'IoU-14': 51.34528196890129, 'IoU-15': 50.39866859763629, 'IoU-16': 51.741134292596556, 'IoU-17': 47.45232238016794, 'IoU-18': 47.897108162130294, 'IoU-19': 48.382597783278264, 'IoU-20': 48.528151290974726, 'IoU-21': 48.721912953964086, 'IoU-22': 49.414615375638824, 'IoU-23': 47.76078141016473, 'IoU-24': 46.539242152510326, 'IoU-25': 47.20855048129278, 'IoU-26': 44.78560877643542, 'IoU-27': 47.23752671989827, 'IoU-28': 45.101159108952736, 'IoU-29': 47.24933956900936, 'IoU-30': 45.6620644729303, 'IoU-31': 45.542753109945345, 'IoU-32': 44.67625774822447, 'IoU-33': 43.32560839306022, 'IoU-34': 43.54731089383531, 'IoU-35': 44.352942912089745, 'IoU-36': 44.79706562284642, 'IoU-37': 43.02321724669439, 'IoU-38': 43.41895423903971, 'IoU-39': 42.99235757166463, 'IoU-40': 43.61106596447942, 'IoU-41': 41.41069776362835, 'IoU-42': 40.56010576535046, 'IoU-43': 40.18383935194567, 'IoU-44': 39.4432852698477, 'IoU-45': 38.72669796132412, 'IoU-46': 36.83615198933555, 'IoU-47': 36.981189984888616, 'IoU-48': 36.6893663691021, 'IoU-49': 35.6641558667946, 'IoU-50': 35.344331312004776, 'IoU-51': 33.67865627349907, 'IoU-52': 33.885149684345436, 'IoU-53': 33.00394913528996, 'IoU-54': 33.29384734597702, 'IoU-55': 32.31764750502736, 'IoU-56': 31.107752995400894, 'IoU-57': 30.646503680542907, 'IoU-58': 28.82073855284008, 'IoU-59': 27.50916683478807, 'IoU-60': 27.330944481843005, 'IoU-61': 27.058768652637134, 'IoU-62': 26.672920763830536, 'IoU-63': 26.391821264673343, 'IoU-64': 24.86119683387808, 'IoU-65': 24.11945250439035, 'IoU-66': 23.179410733734496, 'IoU-67': 22.46113927113066, 'IoU-68': 22.147182444808884, 'IoU-69': 22.132811571423062, 'IoU-70': 21.693866292916553, 'IoU-71': 19.449338612766955, 'IoU-72': 19.58005548404535, 'IoU-73': 18.334261498129973, 'IoU-74': 19.626599921265992, 'IoU-75': 18.648942484632553, 'IoU-76': 18.970410038849877, 'IoU-77': 17.93786894328873, 'IoU-78': 18.88910960252716, 'IoU-79': 18.88524993125145, 'IoU-80': 18.263135684288635, 'IoU-81': 19.271310089118767, 'IoU-82': 18.60552939729748, 'IoU-83': 18.366780354187462, 'IoU-84': 17.64699165236601, 'IoU-85': 18.979464158550464, 'IoU-86': 17.68512429554875, 'IoU-87': 17.53944911086287, 'IoU-88': 17.669840520142053, 'IoU-89': 16.64810722065423, 'IoU-90': 17.517035501169232, 'IoU-91': 16.383295498834226, 'IoU-92': 16.19373258101638, 'IoU-93': 16.99967594647874, 'IoU-94': 17.015221519312103, 'IoU-95': 17.293095649875806, 'IoU-96': 16.897186259152534, 'IoU-97': 16.476994113002725, 'IoU-98': 16.755241349875543, 'IoU-99': 15.560907682332767, 'IoU-100': 15.195825263863174, 'IoU-101': 15.308072855366802, 'IoU-102': 14.739161451821403, 'IoU-103': 14.27719668954332, 'IoU-104': 13.891177268809981, 'IoU-105': 13.635327304631438, 'IoU-106': 13.214040959389756, 'IoU-107': 13.906132250043598, 'IoU-108': 13.022846673367692, 'IoU-109': 13.689970274994762, 'IoU-110': 13.049153159910881, 'IoU-111': 13.345778779748155, 'IoU-112': 11.812112895243628, 'IoU-113': 12.609699871562025, 'IoU-114': 13.32101899503146, 'IoU-115': 12.679260349692631, 'IoU-116': 11.491403168253582, 'IoU-117': 11.881834867133719, 'IoU-118': 10.83651745329484, 'IoU-119': 13.010945842910992, 'IoU-120': 11.002402483750032, 'IoU-121': 10.855392991781098, 'IoU-122': 11.446722416128448, 'IoU-123': 10.804403762100906, 'IoU-124': 10.811254218548685, 'IoU-125': 9.842499710304697, 'IoU-126': 10.3121024100397, 'IoU-127': 8.555019807957919, 'IoU-128': 9.418801423819804, 'IoU-129': 8.361493368359836, 'IoU-130': 8.052074421495677, 'IoU-131': 7.799730293805472, 'IoU-132': 8.02365306695551, 'IoU-133': 7.621492105641418, 'IoU-134': 6.9314760524873895, 'IoU-135': 7.155739105813209, 'IoU-136': 7.143582193667941, 'IoU-137': 7.05116275470625, 'IoU-138': 6.559216686159338, 'IoU-139': 6.008169051676799, 'IoU-140': 5.953411262349948, 'IoU-141': 4.40288339670469, 'IoU-142': 5.723509189723463, 'IoU-143': 5.498529981098694, 'IoU-144': 6.766830111828556, 'IoU-145': 5.082295290786631, 'IoU-146': 5.7707936510619815, 'IoU-147': 4.4436787516352085, 'IoU-148': 5.68034547915479, 'IoU-149': 4.519248858759261, 'IoU-150': 4.521327114804026, 'IoU-151': 4.324863786349807, 'IoU-152': 5.032111878518871, 'IoU-153': 4.161825014934553, 'IoU-154': 3.1916193175775174, 'IoU-155': 3.8466736902369423, 'IoU-156': 3.1862825840396445, 'IoU-157': 2.626724678446845, 'IoU-158': 3.5269677183595283, 'IoU-159': 3.2807894760958467, 'IoU-160': 3.254309154190376, 'IoU-161': 4.476617160734173, 'IoU-162': 2.6870429566055085, 'IoU-163': 2.100957375587914, 'IoU-164': 1.8379050018229128, 'IoU-165': 3.508792839939233, 'IoU-166': 1.9029134929785103, 'IoU-167': 2.125116044235248, 'IoU-168': 2.6194789644303276, 'IoU-169': 2.0419250839530134, 'IoU-170': 2.2162787601567717, 'IoU-171': 1.9384640267667361, 'IoU-172': 2.092579953562881, 'IoU-173': 2.1723060223827524, 'IoU-174': 1.551244633565706, 'IoU-175': 0.9791527045361936, 'IoU-176': 1.513287349437585, 'IoU-177': 0.8599297348911141, 'IoU-178': 1.4653097893345628, 'IoU-179': 1.1689005234078405, 'IoU-180': 2.142811227769445, 'IoU-181': 2.133905782684189, 'IoU-182': 1.0192497733639327, 'IoU-183': 1.761250250675597, 'IoU-184': 2.3734457893296494, 'IoU-185': 2.218509108921287, 'IoU-186': 2.452971588701687, 'IoU-187': 1.4562023402844637, 'IoU-188': 2.4213772929207265, 'IoU-189': 1.8888204367512218, 'IoU-190': 1.8432890408618854, 'IoU-191': 2.5872115789015706, 'mACC': 30.869714056383867, 'pACC': 57.85271369856196, 'ACC-0': nan, 'ACC-1': 98.73891462928587, 'ACC-2': 57.672755659294495, 'ACC-3': 72.25945174014619, 'ACC-4': 69.47020196128125, 'ACC-5': 63.3998955247671, 'ACC-6': 58.26551254921641, 'ACC-7': 48.49772653518127, 'ACC-8': 33.643249197450054, 'ACC-9': 46.04479437508224, 'ACC-10': 55.605313260958745, 'ACC-11': 65.09680461560784, 'ACC-12': 68.34915356416897, 'ACC-13': 69.39514775915357, 'ACC-14': 68.93420316533381, 'ACC-15': 70.9158828953401, 'ACC-16': 67.61801118073903, 'ACC-17': 65.34980689429933, 'ACC-18': 64.41948969600571, 'ACC-19': 65.35435468285988, 'ACC-20': 64.59635719451681, 'ACC-21': 64.14205711334348, 'ACC-22': 65.32429764259439, 'ACC-23': 64.86772424523537, 'ACC-24': 64.39479286962246, 'ACC-25': 63.959217417149695, 'ACC-26': 64.61928289542206, 'ACC-27': 63.493262981543374, 'ACC-28': 63.3206798762192, 'ACC-29': 63.06102070566484, 'ACC-30': 63.397356737721026, 'ACC-31': 61.75203871929611, 'ACC-32': 61.44552259099193, 'ACC-33': 62.15069783236365, 'ACC-34': 60.62028364696591, 'ACC-35': 60.76296810142633, 'ACC-36': 61.67825214074965, 'ACC-37': 60.336906750449096, 'ACC-38': 60.944958707751915, 'ACC-39': 59.36901319416339, 'ACC-40': 59.63509204388823, 'ACC-41': 59.29694523188478, 'ACC-42': 57.91500926961618, 'ACC-43': 58.590884726229085, 'ACC-44': 56.698522018008134, 'ACC-45': 55.207146602533705, 'ACC-46': 53.95898362349049, 'ACC-47': 54.86108676388526, 'ACC-48': 53.586699602494356, 'ACC-49': 53.02173520507903, 'ACC-50': 52.77364003774686, 'ACC-51': 49.54817290246081, 'ACC-52': 49.714534029312226, 'ACC-53': 48.99428972250723, 'ACC-54': 49.28196886018452, 'ACC-55': 48.152989912338164, 'ACC-56': 47.59748994162244, 'ACC-57': 45.76998537428775, 'ACC-58': 44.21016788173333, 'ACC-59': 41.47162178280363, 'ACC-60': 42.21450793431272, 'ACC-61': 41.83569710511133, 'ACC-62': 40.32852081599806, 'ACC-63': 40.45656827025663, 'ACC-64': 38.40307284757129, 'ACC-65': 38.44612483289126, 'ACC-66': 37.23297047339138, 'ACC-67': 36.33476259711965, 'ACC-68': 36.5600028736993, 'ACC-69': 37.37645824786869, 'ACC-70': 33.675660059330895, 'ACC-71': 34.09801053747519, 'ACC-72': 32.4946188996006, 'ACC-73': 28.421243727815593, 'ACC-74': 32.73630769931267, 'ACC-75': 32.50876068399402, 'ACC-76': 31.94869308498538, 'ACC-77': 30.581033412592383, 'ACC-78': 33.60419879173153, 'ACC-79': 32.14172701607338, 'ACC-80': 32.18348618983635, 'ACC-81': 32.74875708712412, 'ACC-82': 31.424368962209403, 'ACC-83': 30.077543002126077, 'ACC-84': 29.27075189749635, 'ACC-85': 30.82808283953, 'ACC-86': 30.81266486697692, 'ACC-87': 30.068824759785024, 'ACC-88': 30.770120227046093, 'ACC-89': 29.277465790485568, 'ACC-90': 29.195403736288018, 'ACC-91': 28.137484177693178, 'ACC-92': 27.577886294563207, 'ACC-93': 27.992146626554177, 'ACC-94': 30.756159211387395, 'ACC-95': 28.05476769845342, 'ACC-96': 29.320406490133255, 'ACC-97': 27.533420341530285, 'ACC-98': 29.915065337064078, 'ACC-99': 26.69535878778809, 'ACC-100': 26.281842310770738, 'ACC-101': 27.005000249324674, 'ACC-102': 25.200974173085637, 'ACC-103': 24.22957400585624, 'ACC-104': 25.541157374388234, 'ACC-105': 24.30084552595535, 'ACC-106': 22.42899439549164, 'ACC-107': 23.963595855688855, 'ACC-108': 22.08144588795076, 'ACC-109': 22.954486500737794, 'ACC-110': 24.220628338997503, 'ACC-111': 23.123488466038065, 'ACC-112': 21.318897939926824, 'ACC-113': 21.673402792588007, 'ACC-114': 24.57327759469125, 'ACC-115': 22.535444198878416, 'ACC-116': 20.304563340779858, 'ACC-117': 21.40096614475134, 'ACC-118': 18.688145469385905, 'ACC-119': 24.774730145822264, 'ACC-120': 18.375782301450926, 'ACC-121': 20.799572316291542, 'ACC-122': 20.57563663468161, 'ACC-123': 17.959160714391448, 'ACC-124': 19.37860562644205, 'ACC-125': 17.091834237401333, 'ACC-126': 19.921686445231742, 'ACC-127': 15.103811361661169, 'ACC-128': 16.7337487544076, 'ACC-129': 15.004213838332939, 'ACC-130': 14.80434317142319, 'ACC-131': 14.514329310639305, 'ACC-132': 16.75577136384809, 'ACC-133': 13.096409398028127, 'ACC-134': 12.68866867729427, 'ACC-135': 11.994185052163504, 'ACC-136': 13.627323658218412, 'ACC-137': 13.909451640453451, 'ACC-138': 12.847345610071834, 'ACC-139': 11.257525089095907, 'ACC-140': 10.933314780672767, 'ACC-141': 7.016287434421911, 'ACC-142': 10.377089080843728, 'ACC-143': 9.967444037981656, 'ACC-144': 12.799007220216607, 'ACC-145': 7.929962655694448, 'ACC-146': 12.478398939937401, 'ACC-147': 7.06510687726733, 'ACC-148': 11.261241137549415, 'ACC-149': 7.819939578264807, 'ACC-150': 8.881822038359031, 'ACC-151': 6.091872068448432, 'ACC-152': 9.678354069728236, 'ACC-153': 7.563005001923817, 'ACC-154': 6.166142400727045, 'ACC-155': 8.033022484180218, 'ACC-156': 5.251604646202176, 'ACC-157': 4.128166286256367, 'ACC-158': 6.701846444363214, 'ACC-159': 6.597918267472291, 'ACC-160': 5.3301588019725825, 'ACC-161': 7.394187212316437, 'ACC-162': 4.5644017720863665, 'ACC-163': 3.4012721522180116, 'ACC-164': 2.565647405020944, 'ACC-165': 6.078100035892729, 'ACC-166': 2.7044178257548412, 'ACC-167': 3.3998822235771717, 'ACC-168': 8.727406990646317, 'ACC-169': 3.205163074471874, 'ACC-170': 7.267487258117182, 'ACC-171': 2.9499879686044865, 'ACC-172': 4.661999481423862, 'ACC-173': 4.367777597995131, 'ACC-174': 2.4401609749459907, 'ACC-175': 1.2128405798709139, 'ACC-176': 1.9432392036220107, 'ACC-177': 1.135145210373725, 'ACC-178': 2.0194664590862472, 'ACC-179': 1.6298051659528363, 'ACC-180': 4.233338413085441, 'ACC-181': 4.311184065602195, 'ACC-182': 1.4783611987045386, 'ACC-183': 2.956791155288628, 'ACC-184': 8.488338910939891, 'ACC-185': 4.48937359403942, 'ACC-186': 5.037852945967192, 'ACC-187': 2.1597265006825404, 'ACC-188': 5.5301945038537585, 'ACC-189': 8.107136714066764, 'ACC-190': 3.3078487062053936, 'ACC-191': 11.640848287112561})])
[01/18 09:39:18] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 09:39:18] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 09:39:18] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 09:39:18] d2.evaluation.testing INFO: copypaste: 2.3147,20.5862,43.5065,30.8697,57.8527
[01/18 09:39:18] d2.utils.events INFO:  eta: 3:13:10  iter: 31999  total_loss: 31.73  loss_ce: 0.2204  loss_mask: 0.33  loss_dice: 2.549  loss_ce_0: 0.5572  loss_mask_0: 0.3266  loss_dice_0: 2.684  loss_ce_1: 0.2764  loss_mask_1: 0.3317  loss_dice_1: 2.599  loss_ce_2: 0.2465  loss_mask_2: 0.3295  loss_dice_2: 2.583  loss_ce_3: 0.2307  loss_mask_3: 0.3293  loss_dice_3: 2.571  loss_ce_4: 0.2239  loss_mask_4: 0.3294  loss_dice_4: 2.551  loss_ce_5: 0.2183  loss_mask_5: 0.3293  loss_dice_5: 2.565  loss_ce_6: 0.2075  loss_mask_6: 0.3289  loss_dice_6: 2.555  loss_ce_7: 0.2172  loss_mask_7: 0.3289  loss_dice_7: 2.554  loss_ce_8: 0.2194  loss_mask_8: 0.3292  loss_dice_8: 2.557  time: 1.4794  data_time: 0.0713  lr: 2.3495e-06  max_mem: 21589M
[01/18 09:39:48] d2.utils.events INFO:  eta: 3:12:42  iter: 32019  total_loss: 32.49  loss_ce: 0.2421  loss_mask: 0.3422  loss_dice: 2.576  loss_ce_0: 0.5539  loss_mask_0: 0.3401  loss_dice_0: 2.698  loss_ce_1: 0.2941  loss_mask_1: 0.3473  loss_dice_1: 2.617  loss_ce_2: 0.28  loss_mask_2: 0.3427  loss_dice_2: 2.599  loss_ce_3: 0.2598  loss_mask_3: 0.3408  loss_dice_3: 2.58  loss_ce_4: 0.2588  loss_mask_4: 0.34  loss_dice_4: 2.58  loss_ce_5: 0.2576  loss_mask_5: 0.3407  loss_dice_5: 2.583  loss_ce_6: 0.2638  loss_mask_6: 0.3407  loss_dice_6: 2.565  loss_ce_7: 0.2453  loss_mask_7: 0.3422  loss_dice_7: 2.58  loss_ce_8: 0.2468  loss_mask_8: 0.3424  loss_dice_8: 2.574  time: 1.4794  data_time: 0.0663  lr: 2.3442e-06  max_mem: 21589M
[01/18 09:40:16] d2.utils.events INFO:  eta: 3:12:13  iter: 32039  total_loss: 32.28  loss_ce: 0.2494  loss_mask: 0.334  loss_dice: 2.59  loss_ce_0: 0.552  loss_mask_0: 0.3339  loss_dice_0: 2.719  loss_ce_1: 0.2964  loss_mask_1: 0.3378  loss_dice_1: 2.632  loss_ce_2: 0.2792  loss_mask_2: 0.3353  loss_dice_2: 2.612  loss_ce_3: 0.2759  loss_mask_3: 0.3343  loss_dice_3: 2.596  loss_ce_4: 0.2709  loss_mask_4: 0.3338  loss_dice_4: 2.595  loss_ce_5: 0.2522  loss_mask_5: 0.3335  loss_dice_5: 2.596  loss_ce_6: 0.236  loss_mask_6: 0.3328  loss_dice_6: 2.591  loss_ce_7: 0.242  loss_mask_7: 0.3345  loss_dice_7: 2.586  loss_ce_8: 0.2583  loss_mask_8: 0.3339  loss_dice_8: 2.587  time: 1.4794  data_time: 0.0695  lr: 2.3389e-06  max_mem: 21589M
[01/18 09:40:45] d2.utils.events INFO:  eta: 3:11:44  iter: 32059  total_loss: 31.98  loss_ce: 0.2421  loss_mask: 0.3321  loss_dice: 2.557  loss_ce_0: 0.5602  loss_mask_0: 0.3341  loss_dice_0: 2.683  loss_ce_1: 0.2804  loss_mask_1: 0.3432  loss_dice_1: 2.603  loss_ce_2: 0.2745  loss_mask_2: 0.3372  loss_dice_2: 2.586  loss_ce_3: 0.2554  loss_mask_3: 0.3348  loss_dice_3: 2.569  loss_ce_4: 0.2648  loss_mask_4: 0.3334  loss_dice_4: 2.568  loss_ce_5: 0.2488  loss_mask_5: 0.331  loss_dice_5: 2.56  loss_ce_6: 0.2372  loss_mask_6: 0.3302  loss_dice_6: 2.567  loss_ce_7: 0.252  loss_mask_7: 0.332  loss_dice_7: 2.557  loss_ce_8: 0.2389  loss_mask_8: 0.3327  loss_dice_8: 2.556  time: 1.4794  data_time: 0.0699  lr: 2.3336e-06  max_mem: 21589M
[01/18 09:41:14] d2.utils.events INFO:  eta: 3:11:17  iter: 32079  total_loss: 32.24  loss_ce: 0.2287  loss_mask: 0.3312  loss_dice: 2.592  loss_ce_0: 0.5526  loss_mask_0: 0.336  loss_dice_0: 2.738  loss_ce_1: 0.2702  loss_mask_1: 0.3385  loss_dice_1: 2.648  loss_ce_2: 0.2691  loss_mask_2: 0.3341  loss_dice_2: 2.616  loss_ce_3: 0.232  loss_mask_3: 0.3319  loss_dice_3: 2.601  loss_ce_4: 0.2561  loss_mask_4: 0.3328  loss_dice_4: 2.598  loss_ce_5: 0.2249  loss_mask_5: 0.332  loss_dice_5: 2.602  loss_ce_6: 0.2305  loss_mask_6: 0.3331  loss_dice_6: 2.594  loss_ce_7: 0.2428  loss_mask_7: 0.3317  loss_dice_7: 2.596  loss_ce_8: 0.2403  loss_mask_8: 0.3327  loss_dice_8: 2.592  time: 1.4794  data_time: 0.0705  lr: 2.3283e-06  max_mem: 21589M
[01/18 09:41:44] d2.utils.events INFO:  eta: 3:10:48  iter: 32099  total_loss: 32.64  loss_ce: 0.221  loss_mask: 0.3413  loss_dice: 2.626  loss_ce_0: 0.5552  loss_mask_0: 0.3363  loss_dice_0: 2.767  loss_ce_1: 0.2813  loss_mask_1: 0.3446  loss_dice_1: 2.678  loss_ce_2: 0.2732  loss_mask_2: 0.3435  loss_dice_2: 2.65  loss_ce_3: 0.243  loss_mask_3: 0.3424  loss_dice_3: 2.644  loss_ce_4: 0.2436  loss_mask_4: 0.3417  loss_dice_4: 2.637  loss_ce_5: 0.2446  loss_mask_5: 0.3413  loss_dice_5: 2.635  loss_ce_6: 0.2255  loss_mask_6: 0.3402  loss_dice_6: 2.628  loss_ce_7: 0.2265  loss_mask_7: 0.3411  loss_dice_7: 2.638  loss_ce_8: 0.2359  loss_mask_8: 0.342  loss_dice_8: 2.629  time: 1.4793  data_time: 0.0698  lr: 2.3231e-06  max_mem: 21589M
[01/18 09:42:13] d2.utils.events INFO:  eta: 3:10:24  iter: 32119  total_loss: 32.05  loss_ce: 0.2206  loss_mask: 0.3272  loss_dice: 2.571  loss_ce_0: 0.5536  loss_mask_0: 0.3276  loss_dice_0: 2.709  loss_ce_1: 0.2697  loss_mask_1: 0.333  loss_dice_1: 2.619  loss_ce_2: 0.2429  loss_mask_2: 0.3301  loss_dice_2: 2.594  loss_ce_3: 0.2336  loss_mask_3: 0.3305  loss_dice_3: 2.573  loss_ce_4: 0.2242  loss_mask_4: 0.3295  loss_dice_4: 2.577  loss_ce_5: 0.229  loss_mask_5: 0.3269  loss_dice_5: 2.57  loss_ce_6: 0.2306  loss_mask_6: 0.3273  loss_dice_6: 2.563  loss_ce_7: 0.2186  loss_mask_7: 0.3277  loss_dice_7: 2.565  loss_ce_8: 0.2301  loss_mask_8: 0.3283  loss_dice_8: 2.576  time: 1.4793  data_time: 0.0718  lr: 2.3178e-06  max_mem: 21589M
[01/18 09:42:42] d2.utils.events INFO:  eta: 3:09:57  iter: 32139  total_loss: 32.77  loss_ce: 0.2389  loss_mask: 0.3361  loss_dice: 2.581  loss_ce_0: 0.5737  loss_mask_0: 0.33  loss_dice_0: 2.738  loss_ce_1: 0.3041  loss_mask_1: 0.3402  loss_dice_1: 2.635  loss_ce_2: 0.298  loss_mask_2: 0.3356  loss_dice_2: 2.612  loss_ce_3: 0.2627  loss_mask_3: 0.3364  loss_dice_3: 2.592  loss_ce_4: 0.2605  loss_mask_4: 0.3364  loss_dice_4: 2.587  loss_ce_5: 0.2709  loss_mask_5: 0.3349  loss_dice_5: 2.6  loss_ce_6: 0.2589  loss_mask_6: 0.3344  loss_dice_6: 2.584  loss_ce_7: 0.2429  loss_mask_7: 0.3366  loss_dice_7: 2.587  loss_ce_8: 0.2453  loss_mask_8: 0.336  loss_dice_8: 2.577  time: 1.4793  data_time: 0.0704  lr: 2.3125e-06  max_mem: 21589M
[01/18 09:43:11] d2.utils.events INFO:  eta: 3:09:23  iter: 32159  total_loss: 32.15  loss_ce: 0.2322  loss_mask: 0.3281  loss_dice: 2.594  loss_ce_0: 0.5569  loss_mask_0: 0.3258  loss_dice_0: 2.727  loss_ce_1: 0.2705  loss_mask_1: 0.3331  loss_dice_1: 2.638  loss_ce_2: 0.2787  loss_mask_2: 0.3292  loss_dice_2: 2.621  loss_ce_3: 0.2593  loss_mask_3: 0.3283  loss_dice_3: 2.602  loss_ce_4: 0.2405  loss_mask_4: 0.3279  loss_dice_4: 2.603  loss_ce_5: 0.2271  loss_mask_5: 0.3276  loss_dice_5: 2.597  loss_ce_6: 0.2304  loss_mask_6: 0.3291  loss_dice_6: 2.598  loss_ce_7: 0.2298  loss_mask_7: 0.33  loss_dice_7: 2.598  loss_ce_8: 0.2328  loss_mask_8: 0.3287  loss_dice_8: 2.602  time: 1.4793  data_time: 0.0683  lr: 2.3072e-06  max_mem: 21589M
[01/18 09:43:39] d2.utils.events INFO:  eta: 3:08:54  iter: 32179  total_loss: 31.79  loss_ce: 0.2431  loss_mask: 0.3344  loss_dice: 2.564  loss_ce_0: 0.5587  loss_mask_0: 0.3324  loss_dice_0: 2.67  loss_ce_1: 0.2913  loss_mask_1: 0.3381  loss_dice_1: 2.595  loss_ce_2: 0.2817  loss_mask_2: 0.3358  loss_dice_2: 2.571  loss_ce_3: 0.2437  loss_mask_3: 0.3347  loss_dice_3: 2.568  loss_ce_4: 0.2362  loss_mask_4: 0.3339  loss_dice_4: 2.558  loss_ce_5: 0.2442  loss_mask_5: 0.334  loss_dice_5: 2.558  loss_ce_6: 0.241  loss_mask_6: 0.3332  loss_dice_6: 2.552  loss_ce_7: 0.2395  loss_mask_7: 0.3334  loss_dice_7: 2.562  loss_ce_8: 0.2466  loss_mask_8: 0.3323  loss_dice_8: 2.56  time: 1.4793  data_time: 0.0707  lr: 2.3019e-06  max_mem: 21589M
[01/18 09:44:08] d2.utils.events INFO:  eta: 3:08:24  iter: 32199  total_loss: 32.57  loss_ce: 0.2372  loss_mask: 0.3319  loss_dice: 2.583  loss_ce_0: 0.5431  loss_mask_0: 0.3338  loss_dice_0: 2.726  loss_ce_1: 0.2848  loss_mask_1: 0.3402  loss_dice_1: 2.632  loss_ce_2: 0.2796  loss_mask_2: 0.3355  loss_dice_2: 2.612  loss_ce_3: 0.2718  loss_mask_3: 0.3316  loss_dice_3: 2.594  loss_ce_4: 0.2569  loss_mask_4: 0.3309  loss_dice_4: 2.593  loss_ce_5: 0.2515  loss_mask_5: 0.3303  loss_dice_5: 2.586  loss_ce_6: 0.2296  loss_mask_6: 0.3294  loss_dice_6: 2.583  loss_ce_7: 0.2321  loss_mask_7: 0.3302  loss_dice_7: 2.577  loss_ce_8: 0.2451  loss_mask_8: 0.3312  loss_dice_8: 2.583  time: 1.4792  data_time: 0.0655  lr: 2.2966e-06  max_mem: 21589M
[01/18 09:44:37] d2.utils.events INFO:  eta: 3:07:52  iter: 32219  total_loss: 31.86  loss_ce: 0.2273  loss_mask: 0.3279  loss_dice: 2.546  loss_ce_0: 0.551  loss_mask_0: 0.3193  loss_dice_0: 2.685  loss_ce_1: 0.2958  loss_mask_1: 0.3326  loss_dice_1: 2.59  loss_ce_2: 0.2857  loss_mask_2: 0.3296  loss_dice_2: 2.567  loss_ce_3: 0.2415  loss_mask_3: 0.3283  loss_dice_3: 2.555  loss_ce_4: 0.2421  loss_mask_4: 0.329  loss_dice_4: 2.552  loss_ce_5: 0.2433  loss_mask_5: 0.3277  loss_dice_5: 2.56  loss_ce_6: 0.2506  loss_mask_6: 0.3287  loss_dice_6: 2.544  loss_ce_7: 0.2343  loss_mask_7: 0.3275  loss_dice_7: 2.55  loss_ce_8: 0.2381  loss_mask_8: 0.3269  loss_dice_8: 2.543  time: 1.4792  data_time: 0.0691  lr: 2.2913e-06  max_mem: 21589M
[01/18 09:45:06] d2.utils.events INFO:  eta: 3:07:24  iter: 32239  total_loss: 32.39  loss_ce: 0.2286  loss_mask: 0.3376  loss_dice: 2.616  loss_ce_0: 0.55  loss_mask_0: 0.3379  loss_dice_0: 2.743  loss_ce_1: 0.2518  loss_mask_1: 0.3436  loss_dice_1: 2.658  loss_ce_2: 0.2695  loss_mask_2: 0.3386  loss_dice_2: 2.631  loss_ce_3: 0.2469  loss_mask_3: 0.337  loss_dice_3: 2.63  loss_ce_4: 0.2404  loss_mask_4: 0.3359  loss_dice_4: 2.622  loss_ce_5: 0.2367  loss_mask_5: 0.3375  loss_dice_5: 2.614  loss_ce_6: 0.2376  loss_mask_6: 0.3379  loss_dice_6: 2.61  loss_ce_7: 0.2295  loss_mask_7: 0.3368  loss_dice_7: 2.622  loss_ce_8: 0.2243  loss_mask_8: 0.3377  loss_dice_8: 2.62  time: 1.4792  data_time: 0.0679  lr: 2.286e-06  max_mem: 21589M
[01/18 09:45:35] d2.utils.events INFO:  eta: 3:06:55  iter: 32259  total_loss: 32.28  loss_ce: 0.2351  loss_mask: 0.3324  loss_dice: 2.567  loss_ce_0: 0.5585  loss_mask_0: 0.3312  loss_dice_0: 2.722  loss_ce_1: 0.2731  loss_mask_1: 0.3362  loss_dice_1: 2.621  loss_ce_2: 0.2769  loss_mask_2: 0.3344  loss_dice_2: 2.588  loss_ce_3: 0.2623  loss_mask_3: 0.3319  loss_dice_3: 2.588  loss_ce_4: 0.2511  loss_mask_4: 0.3325  loss_dice_4: 2.578  loss_ce_5: 0.2432  loss_mask_5: 0.3327  loss_dice_5: 2.57  loss_ce_6: 0.2216  loss_mask_6: 0.3323  loss_dice_6: 2.575  loss_ce_7: 0.2386  loss_mask_7: 0.3332  loss_dice_7: 2.579  loss_ce_8: 0.2262  loss_mask_8: 0.3333  loss_dice_8: 2.566  time: 1.4792  data_time: 0.0668  lr: 2.2807e-06  max_mem: 21589M
[01/18 09:46:04] d2.utils.events INFO:  eta: 3:06:23  iter: 32279  total_loss: 31.85  loss_ce: 0.2082  loss_mask: 0.3336  loss_dice: 2.552  loss_ce_0: 0.5663  loss_mask_0: 0.33  loss_dice_0: 2.685  loss_ce_1: 0.276  loss_mask_1: 0.3364  loss_dice_1: 2.602  loss_ce_2: 0.2504  loss_mask_2: 0.3351  loss_dice_2: 2.571  loss_ce_3: 0.2412  loss_mask_3: 0.3339  loss_dice_3: 2.555  loss_ce_4: 0.2425  loss_mask_4: 0.3334  loss_dice_4: 2.552  loss_ce_5: 0.2398  loss_mask_5: 0.3327  loss_dice_5: 2.555  loss_ce_6: 0.2249  loss_mask_6: 0.3349  loss_dice_6: 2.552  loss_ce_7: 0.2165  loss_mask_7: 0.335  loss_dice_7: 2.549  loss_ce_8: 0.2152  loss_mask_8: 0.3357  loss_dice_8: 2.542  time: 1.4792  data_time: 0.0710  lr: 2.2754e-06  max_mem: 21589M
[01/18 09:46:33] d2.utils.events INFO:  eta: 3:05:50  iter: 32299  total_loss: 32.59  loss_ce: 0.2302  loss_mask: 0.3384  loss_dice: 2.598  loss_ce_0: 0.5507  loss_mask_0: 0.341  loss_dice_0: 2.741  loss_ce_1: 0.2838  loss_mask_1: 0.3439  loss_dice_1: 2.658  loss_ce_2: 0.2624  loss_mask_2: 0.342  loss_dice_2: 2.626  loss_ce_3: 0.261  loss_mask_3: 0.3387  loss_dice_3: 2.62  loss_ce_4: 0.2569  loss_mask_4: 0.3372  loss_dice_4: 2.611  loss_ce_5: 0.2437  loss_mask_5: 0.339  loss_dice_5: 2.611  loss_ce_6: 0.2489  loss_mask_6: 0.3373  loss_dice_6: 2.605  loss_ce_7: 0.232  loss_mask_7: 0.337  loss_dice_7: 2.599  loss_ce_8: 0.2377  loss_mask_8: 0.3378  loss_dice_8: 2.607  time: 1.4791  data_time: 0.0699  lr: 2.2701e-06  max_mem: 21589M
[01/18 09:47:02] d2.utils.events INFO:  eta: 3:05:22  iter: 32319  total_loss: 32.03  loss_ce: 0.2289  loss_mask: 0.329  loss_dice: 2.595  loss_ce_0: 0.5485  loss_mask_0: 0.3255  loss_dice_0: 2.725  loss_ce_1: 0.2654  loss_mask_1: 0.3346  loss_dice_1: 2.639  loss_ce_2: 0.2679  loss_mask_2: 0.3316  loss_dice_2: 2.607  loss_ce_3: 0.2527  loss_mask_3: 0.3286  loss_dice_3: 2.598  loss_ce_4: 0.2407  loss_mask_4: 0.3283  loss_dice_4: 2.593  loss_ce_5: 0.2239  loss_mask_5: 0.3286  loss_dice_5: 2.6  loss_ce_6: 0.2189  loss_mask_6: 0.3289  loss_dice_6: 2.59  loss_ce_7: 0.2174  loss_mask_7: 0.3276  loss_dice_7: 2.593  loss_ce_8: 0.2224  loss_mask_8: 0.3286  loss_dice_8: 2.602  time: 1.4791  data_time: 0.0711  lr: 2.2648e-06  max_mem: 21589M
[01/18 09:47:31] d2.utils.events INFO:  eta: 3:04:53  iter: 32339  total_loss: 32.03  loss_ce: 0.2106  loss_mask: 0.3291  loss_dice: 2.575  loss_ce_0: 0.534  loss_mask_0: 0.3274  loss_dice_0: 2.718  loss_ce_1: 0.2665  loss_mask_1: 0.3335  loss_dice_1: 2.632  loss_ce_2: 0.2679  loss_mask_2: 0.3293  loss_dice_2: 2.605  loss_ce_3: 0.2441  loss_mask_3: 0.3309  loss_dice_3: 2.592  loss_ce_4: 0.2363  loss_mask_4: 0.3298  loss_dice_4: 2.58  loss_ce_5: 0.2299  loss_mask_5: 0.3288  loss_dice_5: 2.584  loss_ce_6: 0.2196  loss_mask_6: 0.3294  loss_dice_6: 2.579  loss_ce_7: 0.2209  loss_mask_7: 0.3289  loss_dice_7: 2.578  loss_ce_8: 0.213  loss_mask_8: 0.3282  loss_dice_8: 2.576  time: 1.4791  data_time: 0.0671  lr: 2.2595e-06  max_mem: 21589M
[01/18 09:48:01] d2.utils.events INFO:  eta: 3:04:27  iter: 32359  total_loss: 31.66  loss_ce: 0.2311  loss_mask: 0.3364  loss_dice: 2.553  loss_ce_0: 0.5516  loss_mask_0: 0.3287  loss_dice_0: 2.701  loss_ce_1: 0.2786  loss_mask_1: 0.3413  loss_dice_1: 2.603  loss_ce_2: 0.2584  loss_mask_2: 0.3368  loss_dice_2: 2.569  loss_ce_3: 0.2633  loss_mask_3: 0.3367  loss_dice_3: 2.563  loss_ce_4: 0.2419  loss_mask_4: 0.3356  loss_dice_4: 2.555  loss_ce_5: 0.24  loss_mask_5: 0.3376  loss_dice_5: 2.564  loss_ce_6: 0.2265  loss_mask_6: 0.3373  loss_dice_6: 2.556  loss_ce_7: 0.2315  loss_mask_7: 0.3373  loss_dice_7: 2.563  loss_ce_8: 0.2222  loss_mask_8: 0.3352  loss_dice_8: 2.557  time: 1.4791  data_time: 0.0698  lr: 2.2541e-06  max_mem: 21589M
[01/18 09:48:30] d2.utils.events INFO:  eta: 3:04:00  iter: 32379  total_loss: 32.31  loss_ce: 0.2226  loss_mask: 0.3342  loss_dice: 2.593  loss_ce_0: 0.5304  loss_mask_0: 0.3341  loss_dice_0: 2.729  loss_ce_1: 0.292  loss_mask_1: 0.3391  loss_dice_1: 2.64  loss_ce_2: 0.2714  loss_mask_2: 0.3374  loss_dice_2: 2.614  loss_ce_3: 0.2513  loss_mask_3: 0.3357  loss_dice_3: 2.602  loss_ce_4: 0.2405  loss_mask_4: 0.3355  loss_dice_4: 2.592  loss_ce_5: 0.2419  loss_mask_5: 0.3343  loss_dice_5: 2.591  loss_ce_6: 0.2313  loss_mask_6: 0.3324  loss_dice_6: 2.592  loss_ce_7: 0.2401  loss_mask_7: 0.3324  loss_dice_7: 2.588  loss_ce_8: 0.2264  loss_mask_8: 0.3349  loss_dice_8: 2.586  time: 1.4791  data_time: 0.0664  lr: 2.2488e-06  max_mem: 21589M
[01/18 09:48:59] d2.utils.events INFO:  eta: 3:03:34  iter: 32399  total_loss: 32.81  loss_ce: 0.2474  loss_mask: 0.3294  loss_dice: 2.619  loss_ce_0: 0.5669  loss_mask_0: 0.333  loss_dice_0: 2.757  loss_ce_1: 0.3078  loss_mask_1: 0.3334  loss_dice_1: 2.677  loss_ce_2: 0.31  loss_mask_2: 0.3315  loss_dice_2: 2.657  loss_ce_3: 0.2768  loss_mask_3: 0.3315  loss_dice_3: 2.635  loss_ce_4: 0.2537  loss_mask_4: 0.3298  loss_dice_4: 2.633  loss_ce_5: 0.2408  loss_mask_5: 0.3301  loss_dice_5: 2.639  loss_ce_6: 0.2458  loss_mask_6: 0.33  loss_dice_6: 2.628  loss_ce_7: 0.241  loss_mask_7: 0.3277  loss_dice_7: 2.633  loss_ce_8: 0.2372  loss_mask_8: 0.3309  loss_dice_8: 2.628  time: 1.4791  data_time: 0.0690  lr: 2.2435e-06  max_mem: 21589M
[01/18 09:49:28] d2.utils.events INFO:  eta: 3:03:05  iter: 32419  total_loss: 32.34  loss_ce: 0.2253  loss_mask: 0.3262  loss_dice: 2.603  loss_ce_0: 0.5512  loss_mask_0: 0.3305  loss_dice_0: 2.737  loss_ce_1: 0.2725  loss_mask_1: 0.3315  loss_dice_1: 2.646  loss_ce_2: 0.2515  loss_mask_2: 0.3269  loss_dice_2: 2.64  loss_ce_3: 0.2497  loss_mask_3: 0.3261  loss_dice_3: 2.616  loss_ce_4: 0.2389  loss_mask_4: 0.3262  loss_dice_4: 2.616  loss_ce_5: 0.2211  loss_mask_5: 0.326  loss_dice_5: 2.619  loss_ce_6: 0.2151  loss_mask_6: 0.3252  loss_dice_6: 2.609  loss_ce_7: 0.2197  loss_mask_7: 0.327  loss_dice_7: 2.609  loss_ce_8: 0.226  loss_mask_8: 0.3276  loss_dice_8: 2.604  time: 1.4791  data_time: 0.0672  lr: 2.2382e-06  max_mem: 21589M
[01/18 09:49:57] d2.utils.events INFO:  eta: 3:02:34  iter: 32439  total_loss: 32.89  loss_ce: 0.2267  loss_mask: 0.3348  loss_dice: 2.675  loss_ce_0: 0.5567  loss_mask_0: 0.3342  loss_dice_0: 2.808  loss_ce_1: 0.2861  loss_mask_1: 0.341  loss_dice_1: 2.708  loss_ce_2: 0.2868  loss_mask_2: 0.3374  loss_dice_2: 2.704  loss_ce_3: 0.2437  loss_mask_3: 0.3363  loss_dice_3: 2.685  loss_ce_4: 0.24  loss_mask_4: 0.336  loss_dice_4: 2.681  loss_ce_5: 0.2363  loss_mask_5: 0.3366  loss_dice_5: 2.689  loss_ce_6: 0.2341  loss_mask_6: 0.3349  loss_dice_6: 2.688  loss_ce_7: 0.2404  loss_mask_7: 0.3353  loss_dice_7: 2.68  loss_ce_8: 0.2297  loss_mask_8: 0.3335  loss_dice_8: 2.673  time: 1.4790  data_time: 0.0713  lr: 2.2329e-06  max_mem: 21589M
[01/18 09:50:26] d2.utils.events INFO:  eta: 3:02:05  iter: 32459  total_loss: 32.06  loss_ce: 0.226  loss_mask: 0.3361  loss_dice: 2.544  loss_ce_0: 0.5577  loss_mask_0: 0.3319  loss_dice_0: 2.689  loss_ce_1: 0.2702  loss_mask_1: 0.3395  loss_dice_1: 2.6  loss_ce_2: 0.2624  loss_mask_2: 0.3362  loss_dice_2: 2.576  loss_ce_3: 0.2571  loss_mask_3: 0.336  loss_dice_3: 2.558  loss_ce_4: 0.233  loss_mask_4: 0.336  loss_dice_4: 2.556  loss_ce_5: 0.234  loss_mask_5: 0.3353  loss_dice_5: 2.557  loss_ce_6: 0.2232  loss_mask_6: 0.3352  loss_dice_6: 2.555  loss_ce_7: 0.2278  loss_mask_7: 0.335  loss_dice_7: 2.557  loss_ce_8: 0.2354  loss_mask_8: 0.336  loss_dice_8: 2.545  time: 1.4790  data_time: 0.0672  lr: 2.2276e-06  max_mem: 21589M
[01/18 09:50:55] d2.utils.events INFO:  eta: 3:01:38  iter: 32479  total_loss: 31.96  loss_ce: 0.2212  loss_mask: 0.3286  loss_dice: 2.547  loss_ce_0: 0.5316  loss_mask_0: 0.3228  loss_dice_0: 2.691  loss_ce_1: 0.277  loss_mask_1: 0.334  loss_dice_1: 2.597  loss_ce_2: 0.2765  loss_mask_2: 0.332  loss_dice_2: 2.571  loss_ce_3: 0.2418  loss_mask_3: 0.3297  loss_dice_3: 2.566  loss_ce_4: 0.245  loss_mask_4: 0.3278  loss_dice_4: 2.56  loss_ce_5: 0.221  loss_mask_5: 0.3288  loss_dice_5: 2.562  loss_ce_6: 0.2411  loss_mask_6: 0.3296  loss_dice_6: 2.559  loss_ce_7: 0.2305  loss_mask_7: 0.3292  loss_dice_7: 2.556  loss_ce_8: 0.2146  loss_mask_8: 0.3288  loss_dice_8: 2.556  time: 1.4790  data_time: 0.0691  lr: 2.2223e-06  max_mem: 21589M
[01/18 09:51:24] d2.utils.events INFO:  eta: 3:01:10  iter: 32499  total_loss: 32.29  loss_ce: 0.2218  loss_mask: 0.329  loss_dice: 2.613  loss_ce_0: 0.5851  loss_mask_0: 0.3241  loss_dice_0: 2.75  loss_ce_1: 0.2779  loss_mask_1: 0.3302  loss_dice_1: 2.661  loss_ce_2: 0.2701  loss_mask_2: 0.3296  loss_dice_2: 2.638  loss_ce_3: 0.2524  loss_mask_3: 0.3296  loss_dice_3: 2.626  loss_ce_4: 0.2372  loss_mask_4: 0.3275  loss_dice_4: 2.62  loss_ce_5: 0.2373  loss_mask_5: 0.3275  loss_dice_5: 2.618  loss_ce_6: 0.239  loss_mask_6: 0.3289  loss_dice_6: 2.609  loss_ce_7: 0.2212  loss_mask_7: 0.3273  loss_dice_7: 2.618  loss_ce_8: 0.2235  loss_mask_8: 0.328  loss_dice_8: 2.61  time: 1.4790  data_time: 0.0697  lr: 2.2169e-06  max_mem: 21589M
[01/18 09:51:54] d2.utils.events INFO:  eta: 3:00:44  iter: 32519  total_loss: 32.08  loss_ce: 0.2309  loss_mask: 0.3282  loss_dice: 2.589  loss_ce_0: 0.5393  loss_mask_0: 0.3295  loss_dice_0: 2.73  loss_ce_1: 0.2737  loss_mask_1: 0.3335  loss_dice_1: 2.645  loss_ce_2: 0.2605  loss_mask_2: 0.3305  loss_dice_2: 2.616  loss_ce_3: 0.251  loss_mask_3: 0.3297  loss_dice_3: 2.6  loss_ce_4: 0.2423  loss_mask_4: 0.3292  loss_dice_4: 2.594  loss_ce_5: 0.232  loss_mask_5: 0.3309  loss_dice_5: 2.595  loss_ce_6: 0.2342  loss_mask_6: 0.3288  loss_dice_6: 2.592  loss_ce_7: 0.2192  loss_mask_7: 0.3289  loss_dice_7: 2.591  loss_ce_8: 0.2376  loss_mask_8: 0.3299  loss_dice_8: 2.601  time: 1.4790  data_time: 0.0694  lr: 2.2116e-06  max_mem: 21589M
[01/18 09:52:23] d2.utils.events INFO:  eta: 3:00:12  iter: 32539  total_loss: 31.8  loss_ce: 0.2192  loss_mask: 0.3285  loss_dice: 2.576  loss_ce_0: 0.5473  loss_mask_0: 0.3221  loss_dice_0: 2.718  loss_ce_1: 0.2461  loss_mask_1: 0.3299  loss_dice_1: 2.623  loss_ce_2: 0.2528  loss_mask_2: 0.3285  loss_dice_2: 2.597  loss_ce_3: 0.2567  loss_mask_3: 0.3284  loss_dice_3: 2.579  loss_ce_4: 0.2365  loss_mask_4: 0.3277  loss_dice_4: 2.582  loss_ce_5: 0.2331  loss_mask_5: 0.3277  loss_dice_5: 2.591  loss_ce_6: 0.2148  loss_mask_6: 0.3264  loss_dice_6: 2.575  loss_ce_7: 0.2158  loss_mask_7: 0.3284  loss_dice_7: 2.58  loss_ce_8: 0.2308  loss_mask_8: 0.3279  loss_dice_8: 2.574  time: 1.4789  data_time: 0.0698  lr: 2.2063e-06  max_mem: 21589M
[01/18 09:52:52] d2.utils.events INFO:  eta: 2:59:44  iter: 32559  total_loss: 32.89  loss_ce: 0.2438  loss_mask: 0.341  loss_dice: 2.599  loss_ce_0: 0.5826  loss_mask_0: 0.3438  loss_dice_0: 2.722  loss_ce_1: 0.3035  loss_mask_1: 0.3495  loss_dice_1: 2.653  loss_ce_2: 0.295  loss_mask_2: 0.343  loss_dice_2: 2.627  loss_ce_3: 0.2693  loss_mask_3: 0.3407  loss_dice_3: 2.616  loss_ce_4: 0.257  loss_mask_4: 0.3396  loss_dice_4: 2.61  loss_ce_5: 0.2484  loss_mask_5: 0.3416  loss_dice_5: 2.608  loss_ce_6: 0.242  loss_mask_6: 0.3399  loss_dice_6: 2.6  loss_ce_7: 0.2382  loss_mask_7: 0.3414  loss_dice_7: 2.601  loss_ce_8: 0.2441  loss_mask_8: 0.3409  loss_dice_8: 2.6  time: 1.4789  data_time: 0.0730  lr: 2.201e-06  max_mem: 21589M
[01/18 09:53:21] d2.utils.events INFO:  eta: 2:59:16  iter: 32579  total_loss: 31.7  loss_ce: 0.2203  loss_mask: 0.3219  loss_dice: 2.55  loss_ce_0: 0.5159  loss_mask_0: 0.3191  loss_dice_0: 2.702  loss_ce_1: 0.2741  loss_mask_1: 0.329  loss_dice_1: 2.602  loss_ce_2: 0.2545  loss_mask_2: 0.3245  loss_dice_2: 2.576  loss_ce_3: 0.2491  loss_mask_3: 0.3221  loss_dice_3: 2.567  loss_ce_4: 0.2451  loss_mask_4: 0.3225  loss_dice_4: 2.554  loss_ce_5: 0.2403  loss_mask_5: 0.3213  loss_dice_5: 2.566  loss_ce_6: 0.2245  loss_mask_6: 0.3211  loss_dice_6: 2.557  loss_ce_7: 0.2251  loss_mask_7: 0.3218  loss_dice_7: 2.555  loss_ce_8: 0.2274  loss_mask_8: 0.3214  loss_dice_8: 2.555  time: 1.4789  data_time: 0.0738  lr: 2.1956e-06  max_mem: 21589M
[01/18 09:53:50] d2.utils.events INFO:  eta: 2:58:48  iter: 32599  total_loss: 32.36  loss_ce: 0.2271  loss_mask: 0.3354  loss_dice: 2.595  loss_ce_0: 0.5395  loss_mask_0: 0.3387  loss_dice_0: 2.732  loss_ce_1: 0.2587  loss_mask_1: 0.3442  loss_dice_1: 2.647  loss_ce_2: 0.2667  loss_mask_2: 0.3409  loss_dice_2: 2.621  loss_ce_3: 0.2343  loss_mask_3: 0.3382  loss_dice_3: 2.603  loss_ce_4: 0.2357  loss_mask_4: 0.3388  loss_dice_4: 2.606  loss_ce_5: 0.2232  loss_mask_5: 0.3371  loss_dice_5: 2.603  loss_ce_6: 0.2205  loss_mask_6: 0.3365  loss_dice_6: 2.601  loss_ce_7: 0.2081  loss_mask_7: 0.3348  loss_dice_7: 2.59  loss_ce_8: 0.2124  loss_mask_8: 0.335  loss_dice_8: 2.595  time: 1.4789  data_time: 0.0697  lr: 2.1903e-06  max_mem: 21589M
[01/18 09:54:18] d2.utils.events INFO:  eta: 2:58:19  iter: 32619  total_loss: 32.13  loss_ce: 0.24  loss_mask: 0.3284  loss_dice: 2.54  loss_ce_0: 0.5963  loss_mask_0: 0.3301  loss_dice_0: 2.665  loss_ce_1: 0.2631  loss_mask_1: 0.3315  loss_dice_1: 2.597  loss_ce_2: 0.258  loss_mask_2: 0.3291  loss_dice_2: 2.555  loss_ce_3: 0.241  loss_mask_3: 0.3299  loss_dice_3: 2.553  loss_ce_4: 0.2306  loss_mask_4: 0.3286  loss_dice_4: 2.551  loss_ce_5: 0.2482  loss_mask_5: 0.3291  loss_dice_5: 2.545  loss_ce_6: 0.24  loss_mask_6: 0.3283  loss_dice_6: 2.537  loss_ce_7: 0.2388  loss_mask_7: 0.3277  loss_dice_7: 2.548  loss_ce_8: 0.2269  loss_mask_8: 0.3284  loss_dice_8: 2.533  time: 1.4789  data_time: 0.0624  lr: 2.185e-06  max_mem: 21589M
[01/18 09:54:48] d2.utils.events INFO:  eta: 2:57:52  iter: 32639  total_loss: 32.83  loss_ce: 0.2289  loss_mask: 0.3323  loss_dice: 2.629  loss_ce_0: 0.5482  loss_mask_0: 0.3367  loss_dice_0: 2.766  loss_ce_1: 0.2838  loss_mask_1: 0.3412  loss_dice_1: 2.671  loss_ce_2: 0.2647  loss_mask_2: 0.3397  loss_dice_2: 2.647  loss_ce_3: 0.2632  loss_mask_3: 0.3364  loss_dice_3: 2.634  loss_ce_4: 0.24  loss_mask_4: 0.3344  loss_dice_4: 2.633  loss_ce_5: 0.2395  loss_mask_5: 0.3343  loss_dice_5: 2.623  loss_ce_6: 0.238  loss_mask_6: 0.3339  loss_dice_6: 2.63  loss_ce_7: 0.2156  loss_mask_7: 0.3323  loss_dice_7: 2.619  loss_ce_8: 0.2276  loss_mask_8: 0.3337  loss_dice_8: 2.623  time: 1.4789  data_time: 0.0730  lr: 2.1797e-06  max_mem: 21589M
[01/18 09:55:17] d2.utils.events INFO:  eta: 2:57:24  iter: 32659  total_loss: 32.18  loss_ce: 0.2192  loss_mask: 0.3356  loss_dice: 2.599  loss_ce_0: 0.5334  loss_mask_0: 0.3351  loss_dice_0: 2.723  loss_ce_1: 0.2848  loss_mask_1: 0.3407  loss_dice_1: 2.638  loss_ce_2: 0.2686  loss_mask_2: 0.3376  loss_dice_2: 2.621  loss_ce_3: 0.2504  loss_mask_3: 0.3345  loss_dice_3: 2.615  loss_ce_4: 0.2339  loss_mask_4: 0.3335  loss_dice_4: 2.594  loss_ce_5: 0.24  loss_mask_5: 0.3357  loss_dice_5: 2.603  loss_ce_6: 0.2314  loss_mask_6: 0.3347  loss_dice_6: 2.6  loss_ce_7: 0.2266  loss_mask_7: 0.3344  loss_dice_7: 2.592  loss_ce_8: 0.2197  loss_mask_8: 0.3355  loss_dice_8: 2.595  time: 1.4788  data_time: 0.0719  lr: 2.1743e-06  max_mem: 21589M
[01/18 09:55:46] d2.utils.events INFO:  eta: 2:56:53  iter: 32679  total_loss: 32.5  loss_ce: 0.2186  loss_mask: 0.3277  loss_dice: 2.628  loss_ce_0: 0.5423  loss_mask_0: 0.3347  loss_dice_0: 2.776  loss_ce_1: 0.2604  loss_mask_1: 0.3348  loss_dice_1: 2.684  loss_ce_2: 0.2693  loss_mask_2: 0.3317  loss_dice_2: 2.65  loss_ce_3: 0.2366  loss_mask_3: 0.3308  loss_dice_3: 2.644  loss_ce_4: 0.2382  loss_mask_4: 0.3285  loss_dice_4: 2.642  loss_ce_5: 0.2221  loss_mask_5: 0.3287  loss_dice_5: 2.64  loss_ce_6: 0.2227  loss_mask_6: 0.327  loss_dice_6: 2.632  loss_ce_7: 0.215  loss_mask_7: 0.3285  loss_dice_7: 2.629  loss_ce_8: 0.222  loss_mask_8: 0.3274  loss_dice_8: 2.631  time: 1.4788  data_time: 0.0700  lr: 2.169e-06  max_mem: 21589M
[01/18 09:56:15] d2.utils.events INFO:  eta: 2:56:26  iter: 32699  total_loss: 32.11  loss_ce: 0.2399  loss_mask: 0.3291  loss_dice: 2.583  loss_ce_0: 0.5355  loss_mask_0: 0.3293  loss_dice_0: 2.711  loss_ce_1: 0.3073  loss_mask_1: 0.3374  loss_dice_1: 2.622  loss_ce_2: 0.2806  loss_mask_2: 0.3343  loss_dice_2: 2.593  loss_ce_3: 0.2576  loss_mask_3: 0.3331  loss_dice_3: 2.581  loss_ce_4: 0.2498  loss_mask_4: 0.3314  loss_dice_4: 2.592  loss_ce_5: 0.2426  loss_mask_5: 0.3303  loss_dice_5: 2.585  loss_ce_6: 0.2446  loss_mask_6: 0.3296  loss_dice_6: 2.579  loss_ce_7: 0.2409  loss_mask_7: 0.3278  loss_dice_7: 2.588  loss_ce_8: 0.2347  loss_mask_8: 0.3288  loss_dice_8: 2.579  time: 1.4788  data_time: 0.0737  lr: 2.1637e-06  max_mem: 21589M
[01/18 09:56:44] d2.utils.events INFO:  eta: 2:55:59  iter: 32719  total_loss: 31.8  loss_ce: 0.2336  loss_mask: 0.3381  loss_dice: 2.53  loss_ce_0: 0.5556  loss_mask_0: 0.3358  loss_dice_0: 2.682  loss_ce_1: 0.2815  loss_mask_1: 0.3431  loss_dice_1: 2.592  loss_ce_2: 0.271  loss_mask_2: 0.3388  loss_dice_2: 2.555  loss_ce_3: 0.2567  loss_mask_3: 0.335  loss_dice_3: 2.54  loss_ce_4: 0.2442  loss_mask_4: 0.3358  loss_dice_4: 2.533  loss_ce_5: 0.2487  loss_mask_5: 0.3351  loss_dice_5: 2.528  loss_ce_6: 0.2476  loss_mask_6: 0.3366  loss_dice_6: 2.538  loss_ce_7: 0.2274  loss_mask_7: 0.3357  loss_dice_7: 2.541  loss_ce_8: 0.2233  loss_mask_8: 0.3365  loss_dice_8: 2.539  time: 1.4788  data_time: 0.0714  lr: 2.1583e-06  max_mem: 21589M
[01/18 09:57:13] d2.utils.events INFO:  eta: 2:55:32  iter: 32739  total_loss: 33.11  loss_ce: 0.2328  loss_mask: 0.3277  loss_dice: 2.655  loss_ce_0: 0.5744  loss_mask_0: 0.3304  loss_dice_0: 2.792  loss_ce_1: 0.2929  loss_mask_1: 0.3338  loss_dice_1: 2.692  loss_ce_2: 0.2773  loss_mask_2: 0.3305  loss_dice_2: 2.678  loss_ce_3: 0.262  loss_mask_3: 0.3288  loss_dice_3: 2.665  loss_ce_4: 0.2474  loss_mask_4: 0.3283  loss_dice_4: 2.656  loss_ce_5: 0.2459  loss_mask_5: 0.3271  loss_dice_5: 2.66  loss_ce_6: 0.2529  loss_mask_6: 0.3261  loss_dice_6: 2.657  loss_ce_7: 0.2356  loss_mask_7: 0.327  loss_dice_7: 2.655  loss_ce_8: 0.232  loss_mask_8: 0.3264  loss_dice_8: 2.652  time: 1.4788  data_time: 0.0718  lr: 2.153e-06  max_mem: 21589M
[01/18 09:57:42] d2.utils.events INFO:  eta: 2:55:05  iter: 32759  total_loss: 32.42  loss_ce: 0.239  loss_mask: 0.3329  loss_dice: 2.623  loss_ce_0: 0.5625  loss_mask_0: 0.3339  loss_dice_0: 2.756  loss_ce_1: 0.2995  loss_mask_1: 0.3393  loss_dice_1: 2.663  loss_ce_2: 0.2858  loss_mask_2: 0.3344  loss_dice_2: 2.636  loss_ce_3: 0.2634  loss_mask_3: 0.3319  loss_dice_3: 2.637  loss_ce_4: 0.2538  loss_mask_4: 0.3309  loss_dice_4: 2.622  loss_ce_5: 0.2489  loss_mask_5: 0.3303  loss_dice_5: 2.625  loss_ce_6: 0.2283  loss_mask_6: 0.3312  loss_dice_6: 2.618  loss_ce_7: 0.2322  loss_mask_7: 0.331  loss_dice_7: 2.622  loss_ce_8: 0.2431  loss_mask_8: 0.3307  loss_dice_8: 2.622  time: 1.4787  data_time: 0.0723  lr: 2.1477e-06  max_mem: 21589M
[01/18 09:58:11] d2.utils.events INFO:  eta: 2:54:30  iter: 32779  total_loss: 32.35  loss_ce: 0.2341  loss_mask: 0.3307  loss_dice: 2.58  loss_ce_0: 0.5691  loss_mask_0: 0.3362  loss_dice_0: 2.714  loss_ce_1: 0.284  loss_mask_1: 0.3388  loss_dice_1: 2.623  loss_ce_2: 0.2687  loss_mask_2: 0.3351  loss_dice_2: 2.599  loss_ce_3: 0.2561  loss_mask_3: 0.333  loss_dice_3: 2.592  loss_ce_4: 0.2297  loss_mask_4: 0.3318  loss_dice_4: 2.596  loss_ce_5: 0.2309  loss_mask_5: 0.3296  loss_dice_5: 2.595  loss_ce_6: 0.2296  loss_mask_6: 0.3317  loss_dice_6: 2.591  loss_ce_7: 0.2248  loss_mask_7: 0.3305  loss_dice_7: 2.584  loss_ce_8: 0.231  loss_mask_8: 0.3315  loss_dice_8: 2.591  time: 1.4787  data_time: 0.0698  lr: 2.1423e-06  max_mem: 21589M
[01/18 09:58:40] d2.utils.events INFO:  eta: 2:53:56  iter: 32799  total_loss: 32.43  loss_ce: 0.2343  loss_mask: 0.3258  loss_dice: 2.619  loss_ce_0: 0.5517  loss_mask_0: 0.3254  loss_dice_0: 2.762  loss_ce_1: 0.2769  loss_mask_1: 0.33  loss_dice_1: 2.675  loss_ce_2: 0.285  loss_mask_2: 0.3286  loss_dice_2: 2.642  loss_ce_3: 0.2599  loss_mask_3: 0.3267  loss_dice_3: 2.619  loss_ce_4: 0.2597  loss_mask_4: 0.3248  loss_dice_4: 2.628  loss_ce_5: 0.2402  loss_mask_5: 0.3252  loss_dice_5: 2.626  loss_ce_6: 0.2327  loss_mask_6: 0.3264  loss_dice_6: 2.62  loss_ce_7: 0.2319  loss_mask_7: 0.325  loss_dice_7: 2.625  loss_ce_8: 0.2364  loss_mask_8: 0.3263  loss_dice_8: 2.623  time: 1.4787  data_time: 0.0677  lr: 2.137e-06  max_mem: 21589M
[01/18 09:59:09] d2.utils.events INFO:  eta: 2:53:24  iter: 32819  total_loss: 32.79  loss_ce: 0.2282  loss_mask: 0.3366  loss_dice: 2.657  loss_ce_0: 0.55  loss_mask_0: 0.3366  loss_dice_0: 2.791  loss_ce_1: 0.275  loss_mask_1: 0.3429  loss_dice_1: 2.698  loss_ce_2: 0.2702  loss_mask_2: 0.3407  loss_dice_2: 2.672  loss_ce_3: 0.2514  loss_mask_3: 0.3373  loss_dice_3: 2.654  loss_ce_4: 0.2363  loss_mask_4: 0.3374  loss_dice_4: 2.658  loss_ce_5: 0.244  loss_mask_5: 0.3371  loss_dice_5: 2.653  loss_ce_6: 0.2389  loss_mask_6: 0.3375  loss_dice_6: 2.662  loss_ce_7: 0.2361  loss_mask_7: 0.336  loss_dice_7: 2.653  loss_ce_8: 0.2201  loss_mask_8: 0.3373  loss_dice_8: 2.662  time: 1.4787  data_time: 0.0689  lr: 2.1316e-06  max_mem: 21589M
[01/18 09:59:38] d2.utils.events INFO:  eta: 2:52:47  iter: 32839  total_loss: 32.12  loss_ce: 0.2252  loss_mask: 0.3298  loss_dice: 2.586  loss_ce_0: 0.5294  loss_mask_0: 0.3265  loss_dice_0: 2.721  loss_ce_1: 0.2609  loss_mask_1: 0.3311  loss_dice_1: 2.636  loss_ce_2: 0.2705  loss_mask_2: 0.3281  loss_dice_2: 2.614  loss_ce_3: 0.2559  loss_mask_3: 0.3294  loss_dice_3: 2.598  loss_ce_4: 0.2395  loss_mask_4: 0.3283  loss_dice_4: 2.604  loss_ce_5: 0.2333  loss_mask_5: 0.328  loss_dice_5: 2.598  loss_ce_6: 0.2397  loss_mask_6: 0.3275  loss_dice_6: 2.59  loss_ce_7: 0.2274  loss_mask_7: 0.3295  loss_dice_7: 2.59  loss_ce_8: 0.2279  loss_mask_8: 0.3299  loss_dice_8: 2.594  time: 1.4787  data_time: 0.0738  lr: 2.1263e-06  max_mem: 21589M
[01/18 10:00:06] d2.utils.events INFO:  eta: 2:52:09  iter: 32859  total_loss: 32.02  loss_ce: 0.2172  loss_mask: 0.3266  loss_dice: 2.579  loss_ce_0: 0.5516  loss_mask_0: 0.3207  loss_dice_0: 2.723  loss_ce_1: 0.2869  loss_mask_1: 0.3313  loss_dice_1: 2.632  loss_ce_2: 0.2817  loss_mask_2: 0.3294  loss_dice_2: 2.612  loss_ce_3: 0.2439  loss_mask_3: 0.3285  loss_dice_3: 2.596  loss_ce_4: 0.2418  loss_mask_4: 0.3275  loss_dice_4: 2.592  loss_ce_5: 0.2183  loss_mask_5: 0.3283  loss_dice_5: 2.587  loss_ce_6: 0.2234  loss_mask_6: 0.3293  loss_dice_6: 2.579  loss_ce_7: 0.2241  loss_mask_7: 0.3276  loss_dice_7: 2.584  loss_ce_8: 0.218  loss_mask_8: 0.328  loss_dice_8: 2.589  time: 1.4786  data_time: 0.0668  lr: 2.1209e-06  max_mem: 21589M
[01/18 10:00:35] d2.utils.events INFO:  eta: 2:51:41  iter: 32879  total_loss: 31.84  loss_ce: 0.2175  loss_mask: 0.3345  loss_dice: 2.572  loss_ce_0: 0.5251  loss_mask_0: 0.3325  loss_dice_0: 2.699  loss_ce_1: 0.2643  loss_mask_1: 0.3382  loss_dice_1: 2.612  loss_ce_2: 0.2695  loss_mask_2: 0.3344  loss_dice_2: 2.594  loss_ce_3: 0.2522  loss_mask_3: 0.3346  loss_dice_3: 2.568  loss_ce_4: 0.2394  loss_mask_4: 0.3348  loss_dice_4: 2.577  loss_ce_5: 0.2271  loss_mask_5: 0.3356  loss_dice_5: 2.573  loss_ce_6: 0.2232  loss_mask_6: 0.3351  loss_dice_6: 2.562  loss_ce_7: 0.2271  loss_mask_7: 0.3353  loss_dice_7: 2.568  loss_ce_8: 0.2176  loss_mask_8: 0.3349  loss_dice_8: 2.57  time: 1.4786  data_time: 0.0740  lr: 2.1156e-06  max_mem: 21589M
[01/18 10:01:04] d2.utils.events INFO:  eta: 2:51:09  iter: 32899  total_loss: 31.77  loss_ce: 0.2147  loss_mask: 0.3337  loss_dice: 2.534  loss_ce_0: 0.5647  loss_mask_0: 0.3362  loss_dice_0: 2.689  loss_ce_1: 0.2909  loss_mask_1: 0.3384  loss_dice_1: 2.576  loss_ce_2: 0.2635  loss_mask_2: 0.3358  loss_dice_2: 2.553  loss_ce_3: 0.2534  loss_mask_3: 0.3348  loss_dice_3: 2.548  loss_ce_4: 0.2303  loss_mask_4: 0.3344  loss_dice_4: 2.529  loss_ce_5: 0.2425  loss_mask_5: 0.3332  loss_dice_5: 2.531  loss_ce_6: 0.2337  loss_mask_6: 0.3335  loss_dice_6: 2.54  loss_ce_7: 0.2268  loss_mask_7: 0.3341  loss_dice_7: 2.536  loss_ce_8: 0.2362  loss_mask_8: 0.3338  loss_dice_8: 2.533  time: 1.4786  data_time: 0.0734  lr: 2.1102e-06  max_mem: 21589M
[01/18 10:01:33] d2.utils.events INFO:  eta: 2:50:39  iter: 32919  total_loss: 32.15  loss_ce: 0.218  loss_mask: 0.3341  loss_dice: 2.557  loss_ce_0: 0.5436  loss_mask_0: 0.3355  loss_dice_0: 2.694  loss_ce_1: 0.2754  loss_mask_1: 0.3426  loss_dice_1: 2.623  loss_ce_2: 0.2575  loss_mask_2: 0.3398  loss_dice_2: 2.584  loss_ce_3: 0.2443  loss_mask_3: 0.3366  loss_dice_3: 2.571  loss_ce_4: 0.2312  loss_mask_4: 0.3344  loss_dice_4: 2.57  loss_ce_5: 0.226  loss_mask_5: 0.3347  loss_dice_5: 2.575  loss_ce_6: 0.2229  loss_mask_6: 0.3348  loss_dice_6: 2.569  loss_ce_7: 0.2293  loss_mask_7: 0.3335  loss_dice_7: 2.564  loss_ce_8: 0.2314  loss_mask_8: 0.3336  loss_dice_8: 2.558  time: 1.4786  data_time: 0.0686  lr: 2.1049e-06  max_mem: 21589M
[01/18 10:02:02] d2.utils.events INFO:  eta: 2:50:04  iter: 32939  total_loss: 31.78  loss_ce: 0.2218  loss_mask: 0.3406  loss_dice: 2.543  loss_ce_0: 0.5734  loss_mask_0: 0.3341  loss_dice_0: 2.693  loss_ce_1: 0.2708  loss_mask_1: 0.3465  loss_dice_1: 2.593  loss_ce_2: 0.2581  loss_mask_2: 0.3427  loss_dice_2: 2.563  loss_ce_3: 0.2428  loss_mask_3: 0.3414  loss_dice_3: 2.556  loss_ce_4: 0.2369  loss_mask_4: 0.3407  loss_dice_4: 2.558  loss_ce_5: 0.2342  loss_mask_5: 0.3394  loss_dice_5: 2.553  loss_ce_6: 0.219  loss_mask_6: 0.3397  loss_dice_6: 2.551  loss_ce_7: 0.231  loss_mask_7: 0.3382  loss_dice_7: 2.551  loss_ce_8: 0.2227  loss_mask_8: 0.3405  loss_dice_8: 2.546  time: 1.4785  data_time: 0.0711  lr: 2.0995e-06  max_mem: 21589M
[01/18 10:02:31] d2.utils.events INFO:  eta: 2:49:29  iter: 32959  total_loss: 31.24  loss_ce: 0.2074  loss_mask: 0.331  loss_dice: 2.518  loss_ce_0: 0.5602  loss_mask_0: 0.3299  loss_dice_0: 2.652  loss_ce_1: 0.2682  loss_mask_1: 0.3358  loss_dice_1: 2.558  loss_ce_2: 0.2703  loss_mask_2: 0.3318  loss_dice_2: 2.536  loss_ce_3: 0.2477  loss_mask_3: 0.3315  loss_dice_3: 2.532  loss_ce_4: 0.2409  loss_mask_4: 0.3309  loss_dice_4: 2.532  loss_ce_5: 0.2392  loss_mask_5: 0.3303  loss_dice_5: 2.527  loss_ce_6: 0.2266  loss_mask_6: 0.3292  loss_dice_6: 2.524  loss_ce_7: 0.2145  loss_mask_7: 0.33  loss_dice_7: 2.522  loss_ce_8: 0.2149  loss_mask_8: 0.331  loss_dice_8: 2.523  time: 1.4785  data_time: 0.0705  lr: 2.0942e-06  max_mem: 21589M
[01/18 10:03:00] d2.utils.events INFO:  eta: 2:49:01  iter: 32979  total_loss: 32.03  loss_ce: 0.2343  loss_mask: 0.3335  loss_dice: 2.567  loss_ce_0: 0.542  loss_mask_0: 0.3291  loss_dice_0: 2.693  loss_ce_1: 0.2847  loss_mask_1: 0.3386  loss_dice_1: 2.609  loss_ce_2: 0.2631  loss_mask_2: 0.3346  loss_dice_2: 2.591  loss_ce_3: 0.2426  loss_mask_3: 0.3334  loss_dice_3: 2.573  loss_ce_4: 0.2387  loss_mask_4: 0.3337  loss_dice_4: 2.581  loss_ce_5: 0.2325  loss_mask_5: 0.3335  loss_dice_5: 2.573  loss_ce_6: 0.2271  loss_mask_6: 0.3323  loss_dice_6: 2.576  loss_ce_7: 0.2204  loss_mask_7: 0.3327  loss_dice_7: 2.574  loss_ce_8: 0.2262  loss_mask_8: 0.3345  loss_dice_8: 2.57  time: 1.4785  data_time: 0.0689  lr: 2.0888e-06  max_mem: 21589M
[01/18 10:03:29] d2.utils.events INFO:  eta: 2:48:31  iter: 32999  total_loss: 32.02  loss_ce: 0.2244  loss_mask: 0.3359  loss_dice: 2.573  loss_ce_0: 0.5447  loss_mask_0: 0.3363  loss_dice_0: 2.711  loss_ce_1: 0.2742  loss_mask_1: 0.341  loss_dice_1: 2.613  loss_ce_2: 0.2599  loss_mask_2: 0.3356  loss_dice_2: 2.606  loss_ce_3: 0.2613  loss_mask_3: 0.335  loss_dice_3: 2.582  loss_ce_4: 0.2597  loss_mask_4: 0.3351  loss_dice_4: 2.582  loss_ce_5: 0.2389  loss_mask_5: 0.3342  loss_dice_5: 2.587  loss_ce_6: 0.2374  loss_mask_6: 0.3358  loss_dice_6: 2.579  loss_ce_7: 0.2305  loss_mask_7: 0.3357  loss_dice_7: 2.58  loss_ce_8: 0.2395  loss_mask_8: 0.3375  loss_dice_8: 2.574  time: 1.4785  data_time: 0.0693  lr: 2.0835e-06  max_mem: 21589M
[01/18 10:03:58] d2.utils.events INFO:  eta: 2:48:02  iter: 33019  total_loss: 32.74  loss_ce: 0.2247  loss_mask: 0.3409  loss_dice: 2.601  loss_ce_0: 0.5625  loss_mask_0: 0.3353  loss_dice_0: 2.756  loss_ce_1: 0.2616  loss_mask_1: 0.3446  loss_dice_1: 2.652  loss_ce_2: 0.2573  loss_mask_2: 0.3431  loss_dice_2: 2.623  loss_ce_3: 0.2461  loss_mask_3: 0.3409  loss_dice_3: 2.617  loss_ce_4: 0.2362  loss_mask_4: 0.3416  loss_dice_4: 2.615  loss_ce_5: 0.2283  loss_mask_5: 0.3399  loss_dice_5: 2.608  loss_ce_6: 0.2217  loss_mask_6: 0.34  loss_dice_6: 2.61  loss_ce_7: 0.2219  loss_mask_7: 0.3389  loss_dice_7: 2.612  loss_ce_8: 0.2283  loss_mask_8: 0.3395  loss_dice_8: 2.606  time: 1.4785  data_time: 0.0720  lr: 2.0781e-06  max_mem: 21589M
[01/18 10:04:27] d2.utils.events INFO:  eta: 2:47:34  iter: 33039  total_loss: 32.48  loss_ce: 0.257  loss_mask: 0.3314  loss_dice: 2.589  loss_ce_0: 0.5774  loss_mask_0: 0.3267  loss_dice_0: 2.741  loss_ce_1: 0.3022  loss_mask_1: 0.3371  loss_dice_1: 2.656  loss_ce_2: 0.28  loss_mask_2: 0.3333  loss_dice_2: 2.626  loss_ce_3: 0.2711  loss_mask_3: 0.3309  loss_dice_3: 2.616  loss_ce_4: 0.2591  loss_mask_4: 0.3318  loss_dice_4: 2.607  loss_ce_5: 0.2526  loss_mask_5: 0.3318  loss_dice_5: 2.612  loss_ce_6: 0.2519  loss_mask_6: 0.3305  loss_dice_6: 2.611  loss_ce_7: 0.2328  loss_mask_7: 0.3297  loss_dice_7: 2.601  loss_ce_8: 0.246  loss_mask_8: 0.3313  loss_dice_8: 2.61  time: 1.4784  data_time: 0.0697  lr: 2.0728e-06  max_mem: 21589M
[01/18 10:04:56] d2.utils.events INFO:  eta: 2:47:05  iter: 33059  total_loss: 32.07  loss_ce: 0.2283  loss_mask: 0.3291  loss_dice: 2.569  loss_ce_0: 0.5526  loss_mask_0: 0.3335  loss_dice_0: 2.711  loss_ce_1: 0.2807  loss_mask_1: 0.3374  loss_dice_1: 2.604  loss_ce_2: 0.2443  loss_mask_2: 0.3344  loss_dice_2: 2.593  loss_ce_3: 0.2486  loss_mask_3: 0.3318  loss_dice_3: 2.573  loss_ce_4: 0.2436  loss_mask_4: 0.3297  loss_dice_4: 2.577  loss_ce_5: 0.2434  loss_mask_5: 0.3308  loss_dice_5: 2.566  loss_ce_6: 0.2309  loss_mask_6: 0.3299  loss_dice_6: 2.57  loss_ce_7: 0.2344  loss_mask_7: 0.3283  loss_dice_7: 2.57  loss_ce_8: 0.2378  loss_mask_8: 0.3292  loss_dice_8: 2.569  time: 1.4784  data_time: 0.0710  lr: 2.0674e-06  max_mem: 21589M
[01/18 10:05:25] d2.utils.events INFO:  eta: 2:46:36  iter: 33079  total_loss: 31.5  loss_ce: 0.2163  loss_mask: 0.3277  loss_dice: 2.515  loss_ce_0: 0.5279  loss_mask_0: 0.3291  loss_dice_0: 2.648  loss_ce_1: 0.2575  loss_mask_1: 0.3297  loss_dice_1: 2.565  loss_ce_2: 0.2574  loss_mask_2: 0.3277  loss_dice_2: 2.532  loss_ce_3: 0.2482  loss_mask_3: 0.3278  loss_dice_3: 2.509  loss_ce_4: 0.238  loss_mask_4: 0.3276  loss_dice_4: 2.531  loss_ce_5: 0.2388  loss_mask_5: 0.3263  loss_dice_5: 2.522  loss_ce_6: 0.2298  loss_mask_6: 0.3264  loss_dice_6: 2.519  loss_ce_7: 0.2165  loss_mask_7: 0.327  loss_dice_7: 2.529  loss_ce_8: 0.2207  loss_mask_8: 0.3265  loss_dice_8: 2.519  time: 1.4784  data_time: 0.0710  lr: 2.062e-06  max_mem: 21589M
[01/18 10:05:54] d2.utils.events INFO:  eta: 2:46:05  iter: 33099  total_loss: 31.52  loss_ce: 0.2126  loss_mask: 0.338  loss_dice: 2.539  loss_ce_0: 0.5525  loss_mask_0: 0.3376  loss_dice_0: 2.652  loss_ce_1: 0.2808  loss_mask_1: 0.3433  loss_dice_1: 2.555  loss_ce_2: 0.2872  loss_mask_2: 0.3401  loss_dice_2: 2.541  loss_ce_3: 0.2387  loss_mask_3: 0.3394  loss_dice_3: 2.531  loss_ce_4: 0.2332  loss_mask_4: 0.3365  loss_dice_4: 2.529  loss_ce_5: 0.2131  loss_mask_5: 0.3361  loss_dice_5: 2.532  loss_ce_6: 0.2143  loss_mask_6: 0.3354  loss_dice_6: 2.531  loss_ce_7: 0.2205  loss_mask_7: 0.3361  loss_dice_7: 2.521  loss_ce_8: 0.2147  loss_mask_8: 0.3365  loss_dice_8: 2.531  time: 1.4784  data_time: 0.0668  lr: 2.0567e-06  max_mem: 21589M
[01/18 10:06:23] d2.utils.events INFO:  eta: 2:45:35  iter: 33119  total_loss: 31.8  loss_ce: 0.2161  loss_mask: 0.3324  loss_dice: 2.545  loss_ce_0: 0.5656  loss_mask_0: 0.3292  loss_dice_0: 2.688  loss_ce_1: 0.2647  loss_mask_1: 0.3395  loss_dice_1: 2.595  loss_ce_2: 0.2684  loss_mask_2: 0.3362  loss_dice_2: 2.567  loss_ce_3: 0.2533  loss_mask_3: 0.3343  loss_dice_3: 2.554  loss_ce_4: 0.2344  loss_mask_4: 0.333  loss_dice_4: 2.554  loss_ce_5: 0.2247  loss_mask_5: 0.3318  loss_dice_5: 2.553  loss_ce_6: 0.219  loss_mask_6: 0.3317  loss_dice_6: 2.551  loss_ce_7: 0.2102  loss_mask_7: 0.3314  loss_dice_7: 2.545  loss_ce_8: 0.2273  loss_mask_8: 0.3318  loss_dice_8: 2.538  time: 1.4784  data_time: 0.0723  lr: 2.0513e-06  max_mem: 21589M
[01/18 10:06:52] d2.utils.events INFO:  eta: 2:45:09  iter: 33139  total_loss: 31.55  loss_ce: 0.2062  loss_mask: 0.3283  loss_dice: 2.544  loss_ce_0: 0.5356  loss_mask_0: 0.3232  loss_dice_0: 2.685  loss_ce_1: 0.2773  loss_mask_1: 0.3309  loss_dice_1: 2.597  loss_ce_2: 0.2505  loss_mask_2: 0.3269  loss_dice_2: 2.576  loss_ce_3: 0.2334  loss_mask_3: 0.3274  loss_dice_3: 2.565  loss_ce_4: 0.2278  loss_mask_4: 0.3284  loss_dice_4: 2.551  loss_ce_5: 0.2214  loss_mask_5: 0.3274  loss_dice_5: 2.552  loss_ce_6: 0.2125  loss_mask_6: 0.3279  loss_dice_6: 2.552  loss_ce_7: 0.2135  loss_mask_7: 0.327  loss_dice_7: 2.55  loss_ce_8: 0.206  loss_mask_8: 0.3276  loss_dice_8: 2.549  time: 1.4783  data_time: 0.0679  lr: 2.0459e-06  max_mem: 21589M
[01/18 10:07:21] d2.utils.events INFO:  eta: 2:44:42  iter: 33159  total_loss: 32.37  loss_ce: 0.2172  loss_mask: 0.3352  loss_dice: 2.594  loss_ce_0: 0.5671  loss_mask_0: 0.338  loss_dice_0: 2.72  loss_ce_1: 0.2634  loss_mask_1: 0.3428  loss_dice_1: 2.635  loss_ce_2: 0.2732  loss_mask_2: 0.3387  loss_dice_2: 2.61  loss_ce_3: 0.267  loss_mask_3: 0.3354  loss_dice_3: 2.595  loss_ce_4: 0.2638  loss_mask_4: 0.3352  loss_dice_4: 2.604  loss_ce_5: 0.2421  loss_mask_5: 0.3354  loss_dice_5: 2.601  loss_ce_6: 0.2284  loss_mask_6: 0.335  loss_dice_6: 2.589  loss_ce_7: 0.2343  loss_mask_7: 0.3357  loss_dice_7: 2.592  loss_ce_8: 0.2276  loss_mask_8: 0.3352  loss_dice_8: 2.584  time: 1.4783  data_time: 0.0692  lr: 2.0406e-06  max_mem: 21589M
[01/18 10:07:50] d2.utils.events INFO:  eta: 2:44:14  iter: 33179  total_loss: 32.2  loss_ce: 0.2363  loss_mask: 0.328  loss_dice: 2.556  loss_ce_0: 0.5472  loss_mask_0: 0.3246  loss_dice_0: 2.71  loss_ce_1: 0.3001  loss_mask_1: 0.3322  loss_dice_1: 2.604  loss_ce_2: 0.2932  loss_mask_2: 0.3317  loss_dice_2: 2.581  loss_ce_3: 0.2747  loss_mask_3: 0.3278  loss_dice_3: 2.567  loss_ce_4: 0.256  loss_mask_4: 0.3285  loss_dice_4: 2.555  loss_ce_5: 0.2403  loss_mask_5: 0.3279  loss_dice_5: 2.55  loss_ce_6: 0.2467  loss_mask_6: 0.328  loss_dice_6: 2.555  loss_ce_7: 0.2364  loss_mask_7: 0.3287  loss_dice_7: 2.555  loss_ce_8: 0.2438  loss_mask_8: 0.3286  loss_dice_8: 2.549  time: 1.4783  data_time: 0.0724  lr: 2.0352e-06  max_mem: 21589M
[01/18 10:08:19] d2.utils.events INFO:  eta: 2:43:44  iter: 33199  total_loss: 32.26  loss_ce: 0.229  loss_mask: 0.3291  loss_dice: 2.59  loss_ce_0: 0.5507  loss_mask_0: 0.3224  loss_dice_0: 2.703  loss_ce_1: 0.2796  loss_mask_1: 0.333  loss_dice_1: 2.624  loss_ce_2: 0.2859  loss_mask_2: 0.3313  loss_dice_2: 2.603  loss_ce_3: 0.2571  loss_mask_3: 0.33  loss_dice_3: 2.595  loss_ce_4: 0.2392  loss_mask_4: 0.3295  loss_dice_4: 2.591  loss_ce_5: 0.2512  loss_mask_5: 0.3285  loss_dice_5: 2.59  loss_ce_6: 0.25  loss_mask_6: 0.3287  loss_dice_6: 2.595  loss_ce_7: 0.2407  loss_mask_7: 0.3291  loss_dice_7: 2.59  loss_ce_8: 0.2238  loss_mask_8: 0.3295  loss_dice_8: 2.589  time: 1.4783  data_time: 0.0716  lr: 2.0298e-06  max_mem: 21589M
[01/18 10:08:49] d2.utils.events INFO:  eta: 2:43:23  iter: 33219  total_loss: 31.58  loss_ce: 0.2358  loss_mask: 0.3336  loss_dice: 2.523  loss_ce_0: 0.5453  loss_mask_0: 0.3305  loss_dice_0: 2.67  loss_ce_1: 0.288  loss_mask_1: 0.3381  loss_dice_1: 2.568  loss_ce_2: 0.2816  loss_mask_2: 0.335  loss_dice_2: 2.534  loss_ce_3: 0.2477  loss_mask_3: 0.3326  loss_dice_3: 2.527  loss_ce_4: 0.2493  loss_mask_4: 0.3331  loss_dice_4: 2.531  loss_ce_5: 0.2566  loss_mask_5: 0.3317  loss_dice_5: 2.522  loss_ce_6: 0.2397  loss_mask_6: 0.332  loss_dice_6: 2.528  loss_ce_7: 0.2373  loss_mask_7: 0.3317  loss_dice_7: 2.525  loss_ce_8: 0.2272  loss_mask_8: 0.3324  loss_dice_8: 2.535  time: 1.4783  data_time: 0.0767  lr: 2.0245e-06  max_mem: 21589M
[01/18 10:09:18] d2.utils.events INFO:  eta: 2:42:53  iter: 33239  total_loss: 31.7  loss_ce: 0.2144  loss_mask: 0.3275  loss_dice: 2.56  loss_ce_0: 0.5349  loss_mask_0: 0.3235  loss_dice_0: 2.689  loss_ce_1: 0.2636  loss_mask_1: 0.3322  loss_dice_1: 2.596  loss_ce_2: 0.2567  loss_mask_2: 0.3298  loss_dice_2: 2.573  loss_ce_3: 0.2296  loss_mask_3: 0.3299  loss_dice_3: 2.561  loss_ce_4: 0.2358  loss_mask_4: 0.3297  loss_dice_4: 2.558  loss_ce_5: 0.2264  loss_mask_5: 0.3294  loss_dice_5: 2.562  loss_ce_6: 0.2306  loss_mask_6: 0.3279  loss_dice_6: 2.562  loss_ce_7: 0.201  loss_mask_7: 0.3285  loss_dice_7: 2.561  loss_ce_8: 0.2094  loss_mask_8: 0.3283  loss_dice_8: 2.56  time: 1.4783  data_time: 0.0684  lr: 2.0191e-06  max_mem: 21589M
[01/18 10:09:47] d2.utils.events INFO:  eta: 2:42:27  iter: 33259  total_loss: 32.27  loss_ce: 0.2266  loss_mask: 0.3316  loss_dice: 2.579  loss_ce_0: 0.5695  loss_mask_0: 0.334  loss_dice_0: 2.703  loss_ce_1: 0.2914  loss_mask_1: 0.3391  loss_dice_1: 2.612  loss_ce_2: 0.2816  loss_mask_2: 0.336  loss_dice_2: 2.59  loss_ce_3: 0.261  loss_mask_3: 0.3326  loss_dice_3: 2.577  loss_ce_4: 0.2529  loss_mask_4: 0.3332  loss_dice_4: 2.57  loss_ce_5: 0.2427  loss_mask_5: 0.3323  loss_dice_5: 2.583  loss_ce_6: 0.2255  loss_mask_6: 0.3316  loss_dice_6: 2.567  loss_ce_7: 0.2231  loss_mask_7: 0.332  loss_dice_7: 2.573  loss_ce_8: 0.2242  loss_mask_8: 0.3318  loss_dice_8: 2.568  time: 1.4783  data_time: 0.0712  lr: 2.0137e-06  max_mem: 21589M
[01/18 10:10:17] d2.utils.events INFO:  eta: 2:42:00  iter: 33279  total_loss: 31.72  loss_ce: 0.2068  loss_mask: 0.3376  loss_dice: 2.524  loss_ce_0: 0.5483  loss_mask_0: 0.3363  loss_dice_0: 2.66  loss_ce_1: 0.2594  loss_mask_1: 0.3403  loss_dice_1: 2.558  loss_ce_2: 0.253  loss_mask_2: 0.3382  loss_dice_2: 2.547  loss_ce_3: 0.2257  loss_mask_3: 0.3356  loss_dice_3: 2.516  loss_ce_4: 0.2248  loss_mask_4: 0.3374  loss_dice_4: 2.53  loss_ce_5: 0.217  loss_mask_5: 0.3367  loss_dice_5: 2.532  loss_ce_6: 0.2211  loss_mask_6: 0.3363  loss_dice_6: 2.519  loss_ce_7: 0.2236  loss_mask_7: 0.3369  loss_dice_7: 2.52  loss_ce_8: 0.2183  loss_mask_8: 0.3366  loss_dice_8: 2.519  time: 1.4783  data_time: 0.0694  lr: 2.0083e-06  max_mem: 21589M
[01/18 10:10:46] d2.utils.events INFO:  eta: 2:41:31  iter: 33299  total_loss: 32.14  loss_ce: 0.2208  loss_mask: 0.3342  loss_dice: 2.546  loss_ce_0: 0.5632  loss_mask_0: 0.3398  loss_dice_0: 2.693  loss_ce_1: 0.2767  loss_mask_1: 0.3439  loss_dice_1: 2.591  loss_ce_2: 0.2674  loss_mask_2: 0.34  loss_dice_2: 2.574  loss_ce_3: 0.2573  loss_mask_3: 0.3375  loss_dice_3: 2.561  loss_ce_4: 0.2426  loss_mask_4: 0.3372  loss_dice_4: 2.551  loss_ce_5: 0.2405  loss_mask_5: 0.3353  loss_dice_5: 2.559  loss_ce_6: 0.2326  loss_mask_6: 0.334  loss_dice_6: 2.546  loss_ce_7: 0.2227  loss_mask_7: 0.3351  loss_dice_7: 2.544  loss_ce_8: 0.2188  loss_mask_8: 0.3344  loss_dice_8: 2.545  time: 1.4783  data_time: 0.0679  lr: 2.003e-06  max_mem: 21589M
[01/18 10:11:16] d2.utils.events INFO:  eta: 2:41:07  iter: 33319  total_loss: 31.99  loss_ce: 0.2289  loss_mask: 0.3298  loss_dice: 2.575  loss_ce_0: 0.5449  loss_mask_0: 0.3256  loss_dice_0: 2.724  loss_ce_1: 0.271  loss_mask_1: 0.335  loss_dice_1: 2.632  loss_ce_2: 0.2689  loss_mask_2: 0.3342  loss_dice_2: 2.592  loss_ce_3: 0.2537  loss_mask_3: 0.3333  loss_dice_3: 2.582  loss_ce_4: 0.2451  loss_mask_4: 0.3318  loss_dice_4: 2.588  loss_ce_5: 0.242  loss_mask_5: 0.3294  loss_dice_5: 2.574  loss_ce_6: 0.2292  loss_mask_6: 0.3305  loss_dice_6: 2.574  loss_ce_7: 0.225  loss_mask_7: 0.3303  loss_dice_7: 2.582  loss_ce_8: 0.2178  loss_mask_8: 0.3303  loss_dice_8: 2.58  time: 1.4783  data_time: 0.0745  lr: 1.9976e-06  max_mem: 21589M
[01/18 10:11:45] d2.utils.events INFO:  eta: 2:40:40  iter: 33339  total_loss: 32.37  loss_ce: 0.2326  loss_mask: 0.3355  loss_dice: 2.607  loss_ce_0: 0.5493  loss_mask_0: 0.3308  loss_dice_0: 2.728  loss_ce_1: 0.2691  loss_mask_1: 0.336  loss_dice_1: 2.646  loss_ce_2: 0.2598  loss_mask_2: 0.3355  loss_dice_2: 2.625  loss_ce_3: 0.2483  loss_mask_3: 0.336  loss_dice_3: 2.607  loss_ce_4: 0.2379  loss_mask_4: 0.3354  loss_dice_4: 2.605  loss_ce_5: 0.2347  loss_mask_5: 0.3362  loss_dice_5: 2.604  loss_ce_6: 0.2294  loss_mask_6: 0.336  loss_dice_6: 2.604  loss_ce_7: 0.2219  loss_mask_7: 0.3349  loss_dice_7: 2.607  loss_ce_8: 0.2182  loss_mask_8: 0.3347  loss_dice_8: 2.603  time: 1.4783  data_time: 0.0711  lr: 1.9922e-06  max_mem: 21589M
[01/18 10:12:15] d2.utils.events INFO:  eta: 2:40:12  iter: 33359  total_loss: 31.98  loss_ce: 0.2088  loss_mask: 0.3324  loss_dice: 2.545  loss_ce_0: 0.5436  loss_mask_0: 0.336  loss_dice_0: 2.687  loss_ce_1: 0.2727  loss_mask_1: 0.3396  loss_dice_1: 2.588  loss_ce_2: 0.2789  loss_mask_2: 0.3365  loss_dice_2: 2.556  loss_ce_3: 0.2506  loss_mask_3: 0.333  loss_dice_3: 2.549  loss_ce_4: 0.2563  loss_mask_4: 0.3334  loss_dice_4: 2.542  loss_ce_5: 0.2364  loss_mask_5: 0.3327  loss_dice_5: 2.535  loss_ce_6: 0.2286  loss_mask_6: 0.3307  loss_dice_6: 2.551  loss_ce_7: 0.2223  loss_mask_7: 0.3326  loss_dice_7: 2.543  loss_ce_8: 0.2153  loss_mask_8: 0.3323  loss_dice_8: 2.54  time: 1.4783  data_time: 0.0724  lr: 1.9868e-06  max_mem: 21589M
[01/18 10:12:44] d2.utils.events INFO:  eta: 2:39:43  iter: 33379  total_loss: 31.14  loss_ce: 0.2115  loss_mask: 0.3413  loss_dice: 2.49  loss_ce_0: 0.5253  loss_mask_0: 0.3311  loss_dice_0: 2.62  loss_ce_1: 0.2668  loss_mask_1: 0.3404  loss_dice_1: 2.536  loss_ce_2: 0.2586  loss_mask_2: 0.339  loss_dice_2: 2.51  loss_ce_3: 0.2276  loss_mask_3: 0.3398  loss_dice_3: 2.489  loss_ce_4: 0.222  loss_mask_4: 0.3407  loss_dice_4: 2.487  loss_ce_5: 0.2118  loss_mask_5: 0.34  loss_dice_5: 2.489  loss_ce_6: 0.2226  loss_mask_6: 0.3397  loss_dice_6: 2.48  loss_ce_7: 0.2045  loss_mask_7: 0.341  loss_dice_7: 2.483  loss_ce_8: 0.2115  loss_mask_8: 0.3412  loss_dice_8: 2.49  time: 1.4782  data_time: 0.0706  lr: 1.9814e-06  max_mem: 21589M
[01/18 10:13:14] d2.utils.events INFO:  eta: 2:39:15  iter: 33399  total_loss: 32.56  loss_ce: 0.2353  loss_mask: 0.3247  loss_dice: 2.609  loss_ce_0: 0.5277  loss_mask_0: 0.3328  loss_dice_0: 2.736  loss_ce_1: 0.2857  loss_mask_1: 0.3331  loss_dice_1: 2.65  loss_ce_2: 0.2961  loss_mask_2: 0.328  loss_dice_2: 2.625  loss_ce_3: 0.2668  loss_mask_3: 0.3255  loss_dice_3: 2.619  loss_ce_4: 0.2574  loss_mask_4: 0.3254  loss_dice_4: 2.62  loss_ce_5: 0.2554  loss_mask_5: 0.3244  loss_dice_5: 2.614  loss_ce_6: 0.2464  loss_mask_6: 0.3236  loss_dice_6: 2.604  loss_ce_7: 0.2455  loss_mask_7: 0.3233  loss_dice_7: 2.611  loss_ce_8: 0.2382  loss_mask_8: 0.3233  loss_dice_8: 2.613  time: 1.4782  data_time: 0.0655  lr: 1.976e-06  max_mem: 21589M
[01/18 10:13:43] d2.utils.events INFO:  eta: 2:38:46  iter: 33419  total_loss: 32.06  loss_ce: 0.2444  loss_mask: 0.3272  loss_dice: 2.581  loss_ce_0: 0.573  loss_mask_0: 0.3284  loss_dice_0: 2.708  loss_ce_1: 0.301  loss_mask_1: 0.334  loss_dice_1: 2.609  loss_ce_2: 0.2944  loss_mask_2: 0.3297  loss_dice_2: 2.595  loss_ce_3: 0.2525  loss_mask_3: 0.3263  loss_dice_3: 2.581  loss_ce_4: 0.2622  loss_mask_4: 0.3265  loss_dice_4: 2.581  loss_ce_5: 0.2555  loss_mask_5: 0.327  loss_dice_5: 2.584  loss_ce_6: 0.2404  loss_mask_6: 0.3265  loss_dice_6: 2.581  loss_ce_7: 0.2412  loss_mask_7: 0.3271  loss_dice_7: 2.573  loss_ce_8: 0.2275  loss_mask_8: 0.3269  loss_dice_8: 2.58  time: 1.4782  data_time: 0.0703  lr: 1.9706e-06  max_mem: 21589M
[01/18 10:14:12] d2.utils.events INFO:  eta: 2:38:17  iter: 33439  total_loss: 31.66  loss_ce: 0.2229  loss_mask: 0.3346  loss_dice: 2.535  loss_ce_0: 0.5387  loss_mask_0: 0.3317  loss_dice_0: 2.667  loss_ce_1: 0.263  loss_mask_1: 0.3403  loss_dice_1: 2.582  loss_ce_2: 0.2484  loss_mask_2: 0.3367  loss_dice_2: 2.559  loss_ce_3: 0.2392  loss_mask_3: 0.3348  loss_dice_3: 2.534  loss_ce_4: 0.2268  loss_mask_4: 0.3348  loss_dice_4: 2.533  loss_ce_5: 0.2326  loss_mask_5: 0.3356  loss_dice_5: 2.537  loss_ce_6: 0.2243  loss_mask_6: 0.3343  loss_dice_6: 2.538  loss_ce_7: 0.2228  loss_mask_7: 0.3345  loss_dice_7: 2.537  loss_ce_8: 0.2286  loss_mask_8: 0.3346  loss_dice_8: 2.539  time: 1.4782  data_time: 0.0706  lr: 1.9653e-06  max_mem: 21589M
[01/18 10:14:41] d2.utils.events INFO:  eta: 2:37:46  iter: 33459  total_loss: 32.01  loss_ce: 0.2324  loss_mask: 0.3273  loss_dice: 2.563  loss_ce_0: 0.5516  loss_mask_0: 0.3299  loss_dice_0: 2.704  loss_ce_1: 0.2676  loss_mask_1: 0.3366  loss_dice_1: 2.615  loss_ce_2: 0.2779  loss_mask_2: 0.3334  loss_dice_2: 2.589  loss_ce_3: 0.2604  loss_mask_3: 0.3327  loss_dice_3: 2.573  loss_ce_4: 0.2435  loss_mask_4: 0.3309  loss_dice_4: 2.563  loss_ce_5: 0.2276  loss_mask_5: 0.3298  loss_dice_5: 2.566  loss_ce_6: 0.2235  loss_mask_6: 0.3296  loss_dice_6: 2.571  loss_ce_7: 0.2301  loss_mask_7: 0.3278  loss_dice_7: 2.567  loss_ce_8: 0.2319  loss_mask_8: 0.3268  loss_dice_8: 2.562  time: 1.4782  data_time: 0.0692  lr: 1.9599e-06  max_mem: 21589M
[01/18 10:15:10] d2.utils.events INFO:  eta: 2:37:18  iter: 33479  total_loss: 32.35  loss_ce: 0.2183  loss_mask: 0.3399  loss_dice: 2.618  loss_ce_0: 0.5627  loss_mask_0: 0.3395  loss_dice_0: 2.749  loss_ce_1: 0.2618  loss_mask_1: 0.3474  loss_dice_1: 2.66  loss_ce_2: 0.2565  loss_mask_2: 0.3436  loss_dice_2: 2.64  loss_ce_3: 0.2295  loss_mask_3: 0.3419  loss_dice_3: 2.627  loss_ce_4: 0.2181  loss_mask_4: 0.3407  loss_dice_4: 2.625  loss_ce_5: 0.2177  loss_mask_5: 0.3422  loss_dice_5: 2.625  loss_ce_6: 0.2141  loss_mask_6: 0.3419  loss_dice_6: 2.621  loss_ce_7: 0.2037  loss_mask_7: 0.3409  loss_dice_7: 2.622  loss_ce_8: 0.2159  loss_mask_8: 0.3405  loss_dice_8: 2.625  time: 1.4782  data_time: 0.0726  lr: 1.9545e-06  max_mem: 21589M
[01/18 10:15:40] d2.utils.events INFO:  eta: 2:36:49  iter: 33499  total_loss: 32.07  loss_ce: 0.2211  loss_mask: 0.3357  loss_dice: 2.582  loss_ce_0: 0.5695  loss_mask_0: 0.3377  loss_dice_0: 2.714  loss_ce_1: 0.2502  loss_mask_1: 0.3411  loss_dice_1: 2.63  loss_ce_2: 0.2628  loss_mask_2: 0.3391  loss_dice_2: 2.603  loss_ce_3: 0.2414  loss_mask_3: 0.3366  loss_dice_3: 2.584  loss_ce_4: 0.2419  loss_mask_4: 0.3379  loss_dice_4: 2.586  loss_ce_5: 0.23  loss_mask_5: 0.3379  loss_dice_5: 2.584  loss_ce_6: 0.2328  loss_mask_6: 0.3374  loss_dice_6: 2.586  loss_ce_7: 0.2283  loss_mask_7: 0.3372  loss_dice_7: 2.577  loss_ce_8: 0.2308  loss_mask_8: 0.3374  loss_dice_8: 2.581  time: 1.4782  data_time: 0.0684  lr: 1.9491e-06  max_mem: 21589M
[01/18 10:16:08] d2.utils.events INFO:  eta: 2:36:14  iter: 33519  total_loss: 31.48  loss_ce: 0.2226  loss_mask: 0.3281  loss_dice: 2.53  loss_ce_0: 0.5477  loss_mask_0: 0.3256  loss_dice_0: 2.648  loss_ce_1: 0.2725  loss_mask_1: 0.3317  loss_dice_1: 2.56  loss_ce_2: 0.2476  loss_mask_2: 0.3309  loss_dice_2: 2.547  loss_ce_3: 0.2267  loss_mask_3: 0.3301  loss_dice_3: 2.538  loss_ce_4: 0.2485  loss_mask_4: 0.3296  loss_dice_4: 2.524  loss_ce_5: 0.2343  loss_mask_5: 0.3301  loss_dice_5: 2.528  loss_ce_6: 0.2356  loss_mask_6: 0.329  loss_dice_6: 2.522  loss_ce_7: 0.2199  loss_mask_7: 0.3291  loss_dice_7: 2.528  loss_ce_8: 0.2152  loss_mask_8: 0.3281  loss_dice_8: 2.525  time: 1.4782  data_time: 0.0671  lr: 1.9437e-06  max_mem: 21589M
[01/18 10:16:37] d2.utils.events INFO:  eta: 2:35:47  iter: 33539  total_loss: 31.43  loss_ce: 0.2308  loss_mask: 0.3249  loss_dice: 2.56  loss_ce_0: 0.5593  loss_mask_0: 0.3297  loss_dice_0: 2.677  loss_ce_1: 0.2707  loss_mask_1: 0.3326  loss_dice_1: 2.599  loss_ce_2: 0.262  loss_mask_2: 0.3283  loss_dice_2: 2.59  loss_ce_3: 0.2387  loss_mask_3: 0.3275  loss_dice_3: 2.572  loss_ce_4: 0.2253  loss_mask_4: 0.3268  loss_dice_4: 2.561  loss_ce_5: 0.2382  loss_mask_5: 0.3275  loss_dice_5: 2.56  loss_ce_6: 0.2305  loss_mask_6: 0.3268  loss_dice_6: 2.561  loss_ce_7: 0.2162  loss_mask_7: 0.3251  loss_dice_7: 2.563  loss_ce_8: 0.2216  loss_mask_8: 0.3248  loss_dice_8: 2.565  time: 1.4781  data_time: 0.0714  lr: 1.9383e-06  max_mem: 21589M
[01/18 10:17:06] d2.utils.events INFO:  eta: 2:35:18  iter: 33559  total_loss: 32.25  loss_ce: 0.2447  loss_mask: 0.3441  loss_dice: 2.607  loss_ce_0: 0.5433  loss_mask_0: 0.3424  loss_dice_0: 2.73  loss_ce_1: 0.2707  loss_mask_1: 0.3506  loss_dice_1: 2.641  loss_ce_2: 0.2607  loss_mask_2: 0.3458  loss_dice_2: 2.631  loss_ce_3: 0.2689  loss_mask_3: 0.3436  loss_dice_3: 2.617  loss_ce_4: 0.2728  loss_mask_4: 0.3432  loss_dice_4: 2.608  loss_ce_5: 0.2442  loss_mask_5: 0.3437  loss_dice_5: 2.609  loss_ce_6: 0.2374  loss_mask_6: 0.3441  loss_dice_6: 2.606  loss_ce_7: 0.2487  loss_mask_7: 0.3446  loss_dice_7: 2.605  loss_ce_8: 0.2488  loss_mask_8: 0.3441  loss_dice_8: 2.603  time: 1.4781  data_time: 0.0681  lr: 1.9329e-06  max_mem: 21589M
[01/18 10:17:35] d2.utils.events INFO:  eta: 2:34:49  iter: 33579  total_loss: 31.48  loss_ce: 0.2314  loss_mask: 0.3287  loss_dice: 2.51  loss_ce_0: 0.5765  loss_mask_0: 0.3313  loss_dice_0: 2.64  loss_ce_1: 0.2904  loss_mask_1: 0.3356  loss_dice_1: 2.545  loss_ce_2: 0.273  loss_mask_2: 0.3317  loss_dice_2: 2.528  loss_ce_3: 0.2714  loss_mask_3: 0.3315  loss_dice_3: 2.51  loss_ce_4: 0.2447  loss_mask_4: 0.331  loss_dice_4: 2.501  loss_ce_5: 0.2342  loss_mask_5: 0.329  loss_dice_5: 2.518  loss_ce_6: 0.2405  loss_mask_6: 0.3298  loss_dice_6: 2.505  loss_ce_7: 0.2292  loss_mask_7: 0.3285  loss_dice_7: 2.508  loss_ce_8: 0.2281  loss_mask_8: 0.3287  loss_dice_8: 2.51  time: 1.4781  data_time: 0.0665  lr: 1.9275e-06  max_mem: 21589M
[01/18 10:18:04] d2.utils.events INFO:  eta: 2:34:20  iter: 33599  total_loss: 32.31  loss_ce: 0.2187  loss_mask: 0.3267  loss_dice: 2.597  loss_ce_0: 0.5437  loss_mask_0: 0.3247  loss_dice_0: 2.738  loss_ce_1: 0.2693  loss_mask_1: 0.3296  loss_dice_1: 2.647  loss_ce_2: 0.2734  loss_mask_2: 0.3266  loss_dice_2: 2.633  loss_ce_3: 0.2338  loss_mask_3: 0.3259  loss_dice_3: 2.617  loss_ce_4: 0.2432  loss_mask_4: 0.3251  loss_dice_4: 2.609  loss_ce_5: 0.2223  loss_mask_5: 0.3245  loss_dice_5: 2.607  loss_ce_6: 0.2231  loss_mask_6: 0.326  loss_dice_6: 2.604  loss_ce_7: 0.216  loss_mask_7: 0.3255  loss_dice_7: 2.604  loss_ce_8: 0.2202  loss_mask_8: 0.324  loss_dice_8: 2.609  time: 1.4781  data_time: 0.0722  lr: 1.9221e-06  max_mem: 21589M
[01/18 10:18:33] d2.utils.events INFO:  eta: 2:33:53  iter: 33619  total_loss: 32.17  loss_ce: 0.2226  loss_mask: 0.3279  loss_dice: 2.537  loss_ce_0: 0.5713  loss_mask_0: 0.3287  loss_dice_0: 2.665  loss_ce_1: 0.2665  loss_mask_1: 0.333  loss_dice_1: 2.589  loss_ce_2: 0.2682  loss_mask_2: 0.3294  loss_dice_2: 2.566  loss_ce_3: 0.2557  loss_mask_3: 0.3285  loss_dice_3: 2.546  loss_ce_4: 0.2357  loss_mask_4: 0.329  loss_dice_4: 2.547  loss_ce_5: 0.229  loss_mask_5: 0.3282  loss_dice_5: 2.553  loss_ce_6: 0.22  loss_mask_6: 0.3274  loss_dice_6: 2.544  loss_ce_7: 0.2244  loss_mask_7: 0.328  loss_dice_7: 2.55  loss_ce_8: 0.2357  loss_mask_8: 0.3279  loss_dice_8: 2.542  time: 1.4781  data_time: 0.0676  lr: 1.9167e-06  max_mem: 21589M
[01/18 10:19:03] d2.utils.events INFO:  eta: 2:33:24  iter: 33639  total_loss: 32.12  loss_ce: 0.2076  loss_mask: 0.3307  loss_dice: 2.592  loss_ce_0: 0.5668  loss_mask_0: 0.3323  loss_dice_0: 2.747  loss_ce_1: 0.2629  loss_mask_1: 0.3362  loss_dice_1: 2.657  loss_ce_2: 0.2498  loss_mask_2: 0.3353  loss_dice_2: 2.621  loss_ce_3: 0.2418  loss_mask_3: 0.3309  loss_dice_3: 2.604  loss_ce_4: 0.2164  loss_mask_4: 0.3298  loss_dice_4: 2.601  loss_ce_5: 0.2281  loss_mask_5: 0.3293  loss_dice_5: 2.592  loss_ce_6: 0.2037  loss_mask_6: 0.3302  loss_dice_6: 2.594  loss_ce_7: 0.2076  loss_mask_7: 0.3288  loss_dice_7: 2.6  loss_ce_8: 0.1997  loss_mask_8: 0.3297  loss_dice_8: 2.596  time: 1.4780  data_time: 0.0705  lr: 1.9113e-06  max_mem: 21589M
[01/18 10:19:32] d2.utils.events INFO:  eta: 2:32:57  iter: 33659  total_loss: 32.42  loss_ce: 0.2348  loss_mask: 0.3318  loss_dice: 2.608  loss_ce_0: 0.5863  loss_mask_0: 0.3336  loss_dice_0: 2.753  loss_ce_1: 0.2823  loss_mask_1: 0.3342  loss_dice_1: 2.663  loss_ce_2: 0.2896  loss_mask_2: 0.3318  loss_dice_2: 2.632  loss_ce_3: 0.2842  loss_mask_3: 0.33  loss_dice_3: 2.62  loss_ce_4: 0.2506  loss_mask_4: 0.3307  loss_dice_4: 2.62  loss_ce_5: 0.2537  loss_mask_5: 0.3265  loss_dice_5: 2.623  loss_ce_6: 0.2427  loss_mask_6: 0.3283  loss_dice_6: 2.614  loss_ce_7: 0.2325  loss_mask_7: 0.329  loss_dice_7: 2.608  loss_ce_8: 0.2481  loss_mask_8: 0.3296  loss_dice_8: 2.61  time: 1.4780  data_time: 0.0693  lr: 1.9058e-06  max_mem: 21589M
[01/18 10:20:01] d2.utils.events INFO:  eta: 2:32:27  iter: 33679  total_loss: 31.7  loss_ce: 0.2117  loss_mask: 0.3334  loss_dice: 2.516  loss_ce_0: 0.5718  loss_mask_0: 0.3327  loss_dice_0: 2.643  loss_ce_1: 0.2778  loss_mask_1: 0.3377  loss_dice_1: 2.56  loss_ce_2: 0.2705  loss_mask_2: 0.3361  loss_dice_2: 2.536  loss_ce_3: 0.2611  loss_mask_3: 0.3346  loss_dice_3: 2.526  loss_ce_4: 0.2473  loss_mask_4: 0.3319  loss_dice_4: 2.515  loss_ce_5: 0.2278  loss_mask_5: 0.3316  loss_dice_5: 2.515  loss_ce_6: 0.2181  loss_mask_6: 0.3323  loss_dice_6: 2.511  loss_ce_7: 0.2224  loss_mask_7: 0.3322  loss_dice_7: 2.516  loss_ce_8: 0.2173  loss_mask_8: 0.3317  loss_dice_8: 2.517  time: 1.4780  data_time: 0.0689  lr: 1.9004e-06  max_mem: 21589M
[01/18 10:20:29] d2.utils.events INFO:  eta: 2:31:55  iter: 33699  total_loss: 30.97  loss_ce: 0.2262  loss_mask: 0.3351  loss_dice: 2.482  loss_ce_0: 0.5767  loss_mask_0: 0.3281  loss_dice_0: 2.613  loss_ce_1: 0.2651  loss_mask_1: 0.338  loss_dice_1: 2.531  loss_ce_2: 0.255  loss_mask_2: 0.3345  loss_dice_2: 2.512  loss_ce_3: 0.2353  loss_mask_3: 0.3358  loss_dice_3: 2.499  loss_ce_4: 0.2469  loss_mask_4: 0.3358  loss_dice_4: 2.494  loss_ce_5: 0.2229  loss_mask_5: 0.3342  loss_dice_5: 2.491  loss_ce_6: 0.2263  loss_mask_6: 0.3354  loss_dice_6: 2.485  loss_ce_7: 0.219  loss_mask_7: 0.3355  loss_dice_7: 2.489  loss_ce_8: 0.21  loss_mask_8: 0.3358  loss_dice_8: 2.491  time: 1.4780  data_time: 0.0681  lr: 1.895e-06  max_mem: 21589M
[01/18 10:20:59] d2.utils.events INFO:  eta: 2:31:30  iter: 33719  total_loss: 31.65  loss_ce: 0.2081  loss_mask: 0.3195  loss_dice: 2.572  loss_ce_0: 0.5119  loss_mask_0: 0.3144  loss_dice_0: 2.719  loss_ce_1: 0.2568  loss_mask_1: 0.3235  loss_dice_1: 2.621  loss_ce_2: 0.2388  loss_mask_2: 0.3227  loss_dice_2: 2.596  loss_ce_3: 0.217  loss_mask_3: 0.3224  loss_dice_3: 2.583  loss_ce_4: 0.2211  loss_mask_4: 0.321  loss_dice_4: 2.575  loss_ce_5: 0.2001  loss_mask_5: 0.32  loss_dice_5: 2.581  loss_ce_6: 0.1984  loss_mask_6: 0.3209  loss_dice_6: 2.57  loss_ce_7: 0.1908  loss_mask_7: 0.32  loss_dice_7: 2.574  loss_ce_8: 0.1884  loss_mask_8: 0.3201  loss_dice_8: 2.574  time: 1.4780  data_time: 0.0727  lr: 1.8896e-06  max_mem: 21589M
[01/18 10:21:28] d2.utils.events INFO:  eta: 2:31:01  iter: 33739  total_loss: 31.56  loss_ce: 0.2167  loss_mask: 0.3233  loss_dice: 2.565  loss_ce_0: 0.5591  loss_mask_0: 0.3224  loss_dice_0: 2.694  loss_ce_1: 0.2671  loss_mask_1: 0.3295  loss_dice_1: 2.606  loss_ce_2: 0.2516  loss_mask_2: 0.3276  loss_dice_2: 2.582  loss_ce_3: 0.2404  loss_mask_3: 0.3267  loss_dice_3: 2.569  loss_ce_4: 0.2333  loss_mask_4: 0.3256  loss_dice_4: 2.571  loss_ce_5: 0.2116  loss_mask_5: 0.3239  loss_dice_5: 2.559  loss_ce_6: 0.2162  loss_mask_6: 0.3231  loss_dice_6: 2.566  loss_ce_7: 0.2141  loss_mask_7: 0.3231  loss_dice_7: 2.565  loss_ce_8: 0.2118  loss_mask_8: 0.3222  loss_dice_8: 2.558  time: 1.4780  data_time: 0.0699  lr: 1.8842e-06  max_mem: 21589M
[01/18 10:21:57] d2.utils.events INFO:  eta: 2:30:30  iter: 33759  total_loss: 31.7  loss_ce: 0.2186  loss_mask: 0.3351  loss_dice: 2.504  loss_ce_0: 0.5241  loss_mask_0: 0.3321  loss_dice_0: 2.653  loss_ce_1: 0.2578  loss_mask_1: 0.3372  loss_dice_1: 2.563  loss_ce_2: 0.2484  loss_mask_2: 0.3344  loss_dice_2: 2.54  loss_ce_3: 0.2475  loss_mask_3: 0.3359  loss_dice_3: 2.523  loss_ce_4: 0.2419  loss_mask_4: 0.3371  loss_dice_4: 2.52  loss_ce_5: 0.227  loss_mask_5: 0.3363  loss_dice_5: 2.518  loss_ce_6: 0.2181  loss_mask_6: 0.3347  loss_dice_6: 2.505  loss_ce_7: 0.2192  loss_mask_7: 0.336  loss_dice_7: 2.502  loss_ce_8: 0.2146  loss_mask_8: 0.3364  loss_dice_8: 2.508  time: 1.4779  data_time: 0.0722  lr: 1.8788e-06  max_mem: 21589M
[01/18 10:22:26] d2.utils.events INFO:  eta: 2:30:02  iter: 33779  total_loss: 32.33  loss_ce: 0.2344  loss_mask: 0.3265  loss_dice: 2.575  loss_ce_0: 0.5451  loss_mask_0: 0.3241  loss_dice_0: 2.721  loss_ce_1: 0.288  loss_mask_1: 0.3299  loss_dice_1: 2.61  loss_ce_2: 0.2734  loss_mask_2: 0.3274  loss_dice_2: 2.596  loss_ce_3: 0.2652  loss_mask_3: 0.3284  loss_dice_3: 2.583  loss_ce_4: 0.2445  loss_mask_4: 0.3271  loss_dice_4: 2.577  loss_ce_5: 0.2514  loss_mask_5: 0.3261  loss_dice_5: 2.579  loss_ce_6: 0.2477  loss_mask_6: 0.3261  loss_dice_6: 2.565  loss_ce_7: 0.2379  loss_mask_7: 0.3252  loss_dice_7: 2.567  loss_ce_8: 0.2438  loss_mask_8: 0.3271  loss_dice_8: 2.571  time: 1.4779  data_time: 0.0728  lr: 1.8734e-06  max_mem: 21589M
[01/18 10:22:55] d2.utils.events INFO:  eta: 2:29:34  iter: 33799  total_loss: 31.22  loss_ce: 0.2189  loss_mask: 0.3359  loss_dice: 2.502  loss_ce_0: 0.5449  loss_mask_0: 0.3308  loss_dice_0: 2.646  loss_ce_1: 0.2768  loss_mask_1: 0.3416  loss_dice_1: 2.553  loss_ce_2: 0.2714  loss_mask_2: 0.3355  loss_dice_2: 2.522  loss_ce_3: 0.2407  loss_mask_3: 0.3356  loss_dice_3: 2.508  loss_ce_4: 0.2307  loss_mask_4: 0.3344  loss_dice_4: 2.509  loss_ce_5: 0.2333  loss_mask_5: 0.334  loss_dice_5: 2.509  loss_ce_6: 0.2179  loss_mask_6: 0.3351  loss_dice_6: 2.502  loss_ce_7: 0.2113  loss_mask_7: 0.3352  loss_dice_7: 2.5  loss_ce_8: 0.2162  loss_mask_8: 0.336  loss_dice_8: 2.511  time: 1.4779  data_time: 0.0683  lr: 1.8679e-06  max_mem: 21589M
[01/18 10:23:23] d2.utils.events INFO:  eta: 2:29:04  iter: 33819  total_loss: 31.51  loss_ce: 0.2055  loss_mask: 0.3336  loss_dice: 2.483  loss_ce_0: 0.5247  loss_mask_0: 0.325  loss_dice_0: 2.618  loss_ce_1: 0.2559  loss_mask_1: 0.3392  loss_dice_1: 2.528  loss_ce_2: 0.2498  loss_mask_2: 0.334  loss_dice_2: 2.495  loss_ce_3: 0.2341  loss_mask_3: 0.3343  loss_dice_3: 2.481  loss_ce_4: 0.2392  loss_mask_4: 0.334  loss_dice_4: 2.475  loss_ce_5: 0.2274  loss_mask_5: 0.334  loss_dice_5: 2.482  loss_ce_6: 0.2237  loss_mask_6: 0.3342  loss_dice_6: 2.47  loss_ce_7: 0.2115  loss_mask_7: 0.3328  loss_dice_7: 2.484  loss_ce_8: 0.1997  loss_mask_8: 0.3339  loss_dice_8: 2.478  time: 1.4779  data_time: 0.0698  lr: 1.8625e-06  max_mem: 21589M
[01/18 10:23:52] d2.utils.events INFO:  eta: 2:28:35  iter: 33839  total_loss: 31.74  loss_ce: 0.2294  loss_mask: 0.3238  loss_dice: 2.534  loss_ce_0: 0.5556  loss_mask_0: 0.3258  loss_dice_0: 2.689  loss_ce_1: 0.269  loss_mask_1: 0.33  loss_dice_1: 2.601  loss_ce_2: 0.2643  loss_mask_2: 0.3272  loss_dice_2: 2.567  loss_ce_3: 0.2379  loss_mask_3: 0.3256  loss_dice_3: 2.552  loss_ce_4: 0.216  loss_mask_4: 0.324  loss_dice_4: 2.556  loss_ce_5: 0.2264  loss_mask_5: 0.3225  loss_dice_5: 2.547  loss_ce_6: 0.2207  loss_mask_6: 0.3212  loss_dice_6: 2.536  loss_ce_7: 0.2194  loss_mask_7: 0.3227  loss_dice_7: 2.549  loss_ce_8: 0.2108  loss_mask_8: 0.3238  loss_dice_8: 2.543  time: 1.4779  data_time: 0.0668  lr: 1.8571e-06  max_mem: 21589M
[01/18 10:24:21] d2.utils.events INFO:  eta: 2:28:09  iter: 33859  total_loss: 31.43  loss_ce: 0.2034  loss_mask: 0.3343  loss_dice: 2.514  loss_ce_0: 0.534  loss_mask_0: 0.3289  loss_dice_0: 2.656  loss_ce_1: 0.2557  loss_mask_1: 0.3384  loss_dice_1: 2.563  loss_ce_2: 0.2446  loss_mask_2: 0.3345  loss_dice_2: 2.538  loss_ce_3: 0.2403  loss_mask_3: 0.3352  loss_dice_3: 2.526  loss_ce_4: 0.2225  loss_mask_4: 0.3347  loss_dice_4: 2.525  loss_ce_5: 0.2073  loss_mask_5: 0.3339  loss_dice_5: 2.514  loss_ce_6: 0.2098  loss_mask_6: 0.3353  loss_dice_6: 2.526  loss_ce_7: 0.2126  loss_mask_7: 0.3343  loss_dice_7: 2.512  loss_ce_8: 0.2042  loss_mask_8: 0.3349  loss_dice_8: 2.511  time: 1.4778  data_time: 0.0686  lr: 1.8517e-06  max_mem: 21589M
[01/18 10:24:50] d2.utils.events INFO:  eta: 2:27:40  iter: 33879  total_loss: 31.82  loss_ce: 0.2139  loss_mask: 0.3269  loss_dice: 2.542  loss_ce_0: 0.5698  loss_mask_0: 0.3271  loss_dice_0: 2.712  loss_ce_1: 0.2849  loss_mask_1: 0.3309  loss_dice_1: 2.606  loss_ce_2: 0.2649  loss_mask_2: 0.3279  loss_dice_2: 2.572  loss_ce_3: 0.2394  loss_mask_3: 0.328  loss_dice_3: 2.555  loss_ce_4: 0.2435  loss_mask_4: 0.3273  loss_dice_4: 2.56  loss_ce_5: 0.22  loss_mask_5: 0.3257  loss_dice_5: 2.548  loss_ce_6: 0.2244  loss_mask_6: 0.3268  loss_dice_6: 2.545  loss_ce_7: 0.2173  loss_mask_7: 0.3263  loss_dice_7: 2.551  loss_ce_8: 0.2054  loss_mask_8: 0.3259  loss_dice_8: 2.546  time: 1.4778  data_time: 0.0740  lr: 1.8462e-06  max_mem: 21589M
[01/18 10:25:19] d2.utils.events INFO:  eta: 2:27:11  iter: 33899  total_loss: 31.69  loss_ce: 0.2271  loss_mask: 0.3317  loss_dice: 2.54  loss_ce_0: 0.5347  loss_mask_0: 0.3262  loss_dice_0: 2.669  loss_ce_1: 0.2756  loss_mask_1: 0.3387  loss_dice_1: 2.6  loss_ce_2: 0.2654  loss_mask_2: 0.335  loss_dice_2: 2.571  loss_ce_3: 0.2512  loss_mask_3: 0.3333  loss_dice_3: 2.545  loss_ce_4: 0.2316  loss_mask_4: 0.3339  loss_dice_4: 2.558  loss_ce_5: 0.2309  loss_mask_5: 0.3342  loss_dice_5: 2.544  loss_ce_6: 0.2227  loss_mask_6: 0.3326  loss_dice_6: 2.552  loss_ce_7: 0.2115  loss_mask_7: 0.3327  loss_dice_7: 2.547  loss_ce_8: 0.2108  loss_mask_8: 0.3334  loss_dice_8: 2.544  time: 1.4778  data_time: 0.0695  lr: 1.8408e-06  max_mem: 21589M
[01/18 10:25:48] d2.utils.events INFO:  eta: 2:26:39  iter: 33919  total_loss: 31.72  loss_ce: 0.2373  loss_mask: 0.3301  loss_dice: 2.54  loss_ce_0: 0.5649  loss_mask_0: 0.3271  loss_dice_0: 2.681  loss_ce_1: 0.2872  loss_mask_1: 0.337  loss_dice_1: 2.593  loss_ce_2: 0.2843  loss_mask_2: 0.3347  loss_dice_2: 2.566  loss_ce_3: 0.2598  loss_mask_3: 0.3334  loss_dice_3: 2.553  loss_ce_4: 0.2496  loss_mask_4: 0.3322  loss_dice_4: 2.551  loss_ce_5: 0.2411  loss_mask_5: 0.3288  loss_dice_5: 2.548  loss_ce_6: 0.2339  loss_mask_6: 0.3296  loss_dice_6: 2.552  loss_ce_7: 0.2269  loss_mask_7: 0.3296  loss_dice_7: 2.541  loss_ce_8: 0.2291  loss_mask_8: 0.3295  loss_dice_8: 2.548  time: 1.4778  data_time: 0.0697  lr: 1.8354e-06  max_mem: 21589M
[01/18 10:26:17] d2.utils.events INFO:  eta: 2:26:09  iter: 33939  total_loss: 31.63  loss_ce: 0.2034  loss_mask: 0.3291  loss_dice: 2.548  loss_ce_0: 0.523  loss_mask_0: 0.3279  loss_dice_0: 2.711  loss_ce_1: 0.2639  loss_mask_1: 0.3318  loss_dice_1: 2.608  loss_ce_2: 0.2479  loss_mask_2: 0.3279  loss_dice_2: 2.577  loss_ce_3: 0.2259  loss_mask_3: 0.3285  loss_dice_3: 2.565  loss_ce_4: 0.2199  loss_mask_4: 0.3287  loss_dice_4: 2.561  loss_ce_5: 0.2115  loss_mask_5: 0.3283  loss_dice_5: 2.557  loss_ce_6: 0.2085  loss_mask_6: 0.3278  loss_dice_6: 2.562  loss_ce_7: 0.2038  loss_mask_7: 0.3287  loss_dice_7: 2.56  loss_ce_8: 0.2029  loss_mask_8: 0.3286  loss_dice_8: 2.557  time: 1.4778  data_time: 0.0668  lr: 1.8299e-06  max_mem: 21589M
[01/18 10:26:46] d2.utils.events INFO:  eta: 2:25:41  iter: 33959  total_loss: 31  loss_ce: 0.2252  loss_mask: 0.3304  loss_dice: 2.495  loss_ce_0: 0.5548  loss_mask_0: 0.3296  loss_dice_0: 2.643  loss_ce_1: 0.2703  loss_mask_1: 0.3374  loss_dice_1: 2.541  loss_ce_2: 0.2581  loss_mask_2: 0.3332  loss_dice_2: 2.514  loss_ce_3: 0.2318  loss_mask_3: 0.3312  loss_dice_3: 2.504  loss_ce_4: 0.2215  loss_mask_4: 0.3309  loss_dice_4: 2.502  loss_ce_5: 0.2107  loss_mask_5: 0.3308  loss_dice_5: 2.495  loss_ce_6: 0.2175  loss_mask_6: 0.331  loss_dice_6: 2.491  loss_ce_7: 0.2178  loss_mask_7: 0.3305  loss_dice_7: 2.495  loss_ce_8: 0.2057  loss_mask_8: 0.33  loss_dice_8: 2.495  time: 1.4777  data_time: 0.0677  lr: 1.8245e-06  max_mem: 21589M
[01/18 10:27:15] d2.utils.events INFO:  eta: 2:25:15  iter: 33979  total_loss: 32.68  loss_ce: 0.2252  loss_mask: 0.3299  loss_dice: 2.653  loss_ce_0: 0.552  loss_mask_0: 0.3298  loss_dice_0: 2.785  loss_ce_1: 0.2631  loss_mask_1: 0.3367  loss_dice_1: 2.696  loss_ce_2: 0.2578  loss_mask_2: 0.3333  loss_dice_2: 2.68  loss_ce_3: 0.2358  loss_mask_3: 0.3316  loss_dice_3: 2.662  loss_ce_4: 0.2409  loss_mask_4: 0.3306  loss_dice_4: 2.661  loss_ce_5: 0.2251  loss_mask_5: 0.331  loss_dice_5: 2.668  loss_ce_6: 0.2143  loss_mask_6: 0.3309  loss_dice_6: 2.664  loss_ce_7: 0.2264  loss_mask_7: 0.3313  loss_dice_7: 2.665  loss_ce_8: 0.2159  loss_mask_8: 0.3309  loss_dice_8: 2.66  time: 1.4777  data_time: 0.0708  lr: 1.8191e-06  max_mem: 21589M
[01/18 10:27:44] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 10:27:45] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 10:27:45] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 10:27:46] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 10:28:00] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0086 s/iter. Inference: 0.1923 s/iter. Eval: 0.2334 s/iter. Total: 0.4344 s/iter. ETA=0:07:49
[01/18 10:28:06] d2.evaluation.evaluator INFO: Inference done 24/1093. Dataloading: 0.0110 s/iter. Inference: 0.1737 s/iter. Eval: 0.2240 s/iter. Total: 0.4088 s/iter. ETA=0:07:16
[01/18 10:28:11] d2.evaluation.evaluator INFO: Inference done 38/1093. Dataloading: 0.0114 s/iter. Inference: 0.1640 s/iter. Eval: 0.2168 s/iter. Total: 0.3923 s/iter. ETA=0:06:53
[01/18 10:28:16] d2.evaluation.evaluator INFO: Inference done 50/1093. Dataloading: 0.0121 s/iter. Inference: 0.1627 s/iter. Eval: 0.2240 s/iter. Total: 0.3989 s/iter. ETA=0:06:56
[01/18 10:28:21] d2.evaluation.evaluator INFO: Inference done 63/1093. Dataloading: 0.0124 s/iter. Inference: 0.1607 s/iter. Eval: 0.2316 s/iter. Total: 0.4047 s/iter. ETA=0:06:56
[01/18 10:28:26] d2.evaluation.evaluator INFO: Inference done 76/1093. Dataloading: 0.0123 s/iter. Inference: 0.1602 s/iter. Eval: 0.2311 s/iter. Total: 0.4037 s/iter. ETA=0:06:50
[01/18 10:28:32] d2.evaluation.evaluator INFO: Inference done 88/1093. Dataloading: 0.0123 s/iter. Inference: 0.1598 s/iter. Eval: 0.2370 s/iter. Total: 0.4092 s/iter. ETA=0:06:51
[01/18 10:28:37] d2.evaluation.evaluator INFO: Inference done 100/1093. Dataloading: 0.0123 s/iter. Inference: 0.1600 s/iter. Eval: 0.2387 s/iter. Total: 0.4112 s/iter. ETA=0:06:48
[01/18 10:28:42] d2.evaluation.evaluator INFO: Inference done 114/1093. Dataloading: 0.0122 s/iter. Inference: 0.1589 s/iter. Eval: 0.2366 s/iter. Total: 0.4078 s/iter. ETA=0:06:39
[01/18 10:28:48] d2.evaluation.evaluator INFO: Inference done 127/1093. Dataloading: 0.0122 s/iter. Inference: 0.1590 s/iter. Eval: 0.2360 s/iter. Total: 0.4074 s/iter. ETA=0:06:33
[01/18 10:28:53] d2.evaluation.evaluator INFO: Inference done 141/1093. Dataloading: 0.0121 s/iter. Inference: 0.1577 s/iter. Eval: 0.2323 s/iter. Total: 0.4022 s/iter. ETA=0:06:22
[01/18 10:28:58] d2.evaluation.evaluator INFO: Inference done 156/1093. Dataloading: 0.0120 s/iter. Inference: 0.1578 s/iter. Eval: 0.2271 s/iter. Total: 0.3970 s/iter. ETA=0:06:11
[01/18 10:29:03] d2.evaluation.evaluator INFO: Inference done 168/1093. Dataloading: 0.0122 s/iter. Inference: 0.1579 s/iter. Eval: 0.2295 s/iter. Total: 0.3997 s/iter. ETA=0:06:09
[01/18 10:29:08] d2.evaluation.evaluator INFO: Inference done 182/1093. Dataloading: 0.0122 s/iter. Inference: 0.1579 s/iter. Eval: 0.2280 s/iter. Total: 0.3982 s/iter. ETA=0:06:02
[01/18 10:29:14] d2.evaluation.evaluator INFO: Inference done 196/1093. Dataloading: 0.0121 s/iter. Inference: 0.1579 s/iter. Eval: 0.2266 s/iter. Total: 0.3967 s/iter. ETA=0:05:55
[01/18 10:29:19] d2.evaluation.evaluator INFO: Inference done 208/1093. Dataloading: 0.0122 s/iter. Inference: 0.1583 s/iter. Eval: 0.2283 s/iter. Total: 0.3989 s/iter. ETA=0:05:53
[01/18 10:29:24] d2.evaluation.evaluator INFO: Inference done 223/1093. Dataloading: 0.0121 s/iter. Inference: 0.1571 s/iter. Eval: 0.2263 s/iter. Total: 0.3956 s/iter. ETA=0:05:44
[01/18 10:29:29] d2.evaluation.evaluator INFO: Inference done 236/1093. Dataloading: 0.0121 s/iter. Inference: 0.1572 s/iter. Eval: 0.2271 s/iter. Total: 0.3965 s/iter. ETA=0:05:39
[01/18 10:29:34] d2.evaluation.evaluator INFO: Inference done 249/1093. Dataloading: 0.0121 s/iter. Inference: 0.1567 s/iter. Eval: 0.2272 s/iter. Total: 0.3962 s/iter. ETA=0:05:34
[01/18 10:29:40] d2.evaluation.evaluator INFO: Inference done 263/1093. Dataloading: 0.0121 s/iter. Inference: 0.1567 s/iter. Eval: 0.2266 s/iter. Total: 0.3955 s/iter. ETA=0:05:28
[01/18 10:29:45] d2.evaluation.evaluator INFO: Inference done 276/1093. Dataloading: 0.0122 s/iter. Inference: 0.1565 s/iter. Eval: 0.2269 s/iter. Total: 0.3957 s/iter. ETA=0:05:23
[01/18 10:29:50] d2.evaluation.evaluator INFO: Inference done 290/1093. Dataloading: 0.0121 s/iter. Inference: 0.1563 s/iter. Eval: 0.2254 s/iter. Total: 0.3939 s/iter. ETA=0:05:16
[01/18 10:29:55] d2.evaluation.evaluator INFO: Inference done 302/1093. Dataloading: 0.0121 s/iter. Inference: 0.1563 s/iter. Eval: 0.2267 s/iter. Total: 0.3952 s/iter. ETA=0:05:12
[01/18 10:30:00] d2.evaluation.evaluator INFO: Inference done 313/1093. Dataloading: 0.0122 s/iter. Inference: 0.1567 s/iter. Eval: 0.2285 s/iter. Total: 0.3975 s/iter. ETA=0:05:10
[01/18 10:30:06] d2.evaluation.evaluator INFO: Inference done 327/1093. Dataloading: 0.0122 s/iter. Inference: 0.1562 s/iter. Eval: 0.2282 s/iter. Total: 0.3967 s/iter. ETA=0:05:03
[01/18 10:30:11] d2.evaluation.evaluator INFO: Inference done 345/1093. Dataloading: 0.0120 s/iter. Inference: 0.1553 s/iter. Eval: 0.2232 s/iter. Total: 0.3906 s/iter. ETA=0:04:52
[01/18 10:30:16] d2.evaluation.evaluator INFO: Inference done 361/1093. Dataloading: 0.0120 s/iter. Inference: 0.1546 s/iter. Eval: 0.2216 s/iter. Total: 0.3883 s/iter. ETA=0:04:44
[01/18 10:30:22] d2.evaluation.evaluator INFO: Inference done 374/1093. Dataloading: 0.0121 s/iter. Inference: 0.1549 s/iter. Eval: 0.2222 s/iter. Total: 0.3893 s/iter. ETA=0:04:39
[01/18 10:30:27] d2.evaluation.evaluator INFO: Inference done 389/1093. Dataloading: 0.0121 s/iter. Inference: 0.1545 s/iter. Eval: 0.2215 s/iter. Total: 0.3882 s/iter. ETA=0:04:33
[01/18 10:30:32] d2.evaluation.evaluator INFO: Inference done 403/1093. Dataloading: 0.0121 s/iter. Inference: 0.1541 s/iter. Eval: 0.2218 s/iter. Total: 0.3881 s/iter. ETA=0:04:27
[01/18 10:30:38] d2.evaluation.evaluator INFO: Inference done 417/1093. Dataloading: 0.0121 s/iter. Inference: 0.1542 s/iter. Eval: 0.2215 s/iter. Total: 0.3879 s/iter. ETA=0:04:22
[01/18 10:30:43] d2.evaluation.evaluator INFO: Inference done 428/1093. Dataloading: 0.0122 s/iter. Inference: 0.1540 s/iter. Eval: 0.2234 s/iter. Total: 0.3898 s/iter. ETA=0:04:19
[01/18 10:30:48] d2.evaluation.evaluator INFO: Inference done 442/1093. Dataloading: 0.0122 s/iter. Inference: 0.1541 s/iter. Eval: 0.2225 s/iter. Total: 0.3889 s/iter. ETA=0:04:13
[01/18 10:30:53] d2.evaluation.evaluator INFO: Inference done 457/1093. Dataloading: 0.0122 s/iter. Inference: 0.1538 s/iter. Eval: 0.2213 s/iter. Total: 0.3875 s/iter. ETA=0:04:06
[01/18 10:30:58] d2.evaluation.evaluator INFO: Inference done 471/1093. Dataloading: 0.0122 s/iter. Inference: 0.1539 s/iter. Eval: 0.2207 s/iter. Total: 0.3868 s/iter. ETA=0:04:00
[01/18 10:31:03] d2.evaluation.evaluator INFO: Inference done 486/1093. Dataloading: 0.0121 s/iter. Inference: 0.1537 s/iter. Eval: 0.2196 s/iter. Total: 0.3856 s/iter. ETA=0:03:54
[01/18 10:31:09] d2.evaluation.evaluator INFO: Inference done 502/1093. Dataloading: 0.0120 s/iter. Inference: 0.1535 s/iter. Eval: 0.2181 s/iter. Total: 0.3837 s/iter. ETA=0:03:46
[01/18 10:31:14] d2.evaluation.evaluator INFO: Inference done 518/1093. Dataloading: 0.0120 s/iter. Inference: 0.1533 s/iter. Eval: 0.2169 s/iter. Total: 0.3823 s/iter. ETA=0:03:39
[01/18 10:31:19] d2.evaluation.evaluator INFO: Inference done 530/1093. Dataloading: 0.0120 s/iter. Inference: 0.1530 s/iter. Eval: 0.2182 s/iter. Total: 0.3834 s/iter. ETA=0:03:35
[01/18 10:31:24] d2.evaluation.evaluator INFO: Inference done 544/1093. Dataloading: 0.0120 s/iter. Inference: 0.1532 s/iter. Eval: 0.2181 s/iter. Total: 0.3834 s/iter. ETA=0:03:30
[01/18 10:31:30] d2.evaluation.evaluator INFO: Inference done 556/1093. Dataloading: 0.0120 s/iter. Inference: 0.1537 s/iter. Eval: 0.2190 s/iter. Total: 0.3849 s/iter. ETA=0:03:26
[01/18 10:31:35] d2.evaluation.evaluator INFO: Inference done 570/1093. Dataloading: 0.0120 s/iter. Inference: 0.1540 s/iter. Eval: 0.2186 s/iter. Total: 0.3847 s/iter. ETA=0:03:21
[01/18 10:31:40] d2.evaluation.evaluator INFO: Inference done 587/1093. Dataloading: 0.0121 s/iter. Inference: 0.1538 s/iter. Eval: 0.2162 s/iter. Total: 0.3821 s/iter. ETA=0:03:13
[01/18 10:31:46] d2.evaluation.evaluator INFO: Inference done 601/1093. Dataloading: 0.0121 s/iter. Inference: 0.1536 s/iter. Eval: 0.2165 s/iter. Total: 0.3823 s/iter. ETA=0:03:08
[01/18 10:31:51] d2.evaluation.evaluator INFO: Inference done 614/1093. Dataloading: 0.0121 s/iter. Inference: 0.1536 s/iter. Eval: 0.2168 s/iter. Total: 0.3825 s/iter. ETA=0:03:03
[01/18 10:31:56] d2.evaluation.evaluator INFO: Inference done 628/1093. Dataloading: 0.0121 s/iter. Inference: 0.1536 s/iter. Eval: 0.2165 s/iter. Total: 0.3823 s/iter. ETA=0:02:57
[01/18 10:32:01] d2.evaluation.evaluator INFO: Inference done 642/1093. Dataloading: 0.0121 s/iter. Inference: 0.1537 s/iter. Eval: 0.2162 s/iter. Total: 0.3820 s/iter. ETA=0:02:52
[01/18 10:32:06] d2.evaluation.evaluator INFO: Inference done 655/1093. Dataloading: 0.0121 s/iter. Inference: 0.1539 s/iter. Eval: 0.2161 s/iter. Total: 0.3822 s/iter. ETA=0:02:47
[01/18 10:32:11] d2.evaluation.evaluator INFO: Inference done 668/1093. Dataloading: 0.0121 s/iter. Inference: 0.1540 s/iter. Eval: 0.2161 s/iter. Total: 0.3823 s/iter. ETA=0:02:42
[01/18 10:32:16] d2.evaluation.evaluator INFO: Inference done 684/1093. Dataloading: 0.0120 s/iter. Inference: 0.1539 s/iter. Eval: 0.2149 s/iter. Total: 0.3809 s/iter. ETA=0:02:35
[01/18 10:32:22] d2.evaluation.evaluator INFO: Inference done 697/1093. Dataloading: 0.0120 s/iter. Inference: 0.1541 s/iter. Eval: 0.2151 s/iter. Total: 0.3812 s/iter. ETA=0:02:30
[01/18 10:32:27] d2.evaluation.evaluator INFO: Inference done 709/1093. Dataloading: 0.0120 s/iter. Inference: 0.1540 s/iter. Eval: 0.2158 s/iter. Total: 0.3820 s/iter. ETA=0:02:26
[01/18 10:32:32] d2.evaluation.evaluator INFO: Inference done 721/1093. Dataloading: 0.0121 s/iter. Inference: 0.1545 s/iter. Eval: 0.2162 s/iter. Total: 0.3829 s/iter. ETA=0:02:22
[01/18 10:32:37] d2.evaluation.evaluator INFO: Inference done 737/1093. Dataloading: 0.0120 s/iter. Inference: 0.1544 s/iter. Eval: 0.2152 s/iter. Total: 0.3817 s/iter. ETA=0:02:15
[01/18 10:32:42] d2.evaluation.evaluator INFO: Inference done 752/1093. Dataloading: 0.0120 s/iter. Inference: 0.1543 s/iter. Eval: 0.2146 s/iter. Total: 0.3809 s/iter. ETA=0:02:09
[01/18 10:32:48] d2.evaluation.evaluator INFO: Inference done 765/1093. Dataloading: 0.0120 s/iter. Inference: 0.1544 s/iter. Eval: 0.2149 s/iter. Total: 0.3814 s/iter. ETA=0:02:05
[01/18 10:32:53] d2.evaluation.evaluator INFO: Inference done 780/1093. Dataloading: 0.0120 s/iter. Inference: 0.1543 s/iter. Eval: 0.2142 s/iter. Total: 0.3806 s/iter. ETA=0:01:59
[01/18 10:32:58] d2.evaluation.evaluator INFO: Inference done 794/1093. Dataloading: 0.0120 s/iter. Inference: 0.1544 s/iter. Eval: 0.2141 s/iter. Total: 0.3805 s/iter. ETA=0:01:53
[01/18 10:33:03] d2.evaluation.evaluator INFO: Inference done 808/1093. Dataloading: 0.0119 s/iter. Inference: 0.1544 s/iter. Eval: 0.2138 s/iter. Total: 0.3803 s/iter. ETA=0:01:48
[01/18 10:33:09] d2.evaluation.evaluator INFO: Inference done 824/1093. Dataloading: 0.0119 s/iter. Inference: 0.1543 s/iter. Eval: 0.2130 s/iter. Total: 0.3793 s/iter. ETA=0:01:42
[01/18 10:33:14] d2.evaluation.evaluator INFO: Inference done 839/1093. Dataloading: 0.0119 s/iter. Inference: 0.1543 s/iter. Eval: 0.2124 s/iter. Total: 0.3787 s/iter. ETA=0:01:36
[01/18 10:33:19] d2.evaluation.evaluator INFO: Inference done 852/1093. Dataloading: 0.0119 s/iter. Inference: 0.1543 s/iter. Eval: 0.2125 s/iter. Total: 0.3788 s/iter. ETA=0:01:31
[01/18 10:33:24] d2.evaluation.evaluator INFO: Inference done 865/1093. Dataloading: 0.0119 s/iter. Inference: 0.1541 s/iter. Eval: 0.2128 s/iter. Total: 0.3789 s/iter. ETA=0:01:26
[01/18 10:33:29] d2.evaluation.evaluator INFO: Inference done 879/1093. Dataloading: 0.0118 s/iter. Inference: 0.1542 s/iter. Eval: 0.2126 s/iter. Total: 0.3788 s/iter. ETA=0:01:21
[01/18 10:33:34] d2.evaluation.evaluator INFO: Inference done 891/1093. Dataloading: 0.0119 s/iter. Inference: 0.1542 s/iter. Eval: 0.2134 s/iter. Total: 0.3795 s/iter. ETA=0:01:16
[01/18 10:33:39] d2.evaluation.evaluator INFO: Inference done 904/1093. Dataloading: 0.0119 s/iter. Inference: 0.1542 s/iter. Eval: 0.2134 s/iter. Total: 0.3796 s/iter. ETA=0:01:11
[01/18 10:33:44] d2.evaluation.evaluator INFO: Inference done 917/1093. Dataloading: 0.0118 s/iter. Inference: 0.1542 s/iter. Eval: 0.2137 s/iter. Total: 0.3799 s/iter. ETA=0:01:06
[01/18 10:33:50] d2.evaluation.evaluator INFO: Inference done 933/1093. Dataloading: 0.0118 s/iter. Inference: 0.1540 s/iter. Eval: 0.2132 s/iter. Total: 0.3791 s/iter. ETA=0:01:00
[01/18 10:33:55] d2.evaluation.evaluator INFO: Inference done 946/1093. Dataloading: 0.0118 s/iter. Inference: 0.1541 s/iter. Eval: 0.2136 s/iter. Total: 0.3796 s/iter. ETA=0:00:55
[01/18 10:34:00] d2.evaluation.evaluator INFO: Inference done 961/1093. Dataloading: 0.0118 s/iter. Inference: 0.1540 s/iter. Eval: 0.2132 s/iter. Total: 0.3791 s/iter. ETA=0:00:50
[01/18 10:34:06] d2.evaluation.evaluator INFO: Inference done 976/1093. Dataloading: 0.0118 s/iter. Inference: 0.1539 s/iter. Eval: 0.2129 s/iter. Total: 0.3787 s/iter. ETA=0:00:44
[01/18 10:34:11] d2.evaluation.evaluator INFO: Inference done 992/1093. Dataloading: 0.0118 s/iter. Inference: 0.1538 s/iter. Eval: 0.2122 s/iter. Total: 0.3778 s/iter. ETA=0:00:38
[01/18 10:34:16] d2.evaluation.evaluator INFO: Inference done 1006/1093. Dataloading: 0.0117 s/iter. Inference: 0.1539 s/iter. Eval: 0.2120 s/iter. Total: 0.3777 s/iter. ETA=0:00:32
[01/18 10:34:21] d2.evaluation.evaluator INFO: Inference done 1021/1093. Dataloading: 0.0117 s/iter. Inference: 0.1537 s/iter. Eval: 0.2117 s/iter. Total: 0.3772 s/iter. ETA=0:00:27
[01/18 10:34:26] d2.evaluation.evaluator INFO: Inference done 1034/1093. Dataloading: 0.0117 s/iter. Inference: 0.1537 s/iter. Eval: 0.2119 s/iter. Total: 0.3774 s/iter. ETA=0:00:22
[01/18 10:34:31] d2.evaluation.evaluator INFO: Inference done 1049/1093. Dataloading: 0.0117 s/iter. Inference: 0.1535 s/iter. Eval: 0.2116 s/iter. Total: 0.3770 s/iter. ETA=0:00:16
[01/18 10:34:36] d2.evaluation.evaluator INFO: Inference done 1064/1093. Dataloading: 0.0117 s/iter. Inference: 0.1533 s/iter. Eval: 0.2112 s/iter. Total: 0.3764 s/iter. ETA=0:00:10
[01/18 10:34:42] d2.evaluation.evaluator INFO: Inference done 1082/1093. Dataloading: 0.0116 s/iter. Inference: 0.1532 s/iter. Eval: 0.2101 s/iter. Total: 0.3750 s/iter. ETA=0:00:04
[01/18 10:34:46] d2.evaluation.evaluator INFO: Total inference time: 0:06:48.215840 (0.375198 s / iter per device, on 4 devices)
[01/18 10:34:46] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:46 (0.153182 s / iter per device, on 4 devices)
[01/18 10:35:10] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.32768035024258, 'mIoU': 20.85337749409397, 'fwIoU': 43.88040725168437, 'IoU-0': nan, 'IoU-1': 95.47841909817909, 'IoU-2': 48.33985171228091, 'IoU-3': 59.41387765358896, 'IoU-4': 53.39006641353738, 'IoU-5': 47.00139743614894, 'IoU-6': 41.87105630244106, 'IoU-7': 34.25457991156033, 'IoU-8': 24.445846337170586, 'IoU-9': 35.86455153497864, 'IoU-10': 41.43572774734239, 'IoU-11': 50.7034662781479, 'IoU-12': 51.85991028595032, 'IoU-13': 51.51921565311067, 'IoU-14': 51.341693967496994, 'IoU-15': 51.14306958438611, 'IoU-16': 51.81727046550692, 'IoU-17': 47.677462652238425, 'IoU-18': 48.70422329386402, 'IoU-19': 48.83377028948588, 'IoU-20': 49.17227105712547, 'IoU-21': 48.353596234075155, 'IoU-22': 49.93458000434495, 'IoU-23': 47.103674478384775, 'IoU-24': 46.932224177706004, 'IoU-25': 47.056800570680025, 'IoU-26': 46.1292172019104, 'IoU-27': 47.89102959594423, 'IoU-28': 45.91616126068988, 'IoU-29': 46.77972762894425, 'IoU-30': 45.50510520951108, 'IoU-31': 47.06522881857881, 'IoU-32': 45.5907524785773, 'IoU-33': 44.737251694294436, 'IoU-34': 43.174438390520706, 'IoU-35': 44.90696164846894, 'IoU-36': 45.26513783164788, 'IoU-37': 44.07028945301616, 'IoU-38': 43.39717477053192, 'IoU-39': 43.38611742831605, 'IoU-40': 43.733939278654724, 'IoU-41': 41.32157934282046, 'IoU-42': 41.06239759821124, 'IoU-43': 40.0191309270489, 'IoU-44': 40.11926607196957, 'IoU-45': 38.748584504349544, 'IoU-46': 38.16000293181126, 'IoU-47': 37.325243404648475, 'IoU-48': 38.00364466587473, 'IoU-49': 36.87637157082713, 'IoU-50': 37.01695566238774, 'IoU-51': 35.30439913351929, 'IoU-52': 35.036295089536736, 'IoU-53': 34.171042984630354, 'IoU-54': 34.39470792671759, 'IoU-55': 32.71132725499565, 'IoU-56': 32.01427198441613, 'IoU-57': 30.334759987459236, 'IoU-58': 28.703717503818797, 'IoU-59': 27.83238012285781, 'IoU-60': 26.70756800569864, 'IoU-61': 26.89301056199469, 'IoU-62': 26.81333673559064, 'IoU-63': 26.37085940833473, 'IoU-64': 25.431671781549515, 'IoU-65': 24.714420882800184, 'IoU-66': 22.697784109412417, 'IoU-67': 22.874793875946914, 'IoU-68': 22.13156109538587, 'IoU-69': 22.365022742045113, 'IoU-70': 22.28276031848146, 'IoU-71': 19.797344323099114, 'IoU-72': 20.543477554993572, 'IoU-73': 20.429417065232418, 'IoU-74': 21.280774539491286, 'IoU-75': 20.185017011366327, 'IoU-76': 20.302286106396043, 'IoU-77': 19.857709149527224, 'IoU-78': 19.10777598954901, 'IoU-79': 19.398629580133587, 'IoU-80': 18.474537117868742, 'IoU-81': 19.167532450189626, 'IoU-82': 18.352603927208445, 'IoU-83': 19.160578313176785, 'IoU-84': 18.232074591774, 'IoU-85': 18.43560652313452, 'IoU-86': 17.784527380094854, 'IoU-87': 17.199871345910417, 'IoU-88': 17.162717337635033, 'IoU-89': 16.931153340482446, 'IoU-90': 17.058513480647544, 'IoU-91': 16.92110116030691, 'IoU-92': 15.524525183081847, 'IoU-93': 16.807442177906204, 'IoU-94': 16.73578900935458, 'IoU-95': 16.77112092686654, 'IoU-96': 16.135826759935874, 'IoU-97': 16.634432930773187, 'IoU-98': 16.331716586695062, 'IoU-99': 15.18690415180962, 'IoU-100': 15.395936742467967, 'IoU-101': 15.350298254056636, 'IoU-102': 14.577005915703397, 'IoU-103': 14.53423204443233, 'IoU-104': 14.129553571906678, 'IoU-105': 13.215266598273583, 'IoU-106': 13.668855962346438, 'IoU-107': 14.564271233991038, 'IoU-108': 13.085176307225929, 'IoU-109': 13.67253398336481, 'IoU-110': 13.377680010900214, 'IoU-111': 12.446684208484427, 'IoU-112': 12.562560927375571, 'IoU-113': 12.458997986825494, 'IoU-114': 12.851048965539698, 'IoU-115': 11.823938742479326, 'IoU-116': 11.679224022753, 'IoU-117': 11.15073268613869, 'IoU-118': 10.93572637177289, 'IoU-119': 12.43744202828692, 'IoU-120': 10.998228589724025, 'IoU-121': 10.623359587751276, 'IoU-122': 10.91399022641531, 'IoU-123': 9.864877790527867, 'IoU-124': 10.576508537337515, 'IoU-125': 9.691901499196828, 'IoU-126': 9.021109347020282, 'IoU-127': 9.523033174548184, 'IoU-128': 9.159742505347507, 'IoU-129': 7.933086901192536, 'IoU-130': 7.7569741065381805, 'IoU-131': 7.6487701609516945, 'IoU-132': 7.207317861763045, 'IoU-133': 8.14310019499832, 'IoU-134': 6.870968596577395, 'IoU-135': 7.434146021641887, 'IoU-136': 7.1617820108713826, 'IoU-137': 6.154766496957361, 'IoU-138': 7.643459077924448, 'IoU-139': 6.1795021160686785, 'IoU-140': 6.216367703046249, 'IoU-141': 6.2574921065096225, 'IoU-142': 6.1609767404550135, 'IoU-143': 6.184441231658925, 'IoU-144': 6.07857258126884, 'IoU-145': 7.563185230444813, 'IoU-146': 4.468533884800445, 'IoU-147': 5.824090463667997, 'IoU-148': 5.158707317229162, 'IoU-149': 4.617699648785065, 'IoU-150': 5.425192084688558, 'IoU-151': 3.699661425580527, 'IoU-152': 4.432225090160636, 'IoU-153': 4.298280873590751, 'IoU-154': 3.8567977350266416, 'IoU-155': 4.3265103409131225, 'IoU-156': 3.926306079558242, 'IoU-157': 2.74872887401585, 'IoU-158': 3.4917238665602945, 'IoU-159': 2.3762829851612097, 'IoU-160': 4.20432057899562, 'IoU-161': 3.558827446346179, 'IoU-162': 3.23235783875712, 'IoU-163': 2.399138000293004, 'IoU-164': 2.8148627374867914, 'IoU-165': 3.071293340471518, 'IoU-166': 2.554415792921847, 'IoU-167': 2.530866432528332, 'IoU-168': 3.019768580856968, 'IoU-169': 2.4177312148406105, 'IoU-170': 2.8671757427122233, 'IoU-171': 1.289854891324726, 'IoU-172': 1.9440130730895324, 'IoU-173': 2.285408197121054, 'IoU-174': 2.517173583341709, 'IoU-175': 1.0629541499836062, 'IoU-176': 1.6064348669385893, 'IoU-177': 1.5966218987146512, 'IoU-178': 1.6870640464155127, 'IoU-179': 2.605756222981629, 'IoU-180': 3.1568383070294597, 'IoU-181': 2.1151945790786315, 'IoU-182': 0.9596508603683036, 'IoU-183': 1.2064981949458482, 'IoU-184': 2.0352824616923653, 'IoU-185': 2.973345041641255, 'IoU-186': 2.724927352656285, 'IoU-187': 2.397420894217503, 'IoU-188': 2.017771179706628, 'IoU-189': 1.8583563493370512, 'IoU-190': 2.2374015270876257, 'IoU-191': 2.9637983281447378, 'mACC': 31.193588788869757, 'pACC': 58.255192960587685, 'ACC-0': nan, 'ACC-1': 98.63753301568433, 'ACC-2': 62.04569222076278, 'ACC-3': 74.30335227417152, 'ACC-4': 70.72748167249313, 'ACC-5': 64.28035976093794, 'ACC-6': 58.44539068738284, 'ACC-7': 49.43008412002144, 'ACC-8': 31.755438518177993, 'ACC-9': 45.71061102781881, 'ACC-10': 56.95757395911131, 'ACC-11': 67.0438298608352, 'ACC-12': 71.1479429119864, 'ACC-13': 69.85940465839474, 'ACC-14': 67.59155791090453, 'ACC-15': 68.63066528142257, 'ACC-16': 66.55665159272395, 'ACC-17': 64.65549833615785, 'ACC-18': 65.69374388035013, 'ACC-19': 65.32809507905607, 'ACC-20': 65.01557074503096, 'ACC-21': 64.66930933299523, 'ACC-22': 65.93080844242628, 'ACC-23': 64.50978321947649, 'ACC-24': 64.74018087954802, 'ACC-25': 63.65500117536893, 'ACC-26': 63.66873882921422, 'ACC-27': 64.3507275822042, 'ACC-28': 64.16727866654452, 'ACC-29': 63.088444056445056, 'ACC-30': 63.393471814664515, 'ACC-31': 63.26210541149205, 'ACC-32': 63.48908420606829, 'ACC-33': 61.58744561884746, 'ACC-34': 62.45070262083862, 'ACC-35': 61.53593584437599, 'ACC-36': 62.30345176903821, 'ACC-37': 61.91531927480113, 'ACC-38': 60.72451020143674, 'ACC-39': 59.76994558872287, 'ACC-40': 60.81143913438546, 'ACC-41': 58.9106012447402, 'ACC-42': 58.69400841973786, 'ACC-43': 57.038191171716775, 'ACC-44': 56.91887346751003, 'ACC-45': 55.66869379065551, 'ACC-46': 56.434019683087065, 'ACC-47': 54.55535755432991, 'ACC-48': 55.052416488512016, 'ACC-49': 54.4923317864497, 'ACC-50': 55.24671610479793, 'ACC-51': 51.2074699210968, 'ACC-52': 51.14705863907837, 'ACC-53': 50.76805368156041, 'ACC-54': 50.87130918258881, 'ACC-55': 49.59251465360195, 'ACC-56': 47.80229393183864, 'ACC-57': 46.69636687044874, 'ACC-58': 45.48907021841277, 'ACC-59': 42.93855149904156, 'ACC-60': 42.00884446082132, 'ACC-61': 42.099371901576994, 'ACC-62': 41.5730065340692, 'ACC-63': 40.89238335347597, 'ACC-64': 39.19493417194516, 'ACC-65': 38.4719917581942, 'ACC-66': 37.769288287842635, 'ACC-67': 36.004243142295294, 'ACC-68': 36.69513362108441, 'ACC-69': 35.07169073234419, 'ACC-70': 35.39036133554232, 'ACC-71': 33.1041337841466, 'ACC-72': 33.467671524802505, 'ACC-73': 33.40309179521155, 'ACC-74': 34.99979902589102, 'ACC-75': 34.28660349728471, 'ACC-76': 32.98277018282534, 'ACC-77': 32.29382148351901, 'ACC-78': 31.123594732920225, 'ACC-79': 33.27332085635176, 'ACC-80': 30.42200293235902, 'ACC-81': 31.80752273918386, 'ACC-82': 30.688464800430427, 'ACC-83': 32.356743886169504, 'ACC-84': 30.146615412725374, 'ACC-85': 30.768460745470595, 'ACC-86': 29.936516764329387, 'ACC-87': 29.27176224859032, 'ACC-88': 29.44509395275227, 'ACC-89': 30.288007357686663, 'ACC-90': 28.086903191247675, 'ACC-91': 29.258864233925475, 'ACC-92': 27.3942078656231, 'ACC-93': 28.240131610446866, 'ACC-94': 29.016730892273706, 'ACC-95': 27.407789239743252, 'ACC-96': 29.342438611067116, 'ACC-97': 28.554319943282024, 'ACC-98': 27.24541941894487, 'ACC-99': 26.761792400071894, 'ACC-100': 27.364032836312298, 'ACC-101': 25.497596166246257, 'ACC-102': 23.964046216583597, 'ACC-103': 25.343794275998867, 'ACC-104': 25.9578465851799, 'ACC-105': 24.1707233541333, 'ACC-106': 24.688864956600657, 'ACC-107': 24.73603079078713, 'ACC-108': 21.501000296537693, 'ACC-109': 22.452334074669412, 'ACC-110': 23.295442056697507, 'ACC-111': 20.511043815172048, 'ACC-112': 22.96520143953544, 'ACC-113': 20.855843437522214, 'ACC-114': 23.731075507735614, 'ACC-115': 21.681363041811373, 'ACC-116': 21.057010710731642, 'ACC-117': 19.24590561805699, 'ACC-118': 18.78093644993465, 'ACC-119': 22.797450780322166, 'ACC-120': 19.373886505234083, 'ACC-121': 20.562747459038246, 'ACC-122': 18.946946323605726, 'ACC-123': 16.742439344029336, 'ACC-124': 19.30348979623366, 'ACC-125': 17.384591287663305, 'ACC-126': 16.187590506288764, 'ACC-127': 18.286195553645022, 'ACC-128': 16.462737119934236, 'ACC-129': 13.304496413374089, 'ACC-130': 14.965767471616925, 'ACC-131': 14.224307226693728, 'ACC-132': 13.371267337548968, 'ACC-133': 14.931787408057593, 'ACC-134': 11.62064627315812, 'ACC-135': 14.221253064249472, 'ACC-136': 14.33701488538949, 'ACC-137': 10.302415554064709, 'ACC-138': 15.712014874229002, 'ACC-139': 10.627643465521299, 'ACC-140': 11.592514001461023, 'ACC-141': 11.184503962877374, 'ACC-142': 10.933041995142617, 'ACC-143': 11.201030145041619, 'ACC-144': 12.272292418772563, 'ACC-145': 13.96185066750151, 'ACC-146': 7.918302072148226, 'ACC-147': 10.115480002934122, 'ACC-148': 8.43362156397361, 'ACC-149': 7.686784596209799, 'ACC-150': 10.893789822692929, 'ACC-151': 6.188644577382304, 'ACC-152': 7.287978791557085, 'ACC-153': 9.000256508913685, 'ACC-154': 7.239331804566808, 'ACC-155': 8.267700036729995, 'ACC-156': 8.00623113369143, 'ACC-157': 4.61487370241979, 'ACC-158': 5.9237796573570565, 'ACC-159': 4.088327610600901, 'ACC-160': 6.626362937157852, 'ACC-161': 5.9287738326260895, 'ACC-162': 6.231757715686654, 'ACC-163': 3.8802018444831368, 'ACC-164': 5.173786066039023, 'ACC-165': 5.639764060896938, 'ACC-166': 5.09909259554608, 'ACC-167': 4.492701509726977, 'ACC-168': 7.523384204233771, 'ACC-169': 4.054269836618453, 'ACC-170': 7.759416215909252, 'ACC-171': 1.8956336136985243, 'ACC-172': 3.5472459902952185, 'ACC-173': 5.37830240862756, 'ACC-174': 4.154853573565513, 'ACC-175': 1.4445005259956478, 'ACC-176': 2.5539818899465985, 'ACC-177': 2.3494608475480234, 'ACC-178': 2.7217887972247063, 'ACC-179': 4.71696007758465, 'ACC-180': 7.156947407633174, 'ACC-181': 4.897662377037337, 'ACC-182': 1.4117875613910809, 'ACC-183': 1.5736278431363315, 'ACC-184': 4.990130112707745, 'ACC-185': 5.427296537006499, 'ACC-186': 6.906539695429554, 'ACC-187': 3.8161852643786465, 'ACC-188': 5.834358984706025, 'ACC-189': 7.59009141997479, 'ACC-190': 6.046092415397675, 'ACC-191': 8.965481239804243})])
[01/18 10:35:10] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 10:35:10] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 10:35:10] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 10:35:10] d2.evaluation.testing INFO: copypaste: 2.3277,20.8534,43.8804,31.1936,58.2552
[01/18 10:35:11] d2.utils.events INFO:  eta: 2:24:48  iter: 33999  total_loss: 32.9  loss_ce: 0.2415  loss_mask: 0.3337  loss_dice: 2.65  loss_ce_0: 0.5549  loss_mask_0: 0.3325  loss_dice_0: 2.788  loss_ce_1: 0.2772  loss_mask_1: 0.3382  loss_dice_1: 2.683  loss_ce_2: 0.258  loss_mask_2: 0.3381  loss_dice_2: 2.676  loss_ce_3: 0.2507  loss_mask_3: 0.3347  loss_dice_3: 2.664  loss_ce_4: 0.2545  loss_mask_4: 0.3341  loss_dice_4: 2.652  loss_ce_5: 0.2498  loss_mask_5: 0.3333  loss_dice_5: 2.652  loss_ce_6: 0.2427  loss_mask_6: 0.3334  loss_dice_6: 2.648  loss_ce_7: 0.2291  loss_mask_7: 0.333  loss_dice_7: 2.647  loss_ce_8: 0.2436  loss_mask_8: 0.3332  loss_dice_8: 2.649  time: 1.4777  data_time: 0.0699  lr: 1.8136e-06  max_mem: 21589M
[01/18 10:35:40] d2.utils.events INFO:  eta: 2:24:19  iter: 34019  total_loss: 31.66  loss_ce: 0.2106  loss_mask: 0.3336  loss_dice: 2.528  loss_ce_0: 0.5236  loss_mask_0: 0.3328  loss_dice_0: 2.668  loss_ce_1: 0.2857  loss_mask_1: 0.3391  loss_dice_1: 2.571  loss_ce_2: 0.2666  loss_mask_2: 0.3359  loss_dice_2: 2.548  loss_ce_3: 0.2571  loss_mask_3: 0.336  loss_dice_3: 2.534  loss_ce_4: 0.2561  loss_mask_4: 0.3349  loss_dice_4: 2.53  loss_ce_5: 0.2176  loss_mask_5: 0.3337  loss_dice_5: 2.528  loss_ce_6: 0.2308  loss_mask_6: 0.3345  loss_dice_6: 2.533  loss_ce_7: 0.2284  loss_mask_7: 0.3325  loss_dice_7: 2.529  loss_ce_8: 0.2136  loss_mask_8: 0.3337  loss_dice_8: 2.523  time: 1.4777  data_time: 0.0686  lr: 1.8082e-06  max_mem: 21589M
[01/18 10:36:09] d2.utils.events INFO:  eta: 2:23:51  iter: 34039  total_loss: 31.69  loss_ce: 0.2209  loss_mask: 0.3258  loss_dice: 2.527  loss_ce_0: 0.5318  loss_mask_0: 0.327  loss_dice_0: 2.663  loss_ce_1: 0.2625  loss_mask_1: 0.3291  loss_dice_1: 2.575  loss_ce_2: 0.2679  loss_mask_2: 0.3278  loss_dice_2: 2.551  loss_ce_3: 0.2447  loss_mask_3: 0.3262  loss_dice_3: 2.541  loss_ce_4: 0.2348  loss_mask_4: 0.3273  loss_dice_4: 2.538  loss_ce_5: 0.225  loss_mask_5: 0.3269  loss_dice_5: 2.537  loss_ce_6: 0.2209  loss_mask_6: 0.3263  loss_dice_6: 2.528  loss_ce_7: 0.2198  loss_mask_7: 0.3255  loss_dice_7: 2.53  loss_ce_8: 0.2152  loss_mask_8: 0.3264  loss_dice_8: 2.531  time: 1.4777  data_time: 0.0665  lr: 1.8027e-06  max_mem: 21589M
[01/18 10:36:39] d2.utils.events INFO:  eta: 2:23:30  iter: 34059  total_loss: 31.99  loss_ce: 0.2309  loss_mask: 0.3255  loss_dice: 2.582  loss_ce_0: 0.5639  loss_mask_0: 0.3224  loss_dice_0: 2.727  loss_ce_1: 0.293  loss_mask_1: 0.3306  loss_dice_1: 2.634  loss_ce_2: 0.2734  loss_mask_2: 0.3277  loss_dice_2: 2.612  loss_ce_3: 0.2526  loss_mask_3: 0.3259  loss_dice_3: 2.603  loss_ce_4: 0.2483  loss_mask_4: 0.3239  loss_dice_4: 2.598  loss_ce_5: 0.2395  loss_mask_5: 0.3256  loss_dice_5: 2.587  loss_ce_6: 0.2365  loss_mask_6: 0.3258  loss_dice_6: 2.588  loss_ce_7: 0.2267  loss_mask_7: 0.3247  loss_dice_7: 2.594  loss_ce_8: 0.2262  loss_mask_8: 0.3251  loss_dice_8: 2.593  time: 1.4777  data_time: 0.0719  lr: 1.7973e-06  max_mem: 21589M
[01/18 10:37:08] d2.utils.events INFO:  eta: 2:23:06  iter: 34079  total_loss: 32.2  loss_ce: 0.2241  loss_mask: 0.3243  loss_dice: 2.585  loss_ce_0: 0.5582  loss_mask_0: 0.3213  loss_dice_0: 2.718  loss_ce_1: 0.2689  loss_mask_1: 0.3306  loss_dice_1: 2.625  loss_ce_2: 0.2664  loss_mask_2: 0.3262  loss_dice_2: 2.6  loss_ce_3: 0.2488  loss_mask_3: 0.3258  loss_dice_3: 2.593  loss_ce_4: 0.2446  loss_mask_4: 0.324  loss_dice_4: 2.587  loss_ce_5: 0.2386  loss_mask_5: 0.3238  loss_dice_5: 2.588  loss_ce_6: 0.2277  loss_mask_6: 0.3237  loss_dice_6: 2.583  loss_ce_7: 0.2223  loss_mask_7: 0.3229  loss_dice_7: 2.584  loss_ce_8: 0.2182  loss_mask_8: 0.3227  loss_dice_8: 2.577  time: 1.4777  data_time: 0.0732  lr: 1.7918e-06  max_mem: 21589M
[01/18 10:37:38] d2.utils.events INFO:  eta: 2:22:39  iter: 34099  total_loss: 32.51  loss_ce: 0.2051  loss_mask: 0.3287  loss_dice: 2.637  loss_ce_0: 0.5416  loss_mask_0: 0.3314  loss_dice_0: 2.781  loss_ce_1: 0.2467  loss_mask_1: 0.3387  loss_dice_1: 2.695  loss_ce_2: 0.2591  loss_mask_2: 0.3339  loss_dice_2: 2.665  loss_ce_3: 0.2258  loss_mask_3: 0.3312  loss_dice_3: 2.649  loss_ce_4: 0.2237  loss_mask_4: 0.3325  loss_dice_4: 2.644  loss_ce_5: 0.2238  loss_mask_5: 0.3309  loss_dice_5: 2.65  loss_ce_6: 0.209  loss_mask_6: 0.3292  loss_dice_6: 2.644  loss_ce_7: 0.21  loss_mask_7: 0.3292  loss_dice_7: 2.647  loss_ce_8: 0.2119  loss_mask_8: 0.3281  loss_dice_8: 2.635  time: 1.4777  data_time: 0.0721  lr: 1.7864e-06  max_mem: 21589M
[01/18 10:38:07] d2.utils.events INFO:  eta: 2:22:11  iter: 34119  total_loss: 31.78  loss_ce: 0.2132  loss_mask: 0.3283  loss_dice: 2.573  loss_ce_0: 0.5376  loss_mask_0: 0.3347  loss_dice_0: 2.714  loss_ce_1: 0.2795  loss_mask_1: 0.3338  loss_dice_1: 2.618  loss_ce_2: 0.2521  loss_mask_2: 0.3317  loss_dice_2: 2.606  loss_ce_3: 0.2446  loss_mask_3: 0.3289  loss_dice_3: 2.585  loss_ce_4: 0.2286  loss_mask_4: 0.3286  loss_dice_4: 2.586  loss_ce_5: 0.2267  loss_mask_5: 0.3266  loss_dice_5: 2.585  loss_ce_6: 0.2299  loss_mask_6: 0.3283  loss_dice_6: 2.58  loss_ce_7: 0.2132  loss_mask_7: 0.327  loss_dice_7: 2.577  loss_ce_8: 0.2167  loss_mask_8: 0.327  loss_dice_8: 2.577  time: 1.4777  data_time: 0.0680  lr: 1.781e-06  max_mem: 21589M
[01/18 10:38:36] d2.utils.events INFO:  eta: 2:21:39  iter: 34139  total_loss: 32.17  loss_ce: 0.2471  loss_mask: 0.3305  loss_dice: 2.581  loss_ce_0: 0.5517  loss_mask_0: 0.3268  loss_dice_0: 2.712  loss_ce_1: 0.2939  loss_mask_1: 0.3364  loss_dice_1: 2.629  loss_ce_2: 0.2816  loss_mask_2: 0.3325  loss_dice_2: 2.601  loss_ce_3: 0.2509  loss_mask_3: 0.3311  loss_dice_3: 2.583  loss_ce_4: 0.2444  loss_mask_4: 0.3313  loss_dice_4: 2.581  loss_ce_5: 0.2299  loss_mask_5: 0.3293  loss_dice_5: 2.587  loss_ce_6: 0.2375  loss_mask_6: 0.3307  loss_dice_6: 2.578  loss_ce_7: 0.2377  loss_mask_7: 0.3311  loss_dice_7: 2.586  loss_ce_8: 0.2256  loss_mask_8: 0.3308  loss_dice_8: 2.594  time: 1.4776  data_time: 0.0686  lr: 1.7755e-06  max_mem: 21589M
[01/18 10:39:05] d2.utils.events INFO:  eta: 2:21:08  iter: 34159  total_loss: 32.1  loss_ce: 0.2407  loss_mask: 0.3269  loss_dice: 2.592  loss_ce_0: 0.5743  loss_mask_0: 0.3197  loss_dice_0: 2.716  loss_ce_1: 0.2832  loss_mask_1: 0.3292  loss_dice_1: 2.636  loss_ce_2: 0.2786  loss_mask_2: 0.3256  loss_dice_2: 2.621  loss_ce_3: 0.2636  loss_mask_3: 0.3259  loss_dice_3: 2.608  loss_ce_4: 0.2452  loss_mask_4: 0.3271  loss_dice_4: 2.604  loss_ce_5: 0.2474  loss_mask_5: 0.3267  loss_dice_5: 2.596  loss_ce_6: 0.2381  loss_mask_6: 0.3277  loss_dice_6: 2.59  loss_ce_7: 0.2376  loss_mask_7: 0.3284  loss_dice_7: 2.597  loss_ce_8: 0.2368  loss_mask_8: 0.3275  loss_dice_8: 2.584  time: 1.4776  data_time: 0.0680  lr: 1.77e-06  max_mem: 21589M
[01/18 10:39:34] d2.utils.events INFO:  eta: 2:20:43  iter: 34179  total_loss: 31.52  loss_ce: 0.225  loss_mask: 0.3385  loss_dice: 2.513  loss_ce_0: 0.5349  loss_mask_0: 0.3315  loss_dice_0: 2.656  loss_ce_1: 0.2539  loss_mask_1: 0.3419  loss_dice_1: 2.565  loss_ce_2: 0.2466  loss_mask_2: 0.3389  loss_dice_2: 2.546  loss_ce_3: 0.2425  loss_mask_3: 0.3408  loss_dice_3: 2.528  loss_ce_4: 0.2293  loss_mask_4: 0.3382  loss_dice_4: 2.526  loss_ce_5: 0.2312  loss_mask_5: 0.3381  loss_dice_5: 2.521  loss_ce_6: 0.2119  loss_mask_6: 0.3387  loss_dice_6: 2.521  loss_ce_7: 0.2225  loss_mask_7: 0.3388  loss_dice_7: 2.52  loss_ce_8: 0.2208  loss_mask_8: 0.3388  loss_dice_8: 2.517  time: 1.4776  data_time: 0.0714  lr: 1.7646e-06  max_mem: 21589M
[01/18 10:40:03] d2.utils.events INFO:  eta: 2:20:15  iter: 34199  total_loss: 31.68  loss_ce: 0.255  loss_mask: 0.3203  loss_dice: 2.517  loss_ce_0: 0.5521  loss_mask_0: 0.3213  loss_dice_0: 2.653  loss_ce_1: 0.294  loss_mask_1: 0.3295  loss_dice_1: 2.566  loss_ce_2: 0.2799  loss_mask_2: 0.3244  loss_dice_2: 2.536  loss_ce_3: 0.2613  loss_mask_3: 0.323  loss_dice_3: 2.52  loss_ce_4: 0.2375  loss_mask_4: 0.3214  loss_dice_4: 2.515  loss_ce_5: 0.2401  loss_mask_5: 0.3214  loss_dice_5: 2.514  loss_ce_6: 0.2446  loss_mask_6: 0.3202  loss_dice_6: 2.508  loss_ce_7: 0.2397  loss_mask_7: 0.3208  loss_dice_7: 2.514  loss_ce_8: 0.2343  loss_mask_8: 0.3211  loss_dice_8: 2.52  time: 1.4776  data_time: 0.0670  lr: 1.7591e-06  max_mem: 21589M
[01/18 10:40:32] d2.utils.events INFO:  eta: 2:19:44  iter: 34219  total_loss: 31.19  loss_ce: 0.1949  loss_mask: 0.3303  loss_dice: 2.526  loss_ce_0: 0.5524  loss_mask_0: 0.3304  loss_dice_0: 2.651  loss_ce_1: 0.2687  loss_mask_1: 0.336  loss_dice_1: 2.561  loss_ce_2: 0.2586  loss_mask_2: 0.333  loss_dice_2: 2.537  loss_ce_3: 0.2418  loss_mask_3: 0.3301  loss_dice_3: 2.521  loss_ce_4: 0.2315  loss_mask_4: 0.3299  loss_dice_4: 2.528  loss_ce_5: 0.229  loss_mask_5: 0.3294  loss_dice_5: 2.522  loss_ce_6: 0.2117  loss_mask_6: 0.3285  loss_dice_6: 2.532  loss_ce_7: 0.2168  loss_mask_7: 0.3293  loss_dice_7: 2.529  loss_ce_8: 0.2022  loss_mask_8: 0.3296  loss_dice_8: 2.529  time: 1.4776  data_time: 0.0695  lr: 1.7537e-06  max_mem: 21589M
[01/18 10:41:01] d2.utils.events INFO:  eta: 2:19:13  iter: 34239  total_loss: 31.12  loss_ce: 0.2153  loss_mask: 0.3278  loss_dice: 2.475  loss_ce_0: 0.5371  loss_mask_0: 0.3272  loss_dice_0: 2.617  loss_ce_1: 0.2643  loss_mask_1: 0.3332  loss_dice_1: 2.532  loss_ce_2: 0.2637  loss_mask_2: 0.3307  loss_dice_2: 2.5  loss_ce_3: 0.2414  loss_mask_3: 0.3301  loss_dice_3: 2.484  loss_ce_4: 0.2311  loss_mask_4: 0.3307  loss_dice_4: 2.48  loss_ce_5: 0.2246  loss_mask_5: 0.3304  loss_dice_5: 2.483  loss_ce_6: 0.2192  loss_mask_6: 0.3303  loss_dice_6: 2.48  loss_ce_7: 0.2113  loss_mask_7: 0.3274  loss_dice_7: 2.479  loss_ce_8: 0.206  loss_mask_8: 0.3284  loss_dice_8: 2.479  time: 1.4776  data_time: 0.0673  lr: 1.7482e-06  max_mem: 21589M
[01/18 10:41:30] d2.utils.events INFO:  eta: 2:18:42  iter: 34259  total_loss: 32.19  loss_ce: 0.2466  loss_mask: 0.3263  loss_dice: 2.573  loss_ce_0: 0.5448  loss_mask_0: 0.3264  loss_dice_0: 2.714  loss_ce_1: 0.2854  loss_mask_1: 0.3333  loss_dice_1: 2.628  loss_ce_2: 0.2745  loss_mask_2: 0.3307  loss_dice_2: 2.602  loss_ce_3: 0.2533  loss_mask_3: 0.3281  loss_dice_3: 2.588  loss_ce_4: 0.242  loss_mask_4: 0.327  loss_dice_4: 2.586  loss_ce_5: 0.2518  loss_mask_5: 0.326  loss_dice_5: 2.589  loss_ce_6: 0.2282  loss_mask_6: 0.3274  loss_dice_6: 2.588  loss_ce_7: 0.2326  loss_mask_7: 0.328  loss_dice_7: 2.582  loss_ce_8: 0.2346  loss_mask_8: 0.3262  loss_dice_8: 2.582  time: 1.4775  data_time: 0.0687  lr: 1.7427e-06  max_mem: 21589M
[01/18 10:41:59] d2.utils.events INFO:  eta: 2:18:13  iter: 34279  total_loss: 31.54  loss_ce: 0.2209  loss_mask: 0.3243  loss_dice: 2.526  loss_ce_0: 0.577  loss_mask_0: 0.3205  loss_dice_0: 2.663  loss_ce_1: 0.2805  loss_mask_1: 0.3271  loss_dice_1: 2.578  loss_ce_2: 0.2577  loss_mask_2: 0.3252  loss_dice_2: 2.559  loss_ce_3: 0.239  loss_mask_3: 0.3248  loss_dice_3: 2.54  loss_ce_4: 0.2374  loss_mask_4: 0.3228  loss_dice_4: 2.541  loss_ce_5: 0.2129  loss_mask_5: 0.3247  loss_dice_5: 2.545  loss_ce_6: 0.2144  loss_mask_6: 0.3231  loss_dice_6: 2.528  loss_ce_7: 0.2071  loss_mask_7: 0.3226  loss_dice_7: 2.536  loss_ce_8: 0.2242  loss_mask_8: 0.3255  loss_dice_8: 2.538  time: 1.4775  data_time: 0.0706  lr: 1.7373e-06  max_mem: 21589M
[01/18 10:42:28] d2.utils.events INFO:  eta: 2:17:39  iter: 34299  total_loss: 31.66  loss_ce: 0.233  loss_mask: 0.328  loss_dice: 2.516  loss_ce_0: 0.5517  loss_mask_0: 0.3201  loss_dice_0: 2.676  loss_ce_1: 0.2784  loss_mask_1: 0.331  loss_dice_1: 2.582  loss_ce_2: 0.2734  loss_mask_2: 0.3294  loss_dice_2: 2.545  loss_ce_3: 0.2557  loss_mask_3: 0.3295  loss_dice_3: 2.527  loss_ce_4: 0.2369  loss_mask_4: 0.3281  loss_dice_4: 2.54  loss_ce_5: 0.2311  loss_mask_5: 0.3271  loss_dice_5: 2.532  loss_ce_6: 0.2266  loss_mask_6: 0.3277  loss_dice_6: 2.53  loss_ce_7: 0.2293  loss_mask_7: 0.3277  loss_dice_7: 2.524  loss_ce_8: 0.2219  loss_mask_8: 0.3287  loss_dice_8: 2.528  time: 1.4775  data_time: 0.0685  lr: 1.7318e-06  max_mem: 21589M
[01/18 10:42:57] d2.utils.events INFO:  eta: 2:17:01  iter: 34319  total_loss: 31.29  loss_ce: 0.2255  loss_mask: 0.3303  loss_dice: 2.542  loss_ce_0: 0.5603  loss_mask_0: 0.3304  loss_dice_0: 2.688  loss_ce_1: 0.2771  loss_mask_1: 0.3349  loss_dice_1: 2.593  loss_ce_2: 0.2632  loss_mask_2: 0.3335  loss_dice_2: 2.576  loss_ce_3: 0.2601  loss_mask_3: 0.3319  loss_dice_3: 2.546  loss_ce_4: 0.2397  loss_mask_4: 0.3316  loss_dice_4: 2.539  loss_ce_5: 0.2327  loss_mask_5: 0.332  loss_dice_5: 2.545  loss_ce_6: 0.229  loss_mask_6: 0.3316  loss_dice_6: 2.545  loss_ce_7: 0.2248  loss_mask_7: 0.3318  loss_dice_7: 2.536  loss_ce_8: 0.2162  loss_mask_8: 0.3317  loss_dice_8: 2.543  time: 1.4775  data_time: 0.0673  lr: 1.7263e-06  max_mem: 21589M
[01/18 10:43:26] d2.utils.events INFO:  eta: 2:16:34  iter: 34339  total_loss: 32.56  loss_ce: 0.2368  loss_mask: 0.3287  loss_dice: 2.585  loss_ce_0: 0.5808  loss_mask_0: 0.3312  loss_dice_0: 2.716  loss_ce_1: 0.2818  loss_mask_1: 0.3336  loss_dice_1: 2.634  loss_ce_2: 0.2604  loss_mask_2: 0.3312  loss_dice_2: 2.609  loss_ce_3: 0.2424  loss_mask_3: 0.3276  loss_dice_3: 2.595  loss_ce_4: 0.2383  loss_mask_4: 0.3279  loss_dice_4: 2.597  loss_ce_5: 0.2531  loss_mask_5: 0.3277  loss_dice_5: 2.595  loss_ce_6: 0.2293  loss_mask_6: 0.3283  loss_dice_6: 2.593  loss_ce_7: 0.2307  loss_mask_7: 0.3283  loss_dice_7: 2.6  loss_ce_8: 0.238  loss_mask_8: 0.3292  loss_dice_8: 2.591  time: 1.4775  data_time: 0.0718  lr: 1.7209e-06  max_mem: 21589M
[01/18 10:43:55] d2.utils.events INFO:  eta: 2:16:05  iter: 34359  total_loss: 31.73  loss_ce: 0.2361  loss_mask: 0.3389  loss_dice: 2.527  loss_ce_0: 0.5809  loss_mask_0: 0.3365  loss_dice_0: 2.697  loss_ce_1: 0.3013  loss_mask_1: 0.3453  loss_dice_1: 2.592  loss_ce_2: 0.2905  loss_mask_2: 0.3394  loss_dice_2: 2.565  loss_ce_3: 0.2519  loss_mask_3: 0.3391  loss_dice_3: 2.55  loss_ce_4: 0.2577  loss_mask_4: 0.3374  loss_dice_4: 2.538  loss_ce_5: 0.2312  loss_mask_5: 0.3362  loss_dice_5: 2.54  loss_ce_6: 0.237  loss_mask_6: 0.3361  loss_dice_6: 2.532  loss_ce_7: 0.2347  loss_mask_7: 0.3378  loss_dice_7: 2.534  loss_ce_8: 0.2285  loss_mask_8: 0.3375  loss_dice_8: 2.539  time: 1.4775  data_time: 0.0729  lr: 1.7154e-06  max_mem: 21589M
[01/18 10:44:24] d2.utils.events INFO:  eta: 2:15:31  iter: 34379  total_loss: 31.82  loss_ce: 0.2123  loss_mask: 0.3293  loss_dice: 2.59  loss_ce_0: 0.5685  loss_mask_0: 0.3231  loss_dice_0: 2.709  loss_ce_1: 0.2685  loss_mask_1: 0.3333  loss_dice_1: 2.618  loss_ce_2: 0.2436  loss_mask_2: 0.3309  loss_dice_2: 2.605  loss_ce_3: 0.241  loss_mask_3: 0.3291  loss_dice_3: 2.586  loss_ce_4: 0.2186  loss_mask_4: 0.3284  loss_dice_4: 2.587  loss_ce_5: 0.2147  loss_mask_5: 0.3276  loss_dice_5: 2.589  loss_ce_6: 0.2162  loss_mask_6: 0.3282  loss_dice_6: 2.579  loss_ce_7: 0.2154  loss_mask_7: 0.3287  loss_dice_7: 2.574  loss_ce_8: 0.2192  loss_mask_8: 0.3286  loss_dice_8: 2.581  time: 1.4774  data_time: 0.0696  lr: 1.7099e-06  max_mem: 21589M
[01/18 10:44:53] d2.utils.events INFO:  eta: 2:15:02  iter: 34399  total_loss: 32.1  loss_ce: 0.2276  loss_mask: 0.3251  loss_dice: 2.601  loss_ce_0: 0.5749  loss_mask_0: 0.3292  loss_dice_0: 2.744  loss_ce_1: 0.2979  loss_mask_1: 0.331  loss_dice_1: 2.652  loss_ce_2: 0.293  loss_mask_2: 0.329  loss_dice_2: 2.617  loss_ce_3: 0.2504  loss_mask_3: 0.3277  loss_dice_3: 2.604  loss_ce_4: 0.2395  loss_mask_4: 0.3282  loss_dice_4: 2.607  loss_ce_5: 0.241  loss_mask_5: 0.3261  loss_dice_5: 2.607  loss_ce_6: 0.2313  loss_mask_6: 0.325  loss_dice_6: 2.598  loss_ce_7: 0.2241  loss_mask_7: 0.3259  loss_dice_7: 2.593  loss_ce_8: 0.2289  loss_mask_8: 0.3245  loss_dice_8: 2.604  time: 1.4774  data_time: 0.0743  lr: 1.7045e-06  max_mem: 21589M
[01/18 10:45:22] d2.utils.events INFO:  eta: 2:14:31  iter: 34419  total_loss: 31.68  loss_ce: 0.23  loss_mask: 0.3224  loss_dice: 2.533  loss_ce_0: 0.5594  loss_mask_0: 0.326  loss_dice_0: 2.671  loss_ce_1: 0.2803  loss_mask_1: 0.3289  loss_dice_1: 2.575  loss_ce_2: 0.2732  loss_mask_2: 0.3248  loss_dice_2: 2.555  loss_ce_3: 0.2515  loss_mask_3: 0.3233  loss_dice_3: 2.54  loss_ce_4: 0.2337  loss_mask_4: 0.3233  loss_dice_4: 2.532  loss_ce_5: 0.228  loss_mask_5: 0.3221  loss_dice_5: 2.543  loss_ce_6: 0.2223  loss_mask_6: 0.3225  loss_dice_6: 2.526  loss_ce_7: 0.2277  loss_mask_7: 0.323  loss_dice_7: 2.542  loss_ce_8: 0.2236  loss_mask_8: 0.3235  loss_dice_8: 2.541  time: 1.4774  data_time: 0.0759  lr: 1.699e-06  max_mem: 21589M
[01/18 10:45:52] d2.utils.events INFO:  eta: 2:14:04  iter: 34439  total_loss: 31.56  loss_ce: 0.2272  loss_mask: 0.3318  loss_dice: 2.516  loss_ce_0: 0.5417  loss_mask_0: 0.3303  loss_dice_0: 2.649  loss_ce_1: 0.2552  loss_mask_1: 0.3349  loss_dice_1: 2.563  loss_ce_2: 0.2669  loss_mask_2: 0.331  loss_dice_2: 2.541  loss_ce_3: 0.2561  loss_mask_3: 0.3299  loss_dice_3: 2.526  loss_ce_4: 0.2288  loss_mask_4: 0.3305  loss_dice_4: 2.531  loss_ce_5: 0.2297  loss_mask_5: 0.3305  loss_dice_5: 2.519  loss_ce_6: 0.2329  loss_mask_6: 0.3323  loss_dice_6: 2.519  loss_ce_7: 0.2211  loss_mask_7: 0.3313  loss_dice_7: 2.52  loss_ce_8: 0.2142  loss_mask_8: 0.3314  loss_dice_8: 2.516  time: 1.4774  data_time: 0.0719  lr: 1.6935e-06  max_mem: 21589M
[01/18 10:46:21] d2.utils.events INFO:  eta: 2:13:35  iter: 34459  total_loss: 31.82  loss_ce: 0.2229  loss_mask: 0.3296  loss_dice: 2.58  loss_ce_0: 0.5473  loss_mask_0: 0.3268  loss_dice_0: 2.72  loss_ce_1: 0.2581  loss_mask_1: 0.3401  loss_dice_1: 2.629  loss_ce_2: 0.2611  loss_mask_2: 0.3366  loss_dice_2: 2.609  loss_ce_3: 0.2346  loss_mask_3: 0.3325  loss_dice_3: 2.598  loss_ce_4: 0.2388  loss_mask_4: 0.3303  loss_dice_4: 2.588  loss_ce_5: 0.2241  loss_mask_5: 0.3297  loss_dice_5: 2.585  loss_ce_6: 0.2274  loss_mask_6: 0.329  loss_dice_6: 2.589  loss_ce_7: 0.2165  loss_mask_7: 0.3284  loss_dice_7: 2.587  loss_ce_8: 0.2305  loss_mask_8: 0.3288  loss_dice_8: 2.582  time: 1.4774  data_time: 0.0696  lr: 1.688e-06  max_mem: 21589M
[01/18 10:46:50] d2.utils.events INFO:  eta: 2:13:07  iter: 34479  total_loss: 32.46  loss_ce: 0.2465  loss_mask: 0.3269  loss_dice: 2.638  loss_ce_0: 0.5488  loss_mask_0: 0.3221  loss_dice_0: 2.785  loss_ce_1: 0.2815  loss_mask_1: 0.3311  loss_dice_1: 2.687  loss_ce_2: 0.2799  loss_mask_2: 0.3281  loss_dice_2: 2.667  loss_ce_3: 0.2591  loss_mask_3: 0.3271  loss_dice_3: 2.652  loss_ce_4: 0.2574  loss_mask_4: 0.3266  loss_dice_4: 2.643  loss_ce_5: 0.2551  loss_mask_5: 0.326  loss_dice_5: 2.643  loss_ce_6: 0.2391  loss_mask_6: 0.3268  loss_dice_6: 2.64  loss_ce_7: 0.2304  loss_mask_7: 0.3278  loss_dice_7: 2.641  loss_ce_8: 0.2351  loss_mask_8: 0.3269  loss_dice_8: 2.636  time: 1.4774  data_time: 0.0688  lr: 1.6825e-06  max_mem: 21589M
[01/18 10:47:19] d2.utils.events INFO:  eta: 2:12:37  iter: 34499  total_loss: 32.12  loss_ce: 0.2322  loss_mask: 0.3276  loss_dice: 2.584  loss_ce_0: 0.5626  loss_mask_0: 0.329  loss_dice_0: 2.732  loss_ce_1: 0.2866  loss_mask_1: 0.3355  loss_dice_1: 2.649  loss_ce_2: 0.2732  loss_mask_2: 0.3327  loss_dice_2: 2.609  loss_ce_3: 0.253  loss_mask_3: 0.3299  loss_dice_3: 2.602  loss_ce_4: 0.247  loss_mask_4: 0.3288  loss_dice_4: 2.602  loss_ce_5: 0.2374  loss_mask_5: 0.3284  loss_dice_5: 2.593  loss_ce_6: 0.2406  loss_mask_6: 0.3279  loss_dice_6: 2.599  loss_ce_7: 0.2303  loss_mask_7: 0.3283  loss_dice_7: 2.595  loss_ce_8: 0.2346  loss_mask_8: 0.3271  loss_dice_8: 2.594  time: 1.4774  data_time: 0.0680  lr: 1.677e-06  max_mem: 21589M
[01/18 10:47:48] d2.utils.events INFO:  eta: 2:12:14  iter: 34519  total_loss: 31.56  loss_ce: 0.2108  loss_mask: 0.3228  loss_dice: 2.537  loss_ce_0: 0.503  loss_mask_0: 0.3217  loss_dice_0: 2.67  loss_ce_1: 0.2673  loss_mask_1: 0.3285  loss_dice_1: 2.577  loss_ce_2: 0.241  loss_mask_2: 0.3257  loss_dice_2: 2.547  loss_ce_3: 0.2383  loss_mask_3: 0.3233  loss_dice_3: 2.543  loss_ce_4: 0.2461  loss_mask_4: 0.323  loss_dice_4: 2.549  loss_ce_5: 0.2452  loss_mask_5: 0.3225  loss_dice_5: 2.537  loss_ce_6: 0.2174  loss_mask_6: 0.3219  loss_dice_6: 2.531  loss_ce_7: 0.211  loss_mask_7: 0.3229  loss_dice_7: 2.538  loss_ce_8: 0.2161  loss_mask_8: 0.3224  loss_dice_8: 2.536  time: 1.4774  data_time: 0.0733  lr: 1.6716e-06  max_mem: 21589M
[01/18 10:48:18] d2.utils.events INFO:  eta: 2:11:48  iter: 34539  total_loss: 32.64  loss_ce: 0.2401  loss_mask: 0.3319  loss_dice: 2.64  loss_ce_0: 0.5642  loss_mask_0: 0.3307  loss_dice_0: 2.772  loss_ce_1: 0.2896  loss_mask_1: 0.3363  loss_dice_1: 2.685  loss_ce_2: 0.2836  loss_mask_2: 0.3321  loss_dice_2: 2.649  loss_ce_3: 0.2636  loss_mask_3: 0.3318  loss_dice_3: 2.653  loss_ce_4: 0.2529  loss_mask_4: 0.3327  loss_dice_4: 2.64  loss_ce_5: 0.2357  loss_mask_5: 0.3313  loss_dice_5: 2.636  loss_ce_6: 0.2425  loss_mask_6: 0.3317  loss_dice_6: 2.634  loss_ce_7: 0.2215  loss_mask_7: 0.3316  loss_dice_7: 2.637  loss_ce_8: 0.2334  loss_mask_8: 0.3318  loss_dice_8: 2.637  time: 1.4774  data_time: 0.0693  lr: 1.6661e-06  max_mem: 21589M
[01/18 10:48:47] d2.utils.events INFO:  eta: 2:11:16  iter: 34559  total_loss: 30.92  loss_ce: 0.2226  loss_mask: 0.3316  loss_dice: 2.446  loss_ce_0: 0.5394  loss_mask_0: 0.3287  loss_dice_0: 2.594  loss_ce_1: 0.2562  loss_mask_1: 0.3393  loss_dice_1: 2.503  loss_ce_2: 0.2469  loss_mask_2: 0.3363  loss_dice_2: 2.473  loss_ce_3: 0.2339  loss_mask_3: 0.3342  loss_dice_3: 2.456  loss_ce_4: 0.225  loss_mask_4: 0.3332  loss_dice_4: 2.45  loss_ce_5: 0.2244  loss_mask_5: 0.3314  loss_dice_5: 2.458  loss_ce_6: 0.2236  loss_mask_6: 0.3322  loss_dice_6: 2.448  loss_ce_7: 0.2067  loss_mask_7: 0.3315  loss_dice_7: 2.453  loss_ce_8: 0.211  loss_mask_8: 0.3302  loss_dice_8: 2.453  time: 1.4773  data_time: 0.0674  lr: 1.6606e-06  max_mem: 21589M
[01/18 10:49:16] d2.utils.events INFO:  eta: 2:10:47  iter: 34579  total_loss: 31.52  loss_ce: 0.2142  loss_mask: 0.3304  loss_dice: 2.515  loss_ce_0: 0.5592  loss_mask_0: 0.3297  loss_dice_0: 2.648  loss_ce_1: 0.2838  loss_mask_1: 0.335  loss_dice_1: 2.556  loss_ce_2: 0.2649  loss_mask_2: 0.3342  loss_dice_2: 2.54  loss_ce_3: 0.2453  loss_mask_3: 0.3306  loss_dice_3: 2.525  loss_ce_4: 0.2444  loss_mask_4: 0.33  loss_dice_4: 2.523  loss_ce_5: 0.2495  loss_mask_5: 0.3299  loss_dice_5: 2.524  loss_ce_6: 0.2426  loss_mask_6: 0.3296  loss_dice_6: 2.518  loss_ce_7: 0.2277  loss_mask_7: 0.3297  loss_dice_7: 2.52  loss_ce_8: 0.218  loss_mask_8: 0.3294  loss_dice_8: 2.513  time: 1.4773  data_time: 0.0682  lr: 1.6551e-06  max_mem: 21589M
[01/18 10:49:45] d2.utils.events INFO:  eta: 2:10:24  iter: 34599  total_loss: 33.11  loss_ce: 0.2211  loss_mask: 0.3368  loss_dice: 2.646  loss_ce_0: 0.5449  loss_mask_0: 0.3392  loss_dice_0: 2.78  loss_ce_1: 0.2761  loss_mask_1: 0.3407  loss_dice_1: 2.689  loss_ce_2: 0.2728  loss_mask_2: 0.338  loss_dice_2: 2.666  loss_ce_3: 0.2623  loss_mask_3: 0.3361  loss_dice_3: 2.654  loss_ce_4: 0.2408  loss_mask_4: 0.3348  loss_dice_4: 2.648  loss_ce_5: 0.2372  loss_mask_5: 0.3357  loss_dice_5: 2.646  loss_ce_6: 0.226  loss_mask_6: 0.337  loss_dice_6: 2.643  loss_ce_7: 0.2301  loss_mask_7: 0.337  loss_dice_7: 2.636  loss_ce_8: 0.2253  loss_mask_8: 0.3368  loss_dice_8: 2.648  time: 1.4773  data_time: 0.0723  lr: 1.6496e-06  max_mem: 21589M
[01/18 10:50:14] d2.utils.events INFO:  eta: 2:09:52  iter: 34619  total_loss: 31.98  loss_ce: 0.2075  loss_mask: 0.335  loss_dice: 2.572  loss_ce_0: 0.5236  loss_mask_0: 0.3341  loss_dice_0: 2.705  loss_ce_1: 0.2761  loss_mask_1: 0.3411  loss_dice_1: 2.618  loss_ce_2: 0.2551  loss_mask_2: 0.3378  loss_dice_2: 2.593  loss_ce_3: 0.2397  loss_mask_3: 0.3368  loss_dice_3: 2.58  loss_ce_4: 0.2358  loss_mask_4: 0.3365  loss_dice_4: 2.574  loss_ce_5: 0.2383  loss_mask_5: 0.3354  loss_dice_5: 2.575  loss_ce_6: 0.238  loss_mask_6: 0.3372  loss_dice_6: 2.569  loss_ce_7: 0.2398  loss_mask_7: 0.3365  loss_dice_7: 2.568  loss_ce_8: 0.221  loss_mask_8: 0.3367  loss_dice_8: 2.569  time: 1.4773  data_time: 0.0721  lr: 1.6441e-06  max_mem: 21589M
[01/18 10:50:42] d2.utils.events INFO:  eta: 2:09:20  iter: 34639  total_loss: 31.28  loss_ce: 0.206  loss_mask: 0.327  loss_dice: 2.497  loss_ce_0: 0.5288  loss_mask_0: 0.3284  loss_dice_0: 2.64  loss_ce_1: 0.2628  loss_mask_1: 0.3342  loss_dice_1: 2.542  loss_ce_2: 0.2553  loss_mask_2: 0.3319  loss_dice_2: 2.523  loss_ce_3: 0.2349  loss_mask_3: 0.3288  loss_dice_3: 2.508  loss_ce_4: 0.2345  loss_mask_4: 0.3276  loss_dice_4: 2.502  loss_ce_5: 0.2208  loss_mask_5: 0.3287  loss_dice_5: 2.505  loss_ce_6: 0.2223  loss_mask_6: 0.3285  loss_dice_6: 2.496  loss_ce_7: 0.2184  loss_mask_7: 0.3284  loss_dice_7: 2.503  loss_ce_8: 0.2234  loss_mask_8: 0.3277  loss_dice_8: 2.493  time: 1.4772  data_time: 0.0705  lr: 1.6386e-06  max_mem: 21589M
[01/18 10:51:11] d2.utils.events INFO:  eta: 2:08:50  iter: 34659  total_loss: 31.75  loss_ce: 0.2102  loss_mask: 0.3304  loss_dice: 2.542  loss_ce_0: 0.5204  loss_mask_0: 0.3246  loss_dice_0: 2.675  loss_ce_1: 0.2544  loss_mask_1: 0.3314  loss_dice_1: 2.588  loss_ce_2: 0.2577  loss_mask_2: 0.3314  loss_dice_2: 2.571  loss_ce_3: 0.2201  loss_mask_3: 0.3319  loss_dice_3: 2.552  loss_ce_4: 0.2102  loss_mask_4: 0.3312  loss_dice_4: 2.556  loss_ce_5: 0.226  loss_mask_5: 0.3301  loss_dice_5: 2.55  loss_ce_6: 0.2245  loss_mask_6: 0.3304  loss_dice_6: 2.549  loss_ce_7: 0.2128  loss_mask_7: 0.3301  loss_dice_7: 2.55  loss_ce_8: 0.2136  loss_mask_8: 0.3294  loss_dice_8: 2.552  time: 1.4772  data_time: 0.0747  lr: 1.6331e-06  max_mem: 21589M
[01/18 10:51:41] d2.utils.events INFO:  eta: 2:08:25  iter: 34679  total_loss: 31.65  loss_ce: 0.2103  loss_mask: 0.3287  loss_dice: 2.551  loss_ce_0: 0.527  loss_mask_0: 0.3304  loss_dice_0: 2.69  loss_ce_1: 0.2591  loss_mask_1: 0.3343  loss_dice_1: 2.594  loss_ce_2: 0.251  loss_mask_2: 0.3312  loss_dice_2: 2.57  loss_ce_3: 0.2304  loss_mask_3: 0.3308  loss_dice_3: 2.565  loss_ce_4: 0.2331  loss_mask_4: 0.3285  loss_dice_4: 2.55  loss_ce_5: 0.2246  loss_mask_5: 0.3279  loss_dice_5: 2.545  loss_ce_6: 0.2072  loss_mask_6: 0.3281  loss_dice_6: 2.546  loss_ce_7: 0.2256  loss_mask_7: 0.3286  loss_dice_7: 2.554  loss_ce_8: 0.2058  loss_mask_8: 0.3281  loss_dice_8: 2.549  time: 1.4772  data_time: 0.0724  lr: 1.6276e-06  max_mem: 21589M
[01/18 10:52:10] d2.utils.events INFO:  eta: 2:07:57  iter: 34699  total_loss: 30.96  loss_ce: 0.2124  loss_mask: 0.3291  loss_dice: 2.498  loss_ce_0: 0.5172  loss_mask_0: 0.3258  loss_dice_0: 2.62  loss_ce_1: 0.2723  loss_mask_1: 0.3352  loss_dice_1: 2.53  loss_ce_2: 0.2614  loss_mask_2: 0.3323  loss_dice_2: 2.524  loss_ce_3: 0.2397  loss_mask_3: 0.3305  loss_dice_3: 2.509  loss_ce_4: 0.2336  loss_mask_4: 0.3296  loss_dice_4: 2.498  loss_ce_5: 0.2261  loss_mask_5: 0.3292  loss_dice_5: 2.496  loss_ce_6: 0.227  loss_mask_6: 0.3304  loss_dice_6: 2.504  loss_ce_7: 0.2215  loss_mask_7: 0.3308  loss_dice_7: 2.49  loss_ce_8: 0.2134  loss_mask_8: 0.3299  loss_dice_8: 2.5  time: 1.4772  data_time: 0.0678  lr: 1.6221e-06  max_mem: 21589M
[01/18 10:52:39] d2.utils.events INFO:  eta: 2:07:27  iter: 34719  total_loss: 31.46  loss_ce: 0.2169  loss_mask: 0.3315  loss_dice: 2.529  loss_ce_0: 0.5267  loss_mask_0: 0.3324  loss_dice_0: 2.664  loss_ce_1: 0.2836  loss_mask_1: 0.337  loss_dice_1: 2.573  loss_ce_2: 0.25  loss_mask_2: 0.3338  loss_dice_2: 2.559  loss_ce_3: 0.2342  loss_mask_3: 0.3315  loss_dice_3: 2.541  loss_ce_4: 0.2292  loss_mask_4: 0.3318  loss_dice_4: 2.537  loss_ce_5: 0.2157  loss_mask_5: 0.3323  loss_dice_5: 2.537  loss_ce_6: 0.2147  loss_mask_6: 0.3315  loss_dice_6: 2.536  loss_ce_7: 0.2089  loss_mask_7: 0.3319  loss_dice_7: 2.543  loss_ce_8: 0.2109  loss_mask_8: 0.3321  loss_dice_8: 2.543  time: 1.4772  data_time: 0.0733  lr: 1.6166e-06  max_mem: 21589M
[01/18 10:53:08] d2.utils.events INFO:  eta: 2:06:58  iter: 34739  total_loss: 31.95  loss_ce: 0.2262  loss_mask: 0.3279  loss_dice: 2.577  loss_ce_0: 0.5545  loss_mask_0: 0.3282  loss_dice_0: 2.731  loss_ce_1: 0.2841  loss_mask_1: 0.3362  loss_dice_1: 2.629  loss_ce_2: 0.275  loss_mask_2: 0.3322  loss_dice_2: 2.601  loss_ce_3: 0.2528  loss_mask_3: 0.3287  loss_dice_3: 2.583  loss_ce_4: 0.2368  loss_mask_4: 0.3279  loss_dice_4: 2.586  loss_ce_5: 0.2481  loss_mask_5: 0.328  loss_dice_5: 2.585  loss_ce_6: 0.23  loss_mask_6: 0.3273  loss_dice_6: 2.579  loss_ce_7: 0.2261  loss_mask_7: 0.3264  loss_dice_7: 2.574  loss_ce_8: 0.2201  loss_mask_8: 0.3279  loss_dice_8: 2.574  time: 1.4772  data_time: 0.0639  lr: 1.611e-06  max_mem: 21589M
[01/18 10:53:37] d2.utils.events INFO:  eta: 2:06:31  iter: 34759  total_loss: 31.32  loss_ce: 0.2034  loss_mask: 0.3262  loss_dice: 2.49  loss_ce_0: 0.5651  loss_mask_0: 0.3249  loss_dice_0: 2.621  loss_ce_1: 0.271  loss_mask_1: 0.3292  loss_dice_1: 2.531  loss_ce_2: 0.2518  loss_mask_2: 0.328  loss_dice_2: 2.508  loss_ce_3: 0.2412  loss_mask_3: 0.325  loss_dice_3: 2.491  loss_ce_4: 0.2309  loss_mask_4: 0.3242  loss_dice_4: 2.494  loss_ce_5: 0.2299  loss_mask_5: 0.324  loss_dice_5: 2.499  loss_ce_6: 0.2156  loss_mask_6: 0.3252  loss_dice_6: 2.486  loss_ce_7: 0.2125  loss_mask_7: 0.3253  loss_dice_7: 2.497  loss_ce_8: 0.2178  loss_mask_8: 0.325  loss_dice_8: 2.499  time: 1.4772  data_time: 0.0704  lr: 1.6055e-06  max_mem: 21589M
[01/18 10:54:06] d2.utils.events INFO:  eta: 2:06:02  iter: 34779  total_loss: 32.03  loss_ce: 0.2225  loss_mask: 0.3208  loss_dice: 2.558  loss_ce_0: 0.5603  loss_mask_0: 0.323  loss_dice_0: 2.693  loss_ce_1: 0.2768  loss_mask_1: 0.3255  loss_dice_1: 2.598  loss_ce_2: 0.2777  loss_mask_2: 0.3232  loss_dice_2: 2.579  loss_ce_3: 0.261  loss_mask_3: 0.3203  loss_dice_3: 2.569  loss_ce_4: 0.2433  loss_mask_4: 0.3202  loss_dice_4: 2.565  loss_ce_5: 0.2442  loss_mask_5: 0.319  loss_dice_5: 2.564  loss_ce_6: 0.2337  loss_mask_6: 0.3198  loss_dice_6: 2.561  loss_ce_7: 0.2336  loss_mask_7: 0.3195  loss_dice_7: 2.557  loss_ce_8: 0.2367  loss_mask_8: 0.3197  loss_dice_8: 2.56  time: 1.4772  data_time: 0.0729  lr: 1.6e-06  max_mem: 21589M
[01/18 10:54:36] d2.utils.events INFO:  eta: 2:05:43  iter: 34799  total_loss: 31.73  loss_ce: 0.2184  loss_mask: 0.33  loss_dice: 2.559  loss_ce_0: 0.5577  loss_mask_0: 0.3293  loss_dice_0: 2.71  loss_ce_1: 0.2716  loss_mask_1: 0.3361  loss_dice_1: 2.61  loss_ce_2: 0.2679  loss_mask_2: 0.3326  loss_dice_2: 2.58  loss_ce_3: 0.2533  loss_mask_3: 0.3324  loss_dice_3: 2.573  loss_ce_4: 0.2334  loss_mask_4: 0.3323  loss_dice_4: 2.566  loss_ce_5: 0.2314  loss_mask_5: 0.3319  loss_dice_5: 2.564  loss_ce_6: 0.2181  loss_mask_6: 0.3298  loss_dice_6: 2.562  loss_ce_7: 0.2162  loss_mask_7: 0.33  loss_dice_7: 2.558  loss_ce_8: 0.2277  loss_mask_8: 0.3302  loss_dice_8: 2.555  time: 1.4772  data_time: 0.0761  lr: 1.5945e-06  max_mem: 21589M
[01/18 10:55:06] d2.utils.events INFO:  eta: 2:05:15  iter: 34819  total_loss: 31.5  loss_ce: 0.2321  loss_mask: 0.3294  loss_dice: 2.517  loss_ce_0: 0.5475  loss_mask_0: 0.3318  loss_dice_0: 2.669  loss_ce_1: 0.2558  loss_mask_1: 0.3336  loss_dice_1: 2.578  loss_ce_2: 0.2572  loss_mask_2: 0.3284  loss_dice_2: 2.557  loss_ce_3: 0.2449  loss_mask_3: 0.3283  loss_dice_3: 2.536  loss_ce_4: 0.2376  loss_mask_4: 0.3274  loss_dice_4: 2.531  loss_ce_5: 0.2467  loss_mask_5: 0.3277  loss_dice_5: 2.527  loss_ce_6: 0.2238  loss_mask_6: 0.3287  loss_dice_6: 2.525  loss_ce_7: 0.2276  loss_mask_7: 0.3295  loss_dice_7: 2.527  loss_ce_8: 0.2199  loss_mask_8: 0.3302  loss_dice_8: 2.516  time: 1.4772  data_time: 0.0700  lr: 1.589e-06  max_mem: 21589M
[01/18 10:55:35] d2.utils.events INFO:  eta: 2:04:46  iter: 34839  total_loss: 31.28  loss_ce: 0.2285  loss_mask: 0.3237  loss_dice: 2.51  loss_ce_0: 0.5798  loss_mask_0: 0.3223  loss_dice_0: 2.648  loss_ce_1: 0.2812  loss_mask_1: 0.3288  loss_dice_1: 2.56  loss_ce_2: 0.2768  loss_mask_2: 0.3251  loss_dice_2: 2.53  loss_ce_3: 0.2501  loss_mask_3: 0.3249  loss_dice_3: 2.521  loss_ce_4: 0.2471  loss_mask_4: 0.3237  loss_dice_4: 2.52  loss_ce_5: 0.234  loss_mask_5: 0.323  loss_dice_5: 2.52  loss_ce_6: 0.2346  loss_mask_6: 0.3232  loss_dice_6: 2.517  loss_ce_7: 0.2288  loss_mask_7: 0.3248  loss_dice_7: 2.505  loss_ce_8: 0.2243  loss_mask_8: 0.3246  loss_dice_8: 2.519  time: 1.4771  data_time: 0.0692  lr: 1.5835e-06  max_mem: 21589M
[01/18 10:56:04] d2.utils.events INFO:  eta: 2:04:17  iter: 34859  total_loss: 31.68  loss_ce: 0.2348  loss_mask: 0.3237  loss_dice: 2.576  loss_ce_0: 0.5629  loss_mask_0: 0.3217  loss_dice_0: 2.727  loss_ce_1: 0.2803  loss_mask_1: 0.3294  loss_dice_1: 2.629  loss_ce_2: 0.2716  loss_mask_2: 0.327  loss_dice_2: 2.604  loss_ce_3: 0.2361  loss_mask_3: 0.3242  loss_dice_3: 2.585  loss_ce_4: 0.2254  loss_mask_4: 0.3242  loss_dice_4: 2.591  loss_ce_5: 0.2175  loss_mask_5: 0.3235  loss_dice_5: 2.587  loss_ce_6: 0.2135  loss_mask_6: 0.3243  loss_dice_6: 2.577  loss_ce_7: 0.2189  loss_mask_7: 0.3241  loss_dice_7: 2.591  loss_ce_8: 0.227  loss_mask_8: 0.3238  loss_dice_8: 2.591  time: 1.4771  data_time: 0.0700  lr: 1.5779e-06  max_mem: 21589M
[01/18 10:56:33] d2.utils.events INFO:  eta: 2:03:47  iter: 34879  total_loss: 30.97  loss_ce: 0.2081  loss_mask: 0.3309  loss_dice: 2.499  loss_ce_0: 0.5543  loss_mask_0: 0.3251  loss_dice_0: 2.634  loss_ce_1: 0.2696  loss_mask_1: 0.3384  loss_dice_1: 2.547  loss_ce_2: 0.2609  loss_mask_2: 0.3345  loss_dice_2: 2.513  loss_ce_3: 0.2289  loss_mask_3: 0.3319  loss_dice_3: 2.509  loss_ce_4: 0.2186  loss_mask_4: 0.3305  loss_dice_4: 2.509  loss_ce_5: 0.2123  loss_mask_5: 0.3298  loss_dice_5: 2.502  loss_ce_6: 0.2055  loss_mask_6: 0.3308  loss_dice_6: 2.498  loss_ce_7: 0.2207  loss_mask_7: 0.3301  loss_dice_7: 2.499  loss_ce_8: 0.2032  loss_mask_8: 0.3308  loss_dice_8: 2.498  time: 1.4771  data_time: 0.0696  lr: 1.5724e-06  max_mem: 21589M
[01/18 10:57:02] d2.utils.events INFO:  eta: 2:03:22  iter: 34899  total_loss: 31.76  loss_ce: 0.22  loss_mask: 0.3281  loss_dice: 2.554  loss_ce_0: 0.5331  loss_mask_0: 0.3272  loss_dice_0: 2.676  loss_ce_1: 0.266  loss_mask_1: 0.3351  loss_dice_1: 2.591  loss_ce_2: 0.2718  loss_mask_2: 0.3323  loss_dice_2: 2.573  loss_ce_3: 0.2406  loss_mask_3: 0.3293  loss_dice_3: 2.558  loss_ce_4: 0.2235  loss_mask_4: 0.3287  loss_dice_4: 2.561  loss_ce_5: 0.2203  loss_mask_5: 0.3285  loss_dice_5: 2.556  loss_ce_6: 0.22  loss_mask_6: 0.3298  loss_dice_6: 2.548  loss_ce_7: 0.2161  loss_mask_7: 0.3282  loss_dice_7: 2.554  loss_ce_8: 0.2104  loss_mask_8: 0.3283  loss_dice_8: 2.551  time: 1.4771  data_time: 0.0654  lr: 1.5669e-06  max_mem: 21589M
[01/18 10:57:31] d2.utils.events INFO:  eta: 2:02:53  iter: 34919  total_loss: 31.79  loss_ce: 0.211  loss_mask: 0.3346  loss_dice: 2.562  loss_ce_0: 0.5483  loss_mask_0: 0.3321  loss_dice_0: 2.686  loss_ce_1: 0.266  loss_mask_1: 0.338  loss_dice_1: 2.613  loss_ce_2: 0.2574  loss_mask_2: 0.3342  loss_dice_2: 2.581  loss_ce_3: 0.2277  loss_mask_3: 0.3317  loss_dice_3: 2.575  loss_ce_4: 0.2216  loss_mask_4: 0.3332  loss_dice_4: 2.566  loss_ce_5: 0.2243  loss_mask_5: 0.3323  loss_dice_5: 2.564  loss_ce_6: 0.2159  loss_mask_6: 0.3357  loss_dice_6: 2.562  loss_ce_7: 0.2118  loss_mask_7: 0.3341  loss_dice_7: 2.565  loss_ce_8: 0.2046  loss_mask_8: 0.3352  loss_dice_8: 2.566  time: 1.4771  data_time: 0.0670  lr: 1.5614e-06  max_mem: 21589M
[01/18 10:57:59] d2.utils.events INFO:  eta: 2:02:21  iter: 34939  total_loss: 32.26  loss_ce: 0.2396  loss_mask: 0.3302  loss_dice: 2.57  loss_ce_0: 0.5917  loss_mask_0: 0.3305  loss_dice_0: 2.711  loss_ce_1: 0.2948  loss_mask_1: 0.3339  loss_dice_1: 2.623  loss_ce_2: 0.2909  loss_mask_2: 0.3323  loss_dice_2: 2.601  loss_ce_3: 0.2708  loss_mask_3: 0.329  loss_dice_3: 2.581  loss_ce_4: 0.2577  loss_mask_4: 0.331  loss_dice_4: 2.581  loss_ce_5: 0.2471  loss_mask_5: 0.328  loss_dice_5: 2.585  loss_ce_6: 0.2494  loss_mask_6: 0.3289  loss_dice_6: 2.573  loss_ce_7: 0.25  loss_mask_7: 0.3296  loss_dice_7: 2.581  loss_ce_8: 0.2437  loss_mask_8: 0.3296  loss_dice_8: 2.576  time: 1.4770  data_time: 0.0619  lr: 1.5558e-06  max_mem: 21589M
[01/18 10:58:28] d2.utils.events INFO:  eta: 2:01:51  iter: 34959  total_loss: 32.18  loss_ce: 0.2307  loss_mask: 0.3179  loss_dice: 2.562  loss_ce_0: 0.5558  loss_mask_0: 0.3125  loss_dice_0: 2.717  loss_ce_1: 0.2756  loss_mask_1: 0.3209  loss_dice_1: 2.623  loss_ce_2: 0.2695  loss_mask_2: 0.3175  loss_dice_2: 2.599  loss_ce_3: 0.2532  loss_mask_3: 0.317  loss_dice_3: 2.573  loss_ce_4: 0.2455  loss_mask_4: 0.3185  loss_dice_4: 2.571  loss_ce_5: 0.2366  loss_mask_5: 0.3169  loss_dice_5: 2.57  loss_ce_6: 0.2395  loss_mask_6: 0.3171  loss_dice_6: 2.56  loss_ce_7: 0.2238  loss_mask_7: 0.3183  loss_dice_7: 2.57  loss_ce_8: 0.2357  loss_mask_8: 0.3162  loss_dice_8: 2.558  time: 1.4770  data_time: 0.0656  lr: 1.5503e-06  max_mem: 21589M
[01/18 10:58:56] d2.utils.events INFO:  eta: 2:01:13  iter: 34979  total_loss: 30.77  loss_ce: 0.2125  loss_mask: 0.3244  loss_dice: 2.467  loss_ce_0: 0.5246  loss_mask_0: 0.3231  loss_dice_0: 2.62  loss_ce_1: 0.2367  loss_mask_1: 0.3323  loss_dice_1: 2.521  loss_ce_2: 0.2382  loss_mask_2: 0.3303  loss_dice_2: 2.492  loss_ce_3: 0.2191  loss_mask_3: 0.3305  loss_dice_3: 2.482  loss_ce_4: 0.2165  loss_mask_4: 0.3291  loss_dice_4: 2.479  loss_ce_5: 0.2178  loss_mask_5: 0.3265  loss_dice_5: 2.475  loss_ce_6: 0.2112  loss_mask_6: 0.3274  loss_dice_6: 2.48  loss_ce_7: 0.2033  loss_mask_7: 0.327  loss_dice_7: 2.473  loss_ce_8: 0.1946  loss_mask_8: 0.3258  loss_dice_8: 2.475  time: 1.4770  data_time: 0.0599  lr: 1.5447e-06  max_mem: 21589M
[01/18 10:59:24] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0034999.pth
[01/18 10:59:25] d2.utils.events INFO:  eta: 2:00:32  iter: 34999  total_loss: 31.33  loss_ce: 0.1951  loss_mask: 0.3294  loss_dice: 2.528  loss_ce_0: 0.536  loss_mask_0: 0.3281  loss_dice_0: 2.657  loss_ce_1: 0.2687  loss_mask_1: 0.3374  loss_dice_1: 2.572  loss_ce_2: 0.2375  loss_mask_2: 0.3336  loss_dice_2: 2.553  loss_ce_3: 0.2288  loss_mask_3: 0.3313  loss_dice_3: 2.541  loss_ce_4: 0.216  loss_mask_4: 0.3301  loss_dice_4: 2.538  loss_ce_5: 0.2119  loss_mask_5: 0.3298  loss_dice_5: 2.533  loss_ce_6: 0.1999  loss_mask_6: 0.3293  loss_dice_6: 2.536  loss_ce_7: 0.2053  loss_mask_7: 0.3289  loss_dice_7: 2.529  loss_ce_8: 0.2022  loss_mask_8: 0.3283  loss_dice_8: 2.527  time: 1.4769  data_time: 0.0637  lr: 1.5392e-06  max_mem: 21589M
[01/18 10:59:52] d2.utils.events INFO:  eta: 1:59:55  iter: 35019  total_loss: 31.21  loss_ce: 0.1951  loss_mask: 0.3261  loss_dice: 2.52  loss_ce_0: 0.5497  loss_mask_0: 0.3261  loss_dice_0: 2.636  loss_ce_1: 0.2516  loss_mask_1: 0.3316  loss_dice_1: 2.561  loss_ce_2: 0.2553  loss_mask_2: 0.3291  loss_dice_2: 2.544  loss_ce_3: 0.2287  loss_mask_3: 0.3256  loss_dice_3: 2.522  loss_ce_4: 0.2228  loss_mask_4: 0.3253  loss_dice_4: 2.526  loss_ce_5: 0.2245  loss_mask_5: 0.3254  loss_dice_5: 2.526  loss_ce_6: 0.2081  loss_mask_6: 0.3258  loss_dice_6: 2.519  loss_ce_7: 0.1946  loss_mask_7: 0.3263  loss_dice_7: 2.52  loss_ce_8: 0.207  loss_mask_8: 0.3257  loss_dice_8: 2.526  time: 1.4769  data_time: 0.0598  lr: 1.5337e-06  max_mem: 21589M
[01/18 11:00:21] d2.utils.events INFO:  eta: 1:59:21  iter: 35039  total_loss: 31.66  loss_ce: 0.235  loss_mask: 0.3279  loss_dice: 2.548  loss_ce_0: 0.5529  loss_mask_0: 0.3303  loss_dice_0: 2.69  loss_ce_1: 0.2684  loss_mask_1: 0.3309  loss_dice_1: 2.599  loss_ce_2: 0.2521  loss_mask_2: 0.3283  loss_dice_2: 2.576  loss_ce_3: 0.2365  loss_mask_3: 0.3268  loss_dice_3: 2.56  loss_ce_4: 0.2243  loss_mask_4: 0.3269  loss_dice_4: 2.558  loss_ce_5: 0.2269  loss_mask_5: 0.3282  loss_dice_5: 2.563  loss_ce_6: 0.2301  loss_mask_6: 0.3269  loss_dice_6: 2.553  loss_ce_7: 0.2232  loss_mask_7: 0.3281  loss_dice_7: 2.558  loss_ce_8: 0.2192  loss_mask_8: 0.3284  loss_dice_8: 2.558  time: 1.4768  data_time: 0.0646  lr: 1.5281e-06  max_mem: 21589M
[01/18 11:00:49] d2.utils.events INFO:  eta: 1:58:42  iter: 35059  total_loss: 31.99  loss_ce: 0.2077  loss_mask: 0.3257  loss_dice: 2.617  loss_ce_0: 0.522  loss_mask_0: 0.3256  loss_dice_0: 2.749  loss_ce_1: 0.2597  loss_mask_1: 0.3308  loss_dice_1: 2.664  loss_ce_2: 0.2676  loss_mask_2: 0.3276  loss_dice_2: 2.639  loss_ce_3: 0.2353  loss_mask_3: 0.3286  loss_dice_3: 2.627  loss_ce_4: 0.2303  loss_mask_4: 0.3267  loss_dice_4: 2.625  loss_ce_5: 0.23  loss_mask_5: 0.3271  loss_dice_5: 2.614  loss_ce_6: 0.2095  loss_mask_6: 0.3274  loss_dice_6: 2.618  loss_ce_7: 0.2123  loss_mask_7: 0.3261  loss_dice_7: 2.616  loss_ce_8: 0.2176  loss_mask_8: 0.3257  loss_dice_8: 2.627  time: 1.4768  data_time: 0.0638  lr: 1.5226e-06  max_mem: 21589M
[01/18 11:01:17] d2.utils.events INFO:  eta: 1:58:12  iter: 35079  total_loss: 32.17  loss_ce: 0.2296  loss_mask: 0.3254  loss_dice: 2.604  loss_ce_0: 0.573  loss_mask_0: 0.3274  loss_dice_0: 2.744  loss_ce_1: 0.2651  loss_mask_1: 0.33  loss_dice_1: 2.651  loss_ce_2: 0.2621  loss_mask_2: 0.3261  loss_dice_2: 2.629  loss_ce_3: 0.2462  loss_mask_3: 0.3251  loss_dice_3: 2.612  loss_ce_4: 0.2408  loss_mask_4: 0.3244  loss_dice_4: 2.617  loss_ce_5: 0.2339  loss_mask_5: 0.3229  loss_dice_5: 2.611  loss_ce_6: 0.2264  loss_mask_6: 0.3229  loss_dice_6: 2.617  loss_ce_7: 0.2333  loss_mask_7: 0.3232  loss_dice_7: 2.605  loss_ce_8: 0.2221  loss_mask_8: 0.3247  loss_dice_8: 2.596  time: 1.4768  data_time: 0.0647  lr: 1.517e-06  max_mem: 21589M
[01/18 11:01:46] d2.utils.events INFO:  eta: 1:57:39  iter: 35099  total_loss: 31.81  loss_ce: 0.215  loss_mask: 0.3313  loss_dice: 2.565  loss_ce_0: 0.519  loss_mask_0: 0.3274  loss_dice_0: 2.732  loss_ce_1: 0.2764  loss_mask_1: 0.3341  loss_dice_1: 2.623  loss_ce_2: 0.2603  loss_mask_2: 0.3328  loss_dice_2: 2.595  loss_ce_3: 0.2392  loss_mask_3: 0.3303  loss_dice_3: 2.578  loss_ce_4: 0.2294  loss_mask_4: 0.3299  loss_dice_4: 2.583  loss_ce_5: 0.2279  loss_mask_5: 0.3316  loss_dice_5: 2.575  loss_ce_6: 0.2193  loss_mask_6: 0.3295  loss_dice_6: 2.568  loss_ce_7: 0.2143  loss_mask_7: 0.3315  loss_dice_7: 2.573  loss_ce_8: 0.2091  loss_mask_8: 0.3324  loss_dice_8: 2.566  time: 1.4767  data_time: 0.0637  lr: 1.5115e-06  max_mem: 21589M
[01/18 11:02:14] d2.utils.events INFO:  eta: 1:57:03  iter: 35119  total_loss: 32.07  loss_ce: 0.2126  loss_mask: 0.3311  loss_dice: 2.598  loss_ce_0: 0.5481  loss_mask_0: 0.3344  loss_dice_0: 2.725  loss_ce_1: 0.2573  loss_mask_1: 0.3426  loss_dice_1: 2.637  loss_ce_2: 0.2486  loss_mask_2: 0.3363  loss_dice_2: 2.626  loss_ce_3: 0.2307  loss_mask_3: 0.3342  loss_dice_3: 2.607  loss_ce_4: 0.2251  loss_mask_4: 0.3326  loss_dice_4: 2.604  loss_ce_5: 0.2257  loss_mask_5: 0.3332  loss_dice_5: 2.599  loss_ce_6: 0.2283  loss_mask_6: 0.3325  loss_dice_6: 2.609  loss_ce_7: 0.222  loss_mask_7: 0.3303  loss_dice_7: 2.605  loss_ce_8: 0.2027  loss_mask_8: 0.3315  loss_dice_8: 2.605  time: 1.4767  data_time: 0.0697  lr: 1.5059e-06  max_mem: 21589M
[01/18 11:02:43] d2.utils.events INFO:  eta: 1:56:33  iter: 35139  total_loss: 31.48  loss_ce: 0.237  loss_mask: 0.3227  loss_dice: 2.528  loss_ce_0: 0.5419  loss_mask_0: 0.324  loss_dice_0: 2.686  loss_ce_1: 0.2635  loss_mask_1: 0.3256  loss_dice_1: 2.586  loss_ce_2: 0.2712  loss_mask_2: 0.3228  loss_dice_2: 2.556  loss_ce_3: 0.2538  loss_mask_3: 0.3246  loss_dice_3: 2.537  loss_ce_4: 0.2533  loss_mask_4: 0.3228  loss_dice_4: 2.546  loss_ce_5: 0.226  loss_mask_5: 0.3227  loss_dice_5: 2.537  loss_ce_6: 0.2524  loss_mask_6: 0.3215  loss_dice_6: 2.535  loss_ce_7: 0.2348  loss_mask_7: 0.3239  loss_dice_7: 2.539  loss_ce_8: 0.2335  loss_mask_8: 0.3227  loss_dice_8: 2.539  time: 1.4766  data_time: 0.0664  lr: 1.5004e-06  max_mem: 21589M
[01/18 11:03:10] d2.utils.events INFO:  eta: 1:56:00  iter: 35159  total_loss: 30.84  loss_ce: 0.2049  loss_mask: 0.3161  loss_dice: 2.478  loss_ce_0: 0.5241  loss_mask_0: 0.3103  loss_dice_0: 2.617  loss_ce_1: 0.2705  loss_mask_1: 0.3198  loss_dice_1: 2.52  loss_ce_2: 0.2674  loss_mask_2: 0.3164  loss_dice_2: 2.497  loss_ce_3: 0.2337  loss_mask_3: 0.3154  loss_dice_3: 2.488  loss_ce_4: 0.2175  loss_mask_4: 0.3153  loss_dice_4: 2.489  loss_ce_5: 0.2216  loss_mask_5: 0.3156  loss_dice_5: 2.486  loss_ce_6: 0.2245  loss_mask_6: 0.3161  loss_dice_6: 2.485  loss_ce_7: 0.2157  loss_mask_7: 0.3153  loss_dice_7: 2.484  loss_ce_8: 0.2023  loss_mask_8: 0.3156  loss_dice_8: 2.481  time: 1.4766  data_time: 0.0620  lr: 1.4948e-06  max_mem: 21589M
[01/18 11:03:39] d2.utils.events INFO:  eta: 1:55:29  iter: 35179  total_loss: 31.51  loss_ce: 0.2148  loss_mask: 0.3184  loss_dice: 2.524  loss_ce_0: 0.5301  loss_mask_0: 0.3159  loss_dice_0: 2.664  loss_ce_1: 0.2451  loss_mask_1: 0.3229  loss_dice_1: 2.576  loss_ce_2: 0.2443  loss_mask_2: 0.3213  loss_dice_2: 2.551  loss_ce_3: 0.2359  loss_mask_3: 0.3194  loss_dice_3: 2.532  loss_ce_4: 0.2369  loss_mask_4: 0.32  loss_dice_4: 2.527  loss_ce_5: 0.2171  loss_mask_5: 0.3193  loss_dice_5: 2.536  loss_ce_6: 0.2093  loss_mask_6: 0.3195  loss_dice_6: 2.531  loss_ce_7: 0.2135  loss_mask_7: 0.3183  loss_dice_7: 2.527  loss_ce_8: 0.211  loss_mask_8: 0.3183  loss_dice_8: 2.524  time: 1.4766  data_time: 0.0658  lr: 1.4893e-06  max_mem: 21589M
[01/18 11:04:07] d2.utils.events INFO:  eta: 1:54:52  iter: 35199  total_loss: 31.51  loss_ce: 0.2019  loss_mask: 0.3302  loss_dice: 2.545  loss_ce_0: 0.5534  loss_mask_0: 0.3352  loss_dice_0: 2.701  loss_ce_1: 0.2621  loss_mask_1: 0.3365  loss_dice_1: 2.604  loss_ce_2: 0.2512  loss_mask_2: 0.3322  loss_dice_2: 2.575  loss_ce_3: 0.2255  loss_mask_3: 0.3294  loss_dice_3: 2.562  loss_ce_4: 0.2134  loss_mask_4: 0.3292  loss_dice_4: 2.557  loss_ce_5: 0.2077  loss_mask_5: 0.328  loss_dice_5: 2.559  loss_ce_6: 0.2168  loss_mask_6: 0.3295  loss_dice_6: 2.553  loss_ce_7: 0.2052  loss_mask_7: 0.3289  loss_dice_7: 2.554  loss_ce_8: 0.2109  loss_mask_8: 0.3283  loss_dice_8: 2.554  time: 1.4765  data_time: 0.0613  lr: 1.4837e-06  max_mem: 21589M
[01/18 11:04:35] d2.utils.events INFO:  eta: 1:54:18  iter: 35219  total_loss: 31.01  loss_ce: 0.1924  loss_mask: 0.3273  loss_dice: 2.468  loss_ce_0: 0.5359  loss_mask_0: 0.322  loss_dice_0: 2.609  loss_ce_1: 0.2651  loss_mask_1: 0.3285  loss_dice_1: 2.512  loss_ce_2: 0.2487  loss_mask_2: 0.3274  loss_dice_2: 2.502  loss_ce_3: 0.2265  loss_mask_3: 0.3262  loss_dice_3: 2.487  loss_ce_4: 0.2093  loss_mask_4: 0.3265  loss_dice_4: 2.476  loss_ce_5: 0.2108  loss_mask_5: 0.3254  loss_dice_5: 2.48  loss_ce_6: 0.2137  loss_mask_6: 0.3262  loss_dice_6: 2.48  loss_ce_7: 0.2076  loss_mask_7: 0.3265  loss_dice_7: 2.477  loss_ce_8: 0.2005  loss_mask_8: 0.3259  loss_dice_8: 2.474  time: 1.4765  data_time: 0.0639  lr: 1.4781e-06  max_mem: 21589M
[01/18 11:05:02] d2.utils.events INFO:  eta: 1:53:46  iter: 35239  total_loss: 31.87  loss_ce: 0.2196  loss_mask: 0.3243  loss_dice: 2.529  loss_ce_0: 0.5487  loss_mask_0: 0.3194  loss_dice_0: 2.675  loss_ce_1: 0.2755  loss_mask_1: 0.3283  loss_dice_1: 2.571  loss_ce_2: 0.2675  loss_mask_2: 0.3264  loss_dice_2: 2.552  loss_ce_3: 0.2533  loss_mask_3: 0.3259  loss_dice_3: 2.529  loss_ce_4: 0.2318  loss_mask_4: 0.3264  loss_dice_4: 2.535  loss_ce_5: 0.2247  loss_mask_5: 0.3254  loss_dice_5: 2.53  loss_ce_6: 0.2246  loss_mask_6: 0.3248  loss_dice_6: 2.53  loss_ce_7: 0.2187  loss_mask_7: 0.3249  loss_dice_7: 2.53  loss_ce_8: 0.224  loss_mask_8: 0.3242  loss_dice_8: 2.524  time: 1.4764  data_time: 0.0612  lr: 1.4726e-06  max_mem: 21589M
[01/18 11:05:30] d2.utils.events INFO:  eta: 1:53:13  iter: 35259  total_loss: 31.43  loss_ce: 0.2153  loss_mask: 0.3225  loss_dice: 2.515  loss_ce_0: 0.5295  loss_mask_0: 0.3219  loss_dice_0: 2.671  loss_ce_1: 0.2787  loss_mask_1: 0.3276  loss_dice_1: 2.576  loss_ce_2: 0.2597  loss_mask_2: 0.3233  loss_dice_2: 2.545  loss_ce_3: 0.2345  loss_mask_3: 0.3205  loss_dice_3: 2.528  loss_ce_4: 0.2363  loss_mask_4: 0.3219  loss_dice_4: 2.521  loss_ce_5: 0.235  loss_mask_5: 0.3217  loss_dice_5: 2.53  loss_ce_6: 0.2229  loss_mask_6: 0.3226  loss_dice_6: 2.529  loss_ce_7: 0.2125  loss_mask_7: 0.3222  loss_dice_7: 2.524  loss_ce_8: 0.2121  loss_mask_8: 0.3236  loss_dice_8: 2.52  time: 1.4764  data_time: 0.0573  lr: 1.467e-06  max_mem: 21589M
[01/18 11:05:58] d2.utils.events INFO:  eta: 1:52:34  iter: 35279  total_loss: 31.53  loss_ce: 0.2096  loss_mask: 0.3171  loss_dice: 2.528  loss_ce_0: 0.5371  loss_mask_0: 0.3214  loss_dice_0: 2.682  loss_ce_1: 0.2643  loss_mask_1: 0.3245  loss_dice_1: 2.58  loss_ce_2: 0.2499  loss_mask_2: 0.3206  loss_dice_2: 2.562  loss_ce_3: 0.2277  loss_mask_3: 0.3195  loss_dice_3: 2.542  loss_ce_4: 0.203  loss_mask_4: 0.3174  loss_dice_4: 2.548  loss_ce_5: 0.2129  loss_mask_5: 0.317  loss_dice_5: 2.538  loss_ce_6: 0.2089  loss_mask_6: 0.3168  loss_dice_6: 2.531  loss_ce_7: 0.1978  loss_mask_7: 0.317  loss_dice_7: 2.538  loss_ce_8: 0.1909  loss_mask_8: 0.3174  loss_dice_8: 2.533  time: 1.4763  data_time: 0.0652  lr: 1.4614e-06  max_mem: 21589M
[01/18 11:06:25] d2.utils.events INFO:  eta: 1:51:57  iter: 35299  total_loss: 31.48  loss_ce: 0.2309  loss_mask: 0.3296  loss_dice: 2.51  loss_ce_0: 0.5676  loss_mask_0: 0.3273  loss_dice_0: 2.644  loss_ce_1: 0.2914  loss_mask_1: 0.337  loss_dice_1: 2.562  loss_ce_2: 0.2817  loss_mask_2: 0.3328  loss_dice_2: 2.542  loss_ce_3: 0.2651  loss_mask_3: 0.329  loss_dice_3: 2.522  loss_ce_4: 0.2342  loss_mask_4: 0.3292  loss_dice_4: 2.519  loss_ce_5: 0.2467  loss_mask_5: 0.3303  loss_dice_5: 2.517  loss_ce_6: 0.2374  loss_mask_6: 0.3299  loss_dice_6: 2.515  loss_ce_7: 0.2256  loss_mask_7: 0.3293  loss_dice_7: 2.516  loss_ce_8: 0.2379  loss_mask_8: 0.3306  loss_dice_8: 2.517  time: 1.4762  data_time: 0.0523  lr: 1.4559e-06  max_mem: 21589M
[01/18 11:06:51] d2.utils.events INFO:  eta: 1:51:21  iter: 35319  total_loss: 31.5  loss_ce: 0.2406  loss_mask: 0.3261  loss_dice: 2.493  loss_ce_0: 0.5733  loss_mask_0: 0.3236  loss_dice_0: 2.617  loss_ce_1: 0.2862  loss_mask_1: 0.3267  loss_dice_1: 2.532  loss_ce_2: 0.2759  loss_mask_2: 0.326  loss_dice_2: 2.511  loss_ce_3: 0.2646  loss_mask_3: 0.3252  loss_dice_3: 2.508  loss_ce_4: 0.2537  loss_mask_4: 0.3256  loss_dice_4: 2.497  loss_ce_5: 0.2427  loss_mask_5: 0.3247  loss_dice_5: 2.489  loss_ce_6: 0.2317  loss_mask_6: 0.3244  loss_dice_6: 2.493  loss_ce_7: 0.226  loss_mask_7: 0.3253  loss_dice_7: 2.496  loss_ce_8: 0.2395  loss_mask_8: 0.3266  loss_dice_8: 2.495  time: 1.4762  data_time: 0.0553  lr: 1.4503e-06  max_mem: 21589M
[01/18 11:07:18] d2.utils.events INFO:  eta: 1:50:38  iter: 35339  total_loss: 32.29  loss_ce: 0.2181  loss_mask: 0.3229  loss_dice: 2.6  loss_ce_0: 0.5566  loss_mask_0: 0.3259  loss_dice_0: 2.731  loss_ce_1: 0.2743  loss_mask_1: 0.3299  loss_dice_1: 2.648  loss_ce_2: 0.2613  loss_mask_2: 0.3267  loss_dice_2: 2.638  loss_ce_3: 0.2301  loss_mask_3: 0.3231  loss_dice_3: 2.619  loss_ce_4: 0.2361  loss_mask_4: 0.3225  loss_dice_4: 2.616  loss_ce_5: 0.2229  loss_mask_5: 0.3228  loss_dice_5: 2.612  loss_ce_6: 0.2215  loss_mask_6: 0.3235  loss_dice_6: 2.615  loss_ce_7: 0.2258  loss_mask_7: 0.3224  loss_dice_7: 2.602  loss_ce_8: 0.2224  loss_mask_8: 0.3231  loss_dice_8: 2.603  time: 1.4761  data_time: 0.0528  lr: 1.4447e-06  max_mem: 21589M
[01/18 11:07:45] d2.utils.events INFO:  eta: 1:50:00  iter: 35359  total_loss: 32.23  loss_ce: 0.2472  loss_mask: 0.3266  loss_dice: 2.562  loss_ce_0: 0.5776  loss_mask_0: 0.3289  loss_dice_0: 2.707  loss_ce_1: 0.2781  loss_mask_1: 0.3316  loss_dice_1: 2.618  loss_ce_2: 0.2899  loss_mask_2: 0.3313  loss_dice_2: 2.584  loss_ce_3: 0.2679  loss_mask_3: 0.3284  loss_dice_3: 2.57  loss_ce_4: 0.2566  loss_mask_4: 0.3281  loss_dice_4: 2.573  loss_ce_5: 0.257  loss_mask_5: 0.3273  loss_dice_5: 2.57  loss_ce_6: 0.2414  loss_mask_6: 0.3268  loss_dice_6: 2.564  loss_ce_7: 0.2262  loss_mask_7: 0.3272  loss_dice_7: 2.564  loss_ce_8: 0.2448  loss_mask_8: 0.3267  loss_dice_8: 2.567  time: 1.4760  data_time: 0.0572  lr: 1.4391e-06  max_mem: 21589M
[01/18 11:08:11] d2.utils.events INFO:  eta: 1:49:25  iter: 35379  total_loss: 31.54  loss_ce: 0.2238  loss_mask: 0.3263  loss_dice: 2.519  loss_ce_0: 0.5437  loss_mask_0: 0.3287  loss_dice_0: 2.656  loss_ce_1: 0.2671  loss_mask_1: 0.3321  loss_dice_1: 2.567  loss_ce_2: 0.2597  loss_mask_2: 0.3311  loss_dice_2: 2.555  loss_ce_3: 0.2411  loss_mask_3: 0.3283  loss_dice_3: 2.533  loss_ce_4: 0.2339  loss_mask_4: 0.3276  loss_dice_4: 2.53  loss_ce_5: 0.2221  loss_mask_5: 0.3268  loss_dice_5: 2.527  loss_ce_6: 0.2325  loss_mask_6: 0.3253  loss_dice_6: 2.519  loss_ce_7: 0.2257  loss_mask_7: 0.3262  loss_dice_7: 2.517  loss_ce_8: 0.2247  loss_mask_8: 0.3271  loss_dice_8: 2.525  time: 1.4759  data_time: 0.0513  lr: 1.4335e-06  max_mem: 21589M
[01/18 11:08:38] d2.utils.events INFO:  eta: 1:48:47  iter: 35399  total_loss: 31.44  loss_ce: 0.244  loss_mask: 0.325  loss_dice: 2.491  loss_ce_0: 0.5964  loss_mask_0: 0.3222  loss_dice_0: 2.66  loss_ce_1: 0.2804  loss_mask_1: 0.3292  loss_dice_1: 2.541  loss_ce_2: 0.2673  loss_mask_2: 0.3295  loss_dice_2: 2.506  loss_ce_3: 0.261  loss_mask_3: 0.3252  loss_dice_3: 2.493  loss_ce_4: 0.2489  loss_mask_4: 0.3236  loss_dice_4: 2.499  loss_ce_5: 0.2393  loss_mask_5: 0.3245  loss_dice_5: 2.497  loss_ce_6: 0.2386  loss_mask_6: 0.3238  loss_dice_6: 2.493  loss_ce_7: 0.2229  loss_mask_7: 0.3238  loss_dice_7: 2.487  loss_ce_8: 0.2251  loss_mask_8: 0.3257  loss_dice_8: 2.498  time: 1.4758  data_time: 0.0526  lr: 1.428e-06  max_mem: 21589M
[01/18 11:09:04] d2.utils.events INFO:  eta: 1:48:09  iter: 35419  total_loss: 31.03  loss_ce: 0.2048  loss_mask: 0.323  loss_dice: 2.516  loss_ce_0: 0.5345  loss_mask_0: 0.3209  loss_dice_0: 2.656  loss_ce_1: 0.2571  loss_mask_1: 0.3274  loss_dice_1: 2.567  loss_ce_2: 0.2553  loss_mask_2: 0.3242  loss_dice_2: 2.542  loss_ce_3: 0.2423  loss_mask_3: 0.3232  loss_dice_3: 2.514  loss_ce_4: 0.2283  loss_mask_4: 0.3239  loss_dice_4: 2.521  loss_ce_5: 0.224  loss_mask_5: 0.3226  loss_dice_5: 2.522  loss_ce_6: 0.2258  loss_mask_6: 0.3217  loss_dice_6: 2.513  loss_ce_7: 0.212  loss_mask_7: 0.3208  loss_dice_7: 2.516  loss_ce_8: 0.214  loss_mask_8: 0.3211  loss_dice_8: 2.519  time: 1.4757  data_time: 0.0525  lr: 1.4224e-06  max_mem: 21589M
[01/18 11:09:31] d2.utils.events INFO:  eta: 1:47:28  iter: 35439  total_loss: 31.92  loss_ce: 0.2419  loss_mask: 0.3313  loss_dice: 2.541  loss_ce_0: 0.6064  loss_mask_0: 0.3305  loss_dice_0: 2.683  loss_ce_1: 0.3032  loss_mask_1: 0.3333  loss_dice_1: 2.584  loss_ce_2: 0.2846  loss_mask_2: 0.3317  loss_dice_2: 2.564  loss_ce_3: 0.261  loss_mask_3: 0.3313  loss_dice_3: 2.545  loss_ce_4: 0.2653  loss_mask_4: 0.3311  loss_dice_4: 2.55  loss_ce_5: 0.2587  loss_mask_5: 0.3307  loss_dice_5: 2.543  loss_ce_6: 0.2491  loss_mask_6: 0.3312  loss_dice_6: 2.544  loss_ce_7: 0.2386  loss_mask_7: 0.3309  loss_dice_7: 2.545  loss_ce_8: 0.2459  loss_mask_8: 0.3306  loss_dice_8: 2.535  time: 1.4756  data_time: 0.0537  lr: 1.4168e-06  max_mem: 21589M
[01/18 11:09:57] d2.utils.events INFO:  eta: 1:46:55  iter: 35459  total_loss: 31.21  loss_ce: 0.2208  loss_mask: 0.3302  loss_dice: 2.515  loss_ce_0: 0.5293  loss_mask_0: 0.326  loss_dice_0: 2.652  loss_ce_1: 0.2617  loss_mask_1: 0.3337  loss_dice_1: 2.562  loss_ce_2: 0.2524  loss_mask_2: 0.3318  loss_dice_2: 2.536  loss_ce_3: 0.2391  loss_mask_3: 0.331  loss_dice_3: 2.524  loss_ce_4: 0.235  loss_mask_4: 0.3315  loss_dice_4: 2.526  loss_ce_5: 0.2269  loss_mask_5: 0.3302  loss_dice_5: 2.521  loss_ce_6: 0.2211  loss_mask_6: 0.3313  loss_dice_6: 2.514  loss_ce_7: 0.2041  loss_mask_7: 0.3304  loss_dice_7: 2.52  loss_ce_8: 0.232  loss_mask_8: 0.3303  loss_dice_8: 2.517  time: 1.4756  data_time: 0.0524  lr: 1.4112e-06  max_mem: 21589M
[01/18 11:10:24] d2.utils.events INFO:  eta: 1:46:12  iter: 35479  total_loss: 31.62  loss_ce: 0.2144  loss_mask: 0.3134  loss_dice: 2.545  loss_ce_0: 0.5294  loss_mask_0: 0.3137  loss_dice_0: 2.69  loss_ce_1: 0.2602  loss_mask_1: 0.3168  loss_dice_1: 2.597  loss_ce_2: 0.2617  loss_mask_2: 0.3138  loss_dice_2: 2.567  loss_ce_3: 0.2484  loss_mask_3: 0.3131  loss_dice_3: 2.555  loss_ce_4: 0.2374  loss_mask_4: 0.3141  loss_dice_4: 2.554  loss_ce_5: 0.2294  loss_mask_5: 0.3133  loss_dice_5: 2.56  loss_ce_6: 0.2226  loss_mask_6: 0.3129  loss_dice_6: 2.549  loss_ce_7: 0.2117  loss_mask_7: 0.3136  loss_dice_7: 2.548  loss_ce_8: 0.2065  loss_mask_8: 0.3121  loss_dice_8: 2.542  time: 1.4755  data_time: 0.0517  lr: 1.4056e-06  max_mem: 21589M
[01/18 11:10:51] d2.utils.events INFO:  eta: 1:45:30  iter: 35499  total_loss: 31.9  loss_ce: 0.2116  loss_mask: 0.323  loss_dice: 2.592  loss_ce_0: 0.5192  loss_mask_0: 0.324  loss_dice_0: 2.719  loss_ce_1: 0.2478  loss_mask_1: 0.3286  loss_dice_1: 2.63  loss_ce_2: 0.2516  loss_mask_2: 0.3266  loss_dice_2: 2.612  loss_ce_3: 0.2343  loss_mask_3: 0.3251  loss_dice_3: 2.596  loss_ce_4: 0.2117  loss_mask_4: 0.3246  loss_dice_4: 2.585  loss_ce_5: 0.2142  loss_mask_5: 0.3218  loss_dice_5: 2.596  loss_ce_6: 0.217  loss_mask_6: 0.3213  loss_dice_6: 2.59  loss_ce_7: 0.2094  loss_mask_7: 0.3219  loss_dice_7: 2.584  loss_ce_8: 0.2001  loss_mask_8: 0.3228  loss_dice_8: 2.594  time: 1.4754  data_time: 0.0533  lr: 1.4e-06  max_mem: 21589M
[01/18 11:11:18] d2.utils.events INFO:  eta: 1:44:48  iter: 35519  total_loss: 30.74  loss_ce: 0.1971  loss_mask: 0.3269  loss_dice: 2.507  loss_ce_0: 0.5314  loss_mask_0: 0.3271  loss_dice_0: 2.673  loss_ce_1: 0.2443  loss_mask_1: 0.329  loss_dice_1: 2.562  loss_ce_2: 0.2223  loss_mask_2: 0.3271  loss_dice_2: 2.542  loss_ce_3: 0.2045  loss_mask_3: 0.328  loss_dice_3: 2.52  loss_ce_4: 0.2069  loss_mask_4: 0.3263  loss_dice_4: 2.516  loss_ce_5: 0.1888  loss_mask_5: 0.3258  loss_dice_5: 2.519  loss_ce_6: 0.1912  loss_mask_6: 0.3261  loss_dice_6: 2.512  loss_ce_7: 0.1906  loss_mask_7: 0.3262  loss_dice_7: 2.514  loss_ce_8: 0.1887  loss_mask_8: 0.3265  loss_dice_8: 2.508  time: 1.4753  data_time: 0.0523  lr: 1.3944e-06  max_mem: 21589M
[01/18 11:11:44] d2.utils.events INFO:  eta: 1:44:06  iter: 35539  total_loss: 30.79  loss_ce: 0.2044  loss_mask: 0.3282  loss_dice: 2.46  loss_ce_0: 0.5568  loss_mask_0: 0.3264  loss_dice_0: 2.603  loss_ce_1: 0.2649  loss_mask_1: 0.3333  loss_dice_1: 2.506  loss_ce_2: 0.2435  loss_mask_2: 0.3291  loss_dice_2: 2.493  loss_ce_3: 0.2198  loss_mask_3: 0.3275  loss_dice_3: 2.479  loss_ce_4: 0.2054  loss_mask_4: 0.3277  loss_dice_4: 2.468  loss_ce_5: 0.2059  loss_mask_5: 0.3276  loss_dice_5: 2.471  loss_ce_6: 0.2105  loss_mask_6: 0.3265  loss_dice_6: 2.461  loss_ce_7: 0.2007  loss_mask_7: 0.3269  loss_dice_7: 2.459  loss_ce_8: 0.2025  loss_mask_8: 0.328  loss_dice_8: 2.462  time: 1.4752  data_time: 0.0494  lr: 1.3888e-06  max_mem: 21589M
[01/18 11:12:11] d2.utils.events INFO:  eta: 1:43:31  iter: 35559  total_loss: 31.7  loss_ce: 0.1986  loss_mask: 0.3334  loss_dice: 2.549  loss_ce_0: 0.5391  loss_mask_0: 0.3304  loss_dice_0: 2.684  loss_ce_1: 0.272  loss_mask_1: 0.3396  loss_dice_1: 2.596  loss_ce_2: 0.2712  loss_mask_2: 0.3361  loss_dice_2: 2.574  loss_ce_3: 0.2446  loss_mask_3: 0.3355  loss_dice_3: 2.556  loss_ce_4: 0.239  loss_mask_4: 0.3355  loss_dice_4: 2.551  loss_ce_5: 0.2331  loss_mask_5: 0.3338  loss_dice_5: 2.551  loss_ce_6: 0.232  loss_mask_6: 0.3346  loss_dice_6: 2.543  loss_ce_7: 0.2273  loss_mask_7: 0.3353  loss_dice_7: 2.544  loss_ce_8: 0.2111  loss_mask_8: 0.3336  loss_dice_8: 2.549  time: 1.4752  data_time: 0.0542  lr: 1.3832e-06  max_mem: 21589M
[01/18 11:12:37] d2.utils.events INFO:  eta: 1:42:45  iter: 35579  total_loss: 30.62  loss_ce: 0.2006  loss_mask: 0.3244  loss_dice: 2.475  loss_ce_0: 0.5214  loss_mask_0: 0.3254  loss_dice_0: 2.614  loss_ce_1: 0.2294  loss_mask_1: 0.3292  loss_dice_1: 2.527  loss_ce_2: 0.2285  loss_mask_2: 0.3251  loss_dice_2: 2.501  loss_ce_3: 0.2127  loss_mask_3: 0.3252  loss_dice_3: 2.485  loss_ce_4: 0.2116  loss_mask_4: 0.3238  loss_dice_4: 2.473  loss_ce_5: 0.2027  loss_mask_5: 0.3225  loss_dice_5: 2.478  loss_ce_6: 0.1979  loss_mask_6: 0.3234  loss_dice_6: 2.482  loss_ce_7: 0.1859  loss_mask_7: 0.3242  loss_dice_7: 2.479  loss_ce_8: 0.1926  loss_mask_8: 0.3222  loss_dice_8: 2.482  time: 1.4751  data_time: 0.0500  lr: 1.3776e-06  max_mem: 21589M
[01/18 11:13:03] d2.utils.events INFO:  eta: 1:42:02  iter: 35599  total_loss: 31.43  loss_ce: 0.2185  loss_mask: 0.3282  loss_dice: 2.553  loss_ce_0: 0.5247  loss_mask_0: 0.3309  loss_dice_0: 2.682  loss_ce_1: 0.2836  loss_mask_1: 0.334  loss_dice_1: 2.597  loss_ce_2: 0.2699  loss_mask_2: 0.3302  loss_dice_2: 2.575  loss_ce_3: 0.2458  loss_mask_3: 0.3288  loss_dice_3: 2.565  loss_ce_4: 0.2329  loss_mask_4: 0.328  loss_dice_4: 2.549  loss_ce_5: 0.2288  loss_mask_5: 0.3281  loss_dice_5: 2.555  loss_ce_6: 0.2274  loss_mask_6: 0.3287  loss_dice_6: 2.549  loss_ce_7: 0.2183  loss_mask_7: 0.3284  loss_dice_7: 2.553  loss_ce_8: 0.2161  loss_mask_8: 0.3287  loss_dice_8: 2.553  time: 1.4750  data_time: 0.0523  lr: 1.372e-06  max_mem: 21589M
[01/18 11:13:30] d2.utils.events INFO:  eta: 1:41:21  iter: 35619  total_loss: 31.9  loss_ce: 0.2286  loss_mask: 0.325  loss_dice: 2.579  loss_ce_0: 0.5723  loss_mask_0: 0.3299  loss_dice_0: 2.723  loss_ce_1: 0.2799  loss_mask_1: 0.3317  loss_dice_1: 2.61  loss_ce_2: 0.2812  loss_mask_2: 0.328  loss_dice_2: 2.601  loss_ce_3: 0.2544  loss_mask_3: 0.3262  loss_dice_3: 2.589  loss_ce_4: 0.2509  loss_mask_4: 0.3271  loss_dice_4: 2.582  loss_ce_5: 0.2455  loss_mask_5: 0.3263  loss_dice_5: 2.58  loss_ce_6: 0.2206  loss_mask_6: 0.325  loss_dice_6: 2.58  loss_ce_7: 0.2337  loss_mask_7: 0.325  loss_dice_7: 2.581  loss_ce_8: 0.2312  loss_mask_8: 0.3251  loss_dice_8: 2.58  time: 1.4749  data_time: 0.0564  lr: 1.3663e-06  max_mem: 21589M
[01/18 11:13:57] d2.utils.events INFO:  eta: 1:40:44  iter: 35639  total_loss: 30.86  loss_ce: 0.2142  loss_mask: 0.3182  loss_dice: 2.474  loss_ce_0: 0.55  loss_mask_0: 0.3164  loss_dice_0: 2.611  loss_ce_1: 0.2578  loss_mask_1: 0.3246  loss_dice_1: 2.523  loss_ce_2: 0.2535  loss_mask_2: 0.322  loss_dice_2: 2.504  loss_ce_3: 0.2352  loss_mask_3: 0.3214  loss_dice_3: 2.483  loss_ce_4: 0.2246  loss_mask_4: 0.3212  loss_dice_4: 2.486  loss_ce_5: 0.2226  loss_mask_5: 0.3195  loss_dice_5: 2.488  loss_ce_6: 0.2107  loss_mask_6: 0.3185  loss_dice_6: 2.481  loss_ce_7: 0.2054  loss_mask_7: 0.3175  loss_dice_7: 2.485  loss_ce_8: 0.2104  loss_mask_8: 0.3183  loss_dice_8: 2.482  time: 1.4748  data_time: 0.0527  lr: 1.3607e-06  max_mem: 21589M
[01/18 11:14:23] d2.utils.events INFO:  eta: 1:40:07  iter: 35659  total_loss: 32.41  loss_ce: 0.2168  loss_mask: 0.3231  loss_dice: 2.622  loss_ce_0: 0.5746  loss_mask_0: 0.3319  loss_dice_0: 2.767  loss_ce_1: 0.2823  loss_mask_1: 0.3318  loss_dice_1: 2.683  loss_ce_2: 0.2642  loss_mask_2: 0.3285  loss_dice_2: 2.653  loss_ce_3: 0.2346  loss_mask_3: 0.3268  loss_dice_3: 2.64  loss_ce_4: 0.2461  loss_mask_4: 0.3268  loss_dice_4: 2.638  loss_ce_5: 0.2157  loss_mask_5: 0.3259  loss_dice_5: 2.63  loss_ce_6: 0.2223  loss_mask_6: 0.3235  loss_dice_6: 2.631  loss_ce_7: 0.2196  loss_mask_7: 0.3222  loss_dice_7: 2.627  loss_ce_8: 0.2227  loss_mask_8: 0.3227  loss_dice_8: 2.627  time: 1.4747  data_time: 0.0518  lr: 1.3551e-06  max_mem: 21589M
[01/18 11:14:49] d2.utils.events INFO:  eta: 1:39:20  iter: 35679  total_loss: 30.85  loss_ce: 0.2046  loss_mask: 0.3235  loss_dice: 2.458  loss_ce_0: 0.5127  loss_mask_0: 0.3227  loss_dice_0: 2.607  loss_ce_1: 0.2684  loss_mask_1: 0.3281  loss_dice_1: 2.505  loss_ce_2: 0.2679  loss_mask_2: 0.3266  loss_dice_2: 2.482  loss_ce_3: 0.2352  loss_mask_3: 0.3239  loss_dice_3: 2.474  loss_ce_4: 0.2287  loss_mask_4: 0.3236  loss_dice_4: 2.471  loss_ce_5: 0.218  loss_mask_5: 0.3226  loss_dice_5: 2.463  loss_ce_6: 0.2128  loss_mask_6: 0.3227  loss_dice_6: 2.458  loss_ce_7: 0.2161  loss_mask_7: 0.3221  loss_dice_7: 2.463  loss_ce_8: 0.2078  loss_mask_8: 0.3239  loss_dice_8: 2.462  time: 1.4746  data_time: 0.0508  lr: 1.3495e-06  max_mem: 21589M
[01/18 11:15:16] d2.utils.events INFO:  eta: 1:38:37  iter: 35699  total_loss: 31.72  loss_ce: 0.2147  loss_mask: 0.3133  loss_dice: 2.565  loss_ce_0: 0.5623  loss_mask_0: 0.309  loss_dice_0: 2.702  loss_ce_1: 0.266  loss_mask_1: 0.3184  loss_dice_1: 2.617  loss_ce_2: 0.2584  loss_mask_2: 0.3153  loss_dice_2: 2.587  loss_ce_3: 0.2256  loss_mask_3: 0.3141  loss_dice_3: 2.574  loss_ce_4: 0.2213  loss_mask_4: 0.3132  loss_dice_4: 2.567  loss_ce_5: 0.2153  loss_mask_5: 0.313  loss_dice_5: 2.576  loss_ce_6: 0.2122  loss_mask_6: 0.314  loss_dice_6: 2.571  loss_ce_7: 0.2089  loss_mask_7: 0.3145  loss_dice_7: 2.57  loss_ce_8: 0.2082  loss_mask_8: 0.3144  loss_dice_8: 2.563  time: 1.4746  data_time: 0.0540  lr: 1.3439e-06  max_mem: 21589M
[01/18 11:15:43] d2.utils.events INFO:  eta: 1:37:56  iter: 35719  total_loss: 31.18  loss_ce: 0.2415  loss_mask: 0.3149  loss_dice: 2.491  loss_ce_0: 0.5361  loss_mask_0: 0.3123  loss_dice_0: 2.64  loss_ce_1: 0.2723  loss_mask_1: 0.3182  loss_dice_1: 2.537  loss_ce_2: 0.2675  loss_mask_2: 0.3162  loss_dice_2: 2.524  loss_ce_3: 0.2439  loss_mask_3: 0.3153  loss_dice_3: 2.502  loss_ce_4: 0.2535  loss_mask_4: 0.3147  loss_dice_4: 2.502  loss_ce_5: 0.2486  loss_mask_5: 0.314  loss_dice_5: 2.504  loss_ce_6: 0.2325  loss_mask_6: 0.3141  loss_dice_6: 2.492  loss_ce_7: 0.2357  loss_mask_7: 0.3152  loss_dice_7: 2.496  loss_ce_8: 0.2285  loss_mask_8: 0.3143  loss_dice_8: 2.481  time: 1.4745  data_time: 0.0513  lr: 1.3382e-06  max_mem: 21589M
[01/18 11:16:09] d2.utils.events INFO:  eta: 1:37:15  iter: 35739  total_loss: 30.7  loss_ce: 0.2194  loss_mask: 0.3224  loss_dice: 2.469  loss_ce_0: 0.5446  loss_mask_0: 0.3193  loss_dice_0: 2.615  loss_ce_1: 0.2671  loss_mask_1: 0.3286  loss_dice_1: 2.522  loss_ce_2: 0.2705  loss_mask_2: 0.3245  loss_dice_2: 2.491  loss_ce_3: 0.2369  loss_mask_3: 0.3229  loss_dice_3: 2.484  loss_ce_4: 0.2331  loss_mask_4: 0.3238  loss_dice_4: 2.48  loss_ce_5: 0.2304  loss_mask_5: 0.3247  loss_dice_5: 2.469  loss_ce_6: 0.226  loss_mask_6: 0.3222  loss_dice_6: 2.465  loss_ce_7: 0.2123  loss_mask_7: 0.322  loss_dice_7: 2.471  loss_ce_8: 0.2317  loss_mask_8: 0.323  loss_dice_8: 2.466  time: 1.4744  data_time: 0.0497  lr: 1.3326e-06  max_mem: 21589M
[01/18 11:16:35] d2.utils.events INFO:  eta: 1:36:37  iter: 35759  total_loss: 30.68  loss_ce: 0.2076  loss_mask: 0.3326  loss_dice: 2.447  loss_ce_0: 0.5266  loss_mask_0: 0.3309  loss_dice_0: 2.592  loss_ce_1: 0.2519  loss_mask_1: 0.3344  loss_dice_1: 2.498  loss_ce_2: 0.2343  loss_mask_2: 0.3341  loss_dice_2: 2.469  loss_ce_3: 0.2283  loss_mask_3: 0.3331  loss_dice_3: 2.462  loss_ce_4: 0.2191  loss_mask_4: 0.3335  loss_dice_4: 2.447  loss_ce_5: 0.2105  loss_mask_5: 0.3326  loss_dice_5: 2.45  loss_ce_6: 0.216  loss_mask_6: 0.3316  loss_dice_6: 2.45  loss_ce_7: 0.2028  loss_mask_7: 0.333  loss_dice_7: 2.452  loss_ce_8: 0.1958  loss_mask_8: 0.3328  loss_dice_8: 2.448  time: 1.4743  data_time: 0.0516  lr: 1.327e-06  max_mem: 21589M
[01/18 11:17:02] d2.utils.events INFO:  eta: 1:36:00  iter: 35779  total_loss: 31.44  loss_ce: 0.2088  loss_mask: 0.3287  loss_dice: 2.544  loss_ce_0: 0.5572  loss_mask_0: 0.3229  loss_dice_0: 2.674  loss_ce_1: 0.26  loss_mask_1: 0.3314  loss_dice_1: 2.583  loss_ce_2: 0.2475  loss_mask_2: 0.33  loss_dice_2: 2.562  loss_ce_3: 0.2429  loss_mask_3: 0.3276  loss_dice_3: 2.557  loss_ce_4: 0.2126  loss_mask_4: 0.3265  loss_dice_4: 2.543  loss_ce_5: 0.2149  loss_mask_5: 0.3263  loss_dice_5: 2.55  loss_ce_6: 0.2248  loss_mask_6: 0.3277  loss_dice_6: 2.542  loss_ce_7: 0.2106  loss_mask_7: 0.327  loss_dice_7: 2.548  loss_ce_8: 0.211  loss_mask_8: 0.3274  loss_dice_8: 2.546  time: 1.4742  data_time: 0.0518  lr: 1.3214e-06  max_mem: 21589M
[01/18 11:17:28] d2.utils.events INFO:  eta: 1:35:19  iter: 35799  total_loss: 31.62  loss_ce: 0.2189  loss_mask: 0.3143  loss_dice: 2.535  loss_ce_0: 0.5613  loss_mask_0: 0.3175  loss_dice_0: 2.661  loss_ce_1: 0.2782  loss_mask_1: 0.3225  loss_dice_1: 2.58  loss_ce_2: 0.2771  loss_mask_2: 0.3207  loss_dice_2: 2.548  loss_ce_3: 0.2377  loss_mask_3: 0.3179  loss_dice_3: 2.543  loss_ce_4: 0.2341  loss_mask_4: 0.317  loss_dice_4: 2.533  loss_ce_5: 0.213  loss_mask_5: 0.3155  loss_dice_5: 2.539  loss_ce_6: 0.2225  loss_mask_6: 0.3154  loss_dice_6: 2.528  loss_ce_7: 0.2148  loss_mask_7: 0.3143  loss_dice_7: 2.534  loss_ce_8: 0.2105  loss_mask_8: 0.314  loss_dice_8: 2.533  time: 1.4741  data_time: 0.0532  lr: 1.3157e-06  max_mem: 21589M
[01/18 11:17:55] d2.utils.events INFO:  eta: 1:34:42  iter: 35819  total_loss: 32.51  loss_ce: 0.2372  loss_mask: 0.3271  loss_dice: 2.596  loss_ce_0: 0.5828  loss_mask_0: 0.3291  loss_dice_0: 2.736  loss_ce_1: 0.2839  loss_mask_1: 0.3349  loss_dice_1: 2.647  loss_ce_2: 0.2779  loss_mask_2: 0.3301  loss_dice_2: 2.616  loss_ce_3: 0.2497  loss_mask_3: 0.3291  loss_dice_3: 2.601  loss_ce_4: 0.2536  loss_mask_4: 0.3281  loss_dice_4: 2.603  loss_ce_5: 0.2455  loss_mask_5: 0.3263  loss_dice_5: 2.604  loss_ce_6: 0.2298  loss_mask_6: 0.3264  loss_dice_6: 2.595  loss_ce_7: 0.2345  loss_mask_7: 0.3268  loss_dice_7: 2.593  loss_ce_8: 0.2485  loss_mask_8: 0.3264  loss_dice_8: 2.6  time: 1.4740  data_time: 0.0548  lr: 1.3101e-06  max_mem: 21589M
[01/18 11:18:21] d2.utils.events INFO:  eta: 1:34:04  iter: 35839  total_loss: 32.29  loss_ce: 0.2208  loss_mask: 0.3276  loss_dice: 2.575  loss_ce_0: 0.5576  loss_mask_0: 0.3274  loss_dice_0: 2.7  loss_ce_1: 0.2533  loss_mask_1: 0.3286  loss_dice_1: 2.612  loss_ce_2: 0.2646  loss_mask_2: 0.3279  loss_dice_2: 2.582  loss_ce_3: 0.2448  loss_mask_3: 0.3277  loss_dice_3: 2.577  loss_ce_4: 0.2344  loss_mask_4: 0.3268  loss_dice_4: 2.571  loss_ce_5: 0.2208  loss_mask_5: 0.3257  loss_dice_5: 2.575  loss_ce_6: 0.2248  loss_mask_6: 0.3271  loss_dice_6: 2.574  loss_ce_7: 0.2193  loss_mask_7: 0.328  loss_dice_7: 2.574  loss_ce_8: 0.2108  loss_mask_8: 0.3273  loss_dice_8: 2.57  time: 1.4740  data_time: 0.0526  lr: 1.3044e-06  max_mem: 21589M
[01/18 11:18:48] d2.utils.events INFO:  eta: 1:33:26  iter: 35859  total_loss: 31.9  loss_ce: 0.2299  loss_mask: 0.3228  loss_dice: 2.559  loss_ce_0: 0.5458  loss_mask_0: 0.3277  loss_dice_0: 2.692  loss_ce_1: 0.2679  loss_mask_1: 0.3299  loss_dice_1: 2.6  loss_ce_2: 0.2789  loss_mask_2: 0.3262  loss_dice_2: 2.577  loss_ce_3: 0.2642  loss_mask_3: 0.3242  loss_dice_3: 2.561  loss_ce_4: 0.2432  loss_mask_4: 0.3249  loss_dice_4: 2.553  loss_ce_5: 0.2378  loss_mask_5: 0.3242  loss_dice_5: 2.554  loss_ce_6: 0.2334  loss_mask_6: 0.3242  loss_dice_6: 2.564  loss_ce_7: 0.2271  loss_mask_7: 0.323  loss_dice_7: 2.554  loss_ce_8: 0.2264  loss_mask_8: 0.3231  loss_dice_8: 2.547  time: 1.4739  data_time: 0.0542  lr: 1.2988e-06  max_mem: 21589M
[01/18 11:19:14] d2.utils.events INFO:  eta: 1:32:52  iter: 35879  total_loss: 31.27  loss_ce: 0.2262  loss_mask: 0.3266  loss_dice: 2.496  loss_ce_0: 0.5576  loss_mask_0: 0.3268  loss_dice_0: 2.639  loss_ce_1: 0.2953  loss_mask_1: 0.3335  loss_dice_1: 2.539  loss_ce_2: 0.2697  loss_mask_2: 0.3295  loss_dice_2: 2.521  loss_ce_3: 0.2439  loss_mask_3: 0.3266  loss_dice_3: 2.511  loss_ce_4: 0.247  loss_mask_4: 0.3273  loss_dice_4: 2.503  loss_ce_5: 0.2273  loss_mask_5: 0.3266  loss_dice_5: 2.499  loss_ce_6: 0.2349  loss_mask_6: 0.3265  loss_dice_6: 2.492  loss_ce_7: 0.2386  loss_mask_7: 0.3267  loss_dice_7: 2.497  loss_ce_8: 0.2285  loss_mask_8: 0.3268  loss_dice_8: 2.506  time: 1.4738  data_time: 0.0523  lr: 1.2931e-06  max_mem: 21589M
[01/18 11:19:40] d2.utils.events INFO:  eta: 1:32:08  iter: 35899  total_loss: 31.33  loss_ce: 0.1945  loss_mask: 0.3303  loss_dice: 2.55  loss_ce_0: 0.5411  loss_mask_0: 0.3279  loss_dice_0: 2.665  loss_ce_1: 0.247  loss_mask_1: 0.3358  loss_dice_1: 2.588  loss_ce_2: 0.238  loss_mask_2: 0.332  loss_dice_2: 2.574  loss_ce_3: 0.2161  loss_mask_3: 0.3298  loss_dice_3: 2.559  loss_ce_4: 0.208  loss_mask_4: 0.3301  loss_dice_4: 2.548  loss_ce_5: 0.2079  loss_mask_5: 0.3297  loss_dice_5: 2.548  loss_ce_6: 0.2008  loss_mask_6: 0.3295  loss_dice_6: 2.55  loss_ce_7: 0.1913  loss_mask_7: 0.3297  loss_dice_7: 2.548  loss_ce_8: 0.1877  loss_mask_8: 0.3296  loss_dice_8: 2.548  time: 1.4737  data_time: 0.0522  lr: 1.2875e-06  max_mem: 21589M
[01/18 11:20:06] d2.utils.events INFO:  eta: 1:31:32  iter: 35919  total_loss: 32.03  loss_ce: 0.2161  loss_mask: 0.3269  loss_dice: 2.521  loss_ce_0: 0.5604  loss_mask_0: 0.3282  loss_dice_0: 2.667  loss_ce_1: 0.2631  loss_mask_1: 0.3359  loss_dice_1: 2.583  loss_ce_2: 0.2623  loss_mask_2: 0.3306  loss_dice_2: 2.543  loss_ce_3: 0.2452  loss_mask_3: 0.329  loss_dice_3: 2.534  loss_ce_4: 0.2303  loss_mask_4: 0.3268  loss_dice_4: 2.523  loss_ce_5: 0.2203  loss_mask_5: 0.3266  loss_dice_5: 2.528  loss_ce_6: 0.2201  loss_mask_6: 0.326  loss_dice_6: 2.525  loss_ce_7: 0.2184  loss_mask_7: 0.3268  loss_dice_7: 2.525  loss_ce_8: 0.2101  loss_mask_8: 0.326  loss_dice_8: 2.528  time: 1.4736  data_time: 0.0505  lr: 1.2818e-06  max_mem: 21589M
[01/18 11:20:33] d2.utils.events INFO:  eta: 1:30:54  iter: 35939  total_loss: 31.1  loss_ce: 0.2076  loss_mask: 0.3269  loss_dice: 2.478  loss_ce_0: 0.5373  loss_mask_0: 0.3246  loss_dice_0: 2.632  loss_ce_1: 0.2577  loss_mask_1: 0.3332  loss_dice_1: 2.531  loss_ce_2: 0.2499  loss_mask_2: 0.3288  loss_dice_2: 2.518  loss_ce_3: 0.2208  loss_mask_3: 0.3273  loss_dice_3: 2.504  loss_ce_4: 0.2397  loss_mask_4: 0.3265  loss_dice_4: 2.495  loss_ce_5: 0.2125  loss_mask_5: 0.3268  loss_dice_5: 2.489  loss_ce_6: 0.2128  loss_mask_6: 0.328  loss_dice_6: 2.491  loss_ce_7: 0.2035  loss_mask_7: 0.328  loss_dice_7: 2.487  loss_ce_8: 0.2031  loss_mask_8: 0.3273  loss_dice_8: 2.483  time: 1.4735  data_time: 0.0525  lr: 1.2762e-06  max_mem: 21589M
[01/18 11:21:00] d2.utils.events INFO:  eta: 1:30:19  iter: 35959  total_loss: 32.07  loss_ce: 0.2264  loss_mask: 0.331  loss_dice: 2.557  loss_ce_0: 0.5605  loss_mask_0: 0.3315  loss_dice_0: 2.692  loss_ce_1: 0.2715  loss_mask_1: 0.3395  loss_dice_1: 2.609  loss_ce_2: 0.2693  loss_mask_2: 0.3341  loss_dice_2: 2.594  loss_ce_3: 0.2626  loss_mask_3: 0.3312  loss_dice_3: 2.574  loss_ce_4: 0.2434  loss_mask_4: 0.3306  loss_dice_4: 2.579  loss_ce_5: 0.2402  loss_mask_5: 0.3301  loss_dice_5: 2.573  loss_ce_6: 0.232  loss_mask_6: 0.3327  loss_dice_6: 2.568  loss_ce_7: 0.2297  loss_mask_7: 0.331  loss_dice_7: 2.581  loss_ce_8: 0.2326  loss_mask_8: 0.3305  loss_dice_8: 2.578  time: 1.4734  data_time: 0.0531  lr: 1.2705e-06  max_mem: 21589M
[01/18 11:21:26] d2.utils.events INFO:  eta: 1:29:47  iter: 35979  total_loss: 31.19  loss_ce: 0.2056  loss_mask: 0.3276  loss_dice: 2.5  loss_ce_0: 0.5416  loss_mask_0: 0.3286  loss_dice_0: 2.645  loss_ce_1: 0.2679  loss_mask_1: 0.3349  loss_dice_1: 2.542  loss_ce_2: 0.2532  loss_mask_2: 0.3306  loss_dice_2: 2.531  loss_ce_3: 0.2163  loss_mask_3: 0.3301  loss_dice_3: 2.506  loss_ce_4: 0.2184  loss_mask_4: 0.3294  loss_dice_4: 2.502  loss_ce_5: 0.2173  loss_mask_5: 0.3277  loss_dice_5: 2.504  loss_ce_6: 0.2181  loss_mask_6: 0.3266  loss_dice_6: 2.498  loss_ce_7: 0.206  loss_mask_7: 0.3274  loss_dice_7: 2.499  loss_ce_8: 0.2065  loss_mask_8: 0.3283  loss_dice_8: 2.505  time: 1.4733  data_time: 0.0546  lr: 1.2649e-06  max_mem: 21589M
[01/18 11:21:53] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 11:21:53] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 11:21:53] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 11:21:54] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 11:22:05] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0059 s/iter. Inference: 0.1162 s/iter. Eval: 0.1641 s/iter. Total: 0.2863 s/iter. ETA=0:05:09
[01/18 11:22:10] d2.evaluation.evaluator INFO: Inference done 28/1093. Dataloading: 0.0074 s/iter. Inference: 0.1203 s/iter. Eval: 0.1668 s/iter. Total: 0.2946 s/iter. ETA=0:05:13
[01/18 11:22:15] d2.evaluation.evaluator INFO: Inference done 45/1093. Dataloading: 0.0079 s/iter. Inference: 0.1237 s/iter. Eval: 0.1630 s/iter. Total: 0.2946 s/iter. ETA=0:05:08
[01/18 11:22:20] d2.evaluation.evaluator INFO: Inference done 60/1093. Dataloading: 0.0083 s/iter. Inference: 0.1282 s/iter. Eval: 0.1700 s/iter. Total: 0.3066 s/iter. ETA=0:05:16
[01/18 11:22:25] d2.evaluation.evaluator INFO: Inference done 77/1093. Dataloading: 0.0083 s/iter. Inference: 0.1277 s/iter. Eval: 0.1684 s/iter. Total: 0.3044 s/iter. ETA=0:05:09
[01/18 11:22:30] d2.evaluation.evaluator INFO: Inference done 93/1093. Dataloading: 0.0083 s/iter. Inference: 0.1265 s/iter. Eval: 0.1721 s/iter. Total: 0.3069 s/iter. ETA=0:05:06
[01/18 11:22:35] d2.evaluation.evaluator INFO: Inference done 111/1093. Dataloading: 0.0082 s/iter. Inference: 0.1279 s/iter. Eval: 0.1678 s/iter. Total: 0.3040 s/iter. ETA=0:04:58
[01/18 11:22:40] d2.evaluation.evaluator INFO: Inference done 128/1093. Dataloading: 0.0081 s/iter. Inference: 0.1285 s/iter. Eval: 0.1668 s/iter. Total: 0.3035 s/iter. ETA=0:04:52
[01/18 11:22:45] d2.evaluation.evaluator INFO: Inference done 147/1093. Dataloading: 0.0080 s/iter. Inference: 0.1287 s/iter. Eval: 0.1622 s/iter. Total: 0.2990 s/iter. ETA=0:04:42
[01/18 11:22:51] d2.evaluation.evaluator INFO: Inference done 165/1093. Dataloading: 0.0080 s/iter. Inference: 0.1289 s/iter. Eval: 0.1609 s/iter. Total: 0.2978 s/iter. ETA=0:04:36
[01/18 11:22:56] d2.evaluation.evaluator INFO: Inference done 182/1093. Dataloading: 0.0080 s/iter. Inference: 0.1291 s/iter. Eval: 0.1607 s/iter. Total: 0.2979 s/iter. ETA=0:04:31
[01/18 11:23:01] d2.evaluation.evaluator INFO: Inference done 200/1093. Dataloading: 0.0079 s/iter. Inference: 0.1298 s/iter. Eval: 0.1591 s/iter. Total: 0.2969 s/iter. ETA=0:04:25
[01/18 11:23:06] d2.evaluation.evaluator INFO: Inference done 216/1093. Dataloading: 0.0079 s/iter. Inference: 0.1313 s/iter. Eval: 0.1600 s/iter. Total: 0.2992 s/iter. ETA=0:04:22
[01/18 11:23:11] d2.evaluation.evaluator INFO: Inference done 233/1093. Dataloading: 0.0079 s/iter. Inference: 0.1315 s/iter. Eval: 0.1609 s/iter. Total: 0.3004 s/iter. ETA=0:04:18
[01/18 11:23:17] d2.evaluation.evaluator INFO: Inference done 249/1093. Dataloading: 0.0080 s/iter. Inference: 0.1321 s/iter. Eval: 0.1617 s/iter. Total: 0.3018 s/iter. ETA=0:04:14
[01/18 11:23:22] d2.evaluation.evaluator INFO: Inference done 265/1093. Dataloading: 0.0080 s/iter. Inference: 0.1326 s/iter. Eval: 0.1626 s/iter. Total: 0.3033 s/iter. ETA=0:04:11
[01/18 11:23:27] d2.evaluation.evaluator INFO: Inference done 282/1093. Dataloading: 0.0080 s/iter. Inference: 0.1330 s/iter. Eval: 0.1622 s/iter. Total: 0.3033 s/iter. ETA=0:04:06
[01/18 11:23:32] d2.evaluation.evaluator INFO: Inference done 300/1093. Dataloading: 0.0080 s/iter. Inference: 0.1332 s/iter. Eval: 0.1614 s/iter. Total: 0.3027 s/iter. ETA=0:04:00
[01/18 11:23:37] d2.evaluation.evaluator INFO: Inference done 316/1093. Dataloading: 0.0080 s/iter. Inference: 0.1325 s/iter. Eval: 0.1630 s/iter. Total: 0.3036 s/iter. ETA=0:03:55
[01/18 11:23:42] d2.evaluation.evaluator INFO: Inference done 334/1093. Dataloading: 0.0080 s/iter. Inference: 0.1321 s/iter. Eval: 0.1623 s/iter. Total: 0.3024 s/iter. ETA=0:03:49
[01/18 11:23:48] d2.evaluation.evaluator INFO: Inference done 354/1093. Dataloading: 0.0079 s/iter. Inference: 0.1320 s/iter. Eval: 0.1600 s/iter. Total: 0.3000 s/iter. ETA=0:03:41
[01/18 11:23:53] d2.evaluation.evaluator INFO: Inference done 370/1093. Dataloading: 0.0079 s/iter. Inference: 0.1319 s/iter. Eval: 0.1609 s/iter. Total: 0.3007 s/iter. ETA=0:03:37
[01/18 11:23:58] d2.evaluation.evaluator INFO: Inference done 388/1093. Dataloading: 0.0079 s/iter. Inference: 0.1319 s/iter. Eval: 0.1601 s/iter. Total: 0.3000 s/iter. ETA=0:03:31
[01/18 11:24:03] d2.evaluation.evaluator INFO: Inference done 405/1093. Dataloading: 0.0079 s/iter. Inference: 0.1318 s/iter. Eval: 0.1601 s/iter. Total: 0.2998 s/iter. ETA=0:03:26
[01/18 11:24:08] d2.evaluation.evaluator INFO: Inference done 422/1093. Dataloading: 0.0079 s/iter. Inference: 0.1321 s/iter. Eval: 0.1602 s/iter. Total: 0.3003 s/iter. ETA=0:03:21
[01/18 11:24:13] d2.evaluation.evaluator INFO: Inference done 440/1093. Dataloading: 0.0079 s/iter. Inference: 0.1319 s/iter. Eval: 0.1598 s/iter. Total: 0.2997 s/iter. ETA=0:03:15
[01/18 11:24:19] d2.evaluation.evaluator INFO: Inference done 458/1093. Dataloading: 0.0079 s/iter. Inference: 0.1320 s/iter. Eval: 0.1594 s/iter. Total: 0.2993 s/iter. ETA=0:03:10
[01/18 11:24:24] d2.evaluation.evaluator INFO: Inference done 477/1093. Dataloading: 0.0078 s/iter. Inference: 0.1324 s/iter. Eval: 0.1581 s/iter. Total: 0.2984 s/iter. ETA=0:03:03
[01/18 11:24:29] d2.evaluation.evaluator INFO: Inference done 495/1093. Dataloading: 0.0078 s/iter. Inference: 0.1324 s/iter. Eval: 0.1575 s/iter. Total: 0.2978 s/iter. ETA=0:02:58
[01/18 11:24:34] d2.evaluation.evaluator INFO: Inference done 515/1093. Dataloading: 0.0077 s/iter. Inference: 0.1323 s/iter. Eval: 0.1563 s/iter. Total: 0.2964 s/iter. ETA=0:02:51
[01/18 11:24:39] d2.evaluation.evaluator INFO: Inference done 530/1093. Dataloading: 0.0078 s/iter. Inference: 0.1322 s/iter. Eval: 0.1575 s/iter. Total: 0.2976 s/iter. ETA=0:02:47
[01/18 11:24:44] d2.evaluation.evaluator INFO: Inference done 548/1093. Dataloading: 0.0077 s/iter. Inference: 0.1324 s/iter. Eval: 0.1572 s/iter. Total: 0.2974 s/iter. ETA=0:02:42
[01/18 11:24:49] d2.evaluation.evaluator INFO: Inference done 565/1093. Dataloading: 0.0077 s/iter. Inference: 0.1327 s/iter. Eval: 0.1568 s/iter. Total: 0.2973 s/iter. ETA=0:02:36
[01/18 11:24:54] d2.evaluation.evaluator INFO: Inference done 584/1093. Dataloading: 0.0078 s/iter. Inference: 0.1331 s/iter. Eval: 0.1554 s/iter. Total: 0.2963 s/iter. ETA=0:02:30
[01/18 11:25:00] d2.evaluation.evaluator INFO: Inference done 601/1093. Dataloading: 0.0078 s/iter. Inference: 0.1329 s/iter. Eval: 0.1556 s/iter. Total: 0.2964 s/iter. ETA=0:02:25
[01/18 11:25:05] d2.evaluation.evaluator INFO: Inference done 619/1093. Dataloading: 0.0078 s/iter. Inference: 0.1326 s/iter. Eval: 0.1556 s/iter. Total: 0.2961 s/iter. ETA=0:02:20
[01/18 11:25:10] d2.evaluation.evaluator INFO: Inference done 637/1093. Dataloading: 0.0078 s/iter. Inference: 0.1324 s/iter. Eval: 0.1555 s/iter. Total: 0.2957 s/iter. ETA=0:02:14
[01/18 11:25:15] d2.evaluation.evaluator INFO: Inference done 657/1093. Dataloading: 0.0077 s/iter. Inference: 0.1321 s/iter. Eval: 0.1548 s/iter. Total: 0.2947 s/iter. ETA=0:02:08
[01/18 11:25:20] d2.evaluation.evaluator INFO: Inference done 675/1093. Dataloading: 0.0077 s/iter. Inference: 0.1323 s/iter. Eval: 0.1545 s/iter. Total: 0.2946 s/iter. ETA=0:02:03
[01/18 11:25:25] d2.evaluation.evaluator INFO: Inference done 694/1093. Dataloading: 0.0077 s/iter. Inference: 0.1323 s/iter. Eval: 0.1537 s/iter. Total: 0.2938 s/iter. ETA=0:01:57
[01/18 11:25:31] d2.evaluation.evaluator INFO: Inference done 710/1093. Dataloading: 0.0077 s/iter. Inference: 0.1323 s/iter. Eval: 0.1543 s/iter. Total: 0.2945 s/iter. ETA=0:01:52
[01/18 11:25:36] d2.evaluation.evaluator INFO: Inference done 727/1093. Dataloading: 0.0077 s/iter. Inference: 0.1326 s/iter. Eval: 0.1543 s/iter. Total: 0.2947 s/iter. ETA=0:01:47
[01/18 11:25:41] d2.evaluation.evaluator INFO: Inference done 746/1093. Dataloading: 0.0077 s/iter. Inference: 0.1325 s/iter. Eval: 0.1538 s/iter. Total: 0.2940 s/iter. ETA=0:01:42
[01/18 11:25:46] d2.evaluation.evaluator INFO: Inference done 763/1093. Dataloading: 0.0077 s/iter. Inference: 0.1326 s/iter. Eval: 0.1537 s/iter. Total: 0.2941 s/iter. ETA=0:01:37
[01/18 11:25:51] d2.evaluation.evaluator INFO: Inference done 780/1093. Dataloading: 0.0077 s/iter. Inference: 0.1327 s/iter. Eval: 0.1537 s/iter. Total: 0.2941 s/iter. ETA=0:01:32
[01/18 11:25:56] d2.evaluation.evaluator INFO: Inference done 799/1093. Dataloading: 0.0077 s/iter. Inference: 0.1327 s/iter. Eval: 0.1531 s/iter. Total: 0.2936 s/iter. ETA=0:01:26
[01/18 11:26:01] d2.evaluation.evaluator INFO: Inference done 818/1093. Dataloading: 0.0077 s/iter. Inference: 0.1327 s/iter. Eval: 0.1527 s/iter. Total: 0.2931 s/iter. ETA=0:01:20
[01/18 11:26:06] d2.evaluation.evaluator INFO: Inference done 837/1093. Dataloading: 0.0076 s/iter. Inference: 0.1328 s/iter. Eval: 0.1521 s/iter. Total: 0.2925 s/iter. ETA=0:01:14
[01/18 11:26:11] d2.evaluation.evaluator INFO: Inference done 853/1093. Dataloading: 0.0076 s/iter. Inference: 0.1330 s/iter. Eval: 0.1524 s/iter. Total: 0.2931 s/iter. ETA=0:01:10
[01/18 11:26:17] d2.evaluation.evaluator INFO: Inference done 870/1093. Dataloading: 0.0076 s/iter. Inference: 0.1329 s/iter. Eval: 0.1527 s/iter. Total: 0.2933 s/iter. ETA=0:01:05
[01/18 11:26:22] d2.evaluation.evaluator INFO: Inference done 887/1093. Dataloading: 0.0076 s/iter. Inference: 0.1328 s/iter. Eval: 0.1531 s/iter. Total: 0.2936 s/iter. ETA=0:01:00
[01/18 11:26:27] d2.evaluation.evaluator INFO: Inference done 905/1093. Dataloading: 0.0076 s/iter. Inference: 0.1329 s/iter. Eval: 0.1529 s/iter. Total: 0.2935 s/iter. ETA=0:00:55
[01/18 11:26:32] d2.evaluation.evaluator INFO: Inference done 923/1093. Dataloading: 0.0076 s/iter. Inference: 0.1327 s/iter. Eval: 0.1529 s/iter. Total: 0.2933 s/iter. ETA=0:00:49
[01/18 11:26:37] d2.evaluation.evaluator INFO: Inference done 941/1093. Dataloading: 0.0076 s/iter. Inference: 0.1327 s/iter. Eval: 0.1530 s/iter. Total: 0.2933 s/iter. ETA=0:00:44
[01/18 11:26:43] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0076 s/iter. Inference: 0.1325 s/iter. Eval: 0.1533 s/iter. Total: 0.2935 s/iter. ETA=0:00:39
[01/18 11:26:48] d2.evaluation.evaluator INFO: Inference done 976/1093. Dataloading: 0.0076 s/iter. Inference: 0.1325 s/iter. Eval: 0.1531 s/iter. Total: 0.2933 s/iter. ETA=0:00:34
[01/18 11:26:53] d2.evaluation.evaluator INFO: Inference done 996/1093. Dataloading: 0.0076 s/iter. Inference: 0.1324 s/iter. Eval: 0.1526 s/iter. Total: 0.2926 s/iter. ETA=0:00:28
[01/18 11:26:58] d2.evaluation.evaluator INFO: Inference done 1015/1093. Dataloading: 0.0076 s/iter. Inference: 0.1322 s/iter. Eval: 0.1524 s/iter. Total: 0.2922 s/iter. ETA=0:00:22
[01/18 11:27:03] d2.evaluation.evaluator INFO: Inference done 1034/1093. Dataloading: 0.0076 s/iter. Inference: 0.1320 s/iter. Eval: 0.1522 s/iter. Total: 0.2919 s/iter. ETA=0:00:17
[01/18 11:27:08] d2.evaluation.evaluator INFO: Inference done 1053/1093. Dataloading: 0.0075 s/iter. Inference: 0.1318 s/iter. Eval: 0.1520 s/iter. Total: 0.2914 s/iter. ETA=0:00:11
[01/18 11:27:13] d2.evaluation.evaluator INFO: Inference done 1074/1093. Dataloading: 0.0075 s/iter. Inference: 0.1314 s/iter. Eval: 0.1514 s/iter. Total: 0.2904 s/iter. ETA=0:00:05
[01/18 11:27:18] d2.evaluation.evaluator INFO: Total inference time: 0:05:15.505864 (0.289987 s / iter per device, on 4 devices)
[01/18 11:27:18] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:22 (0.131157 s / iter per device, on 4 devices)
[01/18 11:27:37] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.3293042556427177, 'mIoU': 20.87735625453845, 'fwIoU': 44.09069280533125, 'IoU-0': nan, 'IoU-1': 95.50952245371491, 'IoU-2': 45.245821650981824, 'IoU-3': 58.51932650113947, 'IoU-4': 52.61922317340475, 'IoU-5': 46.75592904881419, 'IoU-6': 41.68707191161797, 'IoU-7': 33.651036224201924, 'IoU-8': 23.756233233322337, 'IoU-9': 34.90201258451206, 'IoU-10': 40.42251859006451, 'IoU-11': 50.07146828582986, 'IoU-12': 51.899383760853446, 'IoU-13': 51.61174720247333, 'IoU-14': 52.24172449805574, 'IoU-15': 51.80085963289355, 'IoU-16': 52.585145592566626, 'IoU-17': 47.661934255437934, 'IoU-18': 48.935400712915886, 'IoU-19': 49.35608780518973, 'IoU-20': 49.38992755667884, 'IoU-21': 48.097494252048016, 'IoU-22': 50.189019278599375, 'IoU-23': 47.864533056000184, 'IoU-24': 47.171374927473195, 'IoU-25': 47.611773093922075, 'IoU-26': 45.92136363310488, 'IoU-27': 48.18410440161629, 'IoU-28': 45.888151538068996, 'IoU-29': 47.60905636726065, 'IoU-30': 46.28365587875619, 'IoU-31': 47.07863687776835, 'IoU-32': 46.25562760465323, 'IoU-33': 44.47141475049433, 'IoU-34': 43.82545791162259, 'IoU-35': 45.86799250093555, 'IoU-36': 45.68448856276486, 'IoU-37': 43.957433778367026, 'IoU-38': 44.005204007926196, 'IoU-39': 43.776313177333726, 'IoU-40': 44.18726404087681, 'IoU-41': 41.52434518443138, 'IoU-42': 41.37946890137596, 'IoU-43': 40.79224138751781, 'IoU-44': 40.2218868579918, 'IoU-45': 39.03138876451925, 'IoU-46': 38.348648338854915, 'IoU-47': 37.94995085669209, 'IoU-48': 38.64010202730337, 'IoU-49': 37.29020862274173, 'IoU-50': 37.33763071478273, 'IoU-51': 36.07979036435992, 'IoU-52': 35.14846776584498, 'IoU-53': 34.98657400797826, 'IoU-54': 34.43192473483375, 'IoU-55': 33.60301399252742, 'IoU-56': 32.09771594111698, 'IoU-57': 31.09507358370145, 'IoU-58': 29.12084703722399, 'IoU-59': 27.81450819873697, 'IoU-60': 26.892674529292528, 'IoU-61': 26.445493603040553, 'IoU-62': 26.42898928573413, 'IoU-63': 25.773763363756117, 'IoU-64': 25.505785295605737, 'IoU-65': 24.151132475983847, 'IoU-66': 22.7629118649399, 'IoU-67': 22.74897217417637, 'IoU-68': 21.972295838746987, 'IoU-69': 22.043799701937093, 'IoU-70': 21.830623224308262, 'IoU-71': 19.558776185886895, 'IoU-72': 20.418844661944156, 'IoU-73': 20.392860129656746, 'IoU-74': 20.721941890182528, 'IoU-75': 20.37530496724177, 'IoU-76': 19.563461899847727, 'IoU-77': 19.944690428031834, 'IoU-78': 19.78376188436262, 'IoU-79': 19.28762962583341, 'IoU-80': 18.5639449562691, 'IoU-81': 19.469015692404056, 'IoU-82': 18.77924179202079, 'IoU-83': 18.981075187235053, 'IoU-84': 17.66442872065788, 'IoU-85': 18.944469965810857, 'IoU-86': 18.180664015964044, 'IoU-87': 17.012664517148348, 'IoU-88': 17.681296847947927, 'IoU-89': 17.50340057314912, 'IoU-90': 17.06041851624123, 'IoU-91': 17.163611755290606, 'IoU-92': 16.504733042511578, 'IoU-93': 16.9718853887937, 'IoU-94': 16.74106774368668, 'IoU-95': 17.184535807624684, 'IoU-96': 17.032605349084236, 'IoU-97': 17.196804142936006, 'IoU-98': 16.47851843887754, 'IoU-99': 15.91865015818624, 'IoU-100': 15.718249893291894, 'IoU-101': 14.759436180667116, 'IoU-102': 14.617671694166361, 'IoU-103': 14.122621604048408, 'IoU-104': 13.076290003230515, 'IoU-105': 13.086674572578657, 'IoU-106': 13.207255162881165, 'IoU-107': 13.79775049132799, 'IoU-108': 12.820791086492555, 'IoU-109': 13.647782012260103, 'IoU-110': 12.628233558246501, 'IoU-111': 12.222227630782134, 'IoU-112': 12.23724989609927, 'IoU-113': 12.017472982206819, 'IoU-114': 12.627853117613533, 'IoU-115': 11.992459609082774, 'IoU-116': 11.083290542765646, 'IoU-117': 11.52201734129391, 'IoU-118': 11.877438577629677, 'IoU-119': 12.42168948274274, 'IoU-120': 10.643190788898206, 'IoU-121': 10.479656100400653, 'IoU-122': 10.622244248556829, 'IoU-123': 9.989836171886473, 'IoU-124': 10.581797769568368, 'IoU-125': 9.921317578047887, 'IoU-126': 9.371760948331456, 'IoU-127': 9.252452680978585, 'IoU-128': 8.710966413040817, 'IoU-129': 7.876067075109074, 'IoU-130': 8.199595401468878, 'IoU-131': 7.550935163257315, 'IoU-132': 7.223041715485051, 'IoU-133': 7.952036339401902, 'IoU-134': 6.971548094551312, 'IoU-135': 7.138232704432129, 'IoU-136': 6.272624536124058, 'IoU-137': 6.779144745819567, 'IoU-138': 7.733430010552269, 'IoU-139': 6.1369865354749535, 'IoU-140': 5.78660427014905, 'IoU-141': 6.189663357436986, 'IoU-142': 6.492050519837029, 'IoU-143': 5.143204814149203, 'IoU-144': 6.882529562393549, 'IoU-145': 6.2085727026686515, 'IoU-146': 4.729009548486931, 'IoU-147': 5.618415245174372, 'IoU-148': 6.093201861997432, 'IoU-149': 4.156697451556111, 'IoU-150': 4.687854855786608, 'IoU-151': 4.114301952921489, 'IoU-152': 3.869363024359356, 'IoU-153': 4.559925507648211, 'IoU-154': 3.4037062435870333, 'IoU-155': 3.4819468292365303, 'IoU-156': 4.3110153167616305, 'IoU-157': 3.0180808463867232, 'IoU-158': 4.909465174594858, 'IoU-159': 2.7739713374358046, 'IoU-160': 3.821995122346579, 'IoU-161': 2.6946200931780884, 'IoU-162': 3.3849073249494386, 'IoU-163': 2.980381478455933, 'IoU-164': 1.9945038338117111, 'IoU-165': 3.1303940229870495, 'IoU-166': 2.666190116006838, 'IoU-167': 3.1569775163339413, 'IoU-168': 2.397606994937874, 'IoU-169': 2.4223086916138747, 'IoU-170': 2.173225668641581, 'IoU-171': 1.7414915134992524, 'IoU-172': 2.263690691745052, 'IoU-173': 1.9820826053306853, 'IoU-174': 2.954424329946533, 'IoU-175': 1.642758634688053, 'IoU-176': 2.0929601939114892, 'IoU-177': 2.051785565033217, 'IoU-178': 1.2277959507510958, 'IoU-179': 0.9194619444917418, 'IoU-180': 2.260595763031258, 'IoU-181': 2.244933104214701, 'IoU-182': 1.0317402760654628, 'IoU-183': 2.8358738892364324, 'IoU-184': 1.6011664912423489, 'IoU-185': 2.9587485326919216, 'IoU-186': 2.4033123979001125, 'IoU-187': 2.590445032281318, 'IoU-188': 1.694773210471338, 'IoU-189': 1.3497694724813658, 'IoU-190': 3.529503690495233, 'IoU-191': 2.970452341566368, 'mACC': 31.18373751140832, 'pACC': 58.47224277145645, 'ACC-0': nan, 'ACC-1': 98.78320985788442, 'ACC-2': 54.93405871702935, 'ACC-3': 72.89584541687945, 'ACC-4': 70.24151946329604, 'ACC-5': 65.41585719403513, 'ACC-6': 59.6617670138332, 'ACC-7': 48.49572107303164, 'ACC-8': 30.2272346199672, 'ACC-9': 44.27476082547719, 'ACC-10': 55.52067812840152, 'ACC-11': 65.83845031666411, 'ACC-12': 69.93123994344862, 'ACC-13': 69.56345475847625, 'ACC-14': 70.45149350634058, 'ACC-15': 68.16880723843336, 'ACC-16': 69.65951999022064, 'ACC-17': 66.07452168932915, 'ACC-18': 66.19957673401086, 'ACC-19': 66.19054391818203, 'ACC-20': 65.26920810654222, 'ACC-21': 66.38569255924149, 'ACC-22': 66.45141435230957, 'ACC-23': 64.31432182280516, 'ACC-24': 65.71183215383707, 'ACC-25': 64.1794658620193, 'ACC-26': 63.499354175583655, 'ACC-27': 64.00088306270506, 'ACC-28': 63.85003395334879, 'ACC-29': 63.09937468523364, 'ACC-30': 63.92354099609482, 'ACC-31': 63.67355555683609, 'ACC-32': 62.85799520250441, 'ACC-33': 62.02058669101479, 'ACC-34': 62.01550113021995, 'ACC-35': 62.50919584145994, 'ACC-36': 62.40928079449264, 'ACC-37': 61.387109989773705, 'ACC-38': 60.75105367862938, 'ACC-39': 60.47062038579386, 'ACC-40': 60.51616831684895, 'ACC-41': 59.08626386253444, 'ACC-42': 58.66348618946853, 'ACC-43': 58.184347554331076, 'ACC-44': 57.155755846736376, 'ACC-45': 56.48497600936202, 'ACC-46': 55.25068878459097, 'ACC-47': 55.82655325514683, 'ACC-48': 55.480938823275004, 'ACC-49': 55.14349668467533, 'ACC-50': 54.61180285542836, 'ACC-51': 52.61974949028863, 'ACC-52': 50.792675916189864, 'ACC-53': 51.32848638602042, 'ACC-54': 50.6077681937222, 'ACC-55': 50.65960292457811, 'ACC-56': 49.60027143601323, 'ACC-57': 47.816425347381724, 'ACC-58': 46.04684967889602, 'ACC-59': 43.622452588312775, 'ACC-60': 42.94889403981665, 'ACC-61': 41.92687187486644, 'ACC-62': 40.84617808475573, 'ACC-63': 40.36321489758244, 'ACC-64': 39.51965503590729, 'ACC-65': 38.31418867622177, 'ACC-66': 37.559908275896746, 'ACC-67': 36.195980187665725, 'ACC-68': 36.212893654464196, 'ACC-69': 35.326186026771, 'ACC-70': 34.45291124215738, 'ACC-71': 33.566924875420845, 'ACC-72': 33.96097989481909, 'ACC-73': 32.6823292460915, 'ACC-74': 32.84840659121006, 'ACC-75': 35.510085297229885, 'ACC-76': 30.683802014692287, 'ACC-77': 32.74534919795238, 'ACC-78': 33.04017305219229, 'ACC-79': 31.061747470214573, 'ACC-80': 31.583977009986807, 'ACC-81': 32.88298170560776, 'ACC-82': 30.113395870241806, 'ACC-83': 30.972894061365512, 'ACC-84': 29.763889426044855, 'ACC-85': 30.81065448672459, 'ACC-86': 30.990811621339425, 'ACC-87': 29.575964407955844, 'ACC-88': 29.10203877459861, 'ACC-89': 30.6719334569096, 'ACC-90': 28.467571662904533, 'ACC-91': 29.3908857525349, 'ACC-92': 27.776694695508603, 'ACC-93': 30.206819152460895, 'ACC-94': 29.45092275654359, 'ACC-95': 28.051840122895218, 'ACC-96': 29.969130050963642, 'ACC-97': 29.477253999389468, 'ACC-98': 27.669469899889858, 'ACC-99': 29.032356351053334, 'ACC-100': 27.86954839933925, 'ACC-101': 26.670883692618787, 'ACC-102': 25.11812415043045, 'ACC-103': 24.947270237083217, 'ACC-104': 23.54322323602469, 'ACC-105': 22.69889984832275, 'ACC-106': 22.78050534189127, 'ACC-107': 22.92196998376913, 'ACC-108': 20.634187175808716, 'ACC-109': 22.69029870315798, 'ACC-110': 22.950804370388862, 'ACC-111': 21.246107266300434, 'ACC-112': 22.350569454648827, 'ACC-113': 19.71758553666204, 'ACC-114': 23.141679819620794, 'ACC-115': 21.549095665509, 'ACC-116': 19.867676165831618, 'ACC-117': 20.273569568127993, 'ACC-118': 21.263200146786094, 'ACC-119': 22.50099397950794, 'ACC-120': 18.97307231434726, 'ACC-121': 19.938981489598373, 'ACC-122': 18.81721343183598, 'ACC-123': 16.94977391551486, 'ACC-124': 19.804102176707595, 'ACC-125': 19.13281261047051, 'ACC-126': 17.8181267588958, 'ACC-127': 16.550710687886067, 'ACC-128': 16.062257056120874, 'ACC-129': 13.215575952945576, 'ACC-130': 16.170226213294193, 'ACC-131': 13.455522747527269, 'ACC-132': 14.357953574408622, 'ACC-133': 14.72752386590187, 'ACC-134': 12.632485997414907, 'ACC-135': 12.96412690268514, 'ACC-136': 11.931752003354454, 'ACC-137': 12.247207905255388, 'ACC-138': 14.771270655126026, 'ACC-139': 11.776318648119743, 'ACC-140': 10.638791294164028, 'ACC-141': 10.50057309821439, 'ACC-142': 11.363499411821728, 'ACC-143': 9.859702943695169, 'ACC-144': 13.242328519855596, 'ACC-145': 10.622777790201035, 'ACC-146': 8.579379040917502, 'ACC-147': 9.668490588987941, 'ACC-148': 12.03886503601061, 'ACC-149': 6.002622534528517, 'ACC-150': 9.16532990622352, 'ACC-151': 6.9436406713113215, 'ACC-152': 6.451674084564029, 'ACC-153': 8.030332179043222, 'ACC-154': 6.200780351945068, 'ACC-155': 5.866199283641841, 'ACC-156': 8.269050527771853, 'ACC-157': 5.112605531533224, 'ACC-158': 9.256637842582961, 'ACC-159': 4.789799453906722, 'ACC-160': 5.867133420802606, 'ACC-161': 3.8918548516191724, 'ACC-162': 5.445238187330594, 'ACC-163': 4.91908029601004, 'ACC-164': 2.9504469509352833, 'ACC-165': 4.7092530183243495, 'ACC-166': 4.846597443150817, 'ACC-167': 5.07897795079447, 'ACC-168': 4.946550390322808, 'ACC-169': 4.132354670087192, 'ACC-170': 5.4887701066201, 'ACC-171': 2.9099445453693824, 'ACC-172': 4.561432751787236, 'ACC-173': 4.153480863809627, 'ACC-174': 5.125251406853178, 'ACC-175': 2.693393531640164, 'ACC-176': 4.1161191084281405, 'ACC-177': 4.122583133561801, 'ACC-178': 1.7994595440313226, 'ACC-179': 1.1942030632140415, 'ACC-180': 5.37014460361001, 'ACC-181': 4.291997981908233, 'ACC-182': 1.5167417298247527, 'ACC-183': 5.528415471652803, 'ACC-184': 2.7373902461457913, 'ACC-185': 6.880950071366213, 'ACC-186': 5.723384315401085, 'ACC-187': 4.966235397866608, 'ACC-188': 4.284280295387596, 'ACC-189': 4.970206997825534, 'ACC-190': 12.545196348659044, 'ACC-191': 8.155301794453507})])
[01/18 11:27:38] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 11:27:38] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 11:27:38] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 11:27:38] d2.evaluation.testing INFO: copypaste: 2.3293,20.8774,44.0907,31.1837,58.4722
[01/18 11:27:38] d2.utils.events INFO:  eta: 1:29:14  iter: 35999  total_loss: 31.81  loss_ce: 0.2118  loss_mask: 0.3213  loss_dice: 2.561  loss_ce_0: 0.5399  loss_mask_0: 0.3243  loss_dice_0: 2.699  loss_ce_1: 0.2538  loss_mask_1: 0.3279  loss_dice_1: 2.606  loss_ce_2: 0.2394  loss_mask_2: 0.3242  loss_dice_2: 2.59  loss_ce_3: 0.2428  loss_mask_3: 0.3224  loss_dice_3: 2.571  loss_ce_4: 0.2225  loss_mask_4: 0.3211  loss_dice_4: 2.571  loss_ce_5: 0.2282  loss_mask_5: 0.3203  loss_dice_5: 2.572  loss_ce_6: 0.2165  loss_mask_6: 0.321  loss_dice_6: 2.567  loss_ce_7: 0.2121  loss_mask_7: 0.3203  loss_dice_7: 2.563  loss_ce_8: 0.2202  loss_mask_8: 0.3202  loss_dice_8: 2.573  time: 1.4733  data_time: 0.0565  lr: 1.2592e-06  max_mem: 21589M
[01/18 11:28:04] d2.utils.events INFO:  eta: 1:28:40  iter: 36019  total_loss: 31.36  loss_ce: 0.2054  loss_mask: 0.3195  loss_dice: 2.515  loss_ce_0: 0.5365  loss_mask_0: 0.3238  loss_dice_0: 2.667  loss_ce_1: 0.2738  loss_mask_1: 0.3245  loss_dice_1: 2.551  loss_ce_2: 0.2533  loss_mask_2: 0.3221  loss_dice_2: 2.528  loss_ce_3: 0.2354  loss_mask_3: 0.3212  loss_dice_3: 2.516  loss_ce_4: 0.2221  loss_mask_4: 0.3206  loss_dice_4: 2.516  loss_ce_5: 0.2327  loss_mask_5: 0.3199  loss_dice_5: 2.512  loss_ce_6: 0.2216  loss_mask_6: 0.3198  loss_dice_6: 2.513  loss_ce_7: 0.2156  loss_mask_7: 0.3194  loss_dice_7: 2.514  loss_ce_8: 0.2066  loss_mask_8: 0.3188  loss_dice_8: 2.512  time: 1.4732  data_time: 0.0525  lr: 1.2535e-06  max_mem: 21589M
[01/18 11:28:31] d2.utils.events INFO:  eta: 1:28:08  iter: 36039  total_loss: 31.85  loss_ce: 0.2173  loss_mask: 0.3172  loss_dice: 2.56  loss_ce_0: 0.5413  loss_mask_0: 0.3164  loss_dice_0: 2.706  loss_ce_1: 0.2784  loss_mask_1: 0.3243  loss_dice_1: 2.602  loss_ce_2: 0.2657  loss_mask_2: 0.3202  loss_dice_2: 2.585  loss_ce_3: 0.2452  loss_mask_3: 0.3194  loss_dice_3: 2.572  loss_ce_4: 0.2305  loss_mask_4: 0.3179  loss_dice_4: 2.565  loss_ce_5: 0.2252  loss_mask_5: 0.3171  loss_dice_5: 2.555  loss_ce_6: 0.2178  loss_mask_6: 0.3185  loss_dice_6: 2.566  loss_ce_7: 0.2134  loss_mask_7: 0.3188  loss_dice_7: 2.562  loss_ce_8: 0.2226  loss_mask_8: 0.3171  loss_dice_8: 2.563  time: 1.4731  data_time: 0.0530  lr: 1.2479e-06  max_mem: 21589M
[01/18 11:28:58] d2.utils.events INFO:  eta: 1:27:37  iter: 36059  total_loss: 31.71  loss_ce: 0.2048  loss_mask: 0.3275  loss_dice: 2.553  loss_ce_0: 0.5197  loss_mask_0: 0.3192  loss_dice_0: 2.671  loss_ce_1: 0.2772  loss_mask_1: 0.3307  loss_dice_1: 2.589  loss_ce_2: 0.2613  loss_mask_2: 0.329  loss_dice_2: 2.567  loss_ce_3: 0.233  loss_mask_3: 0.3286  loss_dice_3: 2.558  loss_ce_4: 0.2383  loss_mask_4: 0.3292  loss_dice_4: 2.549  loss_ce_5: 0.2305  loss_mask_5: 0.3279  loss_dice_5: 2.558  loss_ce_6: 0.2233  loss_mask_6: 0.3269  loss_dice_6: 2.544  loss_ce_7: 0.2211  loss_mask_7: 0.3268  loss_dice_7: 2.545  loss_ce_8: 0.2073  loss_mask_8: 0.3271  loss_dice_8: 2.545  time: 1.4730  data_time: 0.0530  lr: 1.2422e-06  max_mem: 21589M
[01/18 11:29:24] d2.utils.events INFO:  eta: 1:27:07  iter: 36079  total_loss: 31.05  loss_ce: 0.218  loss_mask: 0.3195  loss_dice: 2.488  loss_ce_0: 0.5914  loss_mask_0: 0.3246  loss_dice_0: 2.633  loss_ce_1: 0.288  loss_mask_1: 0.3265  loss_dice_1: 2.545  loss_ce_2: 0.2652  loss_mask_2: 0.3228  loss_dice_2: 2.512  loss_ce_3: 0.2552  loss_mask_3: 0.3211  loss_dice_3: 2.502  loss_ce_4: 0.2423  loss_mask_4: 0.3205  loss_dice_4: 2.494  loss_ce_5: 0.2316  loss_mask_5: 0.3184  loss_dice_5: 2.504  loss_ce_6: 0.2226  loss_mask_6: 0.3185  loss_dice_6: 2.488  loss_ce_7: 0.2221  loss_mask_7: 0.3187  loss_dice_7: 2.489  loss_ce_8: 0.2195  loss_mask_8: 0.3198  loss_dice_8: 2.494  time: 1.4730  data_time: 0.0526  lr: 1.2365e-06  max_mem: 21589M
[01/18 11:29:51] d2.utils.events INFO:  eta: 1:26:37  iter: 36099  total_loss: 31.43  loss_ce: 0.2144  loss_mask: 0.3235  loss_dice: 2.518  loss_ce_0: 0.5407  loss_mask_0: 0.3212  loss_dice_0: 2.672  loss_ce_1: 0.2573  loss_mask_1: 0.3293  loss_dice_1: 2.568  loss_ce_2: 0.2521  loss_mask_2: 0.3252  loss_dice_2: 2.537  loss_ce_3: 0.2153  loss_mask_3: 0.3244  loss_dice_3: 2.526  loss_ce_4: 0.2272  loss_mask_4: 0.3228  loss_dice_4: 2.515  loss_ce_5: 0.2182  loss_mask_5: 0.3225  loss_dice_5: 2.524  loss_ce_6: 0.2225  loss_mask_6: 0.3222  loss_dice_6: 2.518  loss_ce_7: 0.2108  loss_mask_7: 0.3227  loss_dice_7: 2.509  loss_ce_8: 0.2185  loss_mask_8: 0.3233  loss_dice_8: 2.506  time: 1.4729  data_time: 0.0548  lr: 1.2308e-06  max_mem: 21589M
[01/18 11:30:18] d2.utils.events INFO:  eta: 1:26:06  iter: 36119  total_loss: 31.79  loss_ce: 0.219  loss_mask: 0.3253  loss_dice: 2.537  loss_ce_0: 0.5699  loss_mask_0: 0.3235  loss_dice_0: 2.691  loss_ce_1: 0.272  loss_mask_1: 0.3266  loss_dice_1: 2.604  loss_ce_2: 0.2506  loss_mask_2: 0.3274  loss_dice_2: 2.563  loss_ce_3: 0.2488  loss_mask_3: 0.3255  loss_dice_3: 2.551  loss_ce_4: 0.2179  loss_mask_4: 0.3251  loss_dice_4: 2.551  loss_ce_5: 0.2096  loss_mask_5: 0.3253  loss_dice_5: 2.551  loss_ce_6: 0.218  loss_mask_6: 0.3253  loss_dice_6: 2.54  loss_ce_7: 0.2108  loss_mask_7: 0.3244  loss_dice_7: 2.543  loss_ce_8: 0.2163  loss_mask_8: 0.3242  loss_dice_8: 2.538  time: 1.4728  data_time: 0.0531  lr: 1.2252e-06  max_mem: 21589M
[01/18 11:30:44] d2.utils.events INFO:  eta: 1:25:34  iter: 36139  total_loss: 31.48  loss_ce: 0.2166  loss_mask: 0.3248  loss_dice: 2.522  loss_ce_0: 0.5625  loss_mask_0: 0.3226  loss_dice_0: 2.672  loss_ce_1: 0.269  loss_mask_1: 0.3282  loss_dice_1: 2.58  loss_ce_2: 0.2729  loss_mask_2: 0.326  loss_dice_2: 2.552  loss_ce_3: 0.2373  loss_mask_3: 0.3234  loss_dice_3: 2.53  loss_ce_4: 0.2386  loss_mask_4: 0.3229  loss_dice_4: 2.539  loss_ce_5: 0.2349  loss_mask_5: 0.3237  loss_dice_5: 2.526  loss_ce_6: 0.2237  loss_mask_6: 0.3235  loss_dice_6: 2.518  loss_ce_7: 0.2241  loss_mask_7: 0.324  loss_dice_7: 2.533  loss_ce_8: 0.2255  loss_mask_8: 0.3237  loss_dice_8: 2.528  time: 1.4727  data_time: 0.0540  lr: 1.2195e-06  max_mem: 21589M
[01/18 11:31:11] d2.utils.events INFO:  eta: 1:25:05  iter: 36159  total_loss: 32.27  loss_ce: 0.2275  loss_mask: 0.3288  loss_dice: 2.603  loss_ce_0: 0.5694  loss_mask_0: 0.3326  loss_dice_0: 2.726  loss_ce_1: 0.3055  loss_mask_1: 0.3379  loss_dice_1: 2.649  loss_ce_2: 0.2728  loss_mask_2: 0.3361  loss_dice_2: 2.623  loss_ce_3: 0.2634  loss_mask_3: 0.3319  loss_dice_3: 2.607  loss_ce_4: 0.2492  loss_mask_4: 0.3299  loss_dice_4: 2.612  loss_ce_5: 0.2361  loss_mask_5: 0.329  loss_dice_5: 2.603  loss_ce_6: 0.2295  loss_mask_6: 0.3295  loss_dice_6: 2.6  loss_ce_7: 0.2316  loss_mask_7: 0.3294  loss_dice_7: 2.604  loss_ce_8: 0.2431  loss_mask_8: 0.3297  loss_dice_8: 2.611  time: 1.4727  data_time: 0.0541  lr: 1.2138e-06  max_mem: 21589M
[01/18 11:31:38] d2.utils.events INFO:  eta: 1:24:35  iter: 36179  total_loss: 32.09  loss_ce: 0.2148  loss_mask: 0.3257  loss_dice: 2.565  loss_ce_0: 0.5587  loss_mask_0: 0.3232  loss_dice_0: 2.714  loss_ce_1: 0.27  loss_mask_1: 0.3314  loss_dice_1: 2.619  loss_ce_2: 0.2469  loss_mask_2: 0.3304  loss_dice_2: 2.591  loss_ce_3: 0.2399  loss_mask_3: 0.3282  loss_dice_3: 2.58  loss_ce_4: 0.2268  loss_mask_4: 0.3279  loss_dice_4: 2.572  loss_ce_5: 0.2256  loss_mask_5: 0.3253  loss_dice_5: 2.575  loss_ce_6: 0.2178  loss_mask_6: 0.3258  loss_dice_6: 2.566  loss_ce_7: 0.2087  loss_mask_7: 0.3249  loss_dice_7: 2.57  loss_ce_8: 0.2115  loss_mask_8: 0.3247  loss_dice_8: 2.569  time: 1.4726  data_time: 0.0551  lr: 1.2081e-06  max_mem: 21589M
[01/18 11:32:05] d2.utils.events INFO:  eta: 1:24:07  iter: 36199  total_loss: 30.92  loss_ce: 0.1972  loss_mask: 0.318  loss_dice: 2.496  loss_ce_0: 0.5136  loss_mask_0: 0.3232  loss_dice_0: 2.62  loss_ce_1: 0.2586  loss_mask_1: 0.3251  loss_dice_1: 2.521  loss_ce_2: 0.2515  loss_mask_2: 0.3222  loss_dice_2: 2.514  loss_ce_3: 0.2447  loss_mask_3: 0.321  loss_dice_3: 2.497  loss_ce_4: 0.2147  loss_mask_4: 0.3193  loss_dice_4: 2.496  loss_ce_5: 0.2147  loss_mask_5: 0.3181  loss_dice_5: 2.505  loss_ce_6: 0.211  loss_mask_6: 0.3182  loss_dice_6: 2.499  loss_ce_7: 0.2028  loss_mask_7: 0.3173  loss_dice_7: 2.494  loss_ce_8: 0.2086  loss_mask_8: 0.3166  loss_dice_8: 2.494  time: 1.4725  data_time: 0.0546  lr: 1.2024e-06  max_mem: 21589M
[01/18 11:32:32] d2.utils.events INFO:  eta: 1:23:36  iter: 36219  total_loss: 31.55  loss_ce: 0.2212  loss_mask: 0.3335  loss_dice: 2.522  loss_ce_0: 0.5551  loss_mask_0: 0.329  loss_dice_0: 2.633  loss_ce_1: 0.2812  loss_mask_1: 0.3375  loss_dice_1: 2.56  loss_ce_2: 0.2592  loss_mask_2: 0.3354  loss_dice_2: 2.54  loss_ce_3: 0.2588  loss_mask_3: 0.3315  loss_dice_3: 2.525  loss_ce_4: 0.2374  loss_mask_4: 0.332  loss_dice_4: 2.52  loss_ce_5: 0.2371  loss_mask_5: 0.3316  loss_dice_5: 2.528  loss_ce_6: 0.2283  loss_mask_6: 0.3328  loss_dice_6: 2.522  loss_ce_7: 0.227  loss_mask_7: 0.3336  loss_dice_7: 2.513  loss_ce_8: 0.2231  loss_mask_8: 0.3322  loss_dice_8: 2.521  time: 1.4724  data_time: 0.0539  lr: 1.1967e-06  max_mem: 21589M
[01/18 11:32:59] d2.utils.events INFO:  eta: 1:23:07  iter: 36239  total_loss: 31.75  loss_ce: 0.2126  loss_mask: 0.3216  loss_dice: 2.54  loss_ce_0: 0.5577  loss_mask_0: 0.3254  loss_dice_0: 2.679  loss_ce_1: 0.2646  loss_mask_1: 0.3297  loss_dice_1: 2.579  loss_ce_2: 0.2589  loss_mask_2: 0.325  loss_dice_2: 2.563  loss_ce_3: 0.2357  loss_mask_3: 0.3234  loss_dice_3: 2.544  loss_ce_4: 0.2293  loss_mask_4: 0.3228  loss_dice_4: 2.535  loss_ce_5: 0.2264  loss_mask_5: 0.3204  loss_dice_5: 2.539  loss_ce_6: 0.2231  loss_mask_6: 0.3209  loss_dice_6: 2.536  loss_ce_7: 0.213  loss_mask_7: 0.3204  loss_dice_7: 2.535  loss_ce_8: 0.2236  loss_mask_8: 0.3208  loss_dice_8: 2.534  time: 1.4724  data_time: 0.0531  lr: 1.191e-06  max_mem: 21589M
[01/18 11:33:25] d2.utils.events INFO:  eta: 1:22:36  iter: 36259  total_loss: 31.52  loss_ce: 0.2079  loss_mask: 0.3266  loss_dice: 2.54  loss_ce_0: 0.563  loss_mask_0: 0.3242  loss_dice_0: 2.673  loss_ce_1: 0.264  loss_mask_1: 0.3286  loss_dice_1: 2.584  loss_ce_2: 0.2587  loss_mask_2: 0.3276  loss_dice_2: 2.553  loss_ce_3: 0.2484  loss_mask_3: 0.3272  loss_dice_3: 2.553  loss_ce_4: 0.2227  loss_mask_4: 0.3268  loss_dice_4: 2.552  loss_ce_5: 0.2281  loss_mask_5: 0.3251  loss_dice_5: 2.544  loss_ce_6: 0.2288  loss_mask_6: 0.3259  loss_dice_6: 2.543  loss_ce_7: 0.2115  loss_mask_7: 0.3257  loss_dice_7: 2.547  loss_ce_8: 0.2195  loss_mask_8: 0.3264  loss_dice_8: 2.544  time: 1.4723  data_time: 0.0525  lr: 1.1853e-06  max_mem: 21589M
[01/18 11:33:52] d2.utils.events INFO:  eta: 1:22:03  iter: 36279  total_loss: 30.58  loss_ce: 0.2116  loss_mask: 0.3282  loss_dice: 2.461  loss_ce_0: 0.5296  loss_mask_0: 0.3244  loss_dice_0: 2.601  loss_ce_1: 0.2484  loss_mask_1: 0.3332  loss_dice_1: 2.519  loss_ce_2: 0.2322  loss_mask_2: 0.3301  loss_dice_2: 2.494  loss_ce_3: 0.2222  loss_mask_3: 0.3294  loss_dice_3: 2.482  loss_ce_4: 0.2133  loss_mask_4: 0.3295  loss_dice_4: 2.486  loss_ce_5: 0.2049  loss_mask_5: 0.3301  loss_dice_5: 2.479  loss_ce_6: 0.2117  loss_mask_6: 0.331  loss_dice_6: 2.471  loss_ce_7: 0.2144  loss_mask_7: 0.3293  loss_dice_7: 2.472  loss_ce_8: 0.2033  loss_mask_8: 0.3293  loss_dice_8: 2.472  time: 1.4722  data_time: 0.0491  lr: 1.1796e-06  max_mem: 21589M
[01/18 11:34:19] d2.utils.events INFO:  eta: 1:21:39  iter: 36299  total_loss: 31.17  loss_ce: 0.2074  loss_mask: 0.332  loss_dice: 2.519  loss_ce_0: 0.5293  loss_mask_0: 0.3295  loss_dice_0: 2.652  loss_ce_1: 0.2748  loss_mask_1: 0.3341  loss_dice_1: 2.561  loss_ce_2: 0.2553  loss_mask_2: 0.3337  loss_dice_2: 2.54  loss_ce_3: 0.2248  loss_mask_3: 0.3308  loss_dice_3: 2.518  loss_ce_4: 0.2044  loss_mask_4: 0.33  loss_dice_4: 2.522  loss_ce_5: 0.2233  loss_mask_5: 0.3316  loss_dice_5: 2.521  loss_ce_6: 0.2078  loss_mask_6: 0.3318  loss_dice_6: 2.51  loss_ce_7: 0.2024  loss_mask_7: 0.3316  loss_dice_7: 2.523  loss_ce_8: 0.2025  loss_mask_8: 0.3309  loss_dice_8: 2.516  time: 1.4721  data_time: 0.0505  lr: 1.1739e-06  max_mem: 21589M
[01/18 11:34:45] d2.utils.events INFO:  eta: 1:21:13  iter: 36319  total_loss: 31.51  loss_ce: 0.2219  loss_mask: 0.3291  loss_dice: 2.546  loss_ce_0: 0.5331  loss_mask_0: 0.3304  loss_dice_0: 2.689  loss_ce_1: 0.2579  loss_mask_1: 0.3336  loss_dice_1: 2.585  loss_ce_2: 0.2459  loss_mask_2: 0.3308  loss_dice_2: 2.571  loss_ce_3: 0.2366  loss_mask_3: 0.3298  loss_dice_3: 2.555  loss_ce_4: 0.2108  loss_mask_4: 0.3286  loss_dice_4: 2.555  loss_ce_5: 0.2015  loss_mask_5: 0.328  loss_dice_5: 2.55  loss_ce_6: 0.2212  loss_mask_6: 0.3274  loss_dice_6: 2.551  loss_ce_7: 0.2073  loss_mask_7: 0.3277  loss_dice_7: 2.542  loss_ce_8: 0.2028  loss_mask_8: 0.3287  loss_dice_8: 2.552  time: 1.4720  data_time: 0.0539  lr: 1.1682e-06  max_mem: 21589M
[01/18 11:35:12] d2.utils.events INFO:  eta: 1:20:47  iter: 36339  total_loss: 31.37  loss_ce: 0.2378  loss_mask: 0.3289  loss_dice: 2.495  loss_ce_0: 0.5707  loss_mask_0: 0.3265  loss_dice_0: 2.647  loss_ce_1: 0.2773  loss_mask_1: 0.3309  loss_dice_1: 2.544  loss_ce_2: 0.2757  loss_mask_2: 0.3277  loss_dice_2: 2.527  loss_ce_3: 0.2645  loss_mask_3: 0.3293  loss_dice_3: 2.506  loss_ce_4: 0.2349  loss_mask_4: 0.3277  loss_dice_4: 2.506  loss_ce_5: 0.2465  loss_mask_5: 0.3272  loss_dice_5: 2.502  loss_ce_6: 0.2469  loss_mask_6: 0.3269  loss_dice_6: 2.496  loss_ce_7: 0.2355  loss_mask_7: 0.3281  loss_dice_7: 2.506  loss_ce_8: 0.2248  loss_mask_8: 0.3286  loss_dice_8: 2.504  time: 1.4720  data_time: 0.0525  lr: 1.1625e-06  max_mem: 21589M
[01/18 11:35:39] d2.utils.events INFO:  eta: 1:20:20  iter: 36359  total_loss: 32.02  loss_ce: 0.2203  loss_mask: 0.3174  loss_dice: 2.557  loss_ce_0: 0.5704  loss_mask_0: 0.3159  loss_dice_0: 2.693  loss_ce_1: 0.2742  loss_mask_1: 0.3235  loss_dice_1: 2.616  loss_ce_2: 0.2686  loss_mask_2: 0.3198  loss_dice_2: 2.594  loss_ce_3: 0.2498  loss_mask_3: 0.3173  loss_dice_3: 2.569  loss_ce_4: 0.2392  loss_mask_4: 0.3168  loss_dice_4: 2.562  loss_ce_5: 0.222  loss_mask_5: 0.3178  loss_dice_5: 2.562  loss_ce_6: 0.2292  loss_mask_6: 0.3172  loss_dice_6: 2.562  loss_ce_7: 0.2277  loss_mask_7: 0.317  loss_dice_7: 2.564  loss_ce_8: 0.2268  loss_mask_8: 0.3169  loss_dice_8: 2.558  time: 1.4719  data_time: 0.0533  lr: 1.1568e-06  max_mem: 21589M
[01/18 11:36:05] d2.utils.events INFO:  eta: 1:19:54  iter: 36379  total_loss: 31.66  loss_ce: 0.202  loss_mask: 0.3258  loss_dice: 2.55  loss_ce_0: 0.5328  loss_mask_0: 0.3265  loss_dice_0: 2.684  loss_ce_1: 0.2556  loss_mask_1: 0.3361  loss_dice_1: 2.595  loss_ce_2: 0.2483  loss_mask_2: 0.3307  loss_dice_2: 2.57  loss_ce_3: 0.2289  loss_mask_3: 0.328  loss_dice_3: 2.56  loss_ce_4: 0.2157  loss_mask_4: 0.3275  loss_dice_4: 2.554  loss_ce_5: 0.2144  loss_mask_5: 0.3262  loss_dice_5: 2.557  loss_ce_6: 0.2209  loss_mask_6: 0.3252  loss_dice_6: 2.55  loss_ce_7: 0.1976  loss_mask_7: 0.3259  loss_dice_7: 2.551  loss_ce_8: 0.2021  loss_mask_8: 0.3239  loss_dice_8: 2.548  time: 1.4718  data_time: 0.0504  lr: 1.151e-06  max_mem: 21589M
[01/18 11:36:31] d2.utils.events INFO:  eta: 1:19:27  iter: 36399  total_loss: 30.66  loss_ce: 0.2249  loss_mask: 0.3209  loss_dice: 2.461  loss_ce_0: 0.5604  loss_mask_0: 0.3162  loss_dice_0: 2.6  loss_ce_1: 0.2794  loss_mask_1: 0.3261  loss_dice_1: 2.512  loss_ce_2: 0.2568  loss_mask_2: 0.3226  loss_dice_2: 2.491  loss_ce_3: 0.2317  loss_mask_3: 0.3218  loss_dice_3: 2.47  loss_ce_4: 0.2376  loss_mask_4: 0.3214  loss_dice_4: 2.473  loss_ce_5: 0.2284  loss_mask_5: 0.3211  loss_dice_5: 2.475  loss_ce_6: 0.2205  loss_mask_6: 0.3214  loss_dice_6: 2.471  loss_ce_7: 0.2144  loss_mask_7: 0.321  loss_dice_7: 2.467  loss_ce_8: 0.2173  loss_mask_8: 0.3204  loss_dice_8: 2.466  time: 1.4717  data_time: 0.0535  lr: 1.1453e-06  max_mem: 21589M
[01/18 11:36:58] d2.utils.events INFO:  eta: 1:19:01  iter: 36419  total_loss: 31.14  loss_ce: 0.1983  loss_mask: 0.3167  loss_dice: 2.541  loss_ce_0: 0.5492  loss_mask_0: 0.3139  loss_dice_0: 2.681  loss_ce_1: 0.2388  loss_mask_1: 0.3192  loss_dice_1: 2.572  loss_ce_2: 0.2327  loss_mask_2: 0.3165  loss_dice_2: 2.562  loss_ce_3: 0.2114  loss_mask_3: 0.317  loss_dice_3: 2.551  loss_ce_4: 0.2034  loss_mask_4: 0.3146  loss_dice_4: 2.536  loss_ce_5: 0.2066  loss_mask_5: 0.3153  loss_dice_5: 2.54  loss_ce_6: 0.1948  loss_mask_6: 0.3153  loss_dice_6: 2.547  loss_ce_7: 0.1955  loss_mask_7: 0.3151  loss_dice_7: 2.534  loss_ce_8: 0.1923  loss_mask_8: 0.3156  loss_dice_8: 2.538  time: 1.4716  data_time: 0.0554  lr: 1.1396e-06  max_mem: 21589M
[01/18 11:37:25] d2.utils.events INFO:  eta: 1:18:35  iter: 36439  total_loss: 30.88  loss_ce: 0.206  loss_mask: 0.3279  loss_dice: 2.5  loss_ce_0: 0.5324  loss_mask_0: 0.3252  loss_dice_0: 2.633  loss_ce_1: 0.2712  loss_mask_1: 0.3338  loss_dice_1: 2.535  loss_ce_2: 0.259  loss_mask_2: 0.3322  loss_dice_2: 2.517  loss_ce_3: 0.2306  loss_mask_3: 0.3279  loss_dice_3: 2.505  loss_ce_4: 0.2326  loss_mask_4: 0.3283  loss_dice_4: 2.505  loss_ce_5: 0.2134  loss_mask_5: 0.3271  loss_dice_5: 2.5  loss_ce_6: 0.207  loss_mask_6: 0.3272  loss_dice_6: 2.498  loss_ce_7: 0.2089  loss_mask_7: 0.327  loss_dice_7: 2.503  loss_ce_8: 0.2037  loss_mask_8: 0.3278  loss_dice_8: 2.504  time: 1.4716  data_time: 0.0536  lr: 1.1339e-06  max_mem: 21589M
[01/18 11:37:51] d2.utils.events INFO:  eta: 1:18:08  iter: 36459  total_loss: 31.76  loss_ce: 0.211  loss_mask: 0.3187  loss_dice: 2.573  loss_ce_0: 0.5145  loss_mask_0: 0.3202  loss_dice_0: 2.715  loss_ce_1: 0.252  loss_mask_1: 0.3252  loss_dice_1: 2.617  loss_ce_2: 0.2399  loss_mask_2: 0.3218  loss_dice_2: 2.591  loss_ce_3: 0.216  loss_mask_3: 0.3199  loss_dice_3: 2.584  loss_ce_4: 0.2089  loss_mask_4: 0.3189  loss_dice_4: 2.576  loss_ce_5: 0.2083  loss_mask_5: 0.3194  loss_dice_5: 2.579  loss_ce_6: 0.2039  loss_mask_6: 0.3188  loss_dice_6: 2.58  loss_ce_7: 0.2069  loss_mask_7: 0.3182  loss_dice_7: 2.578  loss_ce_8: 0.2058  loss_mask_8: 0.3184  loss_dice_8: 2.581  time: 1.4715  data_time: 0.0532  lr: 1.1281e-06  max_mem: 21589M
[01/18 11:38:18] d2.utils.events INFO:  eta: 1:17:42  iter: 36479  total_loss: 31.68  loss_ce: 0.217  loss_mask: 0.3273  loss_dice: 2.513  loss_ce_0: 0.5511  loss_mask_0: 0.3271  loss_dice_0: 2.669  loss_ce_1: 0.2793  loss_mask_1: 0.3281  loss_dice_1: 2.564  loss_ce_2: 0.2761  loss_mask_2: 0.3282  loss_dice_2: 2.539  loss_ce_3: 0.2633  loss_mask_3: 0.3267  loss_dice_3: 2.521  loss_ce_4: 0.2429  loss_mask_4: 0.3265  loss_dice_4: 2.529  loss_ce_5: 0.2247  loss_mask_5: 0.3263  loss_dice_5: 2.523  loss_ce_6: 0.2267  loss_mask_6: 0.3266  loss_dice_6: 2.516  loss_ce_7: 0.215  loss_mask_7: 0.3261  loss_dice_7: 2.522  loss_ce_8: 0.2166  loss_mask_8: 0.3261  loss_dice_8: 2.514  time: 1.4714  data_time: 0.0565  lr: 1.1224e-06  max_mem: 21589M
[01/18 11:38:44] d2.utils.events INFO:  eta: 1:17:12  iter: 36499  total_loss: 30.66  loss_ce: 0.1931  loss_mask: 0.326  loss_dice: 2.438  loss_ce_0: 0.5366  loss_mask_0: 0.3227  loss_dice_0: 2.576  loss_ce_1: 0.2473  loss_mask_1: 0.3298  loss_dice_1: 2.489  loss_ce_2: 0.242  loss_mask_2: 0.3266  loss_dice_2: 2.466  loss_ce_3: 0.2269  loss_mask_3: 0.3251  loss_dice_3: 2.452  loss_ce_4: 0.2199  loss_mask_4: 0.3255  loss_dice_4: 2.45  loss_ce_5: 0.2045  loss_mask_5: 0.324  loss_dice_5: 2.451  loss_ce_6: 0.2031  loss_mask_6: 0.3258  loss_dice_6: 2.44  loss_ce_7: 0.1956  loss_mask_7: 0.3259  loss_dice_7: 2.443  loss_ce_8: 0.1895  loss_mask_8: 0.3268  loss_dice_8: 2.436  time: 1.4713  data_time: 0.0492  lr: 1.1167e-06  max_mem: 21589M
[01/18 11:39:11] d2.utils.events INFO:  eta: 1:16:45  iter: 36519  total_loss: 30.88  loss_ce: 0.2043  loss_mask: 0.33  loss_dice: 2.515  loss_ce_0: 0.5389  loss_mask_0: 0.325  loss_dice_0: 2.65  loss_ce_1: 0.2454  loss_mask_1: 0.3324  loss_dice_1: 2.57  loss_ce_2: 0.2528  loss_mask_2: 0.3296  loss_dice_2: 2.529  loss_ce_3: 0.2272  loss_mask_3: 0.3303  loss_dice_3: 2.525  loss_ce_4: 0.2201  loss_mask_4: 0.3296  loss_dice_4: 2.509  loss_ce_5: 0.2121  loss_mask_5: 0.3306  loss_dice_5: 2.515  loss_ce_6: 0.2009  loss_mask_6: 0.3295  loss_dice_6: 2.511  loss_ce_7: 0.2009  loss_mask_7: 0.3302  loss_dice_7: 2.51  loss_ce_8: 0.2014  loss_mask_8: 0.3295  loss_dice_8: 2.512  time: 1.4713  data_time: 0.0543  lr: 1.1109e-06  max_mem: 21589M
[01/18 11:39:38] d2.utils.events INFO:  eta: 1:16:20  iter: 36539  total_loss: 31.05  loss_ce: 0.2096  loss_mask: 0.3241  loss_dice: 2.483  loss_ce_0: 0.5536  loss_mask_0: 0.3238  loss_dice_0: 2.635  loss_ce_1: 0.2655  loss_mask_1: 0.3302  loss_dice_1: 2.536  loss_ce_2: 0.2495  loss_mask_2: 0.3284  loss_dice_2: 2.506  loss_ce_3: 0.2384  loss_mask_3: 0.3262  loss_dice_3: 2.491  loss_ce_4: 0.2323  loss_mask_4: 0.3275  loss_dice_4: 2.489  loss_ce_5: 0.2082  loss_mask_5: 0.3264  loss_dice_5: 2.489  loss_ce_6: 0.2084  loss_mask_6: 0.327  loss_dice_6: 2.481  loss_ce_7: 0.1977  loss_mask_7: 0.3246  loss_dice_7: 2.488  loss_ce_8: 0.2129  loss_mask_8: 0.3261  loss_dice_8: 2.485  time: 1.4712  data_time: 0.0532  lr: 1.1052e-06  max_mem: 21589M
[01/18 11:40:05] d2.utils.events INFO:  eta: 1:15:51  iter: 36559  total_loss: 31.09  loss_ce: 0.2099  loss_mask: 0.3261  loss_dice: 2.481  loss_ce_0: 0.5704  loss_mask_0: 0.3256  loss_dice_0: 2.64  loss_ce_1: 0.2784  loss_mask_1: 0.3288  loss_dice_1: 2.529  loss_ce_2: 0.2689  loss_mask_2: 0.3273  loss_dice_2: 2.515  loss_ce_3: 0.2319  loss_mask_3: 0.3261  loss_dice_3: 2.491  loss_ce_4: 0.2364  loss_mask_4: 0.3262  loss_dice_4: 2.48  loss_ce_5: 0.2193  loss_mask_5: 0.3259  loss_dice_5: 2.497  loss_ce_6: 0.2243  loss_mask_6: 0.3252  loss_dice_6: 2.484  loss_ce_7: 0.208  loss_mask_7: 0.3259  loss_dice_7: 2.483  loss_ce_8: 0.219  loss_mask_8: 0.3258  loss_dice_8: 2.474  time: 1.4711  data_time: 0.0532  lr: 1.0994e-06  max_mem: 21589M
[01/18 11:40:31] d2.utils.events INFO:  eta: 1:15:27  iter: 36579  total_loss: 31.45  loss_ce: 0.2103  loss_mask: 0.3209  loss_dice: 2.541  loss_ce_0: 0.5214  loss_mask_0: 0.3211  loss_dice_0: 2.685  loss_ce_1: 0.2447  loss_mask_1: 0.3299  loss_dice_1: 2.59  loss_ce_2: 0.244  loss_mask_2: 0.3266  loss_dice_2: 2.57  loss_ce_3: 0.2332  loss_mask_3: 0.3235  loss_dice_3: 2.553  loss_ce_4: 0.2233  loss_mask_4: 0.3231  loss_dice_4: 2.541  loss_ce_5: 0.2218  loss_mask_5: 0.3227  loss_dice_5: 2.546  loss_ce_6: 0.208  loss_mask_6: 0.3217  loss_dice_6: 2.539  loss_ce_7: 0.21  loss_mask_7: 0.3199  loss_dice_7: 2.546  loss_ce_8: 0.2114  loss_mask_8: 0.3204  loss_dice_8: 2.54  time: 1.4710  data_time: 0.0536  lr: 1.0937e-06  max_mem: 21589M
[01/18 11:40:58] d2.utils.events INFO:  eta: 1:15:03  iter: 36599  total_loss: 31.36  loss_ce: 0.2226  loss_mask: 0.3261  loss_dice: 2.509  loss_ce_0: 0.546  loss_mask_0: 0.3262  loss_dice_0: 2.673  loss_ce_1: 0.2656  loss_mask_1: 0.3309  loss_dice_1: 2.566  loss_ce_2: 0.2494  loss_mask_2: 0.328  loss_dice_2: 2.535  loss_ce_3: 0.2449  loss_mask_3: 0.3262  loss_dice_3: 2.523  loss_ce_4: 0.2307  loss_mask_4: 0.3266  loss_dice_4: 2.508  loss_ce_5: 0.2273  loss_mask_5: 0.3272  loss_dice_5: 2.507  loss_ce_6: 0.2201  loss_mask_6: 0.3278  loss_dice_6: 2.509  loss_ce_7: 0.2145  loss_mask_7: 0.3272  loss_dice_7: 2.503  loss_ce_8: 0.2211  loss_mask_8: 0.3261  loss_dice_8: 2.506  time: 1.4710  data_time: 0.0551  lr: 1.0879e-06  max_mem: 21589M
[01/18 11:41:24] d2.utils.events INFO:  eta: 1:14:36  iter: 36619  total_loss: 31.12  loss_ce: 0.1994  loss_mask: 0.3263  loss_dice: 2.507  loss_ce_0: 0.532  loss_mask_0: 0.3252  loss_dice_0: 2.665  loss_ce_1: 0.2472  loss_mask_1: 0.3326  loss_dice_1: 2.56  loss_ce_2: 0.247  loss_mask_2: 0.3297  loss_dice_2: 2.539  loss_ce_3: 0.2315  loss_mask_3: 0.3287  loss_dice_3: 2.52  loss_ce_4: 0.2126  loss_mask_4: 0.3287  loss_dice_4: 2.533  loss_ce_5: 0.2145  loss_mask_5: 0.3257  loss_dice_5: 2.516  loss_ce_6: 0.2071  loss_mask_6: 0.3272  loss_dice_6: 2.51  loss_ce_7: 0.201  loss_mask_7: 0.3273  loss_dice_7: 2.52  loss_ce_8: 0.201  loss_mask_8: 0.3272  loss_dice_8: 2.521  time: 1.4709  data_time: 0.0509  lr: 1.0821e-06  max_mem: 21589M
[01/18 11:41:51] d2.utils.events INFO:  eta: 1:14:08  iter: 36639  total_loss: 30.93  loss_ce: 0.205  loss_mask: 0.3305  loss_dice: 2.481  loss_ce_0: 0.5231  loss_mask_0: 0.3264  loss_dice_0: 2.618  loss_ce_1: 0.2615  loss_mask_1: 0.3357  loss_dice_1: 2.528  loss_ce_2: 0.2448  loss_mask_2: 0.3309  loss_dice_2: 2.509  loss_ce_3: 0.2292  loss_mask_3: 0.3298  loss_dice_3: 2.496  loss_ce_4: 0.2076  loss_mask_4: 0.3274  loss_dice_4: 2.486  loss_ce_5: 0.205  loss_mask_5: 0.3274  loss_dice_5: 2.488  loss_ce_6: 0.2114  loss_mask_6: 0.3289  loss_dice_6: 2.484  loss_ce_7: 0.1997  loss_mask_7: 0.3314  loss_dice_7: 2.483  loss_ce_8: 0.2004  loss_mask_8: 0.3321  loss_dice_8: 2.489  time: 1.4708  data_time: 0.0507  lr: 1.0764e-06  max_mem: 21589M
[01/18 11:42:18] d2.utils.events INFO:  eta: 1:13:43  iter: 36659  total_loss: 30.92  loss_ce: 0.2138  loss_mask: 0.3268  loss_dice: 2.509  loss_ce_0: 0.5201  loss_mask_0: 0.3253  loss_dice_0: 2.666  loss_ce_1: 0.2603  loss_mask_1: 0.3323  loss_dice_1: 2.562  loss_ce_2: 0.26  loss_mask_2: 0.3292  loss_dice_2: 2.535  loss_ce_3: 0.233  loss_mask_3: 0.3278  loss_dice_3: 2.518  loss_ce_4: 0.2337  loss_mask_4: 0.3269  loss_dice_4: 2.523  loss_ce_5: 0.2203  loss_mask_5: 0.3262  loss_dice_5: 2.514  loss_ce_6: 0.2263  loss_mask_6: 0.3265  loss_dice_6: 2.505  loss_ce_7: 0.2141  loss_mask_7: 0.3261  loss_dice_7: 2.511  loss_ce_8: 0.2093  loss_mask_8: 0.3271  loss_dice_8: 2.513  time: 1.4707  data_time: 0.0532  lr: 1.0706e-06  max_mem: 21589M
[01/18 11:42:44] d2.utils.events INFO:  eta: 1:13:17  iter: 36679  total_loss: 31.05  loss_ce: 0.2107  loss_mask: 0.3141  loss_dice: 2.482  loss_ce_0: 0.5494  loss_mask_0: 0.3098  loss_dice_0: 2.606  loss_ce_1: 0.2662  loss_mask_1: 0.3175  loss_dice_1: 2.513  loss_ce_2: 0.2504  loss_mask_2: 0.3169  loss_dice_2: 2.487  loss_ce_3: 0.2293  loss_mask_3: 0.3163  loss_dice_3: 2.483  loss_ce_4: 0.2225  loss_mask_4: 0.3147  loss_dice_4: 2.46  loss_ce_5: 0.2293  loss_mask_5: 0.3138  loss_dice_5: 2.459  loss_ce_6: 0.215  loss_mask_6: 0.3139  loss_dice_6: 2.466  loss_ce_7: 0.2174  loss_mask_7: 0.3124  loss_dice_7: 2.47  loss_ce_8: 0.2092  loss_mask_8: 0.3134  loss_dice_8: 2.474  time: 1.4706  data_time: 0.0513  lr: 1.0648e-06  max_mem: 21589M
[01/18 11:43:10] d2.utils.events INFO:  eta: 1:12:49  iter: 36699  total_loss: 31.76  loss_ce: 0.2145  loss_mask: 0.3265  loss_dice: 2.569  loss_ce_0: 0.5628  loss_mask_0: 0.3316  loss_dice_0: 2.721  loss_ce_1: 0.2743  loss_mask_1: 0.3366  loss_dice_1: 2.633  loss_ce_2: 0.2519  loss_mask_2: 0.3302  loss_dice_2: 2.598  loss_ce_3: 0.245  loss_mask_3: 0.3279  loss_dice_3: 2.592  loss_ce_4: 0.2264  loss_mask_4: 0.3282  loss_dice_4: 2.579  loss_ce_5: 0.2304  loss_mask_5: 0.327  loss_dice_5: 2.577  loss_ce_6: 0.2193  loss_mask_6: 0.3254  loss_dice_6: 2.581  loss_ce_7: 0.2233  loss_mask_7: 0.326  loss_dice_7: 2.574  loss_ce_8: 0.2217  loss_mask_8: 0.3264  loss_dice_8: 2.578  time: 1.4705  data_time: 0.0536  lr: 1.0591e-06  max_mem: 21589M
[01/18 11:43:37] d2.utils.events INFO:  eta: 1:12:24  iter: 36719  total_loss: 31.85  loss_ce: 0.2275  loss_mask: 0.3292  loss_dice: 2.564  loss_ce_0: 0.5684  loss_mask_0: 0.3268  loss_dice_0: 2.703  loss_ce_1: 0.2811  loss_mask_1: 0.3328  loss_dice_1: 2.607  loss_ce_2: 0.272  loss_mask_2: 0.3275  loss_dice_2: 2.59  loss_ce_3: 0.2534  loss_mask_3: 0.3279  loss_dice_3: 2.566  loss_ce_4: 0.2364  loss_mask_4: 0.327  loss_dice_4: 2.564  loss_ce_5: 0.2357  loss_mask_5: 0.328  loss_dice_5: 2.568  loss_ce_6: 0.236  loss_mask_6: 0.3281  loss_dice_6: 2.564  loss_ce_7: 0.2284  loss_mask_7: 0.328  loss_dice_7: 2.566  loss_ce_8: 0.2261  loss_mask_8: 0.3279  loss_dice_8: 2.56  time: 1.4705  data_time: 0.0511  lr: 1.0533e-06  max_mem: 21589M
[01/18 11:44:04] d2.utils.events INFO:  eta: 1:12:02  iter: 36739  total_loss: 31.7  loss_ce: 0.2139  loss_mask: 0.3194  loss_dice: 2.551  loss_ce_0: 0.5511  loss_mask_0: 0.3197  loss_dice_0: 2.695  loss_ce_1: 0.2643  loss_mask_1: 0.3238  loss_dice_1: 2.605  loss_ce_2: 0.2641  loss_mask_2: 0.3235  loss_dice_2: 2.578  loss_ce_3: 0.2353  loss_mask_3: 0.3217  loss_dice_3: 2.554  loss_ce_4: 0.227  loss_mask_4: 0.3211  loss_dice_4: 2.549  loss_ce_5: 0.2283  loss_mask_5: 0.3204  loss_dice_5: 2.554  loss_ce_6: 0.2092  loss_mask_6: 0.3199  loss_dice_6: 2.56  loss_ce_7: 0.2199  loss_mask_7: 0.3194  loss_dice_7: 2.552  loss_ce_8: 0.2129  loss_mask_8: 0.3201  loss_dice_8: 2.548  time: 1.4704  data_time: 0.0538  lr: 1.0475e-06  max_mem: 21589M
[01/18 11:44:31] d2.utils.events INFO:  eta: 1:11:37  iter: 36759  total_loss: 31.91  loss_ce: 0.2057  loss_mask: 0.3206  loss_dice: 2.55  loss_ce_0: 0.5788  loss_mask_0: 0.313  loss_dice_0: 2.706  loss_ce_1: 0.2732  loss_mask_1: 0.3225  loss_dice_1: 2.598  loss_ce_2: 0.2521  loss_mask_2: 0.3216  loss_dice_2: 2.578  loss_ce_3: 0.2408  loss_mask_3: 0.321  loss_dice_3: 2.559  loss_ce_4: 0.2323  loss_mask_4: 0.3194  loss_dice_4: 2.558  loss_ce_5: 0.2255  loss_mask_5: 0.3206  loss_dice_5: 2.556  loss_ce_6: 0.2204  loss_mask_6: 0.3207  loss_dice_6: 2.557  loss_ce_7: 0.2059  loss_mask_7: 0.3212  loss_dice_7: 2.557  loss_ce_8: 0.2042  loss_mask_8: 0.3213  loss_dice_8: 2.546  time: 1.4703  data_time: 0.0515  lr: 1.0417e-06  max_mem: 21589M
[01/18 11:44:58] d2.utils.events INFO:  eta: 1:11:11  iter: 36779  total_loss: 30.7  loss_ce: 0.2165  loss_mask: 0.3231  loss_dice: 2.504  loss_ce_0: 0.5508  loss_mask_0: 0.3234  loss_dice_0: 2.642  loss_ce_1: 0.2696  loss_mask_1: 0.3293  loss_dice_1: 2.557  loss_ce_2: 0.2586  loss_mask_2: 0.3273  loss_dice_2: 2.536  loss_ce_3: 0.2388  loss_mask_3: 0.3258  loss_dice_3: 2.514  loss_ce_4: 0.2361  loss_mask_4: 0.3257  loss_dice_4: 2.514  loss_ce_5: 0.2253  loss_mask_5: 0.3247  loss_dice_5: 2.511  loss_ce_6: 0.2234  loss_mask_6: 0.3245  loss_dice_6: 2.504  loss_ce_7: 0.2162  loss_mask_7: 0.3233  loss_dice_7: 2.509  loss_ce_8: 0.217  loss_mask_8: 0.3246  loss_dice_8: 2.514  time: 1.4703  data_time: 0.0557  lr: 1.0359e-06  max_mem: 21589M
[01/18 11:45:24] d2.utils.events INFO:  eta: 1:10:44  iter: 36799  total_loss: 31.28  loss_ce: 0.2039  loss_mask: 0.3236  loss_dice: 2.501  loss_ce_0: 0.5385  loss_mask_0: 0.3231  loss_dice_0: 2.654  loss_ce_1: 0.2696  loss_mask_1: 0.3293  loss_dice_1: 2.55  loss_ce_2: 0.2505  loss_mask_2: 0.3256  loss_dice_2: 2.532  loss_ce_3: 0.2286  loss_mask_3: 0.3257  loss_dice_3: 2.513  loss_ce_4: 0.2239  loss_mask_4: 0.3245  loss_dice_4: 2.515  loss_ce_5: 0.2233  loss_mask_5: 0.3241  loss_dice_5: 2.513  loss_ce_6: 0.2151  loss_mask_6: 0.3222  loss_dice_6: 2.506  loss_ce_7: 0.2127  loss_mask_7: 0.3243  loss_dice_7: 2.501  loss_ce_8: 0.211  loss_mask_8: 0.3236  loss_dice_8: 2.506  time: 1.4702  data_time: 0.0516  lr: 1.0302e-06  max_mem: 21589M
[01/18 11:45:51] d2.utils.events INFO:  eta: 1:10:18  iter: 36819  total_loss: 31.06  loss_ce: 0.2107  loss_mask: 0.3204  loss_dice: 2.501  loss_ce_0: 0.54  loss_mask_0: 0.3214  loss_dice_0: 2.636  loss_ce_1: 0.2669  loss_mask_1: 0.3272  loss_dice_1: 2.551  loss_ce_2: 0.2475  loss_mask_2: 0.3231  loss_dice_2: 2.526  loss_ce_3: 0.2211  loss_mask_3: 0.3207  loss_dice_3: 2.515  loss_ce_4: 0.2292  loss_mask_4: 0.3211  loss_dice_4: 2.512  loss_ce_5: 0.2149  loss_mask_5: 0.3202  loss_dice_5: 2.506  loss_ce_6: 0.2087  loss_mask_6: 0.3216  loss_dice_6: 2.505  loss_ce_7: 0.1995  loss_mask_7: 0.3211  loss_dice_7: 2.509  loss_ce_8: 0.2013  loss_mask_8: 0.3206  loss_dice_8: 2.509  time: 1.4701  data_time: 0.0548  lr: 1.0244e-06  max_mem: 21589M
[01/18 11:46:18] d2.utils.events INFO:  eta: 1:09:52  iter: 36839  total_loss: 31.12  loss_ce: 0.2227  loss_mask: 0.3262  loss_dice: 2.481  loss_ce_0: 0.5727  loss_mask_0: 0.3305  loss_dice_0: 2.631  loss_ce_1: 0.285  loss_mask_1: 0.3348  loss_dice_1: 2.525  loss_ce_2: 0.269  loss_mask_2: 0.3311  loss_dice_2: 2.511  loss_ce_3: 0.2441  loss_mask_3: 0.3283  loss_dice_3: 2.485  loss_ce_4: 0.2321  loss_mask_4: 0.3275  loss_dice_4: 2.486  loss_ce_5: 0.2285  loss_mask_5: 0.3262  loss_dice_5: 2.485  loss_ce_6: 0.2324  loss_mask_6: 0.327  loss_dice_6: 2.479  loss_ce_7: 0.2181  loss_mask_7: 0.3246  loss_dice_7: 2.469  loss_ce_8: 0.2097  loss_mask_8: 0.3249  loss_dice_8: 2.488  time: 1.4700  data_time: 0.0514  lr: 1.0186e-06  max_mem: 21589M
[01/18 11:46:44] d2.utils.events INFO:  eta: 1:09:26  iter: 36859  total_loss: 31  loss_ce: 0.2233  loss_mask: 0.3151  loss_dice: 2.493  loss_ce_0: 0.5607  loss_mask_0: 0.3124  loss_dice_0: 2.633  loss_ce_1: 0.2895  loss_mask_1: 0.3217  loss_dice_1: 2.538  loss_ce_2: 0.2682  loss_mask_2: 0.3174  loss_dice_2: 2.516  loss_ce_3: 0.2513  loss_mask_3: 0.3165  loss_dice_3: 2.503  loss_ce_4: 0.2357  loss_mask_4: 0.3166  loss_dice_4: 2.508  loss_ce_5: 0.2233  loss_mask_5: 0.3155  loss_dice_5: 2.503  loss_ce_6: 0.2164  loss_mask_6: 0.3151  loss_dice_6: 2.5  loss_ce_7: 0.2239  loss_mask_7: 0.3154  loss_dice_7: 2.494  loss_ce_8: 0.2137  loss_mask_8: 0.3149  loss_dice_8: 2.499  time: 1.4700  data_time: 0.0542  lr: 1.0128e-06  max_mem: 21589M
[01/18 11:47:11] d2.utils.events INFO:  eta: 1:09:03  iter: 36879  total_loss: 31.07  loss_ce: 0.2109  loss_mask: 0.3283  loss_dice: 2.491  loss_ce_0: 0.5401  loss_mask_0: 0.3285  loss_dice_0: 2.62  loss_ce_1: 0.2734  loss_mask_1: 0.3369  loss_dice_1: 2.526  loss_ce_2: 0.2667  loss_mask_2: 0.3329  loss_dice_2: 2.504  loss_ce_3: 0.2392  loss_mask_3: 0.3313  loss_dice_3: 2.482  loss_ce_4: 0.2362  loss_mask_4: 0.3299  loss_dice_4: 2.484  loss_ce_5: 0.233  loss_mask_5: 0.3295  loss_dice_5: 2.486  loss_ce_6: 0.2158  loss_mask_6: 0.3296  loss_dice_6: 2.483  loss_ce_7: 0.2238  loss_mask_7: 0.3293  loss_dice_7: 2.474  loss_ce_8: 0.2166  loss_mask_8: 0.3302  loss_dice_8: 2.479  time: 1.4699  data_time: 0.0541  lr: 1.007e-06  max_mem: 21589M
[01/18 11:47:38] d2.utils.events INFO:  eta: 1:08:37  iter: 36899  total_loss: 30.38  loss_ce: 0.2065  loss_mask: 0.3191  loss_dice: 2.431  loss_ce_0: 0.5391  loss_mask_0: 0.3171  loss_dice_0: 2.572  loss_ce_1: 0.2629  loss_mask_1: 0.3221  loss_dice_1: 2.484  loss_ce_2: 0.2515  loss_mask_2: 0.3185  loss_dice_2: 2.458  loss_ce_3: 0.2331  loss_mask_3: 0.3178  loss_dice_3: 2.448  loss_ce_4: 0.2174  loss_mask_4: 0.3178  loss_dice_4: 2.432  loss_ce_5: 0.2184  loss_mask_5: 0.3181  loss_dice_5: 2.431  loss_ce_6: 0.2071  loss_mask_6: 0.3164  loss_dice_6: 2.436  loss_ce_7: 0.2143  loss_mask_7: 0.3173  loss_dice_7: 2.436  loss_ce_8: 0.2041  loss_mask_8: 0.3177  loss_dice_8: 2.429  time: 1.4698  data_time: 0.0521  lr: 1.0011e-06  max_mem: 21589M
[01/18 11:48:05] d2.utils.events INFO:  eta: 1:08:11  iter: 36919  total_loss: 31.69  loss_ce: 0.1986  loss_mask: 0.3167  loss_dice: 2.522  loss_ce_0: 0.5446  loss_mask_0: 0.317  loss_dice_0: 2.668  loss_ce_1: 0.2503  loss_mask_1: 0.3223  loss_dice_1: 2.569  loss_ce_2: 0.2196  loss_mask_2: 0.3192  loss_dice_2: 2.552  loss_ce_3: 0.2216  loss_mask_3: 0.317  loss_dice_3: 2.53  loss_ce_4: 0.2221  loss_mask_4: 0.3169  loss_dice_4: 2.522  loss_ce_5: 0.2001  loss_mask_5: 0.3179  loss_dice_5: 2.525  loss_ce_6: 0.2043  loss_mask_6: 0.3165  loss_dice_6: 2.525  loss_ce_7: 0.2013  loss_mask_7: 0.3153  loss_dice_7: 2.524  loss_ce_8: 0.1905  loss_mask_8: 0.316  loss_dice_8: 2.526  time: 1.4698  data_time: 0.0526  lr: 9.9533e-07  max_mem: 21589M
[01/18 11:48:32] d2.utils.events INFO:  eta: 1:07:46  iter: 36939  total_loss: 31.7  loss_ce: 0.2307  loss_mask: 0.3164  loss_dice: 2.523  loss_ce_0: 0.5408  loss_mask_0: 0.3163  loss_dice_0: 2.677  loss_ce_1: 0.2579  loss_mask_1: 0.323  loss_dice_1: 2.579  loss_ce_2: 0.2619  loss_mask_2: 0.3188  loss_dice_2: 2.561  loss_ce_3: 0.2474  loss_mask_3: 0.3189  loss_dice_3: 2.532  loss_ce_4: 0.2413  loss_mask_4: 0.3169  loss_dice_4: 2.537  loss_ce_5: 0.2245  loss_mask_5: 0.3178  loss_dice_5: 2.539  loss_ce_6: 0.2158  loss_mask_6: 0.3183  loss_dice_6: 2.534  loss_ce_7: 0.2312  loss_mask_7: 0.3179  loss_dice_7: 2.533  loss_ce_8: 0.2174  loss_mask_8: 0.3168  loss_dice_8: 2.526  time: 1.4697  data_time: 0.0513  lr: 9.8952e-07  max_mem: 21589M
[01/18 11:48:59] d2.utils.events INFO:  eta: 1:07:19  iter: 36959  total_loss: 30.86  loss_ce: 0.2162  loss_mask: 0.3238  loss_dice: 2.461  loss_ce_0: 0.5599  loss_mask_0: 0.3283  loss_dice_0: 2.607  loss_ce_1: 0.2876  loss_mask_1: 0.3308  loss_dice_1: 2.511  loss_ce_2: 0.2621  loss_mask_2: 0.3275  loss_dice_2: 2.492  loss_ce_3: 0.2431  loss_mask_3: 0.3255  loss_dice_3: 2.48  loss_ce_4: 0.2439  loss_mask_4: 0.3251  loss_dice_4: 2.477  loss_ce_5: 0.2202  loss_mask_5: 0.3247  loss_dice_5: 2.476  loss_ce_6: 0.2337  loss_mask_6: 0.324  loss_dice_6: 2.466  loss_ce_7: 0.2228  loss_mask_7: 0.3253  loss_dice_7: 2.472  loss_ce_8: 0.23  loss_mask_8: 0.3248  loss_dice_8: 2.471  time: 1.4696  data_time: 0.0533  lr: 9.837e-07  max_mem: 21589M
[01/18 11:49:25] d2.utils.events INFO:  eta: 1:06:53  iter: 36979  total_loss: 31.27  loss_ce: 0.2051  loss_mask: 0.3195  loss_dice: 2.49  loss_ce_0: 0.5418  loss_mask_0: 0.3166  loss_dice_0: 2.647  loss_ce_1: 0.2638  loss_mask_1: 0.3236  loss_dice_1: 2.54  loss_ce_2: 0.2526  loss_mask_2: 0.3218  loss_dice_2: 2.518  loss_ce_3: 0.242  loss_mask_3: 0.3208  loss_dice_3: 2.5  loss_ce_4: 0.2162  loss_mask_4: 0.321  loss_dice_4: 2.498  loss_ce_5: 0.2255  loss_mask_5: 0.3199  loss_dice_5: 2.498  loss_ce_6: 0.2118  loss_mask_6: 0.3196  loss_dice_6: 2.491  loss_ce_7: 0.2173  loss_mask_7: 0.3204  loss_dice_7: 2.493  loss_ce_8: 0.2195  loss_mask_8: 0.3202  loss_dice_8: 2.487  time: 1.4695  data_time: 0.0498  lr: 9.7787e-07  max_mem: 21589M
[01/18 11:49:52] d2.utils.events INFO:  eta: 1:06:25  iter: 36999  total_loss: 30.6  loss_ce: 0.2063  loss_mask: 0.3302  loss_dice: 2.451  loss_ce_0: 0.5328  loss_mask_0: 0.3232  loss_dice_0: 2.608  loss_ce_1: 0.2592  loss_mask_1: 0.3309  loss_dice_1: 2.51  loss_ce_2: 0.2535  loss_mask_2: 0.3299  loss_dice_2: 2.483  loss_ce_3: 0.2378  loss_mask_3: 0.3302  loss_dice_3: 2.465  loss_ce_4: 0.233  loss_mask_4: 0.3289  loss_dice_4: 2.47  loss_ce_5: 0.2297  loss_mask_5: 0.3293  loss_dice_5: 2.473  loss_ce_6: 0.224  loss_mask_6: 0.3284  loss_dice_6: 2.468  loss_ce_7: 0.2261  loss_mask_7: 0.3297  loss_dice_7: 2.462  loss_ce_8: 0.2093  loss_mask_8: 0.3298  loss_dice_8: 2.458  time: 1.4694  data_time: 0.0519  lr: 9.7204e-07  max_mem: 21589M
[01/18 11:50:19] d2.utils.events INFO:  eta: 1:06:00  iter: 37019  total_loss: 31.19  loss_ce: 0.2286  loss_mask: 0.3228  loss_dice: 2.513  loss_ce_0: 0.5646  loss_mask_0: 0.3235  loss_dice_0: 2.64  loss_ce_1: 0.2799  loss_mask_1: 0.3301  loss_dice_1: 2.556  loss_ce_2: 0.2676  loss_mask_2: 0.3273  loss_dice_2: 2.534  loss_ce_3: 0.2386  loss_mask_3: 0.3254  loss_dice_3: 2.526  loss_ce_4: 0.2402  loss_mask_4: 0.3235  loss_dice_4: 2.53  loss_ce_5: 0.2239  loss_mask_5: 0.3239  loss_dice_5: 2.52  loss_ce_6: 0.2212  loss_mask_6: 0.3237  loss_dice_6: 2.513  loss_ce_7: 0.2182  loss_mask_7: 0.3224  loss_dice_7: 2.517  loss_ce_8: 0.2239  loss_mask_8: 0.323  loss_dice_8: 2.52  time: 1.4694  data_time: 0.0526  lr: 9.6621e-07  max_mem: 21589M
[01/18 11:50:45] d2.utils.events INFO:  eta: 1:05:33  iter: 37039  total_loss: 31.32  loss_ce: 0.2002  loss_mask: 0.3195  loss_dice: 2.522  loss_ce_0: 0.5409  loss_mask_0: 0.3225  loss_dice_0: 2.662  loss_ce_1: 0.268  loss_mask_1: 0.3239  loss_dice_1: 2.565  loss_ce_2: 0.2454  loss_mask_2: 0.3207  loss_dice_2: 2.538  loss_ce_3: 0.2241  loss_mask_3: 0.3203  loss_dice_3: 2.525  loss_ce_4: 0.2151  loss_mask_4: 0.3199  loss_dice_4: 2.527  loss_ce_5: 0.2149  loss_mask_5: 0.3193  loss_dice_5: 2.527  loss_ce_6: 0.2084  loss_mask_6: 0.3202  loss_dice_6: 2.529  loss_ce_7: 0.206  loss_mask_7: 0.3194  loss_dice_7: 2.526  loss_ce_8: 0.197  loss_mask_8: 0.3192  loss_dice_8: 2.517  time: 1.4693  data_time: 0.0533  lr: 9.6037e-07  max_mem: 21589M
[01/18 11:51:12] d2.utils.events INFO:  eta: 1:05:07  iter: 37059  total_loss: 32.22  loss_ce: 0.212  loss_mask: 0.3149  loss_dice: 2.605  loss_ce_0: 0.5525  loss_mask_0: 0.3229  loss_dice_0: 2.752  loss_ce_1: 0.272  loss_mask_1: 0.3213  loss_dice_1: 2.665  loss_ce_2: 0.2395  loss_mask_2: 0.3181  loss_dice_2: 2.646  loss_ce_3: 0.2331  loss_mask_3: 0.3157  loss_dice_3: 2.632  loss_ce_4: 0.2353  loss_mask_4: 0.3144  loss_dice_4: 2.616  loss_ce_5: 0.2182  loss_mask_5: 0.3147  loss_dice_5: 2.622  loss_ce_6: 0.2315  loss_mask_6: 0.3145  loss_dice_6: 2.606  loss_ce_7: 0.2161  loss_mask_7: 0.3144  loss_dice_7: 2.609  loss_ce_8: 0.2104  loss_mask_8: 0.3148  loss_dice_8: 2.61  time: 1.4692  data_time: 0.0535  lr: 9.5453e-07  max_mem: 21589M
[01/18 11:51:38] d2.utils.events INFO:  eta: 1:04:40  iter: 37079  total_loss: 30.35  loss_ce: 0.2135  loss_mask: 0.3229  loss_dice: 2.437  loss_ce_0: 0.5382  loss_mask_0: 0.3159  loss_dice_0: 2.564  loss_ce_1: 0.2622  loss_mask_1: 0.3248  loss_dice_1: 2.48  loss_ce_2: 0.2532  loss_mask_2: 0.3239  loss_dice_2: 2.459  loss_ce_3: 0.2292  loss_mask_3: 0.3237  loss_dice_3: 2.446  loss_ce_4: 0.2259  loss_mask_4: 0.3228  loss_dice_4: 2.445  loss_ce_5: 0.2131  loss_mask_5: 0.3222  loss_dice_5: 2.444  loss_ce_6: 0.2127  loss_mask_6: 0.3222  loss_dice_6: 2.433  loss_ce_7: 0.2027  loss_mask_7: 0.3219  loss_dice_7: 2.435  loss_ce_8: 0.2137  loss_mask_8: 0.3233  loss_dice_8: 2.435  time: 1.4692  data_time: 0.0504  lr: 9.4869e-07  max_mem: 21589M
[01/18 11:52:05] d2.utils.events INFO:  eta: 1:04:13  iter: 37099  total_loss: 31.44  loss_ce: 0.2062  loss_mask: 0.3281  loss_dice: 2.563  loss_ce_0: 0.5581  loss_mask_0: 0.3291  loss_dice_0: 2.705  loss_ce_1: 0.2707  loss_mask_1: 0.3307  loss_dice_1: 2.611  loss_ce_2: 0.2641  loss_mask_2: 0.3289  loss_dice_2: 2.582  loss_ce_3: 0.2365  loss_mask_3: 0.3289  loss_dice_3: 2.568  loss_ce_4: 0.2378  loss_mask_4: 0.3268  loss_dice_4: 2.559  loss_ce_5: 0.2148  loss_mask_5: 0.3285  loss_dice_5: 2.568  loss_ce_6: 0.2238  loss_mask_6: 0.3271  loss_dice_6: 2.567  loss_ce_7: 0.2154  loss_mask_7: 0.3257  loss_dice_7: 2.563  loss_ce_8: 0.2058  loss_mask_8: 0.3272  loss_dice_8: 2.561  time: 1.4691  data_time: 0.0523  lr: 9.4284e-07  max_mem: 21589M
[01/18 11:52:32] d2.utils.events INFO:  eta: 1:03:47  iter: 37119  total_loss: 31.76  loss_ce: 0.1836  loss_mask: 0.3238  loss_dice: 2.58  loss_ce_0: 0.5441  loss_mask_0: 0.3219  loss_dice_0: 2.727  loss_ce_1: 0.2619  loss_mask_1: 0.3284  loss_dice_1: 2.633  loss_ce_2: 0.2422  loss_mask_2: 0.3251  loss_dice_2: 2.609  loss_ce_3: 0.2276  loss_mask_3: 0.3228  loss_dice_3: 2.589  loss_ce_4: 0.2034  loss_mask_4: 0.3247  loss_dice_4: 2.584  loss_ce_5: 0.2086  loss_mask_5: 0.324  loss_dice_5: 2.589  loss_ce_6: 0.1983  loss_mask_6: 0.3224  loss_dice_6: 2.579  loss_ce_7: 0.1928  loss_mask_7: 0.3235  loss_dice_7: 2.579  loss_ce_8: 0.1999  loss_mask_8: 0.3222  loss_dice_8: 2.582  time: 1.4690  data_time: 0.0537  lr: 9.3699e-07  max_mem: 21589M
[01/18 11:52:59] d2.utils.events INFO:  eta: 1:03:21  iter: 37139  total_loss: 32.05  loss_ce: 0.2231  loss_mask: 0.3161  loss_dice: 2.548  loss_ce_0: 0.5571  loss_mask_0: 0.3138  loss_dice_0: 2.716  loss_ce_1: 0.2759  loss_mask_1: 0.3197  loss_dice_1: 2.61  loss_ce_2: 0.2708  loss_mask_2: 0.3159  loss_dice_2: 2.591  loss_ce_3: 0.2314  loss_mask_3: 0.3176  loss_dice_3: 2.576  loss_ce_4: 0.2315  loss_mask_4: 0.3164  loss_dice_4: 2.572  loss_ce_5: 0.2132  loss_mask_5: 0.3148  loss_dice_5: 2.573  loss_ce_6: 0.2138  loss_mask_6: 0.3149  loss_dice_6: 2.565  loss_ce_7: 0.2304  loss_mask_7: 0.3154  loss_dice_7: 2.568  loss_ce_8: 0.2126  loss_mask_8: 0.3168  loss_dice_8: 2.554  time: 1.4689  data_time: 0.0529  lr: 9.3113e-07  max_mem: 21589M
[01/18 11:53:25] d2.utils.events INFO:  eta: 1:02:54  iter: 37159  total_loss: 31.72  loss_ce: 0.2288  loss_mask: 0.3177  loss_dice: 2.522  loss_ce_0: 0.5516  loss_mask_0: 0.3198  loss_dice_0: 2.672  loss_ce_1: 0.2767  loss_mask_1: 0.3221  loss_dice_1: 2.567  loss_ce_2: 0.2782  loss_mask_2: 0.3178  loss_dice_2: 2.548  loss_ce_3: 0.2539  loss_mask_3: 0.3174  loss_dice_3: 2.537  loss_ce_4: 0.2325  loss_mask_4: 0.3183  loss_dice_4: 2.528  loss_ce_5: 0.2337  loss_mask_5: 0.3172  loss_dice_5: 2.529  loss_ce_6: 0.2276  loss_mask_6: 0.317  loss_dice_6: 2.523  loss_ce_7: 0.2225  loss_mask_7: 0.3167  loss_dice_7: 2.526  loss_ce_8: 0.2239  loss_mask_8: 0.3179  loss_dice_8: 2.527  time: 1.4689  data_time: 0.0553  lr: 9.2527e-07  max_mem: 21589M
[01/18 11:53:52] d2.utils.events INFO:  eta: 1:02:25  iter: 37179  total_loss: 31.06  loss_ce: 0.2196  loss_mask: 0.3231  loss_dice: 2.454  loss_ce_0: 0.5732  loss_mask_0: 0.3207  loss_dice_0: 2.591  loss_ce_1: 0.2936  loss_mask_1: 0.3289  loss_dice_1: 2.51  loss_ce_2: 0.2769  loss_mask_2: 0.3279  loss_dice_2: 2.477  loss_ce_3: 0.2473  loss_mask_3: 0.3266  loss_dice_3: 2.464  loss_ce_4: 0.2346  loss_mask_4: 0.3252  loss_dice_4: 2.463  loss_ce_5: 0.2314  loss_mask_5: 0.3236  loss_dice_5: 2.451  loss_ce_6: 0.2232  loss_mask_6: 0.323  loss_dice_6: 2.458  loss_ce_7: 0.2144  loss_mask_7: 0.3239  loss_dice_7: 2.464  loss_ce_8: 0.2134  loss_mask_8: 0.3236  loss_dice_8: 2.455  time: 1.4688  data_time: 0.0523  lr: 9.1941e-07  max_mem: 21589M
[01/18 11:54:18] d2.utils.events INFO:  eta: 1:01:58  iter: 37199  total_loss: 30.67  loss_ce: 0.2187  loss_mask: 0.3125  loss_dice: 2.469  loss_ce_0: 0.5787  loss_mask_0: 0.3105  loss_dice_0: 2.602  loss_ce_1: 0.2732  loss_mask_1: 0.3183  loss_dice_1: 2.513  loss_ce_2: 0.2628  loss_mask_2: 0.316  loss_dice_2: 2.479  loss_ce_3: 0.2515  loss_mask_3: 0.3141  loss_dice_3: 2.469  loss_ce_4: 0.2462  loss_mask_4: 0.3129  loss_dice_4: 2.464  loss_ce_5: 0.2326  loss_mask_5: 0.3137  loss_dice_5: 2.466  loss_ce_6: 0.2384  loss_mask_6: 0.3127  loss_dice_6: 2.463  loss_ce_7: 0.228  loss_mask_7: 0.3124  loss_dice_7: 2.459  loss_ce_8: 0.2328  loss_mask_8: 0.3118  loss_dice_8: 2.459  time: 1.4687  data_time: 0.0522  lr: 9.1354e-07  max_mem: 21589M
[01/18 11:54:45] d2.utils.events INFO:  eta: 1:01:31  iter: 37219  total_loss: 31.35  loss_ce: 0.2015  loss_mask: 0.3201  loss_dice: 2.558  loss_ce_0: 0.5419  loss_mask_0: 0.3156  loss_dice_0: 2.702  loss_ce_1: 0.26  loss_mask_1: 0.3245  loss_dice_1: 2.612  loss_ce_2: 0.2523  loss_mask_2: 0.3228  loss_dice_2: 2.592  loss_ce_3: 0.229  loss_mask_3: 0.3205  loss_dice_3: 2.569  loss_ce_4: 0.2199  loss_mask_4: 0.3204  loss_dice_4: 2.571  loss_ce_5: 0.2215  loss_mask_5: 0.3176  loss_dice_5: 2.563  loss_ce_6: 0.207  loss_mask_6: 0.3183  loss_dice_6: 2.557  loss_ce_7: 0.2058  loss_mask_7: 0.3193  loss_dice_7: 2.564  loss_ce_8: 0.2007  loss_mask_8: 0.3186  loss_dice_8: 2.552  time: 1.4686  data_time: 0.0534  lr: 9.0767e-07  max_mem: 21589M
[01/18 11:55:12] d2.utils.events INFO:  eta: 1:01:05  iter: 37239  total_loss: 31.38  loss_ce: 0.1993  loss_mask: 0.3143  loss_dice: 2.504  loss_ce_0: 0.5539  loss_mask_0: 0.3154  loss_dice_0: 2.652  loss_ce_1: 0.2649  loss_mask_1: 0.3229  loss_dice_1: 2.555  loss_ce_2: 0.252  loss_mask_2: 0.3169  loss_dice_2: 2.526  loss_ce_3: 0.2403  loss_mask_3: 0.3131  loss_dice_3: 2.515  loss_ce_4: 0.2239  loss_mask_4: 0.312  loss_dice_4: 2.513  loss_ce_5: 0.2162  loss_mask_5: 0.3122  loss_dice_5: 2.52  loss_ce_6: 0.2148  loss_mask_6: 0.3122  loss_dice_6: 2.507  loss_ce_7: 0.2084  loss_mask_7: 0.3125  loss_dice_7: 2.498  loss_ce_8: 0.2096  loss_mask_8: 0.3135  loss_dice_8: 2.503  time: 1.4686  data_time: 0.0535  lr: 9.0179e-07  max_mem: 21589M
[01/18 11:55:39] d2.utils.events INFO:  eta: 1:00:39  iter: 37259  total_loss: 31.05  loss_ce: 0.2164  loss_mask: 0.3234  loss_dice: 2.523  loss_ce_0: 0.5396  loss_mask_0: 0.3221  loss_dice_0: 2.672  loss_ce_1: 0.2419  loss_mask_1: 0.329  loss_dice_1: 2.584  loss_ce_2: 0.2499  loss_mask_2: 0.3246  loss_dice_2: 2.547  loss_ce_3: 0.2319  loss_mask_3: 0.3232  loss_dice_3: 2.537  loss_ce_4: 0.2319  loss_mask_4: 0.3238  loss_dice_4: 2.534  loss_ce_5: 0.2267  loss_mask_5: 0.3237  loss_dice_5: 2.529  loss_ce_6: 0.2221  loss_mask_6: 0.3236  loss_dice_6: 2.527  loss_ce_7: 0.2094  loss_mask_7: 0.3229  loss_dice_7: 2.524  loss_ce_8: 0.1983  loss_mask_8: 0.3239  loss_dice_8: 2.528  time: 1.4685  data_time: 0.0575  lr: 8.9591e-07  max_mem: 21589M
[01/18 11:56:05] d2.utils.events INFO:  eta: 1:00:13  iter: 37279  total_loss: 30.76  loss_ce: 0.2073  loss_mask: 0.3192  loss_dice: 2.472  loss_ce_0: 0.5495  loss_mask_0: 0.3175  loss_dice_0: 2.608  loss_ce_1: 0.2718  loss_mask_1: 0.324  loss_dice_1: 2.508  loss_ce_2: 0.2542  loss_mask_2: 0.3184  loss_dice_2: 2.493  loss_ce_3: 0.2387  loss_mask_3: 0.3193  loss_dice_3: 2.473  loss_ce_4: 0.2347  loss_mask_4: 0.3194  loss_dice_4: 2.478  loss_ce_5: 0.2215  loss_mask_5: 0.3189  loss_dice_5: 2.472  loss_ce_6: 0.2159  loss_mask_6: 0.3182  loss_dice_6: 2.46  loss_ce_7: 0.2157  loss_mask_7: 0.318  loss_dice_7: 2.473  loss_ce_8: 0.2094  loss_mask_8: 0.3172  loss_dice_8: 2.468  time: 1.4684  data_time: 0.0512  lr: 8.9002e-07  max_mem: 21589M
[01/18 11:56:31] d2.utils.events INFO:  eta: 0:59:44  iter: 37299  total_loss: 30.86  loss_ce: 0.2082  loss_mask: 0.3317  loss_dice: 2.496  loss_ce_0: 0.5499  loss_mask_0: 0.3326  loss_dice_0: 2.636  loss_ce_1: 0.2595  loss_mask_1: 0.3357  loss_dice_1: 2.547  loss_ce_2: 0.2587  loss_mask_2: 0.3323  loss_dice_2: 2.515  loss_ce_3: 0.2432  loss_mask_3: 0.3298  loss_dice_3: 2.5  loss_ce_4: 0.2287  loss_mask_4: 0.3291  loss_dice_4: 2.495  loss_ce_5: 0.2281  loss_mask_5: 0.3299  loss_dice_5: 2.498  loss_ce_6: 0.2177  loss_mask_6: 0.3305  loss_dice_6: 2.498  loss_ce_7: 0.2076  loss_mask_7: 0.3303  loss_dice_7: 2.489  loss_ce_8: 0.2075  loss_mask_8: 0.33  loss_dice_8: 2.486  time: 1.4683  data_time: 0.0518  lr: 8.8413e-07  max_mem: 21589M
[01/18 11:56:58] d2.utils.events INFO:  eta: 0:59:19  iter: 37319  total_loss: 31.05  loss_ce: 0.2106  loss_mask: 0.312  loss_dice: 2.502  loss_ce_0: 0.57  loss_mask_0: 0.3095  loss_dice_0: 2.665  loss_ce_1: 0.259  loss_mask_1: 0.3157  loss_dice_1: 2.572  loss_ce_2: 0.243  loss_mask_2: 0.3116  loss_dice_2: 2.549  loss_ce_3: 0.2283  loss_mask_3: 0.311  loss_dice_3: 2.524  loss_ce_4: 0.2241  loss_mask_4: 0.3113  loss_dice_4: 2.519  loss_ce_5: 0.2128  loss_mask_5: 0.3107  loss_dice_5: 2.509  loss_ce_6: 0.219  loss_mask_6: 0.3107  loss_dice_6: 2.508  loss_ce_7: 0.1982  loss_mask_7: 0.3108  loss_dice_7: 2.511  loss_ce_8: 0.2096  loss_mask_8: 0.312  loss_dice_8: 2.503  time: 1.4683  data_time: 0.0557  lr: 8.7824e-07  max_mem: 21589M
[01/18 11:57:25] d2.utils.events INFO:  eta: 0:58:53  iter: 37339  total_loss: 31.26  loss_ce: 0.1956  loss_mask: 0.3214  loss_dice: 2.543  loss_ce_0: 0.5362  loss_mask_0: 0.3163  loss_dice_0: 2.678  loss_ce_1: 0.2552  loss_mask_1: 0.3262  loss_dice_1: 2.59  loss_ce_2: 0.2372  loss_mask_2: 0.3212  loss_dice_2: 2.564  loss_ce_3: 0.2167  loss_mask_3: 0.3226  loss_dice_3: 2.557  loss_ce_4: 0.2152  loss_mask_4: 0.3219  loss_dice_4: 2.552  loss_ce_5: 0.2029  loss_mask_5: 0.3211  loss_dice_5: 2.554  loss_ce_6: 0.2001  loss_mask_6: 0.3224  loss_dice_6: 2.551  loss_ce_7: 0.1982  loss_mask_7: 0.3225  loss_dice_7: 2.533  loss_ce_8: 0.1947  loss_mask_8: 0.3217  loss_dice_8: 2.538  time: 1.4682  data_time: 0.0535  lr: 8.7234e-07  max_mem: 21589M
[01/18 11:57:52] d2.utils.events INFO:  eta: 0:58:26  iter: 37359  total_loss: 30.84  loss_ce: 0.2155  loss_mask: 0.3234  loss_dice: 2.474  loss_ce_0: 0.5582  loss_mask_0: 0.3244  loss_dice_0: 2.615  loss_ce_1: 0.2563  loss_mask_1: 0.3323  loss_dice_1: 2.527  loss_ce_2: 0.2633  loss_mask_2: 0.3277  loss_dice_2: 2.498  loss_ce_3: 0.2363  loss_mask_3: 0.326  loss_dice_3: 2.486  loss_ce_4: 0.2335  loss_mask_4: 0.323  loss_dice_4: 2.484  loss_ce_5: 0.2222  loss_mask_5: 0.3234  loss_dice_5: 2.477  loss_ce_6: 0.2221  loss_mask_6: 0.3226  loss_dice_6: 2.468  loss_ce_7: 0.204  loss_mask_7: 0.3233  loss_dice_7: 2.477  loss_ce_8: 0.2149  loss_mask_8: 0.3232  loss_dice_8: 2.47  time: 1.4681  data_time: 0.0499  lr: 8.6644e-07  max_mem: 21589M
[01/18 11:58:19] d2.utils.events INFO:  eta: 0:58:00  iter: 37379  total_loss: 31.35  loss_ce: 0.1933  loss_mask: 0.3197  loss_dice: 2.495  loss_ce_0: 0.5141  loss_mask_0: 0.3215  loss_dice_0: 2.627  loss_ce_1: 0.2369  loss_mask_1: 0.3285  loss_dice_1: 2.54  loss_ce_2: 0.247  loss_mask_2: 0.3218  loss_dice_2: 2.513  loss_ce_3: 0.2177  loss_mask_3: 0.3208  loss_dice_3: 2.5  loss_ce_4: 0.2182  loss_mask_4: 0.3215  loss_dice_4: 2.504  loss_ce_5: 0.2115  loss_mask_5: 0.3209  loss_dice_5: 2.502  loss_ce_6: 0.1978  loss_mask_6: 0.3203  loss_dice_6: 2.504  loss_ce_7: 0.2129  loss_mask_7: 0.3204  loss_dice_7: 2.503  loss_ce_8: 0.1907  loss_mask_8: 0.3208  loss_dice_8: 2.493  time: 1.4681  data_time: 0.0527  lr: 8.6053e-07  max_mem: 21589M
[01/18 11:58:45] d2.utils.events INFO:  eta: 0:57:33  iter: 37399  total_loss: 31.14  loss_ce: 0.2165  loss_mask: 0.3245  loss_dice: 2.477  loss_ce_0: 0.584  loss_mask_0: 0.3237  loss_dice_0: 2.616  loss_ce_1: 0.2824  loss_mask_1: 0.3333  loss_dice_1: 2.534  loss_ce_2: 0.2719  loss_mask_2: 0.3285  loss_dice_2: 2.515  loss_ce_3: 0.2426  loss_mask_3: 0.3271  loss_dice_3: 2.5  loss_ce_4: 0.2298  loss_mask_4: 0.3274  loss_dice_4: 2.489  loss_ce_5: 0.23  loss_mask_5: 0.326  loss_dice_5: 2.491  loss_ce_6: 0.2136  loss_mask_6: 0.3258  loss_dice_6: 2.487  loss_ce_7: 0.2168  loss_mask_7: 0.3249  loss_dice_7: 2.478  loss_ce_8: 0.2204  loss_mask_8: 0.3248  loss_dice_8: 2.482  time: 1.4680  data_time: 0.0519  lr: 8.5462e-07  max_mem: 21589M
[01/18 11:59:12] d2.utils.events INFO:  eta: 0:57:06  iter: 37419  total_loss: 31.07  loss_ce: 0.2184  loss_mask: 0.3189  loss_dice: 2.486  loss_ce_0: 0.573  loss_mask_0: 0.3147  loss_dice_0: 2.623  loss_ce_1: 0.2847  loss_mask_1: 0.3257  loss_dice_1: 2.532  loss_ce_2: 0.2764  loss_mask_2: 0.3229  loss_dice_2: 2.513  loss_ce_3: 0.2554  loss_mask_3: 0.3205  loss_dice_3: 2.487  loss_ce_4: 0.2361  loss_mask_4: 0.3205  loss_dice_4: 2.492  loss_ce_5: 0.2297  loss_mask_5: 0.3199  loss_dice_5: 2.48  loss_ce_6: 0.2271  loss_mask_6: 0.3183  loss_dice_6: 2.488  loss_ce_7: 0.2273  loss_mask_7: 0.3188  loss_dice_7: 2.483  loss_ce_8: 0.2321  loss_mask_8: 0.3194  loss_dice_8: 2.49  time: 1.4679  data_time: 0.0541  lr: 8.487e-07  max_mem: 21589M
[01/18 11:59:38] d2.utils.events INFO:  eta: 0:56:41  iter: 37439  total_loss: 30.81  loss_ce: 0.2019  loss_mask: 0.3258  loss_dice: 2.464  loss_ce_0: 0.5743  loss_mask_0: 0.3251  loss_dice_0: 2.611  loss_ce_1: 0.2571  loss_mask_1: 0.3301  loss_dice_1: 2.516  loss_ce_2: 0.2349  loss_mask_2: 0.3258  loss_dice_2: 2.495  loss_ce_3: 0.2199  loss_mask_3: 0.3256  loss_dice_3: 2.489  loss_ce_4: 0.2139  loss_mask_4: 0.3254  loss_dice_4: 2.474  loss_ce_5: 0.2063  loss_mask_5: 0.3252  loss_dice_5: 2.479  loss_ce_6: 0.202  loss_mask_6: 0.324  loss_dice_6: 2.469  loss_ce_7: 0.1904  loss_mask_7: 0.3241  loss_dice_7: 2.475  loss_ce_8: 0.1931  loss_mask_8: 0.325  loss_dice_8: 2.475  time: 1.4678  data_time: 0.0514  lr: 8.4278e-07  max_mem: 21589M
[01/18 12:00:05] d2.utils.events INFO:  eta: 0:56:14  iter: 37459  total_loss: 31.19  loss_ce: 0.2179  loss_mask: 0.3292  loss_dice: 2.525  loss_ce_0: 0.5334  loss_mask_0: 0.3257  loss_dice_0: 2.668  loss_ce_1: 0.2698  loss_mask_1: 0.3323  loss_dice_1: 2.57  loss_ce_2: 0.2584  loss_mask_2: 0.3299  loss_dice_2: 2.547  loss_ce_3: 0.2393  loss_mask_3: 0.3293  loss_dice_3: 2.532  loss_ce_4: 0.2339  loss_mask_4: 0.3299  loss_dice_4: 2.528  loss_ce_5: 0.2215  loss_mask_5: 0.3282  loss_dice_5: 2.527  loss_ce_6: 0.2184  loss_mask_6: 0.3292  loss_dice_6: 2.535  loss_ce_7: 0.2143  loss_mask_7: 0.3286  loss_dice_7: 2.525  loss_ce_8: 0.2272  loss_mask_8: 0.3291  loss_dice_8: 2.529  time: 1.4677  data_time: 0.0547  lr: 8.3685e-07  max_mem: 21589M
[01/18 12:00:32] d2.utils.events INFO:  eta: 0:55:48  iter: 37479  total_loss: 30.93  loss_ce: 0.197  loss_mask: 0.3241  loss_dice: 2.5  loss_ce_0: 0.5406  loss_mask_0: 0.3279  loss_dice_0: 2.627  loss_ce_1: 0.2562  loss_mask_1: 0.3317  loss_dice_1: 2.538  loss_ce_2: 0.2682  loss_mask_2: 0.3277  loss_dice_2: 2.518  loss_ce_3: 0.2239  loss_mask_3: 0.3255  loss_dice_3: 2.505  loss_ce_4: 0.2268  loss_mask_4: 0.3256  loss_dice_4: 2.51  loss_ce_5: 0.2196  loss_mask_5: 0.324  loss_dice_5: 2.506  loss_ce_6: 0.2176  loss_mask_6: 0.3225  loss_dice_6: 2.498  loss_ce_7: 0.197  loss_mask_7: 0.3227  loss_dice_7: 2.502  loss_ce_8: 0.1998  loss_mask_8: 0.3239  loss_dice_8: 2.496  time: 1.4677  data_time: 0.0538  lr: 8.3092e-07  max_mem: 21589M
[01/18 12:00:58] d2.utils.events INFO:  eta: 0:55:22  iter: 37499  total_loss: 30.68  loss_ce: 0.2051  loss_mask: 0.3298  loss_dice: 2.484  loss_ce_0: 0.5127  loss_mask_0: 0.3277  loss_dice_0: 2.628  loss_ce_1: 0.2649  loss_mask_1: 0.3351  loss_dice_1: 2.543  loss_ce_2: 0.246  loss_mask_2: 0.3321  loss_dice_2: 2.517  loss_ce_3: 0.2281  loss_mask_3: 0.3288  loss_dice_3: 2.495  loss_ce_4: 0.2347  loss_mask_4: 0.3291  loss_dice_4: 2.49  loss_ce_5: 0.21  loss_mask_5: 0.329  loss_dice_5: 2.486  loss_ce_6: 0.2089  loss_mask_6: 0.3276  loss_dice_6: 2.48  loss_ce_7: 0.2044  loss_mask_7: 0.3277  loss_dice_7: 2.481  loss_ce_8: 0.21  loss_mask_8: 0.3288  loss_dice_8: 2.488  time: 1.4676  data_time: 0.0531  lr: 8.2499e-07  max_mem: 21589M
[01/18 12:01:25] d2.utils.events INFO:  eta: 0:54:55  iter: 37519  total_loss: 31.6  loss_ce: 0.1973  loss_mask: 0.3168  loss_dice: 2.546  loss_ce_0: 0.5513  loss_mask_0: 0.3142  loss_dice_0: 2.679  loss_ce_1: 0.2691  loss_mask_1: 0.3202  loss_dice_1: 2.587  loss_ce_2: 0.2554  loss_mask_2: 0.3179  loss_dice_2: 2.585  loss_ce_3: 0.2342  loss_mask_3: 0.3167  loss_dice_3: 2.561  loss_ce_4: 0.2296  loss_mask_4: 0.3172  loss_dice_4: 2.557  loss_ce_5: 0.215  loss_mask_5: 0.3173  loss_dice_5: 2.551  loss_ce_6: 0.2122  loss_mask_6: 0.3176  loss_dice_6: 2.55  loss_ce_7: 0.2076  loss_mask_7: 0.316  loss_dice_7: 2.545  loss_ce_8: 0.2077  loss_mask_8: 0.317  loss_dice_8: 2.557  time: 1.4675  data_time: 0.0508  lr: 8.1905e-07  max_mem: 21589M
[01/18 12:01:51] d2.utils.events INFO:  eta: 0:54:28  iter: 37539  total_loss: 31.1  loss_ce: 0.2098  loss_mask: 0.3181  loss_dice: 2.501  loss_ce_0: 0.567  loss_mask_0: 0.3214  loss_dice_0: 2.645  loss_ce_1: 0.2759  loss_mask_1: 0.3235  loss_dice_1: 2.555  loss_ce_2: 0.2602  loss_mask_2: 0.3214  loss_dice_2: 2.529  loss_ce_3: 0.237  loss_mask_3: 0.3171  loss_dice_3: 2.523  loss_ce_4: 0.2355  loss_mask_4: 0.3172  loss_dice_4: 2.517  loss_ce_5: 0.2221  loss_mask_5: 0.3177  loss_dice_5: 2.514  loss_ce_6: 0.228  loss_mask_6: 0.3193  loss_dice_6: 2.498  loss_ce_7: 0.228  loss_mask_7: 0.3187  loss_dice_7: 2.505  loss_ce_8: 0.2241  loss_mask_8: 0.3172  loss_dice_8: 2.501  time: 1.4674  data_time: 0.0517  lr: 8.131e-07  max_mem: 21589M
[01/18 12:02:18] d2.utils.events INFO:  eta: 0:54:02  iter: 37559  total_loss: 30.55  loss_ce: 0.2117  loss_mask: 0.3259  loss_dice: 2.446  loss_ce_0: 0.5438  loss_mask_0: 0.3225  loss_dice_0: 2.58  loss_ce_1: 0.265  loss_mask_1: 0.3289  loss_dice_1: 2.489  loss_ce_2: 0.2691  loss_mask_2: 0.3263  loss_dice_2: 2.465  loss_ce_3: 0.2473  loss_mask_3: 0.3264  loss_dice_3: 2.45  loss_ce_4: 0.2277  loss_mask_4: 0.3266  loss_dice_4: 2.447  loss_ce_5: 0.2205  loss_mask_5: 0.3251  loss_dice_5: 2.441  loss_ce_6: 0.2169  loss_mask_6: 0.3247  loss_dice_6: 2.436  loss_ce_7: 0.2208  loss_mask_7: 0.3245  loss_dice_7: 2.432  loss_ce_8: 0.212  loss_mask_8: 0.3245  loss_dice_8: 2.444  time: 1.4674  data_time: 0.0532  lr: 8.0716e-07  max_mem: 21589M
[01/18 12:02:45] d2.utils.events INFO:  eta: 0:53:35  iter: 37579  total_loss: 30.74  loss_ce: 0.1982  loss_mask: 0.3272  loss_dice: 2.497  loss_ce_0: 0.5035  loss_mask_0: 0.3225  loss_dice_0: 2.633  loss_ce_1: 0.2451  loss_mask_1: 0.3315  loss_dice_1: 2.546  loss_ce_2: 0.2393  loss_mask_2: 0.3291  loss_dice_2: 2.518  loss_ce_3: 0.2162  loss_mask_3: 0.3283  loss_dice_3: 2.504  loss_ce_4: 0.2137  loss_mask_4: 0.328  loss_dice_4: 2.505  loss_ce_5: 0.2109  loss_mask_5: 0.3275  loss_dice_5: 2.502  loss_ce_6: 0.1966  loss_mask_6: 0.3273  loss_dice_6: 2.495  loss_ce_7: 0.2  loss_mask_7: 0.3274  loss_dice_7: 2.498  loss_ce_8: 0.1978  loss_mask_8: 0.3267  loss_dice_8: 2.495  time: 1.4673  data_time: 0.0516  lr: 8.012e-07  max_mem: 21589M
[01/18 12:03:11] d2.utils.events INFO:  eta: 0:53:08  iter: 37599  total_loss: 31.79  loss_ce: 0.2244  loss_mask: 0.3309  loss_dice: 2.543  loss_ce_0: 0.5481  loss_mask_0: 0.3282  loss_dice_0: 2.672  loss_ce_1: 0.2786  loss_mask_1: 0.3355  loss_dice_1: 2.578  loss_ce_2: 0.2659  loss_mask_2: 0.3312  loss_dice_2: 2.566  loss_ce_3: 0.2565  loss_mask_3: 0.3325  loss_dice_3: 2.553  loss_ce_4: 0.2384  loss_mask_4: 0.331  loss_dice_4: 2.537  loss_ce_5: 0.2338  loss_mask_5: 0.3312  loss_dice_5: 2.54  loss_ce_6: 0.2249  loss_mask_6: 0.3304  loss_dice_6: 2.542  loss_ce_7: 0.219  loss_mask_7: 0.3313  loss_dice_7: 2.534  loss_ce_8: 0.2098  loss_mask_8: 0.3308  loss_dice_8: 2.541  time: 1.4672  data_time: 0.0536  lr: 7.9524e-07  max_mem: 21589M
[01/18 12:03:38] d2.utils.events INFO:  eta: 0:52:42  iter: 37619  total_loss: 31.35  loss_ce: 0.2145  loss_mask: 0.3169  loss_dice: 2.527  loss_ce_0: 0.559  loss_mask_0: 0.3193  loss_dice_0: 2.657  loss_ce_1: 0.2634  loss_mask_1: 0.3239  loss_dice_1: 2.569  loss_ce_2: 0.2615  loss_mask_2: 0.3191  loss_dice_2: 2.544  loss_ce_3: 0.2525  loss_mask_3: 0.3174  loss_dice_3: 2.533  loss_ce_4: 0.2369  loss_mask_4: 0.3177  loss_dice_4: 2.525  loss_ce_5: 0.213  loss_mask_5: 0.3183  loss_dice_5: 2.522  loss_ce_6: 0.2136  loss_mask_6: 0.3165  loss_dice_6: 2.524  loss_ce_7: 0.2199  loss_mask_7: 0.3166  loss_dice_7: 2.535  loss_ce_8: 0.2198  loss_mask_8: 0.317  loss_dice_8: 2.527  time: 1.4672  data_time: 0.0539  lr: 7.8928e-07  max_mem: 21589M
[01/18 12:04:04] d2.utils.events INFO:  eta: 0:52:16  iter: 37639  total_loss: 30.73  loss_ce: 0.2008  loss_mask: 0.3248  loss_dice: 2.481  loss_ce_0: 0.5595  loss_mask_0: 0.3201  loss_dice_0: 2.622  loss_ce_1: 0.2457  loss_mask_1: 0.3299  loss_dice_1: 2.532  loss_ce_2: 0.2525  loss_mask_2: 0.3274  loss_dice_2: 2.507  loss_ce_3: 0.2298  loss_mask_3: 0.3272  loss_dice_3: 2.494  loss_ce_4: 0.2193  loss_mask_4: 0.3258  loss_dice_4: 2.487  loss_ce_5: 0.2084  loss_mask_5: 0.3256  loss_dice_5: 2.489  loss_ce_6: 0.2221  loss_mask_6: 0.3254  loss_dice_6: 2.483  loss_ce_7: 0.216  loss_mask_7: 0.3244  loss_dice_7: 2.478  loss_ce_8: 0.1925  loss_mask_8: 0.3241  loss_dice_8: 2.487  time: 1.4671  data_time: 0.0523  lr: 7.8331e-07  max_mem: 21589M
[01/18 12:04:31] d2.utils.events INFO:  eta: 0:51:49  iter: 37659  total_loss: 31.3  loss_ce: 0.2132  loss_mask: 0.3271  loss_dice: 2.497  loss_ce_0: 0.5548  loss_mask_0: 0.3232  loss_dice_0: 2.631  loss_ce_1: 0.2623  loss_mask_1: 0.3316  loss_dice_1: 2.546  loss_ce_2: 0.2475  loss_mask_2: 0.3288  loss_dice_2: 2.526  loss_ce_3: 0.2232  loss_mask_3: 0.3267  loss_dice_3: 2.512  loss_ce_4: 0.2268  loss_mask_4: 0.3268  loss_dice_4: 2.503  loss_ce_5: 0.2109  loss_mask_5: 0.3266  loss_dice_5: 2.502  loss_ce_6: 0.2201  loss_mask_6: 0.3263  loss_dice_6: 2.504  loss_ce_7: 0.2109  loss_mask_7: 0.3271  loss_dice_7: 2.502  loss_ce_8: 0.2178  loss_mask_8: 0.3275  loss_dice_8: 2.501  time: 1.4670  data_time: 0.0508  lr: 7.7733e-07  max_mem: 21589M
[01/18 12:04:57] d2.utils.events INFO:  eta: 0:51:22  iter: 37679  total_loss: 31.31  loss_ce: 0.2186  loss_mask: 0.3198  loss_dice: 2.517  loss_ce_0: 0.5618  loss_mask_0: 0.3178  loss_dice_0: 2.65  loss_ce_1: 0.2745  loss_mask_1: 0.3211  loss_dice_1: 2.57  loss_ce_2: 0.2652  loss_mask_2: 0.3187  loss_dice_2: 2.547  loss_ce_3: 0.238  loss_mask_3: 0.3193  loss_dice_3: 2.528  loss_ce_4: 0.2343  loss_mask_4: 0.3185  loss_dice_4: 2.538  loss_ce_5: 0.2311  loss_mask_5: 0.3188  loss_dice_5: 2.521  loss_ce_6: 0.2214  loss_mask_6: 0.3197  loss_dice_6: 2.524  loss_ce_7: 0.2207  loss_mask_7: 0.3185  loss_dice_7: 2.521  loss_ce_8: 0.2162  loss_mask_8: 0.3188  loss_dice_8: 2.517  time: 1.4669  data_time: 0.0538  lr: 7.7135e-07  max_mem: 21589M
[01/18 12:05:24] d2.utils.events INFO:  eta: 0:50:56  iter: 37699  total_loss: 30.79  loss_ce: 0.1934  loss_mask: 0.3212  loss_dice: 2.456  loss_ce_0: 0.5593  loss_mask_0: 0.3252  loss_dice_0: 2.614  loss_ce_1: 0.2462  loss_mask_1: 0.3273  loss_dice_1: 2.521  loss_ce_2: 0.2554  loss_mask_2: 0.3235  loss_dice_2: 2.473  loss_ce_3: 0.2168  loss_mask_3: 0.3226  loss_dice_3: 2.469  loss_ce_4: 0.2139  loss_mask_4: 0.3198  loss_dice_4: 2.463  loss_ce_5: 0.2064  loss_mask_5: 0.3221  loss_dice_5: 2.46  loss_ce_6: 0.2102  loss_mask_6: 0.3209  loss_dice_6: 2.47  loss_ce_7: 0.2057  loss_mask_7: 0.3201  loss_dice_7: 2.458  loss_ce_8: 0.2075  loss_mask_8: 0.3199  loss_dice_8: 2.457  time: 1.4669  data_time: 0.0522  lr: 7.6537e-07  max_mem: 21589M
[01/18 12:05:51] d2.utils.events INFO:  eta: 0:50:30  iter: 37719  total_loss: 32.1  loss_ce: 0.2341  loss_mask: 0.3231  loss_dice: 2.584  loss_ce_0: 0.5253  loss_mask_0: 0.3286  loss_dice_0: 2.724  loss_ce_1: 0.272  loss_mask_1: 0.3315  loss_dice_1: 2.631  loss_ce_2: 0.2638  loss_mask_2: 0.329  loss_dice_2: 2.619  loss_ce_3: 0.2496  loss_mask_3: 0.3262  loss_dice_3: 2.596  loss_ce_4: 0.2234  loss_mask_4: 0.3273  loss_dice_4: 2.597  loss_ce_5: 0.2416  loss_mask_5: 0.3256  loss_dice_5: 2.592  loss_ce_6: 0.2232  loss_mask_6: 0.3254  loss_dice_6: 2.592  loss_ce_7: 0.2235  loss_mask_7: 0.3243  loss_dice_7: 2.59  loss_ce_8: 0.2249  loss_mask_8: 0.3241  loss_dice_8: 2.588  time: 1.4668  data_time: 0.0541  lr: 7.5938e-07  max_mem: 21589M
[01/18 12:06:18] d2.utils.events INFO:  eta: 0:50:03  iter: 37739  total_loss: 32.09  loss_ce: 0.2038  loss_mask: 0.3221  loss_dice: 2.578  loss_ce_0: 0.5711  loss_mask_0: 0.3279  loss_dice_0: 2.73  loss_ce_1: 0.2592  loss_mask_1: 0.3263  loss_dice_1: 2.627  loss_ce_2: 0.2421  loss_mask_2: 0.3246  loss_dice_2: 2.605  loss_ce_3: 0.2302  loss_mask_3: 0.3241  loss_dice_3: 2.585  loss_ce_4: 0.2239  loss_mask_4: 0.3223  loss_dice_4: 2.585  loss_ce_5: 0.2194  loss_mask_5: 0.3206  loss_dice_5: 2.59  loss_ce_6: 0.2135  loss_mask_6: 0.3208  loss_dice_6: 2.585  loss_ce_7: 0.214  loss_mask_7: 0.3222  loss_dice_7: 2.59  loss_ce_8: 0.1954  loss_mask_8: 0.3219  loss_dice_8: 2.584  time: 1.4667  data_time: 0.0557  lr: 7.5338e-07  max_mem: 21589M
[01/18 12:06:45] d2.utils.events INFO:  eta: 0:49:36  iter: 37759  total_loss: 30.59  loss_ce: 0.202  loss_mask: 0.3225  loss_dice: 2.447  loss_ce_0: 0.5375  loss_mask_0: 0.3146  loss_dice_0: 2.593  loss_ce_1: 0.2727  loss_mask_1: 0.3254  loss_dice_1: 2.495  loss_ce_2: 0.2554  loss_mask_2: 0.3213  loss_dice_2: 2.472  loss_ce_3: 0.2372  loss_mask_3: 0.3199  loss_dice_3: 2.453  loss_ce_4: 0.229  loss_mask_4: 0.3205  loss_dice_4: 2.458  loss_ce_5: 0.2131  loss_mask_5: 0.3206  loss_dice_5: 2.455  loss_ce_6: 0.2154  loss_mask_6: 0.3197  loss_dice_6: 2.452  loss_ce_7: 0.2214  loss_mask_7: 0.3209  loss_dice_7: 2.448  loss_ce_8: 0.2177  loss_mask_8: 0.3205  loss_dice_8: 2.451  time: 1.4667  data_time: 0.0502  lr: 7.4738e-07  max_mem: 21589M
[01/18 12:07:12] d2.utils.events INFO:  eta: 0:49:10  iter: 37779  total_loss: 31.23  loss_ce: 0.2162  loss_mask: 0.3239  loss_dice: 2.498  loss_ce_0: 0.5365  loss_mask_0: 0.3239  loss_dice_0: 2.644  loss_ce_1: 0.2733  loss_mask_1: 0.3279  loss_dice_1: 2.555  loss_ce_2: 0.2717  loss_mask_2: 0.3254  loss_dice_2: 2.524  loss_ce_3: 0.2592  loss_mask_3: 0.325  loss_dice_3: 2.511  loss_ce_4: 0.2357  loss_mask_4: 0.3237  loss_dice_4: 2.511  loss_ce_5: 0.236  loss_mask_5: 0.3229  loss_dice_5: 2.494  loss_ce_6: 0.2232  loss_mask_6: 0.323  loss_dice_6: 2.496  loss_ce_7: 0.2317  loss_mask_7: 0.3235  loss_dice_7: 2.503  loss_ce_8: 0.209  loss_mask_8: 0.3234  loss_dice_8: 2.499  time: 1.4666  data_time: 0.0527  lr: 7.4138e-07  max_mem: 21589M
[01/18 12:07:38] d2.utils.events INFO:  eta: 0:48:43  iter: 37799  total_loss: 30.29  loss_ce: 0.2079  loss_mask: 0.3239  loss_dice: 2.442  loss_ce_0: 0.5483  loss_mask_0: 0.3232  loss_dice_0: 2.598  loss_ce_1: 0.2856  loss_mask_1: 0.3292  loss_dice_1: 2.496  loss_ce_2: 0.2566  loss_mask_2: 0.3263  loss_dice_2: 2.475  loss_ce_3: 0.2452  loss_mask_3: 0.3257  loss_dice_3: 2.45  loss_ce_4: 0.2237  loss_mask_4: 0.3239  loss_dice_4: 2.456  loss_ce_5: 0.2138  loss_mask_5: 0.3235  loss_dice_5: 2.452  loss_ce_6: 0.227  loss_mask_6: 0.3227  loss_dice_6: 2.454  loss_ce_7: 0.2139  loss_mask_7: 0.3238  loss_dice_7: 2.456  loss_ce_8: 0.2023  loss_mask_8: 0.3239  loss_dice_8: 2.457  time: 1.4665  data_time: 0.0536  lr: 7.3537e-07  max_mem: 21589M
[01/18 12:08:05] d2.utils.events INFO:  eta: 0:48:15  iter: 37819  total_loss: 31.19  loss_ce: 0.2102  loss_mask: 0.333  loss_dice: 2.509  loss_ce_0: 0.5395  loss_mask_0: 0.3312  loss_dice_0: 2.644  loss_ce_1: 0.2671  loss_mask_1: 0.3408  loss_dice_1: 2.561  loss_ce_2: 0.2553  loss_mask_2: 0.3356  loss_dice_2: 2.535  loss_ce_3: 0.2336  loss_mask_3: 0.3343  loss_dice_3: 2.515  loss_ce_4: 0.2368  loss_mask_4: 0.3333  loss_dice_4: 2.508  loss_ce_5: 0.2161  loss_mask_5: 0.3334  loss_dice_5: 2.515  loss_ce_6: 0.2182  loss_mask_6: 0.332  loss_dice_6: 2.506  loss_ce_7: 0.2163  loss_mask_7: 0.3324  loss_dice_7: 2.513  loss_ce_8: 0.2192  loss_mask_8: 0.3322  loss_dice_8: 2.504  time: 1.4664  data_time: 0.0520  lr: 7.2935e-07  max_mem: 21589M
[01/18 12:08:31] d2.utils.events INFO:  eta: 0:47:48  iter: 37839  total_loss: 30.63  loss_ce: 0.19  loss_mask: 0.3267  loss_dice: 2.482  loss_ce_0: 0.5247  loss_mask_0: 0.3249  loss_dice_0: 2.625  loss_ce_1: 0.2388  loss_mask_1: 0.3338  loss_dice_1: 2.526  loss_ce_2: 0.2314  loss_mask_2: 0.3297  loss_dice_2: 2.506  loss_ce_3: 0.2068  loss_mask_3: 0.3282  loss_dice_3: 2.492  loss_ce_4: 0.2132  loss_mask_4: 0.3281  loss_dice_4: 2.49  loss_ce_5: 0.2045  loss_mask_5: 0.3273  loss_dice_5: 2.486  loss_ce_6: 0.1855  loss_mask_6: 0.3261  loss_dice_6: 2.49  loss_ce_7: 0.1845  loss_mask_7: 0.3263  loss_dice_7: 2.489  loss_ce_8: 0.1927  loss_mask_8: 0.3264  loss_dice_8: 2.49  time: 1.4664  data_time: 0.0516  lr: 7.2333e-07  max_mem: 21589M
[01/18 12:08:58] d2.utils.events INFO:  eta: 0:47:21  iter: 37859  total_loss: 30.87  loss_ce: 0.1854  loss_mask: 0.3219  loss_dice: 2.49  loss_ce_0: 0.5408  loss_mask_0: 0.3218  loss_dice_0: 2.621  loss_ce_1: 0.2419  loss_mask_1: 0.3259  loss_dice_1: 2.532  loss_ce_2: 0.242  loss_mask_2: 0.3216  loss_dice_2: 2.515  loss_ce_3: 0.2323  loss_mask_3: 0.3215  loss_dice_3: 2.496  loss_ce_4: 0.2187  loss_mask_4: 0.3208  loss_dice_4: 2.489  loss_ce_5: 0.2211  loss_mask_5: 0.3196  loss_dice_5: 2.493  loss_ce_6: 0.2072  loss_mask_6: 0.3204  loss_dice_6: 2.491  loss_ce_7: 0.1945  loss_mask_7: 0.3221  loss_dice_7: 2.49  loss_ce_8: 0.1926  loss_mask_8: 0.3221  loss_dice_8: 2.497  time: 1.4663  data_time: 0.0534  lr: 7.173e-07  max_mem: 21589M
[01/18 12:09:24] d2.utils.events INFO:  eta: 0:46:54  iter: 37879  total_loss: 30.26  loss_ce: 0.1992  loss_mask: 0.3225  loss_dice: 2.451  loss_ce_0: 0.562  loss_mask_0: 0.3192  loss_dice_0: 2.616  loss_ce_1: 0.2527  loss_mask_1: 0.3262  loss_dice_1: 2.519  loss_ce_2: 0.239  loss_mask_2: 0.3242  loss_dice_2: 2.488  loss_ce_3: 0.2474  loss_mask_3: 0.3207  loss_dice_3: 2.476  loss_ce_4: 0.2108  loss_mask_4: 0.3221  loss_dice_4: 2.46  loss_ce_5: 0.21  loss_mask_5: 0.3216  loss_dice_5: 2.471  loss_ce_6: 0.2066  loss_mask_6: 0.3218  loss_dice_6: 2.467  loss_ce_7: 0.2002  loss_mask_7: 0.3221  loss_dice_7: 2.47  loss_ce_8: 0.1896  loss_mask_8: 0.3213  loss_dice_8: 2.461  time: 1.4662  data_time: 0.0522  lr: 7.1127e-07  max_mem: 21589M
[01/18 12:09:51] d2.utils.events INFO:  eta: 0:46:27  iter: 37899  total_loss: 31.03  loss_ce: 0.1959  loss_mask: 0.3232  loss_dice: 2.506  loss_ce_0: 0.5671  loss_mask_0: 0.314  loss_dice_0: 2.636  loss_ce_1: 0.2628  loss_mask_1: 0.3237  loss_dice_1: 2.548  loss_ce_2: 0.2592  loss_mask_2: 0.3224  loss_dice_2: 2.522  loss_ce_3: 0.2402  loss_mask_3: 0.3221  loss_dice_3: 2.501  loss_ce_4: 0.2134  loss_mask_4: 0.3222  loss_dice_4: 2.497  loss_ce_5: 0.2147  loss_mask_5: 0.3213  loss_dice_5: 2.503  loss_ce_6: 0.2006  loss_mask_6: 0.3215  loss_dice_6: 2.5  loss_ce_7: 0.2019  loss_mask_7: 0.3225  loss_dice_7: 2.499  loss_ce_8: 0.1953  loss_mask_8: 0.3233  loss_dice_8: 2.496  time: 1.4661  data_time: 0.0558  lr: 7.0523e-07  max_mem: 21589M
[01/18 12:10:17] d2.utils.events INFO:  eta: 0:46:00  iter: 37919  total_loss: 30.46  loss_ce: 0.2097  loss_mask: 0.3147  loss_dice: 2.451  loss_ce_0: 0.5269  loss_mask_0: 0.3094  loss_dice_0: 2.597  loss_ce_1: 0.2567  loss_mask_1: 0.3176  loss_dice_1: 2.505  loss_ce_2: 0.2517  loss_mask_2: 0.3152  loss_dice_2: 2.473  loss_ce_3: 0.2346  loss_mask_3: 0.3167  loss_dice_3: 2.467  loss_ce_4: 0.2191  loss_mask_4: 0.3136  loss_dice_4: 2.46  loss_ce_5: 0.2265  loss_mask_5: 0.3145  loss_dice_5: 2.453  loss_ce_6: 0.2134  loss_mask_6: 0.3144  loss_dice_6: 2.46  loss_ce_7: 0.2065  loss_mask_7: 0.3142  loss_dice_7: 2.457  loss_ce_8: 0.2075  loss_mask_8: 0.3147  loss_dice_8: 2.451  time: 1.4661  data_time: 0.0502  lr: 6.9918e-07  max_mem: 21589M
[01/18 12:10:44] d2.utils.events INFO:  eta: 0:45:33  iter: 37939  total_loss: 31.56  loss_ce: 0.2167  loss_mask: 0.3166  loss_dice: 2.5  loss_ce_0: 0.5826  loss_mask_0: 0.3143  loss_dice_0: 2.647  loss_ce_1: 0.2908  loss_mask_1: 0.3214  loss_dice_1: 2.548  loss_ce_2: 0.2508  loss_mask_2: 0.3183  loss_dice_2: 2.531  loss_ce_3: 0.2309  loss_mask_3: 0.318  loss_dice_3: 2.517  loss_ce_4: 0.2352  loss_mask_4: 0.3173  loss_dice_4: 2.505  loss_ce_5: 0.2274  loss_mask_5: 0.3169  loss_dice_5: 2.507  loss_ce_6: 0.2162  loss_mask_6: 0.3161  loss_dice_6: 2.511  loss_ce_7: 0.2236  loss_mask_7: 0.3164  loss_dice_7: 2.501  loss_ce_8: 0.2176  loss_mask_8: 0.3162  loss_dice_8: 2.503  time: 1.4660  data_time: 0.0542  lr: 6.9313e-07  max_mem: 21589M
[01/18 12:11:10] d2.utils.events INFO:  eta: 0:45:05  iter: 37959  total_loss: 30.79  loss_ce: 0.2097  loss_mask: 0.323  loss_dice: 2.438  loss_ce_0: 0.5368  loss_mask_0: 0.322  loss_dice_0: 2.584  loss_ce_1: 0.2558  loss_mask_1: 0.3293  loss_dice_1: 2.476  loss_ce_2: 0.2649  loss_mask_2: 0.3276  loss_dice_2: 2.464  loss_ce_3: 0.2343  loss_mask_3: 0.3246  loss_dice_3: 2.445  loss_ce_4: 0.2265  loss_mask_4: 0.323  loss_dice_4: 2.446  loss_ce_5: 0.2127  loss_mask_5: 0.3223  loss_dice_5: 2.441  loss_ce_6: 0.2213  loss_mask_6: 0.3213  loss_dice_6: 2.433  loss_ce_7: 0.2146  loss_mask_7: 0.3229  loss_dice_7: 2.44  loss_ce_8: 0.2095  loss_mask_8: 0.3224  loss_dice_8: 2.431  time: 1.4659  data_time: 0.0508  lr: 6.8708e-07  max_mem: 21589M
[01/18 12:11:36] d2.utils.events INFO:  eta: 0:44:36  iter: 37979  total_loss: 31.1  loss_ce: 0.2  loss_mask: 0.3324  loss_dice: 2.476  loss_ce_0: 0.5555  loss_mask_0: 0.3322  loss_dice_0: 2.627  loss_ce_1: 0.2569  loss_mask_1: 0.3378  loss_dice_1: 2.532  loss_ce_2: 0.2518  loss_mask_2: 0.3361  loss_dice_2: 2.509  loss_ce_3: 0.2292  loss_mask_3: 0.3322  loss_dice_3: 2.488  loss_ce_4: 0.223  loss_mask_4: 0.3309  loss_dice_4: 2.483  loss_ce_5: 0.2137  loss_mask_5: 0.3327  loss_dice_5: 2.486  loss_ce_6: 0.2093  loss_mask_6: 0.3322  loss_dice_6: 2.479  loss_ce_7: 0.207  loss_mask_7: 0.3326  loss_dice_7: 2.486  loss_ce_8: 0.2154  loss_mask_8: 0.3312  loss_dice_8: 2.486  time: 1.4658  data_time: 0.0515  lr: 6.8101e-07  max_mem: 21589M
[01/18 12:12:03] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 12:12:03] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 12:12:03] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 12:12:04] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 12:12:14] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0061 s/iter. Inference: 0.1189 s/iter. Eval: 0.1573 s/iter. Total: 0.2823 s/iter. ETA=0:05:05
[01/18 12:12:19] d2.evaluation.evaluator INFO: Inference done 27/1093. Dataloading: 0.0079 s/iter. Inference: 0.1263 s/iter. Eval: 0.1704 s/iter. Total: 0.3047 s/iter. ETA=0:05:24
[01/18 12:12:24] d2.evaluation.evaluator INFO: Inference done 45/1093. Dataloading: 0.0083 s/iter. Inference: 0.1262 s/iter. Eval: 0.1642 s/iter. Total: 0.2988 s/iter. ETA=0:05:13
[01/18 12:12:30] d2.evaluation.evaluator INFO: Inference done 61/1093. Dataloading: 0.0089 s/iter. Inference: 0.1242 s/iter. Eval: 0.1705 s/iter. Total: 0.3036 s/iter. ETA=0:05:13
[01/18 12:12:35] d2.evaluation.evaluator INFO: Inference done 78/1093. Dataloading: 0.0088 s/iter. Inference: 0.1251 s/iter. Eval: 0.1687 s/iter. Total: 0.3026 s/iter. ETA=0:05:07
[01/18 12:12:40] d2.evaluation.evaluator INFO: Inference done 93/1093. Dataloading: 0.0090 s/iter. Inference: 0.1273 s/iter. Eval: 0.1725 s/iter. Total: 0.3088 s/iter. ETA=0:05:08
[01/18 12:12:45] d2.evaluation.evaluator INFO: Inference done 111/1093. Dataloading: 0.0088 s/iter. Inference: 0.1291 s/iter. Eval: 0.1677 s/iter. Total: 0.3056 s/iter. ETA=0:05:00
[01/18 12:12:50] d2.evaluation.evaluator INFO: Inference done 128/1093. Dataloading: 0.0086 s/iter. Inference: 0.1297 s/iter. Eval: 0.1666 s/iter. Total: 0.3050 s/iter. ETA=0:04:54
[01/18 12:12:55] d2.evaluation.evaluator INFO: Inference done 148/1093. Dataloading: 0.0084 s/iter. Inference: 0.1290 s/iter. Eval: 0.1613 s/iter. Total: 0.2987 s/iter. ETA=0:04:42
[01/18 12:13:00] d2.evaluation.evaluator INFO: Inference done 164/1093. Dataloading: 0.0084 s/iter. Inference: 0.1307 s/iter. Eval: 0.1610 s/iter. Total: 0.3002 s/iter. ETA=0:04:38
[01/18 12:13:05] d2.evaluation.evaluator INFO: Inference done 181/1093. Dataloading: 0.0084 s/iter. Inference: 0.1306 s/iter. Eval: 0.1608 s/iter. Total: 0.3000 s/iter. ETA=0:04:33
[01/18 12:13:10] d2.evaluation.evaluator INFO: Inference done 199/1093. Dataloading: 0.0083 s/iter. Inference: 0.1309 s/iter. Eval: 0.1589 s/iter. Total: 0.2982 s/iter. ETA=0:04:26
[01/18 12:13:15] d2.evaluation.evaluator INFO: Inference done 215/1093. Dataloading: 0.0084 s/iter. Inference: 0.1312 s/iter. Eval: 0.1598 s/iter. Total: 0.2994 s/iter. ETA=0:04:22
[01/18 12:13:20] d2.evaluation.evaluator INFO: Inference done 232/1093. Dataloading: 0.0084 s/iter. Inference: 0.1310 s/iter. Eval: 0.1598 s/iter. Total: 0.2993 s/iter. ETA=0:04:17
[01/18 12:13:26] d2.evaluation.evaluator INFO: Inference done 249/1093. Dataloading: 0.0084 s/iter. Inference: 0.1305 s/iter. Eval: 0.1607 s/iter. Total: 0.2998 s/iter. ETA=0:04:12
[01/18 12:13:31] d2.evaluation.evaluator INFO: Inference done 265/1093. Dataloading: 0.0084 s/iter. Inference: 0.1315 s/iter. Eval: 0.1607 s/iter. Total: 0.3006 s/iter. ETA=0:04:08
[01/18 12:13:36] d2.evaluation.evaluator INFO: Inference done 282/1093. Dataloading: 0.0083 s/iter. Inference: 0.1323 s/iter. Eval: 0.1604 s/iter. Total: 0.3011 s/iter. ETA=0:04:04
[01/18 12:13:41] d2.evaluation.evaluator INFO: Inference done 300/1093. Dataloading: 0.0083 s/iter. Inference: 0.1318 s/iter. Eval: 0.1598 s/iter. Total: 0.2999 s/iter. ETA=0:03:57
[01/18 12:13:46] d2.evaluation.evaluator INFO: Inference done 316/1093. Dataloading: 0.0083 s/iter. Inference: 0.1316 s/iter. Eval: 0.1613 s/iter. Total: 0.3012 s/iter. ETA=0:03:53
[01/18 12:13:51] d2.evaluation.evaluator INFO: Inference done 334/1093. Dataloading: 0.0082 s/iter. Inference: 0.1319 s/iter. Eval: 0.1604 s/iter. Total: 0.3006 s/iter. ETA=0:03:48
[01/18 12:13:57] d2.evaluation.evaluator INFO: Inference done 354/1093. Dataloading: 0.0081 s/iter. Inference: 0.1316 s/iter. Eval: 0.1583 s/iter. Total: 0.2981 s/iter. ETA=0:03:40
[01/18 12:14:02] d2.evaluation.evaluator INFO: Inference done 371/1093. Dataloading: 0.0081 s/iter. Inference: 0.1317 s/iter. Eval: 0.1587 s/iter. Total: 0.2986 s/iter. ETA=0:03:35
[01/18 12:14:07] d2.evaluation.evaluator INFO: Inference done 390/1093. Dataloading: 0.0081 s/iter. Inference: 0.1313 s/iter. Eval: 0.1580 s/iter. Total: 0.2975 s/iter. ETA=0:03:29
[01/18 12:14:12] d2.evaluation.evaluator INFO: Inference done 408/1093. Dataloading: 0.0081 s/iter. Inference: 0.1313 s/iter. Eval: 0.1579 s/iter. Total: 0.2973 s/iter. ETA=0:03:23
[01/18 12:14:18] d2.evaluation.evaluator INFO: Inference done 425/1093. Dataloading: 0.0081 s/iter. Inference: 0.1312 s/iter. Eval: 0.1586 s/iter. Total: 0.2980 s/iter. ETA=0:03:19
[01/18 12:14:23] d2.evaluation.evaluator INFO: Inference done 443/1093. Dataloading: 0.0081 s/iter. Inference: 0.1314 s/iter. Eval: 0.1581 s/iter. Total: 0.2977 s/iter. ETA=0:03:13
[01/18 12:14:28] d2.evaluation.evaluator INFO: Inference done 461/1093. Dataloading: 0.0080 s/iter. Inference: 0.1313 s/iter. Eval: 0.1579 s/iter. Total: 0.2973 s/iter. ETA=0:03:07
[01/18 12:14:33] d2.evaluation.evaluator INFO: Inference done 481/1093. Dataloading: 0.0080 s/iter. Inference: 0.1310 s/iter. Eval: 0.1568 s/iter. Total: 0.2958 s/iter. ETA=0:03:01
[01/18 12:14:38] d2.evaluation.evaluator INFO: Inference done 500/1093. Dataloading: 0.0079 s/iter. Inference: 0.1306 s/iter. Eval: 0.1561 s/iter. Total: 0.2946 s/iter. ETA=0:02:54
[01/18 12:14:43] d2.evaluation.evaluator INFO: Inference done 519/1093. Dataloading: 0.0079 s/iter. Inference: 0.1307 s/iter. Eval: 0.1550 s/iter. Total: 0.2937 s/iter. ETA=0:02:48
[01/18 12:14:49] d2.evaluation.evaluator INFO: Inference done 534/1093. Dataloading: 0.0079 s/iter. Inference: 0.1312 s/iter. Eval: 0.1559 s/iter. Total: 0.2951 s/iter. ETA=0:02:44
[01/18 12:14:54] d2.evaluation.evaluator INFO: Inference done 551/1093. Dataloading: 0.0079 s/iter. Inference: 0.1314 s/iter. Eval: 0.1560 s/iter. Total: 0.2954 s/iter. ETA=0:02:40
[01/18 12:14:59] d2.evaluation.evaluator INFO: Inference done 569/1093. Dataloading: 0.0079 s/iter. Inference: 0.1315 s/iter. Eval: 0.1555 s/iter. Total: 0.2950 s/iter. ETA=0:02:34
[01/18 12:15:04] d2.evaluation.evaluator INFO: Inference done 590/1093. Dataloading: 0.0079 s/iter. Inference: 0.1311 s/iter. Eval: 0.1540 s/iter. Total: 0.2931 s/iter. ETA=0:02:27
[01/18 12:15:09] d2.evaluation.evaluator INFO: Inference done 606/1093. Dataloading: 0.0079 s/iter. Inference: 0.1309 s/iter. Eval: 0.1547 s/iter. Total: 0.2936 s/iter. ETA=0:02:22
[01/18 12:15:14] d2.evaluation.evaluator INFO: Inference done 624/1093. Dataloading: 0.0079 s/iter. Inference: 0.1305 s/iter. Eval: 0.1547 s/iter. Total: 0.2932 s/iter. ETA=0:02:17
[01/18 12:15:19] d2.evaluation.evaluator INFO: Inference done 643/1093. Dataloading: 0.0079 s/iter. Inference: 0.1302 s/iter. Eval: 0.1543 s/iter. Total: 0.2925 s/iter. ETA=0:02:11
[01/18 12:15:24] d2.evaluation.evaluator INFO: Inference done 663/1093. Dataloading: 0.0079 s/iter. Inference: 0.1298 s/iter. Eval: 0.1538 s/iter. Total: 0.2915 s/iter. ETA=0:02:05
[01/18 12:15:29] d2.evaluation.evaluator INFO: Inference done 683/1093. Dataloading: 0.0078 s/iter. Inference: 0.1296 s/iter. Eval: 0.1530 s/iter. Total: 0.2905 s/iter. ETA=0:01:59
[01/18 12:15:34] d2.evaluation.evaluator INFO: Inference done 702/1093. Dataloading: 0.0078 s/iter. Inference: 0.1292 s/iter. Eval: 0.1526 s/iter. Total: 0.2898 s/iter. ETA=0:01:53
[01/18 12:15:40] d2.evaluation.evaluator INFO: Inference done 718/1093. Dataloading: 0.0079 s/iter. Inference: 0.1291 s/iter. Eval: 0.1535 s/iter. Total: 0.2906 s/iter. ETA=0:01:48
[01/18 12:15:45] d2.evaluation.evaluator INFO: Inference done 738/1093. Dataloading: 0.0078 s/iter. Inference: 0.1289 s/iter. Eval: 0.1527 s/iter. Total: 0.2895 s/iter. ETA=0:01:42
[01/18 12:15:50] d2.evaluation.evaluator INFO: Inference done 757/1093. Dataloading: 0.0078 s/iter. Inference: 0.1290 s/iter. Eval: 0.1522 s/iter. Total: 0.2891 s/iter. ETA=0:01:37
[01/18 12:15:55] d2.evaluation.evaluator INFO: Inference done 775/1093. Dataloading: 0.0078 s/iter. Inference: 0.1288 s/iter. Eval: 0.1523 s/iter. Total: 0.2890 s/iter. ETA=0:01:31
[01/18 12:16:00] d2.evaluation.evaluator INFO: Inference done 796/1093. Dataloading: 0.0078 s/iter. Inference: 0.1285 s/iter. Eval: 0.1516 s/iter. Total: 0.2880 s/iter. ETA=0:01:25
[01/18 12:16:06] d2.evaluation.evaluator INFO: Inference done 815/1093. Dataloading: 0.0077 s/iter. Inference: 0.1286 s/iter. Eval: 0.1513 s/iter. Total: 0.2877 s/iter. ETA=0:01:19
[01/18 12:16:11] d2.evaluation.evaluator INFO: Inference done 835/1093. Dataloading: 0.0077 s/iter. Inference: 0.1285 s/iter. Eval: 0.1506 s/iter. Total: 0.2869 s/iter. ETA=0:01:14
[01/18 12:16:16] d2.evaluation.evaluator INFO: Inference done 853/1093. Dataloading: 0.0077 s/iter. Inference: 0.1287 s/iter. Eval: 0.1505 s/iter. Total: 0.2869 s/iter. ETA=0:01:08
[01/18 12:16:21] d2.evaluation.evaluator INFO: Inference done 870/1093. Dataloading: 0.0077 s/iter. Inference: 0.1288 s/iter. Eval: 0.1508 s/iter. Total: 0.2874 s/iter. ETA=0:01:04
[01/18 12:16:26] d2.evaluation.evaluator INFO: Inference done 887/1093. Dataloading: 0.0077 s/iter. Inference: 0.1288 s/iter. Eval: 0.1511 s/iter. Total: 0.2877 s/iter. ETA=0:00:59
[01/18 12:16:31] d2.evaluation.evaluator INFO: Inference done 906/1093. Dataloading: 0.0077 s/iter. Inference: 0.1288 s/iter. Eval: 0.1509 s/iter. Total: 0.2875 s/iter. ETA=0:00:53
[01/18 12:16:36] d2.evaluation.evaluator INFO: Inference done 923/1093. Dataloading: 0.0077 s/iter. Inference: 0.1289 s/iter. Eval: 0.1510 s/iter. Total: 0.2876 s/iter. ETA=0:00:48
[01/18 12:16:42] d2.evaluation.evaluator INFO: Inference done 941/1093. Dataloading: 0.0077 s/iter. Inference: 0.1289 s/iter. Eval: 0.1510 s/iter. Total: 0.2876 s/iter. ETA=0:00:43
[01/18 12:16:47] d2.evaluation.evaluator INFO: Inference done 958/1093. Dataloading: 0.0077 s/iter. Inference: 0.1288 s/iter. Eval: 0.1515 s/iter. Total: 0.2880 s/iter. ETA=0:00:38
[01/18 12:16:52] d2.evaluation.evaluator INFO: Inference done 977/1093. Dataloading: 0.0076 s/iter. Inference: 0.1286 s/iter. Eval: 0.1513 s/iter. Total: 0.2876 s/iter. ETA=0:00:33
[01/18 12:16:57] d2.evaluation.evaluator INFO: Inference done 996/1093. Dataloading: 0.0076 s/iter. Inference: 0.1287 s/iter. Eval: 0.1508 s/iter. Total: 0.2872 s/iter. ETA=0:00:27
[01/18 12:17:02] d2.evaluation.evaluator INFO: Inference done 1014/1093. Dataloading: 0.0076 s/iter. Inference: 0.1289 s/iter. Eval: 0.1506 s/iter. Total: 0.2872 s/iter. ETA=0:00:22
[01/18 12:17:07] d2.evaluation.evaluator INFO: Inference done 1032/1093. Dataloading: 0.0076 s/iter. Inference: 0.1289 s/iter. Eval: 0.1506 s/iter. Total: 0.2871 s/iter. ETA=0:00:17
[01/18 12:17:12] d2.evaluation.evaluator INFO: Inference done 1050/1093. Dataloading: 0.0076 s/iter. Inference: 0.1289 s/iter. Eval: 0.1504 s/iter. Total: 0.2870 s/iter. ETA=0:00:12
[01/18 12:17:18] d2.evaluation.evaluator INFO: Inference done 1070/1093. Dataloading: 0.0076 s/iter. Inference: 0.1289 s/iter. Eval: 0.1500 s/iter. Total: 0.2866 s/iter. ETA=0:00:06
[01/18 12:17:23] d2.evaluation.evaluator INFO: Inference done 1090/1093. Dataloading: 0.0076 s/iter. Inference: 0.1289 s/iter. Eval: 0.1495 s/iter. Total: 0.2860 s/iter. ETA=0:00:00
[01/18 12:17:24] d2.evaluation.evaluator INFO: Total inference time: 0:05:11.402806 (0.286216 s / iter per device, on 4 devices)
[01/18 12:17:24] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:20 (0.128831 s / iter per device, on 4 devices)
[01/18 12:17:43] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.308200786062091, 'mIoU': 21.12200203586934, 'fwIoU': 44.24962899146961, 'IoU-0': nan, 'IoU-1': 95.39276078858225, 'IoU-2': 47.29200838833419, 'IoU-3': 59.71609404257757, 'IoU-4': 53.569843247234886, 'IoU-5': 46.91712872598299, 'IoU-6': 41.7755309639037, 'IoU-7': 33.915582354808, 'IoU-8': 25.9019918849291, 'IoU-9': 35.29201143161565, 'IoU-10': 40.45143125331367, 'IoU-11': 50.411349254705875, 'IoU-12': 51.854179615271, 'IoU-13': 51.37567168476267, 'IoU-14': 51.87186810515021, 'IoU-15': 51.931056917939536, 'IoU-16': 52.62524399970796, 'IoU-17': 48.36967254182953, 'IoU-18': 49.71227789083508, 'IoU-19': 49.10214001584988, 'IoU-20': 49.458359782666605, 'IoU-21': 49.225369488088795, 'IoU-22': 50.147438646458575, 'IoU-23': 47.91360475102331, 'IoU-24': 47.29610602081777, 'IoU-25': 47.88814624956409, 'IoU-26': 46.71692892959036, 'IoU-27': 48.17026398476968, 'IoU-28': 46.743834936441104, 'IoU-29': 47.90966403005245, 'IoU-30': 46.488307142397275, 'IoU-31': 46.89315089222743, 'IoU-32': 46.0915979078174, 'IoU-33': 44.364754075148255, 'IoU-34': 43.666331406823694, 'IoU-35': 45.49851196370135, 'IoU-36': 45.83558624748765, 'IoU-37': 44.208640870057515, 'IoU-38': 44.173185292863984, 'IoU-39': 43.66442352128298, 'IoU-40': 44.26815196289588, 'IoU-41': 41.91693830770177, 'IoU-42': 41.36640541368391, 'IoU-43': 40.69778806806424, 'IoU-44': 40.37407969490191, 'IoU-45': 39.0208813521158, 'IoU-46': 38.5826360962751, 'IoU-47': 37.89158188715838, 'IoU-48': 38.48223330704661, 'IoU-49': 37.279260359478315, 'IoU-50': 37.36755394392793, 'IoU-51': 35.94732957877498, 'IoU-52': 35.47174560131076, 'IoU-53': 34.855815785187524, 'IoU-54': 34.65306400024251, 'IoU-55': 33.67754139790822, 'IoU-56': 32.381422868839216, 'IoU-57': 31.541881327987507, 'IoU-58': 29.824782696067842, 'IoU-59': 28.50047563031724, 'IoU-60': 27.6724984725349, 'IoU-61': 27.296383394683744, 'IoU-62': 27.46347025120078, 'IoU-63': 26.3833783178814, 'IoU-64': 26.07786021172016, 'IoU-65': 24.986191555477273, 'IoU-66': 23.4532259908947, 'IoU-67': 23.47584490510725, 'IoU-68': 22.76180217592472, 'IoU-69': 22.518237378629976, 'IoU-70': 22.790378346124317, 'IoU-71': 20.488134273536136, 'IoU-72': 20.875686124901847, 'IoU-73': 20.94450279504597, 'IoU-74': 20.89012512821859, 'IoU-75': 20.87542710550487, 'IoU-76': 20.326010852412903, 'IoU-77': 20.11241724206651, 'IoU-78': 19.621878641474094, 'IoU-79': 19.41260789646385, 'IoU-80': 18.968800275269302, 'IoU-81': 19.517797980728517, 'IoU-82': 19.136458256444588, 'IoU-83': 19.004195376392776, 'IoU-84': 17.73820189668804, 'IoU-85': 18.693782215238357, 'IoU-86': 17.613862160756213, 'IoU-87': 17.34702584487115, 'IoU-88': 17.70547888016252, 'IoU-89': 17.43044586156371, 'IoU-90': 17.43120949638371, 'IoU-91': 16.901336391316065, 'IoU-92': 16.516044451816658, 'IoU-93': 17.007548229587023, 'IoU-94': 17.2803918790651, 'IoU-95': 17.33163608581671, 'IoU-96': 17.150125699316934, 'IoU-97': 16.6763185444757, 'IoU-98': 16.734746520388395, 'IoU-99': 15.920929292531937, 'IoU-100': 15.905795785612106, 'IoU-101': 15.373756635083542, 'IoU-102': 14.701895640896506, 'IoU-103': 14.835238158785232, 'IoU-104': 14.152141571720605, 'IoU-105': 13.773566859615022, 'IoU-106': 13.807721189646166, 'IoU-107': 13.959321476880177, 'IoU-108': 13.212046317222887, 'IoU-109': 13.468663982951801, 'IoU-110': 13.232204804755579, 'IoU-111': 12.913393396205667, 'IoU-112': 12.745017004779886, 'IoU-113': 12.75018749710434, 'IoU-114': 13.015737403402602, 'IoU-115': 12.45724882618948, 'IoU-116': 11.356933034206493, 'IoU-117': 12.309545829632789, 'IoU-118': 10.926110362613697, 'IoU-119': 12.971000391484413, 'IoU-120': 11.336233099515907, 'IoU-121': 10.872515583829305, 'IoU-122': 10.883525833706166, 'IoU-123': 10.75085690139766, 'IoU-124': 10.672020581282158, 'IoU-125': 9.903645094107079, 'IoU-126': 9.44286173737466, 'IoU-127': 9.634118053615978, 'IoU-128': 9.197502589022545, 'IoU-129': 8.812701419423748, 'IoU-130': 8.029312606158005, 'IoU-131': 8.253387959548354, 'IoU-132': 7.827787946794174, 'IoU-133': 7.365881425236695, 'IoU-134': 7.696153397009977, 'IoU-135': 6.990720044767623, 'IoU-136': 7.206725483813512, 'IoU-137': 7.01375020199117, 'IoU-138': 7.892514171896307, 'IoU-139': 6.374679219398044, 'IoU-140': 6.286324366991056, 'IoU-141': 7.156124439470596, 'IoU-142': 5.855384197030138, 'IoU-143': 6.026680743291097, 'IoU-144': 6.12018457979322, 'IoU-145': 6.612014466021245, 'IoU-146': 5.593011783118047, 'IoU-147': 4.6760694127989995, 'IoU-148': 6.610929021772217, 'IoU-149': 4.742257606381801, 'IoU-150': 5.558565959012877, 'IoU-151': 4.335337204292251, 'IoU-152': 3.9887105830895173, 'IoU-153': 4.3194448055123305, 'IoU-154': 3.3825720841119282, 'IoU-155': 3.713629172255317, 'IoU-156': 3.8441717191145046, 'IoU-157': 3.047851124017366, 'IoU-158': 3.937762169069266, 'IoU-159': 2.4154765000209437, 'IoU-160': 4.425823140935637, 'IoU-161': 4.213645757689035, 'IoU-162': 2.5966299755737507, 'IoU-163': 2.8909766726990744, 'IoU-164': 2.49533421515157, 'IoU-165': 3.569782524566381, 'IoU-166': 2.138732083872413, 'IoU-167': 3.063894921504217, 'IoU-168': 2.5488086308660822, 'IoU-169': 1.9136188858088168, 'IoU-170': 2.6389614423470613, 'IoU-171': 1.5215584557916944, 'IoU-172': 2.4650035091168547, 'IoU-173': 2.70099551312138, 'IoU-174': 2.476381857398425, 'IoU-175': 1.3810050972615906, 'IoU-176': 1.2686636346403701, 'IoU-177': 1.9502375263640257, 'IoU-178': 1.759322188634842, 'IoU-179': 2.5489053475080086, 'IoU-180': 2.645576707726764, 'IoU-181': 2.01303888733484, 'IoU-182': 1.0458799075800422, 'IoU-183': 1.4892304553446045, 'IoU-184': 2.406084503676392, 'IoU-185': 2.077156953971998, 'IoU-186': 2.331519391897728, 'IoU-187': 2.4068055309164302, 'IoU-188': 2.500801657467243, 'IoU-189': 2.3544837636485143, 'IoU-190': 2.8058052038206394, 'IoU-191': 2.944788125847081, 'mACC': 31.546165275017472, 'pACC': 58.647461139547076, 'ACC-0': nan, 'ACC-1': 98.73567487957098, 'ACC-2': 59.567587596293805, 'ACC-3': 74.6520516417062, 'ACC-4': 70.23414420901739, 'ACC-5': 64.29898223447587, 'ACC-6': 58.43193863102907, 'ACC-7': 48.709029319856626, 'ACC-8': 33.61015893320541, 'ACC-9': 45.772616949500154, 'ACC-10': 55.94058085036613, 'ACC-11': 65.6898316877524, 'ACC-12': 70.068748108596, 'ACC-13': 68.95928565342527, 'ACC-14': 70.10509149742859, 'ACC-15': 69.23516943693484, 'ACC-16': 68.79358327408599, 'ACC-17': 65.49708134191859, 'ACC-18': 67.07758578105648, 'ACC-19': 66.39397799103538, 'ACC-20': 65.43050424974678, 'ACC-21': 65.77056297424893, 'ACC-22': 67.29875122692928, 'ACC-23': 64.7528138284831, 'ACC-24': 63.882476156082454, 'ACC-25': 64.39990385746293, 'ACC-26': 63.5023343188586, 'ACC-27': 64.90608718355158, 'ACC-28': 64.80532150116598, 'ACC-29': 64.311459369057, 'ACC-30': 64.31717036749103, 'ACC-31': 63.050352057892454, 'ACC-32': 63.47739065569472, 'ACC-33': 61.53136377208678, 'ACC-34': 61.37246103666926, 'ACC-35': 62.10390095530845, 'ACC-36': 62.65454545255253, 'ACC-37': 62.22364062172983, 'ACC-38': 60.912047324804284, 'ACC-39': 60.44084970827084, 'ACC-40': 60.529563016139946, 'ACC-41': 59.013436236497995, 'ACC-42': 58.273629567238686, 'ACC-43': 57.83444903753424, 'ACC-44': 57.442320642818345, 'ACC-45': 55.81852293926078, 'ACC-46': 56.28633274417131, 'ACC-47': 54.63903296411555, 'ACC-48': 55.43439829735999, 'ACC-49': 55.088409641653634, 'ACC-50': 55.108221772902645, 'ACC-51': 51.991263010259395, 'ACC-52': 52.20647536305807, 'ACC-53': 51.49658681202611, 'ACC-54': 50.77933003328726, 'ACC-55': 50.581978214664815, 'ACC-56': 48.984772348110965, 'ACC-57': 47.99448144029898, 'ACC-58': 45.97259637483179, 'ACC-59': 44.83900970145442, 'ACC-60': 43.65314185017242, 'ACC-61': 42.255177838796534, 'ACC-62': 42.39113214438183, 'ACC-63': 41.15658022302989, 'ACC-64': 40.74545062246882, 'ACC-65': 40.08925325698222, 'ACC-66': 38.84203634377488, 'ACC-67': 38.13225772116681, 'ACC-68': 37.4336764980479, 'ACC-69': 36.33845808708131, 'ACC-70': 36.476435181870826, 'ACC-71': 34.86223935405729, 'ACC-72': 34.22902150391065, 'ACC-73': 34.01585763793281, 'ACC-74': 33.28961284108002, 'ACC-75': 35.23541096316745, 'ACC-76': 32.599439370081406, 'ACC-77': 32.92925479969932, 'ACC-78': 32.72317616703414, 'ACC-79': 31.906165791930235, 'ACC-80': 31.419035961814973, 'ACC-81': 32.207747887297415, 'ACC-82': 31.41523302600434, 'ACC-83': 30.729234545405777, 'ACC-84': 31.25286307278894, 'ACC-85': 30.78516002375626, 'ACC-86': 30.888759370254842, 'ACC-87': 29.47428229138221, 'ACC-88': 30.64921197228009, 'ACC-89': 30.11412332709814, 'ACC-90': 29.07844933417538, 'ACC-91': 29.758673590607014, 'ACC-92': 28.47305734501128, 'ACC-93': 27.91587868491499, 'ACC-94': 30.37757176116832, 'ACC-95': 29.22878845590851, 'ACC-96': 30.1703167879266, 'ACC-97': 27.929460013296058, 'ACC-98': 27.570412256356917, 'ACC-99': 27.784484347893834, 'ACC-100': 27.347651983096906, 'ACC-101': 26.855985769579295, 'ACC-102': 24.562550974173085, 'ACC-103': 24.990979503164258, 'ACC-104': 26.291630801013255, 'ACC-105': 23.023426332590226, 'ACC-106': 23.944228153548792, 'ACC-107': 23.427389701882866, 'ACC-108': 21.912090529945747, 'ACC-109': 22.4113125333991, 'ACC-110': 22.79874947646446, 'ACC-111': 22.817178077151734, 'ACC-112': 23.20326664838219, 'ACC-113': 21.854142500706555, 'ACC-114': 23.984392111215865, 'ACC-115': 22.501616225399545, 'ACC-116': 20.18048518215161, 'ACC-117': 22.63364051410923, 'ACC-118': 18.25416919112717, 'ACC-119': 24.300364952389774, 'ACC-120': 20.068118646271486, 'ACC-121': 20.70813993469375, 'ACC-122': 19.073141045599932, 'ACC-123': 18.55456915658941, 'ACC-124': 19.624090552399682, 'ACC-125': 19.372924514038527, 'ACC-126': 16.94670488802266, 'ACC-127': 17.82975030137652, 'ACC-128': 16.898282820521267, 'ACC-129': 14.994487182463907, 'ACC-130': 15.737353699151024, 'ACC-131': 15.120424958114937, 'ACC-132': 14.605932047196335, 'ACC-133': 12.476610395530969, 'ACC-134': 14.220146488582508, 'ACC-135': 12.392038652300325, 'ACC-136': 13.66714323984346, 'ACC-137': 13.018988457637171, 'ACC-138': 15.269008427037592, 'ACC-139': 11.184559496881747, 'ACC-140': 11.991893149927032, 'ACC-141': 12.398445481612889, 'ACC-142': 10.192208100703805, 'ACC-143': 11.469708937298996, 'ACC-144': 11.594314079422382, 'ACC-145': 13.203873074084841, 'ACC-146': 10.816476662630508, 'ACC-147': 7.595663069890611, 'ACC-148': 13.500961455290666, 'ACC-149': 7.602206336473411, 'ACC-150': 11.39774590398881, 'ACC-151': 7.4315531667950925, 'ACC-152': 7.110539314856025, 'ACC-153': 7.99116876454314, 'ACC-154': 6.716474866350346, 'ACC-155': 6.842255764020874, 'ACC-156': 7.41556619933164, 'ACC-157': 5.386174297329778, 'ACC-158': 6.845661074502753, 'ACC-159': 4.332878728164066, 'ACC-160': 7.126153689544405, 'ACC-161': 6.347082232213818, 'ACC-162': 4.356874867971235, 'ACC-163': 4.439539139088336, 'ACC-164': 4.366927850481515, 'ACC-165': 5.830662554355249, 'ACC-166': 3.4773484651350195, 'ACC-167': 5.354831873287855, 'ACC-168': 5.610099163091638, 'ACC-169': 2.8914199683098407, 'ACC-170': 6.634119851782444, 'ACC-171': 2.4481593801718926, 'ACC-172': 5.665814720154091, 'ACC-173': 6.074061252365919, 'ACC-174': 3.914211314100593, 'ACC-175': 2.0988058327278374, 'ACC-176': 1.6631718713721848, 'ACC-177': 3.597179392067086, 'ACC-178': 2.6800666781273725, 'ACC-179': 5.138359969242068, 'ACC-180': 5.4404145077720205, 'ACC-181': 3.7841589270952505, 'ACC-182': 1.388806626090706, 'ACC-183': 2.1996416724151326, 'ACC-184': 5.318890516182134, 'ACC-185': 3.547062437352475, 'ACC-186': 4.778890166260242, 'ACC-187': 3.47455272472487, 'ACC-188': 6.825289536338324, 'ACC-189': 10.012810605846669, 'ACC-190': 7.921282371065954, 'ACC-191': 10.25905383360522})])
[01/18 12:19:51] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 12:19:51] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 12:19:51] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 12:19:51] d2.evaluation.testing INFO: copypaste: 2.3082,21.1220,44.2496,31.5462,58.6475
[01/18 12:19:52] d2.utils.events INFO:  eta: 0:44:13  iter: 37999  total_loss: 31.11  loss_ce: 0.2041  loss_mask: 0.3221  loss_dice: 2.512  loss_ce_0: 0.5646  loss_mask_0: 0.321  loss_dice_0: 2.649  loss_ce_1: 0.2718  loss_mask_1: 0.3296  loss_dice_1: 2.565  loss_ce_2: 0.2672  loss_mask_2: 0.3244  loss_dice_2: 2.535  loss_ce_3: 0.2326  loss_mask_3: 0.3238  loss_dice_3: 2.521  loss_ce_4: 0.2372  loss_mask_4: 0.3217  loss_dice_4: 2.512  loss_ce_5: 0.2224  loss_mask_5: 0.3209  loss_dice_5: 2.524  loss_ce_6: 0.2176  loss_mask_6: 0.3218  loss_dice_6: 2.504  loss_ce_7: 0.2119  loss_mask_7: 0.3203  loss_dice_7: 2.513  loss_ce_8: 0.2147  loss_mask_8: 0.3217  loss_dice_8: 2.507  time: 1.4658  data_time: 0.0563  lr: 6.7495e-07  max_mem: 21589M
[01/18 12:20:19] d2.utils.events INFO:  eta: 0:43:46  iter: 38019  total_loss: 31.17  loss_ce: 0.2099  loss_mask: 0.3189  loss_dice: 2.49  loss_ce_0: 0.5519  loss_mask_0: 0.3173  loss_dice_0: 2.643  loss_ce_1: 0.2531  loss_mask_1: 0.3247  loss_dice_1: 2.537  loss_ce_2: 0.2384  loss_mask_2: 0.322  loss_dice_2: 2.521  loss_ce_3: 0.2108  loss_mask_3: 0.3203  loss_dice_3: 2.504  loss_ce_4: 0.2224  loss_mask_4: 0.3188  loss_dice_4: 2.484  loss_ce_5: 0.2102  loss_mask_5: 0.3189  loss_dice_5: 2.493  loss_ce_6: 0.1977  loss_mask_6: 0.318  loss_dice_6: 2.489  loss_ce_7: 0.2052  loss_mask_7: 0.3177  loss_dice_7: 2.488  loss_ce_8: 0.2037  loss_mask_8: 0.3181  loss_dice_8: 2.486  time: 1.4657  data_time: 0.0534  lr: 6.6887e-07  max_mem: 21589M
[01/18 12:20:45] d2.utils.events INFO:  eta: 0:43:20  iter: 38039  total_loss: 31.03  loss_ce: 0.2084  loss_mask: 0.3236  loss_dice: 2.515  loss_ce_0: 0.5433  loss_mask_0: 0.3201  loss_dice_0: 2.64  loss_ce_1: 0.2648  loss_mask_1: 0.3271  loss_dice_1: 2.563  loss_ce_2: 0.2619  loss_mask_2: 0.3269  loss_dice_2: 2.54  loss_ce_3: 0.2287  loss_mask_3: 0.3257  loss_dice_3: 2.519  loss_ce_4: 0.235  loss_mask_4: 0.3256  loss_dice_4: 2.517  loss_ce_5: 0.2215  loss_mask_5: 0.3243  loss_dice_5: 2.522  loss_ce_6: 0.2127  loss_mask_6: 0.3221  loss_dice_6: 2.508  loss_ce_7: 0.2096  loss_mask_7: 0.3224  loss_dice_7: 2.516  loss_ce_8: 0.2022  loss_mask_8: 0.3238  loss_dice_8: 2.513  time: 1.4656  data_time: 0.0577  lr: 6.6279e-07  max_mem: 21589M
[01/18 12:21:12] d2.utils.events INFO:  eta: 0:42:53  iter: 38059  total_loss: 31.19  loss_ce: 0.2085  loss_mask: 0.3238  loss_dice: 2.538  loss_ce_0: 0.5454  loss_mask_0: 0.3198  loss_dice_0: 2.676  loss_ce_1: 0.2774  loss_mask_1: 0.3237  loss_dice_1: 2.567  loss_ce_2: 0.2584  loss_mask_2: 0.3246  loss_dice_2: 2.543  loss_ce_3: 0.2371  loss_mask_3: 0.323  loss_dice_3: 2.541  loss_ce_4: 0.2239  loss_mask_4: 0.3233  loss_dice_4: 2.539  loss_ce_5: 0.2187  loss_mask_5: 0.3208  loss_dice_5: 2.541  loss_ce_6: 0.2129  loss_mask_6: 0.3205  loss_dice_6: 2.535  loss_ce_7: 0.2107  loss_mask_7: 0.3218  loss_dice_7: 2.535  loss_ce_8: 0.209  loss_mask_8: 0.3227  loss_dice_8: 2.532  time: 1.4656  data_time: 0.0534  lr: 6.567e-07  max_mem: 21589M
[01/18 12:21:39] d2.utils.events INFO:  eta: 0:42:27  iter: 38079  total_loss: 30.94  loss_ce: 0.1969  loss_mask: 0.3207  loss_dice: 2.479  loss_ce_0: 0.5241  loss_mask_0: 0.3192  loss_dice_0: 2.623  loss_ce_1: 0.2573  loss_mask_1: 0.3239  loss_dice_1: 2.534  loss_ce_2: 0.242  loss_mask_2: 0.3224  loss_dice_2: 2.511  loss_ce_3: 0.2359  loss_mask_3: 0.3226  loss_dice_3: 2.504  loss_ce_4: 0.2259  loss_mask_4: 0.3214  loss_dice_4: 2.491  loss_ce_5: 0.2165  loss_mask_5: 0.321  loss_dice_5: 2.49  loss_ce_6: 0.1999  loss_mask_6: 0.3221  loss_dice_6: 2.483  loss_ce_7: 0.2111  loss_mask_7: 0.3221  loss_dice_7: 2.488  loss_ce_8: 0.2066  loss_mask_8: 0.3211  loss_dice_8: 2.479  time: 1.4655  data_time: 0.0539  lr: 6.5061e-07  max_mem: 21589M
[01/18 12:22:06] d2.utils.events INFO:  eta: 0:42:01  iter: 38099  total_loss: 31.16  loss_ce: 0.1925  loss_mask: 0.3115  loss_dice: 2.52  loss_ce_0: 0.5346  loss_mask_0: 0.3085  loss_dice_0: 2.67  loss_ce_1: 0.2659  loss_mask_1: 0.3169  loss_dice_1: 2.565  loss_ce_2: 0.2597  loss_mask_2: 0.3153  loss_dice_2: 2.538  loss_ce_3: 0.2336  loss_mask_3: 0.3115  loss_dice_3: 2.53  loss_ce_4: 0.2052  loss_mask_4: 0.3111  loss_dice_4: 2.535  loss_ce_5: 0.1994  loss_mask_5: 0.3106  loss_dice_5: 2.534  loss_ce_6: 0.2071  loss_mask_6: 0.3112  loss_dice_6: 2.528  loss_ce_7: 0.2008  loss_mask_7: 0.3117  loss_dice_7: 2.524  loss_ce_8: 0.1958  loss_mask_8: 0.3114  loss_dice_8: 2.529  time: 1.4654  data_time: 0.0574  lr: 6.4451e-07  max_mem: 21589M
[01/18 12:22:33] d2.utils.events INFO:  eta: 0:41:34  iter: 38119  total_loss: 32.15  loss_ce: 0.2122  loss_mask: 0.3247  loss_dice: 2.567  loss_ce_0: 0.5738  loss_mask_0: 0.3254  loss_dice_0: 2.718  loss_ce_1: 0.2595  loss_mask_1: 0.3292  loss_dice_1: 2.627  loss_ce_2: 0.2685  loss_mask_2: 0.3259  loss_dice_2: 2.597  loss_ce_3: 0.2425  loss_mask_3: 0.3233  loss_dice_3: 2.574  loss_ce_4: 0.2179  loss_mask_4: 0.3235  loss_dice_4: 2.572  loss_ce_5: 0.2219  loss_mask_5: 0.3235  loss_dice_5: 2.572  loss_ce_6: 0.2177  loss_mask_6: 0.3237  loss_dice_6: 2.561  loss_ce_7: 0.2255  loss_mask_7: 0.3256  loss_dice_7: 2.562  loss_ce_8: 0.2054  loss_mask_8: 0.325  loss_dice_8: 2.566  time: 1.4654  data_time: 0.0535  lr: 6.384e-07  max_mem: 21589M
[01/18 12:23:00] d2.utils.events INFO:  eta: 0:41:08  iter: 38139  total_loss: 31.37  loss_ce: 0.2185  loss_mask: 0.3234  loss_dice: 2.481  loss_ce_0: 0.5358  loss_mask_0: 0.329  loss_dice_0: 2.626  loss_ce_1: 0.2647  loss_mask_1: 0.3298  loss_dice_1: 2.535  loss_ce_2: 0.2648  loss_mask_2: 0.3246  loss_dice_2: 2.514  loss_ce_3: 0.2395  loss_mask_3: 0.3246  loss_dice_3: 2.496  loss_ce_4: 0.2458  loss_mask_4: 0.3247  loss_dice_4: 2.489  loss_ce_5: 0.2284  loss_mask_5: 0.3242  loss_dice_5: 2.494  loss_ce_6: 0.2236  loss_mask_6: 0.324  loss_dice_6: 2.494  loss_ce_7: 0.2289  loss_mask_7: 0.3236  loss_dice_7: 2.484  loss_ce_8: 0.2211  loss_mask_8: 0.3239  loss_dice_8: 2.49  time: 1.4653  data_time: 0.0526  lr: 6.3229e-07  max_mem: 21589M
[01/18 12:23:28] d2.utils.events INFO:  eta: 0:40:42  iter: 38159  total_loss: 31.52  loss_ce: 0.2126  loss_mask: 0.314  loss_dice: 2.568  loss_ce_0: 0.5382  loss_mask_0: 0.3144  loss_dice_0: 2.703  loss_ce_1: 0.274  loss_mask_1: 0.3235  loss_dice_1: 2.613  loss_ce_2: 0.2543  loss_mask_2: 0.3177  loss_dice_2: 2.582  loss_ce_3: 0.2235  loss_mask_3: 0.3163  loss_dice_3: 2.567  loss_ce_4: 0.2249  loss_mask_4: 0.3164  loss_dice_4: 2.558  loss_ce_5: 0.221  loss_mask_5: 0.3159  loss_dice_5: 2.562  loss_ce_6: 0.2135  loss_mask_6: 0.3155  loss_dice_6: 2.563  loss_ce_7: 0.2101  loss_mask_7: 0.3151  loss_dice_7: 2.564  loss_ce_8: 0.2136  loss_mask_8: 0.315  loss_dice_8: 2.556  time: 1.4653  data_time: 0.0542  lr: 6.2617e-07  max_mem: 21589M
[01/18 12:23:55] d2.utils.events INFO:  eta: 0:40:17  iter: 38179  total_loss: 30.99  loss_ce: 0.2197  loss_mask: 0.3127  loss_dice: 2.471  loss_ce_0: 0.5257  loss_mask_0: 0.3136  loss_dice_0: 2.613  loss_ce_1: 0.264  loss_mask_1: 0.3193  loss_dice_1: 2.503  loss_ce_2: 0.2582  loss_mask_2: 0.3161  loss_dice_2: 2.49  loss_ce_3: 0.2342  loss_mask_3: 0.3152  loss_dice_3: 2.481  loss_ce_4: 0.2262  loss_mask_4: 0.3145  loss_dice_4: 2.472  loss_ce_5: 0.2136  loss_mask_5: 0.3124  loss_dice_5: 2.468  loss_ce_6: 0.215  loss_mask_6: 0.313  loss_dice_6: 2.466  loss_ce_7: 0.2075  loss_mask_7: 0.3117  loss_dice_7: 2.472  loss_ce_8: 0.2041  loss_mask_8: 0.313  loss_dice_8: 2.461  time: 1.4652  data_time: 0.0535  lr: 6.2005e-07  max_mem: 21589M
[01/18 12:24:22] d2.utils.events INFO:  eta: 0:39:51  iter: 38199  total_loss: 30.93  loss_ce: 0.2262  loss_mask: 0.3143  loss_dice: 2.496  loss_ce_0: 0.5615  loss_mask_0: 0.3169  loss_dice_0: 2.659  loss_ce_1: 0.2742  loss_mask_1: 0.3182  loss_dice_1: 2.553  loss_ce_2: 0.2702  loss_mask_2: 0.3161  loss_dice_2: 2.526  loss_ce_3: 0.2415  loss_mask_3: 0.3144  loss_dice_3: 2.515  loss_ce_4: 0.2304  loss_mask_4: 0.3138  loss_dice_4: 2.5  loss_ce_5: 0.2179  loss_mask_5: 0.3138  loss_dice_5: 2.506  loss_ce_6: 0.217  loss_mask_6: 0.3138  loss_dice_6: 2.495  loss_ce_7: 0.2092  loss_mask_7: 0.3136  loss_dice_7: 2.5  loss_ce_8: 0.2161  loss_mask_8: 0.3144  loss_dice_8: 2.491  time: 1.4651  data_time: 0.0550  lr: 6.1392e-07  max_mem: 21589M
[01/18 12:24:48] d2.utils.events INFO:  eta: 0:39:23  iter: 38219  total_loss: 30.92  loss_ce: 0.2197  loss_mask: 0.3175  loss_dice: 2.456  loss_ce_0: 0.5752  loss_mask_0: 0.3254  loss_dice_0: 2.6  loss_ce_1: 0.2725  loss_mask_1: 0.3216  loss_dice_1: 2.507  loss_ce_2: 0.2619  loss_mask_2: 0.3188  loss_dice_2: 2.488  loss_ce_3: 0.2343  loss_mask_3: 0.318  loss_dice_3: 2.469  loss_ce_4: 0.2242  loss_mask_4: 0.3174  loss_dice_4: 2.462  loss_ce_5: 0.2233  loss_mask_5: 0.3173  loss_dice_5: 2.468  loss_ce_6: 0.221  loss_mask_6: 0.3185  loss_dice_6: 2.458  loss_ce_7: 0.2251  loss_mask_7: 0.317  loss_dice_7: 2.463  loss_ce_8: 0.2214  loss_mask_8: 0.3174  loss_dice_8: 2.461  time: 1.4651  data_time: 0.0538  lr: 6.0778e-07  max_mem: 21589M
[01/18 12:25:15] d2.utils.events INFO:  eta: 0:38:57  iter: 38239  total_loss: 31.62  loss_ce: 0.2275  loss_mask: 0.3161  loss_dice: 2.518  loss_ce_0: 0.5649  loss_mask_0: 0.3213  loss_dice_0: 2.664  loss_ce_1: 0.2728  loss_mask_1: 0.3236  loss_dice_1: 2.568  loss_ce_2: 0.2671  loss_mask_2: 0.3202  loss_dice_2: 2.524  loss_ce_3: 0.2467  loss_mask_3: 0.319  loss_dice_3: 2.523  loss_ce_4: 0.2291  loss_mask_4: 0.3176  loss_dice_4: 2.521  loss_ce_5: 0.2268  loss_mask_5: 0.3163  loss_dice_5: 2.518  loss_ce_6: 0.2294  loss_mask_6: 0.3153  loss_dice_6: 2.509  loss_ce_7: 0.2204  loss_mask_7: 0.3154  loss_dice_7: 2.51  loss_ce_8: 0.2118  loss_mask_8: 0.316  loss_dice_8: 2.513  time: 1.4650  data_time: 0.0534  lr: 6.0163e-07  max_mem: 21589M
[01/18 12:25:42] d2.utils.events INFO:  eta: 0:38:30  iter: 38259  total_loss: 30.78  loss_ce: 0.2097  loss_mask: 0.3235  loss_dice: 2.471  loss_ce_0: 0.5307  loss_mask_0: 0.3189  loss_dice_0: 2.619  loss_ce_1: 0.2541  loss_mask_1: 0.3261  loss_dice_1: 2.516  loss_ce_2: 0.251  loss_mask_2: 0.3241  loss_dice_2: 2.492  loss_ce_3: 0.2305  loss_mask_3: 0.3222  loss_dice_3: 2.474  loss_ce_4: 0.221  loss_mask_4: 0.3224  loss_dice_4: 2.474  loss_ce_5: 0.2107  loss_mask_5: 0.3229  loss_dice_5: 2.468  loss_ce_6: 0.2109  loss_mask_6: 0.3227  loss_dice_6: 2.467  loss_ce_7: 0.2067  loss_mask_7: 0.3235  loss_dice_7: 2.466  loss_ce_8: 0.2062  loss_mask_8: 0.3237  loss_dice_8: 2.468  time: 1.4649  data_time: 0.0552  lr: 5.9548e-07  max_mem: 21589M
[01/18 12:26:09] d2.utils.events INFO:  eta: 0:38:04  iter: 38279  total_loss: 30.92  loss_ce: 0.1972  loss_mask: 0.3143  loss_dice: 2.479  loss_ce_0: 0.5607  loss_mask_0: 0.3172  loss_dice_0: 2.632  loss_ce_1: 0.2576  loss_mask_1: 0.3201  loss_dice_1: 2.53  loss_ce_2: 0.2456  loss_mask_2: 0.3155  loss_dice_2: 2.502  loss_ce_3: 0.2278  loss_mask_3: 0.3147  loss_dice_3: 2.49  loss_ce_4: 0.2031  loss_mask_4: 0.3152  loss_dice_4: 2.479  loss_ce_5: 0.2097  loss_mask_5: 0.314  loss_dice_5: 2.482  loss_ce_6: 0.1968  loss_mask_6: 0.3133  loss_dice_6: 2.477  loss_ce_7: 0.1932  loss_mask_7: 0.3135  loss_dice_7: 2.482  loss_ce_8: 0.1931  loss_mask_8: 0.3141  loss_dice_8: 2.485  time: 1.4649  data_time: 0.0515  lr: 5.8932e-07  max_mem: 21589M
[01/18 12:26:36] d2.utils.events INFO:  eta: 0:37:38  iter: 38299  total_loss: 30.65  loss_ce: 0.1971  loss_mask: 0.3221  loss_dice: 2.445  loss_ce_0: 0.532  loss_mask_0: 0.3164  loss_dice_0: 2.608  loss_ce_1: 0.2593  loss_mask_1: 0.3262  loss_dice_1: 2.507  loss_ce_2: 0.2508  loss_mask_2: 0.3248  loss_dice_2: 2.477  loss_ce_3: 0.2147  loss_mask_3: 0.3235  loss_dice_3: 2.46  loss_ce_4: 0.2192  loss_mask_4: 0.3235  loss_dice_4: 2.467  loss_ce_5: 0.2005  loss_mask_5: 0.3222  loss_dice_5: 2.464  loss_ce_6: 0.2061  loss_mask_6: 0.3224  loss_dice_6: 2.457  loss_ce_7: 0.2054  loss_mask_7: 0.3237  loss_dice_7: 2.459  loss_ce_8: 0.2062  loss_mask_8: 0.323  loss_dice_8: 2.454  time: 1.4648  data_time: 0.0531  lr: 5.8315e-07  max_mem: 21589M
[01/18 12:27:03] d2.utils.events INFO:  eta: 0:37:12  iter: 38319  total_loss: 31.68  loss_ce: 0.1994  loss_mask: 0.3247  loss_dice: 2.51  loss_ce_0: 0.5415  loss_mask_0: 0.322  loss_dice_0: 2.669  loss_ce_1: 0.2512  loss_mask_1: 0.3294  loss_dice_1: 2.571  loss_ce_2: 0.2608  loss_mask_2: 0.3257  loss_dice_2: 2.544  loss_ce_3: 0.2463  loss_mask_3: 0.3244  loss_dice_3: 2.528  loss_ce_4: 0.2203  loss_mask_4: 0.323  loss_dice_4: 2.526  loss_ce_5: 0.2176  loss_mask_5: 0.3228  loss_dice_5: 2.525  loss_ce_6: 0.2138  loss_mask_6: 0.3231  loss_dice_6: 2.515  loss_ce_7: 0.2111  loss_mask_7: 0.3228  loss_dice_7: 2.511  loss_ce_8: 0.1981  loss_mask_8: 0.3225  loss_dice_8: 2.51  time: 1.4648  data_time: 0.0561  lr: 5.7697e-07  max_mem: 21589M
[01/18 12:27:30] d2.utils.events INFO:  eta: 0:36:45  iter: 38339  total_loss: 31.68  loss_ce: 0.1966  loss_mask: 0.3205  loss_dice: 2.601  loss_ce_0: 0.5275  loss_mask_0: 0.3221  loss_dice_0: 2.727  loss_ce_1: 0.2494  loss_mask_1: 0.3273  loss_dice_1: 2.646  loss_ce_2: 0.2406  loss_mask_2: 0.3236  loss_dice_2: 2.625  loss_ce_3: 0.223  loss_mask_3: 0.3208  loss_dice_3: 2.612  loss_ce_4: 0.2122  loss_mask_4: 0.3208  loss_dice_4: 2.606  loss_ce_5: 0.2075  loss_mask_5: 0.3195  loss_dice_5: 2.605  loss_ce_6: 0.2002  loss_mask_6: 0.3209  loss_dice_6: 2.593  loss_ce_7: 0.1998  loss_mask_7: 0.3199  loss_dice_7: 2.599  loss_ce_8: 0.185  loss_mask_8: 0.3207  loss_dice_8: 2.595  time: 1.4647  data_time: 0.0555  lr: 5.7079e-07  max_mem: 21589M
[01/18 12:27:56] d2.utils.events INFO:  eta: 0:36:18  iter: 38359  total_loss: 30.8  loss_ce: 0.2143  loss_mask: 0.3213  loss_dice: 2.476  loss_ce_0: 0.554  loss_mask_0: 0.3206  loss_dice_0: 2.608  loss_ce_1: 0.2852  loss_mask_1: 0.326  loss_dice_1: 2.527  loss_ce_2: 0.2625  loss_mask_2: 0.3209  loss_dice_2: 2.497  loss_ce_3: 0.2381  loss_mask_3: 0.3187  loss_dice_3: 2.482  loss_ce_4: 0.2374  loss_mask_4: 0.3208  loss_dice_4: 2.479  loss_ce_5: 0.2254  loss_mask_5: 0.3197  loss_dice_5: 2.479  loss_ce_6: 0.2116  loss_mask_6: 0.3207  loss_dice_6: 2.478  loss_ce_7: 0.2218  loss_mask_7: 0.3224  loss_dice_7: 2.471  loss_ce_8: 0.2074  loss_mask_8: 0.3221  loss_dice_8: 2.476  time: 1.4646  data_time: 0.0503  lr: 5.646e-07  max_mem: 21589M
[01/18 12:28:24] d2.utils.events INFO:  eta: 0:35:53  iter: 38379  total_loss: 31.59  loss_ce: 0.2196  loss_mask: 0.3176  loss_dice: 2.514  loss_ce_0: 0.5534  loss_mask_0: 0.322  loss_dice_0: 2.653  loss_ce_1: 0.2866  loss_mask_1: 0.3217  loss_dice_1: 2.568  loss_ce_2: 0.2675  loss_mask_2: 0.3193  loss_dice_2: 2.538  loss_ce_3: 0.2419  loss_mask_3: 0.3189  loss_dice_3: 2.528  loss_ce_4: 0.2379  loss_mask_4: 0.3177  loss_dice_4: 2.519  loss_ce_5: 0.2329  loss_mask_5: 0.3168  loss_dice_5: 2.522  loss_ce_6: 0.2237  loss_mask_6: 0.3161  loss_dice_6: 2.526  loss_ce_7: 0.2065  loss_mask_7: 0.316  loss_dice_7: 2.522  loss_ce_8: 0.2215  loss_mask_8: 0.316  loss_dice_8: 2.511  time: 1.4646  data_time: 0.0534  lr: 5.5841e-07  max_mem: 21589M
[01/18 12:28:50] d2.utils.events INFO:  eta: 0:35:27  iter: 38399  total_loss: 30.76  loss_ce: 0.1954  loss_mask: 0.3199  loss_dice: 2.456  loss_ce_0: 0.5431  loss_mask_0: 0.3259  loss_dice_0: 2.593  loss_ce_1: 0.2578  loss_mask_1: 0.3274  loss_dice_1: 2.5  loss_ce_2: 0.2354  loss_mask_2: 0.3255  loss_dice_2: 2.482  loss_ce_3: 0.2193  loss_mask_3: 0.3237  loss_dice_3: 2.453  loss_ce_4: 0.2129  loss_mask_4: 0.3223  loss_dice_4: 2.455  loss_ce_5: 0.2096  loss_mask_5: 0.3214  loss_dice_5: 2.457  loss_ce_6: 0.1993  loss_mask_6: 0.3206  loss_dice_6: 2.455  loss_ce_7: 0.1999  loss_mask_7: 0.321  loss_dice_7: 2.462  loss_ce_8: 0.2064  loss_mask_8: 0.3198  loss_dice_8: 2.454  time: 1.4645  data_time: 0.0515  lr: 5.522e-07  max_mem: 21589M
[01/18 12:29:17] d2.utils.events INFO:  eta: 0:35:00  iter: 38419  total_loss: 31.59  loss_ce: 0.212  loss_mask: 0.3207  loss_dice: 2.517  loss_ce_0: 0.5412  loss_mask_0: 0.3247  loss_dice_0: 2.665  loss_ce_1: 0.2729  loss_mask_1: 0.3289  loss_dice_1: 2.57  loss_ce_2: 0.2614  loss_mask_2: 0.3264  loss_dice_2: 2.544  loss_ce_3: 0.2446  loss_mask_3: 0.3242  loss_dice_3: 2.533  loss_ce_4: 0.2279  loss_mask_4: 0.3231  loss_dice_4: 2.53  loss_ce_5: 0.2239  loss_mask_5: 0.3214  loss_dice_5: 2.525  loss_ce_6: 0.2276  loss_mask_6: 0.3208  loss_dice_6: 2.525  loss_ce_7: 0.2203  loss_mask_7: 0.3208  loss_dice_7: 2.533  loss_ce_8: 0.2179  loss_mask_8: 0.3215  loss_dice_8: 2.532  time: 1.4644  data_time: 0.0531  lr: 5.4599e-07  max_mem: 21589M
[01/18 12:29:44] d2.utils.events INFO:  eta: 0:34:34  iter: 38439  total_loss: 31.24  loss_ce: 0.226  loss_mask: 0.3227  loss_dice: 2.497  loss_ce_0: 0.5799  loss_mask_0: 0.3235  loss_dice_0: 2.641  loss_ce_1: 0.2643  loss_mask_1: 0.3295  loss_dice_1: 2.542  loss_ce_2: 0.2532  loss_mask_2: 0.3256  loss_dice_2: 2.521  loss_ce_3: 0.2344  loss_mask_3: 0.3244  loss_dice_3: 2.5  loss_ce_4: 0.2501  loss_mask_4: 0.3237  loss_dice_4: 2.502  loss_ce_5: 0.2294  loss_mask_5: 0.3219  loss_dice_5: 2.498  loss_ce_6: 0.2281  loss_mask_6: 0.3238  loss_dice_6: 2.5  loss_ce_7: 0.2199  loss_mask_7: 0.3237  loss_dice_7: 2.494  loss_ce_8: 0.2248  loss_mask_8: 0.3226  loss_dice_8: 2.499  time: 1.4644  data_time: 0.0515  lr: 5.3977e-07  max_mem: 21589M
[01/18 12:30:10] d2.utils.events INFO:  eta: 0:34:08  iter: 38459  total_loss: 30.76  loss_ce: 0.1985  loss_mask: 0.3186  loss_dice: 2.502  loss_ce_0: 0.5444  loss_mask_0: 0.3185  loss_dice_0: 2.636  loss_ce_1: 0.2553  loss_mask_1: 0.3253  loss_dice_1: 2.543  loss_ce_2: 0.2515  loss_mask_2: 0.323  loss_dice_2: 2.516  loss_ce_3: 0.2263  loss_mask_3: 0.3203  loss_dice_3: 2.511  loss_ce_4: 0.2204  loss_mask_4: 0.3205  loss_dice_4: 2.504  loss_ce_5: 0.2178  loss_mask_5: 0.3204  loss_dice_5: 2.506  loss_ce_6: 0.204  loss_mask_6: 0.32  loss_dice_6: 2.499  loss_ce_7: 0.2067  loss_mask_7: 0.3195  loss_dice_7: 2.506  loss_ce_8: 0.198  loss_mask_8: 0.3179  loss_dice_8: 2.513  time: 1.4643  data_time: 0.0516  lr: 5.3354e-07  max_mem: 21589M
[01/18 12:30:37] d2.utils.events INFO:  eta: 0:33:40  iter: 38479  total_loss: 30.41  loss_ce: 0.2354  loss_mask: 0.3158  loss_dice: 2.403  loss_ce_0: 0.5635  loss_mask_0: 0.3092  loss_dice_0: 2.569  loss_ce_1: 0.279  loss_mask_1: 0.3201  loss_dice_1: 2.464  loss_ce_2: 0.2701  loss_mask_2: 0.3177  loss_dice_2: 2.443  loss_ce_3: 0.2394  loss_mask_3: 0.3169  loss_dice_3: 2.42  loss_ce_4: 0.2393  loss_mask_4: 0.3172  loss_dice_4: 2.415  loss_ce_5: 0.2407  loss_mask_5: 0.3159  loss_dice_5: 2.407  loss_ce_6: 0.2176  loss_mask_6: 0.3165  loss_dice_6: 2.417  loss_ce_7: 0.2243  loss_mask_7: 0.3166  loss_dice_7: 2.405  loss_ce_8: 0.2185  loss_mask_8: 0.3162  loss_dice_8: 2.411  time: 1.4642  data_time: 0.0521  lr: 5.2731e-07  max_mem: 21589M
[01/18 12:31:04] d2.utils.events INFO:  eta: 0:33:14  iter: 38499  total_loss: 30.63  loss_ce: 0.215  loss_mask: 0.3227  loss_dice: 2.456  loss_ce_0: 0.5364  loss_mask_0: 0.3238  loss_dice_0: 2.598  loss_ce_1: 0.2672  loss_mask_1: 0.3291  loss_dice_1: 2.507  loss_ce_2: 0.2507  loss_mask_2: 0.3258  loss_dice_2: 2.485  loss_ce_3: 0.2385  loss_mask_3: 0.3242  loss_dice_3: 2.472  loss_ce_4: 0.2217  loss_mask_4: 0.3232  loss_dice_4: 2.468  loss_ce_5: 0.2109  loss_mask_5: 0.3235  loss_dice_5: 2.463  loss_ce_6: 0.2139  loss_mask_6: 0.3222  loss_dice_6: 2.463  loss_ce_7: 0.2126  loss_mask_7: 0.322  loss_dice_7: 2.464  loss_ce_8: 0.2064  loss_mask_8: 0.3223  loss_dice_8: 2.463  time: 1.4641  data_time: 0.0506  lr: 5.2106e-07  max_mem: 21589M
[01/18 12:31:30] d2.utils.events INFO:  eta: 0:32:48  iter: 38519  total_loss: 30.47  loss_ce: 0.2072  loss_mask: 0.3237  loss_dice: 2.446  loss_ce_0: 0.5371  loss_mask_0: 0.3228  loss_dice_0: 2.579  loss_ce_1: 0.2871  loss_mask_1: 0.3287  loss_dice_1: 2.499  loss_ce_2: 0.2615  loss_mask_2: 0.3274  loss_dice_2: 2.468  loss_ce_3: 0.2313  loss_mask_3: 0.325  loss_dice_3: 2.452  loss_ce_4: 0.2346  loss_mask_4: 0.3246  loss_dice_4: 2.449  loss_ce_5: 0.2214  loss_mask_5: 0.3242  loss_dice_5: 2.449  loss_ce_6: 0.2184  loss_mask_6: 0.3243  loss_dice_6: 2.441  loss_ce_7: 0.2172  loss_mask_7: 0.3227  loss_dice_7: 2.444  loss_ce_8: 0.2135  loss_mask_8: 0.3236  loss_dice_8: 2.446  time: 1.4641  data_time: 0.0513  lr: 5.1481e-07  max_mem: 21589M
[01/18 12:31:57] d2.utils.events INFO:  eta: 0:32:22  iter: 38539  total_loss: 31.11  loss_ce: 0.2203  loss_mask: 0.3229  loss_dice: 2.495  loss_ce_0: 0.5482  loss_mask_0: 0.3247  loss_dice_0: 2.63  loss_ce_1: 0.268  loss_mask_1: 0.3269  loss_dice_1: 2.539  loss_ce_2: 0.2664  loss_mask_2: 0.3232  loss_dice_2: 2.517  loss_ce_3: 0.233  loss_mask_3: 0.3226  loss_dice_3: 2.506  loss_ce_4: 0.2265  loss_mask_4: 0.3234  loss_dice_4: 2.506  loss_ce_5: 0.2193  loss_mask_5: 0.3227  loss_dice_5: 2.501  loss_ce_6: 0.2147  loss_mask_6: 0.3221  loss_dice_6: 2.504  loss_ce_7: 0.2093  loss_mask_7: 0.322  loss_dice_7: 2.5  loss_ce_8: 0.2091  loss_mask_8: 0.3227  loss_dice_8: 2.491  time: 1.4640  data_time: 0.0548  lr: 5.0855e-07  max_mem: 21589M
[01/18 12:32:24] d2.utils.events INFO:  eta: 0:31:57  iter: 38559  total_loss: 31.25  loss_ce: 0.2071  loss_mask: 0.3204  loss_dice: 2.524  loss_ce_0: 0.5391  loss_mask_0: 0.3216  loss_dice_0: 2.664  loss_ce_1: 0.2839  loss_mask_1: 0.3302  loss_dice_1: 2.567  loss_ce_2: 0.2576  loss_mask_2: 0.3255  loss_dice_2: 2.538  loss_ce_3: 0.2374  loss_mask_3: 0.3221  loss_dice_3: 2.531  loss_ce_4: 0.2286  loss_mask_4: 0.3224  loss_dice_4: 2.521  loss_ce_5: 0.2104  loss_mask_5: 0.3218  loss_dice_5: 2.532  loss_ce_6: 0.222  loss_mask_6: 0.3214  loss_dice_6: 2.529  loss_ce_7: 0.2108  loss_mask_7: 0.322  loss_dice_7: 2.523  loss_ce_8: 0.205  loss_mask_8: 0.322  loss_dice_8: 2.526  time: 1.4639  data_time: 0.0527  lr: 5.0228e-07  max_mem: 21589M
[01/18 12:32:51] d2.utils.events INFO:  eta: 0:31:31  iter: 38579  total_loss: 32  loss_ce: 0.2083  loss_mask: 0.3142  loss_dice: 2.575  loss_ce_0: 0.58  loss_mask_0: 0.3218  loss_dice_0: 2.729  loss_ce_1: 0.2821  loss_mask_1: 0.3235  loss_dice_1: 2.65  loss_ce_2: 0.2611  loss_mask_2: 0.319  loss_dice_2: 2.596  loss_ce_3: 0.2323  loss_mask_3: 0.3163  loss_dice_3: 2.592  loss_ce_4: 0.2329  loss_mask_4: 0.3157  loss_dice_4: 2.593  loss_ce_5: 0.2235  loss_mask_5: 0.3151  loss_dice_5: 2.59  loss_ce_6: 0.2292  loss_mask_6: 0.3159  loss_dice_6: 2.583  loss_ce_7: 0.2144  loss_mask_7: 0.3154  loss_dice_7: 2.572  loss_ce_8: 0.2109  loss_mask_8: 0.3146  loss_dice_8: 2.581  time: 1.4639  data_time: 0.0524  lr: 4.96e-07  max_mem: 21589M
[01/18 12:33:18] d2.utils.events INFO:  eta: 0:31:02  iter: 38599  total_loss: 30.26  loss_ce: 0.2162  loss_mask: 0.3245  loss_dice: 2.408  loss_ce_0: 0.5419  loss_mask_0: 0.3193  loss_dice_0: 2.57  loss_ce_1: 0.2605  loss_mask_1: 0.3296  loss_dice_1: 2.466  loss_ce_2: 0.2507  loss_mask_2: 0.3253  loss_dice_2: 2.436  loss_ce_3: 0.2346  loss_mask_3: 0.3236  loss_dice_3: 2.428  loss_ce_4: 0.2194  loss_mask_4: 0.3233  loss_dice_4: 2.42  loss_ce_5: 0.2151  loss_mask_5: 0.3257  loss_dice_5: 2.424  loss_ce_6: 0.2068  loss_mask_6: 0.325  loss_dice_6: 2.419  loss_ce_7: 0.2109  loss_mask_7: 0.3234  loss_dice_7: 2.41  loss_ce_8: 0.1998  loss_mask_8: 0.3238  loss_dice_8: 2.41  time: 1.4638  data_time: 0.0531  lr: 4.8971e-07  max_mem: 21589M
[01/18 12:33:45] d2.utils.events INFO:  eta: 0:30:37  iter: 38619  total_loss: 30.81  loss_ce: 0.2116  loss_mask: 0.3219  loss_dice: 2.477  loss_ce_0: 0.5734  loss_mask_0: 0.3177  loss_dice_0: 2.635  loss_ce_1: 0.2716  loss_mask_1: 0.3253  loss_dice_1: 2.539  loss_ce_2: 0.2644  loss_mask_2: 0.323  loss_dice_2: 2.507  loss_ce_3: 0.2342  loss_mask_3: 0.3206  loss_dice_3: 2.497  loss_ce_4: 0.2165  loss_mask_4: 0.3206  loss_dice_4: 2.495  loss_ce_5: 0.2256  loss_mask_5: 0.3203  loss_dice_5: 2.493  loss_ce_6: 0.2142  loss_mask_6: 0.3203  loss_dice_6: 2.487  loss_ce_7: 0.2152  loss_mask_7: 0.3207  loss_dice_7: 2.486  loss_ce_8: 0.2069  loss_mask_8: 0.3203  loss_dice_8: 2.487  time: 1.4638  data_time: 0.0523  lr: 4.8342e-07  max_mem: 21589M
[01/18 12:34:11] d2.utils.events INFO:  eta: 0:30:11  iter: 38639  total_loss: 31.19  loss_ce: 0.1966  loss_mask: 0.3131  loss_dice: 2.48  loss_ce_0: 0.5346  loss_mask_0: 0.3104  loss_dice_0: 2.637  loss_ce_1: 0.2435  loss_mask_1: 0.3171  loss_dice_1: 2.535  loss_ce_2: 0.2375  loss_mask_2: 0.3146  loss_dice_2: 2.516  loss_ce_3: 0.2391  loss_mask_3: 0.3136  loss_dice_3: 2.496  loss_ce_4: 0.2232  loss_mask_4: 0.3132  loss_dice_4: 2.5  loss_ce_5: 0.2113  loss_mask_5: 0.3131  loss_dice_5: 2.497  loss_ce_6: 0.2119  loss_mask_6: 0.3125  loss_dice_6: 2.493  loss_ce_7: 0.2049  loss_mask_7: 0.3135  loss_dice_7: 2.495  loss_ce_8: 0.1949  loss_mask_8: 0.3124  loss_dice_8: 2.487  time: 1.4637  data_time: 0.0513  lr: 4.7711e-07  max_mem: 21589M
[01/18 12:34:38] d2.utils.events INFO:  eta: 0:29:45  iter: 38659  total_loss: 30.72  loss_ce: 0.2042  loss_mask: 0.3223  loss_dice: 2.447  loss_ce_0: 0.549  loss_mask_0: 0.3222  loss_dice_0: 2.587  loss_ce_1: 0.2712  loss_mask_1: 0.3283  loss_dice_1: 2.493  loss_ce_2: 0.2611  loss_mask_2: 0.3247  loss_dice_2: 2.467  loss_ce_3: 0.2359  loss_mask_3: 0.3225  loss_dice_3: 2.454  loss_ce_4: 0.2366  loss_mask_4: 0.3231  loss_dice_4: 2.452  loss_ce_5: 0.223  loss_mask_5: 0.3233  loss_dice_5: 2.45  loss_ce_6: 0.2205  loss_mask_6: 0.3227  loss_dice_6: 2.451  loss_ce_7: 0.2114  loss_mask_7: 0.3219  loss_dice_7: 2.443  loss_ce_8: 0.2087  loss_mask_8: 0.321  loss_dice_8: 2.45  time: 1.4636  data_time: 0.0540  lr: 4.708e-07  max_mem: 21589M
[01/18 12:35:05] d2.utils.events INFO:  eta: 0:29:19  iter: 38679  total_loss: 31.11  loss_ce: 0.208  loss_mask: 0.3259  loss_dice: 2.494  loss_ce_0: 0.5519  loss_mask_0: 0.3252  loss_dice_0: 2.643  loss_ce_1: 0.2747  loss_mask_1: 0.3289  loss_dice_1: 2.556  loss_ce_2: 0.2465  loss_mask_2: 0.3277  loss_dice_2: 2.532  loss_ce_3: 0.2239  loss_mask_3: 0.3263  loss_dice_3: 2.512  loss_ce_4: 0.2261  loss_mask_4: 0.3265  loss_dice_4: 2.509  loss_ce_5: 0.2205  loss_mask_5: 0.3253  loss_dice_5: 2.501  loss_ce_6: 0.2183  loss_mask_6: 0.3249  loss_dice_6: 2.504  loss_ce_7: 0.2037  loss_mask_7: 0.3259  loss_dice_7: 2.502  loss_ce_8: 0.2116  loss_mask_8: 0.3258  loss_dice_8: 2.5  time: 1.4636  data_time: 0.0517  lr: 4.6447e-07  max_mem: 21589M
[01/18 12:35:32] d2.utils.events INFO:  eta: 0:28:53  iter: 38699  total_loss: 30.84  loss_ce: 0.2131  loss_mask: 0.3206  loss_dice: 2.467  loss_ce_0: 0.5497  loss_mask_0: 0.3194  loss_dice_0: 2.614  loss_ce_1: 0.2702  loss_mask_1: 0.3238  loss_dice_1: 2.519  loss_ce_2: 0.257  loss_mask_2: 0.3217  loss_dice_2: 2.494  loss_ce_3: 0.2333  loss_mask_3: 0.3202  loss_dice_3: 2.465  loss_ce_4: 0.2271  loss_mask_4: 0.3206  loss_dice_4: 2.477  loss_ce_5: 0.2307  loss_mask_5: 0.3214  loss_dice_5: 2.464  loss_ce_6: 0.2238  loss_mask_6: 0.3201  loss_dice_6: 2.468  loss_ce_7: 0.2255  loss_mask_7: 0.3217  loss_dice_7: 2.465  loss_ce_8: 0.2219  loss_mask_8: 0.3207  loss_dice_8: 2.467  time: 1.4635  data_time: 0.0510  lr: 4.5814e-07  max_mem: 21589M
[01/18 12:35:58] d2.utils.events INFO:  eta: 0:28:26  iter: 38719  total_loss: 30.37  loss_ce: 0.1964  loss_mask: 0.3145  loss_dice: 2.446  loss_ce_0: 0.5296  loss_mask_0: 0.3144  loss_dice_0: 2.582  loss_ce_1: 0.2637  loss_mask_1: 0.3216  loss_dice_1: 2.5  loss_ce_2: 0.2361  loss_mask_2: 0.3166  loss_dice_2: 2.471  loss_ce_3: 0.227  loss_mask_3: 0.3149  loss_dice_3: 2.457  loss_ce_4: 0.2228  loss_mask_4: 0.3152  loss_dice_4: 2.453  loss_ce_5: 0.2057  loss_mask_5: 0.314  loss_dice_5: 2.458  loss_ce_6: 0.2022  loss_mask_6: 0.3141  loss_dice_6: 2.455  loss_ce_7: 0.2034  loss_mask_7: 0.3153  loss_dice_7: 2.451  loss_ce_8: 0.1952  loss_mask_8: 0.3149  loss_dice_8: 2.456  time: 1.4634  data_time: 0.0538  lr: 4.5179e-07  max_mem: 21589M
[01/18 12:36:25] d2.utils.events INFO:  eta: 0:27:59  iter: 38739  total_loss: 31.25  loss_ce: 0.2138  loss_mask: 0.3175  loss_dice: 2.531  loss_ce_0: 0.5394  loss_mask_0: 0.3189  loss_dice_0: 2.674  loss_ce_1: 0.2601  loss_mask_1: 0.3245  loss_dice_1: 2.578  loss_ce_2: 0.2562  loss_mask_2: 0.3215  loss_dice_2: 2.552  loss_ce_3: 0.2362  loss_mask_3: 0.3199  loss_dice_3: 2.534  loss_ce_4: 0.2301  loss_mask_4: 0.3191  loss_dice_4: 2.538  loss_ce_5: 0.2292  loss_mask_5: 0.3183  loss_dice_5: 2.534  loss_ce_6: 0.22  loss_mask_6: 0.3171  loss_dice_6: 2.529  loss_ce_7: 0.2116  loss_mask_7: 0.3175  loss_dice_7: 2.53  loss_ce_8: 0.21  loss_mask_8: 0.3165  loss_dice_8: 2.531  time: 1.4634  data_time: 0.0548  lr: 4.4544e-07  max_mem: 21589M
[01/18 12:36:52] d2.utils.events INFO:  eta: 0:27:31  iter: 38759  total_loss: 31.5  loss_ce: 0.2254  loss_mask: 0.3204  loss_dice: 2.53  loss_ce_0: 0.5522  loss_mask_0: 0.319  loss_dice_0: 2.665  loss_ce_1: 0.275  loss_mask_1: 0.3283  loss_dice_1: 2.578  loss_ce_2: 0.2723  loss_mask_2: 0.3267  loss_dice_2: 2.552  loss_ce_3: 0.2346  loss_mask_3: 0.3237  loss_dice_3: 2.535  loss_ce_4: 0.235  loss_mask_4: 0.3221  loss_dice_4: 2.545  loss_ce_5: 0.2388  loss_mask_5: 0.3224  loss_dice_5: 2.538  loss_ce_6: 0.2263  loss_mask_6: 0.3212  loss_dice_6: 2.541  loss_ce_7: 0.2373  loss_mask_7: 0.3213  loss_dice_7: 2.534  loss_ce_8: 0.2198  loss_mask_8: 0.3212  loss_dice_8: 2.535  time: 1.4633  data_time: 0.0531  lr: 4.3908e-07  max_mem: 21589M
[01/18 12:37:19] d2.utils.events INFO:  eta: 0:27:05  iter: 38779  total_loss: 31.75  loss_ce: 0.2111  loss_mask: 0.3187  loss_dice: 2.561  loss_ce_0: 0.5498  loss_mask_0: 0.3174  loss_dice_0: 2.705  loss_ce_1: 0.2557  loss_mask_1: 0.3253  loss_dice_1: 2.618  loss_ce_2: 0.2527  loss_mask_2: 0.3205  loss_dice_2: 2.586  loss_ce_3: 0.2349  loss_mask_3: 0.3172  loss_dice_3: 2.574  loss_ce_4: 0.2287  loss_mask_4: 0.3186  loss_dice_4: 2.573  loss_ce_5: 0.2153  loss_mask_5: 0.3181  loss_dice_5: 2.57  loss_ce_6: 0.2235  loss_mask_6: 0.3186  loss_dice_6: 2.564  loss_ce_7: 0.2114  loss_mask_7: 0.3176  loss_dice_7: 2.558  loss_ce_8: 0.2108  loss_mask_8: 0.3183  loss_dice_8: 2.557  time: 1.4632  data_time: 0.0519  lr: 4.327e-07  max_mem: 21589M
[01/18 12:37:45] d2.utils.events INFO:  eta: 0:26:39  iter: 38799  total_loss: 30.4  loss_ce: 0.1961  loss_mask: 0.32  loss_dice: 2.437  loss_ce_0: 0.5421  loss_mask_0: 0.3174  loss_dice_0: 2.592  loss_ce_1: 0.2685  loss_mask_1: 0.3234  loss_dice_1: 2.489  loss_ce_2: 0.2373  loss_mask_2: 0.3211  loss_dice_2: 2.466  loss_ce_3: 0.2086  loss_mask_3: 0.3193  loss_dice_3: 2.453  loss_ce_4: 0.2146  loss_mask_4: 0.3187  loss_dice_4: 2.449  loss_ce_5: 0.2115  loss_mask_5: 0.3189  loss_dice_5: 2.45  loss_ce_6: 0.1961  loss_mask_6: 0.3188  loss_dice_6: 2.445  loss_ce_7: 0.1864  loss_mask_7: 0.3195  loss_dice_7: 2.438  loss_ce_8: 0.196  loss_mask_8: 0.3199  loss_dice_8: 2.438  time: 1.4632  data_time: 0.0539  lr: 4.2632e-07  max_mem: 21589M
[01/18 12:38:12] d2.utils.events INFO:  eta: 0:26:12  iter: 38819  total_loss: 30.34  loss_ce: 0.2073  loss_mask: 0.3149  loss_dice: 2.459  loss_ce_0: 0.5224  loss_mask_0: 0.3124  loss_dice_0: 2.619  loss_ce_1: 0.258  loss_mask_1: 0.3197  loss_dice_1: 2.521  loss_ce_2: 0.2554  loss_mask_2: 0.3165  loss_dice_2: 2.488  loss_ce_3: 0.2372  loss_mask_3: 0.3152  loss_dice_3: 2.483  loss_ce_4: 0.2219  loss_mask_4: 0.3147  loss_dice_4: 2.473  loss_ce_5: 0.221  loss_mask_5: 0.3142  loss_dice_5: 2.464  loss_ce_6: 0.2076  loss_mask_6: 0.3147  loss_dice_6: 2.464  loss_ce_7: 0.2026  loss_mask_7: 0.3151  loss_dice_7: 2.471  loss_ce_8: 0.2047  loss_mask_8: 0.3149  loss_dice_8: 2.462  time: 1.4631  data_time: 0.0530  lr: 4.1992e-07  max_mem: 21589M
[01/18 12:38:38] d2.utils.events INFO:  eta: 0:25:46  iter: 38839  total_loss: 30.98  loss_ce: 0.2034  loss_mask: 0.3118  loss_dice: 2.524  loss_ce_0: 0.536  loss_mask_0: 0.3185  loss_dice_0: 2.664  loss_ce_1: 0.2636  loss_mask_1: 0.3167  loss_dice_1: 2.578  loss_ce_2: 0.2564  loss_mask_2: 0.3137  loss_dice_2: 2.548  loss_ce_3: 0.2364  loss_mask_3: 0.3123  loss_dice_3: 2.524  loss_ce_4: 0.2307  loss_mask_4: 0.3115  loss_dice_4: 2.527  loss_ce_5: 0.2186  loss_mask_5: 0.3107  loss_dice_5: 2.52  loss_ce_6: 0.2151  loss_mask_6: 0.3097  loss_dice_6: 2.522  loss_ce_7: 0.2029  loss_mask_7: 0.3101  loss_dice_7: 2.532  loss_ce_8: 0.1957  loss_mask_8: 0.3103  loss_dice_8: 2.52  time: 1.4630  data_time: 0.0530  lr: 4.1352e-07  max_mem: 21589M
[01/18 12:39:06] d2.utils.events INFO:  eta: 0:25:20  iter: 38859  total_loss: 31.08  loss_ce: 0.2089  loss_mask: 0.3209  loss_dice: 2.495  loss_ce_0: 0.5458  loss_mask_0: 0.3252  loss_dice_0: 2.652  loss_ce_1: 0.2662  loss_mask_1: 0.3243  loss_dice_1: 2.553  loss_ce_2: 0.241  loss_mask_2: 0.323  loss_dice_2: 2.523  loss_ce_3: 0.2419  loss_mask_3: 0.3214  loss_dice_3: 2.503  loss_ce_4: 0.2208  loss_mask_4: 0.32  loss_dice_4: 2.512  loss_ce_5: 0.2199  loss_mask_5: 0.3195  loss_dice_5: 2.509  loss_ce_6: 0.198  loss_mask_6: 0.3188  loss_dice_6: 2.496  loss_ce_7: 0.1973  loss_mask_7: 0.3203  loss_dice_7: 2.504  loss_ce_8: 0.1931  loss_mask_8: 0.3203  loss_dice_8: 2.496  time: 1.4630  data_time: 0.0561  lr: 4.071e-07  max_mem: 21589M
[01/18 12:39:33] d2.utils.events INFO:  eta: 0:24:54  iter: 38879  total_loss: 30.76  loss_ce: 0.1966  loss_mask: 0.3229  loss_dice: 2.451  loss_ce_0: 0.5198  loss_mask_0: 0.3172  loss_dice_0: 2.609  loss_ce_1: 0.2434  loss_mask_1: 0.3262  loss_dice_1: 2.507  loss_ce_2: 0.2302  loss_mask_2: 0.3232  loss_dice_2: 2.487  loss_ce_3: 0.2056  loss_mask_3: 0.3222  loss_dice_3: 2.473  loss_ce_4: 0.1976  loss_mask_4: 0.3222  loss_dice_4: 2.467  loss_ce_5: 0.2062  loss_mask_5: 0.3222  loss_dice_5: 2.467  loss_ce_6: 0.189  loss_mask_6: 0.3223  loss_dice_6: 2.464  loss_ce_7: 0.1872  loss_mask_7: 0.3218  loss_dice_7: 2.464  loss_ce_8: 0.1887  loss_mask_8: 0.3222  loss_dice_8: 2.454  time: 1.4629  data_time: 0.0559  lr: 4.0067e-07  max_mem: 21589M
[01/18 12:40:00] d2.utils.events INFO:  eta: 0:24:29  iter: 38899  total_loss: 31.47  loss_ce: 0.1974  loss_mask: 0.3182  loss_dice: 2.527  loss_ce_0: 0.5529  loss_mask_0: 0.321  loss_dice_0: 2.675  loss_ce_1: 0.2524  loss_mask_1: 0.3263  loss_dice_1: 2.582  loss_ce_2: 0.2713  loss_mask_2: 0.3217  loss_dice_2: 2.557  loss_ce_3: 0.2347  loss_mask_3: 0.3193  loss_dice_3: 2.539  loss_ce_4: 0.219  loss_mask_4: 0.3176  loss_dice_4: 2.541  loss_ce_5: 0.2244  loss_mask_5: 0.3179  loss_dice_5: 2.532  loss_ce_6: 0.1982  loss_mask_6: 0.3177  loss_dice_6: 2.539  loss_ce_7: 0.1965  loss_mask_7: 0.3176  loss_dice_7: 2.537  loss_ce_8: 0.207  loss_mask_8: 0.318  loss_dice_8: 2.535  time: 1.4629  data_time: 0.0540  lr: 3.9423e-07  max_mem: 21589M
[01/18 12:40:27] d2.utils.events INFO:  eta: 0:24:02  iter: 38919  total_loss: 30.28  loss_ce: 0.191  loss_mask: 0.3103  loss_dice: 2.457  loss_ce_0: 0.5352  loss_mask_0: 0.3121  loss_dice_0: 2.587  loss_ce_1: 0.2427  loss_mask_1: 0.3182  loss_dice_1: 2.486  loss_ce_2: 0.2323  loss_mask_2: 0.315  loss_dice_2: 2.477  loss_ce_3: 0.2149  loss_mask_3: 0.3123  loss_dice_3: 2.458  loss_ce_4: 0.2132  loss_mask_4: 0.3117  loss_dice_4: 2.465  loss_ce_5: 0.2031  loss_mask_5: 0.3111  loss_dice_5: 2.453  loss_ce_6: 0.2006  loss_mask_6: 0.3117  loss_dice_6: 2.454  loss_ce_7: 0.202  loss_mask_7: 0.3118  loss_dice_7: 2.449  loss_ce_8: 0.1895  loss_mask_8: 0.3109  loss_dice_8: 2.457  time: 1.4628  data_time: 0.0548  lr: 3.8778e-07  max_mem: 21589M
[01/18 12:40:54] d2.utils.events INFO:  eta: 0:23:36  iter: 38939  total_loss: 30.73  loss_ce: 0.2061  loss_mask: 0.3146  loss_dice: 2.433  loss_ce_0: 0.5442  loss_mask_0: 0.3134  loss_dice_0: 2.613  loss_ce_1: 0.2806  loss_mask_1: 0.3188  loss_dice_1: 2.493  loss_ce_2: 0.2572  loss_mask_2: 0.3162  loss_dice_2: 2.457  loss_ce_3: 0.258  loss_mask_3: 0.3147  loss_dice_3: 2.451  loss_ce_4: 0.2298  loss_mask_4: 0.3156  loss_dice_4: 2.445  loss_ce_5: 0.2181  loss_mask_5: 0.3149  loss_dice_5: 2.446  loss_ce_6: 0.214  loss_mask_6: 0.3144  loss_dice_6: 2.439  loss_ce_7: 0.1984  loss_mask_7: 0.3155  loss_dice_7: 2.445  loss_ce_8: 0.2127  loss_mask_8: 0.3145  loss_dice_8: 2.44  time: 1.4627  data_time: 0.0505  lr: 3.8132e-07  max_mem: 21589M
[01/18 12:41:21] d2.utils.events INFO:  eta: 0:23:11  iter: 38959  total_loss: 31.7  loss_ce: 0.2049  loss_mask: 0.3154  loss_dice: 2.528  loss_ce_0: 0.5609  loss_mask_0: 0.3152  loss_dice_0: 2.664  loss_ce_1: 0.2774  loss_mask_1: 0.3193  loss_dice_1: 2.568  loss_ce_2: 0.268  loss_mask_2: 0.3164  loss_dice_2: 2.554  loss_ce_3: 0.2467  loss_mask_3: 0.317  loss_dice_3: 2.535  loss_ce_4: 0.2242  loss_mask_4: 0.3163  loss_dice_4: 2.529  loss_ce_5: 0.212  loss_mask_5: 0.3153  loss_dice_5: 2.537  loss_ce_6: 0.2152  loss_mask_6: 0.3161  loss_dice_6: 2.531  loss_ce_7: 0.2076  loss_mask_7: 0.3169  loss_dice_7: 2.527  loss_ce_8: 0.21  loss_mask_8: 0.3158  loss_dice_8: 2.531  time: 1.4627  data_time: 0.0543  lr: 3.7484e-07  max_mem: 21589M
[01/18 12:41:48] d2.utils.events INFO:  eta: 0:22:45  iter: 38979  total_loss: 30.71  loss_ce: 0.2051  loss_mask: 0.3191  loss_dice: 2.47  loss_ce_0: 0.5275  loss_mask_0: 0.3121  loss_dice_0: 2.616  loss_ce_1: 0.2556  loss_mask_1: 0.3211  loss_dice_1: 2.517  loss_ce_2: 0.2479  loss_mask_2: 0.3197  loss_dice_2: 2.491  loss_ce_3: 0.2337  loss_mask_3: 0.3184  loss_dice_3: 2.481  loss_ce_4: 0.2198  loss_mask_4: 0.3199  loss_dice_4: 2.478  loss_ce_5: 0.2118  loss_mask_5: 0.3192  loss_dice_5: 2.476  loss_ce_6: 0.2189  loss_mask_6: 0.318  loss_dice_6: 2.466  loss_ce_7: 0.2116  loss_mask_7: 0.3176  loss_dice_7: 2.474  loss_ce_8: 0.2122  loss_mask_8: 0.3171  loss_dice_8: 2.476  time: 1.4626  data_time: 0.0534  lr: 3.6836e-07  max_mem: 21589M
[01/18 12:42:15] d2.utils.events INFO:  eta: 0:22:17  iter: 38999  total_loss: 30.82  loss_ce: 0.2242  loss_mask: 0.3155  loss_dice: 2.487  loss_ce_0: 0.5893  loss_mask_0: 0.3153  loss_dice_0: 2.645  loss_ce_1: 0.2832  loss_mask_1: 0.3193  loss_dice_1: 2.543  loss_ce_2: 0.2712  loss_mask_2: 0.316  loss_dice_2: 2.511  loss_ce_3: 0.2397  loss_mask_3: 0.316  loss_dice_3: 2.502  loss_ce_4: 0.2281  loss_mask_4: 0.315  loss_dice_4: 2.499  loss_ce_5: 0.2333  loss_mask_5: 0.3154  loss_dice_5: 2.498  loss_ce_6: 0.2325  loss_mask_6: 0.3166  loss_dice_6: 2.493  loss_ce_7: 0.2157  loss_mask_7: 0.3146  loss_dice_7: 2.489  loss_ce_8: 0.212  loss_mask_8: 0.3141  loss_dice_8: 2.49  time: 1.4625  data_time: 0.0512  lr: 3.6186e-07  max_mem: 21589M
[01/18 12:42:42] d2.utils.events INFO:  eta: 0:21:51  iter: 39019  total_loss: 30.17  loss_ce: 0.1784  loss_mask: 0.3208  loss_dice: 2.474  loss_ce_0: 0.5205  loss_mask_0: 0.3164  loss_dice_0: 2.596  loss_ce_1: 0.236  loss_mask_1: 0.3265  loss_dice_1: 2.522  loss_ce_2: 0.2154  loss_mask_2: 0.3236  loss_dice_2: 2.489  loss_ce_3: 0.2032  loss_mask_3: 0.3212  loss_dice_3: 2.485  loss_ce_4: 0.2011  loss_mask_4: 0.3216  loss_dice_4: 2.475  loss_ce_5: 0.1973  loss_mask_5: 0.3212  loss_dice_5: 2.472  loss_ce_6: 0.1907  loss_mask_6: 0.3201  loss_dice_6: 2.456  loss_ce_7: 0.1781  loss_mask_7: 0.3208  loss_dice_7: 2.469  loss_ce_8: 0.175  loss_mask_8: 0.3207  loss_dice_8: 2.473  time: 1.4625  data_time: 0.0523  lr: 3.5534e-07  max_mem: 21589M
[01/18 12:43:09] d2.utils.events INFO:  eta: 0:21:24  iter: 39039  total_loss: 31.23  loss_ce: 0.203  loss_mask: 0.3162  loss_dice: 2.488  loss_ce_0: 0.5423  loss_mask_0: 0.3163  loss_dice_0: 2.644  loss_ce_1: 0.2612  loss_mask_1: 0.321  loss_dice_1: 2.551  loss_ce_2: 0.2574  loss_mask_2: 0.3176  loss_dice_2: 2.508  loss_ce_3: 0.2198  loss_mask_3: 0.3144  loss_dice_3: 2.494  loss_ce_4: 0.2201  loss_mask_4: 0.3159  loss_dice_4: 2.492  loss_ce_5: 0.2231  loss_mask_5: 0.3148  loss_dice_5: 2.492  loss_ce_6: 0.2142  loss_mask_6: 0.3161  loss_dice_6: 2.483  loss_ce_7: 0.2043  loss_mask_7: 0.3167  loss_dice_7: 2.488  loss_ce_8: 0.2157  loss_mask_8: 0.3163  loss_dice_8: 2.485  time: 1.4624  data_time: 0.0530  lr: 3.4882e-07  max_mem: 21589M
[01/18 12:43:36] d2.utils.events INFO:  eta: 0:20:58  iter: 39059  total_loss: 30.19  loss_ce: 0.1986  loss_mask: 0.3198  loss_dice: 2.452  loss_ce_0: 0.5153  loss_mask_0: 0.3181  loss_dice_0: 2.603  loss_ce_1: 0.2638  loss_mask_1: 0.3244  loss_dice_1: 2.51  loss_ce_2: 0.244  loss_mask_2: 0.3226  loss_dice_2: 2.485  loss_ce_3: 0.2288  loss_mask_3: 0.3199  loss_dice_3: 2.468  loss_ce_4: 0.2216  loss_mask_4: 0.32  loss_dice_4: 2.464  loss_ce_5: 0.2053  loss_mask_5: 0.3197  loss_dice_5: 2.467  loss_ce_6: 0.2126  loss_mask_6: 0.3188  loss_dice_6: 2.453  loss_ce_7: 0.1978  loss_mask_7: 0.3193  loss_dice_7: 2.448  loss_ce_8: 0.1935  loss_mask_8: 0.3202  loss_dice_8: 2.452  time: 1.4624  data_time: 0.0581  lr: 3.4228e-07  max_mem: 21589M
[01/18 12:44:03] d2.utils.events INFO:  eta: 0:20:31  iter: 39079  total_loss: 30.76  loss_ce: 0.2024  loss_mask: 0.3215  loss_dice: 2.49  loss_ce_0: 0.5504  loss_mask_0: 0.319  loss_dice_0: 2.63  loss_ce_1: 0.2576  loss_mask_1: 0.326  loss_dice_1: 2.542  loss_ce_2: 0.2546  loss_mask_2: 0.3221  loss_dice_2: 2.531  loss_ce_3: 0.2319  loss_mask_3: 0.3215  loss_dice_3: 2.514  loss_ce_4: 0.2185  loss_mask_4: 0.3211  loss_dice_4: 2.511  loss_ce_5: 0.2255  loss_mask_5: 0.32  loss_dice_5: 2.508  loss_ce_6: 0.2129  loss_mask_6: 0.3205  loss_dice_6: 2.5  loss_ce_7: 0.199  loss_mask_7: 0.3201  loss_dice_7: 2.5  loss_ce_8: 0.2027  loss_mask_8: 0.3208  loss_dice_8: 2.496  time: 1.4623  data_time: 0.0543  lr: 3.3572e-07  max_mem: 21589M
[01/18 12:44:30] d2.utils.events INFO:  eta: 0:20:03  iter: 39099  total_loss: 31.25  loss_ce: 0.1918  loss_mask: 0.3139  loss_dice: 2.537  loss_ce_0: 0.5405  loss_mask_0: 0.3098  loss_dice_0: 2.694  loss_ce_1: 0.2449  loss_mask_1: 0.3174  loss_dice_1: 2.598  loss_ce_2: 0.2382  loss_mask_2: 0.3146  loss_dice_2: 2.564  loss_ce_3: 0.2269  loss_mask_3: 0.3132  loss_dice_3: 2.548  loss_ce_4: 0.2045  loss_mask_4: 0.3139  loss_dice_4: 2.552  loss_ce_5: 0.2001  loss_mask_5: 0.3135  loss_dice_5: 2.546  loss_ce_6: 0.1904  loss_mask_6: 0.3128  loss_dice_6: 2.547  loss_ce_7: 0.1926  loss_mask_7: 0.3123  loss_dice_7: 2.547  loss_ce_8: 0.1864  loss_mask_8: 0.313  loss_dice_8: 2.543  time: 1.4623  data_time: 0.0517  lr: 3.2915e-07  max_mem: 21589M
[01/18 12:44:57] d2.utils.events INFO:  eta: 0:19:37  iter: 39119  total_loss: 30.67  loss_ce: 0.1984  loss_mask: 0.3082  loss_dice: 2.469  loss_ce_0: 0.5436  loss_mask_0: 0.3058  loss_dice_0: 2.637  loss_ce_1: 0.266  loss_mask_1: 0.3112  loss_dice_1: 2.528  loss_ce_2: 0.2553  loss_mask_2: 0.3101  loss_dice_2: 2.507  loss_ce_3: 0.2308  loss_mask_3: 0.3082  loss_dice_3: 2.487  loss_ce_4: 0.2281  loss_mask_4: 0.3073  loss_dice_4: 2.484  loss_ce_5: 0.2233  loss_mask_5: 0.3068  loss_dice_5: 2.478  loss_ce_6: 0.218  loss_mask_6: 0.3083  loss_dice_6: 2.472  loss_ce_7: 0.2066  loss_mask_7: 0.3071  loss_dice_7: 2.474  loss_ce_8: 0.1985  loss_mask_8: 0.3074  loss_dice_8: 2.471  time: 1.4622  data_time: 0.0539  lr: 3.2257e-07  max_mem: 21589M
[01/18 12:45:24] d2.utils.events INFO:  eta: 0:19:11  iter: 39139  total_loss: 30.62  loss_ce: 0.1929  loss_mask: 0.3236  loss_dice: 2.489  loss_ce_0: 0.5523  loss_mask_0: 0.326  loss_dice_0: 2.622  loss_ce_1: 0.2479  loss_mask_1: 0.3302  loss_dice_1: 2.537  loss_ce_2: 0.2322  loss_mask_2: 0.3266  loss_dice_2: 2.51  loss_ce_3: 0.2319  loss_mask_3: 0.3253  loss_dice_3: 2.497  loss_ce_4: 0.2135  loss_mask_4: 0.3236  loss_dice_4: 2.492  loss_ce_5: 0.2099  loss_mask_5: 0.3231  loss_dice_5: 2.489  loss_ce_6: 0.1996  loss_mask_6: 0.323  loss_dice_6: 2.485  loss_ce_7: 0.198  loss_mask_7: 0.3235  loss_dice_7: 2.489  loss_ce_8: 0.197  loss_mask_8: 0.324  loss_dice_8: 2.489  time: 1.4621  data_time: 0.0517  lr: 3.1597e-07  max_mem: 21589M
[01/18 12:45:51] d2.utils.events INFO:  eta: 0:18:44  iter: 39159  total_loss: 30.98  loss_ce: 0.1828  loss_mask: 0.319  loss_dice: 2.527  loss_ce_0: 0.5417  loss_mask_0: 0.3143  loss_dice_0: 2.669  loss_ce_1: 0.2355  loss_mask_1: 0.3212  loss_dice_1: 2.575  loss_ce_2: 0.2344  loss_mask_2: 0.32  loss_dice_2: 2.547  loss_ce_3: 0.2034  loss_mask_3: 0.3202  loss_dice_3: 2.533  loss_ce_4: 0.1981  loss_mask_4: 0.3193  loss_dice_4: 2.529  loss_ce_5: 0.1905  loss_mask_5: 0.3188  loss_dice_5: 2.53  loss_ce_6: 0.1909  loss_mask_6: 0.3184  loss_dice_6: 2.521  loss_ce_7: 0.1948  loss_mask_7: 0.3191  loss_dice_7: 2.522  loss_ce_8: 0.1892  loss_mask_8: 0.3183  loss_dice_8: 2.531  time: 1.4621  data_time: 0.0592  lr: 3.0936e-07  max_mem: 21589M
[01/18 12:46:19] d2.utils.events INFO:  eta: 0:18:17  iter: 39179  total_loss: 31.64  loss_ce: 0.207  loss_mask: 0.3157  loss_dice: 2.512  loss_ce_0: 0.5462  loss_mask_0: 0.3155  loss_dice_0: 2.671  loss_ce_1: 0.271  loss_mask_1: 0.3206  loss_dice_1: 2.565  loss_ce_2: 0.2531  loss_mask_2: 0.318  loss_dice_2: 2.542  loss_ce_3: 0.2331  loss_mask_3: 0.3163  loss_dice_3: 2.523  loss_ce_4: 0.2187  loss_mask_4: 0.3152  loss_dice_4: 2.528  loss_ce_5: 0.2211  loss_mask_5: 0.3156  loss_dice_5: 2.523  loss_ce_6: 0.2039  loss_mask_6: 0.3159  loss_dice_6: 2.516  loss_ce_7: 0.2025  loss_mask_7: 0.3162  loss_dice_7: 2.523  loss_ce_8: 0.2028  loss_mask_8: 0.3168  loss_dice_8: 2.516  time: 1.4621  data_time: 0.0568  lr: 3.0273e-07  max_mem: 21589M
[01/18 12:46:47] d2.utils.events INFO:  eta: 0:17:51  iter: 39199  total_loss: 31.3  loss_ce: 0.2101  loss_mask: 0.3232  loss_dice: 2.532  loss_ce_0: 0.5678  loss_mask_0: 0.3247  loss_dice_0: 2.676  loss_ce_1: 0.27  loss_mask_1: 0.3292  loss_dice_1: 2.582  loss_ce_2: 0.2591  loss_mask_2: 0.326  loss_dice_2: 2.548  loss_ce_3: 0.2413  loss_mask_3: 0.3249  loss_dice_3: 2.531  loss_ce_4: 0.2314  loss_mask_4: 0.3233  loss_dice_4: 2.521  loss_ce_5: 0.2318  loss_mask_5: 0.3238  loss_dice_5: 2.524  loss_ce_6: 0.229  loss_mask_6: 0.3245  loss_dice_6: 2.529  loss_ce_7: 0.2163  loss_mask_7: 0.3233  loss_dice_7: 2.526  loss_ce_8: 0.2068  loss_mask_8: 0.3226  loss_dice_8: 2.518  time: 1.4620  data_time: 0.0564  lr: 2.9608e-07  max_mem: 21589M
[01/18 12:47:14] d2.utils.events INFO:  eta: 0:17:25  iter: 39219  total_loss: 31.03  loss_ce: 0.206  loss_mask: 0.319  loss_dice: 2.517  loss_ce_0: 0.5576  loss_mask_0: 0.3191  loss_dice_0: 2.674  loss_ce_1: 0.2655  loss_mask_1: 0.3228  loss_dice_1: 2.562  loss_ce_2: 0.2494  loss_mask_2: 0.3192  loss_dice_2: 2.534  loss_ce_3: 0.2197  loss_mask_3: 0.3191  loss_dice_3: 2.522  loss_ce_4: 0.2143  loss_mask_4: 0.3195  loss_dice_4: 2.516  loss_ce_5: 0.2064  loss_mask_5: 0.3202  loss_dice_5: 2.522  loss_ce_6: 0.2267  loss_mask_6: 0.3186  loss_dice_6: 2.515  loss_ce_7: 0.2116  loss_mask_7: 0.3199  loss_dice_7: 2.518  loss_ce_8: 0.2048  loss_mask_8: 0.3197  loss_dice_8: 2.506  time: 1.4620  data_time: 0.0562  lr: 2.8942e-07  max_mem: 21589M
[01/18 12:47:41] d2.utils.events INFO:  eta: 0:16:59  iter: 39239  total_loss: 30.28  loss_ce: 0.1944  loss_mask: 0.3275  loss_dice: 2.441  loss_ce_0: 0.5272  loss_mask_0: 0.3287  loss_dice_0: 2.578  loss_ce_1: 0.2526  loss_mask_1: 0.3324  loss_dice_1: 2.474  loss_ce_2: 0.2463  loss_mask_2: 0.3281  loss_dice_2: 2.455  loss_ce_3: 0.2197  loss_mask_3: 0.3277  loss_dice_3: 2.442  loss_ce_4: 0.2192  loss_mask_4: 0.3294  loss_dice_4: 2.434  loss_ce_5: 0.2077  loss_mask_5: 0.3273  loss_dice_5: 2.438  loss_ce_6: 0.2094  loss_mask_6: 0.3273  loss_dice_6: 2.434  loss_ce_7: 0.1949  loss_mask_7: 0.327  loss_dice_7: 2.437  loss_ce_8: 0.1965  loss_mask_8: 0.3282  loss_dice_8: 2.441  time: 1.4619  data_time: 0.0536  lr: 2.8274e-07  max_mem: 21589M
[01/18 12:48:08] d2.utils.events INFO:  eta: 0:16:31  iter: 39259  total_loss: 30.23  loss_ce: 0.1903  loss_mask: 0.3155  loss_dice: 2.429  loss_ce_0: 0.5134  loss_mask_0: 0.3202  loss_dice_0: 2.58  loss_ce_1: 0.2451  loss_mask_1: 0.3222  loss_dice_1: 2.474  loss_ce_2: 0.2308  loss_mask_2: 0.319  loss_dice_2: 2.451  loss_ce_3: 0.2205  loss_mask_3: 0.3176  loss_dice_3: 2.43  loss_ce_4: 0.2074  loss_mask_4: 0.3166  loss_dice_4: 2.429  loss_ce_5: 0.2011  loss_mask_5: 0.3155  loss_dice_5: 2.436  loss_ce_6: 0.1953  loss_mask_6: 0.3165  loss_dice_6: 2.433  loss_ce_7: 0.1969  loss_mask_7: 0.3165  loss_dice_7: 2.425  loss_ce_8: 0.1949  loss_mask_8: 0.3158  loss_dice_8: 2.427  time: 1.4618  data_time: 0.0560  lr: 2.7605e-07  max_mem: 21589M
[01/18 12:48:35] d2.utils.events INFO:  eta: 0:16:05  iter: 39279  total_loss: 31.2  loss_ce: 0.1946  loss_mask: 0.312  loss_dice: 2.53  loss_ce_0: 0.552  loss_mask_0: 0.3079  loss_dice_0: 2.667  loss_ce_1: 0.2719  loss_mask_1: 0.318  loss_dice_1: 2.568  loss_ce_2: 0.2714  loss_mask_2: 0.3148  loss_dice_2: 2.542  loss_ce_3: 0.2336  loss_mask_3: 0.3133  loss_dice_3: 2.521  loss_ce_4: 0.2238  loss_mask_4: 0.3128  loss_dice_4: 2.524  loss_ce_5: 0.2236  loss_mask_5: 0.3111  loss_dice_5: 2.528  loss_ce_6: 0.2126  loss_mask_6: 0.3122  loss_dice_6: 2.516  loss_ce_7: 0.2116  loss_mask_7: 0.3125  loss_dice_7: 2.523  loss_ce_8: 0.2126  loss_mask_8: 0.3125  loss_dice_8: 2.526  time: 1.4618  data_time: 0.0547  lr: 2.6933e-07  max_mem: 21589M
[01/18 12:49:02] d2.utils.events INFO:  eta: 0:15:39  iter: 39299  total_loss: 30.74  loss_ce: 0.19  loss_mask: 0.3215  loss_dice: 2.486  loss_ce_0: 0.5376  loss_mask_0: 0.3205  loss_dice_0: 2.617  loss_ce_1: 0.2649  loss_mask_1: 0.3248  loss_dice_1: 2.526  loss_ce_2: 0.2387  loss_mask_2: 0.3227  loss_dice_2: 2.505  loss_ce_3: 0.2277  loss_mask_3: 0.3223  loss_dice_3: 2.492  loss_ce_4: 0.2146  loss_mask_4: 0.3211  loss_dice_4: 2.49  loss_ce_5: 0.2153  loss_mask_5: 0.3196  loss_dice_5: 2.488  loss_ce_6: 0.2014  loss_mask_6: 0.3212  loss_dice_6: 2.494  loss_ce_7: 0.1962  loss_mask_7: 0.3216  loss_dice_7: 2.486  loss_ce_8: 0.195  loss_mask_8: 0.3227  loss_dice_8: 2.485  time: 1.4617  data_time: 0.0578  lr: 2.626e-07  max_mem: 21589M
[01/18 12:49:30] d2.utils.events INFO:  eta: 0:15:12  iter: 39319  total_loss: 31.04  loss_ce: 0.2202  loss_mask: 0.3151  loss_dice: 2.476  loss_ce_0: 0.5732  loss_mask_0: 0.3113  loss_dice_0: 2.632  loss_ce_1: 0.2655  loss_mask_1: 0.3177  loss_dice_1: 2.53  loss_ce_2: 0.2795  loss_mask_2: 0.3177  loss_dice_2: 2.502  loss_ce_3: 0.2504  loss_mask_3: 0.3162  loss_dice_3: 2.484  loss_ce_4: 0.2249  loss_mask_4: 0.3153  loss_dice_4: 2.477  loss_ce_5: 0.2232  loss_mask_5: 0.3157  loss_dice_5: 2.481  loss_ce_6: 0.2279  loss_mask_6: 0.3159  loss_dice_6: 2.475  loss_ce_7: 0.2324  loss_mask_7: 0.3166  loss_dice_7: 2.472  loss_ce_8: 0.2259  loss_mask_8: 0.3157  loss_dice_8: 2.479  time: 1.4617  data_time: 0.0580  lr: 2.5585e-07  max_mem: 21589M
[01/18 12:49:58] d2.utils.events INFO:  eta: 0:14:45  iter: 39339  total_loss: 31.42  loss_ce: 0.2002  loss_mask: 0.3227  loss_dice: 2.513  loss_ce_0: 0.5573  loss_mask_0: 0.3168  loss_dice_0: 2.664  loss_ce_1: 0.2578  loss_mask_1: 0.3258  loss_dice_1: 2.573  loss_ce_2: 0.2472  loss_mask_2: 0.3232  loss_dice_2: 2.539  loss_ce_3: 0.2212  loss_mask_3: 0.3222  loss_dice_3: 2.519  loss_ce_4: 0.2242  loss_mask_4: 0.3221  loss_dice_4: 2.519  loss_ce_5: 0.2144  loss_mask_5: 0.3226  loss_dice_5: 2.512  loss_ce_6: 0.2035  loss_mask_6: 0.3215  loss_dice_6: 2.521  loss_ce_7: 0.192  loss_mask_7: 0.3219  loss_dice_7: 2.518  loss_ce_8: 0.1933  loss_mask_8: 0.3223  loss_dice_8: 2.506  time: 1.4617  data_time: 0.0605  lr: 2.4907e-07  max_mem: 21589M
[01/18 12:50:25] d2.utils.events INFO:  eta: 0:14:19  iter: 39359  total_loss: 31.07  loss_ce: 0.2004  loss_mask: 0.3196  loss_dice: 2.495  loss_ce_0: 0.5553  loss_mask_0: 0.3169  loss_dice_0: 2.646  loss_ce_1: 0.2638  loss_mask_1: 0.3251  loss_dice_1: 2.543  loss_ce_2: 0.2498  loss_mask_2: 0.3207  loss_dice_2: 2.52  loss_ce_3: 0.2324  loss_mask_3: 0.3199  loss_dice_3: 2.505  loss_ce_4: 0.2268  loss_mask_4: 0.32  loss_dice_4: 2.503  loss_ce_5: 0.2026  loss_mask_5: 0.3187  loss_dice_5: 2.514  loss_ce_6: 0.2032  loss_mask_6: 0.3197  loss_dice_6: 2.504  loss_ce_7: 0.1989  loss_mask_7: 0.3194  loss_dice_7: 2.497  loss_ce_8: 0.2005  loss_mask_8: 0.3199  loss_dice_8: 2.502  time: 1.4616  data_time: 0.0568  lr: 2.4228e-07  max_mem: 21589M
[01/18 12:50:52] d2.utils.events INFO:  eta: 0:13:52  iter: 39379  total_loss: 30.93  loss_ce: 0.2013  loss_mask: 0.3194  loss_dice: 2.489  loss_ce_0: 0.5606  loss_mask_0: 0.3201  loss_dice_0: 2.634  loss_ce_1: 0.2555  loss_mask_1: 0.3227  loss_dice_1: 2.54  loss_ce_2: 0.2721  loss_mask_2: 0.3193  loss_dice_2: 2.515  loss_ce_3: 0.2249  loss_mask_3: 0.3195  loss_dice_3: 2.501  loss_ce_4: 0.2096  loss_mask_4: 0.3186  loss_dice_4: 2.496  loss_ce_5: 0.204  loss_mask_5: 0.3167  loss_dice_5: 2.498  loss_ce_6: 0.2168  loss_mask_6: 0.3171  loss_dice_6: 2.492  loss_ce_7: 0.2052  loss_mask_7: 0.3178  loss_dice_7: 2.49  loss_ce_8: 0.1987  loss_mask_8: 0.3176  loss_dice_8: 2.485  time: 1.4615  data_time: 0.0534  lr: 2.3547e-07  max_mem: 21589M
[01/18 12:51:19] d2.utils.events INFO:  eta: 0:13:26  iter: 39399  total_loss: 30.79  loss_ce: 0.2067  loss_mask: 0.3175  loss_dice: 2.48  loss_ce_0: 0.5556  loss_mask_0: 0.3156  loss_dice_0: 2.623  loss_ce_1: 0.2661  loss_mask_1: 0.322  loss_dice_1: 2.527  loss_ce_2: 0.2588  loss_mask_2: 0.3192  loss_dice_2: 2.511  loss_ce_3: 0.2281  loss_mask_3: 0.3177  loss_dice_3: 2.489  loss_ce_4: 0.222  loss_mask_4: 0.3177  loss_dice_4: 2.489  loss_ce_5: 0.2213  loss_mask_5: 0.3176  loss_dice_5: 2.494  loss_ce_6: 0.2164  loss_mask_6: 0.3175  loss_dice_6: 2.493  loss_ce_7: 0.2017  loss_mask_7: 0.3166  loss_dice_7: 2.489  loss_ce_8: 0.2058  loss_mask_8: 0.3183  loss_dice_8: 2.481  time: 1.4615  data_time: 0.0593  lr: 2.2863e-07  max_mem: 21589M
[01/18 12:51:46] d2.utils.events INFO:  eta: 0:12:59  iter: 39419  total_loss: 31.22  loss_ce: 0.2036  loss_mask: 0.3122  loss_dice: 2.502  loss_ce_0: 0.55  loss_mask_0: 0.3109  loss_dice_0: 2.663  loss_ce_1: 0.2575  loss_mask_1: 0.3188  loss_dice_1: 2.566  loss_ce_2: 0.2628  loss_mask_2: 0.3168  loss_dice_2: 2.536  loss_ce_3: 0.238  loss_mask_3: 0.314  loss_dice_3: 2.513  loss_ce_4: 0.2334  loss_mask_4: 0.3133  loss_dice_4: 2.515  loss_ce_5: 0.2349  loss_mask_5: 0.3122  loss_dice_5: 2.512  loss_ce_6: 0.2227  loss_mask_6: 0.313  loss_dice_6: 2.511  loss_ce_7: 0.1977  loss_mask_7: 0.3121  loss_dice_7: 2.509  loss_ce_8: 0.2083  loss_mask_8: 0.3133  loss_dice_8: 2.502  time: 1.4614  data_time: 0.0552  lr: 2.2177e-07  max_mem: 21589M
[01/18 12:52:13] d2.utils.events INFO:  eta: 0:12:32  iter: 39439  total_loss: 31.12  loss_ce: 0.1967  loss_mask: 0.3257  loss_dice: 2.486  loss_ce_0: 0.5359  loss_mask_0: 0.321  loss_dice_0: 2.637  loss_ce_1: 0.2592  loss_mask_1: 0.3293  loss_dice_1: 2.534  loss_ce_2: 0.2513  loss_mask_2: 0.326  loss_dice_2: 2.512  loss_ce_3: 0.2277  loss_mask_3: 0.3244  loss_dice_3: 2.49  loss_ce_4: 0.2293  loss_mask_4: 0.3246  loss_dice_4: 2.498  loss_ce_5: 0.2191  loss_mask_5: 0.3239  loss_dice_5: 2.491  loss_ce_6: 0.2126  loss_mask_6: 0.3233  loss_dice_6: 2.487  loss_ce_7: 0.2111  loss_mask_7: 0.324  loss_dice_7: 2.485  loss_ce_8: 0.2057  loss_mask_8: 0.3249  loss_dice_8: 2.488  time: 1.4614  data_time: 0.0549  lr: 2.1489e-07  max_mem: 21589M
[01/18 12:52:40] d2.utils.events INFO:  eta: 0:12:05  iter: 39459  total_loss: 31.06  loss_ce: 0.2138  loss_mask: 0.3108  loss_dice: 2.498  loss_ce_0: 0.5433  loss_mask_0: 0.3039  loss_dice_0: 2.656  loss_ce_1: 0.2797  loss_mask_1: 0.3139  loss_dice_1: 2.55  loss_ce_2: 0.2589  loss_mask_2: 0.3118  loss_dice_2: 2.52  loss_ce_3: 0.2365  loss_mask_3: 0.3095  loss_dice_3: 2.504  loss_ce_4: 0.2307  loss_mask_4: 0.3099  loss_dice_4: 2.51  loss_ce_5: 0.2322  loss_mask_5: 0.3103  loss_dice_5: 2.504  loss_ce_6: 0.2164  loss_mask_6: 0.3108  loss_dice_6: 2.495  loss_ce_7: 0.2175  loss_mask_7: 0.3095  loss_dice_7: 2.51  loss_ce_8: 0.2095  loss_mask_8: 0.3098  loss_dice_8: 2.492  time: 1.4613  data_time: 0.0574  lr: 2.0798e-07  max_mem: 21589M
[01/18 12:53:07] d2.utils.events INFO:  eta: 0:11:39  iter: 39479  total_loss: 30.86  loss_ce: 0.2044  loss_mask: 0.3133  loss_dice: 2.459  loss_ce_0: 0.5663  loss_mask_0: 0.308  loss_dice_0: 2.612  loss_ce_1: 0.2844  loss_mask_1: 0.3157  loss_dice_1: 2.515  loss_ce_2: 0.2624  loss_mask_2: 0.315  loss_dice_2: 2.501  loss_ce_3: 0.2464  loss_mask_3: 0.3132  loss_dice_3: 2.483  loss_ce_4: 0.229  loss_mask_4: 0.3126  loss_dice_4: 2.484  loss_ce_5: 0.2251  loss_mask_5: 0.3122  loss_dice_5: 2.47  loss_ce_6: 0.2127  loss_mask_6: 0.3115  loss_dice_6: 2.463  loss_ce_7: 0.2225  loss_mask_7: 0.3116  loss_dice_7: 2.472  loss_ce_8: 0.2137  loss_mask_8: 0.3127  loss_dice_8: 2.468  time: 1.4613  data_time: 0.0540  lr: 2.0105e-07  max_mem: 21589M
[01/18 12:53:34] d2.utils.events INFO:  eta: 0:11:12  iter: 39499  total_loss: 30.23  loss_ce: 0.2022  loss_mask: 0.315  loss_dice: 2.468  loss_ce_0: 0.5354  loss_mask_0: 0.3132  loss_dice_0: 2.619  loss_ce_1: 0.258  loss_mask_1: 0.3204  loss_dice_1: 2.516  loss_ce_2: 0.25  loss_mask_2: 0.3169  loss_dice_2: 2.5  loss_ce_3: 0.2331  loss_mask_3: 0.317  loss_dice_3: 2.483  loss_ce_4: 0.2196  loss_mask_4: 0.3159  loss_dice_4: 2.478  loss_ce_5: 0.203  loss_mask_5: 0.3142  loss_dice_5: 2.471  loss_ce_6: 0.2038  loss_mask_6: 0.3163  loss_dice_6: 2.471  loss_ce_7: 0.2041  loss_mask_7: 0.3154  loss_dice_7: 2.472  loss_ce_8: 0.1982  loss_mask_8: 0.3151  loss_dice_8: 2.471  time: 1.4612  data_time: 0.0549  lr: 1.9409e-07  max_mem: 21589M
[01/18 12:54:01] d2.utils.events INFO:  eta: 0:10:46  iter: 39519  total_loss: 31.01  loss_ce: 0.2052  loss_mask: 0.3183  loss_dice: 2.491  loss_ce_0: 0.5558  loss_mask_0: 0.3162  loss_dice_0: 2.642  loss_ce_1: 0.2762  loss_mask_1: 0.323  loss_dice_1: 2.551  loss_ce_2: 0.2568  loss_mask_2: 0.32  loss_dice_2: 2.517  loss_ce_3: 0.2376  loss_mask_3: 0.3184  loss_dice_3: 2.5  loss_ce_4: 0.226  loss_mask_4: 0.3178  loss_dice_4: 2.509  loss_ce_5: 0.2099  loss_mask_5: 0.3175  loss_dice_5: 2.504  loss_ce_6: 0.204  loss_mask_6: 0.3177  loss_dice_6: 2.5  loss_ce_7: 0.204  loss_mask_7: 0.3177  loss_dice_7: 2.494  loss_ce_8: 0.2079  loss_mask_8: 0.3173  loss_dice_8: 2.504  time: 1.4611  data_time: 0.0546  lr: 1.871e-07  max_mem: 21589M
[01/18 12:54:28] d2.utils.events INFO:  eta: 0:10:19  iter: 39539  total_loss: 30.93  loss_ce: 0.2086  loss_mask: 0.3229  loss_dice: 2.482  loss_ce_0: 0.565  loss_mask_0: 0.3202  loss_dice_0: 2.655  loss_ce_1: 0.2767  loss_mask_1: 0.3291  loss_dice_1: 2.53  loss_ce_2: 0.2618  loss_mask_2: 0.3242  loss_dice_2: 2.517  loss_ce_3: 0.2341  loss_mask_3: 0.3227  loss_dice_3: 2.487  loss_ce_4: 0.2222  loss_mask_4: 0.3217  loss_dice_4: 2.49  loss_ce_5: 0.2131  loss_mask_5: 0.3224  loss_dice_5: 2.492  loss_ce_6: 0.206  loss_mask_6: 0.3213  loss_dice_6: 2.492  loss_ce_7: 0.2123  loss_mask_7: 0.3223  loss_dice_7: 2.492  loss_ce_8: 0.212  loss_mask_8: 0.3227  loss_dice_8: 2.489  time: 1.4611  data_time: 0.0545  lr: 1.8008e-07  max_mem: 21589M
[01/18 12:54:55] d2.utils.events INFO:  eta: 0:09:52  iter: 39559  total_loss: 30.73  loss_ce: 0.2149  loss_mask: 0.3266  loss_dice: 2.451  loss_ce_0: 0.5493  loss_mask_0: 0.3254  loss_dice_0: 2.592  loss_ce_1: 0.2693  loss_mask_1: 0.3323  loss_dice_1: 2.491  loss_ce_2: 0.278  loss_mask_2: 0.3293  loss_dice_2: 2.475  loss_ce_3: 0.2492  loss_mask_3: 0.3272  loss_dice_3: 2.467  loss_ce_4: 0.2276  loss_mask_4: 0.3267  loss_dice_4: 2.45  loss_ce_5: 0.2205  loss_mask_5: 0.3266  loss_dice_5: 2.459  loss_ce_6: 0.2129  loss_mask_6: 0.3257  loss_dice_6: 2.454  loss_ce_7: 0.2241  loss_mask_7: 0.3257  loss_dice_7: 2.45  loss_ce_8: 0.2115  loss_mask_8: 0.3266  loss_dice_8: 2.451  time: 1.4610  data_time: 0.0569  lr: 1.7304e-07  max_mem: 21589M
[01/18 12:55:22] d2.utils.events INFO:  eta: 0:09:25  iter: 39579  total_loss: 31.08  loss_ce: 0.2036  loss_mask: 0.3193  loss_dice: 2.503  loss_ce_0: 0.5775  loss_mask_0: 0.3178  loss_dice_0: 2.664  loss_ce_1: 0.2819  loss_mask_1: 0.3238  loss_dice_1: 2.569  loss_ce_2: 0.2692  loss_mask_2: 0.323  loss_dice_2: 2.542  loss_ce_3: 0.2504  loss_mask_3: 0.3213  loss_dice_3: 2.516  loss_ce_4: 0.2348  loss_mask_4: 0.3196  loss_dice_4: 2.523  loss_ce_5: 0.2214  loss_mask_5: 0.3193  loss_dice_5: 2.515  loss_ce_6: 0.2116  loss_mask_6: 0.3201  loss_dice_6: 2.518  loss_ce_7: 0.2051  loss_mask_7: 0.3208  loss_dice_7: 2.517  loss_ce_8: 0.2026  loss_mask_8: 0.3197  loss_dice_8: 2.51  time: 1.4610  data_time: 0.0563  lr: 1.6596e-07  max_mem: 21589M
[01/18 12:55:48] d2.utils.events INFO:  eta: 0:08:58  iter: 39599  total_loss: 30.01  loss_ce: 0.211  loss_mask: 0.3178  loss_dice: 2.422  loss_ce_0: 0.5198  loss_mask_0: 0.3133  loss_dice_0: 2.576  loss_ce_1: 0.2616  loss_mask_1: 0.3249  loss_dice_1: 2.472  loss_ce_2: 0.2607  loss_mask_2: 0.321  loss_dice_2: 2.451  loss_ce_3: 0.2391  loss_mask_3: 0.3187  loss_dice_3: 2.426  loss_ce_4: 0.2232  loss_mask_4: 0.3187  loss_dice_4: 2.429  loss_ce_5: 0.2198  loss_mask_5: 0.3168  loss_dice_5: 2.42  loss_ce_6: 0.2117  loss_mask_6: 0.3175  loss_dice_6: 2.433  loss_ce_7: 0.2159  loss_mask_7: 0.3176  loss_dice_7: 2.425  loss_ce_8: 0.2042  loss_mask_8: 0.3182  loss_dice_8: 2.417  time: 1.4609  data_time: 0.0512  lr: 1.5885e-07  max_mem: 21589M
[01/18 12:56:15] d2.utils.events INFO:  eta: 0:08:31  iter: 39619  total_loss: 30.23  loss_ce: 0.1997  loss_mask: 0.3117  loss_dice: 2.434  loss_ce_0: 0.5278  loss_mask_0: 0.31  loss_dice_0: 2.586  loss_ce_1: 0.2511  loss_mask_1: 0.3159  loss_dice_1: 2.482  loss_ce_2: 0.2396  loss_mask_2: 0.312  loss_dice_2: 2.468  loss_ce_3: 0.2224  loss_mask_3: 0.3109  loss_dice_3: 2.443  loss_ce_4: 0.2006  loss_mask_4: 0.3105  loss_dice_4: 2.444  loss_ce_5: 0.2179  loss_mask_5: 0.3109  loss_dice_5: 2.441  loss_ce_6: 0.1945  loss_mask_6: 0.3102  loss_dice_6: 2.438  loss_ce_7: 0.1844  loss_mask_7: 0.3114  loss_dice_7: 2.437  loss_ce_8: 0.202  loss_mask_8: 0.3117  loss_dice_8: 2.439  time: 1.4608  data_time: 0.0550  lr: 1.517e-07  max_mem: 21589M
[01/18 12:56:43] d2.utils.events INFO:  eta: 0:08:04  iter: 39639  total_loss: 31.07  loss_ce: 0.1912  loss_mask: 0.3249  loss_dice: 2.551  loss_ce_0: 0.546  loss_mask_0: 0.3234  loss_dice_0: 2.685  loss_ce_1: 0.2503  loss_mask_1: 0.3298  loss_dice_1: 2.588  loss_ce_2: 0.2413  loss_mask_2: 0.3263  loss_dice_2: 2.566  loss_ce_3: 0.2173  loss_mask_3: 0.3257  loss_dice_3: 2.562  loss_ce_4: 0.2024  loss_mask_4: 0.3263  loss_dice_4: 2.55  loss_ce_5: 0.1974  loss_mask_5: 0.3258  loss_dice_5: 2.555  loss_ce_6: 0.2032  loss_mask_6: 0.326  loss_dice_6: 2.547  loss_ce_7: 0.1945  loss_mask_7: 0.3255  loss_dice_7: 2.543  loss_ce_8: 0.1797  loss_mask_8: 0.3254  loss_dice_8: 2.553  time: 1.4608  data_time: 0.0535  lr: 1.4451e-07  max_mem: 21589M
[01/18 12:57:09] d2.utils.events INFO:  eta: 0:07:37  iter: 39659  total_loss: 30.7  loss_ce: 0.1918  loss_mask: 0.313  loss_dice: 2.446  loss_ce_0: 0.5461  loss_mask_0: 0.3123  loss_dice_0: 2.598  loss_ce_1: 0.2404  loss_mask_1: 0.3177  loss_dice_1: 2.492  loss_ce_2: 0.2506  loss_mask_2: 0.3161  loss_dice_2: 2.469  loss_ce_3: 0.2225  loss_mask_3: 0.3142  loss_dice_3: 2.453  loss_ce_4: 0.2052  loss_mask_4: 0.3144  loss_dice_4: 2.448  loss_ce_5: 0.1968  loss_mask_5: 0.3144  loss_dice_5: 2.453  loss_ce_6: 0.2004  loss_mask_6: 0.3143  loss_dice_6: 2.442  loss_ce_7: 0.2004  loss_mask_7: 0.3133  loss_dice_7: 2.437  loss_ce_8: 0.1975  loss_mask_8: 0.3124  loss_dice_8: 2.45  time: 1.4607  data_time: 0.0519  lr: 1.3729e-07  max_mem: 21589M
[01/18 12:57:36] d2.utils.events INFO:  eta: 0:07:10  iter: 39679  total_loss: 30.52  loss_ce: 0.2019  loss_mask: 0.3232  loss_dice: 2.449  loss_ce_0: 0.5298  loss_mask_0: 0.323  loss_dice_0: 2.587  loss_ce_1: 0.2481  loss_mask_1: 0.3282  loss_dice_1: 2.495  loss_ce_2: 0.2402  loss_mask_2: 0.3237  loss_dice_2: 2.467  loss_ce_3: 0.2419  loss_mask_3: 0.3234  loss_dice_3: 2.452  loss_ce_4: 0.2059  loss_mask_4: 0.3239  loss_dice_4: 2.458  loss_ce_5: 0.2035  loss_mask_5: 0.3212  loss_dice_5: 2.456  loss_ce_6: 0.2072  loss_mask_6: 0.3223  loss_dice_6: 2.453  loss_ce_7: 0.2029  loss_mask_7: 0.3221  loss_dice_7: 2.443  loss_ce_8: 0.1949  loss_mask_8: 0.3224  loss_dice_8: 2.454  time: 1.4606  data_time: 0.0502  lr: 1.3002e-07  max_mem: 21589M
[01/18 12:58:02] d2.utils.events INFO:  eta: 0:06:43  iter: 39699  total_loss: 30.89  loss_ce: 0.2059  loss_mask: 0.3156  loss_dice: 2.495  loss_ce_0: 0.54  loss_mask_0: 0.3159  loss_dice_0: 2.65  loss_ce_1: 0.2676  loss_mask_1: 0.3214  loss_dice_1: 2.546  loss_ce_2: 0.2508  loss_mask_2: 0.3166  loss_dice_2: 2.523  loss_ce_3: 0.2376  loss_mask_3: 0.3156  loss_dice_3: 2.511  loss_ce_4: 0.228  loss_mask_4: 0.3158  loss_dice_4: 2.506  loss_ce_5: 0.2235  loss_mask_5: 0.3156  loss_dice_5: 2.498  loss_ce_6: 0.2093  loss_mask_6: 0.315  loss_dice_6: 2.494  loss_ce_7: 0.2069  loss_mask_7: 0.3145  loss_dice_7: 2.5  loss_ce_8: 0.2114  loss_mask_8: 0.3142  loss_dice_8: 2.493  time: 1.4606  data_time: 0.0524  lr: 1.227e-07  max_mem: 21589M
[01/18 12:58:29] d2.utils.events INFO:  eta: 0:06:16  iter: 39719  total_loss: 30.36  loss_ce: 0.1953  loss_mask: 0.3229  loss_dice: 2.451  loss_ce_0: 0.5304  loss_mask_0: 0.3225  loss_dice_0: 2.611  loss_ce_1: 0.2701  loss_mask_1: 0.3305  loss_dice_1: 2.506  loss_ce_2: 0.2435  loss_mask_2: 0.3258  loss_dice_2: 2.481  loss_ce_3: 0.2201  loss_mask_3: 0.3235  loss_dice_3: 2.453  loss_ce_4: 0.2091  loss_mask_4: 0.3236  loss_dice_4: 2.455  loss_ce_5: 0.2023  loss_mask_5: 0.3227  loss_dice_5: 2.446  loss_ce_6: 0.1981  loss_mask_6: 0.3238  loss_dice_6: 2.444  loss_ce_7: 0.1976  loss_mask_7: 0.3226  loss_dice_7: 2.443  loss_ce_8: 0.1982  loss_mask_8: 0.3227  loss_dice_8: 2.446  time: 1.4605  data_time: 0.0503  lr: 1.1534e-07  max_mem: 21589M
[01/18 12:58:56] d2.utils.events INFO:  eta: 0:05:49  iter: 39739  total_loss: 30.62  loss_ce: 0.206  loss_mask: 0.3166  loss_dice: 2.486  loss_ce_0: 0.5501  loss_mask_0: 0.3125  loss_dice_0: 2.635  loss_ce_1: 0.249  loss_mask_1: 0.3176  loss_dice_1: 2.543  loss_ce_2: 0.2545  loss_mask_2: 0.3159  loss_dice_2: 2.516  loss_ce_3: 0.2274  loss_mask_3: 0.3144  loss_dice_3: 2.498  loss_ce_4: 0.2185  loss_mask_4: 0.3152  loss_dice_4: 2.494  loss_ce_5: 0.2078  loss_mask_5: 0.3151  loss_dice_5: 2.494  loss_ce_6: 0.2147  loss_mask_6: 0.3158  loss_dice_6: 2.487  loss_ce_7: 0.21  loss_mask_7: 0.3158  loss_dice_7: 2.489  loss_ce_8: 0.1966  loss_mask_8: 0.3162  loss_dice_8: 2.494  time: 1.4605  data_time: 0.0533  lr: 1.0793e-07  max_mem: 21589M
[01/18 12:59:23] d2.utils.events INFO:  eta: 0:05:23  iter: 39759  total_loss: 30.28  loss_ce: 0.2031  loss_mask: 0.3111  loss_dice: 2.46  loss_ce_0: 0.5288  loss_mask_0: 0.3093  loss_dice_0: 2.627  loss_ce_1: 0.2544  loss_mask_1: 0.3144  loss_dice_1: 2.533  loss_ce_2: 0.25  loss_mask_2: 0.3134  loss_dice_2: 2.491  loss_ce_3: 0.2292  loss_mask_3: 0.3115  loss_dice_3: 2.482  loss_ce_4: 0.2311  loss_mask_4: 0.3107  loss_dice_4: 2.481  loss_ce_5: 0.2116  loss_mask_5: 0.3095  loss_dice_5: 2.475  loss_ce_6: 0.2066  loss_mask_6: 0.3107  loss_dice_6: 2.477  loss_ce_7: 0.2063  loss_mask_7: 0.3104  loss_dice_7: 2.471  loss_ce_8: 0.1995  loss_mask_8: 0.3111  loss_dice_8: 2.471  time: 1.4604  data_time: 0.0532  lr: 1.0045e-07  max_mem: 21589M
[01/18 12:59:49] d2.utils.events INFO:  eta: 0:04:56  iter: 39779  total_loss: 30.82  loss_ce: 0.2023  loss_mask: 0.3023  loss_dice: 2.471  loss_ce_0: 0.568  loss_mask_0: 0.3037  loss_dice_0: 2.622  loss_ce_1: 0.2721  loss_mask_1: 0.3089  loss_dice_1: 2.531  loss_ce_2: 0.2486  loss_mask_2: 0.3048  loss_dice_2: 2.498  loss_ce_3: 0.2355  loss_mask_3: 0.3057  loss_dice_3: 2.476  loss_ce_4: 0.2312  loss_mask_4: 0.3041  loss_dice_4: 2.482  loss_ce_5: 0.2134  loss_mask_5: 0.3023  loss_dice_5: 2.477  loss_ce_6: 0.2121  loss_mask_6: 0.3018  loss_dice_6: 2.477  loss_ce_7: 0.2056  loss_mask_7: 0.3019  loss_dice_7: 2.478  loss_ce_8: 0.2038  loss_mask_8: 0.3016  loss_dice_8: 2.474  time: 1.4603  data_time: 0.0524  lr: 9.2918e-08  max_mem: 21589M
[01/18 13:00:16] d2.utils.events INFO:  eta: 0:04:29  iter: 39799  total_loss: 30.92  loss_ce: 0.2118  loss_mask: 0.3139  loss_dice: 2.502  loss_ce_0: 0.571  loss_mask_0: 0.3087  loss_dice_0: 2.664  loss_ce_1: 0.2733  loss_mask_1: 0.3175  loss_dice_1: 2.562  loss_ce_2: 0.2729  loss_mask_2: 0.3152  loss_dice_2: 2.531  loss_ce_3: 0.2391  loss_mask_3: 0.3151  loss_dice_3: 2.516  loss_ce_4: 0.2377  loss_mask_4: 0.3137  loss_dice_4: 2.506  loss_ce_5: 0.2232  loss_mask_5: 0.3127  loss_dice_5: 2.506  loss_ce_6: 0.2189  loss_mask_6: 0.3132  loss_dice_6: 2.503  loss_ce_7: 0.2228  loss_mask_7: 0.3126  loss_dice_7: 2.501  loss_ce_8: 0.2154  loss_mask_8: 0.3126  loss_dice_8: 2.494  time: 1.4603  data_time: 0.0539  lr: 8.5314e-08  max_mem: 21589M
[01/18 13:00:43] d2.utils.events INFO:  eta: 0:04:02  iter: 39819  total_loss: 31.28  loss_ce: 0.2038  loss_mask: 0.3106  loss_dice: 2.53  loss_ce_0: 0.5401  loss_mask_0: 0.3112  loss_dice_0: 2.671  loss_ce_1: 0.2621  loss_mask_1: 0.3177  loss_dice_1: 2.587  loss_ce_2: 0.2472  loss_mask_2: 0.3133  loss_dice_2: 2.558  loss_ce_3: 0.2182  loss_mask_3: 0.3119  loss_dice_3: 2.535  loss_ce_4: 0.2176  loss_mask_4: 0.3105  loss_dice_4: 2.54  loss_ce_5: 0.209  loss_mask_5: 0.3093  loss_dice_5: 2.543  loss_ce_6: 0.1959  loss_mask_6: 0.3102  loss_dice_6: 2.551  loss_ce_7: 0.2007  loss_mask_7: 0.3103  loss_dice_7: 2.546  loss_ce_8: 0.1865  loss_mask_8: 0.3108  loss_dice_8: 2.541  time: 1.4602  data_time: 0.0534  lr: 7.7635e-08  max_mem: 21589M
[01/18 13:01:10] d2.utils.events INFO:  eta: 0:03:35  iter: 39839  total_loss: 30.66  loss_ce: 0.1977  loss_mask: 0.3161  loss_dice: 2.466  loss_ce_0: 0.5371  loss_mask_0: 0.314  loss_dice_0: 2.621  loss_ce_1: 0.2414  loss_mask_1: 0.3198  loss_dice_1: 2.515  loss_ce_2: 0.2291  loss_mask_2: 0.3183  loss_dice_2: 2.498  loss_ce_3: 0.2197  loss_mask_3: 0.3173  loss_dice_3: 2.482  loss_ce_4: 0.2043  loss_mask_4: 0.3161  loss_dice_4: 2.474  loss_ce_5: 0.2007  loss_mask_5: 0.317  loss_dice_5: 2.471  loss_ce_6: 0.1996  loss_mask_6: 0.3158  loss_dice_6: 2.469  loss_ce_7: 0.2007  loss_mask_7: 0.3168  loss_dice_7: 2.48  loss_ce_8: 0.1924  loss_mask_8: 0.3165  loss_dice_8: 2.466  time: 1.4602  data_time: 0.0529  lr: 6.987e-08  max_mem: 21589M
[01/18 13:01:37] d2.utils.events INFO:  eta: 0:03:08  iter: 39859  total_loss: 29.76  loss_ce: 0.2122  loss_mask: 0.315  loss_dice: 2.389  loss_ce_0: 0.5667  loss_mask_0: 0.3154  loss_dice_0: 2.546  loss_ce_1: 0.2669  loss_mask_1: 0.3187  loss_dice_1: 2.438  loss_ce_2: 0.2554  loss_mask_2: 0.3172  loss_dice_2: 2.423  loss_ce_3: 0.2199  loss_mask_3: 0.3169  loss_dice_3: 2.408  loss_ce_4: 0.2188  loss_mask_4: 0.3163  loss_dice_4: 2.416  loss_ce_5: 0.2014  loss_mask_5: 0.3161  loss_dice_5: 2.396  loss_ce_6: 0.1977  loss_mask_6: 0.3153  loss_dice_6: 2.399  loss_ce_7: 0.2118  loss_mask_7: 0.3159  loss_dice_7: 2.396  loss_ce_8: 0.2001  loss_mask_8: 0.3162  loss_dice_8: 2.391  time: 1.4601  data_time: 0.0546  lr: 6.2007e-08  max_mem: 21589M
[01/18 13:02:04] d2.utils.events INFO:  eta: 0:02:41  iter: 39879  total_loss: 30.64  loss_ce: 0.2023  loss_mask: 0.3211  loss_dice: 2.461  loss_ce_0: 0.5521  loss_mask_0: 0.3217  loss_dice_0: 2.627  loss_ce_1: 0.2537  loss_mask_1: 0.3269  loss_dice_1: 2.512  loss_ce_2: 0.2591  loss_mask_2: 0.3232  loss_dice_2: 2.48  loss_ce_3: 0.2368  loss_mask_3: 0.3226  loss_dice_3: 2.472  loss_ce_4: 0.2168  loss_mask_4: 0.3209  loss_dice_4: 2.464  loss_ce_5: 0.2208  loss_mask_5: 0.3214  loss_dice_5: 2.463  loss_ce_6: 0.2141  loss_mask_6: 0.3203  loss_dice_6: 2.453  loss_ce_7: 0.2032  loss_mask_7: 0.3217  loss_dice_7: 2.464  loss_ce_8: 0.2089  loss_mask_8: 0.3219  loss_dice_8: 2.462  time: 1.4600  data_time: 0.0536  lr: 5.4032e-08  max_mem: 21589M
[01/18 13:02:30] d2.utils.events INFO:  eta: 0:02:14  iter: 39899  total_loss: 30.91  loss_ce: 0.2189  loss_mask: 0.3112  loss_dice: 2.507  loss_ce_0: 0.5732  loss_mask_0: 0.3075  loss_dice_0: 2.671  loss_ce_1: 0.2636  loss_mask_1: 0.315  loss_dice_1: 2.561  loss_ce_2: 0.2489  loss_mask_2: 0.3136  loss_dice_2: 2.535  loss_ce_3: 0.2388  loss_mask_3: 0.3126  loss_dice_3: 2.523  loss_ce_4: 0.2352  loss_mask_4: 0.3117  loss_dice_4: 2.52  loss_ce_5: 0.216  loss_mask_5: 0.3106  loss_dice_5: 2.509  loss_ce_6: 0.2128  loss_mask_6: 0.3111  loss_dice_6: 2.511  loss_ce_7: 0.2064  loss_mask_7: 0.3107  loss_dice_7: 2.52  loss_ce_8: 0.2066  loss_mask_8: 0.3115  loss_dice_8: 2.507  time: 1.4600  data_time: 0.0552  lr: 4.5924e-08  max_mem: 21589M
[01/18 13:02:57] d2.utils.events INFO:  eta: 0:01:47  iter: 39919  total_loss: 31.32  loss_ce: 0.194  loss_mask: 0.3125  loss_dice: 2.546  loss_ce_0: 0.5522  loss_mask_0: 0.3123  loss_dice_0: 2.701  loss_ce_1: 0.2454  loss_mask_1: 0.3185  loss_dice_1: 2.606  loss_ce_2: 0.2441  loss_mask_2: 0.316  loss_dice_2: 2.564  loss_ce_3: 0.2099  loss_mask_3: 0.3129  loss_dice_3: 2.55  loss_ce_4: 0.2103  loss_mask_4: 0.312  loss_dice_4: 2.552  loss_ce_5: 0.2028  loss_mask_5: 0.3133  loss_dice_5: 2.551  loss_ce_6: 0.1937  loss_mask_6: 0.3115  loss_dice_6: 2.552  loss_ce_7: 0.1954  loss_mask_7: 0.3118  loss_dice_7: 2.553  loss_ce_8: 0.2001  loss_mask_8: 0.3126  loss_dice_8: 2.549  time: 1.4599  data_time: 0.0514  lr: 3.7652e-08  max_mem: 21589M
[01/18 13:03:24] d2.utils.events INFO:  eta: 0:01:20  iter: 39939  total_loss: 30.88  loss_ce: 0.2164  loss_mask: 0.3194  loss_dice: 2.504  loss_ce_0: 0.5426  loss_mask_0: 0.3141  loss_dice_0: 2.669  loss_ce_1: 0.2739  loss_mask_1: 0.3231  loss_dice_1: 2.565  loss_ce_2: 0.2566  loss_mask_2: 0.3209  loss_dice_2: 2.535  loss_ce_3: 0.2446  loss_mask_3: 0.3195  loss_dice_3: 2.514  loss_ce_4: 0.2218  loss_mask_4: 0.3185  loss_dice_4: 2.512  loss_ce_5: 0.2284  loss_mask_5: 0.3179  loss_dice_5: 2.517  loss_ce_6: 0.2228  loss_mask_6: 0.3171  loss_dice_6: 2.5  loss_ce_7: 0.1991  loss_mask_7: 0.3177  loss_dice_7: 2.512  loss_ce_8: 0.2063  loss_mask_8: 0.3183  loss_dice_8: 2.503  time: 1.4599  data_time: 0.0532  lr: 2.917e-08  max_mem: 21589M
[01/18 13:03:51] d2.utils.events INFO:  eta: 0:00:53  iter: 39959  total_loss: 29.91  loss_ce: 0.1839  loss_mask: 0.3196  loss_dice: 2.41  loss_ce_0: 0.5344  loss_mask_0: 0.3158  loss_dice_0: 2.561  loss_ce_1: 0.2574  loss_mask_1: 0.3247  loss_dice_1: 2.458  loss_ce_2: 0.2197  loss_mask_2: 0.3215  loss_dice_2: 2.442  loss_ce_3: 0.2247  loss_mask_3: 0.3205  loss_dice_3: 2.425  loss_ce_4: 0.2105  loss_mask_4: 0.3195  loss_dice_4: 2.419  loss_ce_5: 0.2216  loss_mask_5: 0.3181  loss_dice_5: 2.419  loss_ce_6: 0.1957  loss_mask_6: 0.3201  loss_dice_6: 2.424  loss_ce_7: 0.1931  loss_mask_7: 0.3193  loss_dice_7: 2.423  loss_ce_8: 0.1964  loss_mask_8: 0.319  loss_dice_8: 2.42  time: 1.4598  data_time: 0.0506  lr: 2.0401e-08  max_mem: 21589M
[01/18 13:04:18] d2.utils.events INFO:  eta: 0:00:26  iter: 39979  total_loss: 31.43  loss_ce: 0.2112  loss_mask: 0.314  loss_dice: 2.57  loss_ce_0: 0.5332  loss_mask_0: 0.3114  loss_dice_0: 2.702  loss_ce_1: 0.2599  loss_mask_1: 0.3189  loss_dice_1: 2.617  loss_ce_2: 0.2578  loss_mask_2: 0.3163  loss_dice_2: 2.592  loss_ce_3: 0.2373  loss_mask_3: 0.3149  loss_dice_3: 2.578  loss_ce_4: 0.2277  loss_mask_4: 0.3164  loss_dice_4: 2.578  loss_ce_5: 0.2165  loss_mask_5: 0.313  loss_dice_5: 2.58  loss_ce_6: 0.2056  loss_mask_6: 0.3143  loss_dice_6: 2.567  loss_ce_7: 0.2119  loss_mask_7: 0.3137  loss_dice_7: 2.568  loss_ce_8: 0.2098  loss_mask_8: 0.3146  loss_dice_8: 2.574  time: 1.4597  data_time: 0.0545  lr: 1.1172e-08  max_mem: 21589M
[01/18 13:04:44] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_0039999.pth
[01/18 13:04:45] fvcore.common.checkpoint INFO: Saving checkpoint to ./work_dirs/sceneflow_vanilla_disp192/model_final.pth
[01/18 13:04:47] d2.utils.events INFO:  eta: 0:00:00  iter: 39999  total_loss: 30.01  loss_ce: 0.2009  loss_mask: 0.318  loss_dice: 2.384  loss_ce_0: 0.5282  loss_mask_0: 0.3136  loss_dice_0: 2.53  loss_ce_1: 0.2405  loss_mask_1: 0.3221  loss_dice_1: 2.429  loss_ce_2: 0.2463  loss_mask_2: 0.3213  loss_dice_2: 2.409  loss_ce_3: 0.2366  loss_mask_3: 0.3196  loss_dice_3: 2.389  loss_ce_4: 0.209  loss_mask_4: 0.3188  loss_dice_4: 2.394  loss_ce_5: 0.2062  loss_mask_5: 0.3179  loss_dice_5: 2.389  loss_ce_6: 0.2145  loss_mask_6: 0.3179  loss_dice_6: 2.383  loss_ce_7: 0.2078  loss_mask_7: 0.3183  loss_dice_7: 2.382  loss_ce_8: 0.1939  loss_mask_8: 0.3189  loss_dice_8: 2.379  time: 1.4597  data_time: 0.0539  lr: 7.2135e-10  max_mem: 21589M
[01/18 13:04:47] d2.engine.hooks INFO: Overall training speed: 39998 iterations in 16:13:04 (1.4597 s / it)
[01/18 13:04:47] d2.engine.hooks INFO: Total training time: 18:33:15 (2:20:10 on hooks)
[01/18 13:04:47] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: None
[01/18 13:04:47] d2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[01/18 13:04:47] d2.data.common INFO: Serialized dataset takes 1.50 MiB
[01/18 13:04:48] d2.evaluation.evaluator INFO: Start inference on 1093 batches
[01/18 13:04:59] d2.evaluation.evaluator INFO: Inference done 11/1093. Dataloading: 0.0069 s/iter. Inference: 0.1236 s/iter. Eval: 0.1690 s/iter. Total: 0.2995 s/iter. ETA=0:05:24
[01/18 13:05:04] d2.evaluation.evaluator INFO: Inference done 27/1093. Dataloading: 0.0109 s/iter. Inference: 0.1360 s/iter. Eval: 0.1653 s/iter. Total: 0.3123 s/iter. ETA=0:05:32
[01/18 13:05:09] d2.evaluation.evaluator INFO: Inference done 43/1093. Dataloading: 0.0118 s/iter. Inference: 0.1369 s/iter. Eval: 0.1653 s/iter. Total: 0.3141 s/iter. ETA=0:05:29
[01/18 13:05:14] d2.evaluation.evaluator INFO: Inference done 58/1093. Dataloading: 0.0124 s/iter. Inference: 0.1343 s/iter. Eval: 0.1777 s/iter. Total: 0.3244 s/iter. ETA=0:05:35
[01/18 13:05:19] d2.evaluation.evaluator INFO: Inference done 74/1093. Dataloading: 0.0122 s/iter. Inference: 0.1333 s/iter. Eval: 0.1778 s/iter. Total: 0.3234 s/iter. ETA=0:05:29
[01/18 13:05:25] d2.evaluation.evaluator INFO: Inference done 89/1093. Dataloading: 0.0123 s/iter. Inference: 0.1348 s/iter. Eval: 0.1793 s/iter. Total: 0.3264 s/iter. ETA=0:05:27
[01/18 13:05:30] d2.evaluation.evaluator INFO: Inference done 105/1093. Dataloading: 0.0123 s/iter. Inference: 0.1357 s/iter. Eval: 0.1765 s/iter. Total: 0.3246 s/iter. ETA=0:05:20
[01/18 13:05:35] d2.evaluation.evaluator INFO: Inference done 121/1093. Dataloading: 0.0122 s/iter. Inference: 0.1375 s/iter. Eval: 0.1739 s/iter. Total: 0.3237 s/iter. ETA=0:05:14
[01/18 13:05:40] d2.evaluation.evaluator INFO: Inference done 137/1093. Dataloading: 0.0120 s/iter. Inference: 0.1381 s/iter. Eval: 0.1723 s/iter. Total: 0.3225 s/iter. ETA=0:05:08
[01/18 13:05:45] d2.evaluation.evaluator INFO: Inference done 156/1093. Dataloading: 0.0119 s/iter. Inference: 0.1361 s/iter. Eval: 0.1673 s/iter. Total: 0.3153 s/iter. ETA=0:04:55
[01/18 13:05:50] d2.evaluation.evaluator INFO: Inference done 172/1093. Dataloading: 0.0120 s/iter. Inference: 0.1352 s/iter. Eval: 0.1687 s/iter. Total: 0.3161 s/iter. ETA=0:04:51
[01/18 13:05:55] d2.evaluation.evaluator INFO: Inference done 189/1093. Dataloading: 0.0118 s/iter. Inference: 0.1351 s/iter. Eval: 0.1685 s/iter. Total: 0.3154 s/iter. ETA=0:04:45
[01/18 13:06:01] d2.evaluation.evaluator INFO: Inference done 206/1093. Dataloading: 0.0117 s/iter. Inference: 0.1351 s/iter. Eval: 0.1681 s/iter. Total: 0.3151 s/iter. ETA=0:04:39
[01/18 13:06:06] d2.evaluation.evaluator INFO: Inference done 224/1093. Dataloading: 0.0117 s/iter. Inference: 0.1345 s/iter. Eval: 0.1669 s/iter. Total: 0.3132 s/iter. ETA=0:04:32
[01/18 13:06:11] d2.evaluation.evaluator INFO: Inference done 240/1093. Dataloading: 0.0116 s/iter. Inference: 0.1351 s/iter. Eval: 0.1676 s/iter. Total: 0.3144 s/iter. ETA=0:04:28
[01/18 13:06:16] d2.evaluation.evaluator INFO: Inference done 256/1093. Dataloading: 0.0117 s/iter. Inference: 0.1355 s/iter. Eval: 0.1683 s/iter. Total: 0.3156 s/iter. ETA=0:04:24
[01/18 13:06:21] d2.evaluation.evaluator INFO: Inference done 272/1093. Dataloading: 0.0117 s/iter. Inference: 0.1356 s/iter. Eval: 0.1682 s/iter. Total: 0.3155 s/iter. ETA=0:04:19
[01/18 13:06:27] d2.evaluation.evaluator INFO: Inference done 290/1093. Dataloading: 0.0116 s/iter. Inference: 0.1351 s/iter. Eval: 0.1670 s/iter. Total: 0.3138 s/iter. ETA=0:04:11
[01/18 13:06:32] d2.evaluation.evaluator INFO: Inference done 306/1093. Dataloading: 0.0116 s/iter. Inference: 0.1349 s/iter. Eval: 0.1682 s/iter. Total: 0.3147 s/iter. ETA=0:04:07
[01/18 13:06:37] d2.evaluation.evaluator INFO: Inference done 321/1093. Dataloading: 0.0115 s/iter. Inference: 0.1357 s/iter. Eval: 0.1689 s/iter. Total: 0.3162 s/iter. ETA=0:04:04
[01/18 13:06:42] d2.evaluation.evaluator INFO: Inference done 339/1093. Dataloading: 0.0114 s/iter. Inference: 0.1359 s/iter. Eval: 0.1671 s/iter. Total: 0.3145 s/iter. ETA=0:03:57
[01/18 13:06:47] d2.evaluation.evaluator INFO: Inference done 357/1093. Dataloading: 0.0112 s/iter. Inference: 0.1365 s/iter. Eval: 0.1656 s/iter. Total: 0.3134 s/iter. ETA=0:03:50
[01/18 13:06:53] d2.evaluation.evaluator INFO: Inference done 372/1093. Dataloading: 0.0113 s/iter. Inference: 0.1372 s/iter. Eval: 0.1658 s/iter. Total: 0.3144 s/iter. ETA=0:03:46
[01/18 13:06:58] d2.evaluation.evaluator INFO: Inference done 389/1093. Dataloading: 0.0112 s/iter. Inference: 0.1373 s/iter. Eval: 0.1655 s/iter. Total: 0.3140 s/iter. ETA=0:03:41
[01/18 13:07:03] d2.evaluation.evaluator INFO: Inference done 405/1093. Dataloading: 0.0111 s/iter. Inference: 0.1374 s/iter. Eval: 0.1654 s/iter. Total: 0.3141 s/iter. ETA=0:03:36
[01/18 13:07:08] d2.evaluation.evaluator INFO: Inference done 422/1093. Dataloading: 0.0111 s/iter. Inference: 0.1374 s/iter. Eval: 0.1654 s/iter. Total: 0.3140 s/iter. ETA=0:03:30
[01/18 13:07:13] d2.evaluation.evaluator INFO: Inference done 440/1093. Dataloading: 0.0111 s/iter. Inference: 0.1371 s/iter. Eval: 0.1648 s/iter. Total: 0.3131 s/iter. ETA=0:03:24
[01/18 13:07:19] d2.evaluation.evaluator INFO: Inference done 458/1093. Dataloading: 0.0110 s/iter. Inference: 0.1370 s/iter. Eval: 0.1642 s/iter. Total: 0.3122 s/iter. ETA=0:03:18
[01/18 13:07:24] d2.evaluation.evaluator INFO: Inference done 477/1093. Dataloading: 0.0113 s/iter. Inference: 0.1366 s/iter. Eval: 0.1627 s/iter. Total: 0.3107 s/iter. ETA=0:03:11
[01/18 13:07:29] d2.evaluation.evaluator INFO: Inference done 494/1093. Dataloading: 0.0112 s/iter. Inference: 0.1370 s/iter. Eval: 0.1620 s/iter. Total: 0.3102 s/iter. ETA=0:03:05
[01/18 13:07:34] d2.evaluation.evaluator INFO: Inference done 512/1093. Dataloading: 0.0112 s/iter. Inference: 0.1374 s/iter. Eval: 0.1607 s/iter. Total: 0.3093 s/iter. ETA=0:02:59
[01/18 13:07:39] d2.evaluation.evaluator INFO: Inference done 528/1093. Dataloading: 0.0111 s/iter. Inference: 0.1374 s/iter. Eval: 0.1612 s/iter. Total: 0.3098 s/iter. ETA=0:02:55
[01/18 13:07:44] d2.evaluation.evaluator INFO: Inference done 546/1093. Dataloading: 0.0111 s/iter. Inference: 0.1371 s/iter. Eval: 0.1608 s/iter. Total: 0.3090 s/iter. ETA=0:02:49
[01/18 13:07:49] d2.evaluation.evaluator INFO: Inference done 563/1093. Dataloading: 0.0110 s/iter. Inference: 0.1370 s/iter. Eval: 0.1606 s/iter. Total: 0.3087 s/iter. ETA=0:02:43
[01/18 13:07:55] d2.evaluation.evaluator INFO: Inference done 582/1093. Dataloading: 0.0110 s/iter. Inference: 0.1373 s/iter. Eval: 0.1592 s/iter. Total: 0.3076 s/iter. ETA=0:02:37
[01/18 13:08:00] d2.evaluation.evaluator INFO: Inference done 599/1093. Dataloading: 0.0110 s/iter. Inference: 0.1375 s/iter. Eval: 0.1590 s/iter. Total: 0.3075 s/iter. ETA=0:02:31
[01/18 13:08:05] d2.evaluation.evaluator INFO: Inference done 615/1093. Dataloading: 0.0110 s/iter. Inference: 0.1374 s/iter. Eval: 0.1593 s/iter. Total: 0.3077 s/iter. ETA=0:02:27
[01/18 13:08:10] d2.evaluation.evaluator INFO: Inference done 633/1093. Dataloading: 0.0109 s/iter. Inference: 0.1369 s/iter. Eval: 0.1589 s/iter. Total: 0.3069 s/iter. ETA=0:02:21
[01/18 13:08:15] d2.evaluation.evaluator INFO: Inference done 651/1093. Dataloading: 0.0109 s/iter. Inference: 0.1369 s/iter. Eval: 0.1585 s/iter. Total: 0.3064 s/iter. ETA=0:02:15
[01/18 13:08:20] d2.evaluation.evaluator INFO: Inference done 668/1093. Dataloading: 0.0108 s/iter. Inference: 0.1367 s/iter. Eval: 0.1584 s/iter. Total: 0.3061 s/iter. ETA=0:02:10
[01/18 13:08:25] d2.evaluation.evaluator INFO: Inference done 688/1093. Dataloading: 0.0108 s/iter. Inference: 0.1365 s/iter. Eval: 0.1573 s/iter. Total: 0.3047 s/iter. ETA=0:02:03
[01/18 13:08:30] d2.evaluation.evaluator INFO: Inference done 704/1093. Dataloading: 0.0107 s/iter. Inference: 0.1368 s/iter. Eval: 0.1573 s/iter. Total: 0.3049 s/iter. ETA=0:01:58
[01/18 13:08:36] d2.evaluation.evaluator INFO: Inference done 719/1093. Dataloading: 0.0108 s/iter. Inference: 0.1369 s/iter. Eval: 0.1581 s/iter. Total: 0.3058 s/iter. ETA=0:01:54
[01/18 13:08:41] d2.evaluation.evaluator INFO: Inference done 738/1093. Dataloading: 0.0107 s/iter. Inference: 0.1370 s/iter. Eval: 0.1571 s/iter. Total: 0.3049 s/iter. ETA=0:01:48
[01/18 13:08:46] d2.evaluation.evaluator INFO: Inference done 756/1093. Dataloading: 0.0106 s/iter. Inference: 0.1370 s/iter. Eval: 0.1565 s/iter. Total: 0.3042 s/iter. ETA=0:01:42
[01/18 13:08:51] d2.evaluation.evaluator INFO: Inference done 773/1093. Dataloading: 0.0106 s/iter. Inference: 0.1370 s/iter. Eval: 0.1566 s/iter. Total: 0.3042 s/iter. ETA=0:01:37
[01/18 13:08:56] d2.evaluation.evaluator INFO: Inference done 790/1093. Dataloading: 0.0105 s/iter. Inference: 0.1371 s/iter. Eval: 0.1564 s/iter. Total: 0.3041 s/iter. ETA=0:01:32
[01/18 13:09:01] d2.evaluation.evaluator INFO: Inference done 808/1093. Dataloading: 0.0105 s/iter. Inference: 0.1370 s/iter. Eval: 0.1560 s/iter. Total: 0.3036 s/iter. ETA=0:01:26
[01/18 13:09:06] d2.evaluation.evaluator INFO: Inference done 827/1093. Dataloading: 0.0104 s/iter. Inference: 0.1372 s/iter. Eval: 0.1552 s/iter. Total: 0.3029 s/iter. ETA=0:01:20
[01/18 13:09:11] d2.evaluation.evaluator INFO: Inference done 845/1093. Dataloading: 0.0104 s/iter. Inference: 0.1373 s/iter. Eval: 0.1549 s/iter. Total: 0.3026 s/iter. ETA=0:01:15
[01/18 13:09:16] d2.evaluation.evaluator INFO: Inference done 860/1093. Dataloading: 0.0104 s/iter. Inference: 0.1373 s/iter. Eval: 0.1555 s/iter. Total: 0.3032 s/iter. ETA=0:01:10
[01/18 13:09:22] d2.evaluation.evaluator INFO: Inference done 878/1093. Dataloading: 0.0104 s/iter. Inference: 0.1372 s/iter. Eval: 0.1552 s/iter. Total: 0.3028 s/iter. ETA=0:01:05
[01/18 13:09:27] d2.evaluation.evaluator INFO: Inference done 894/1093. Dataloading: 0.0104 s/iter. Inference: 0.1370 s/iter. Eval: 0.1558 s/iter. Total: 0.3032 s/iter. ETA=0:01:00
[01/18 13:09:32] d2.evaluation.evaluator INFO: Inference done 912/1093. Dataloading: 0.0103 s/iter. Inference: 0.1371 s/iter. Eval: 0.1554 s/iter. Total: 0.3029 s/iter. ETA=0:00:54
[01/18 13:09:37] d2.evaluation.evaluator INFO: Inference done 930/1093. Dataloading: 0.0103 s/iter. Inference: 0.1370 s/iter. Eval: 0.1554 s/iter. Total: 0.3028 s/iter. ETA=0:00:49
[01/18 13:09:43] d2.evaluation.evaluator INFO: Inference done 946/1093. Dataloading: 0.0103 s/iter. Inference: 0.1371 s/iter. Eval: 0.1557 s/iter. Total: 0.3032 s/iter. ETA=0:00:44
[01/18 13:09:48] d2.evaluation.evaluator INFO: Inference done 963/1093. Dataloading: 0.0103 s/iter. Inference: 0.1371 s/iter. Eval: 0.1558 s/iter. Total: 0.3032 s/iter. ETA=0:00:39
[01/18 13:09:53] d2.evaluation.evaluator INFO: Inference done 981/1093. Dataloading: 0.0102 s/iter. Inference: 0.1372 s/iter. Eval: 0.1555 s/iter. Total: 0.3030 s/iter. ETA=0:00:33
[01/18 13:09:58] d2.evaluation.evaluator INFO: Inference done 999/1093. Dataloading: 0.0102 s/iter. Inference: 0.1373 s/iter. Eval: 0.1551 s/iter. Total: 0.3027 s/iter. ETA=0:00:28
[01/18 13:10:03] d2.evaluation.evaluator INFO: Inference done 1018/1093. Dataloading: 0.0101 s/iter. Inference: 0.1371 s/iter. Eval: 0.1548 s/iter. Total: 0.3021 s/iter. ETA=0:00:22
[01/18 13:10:08] d2.evaluation.evaluator INFO: Inference done 1036/1093. Dataloading: 0.0101 s/iter. Inference: 0.1370 s/iter. Eval: 0.1546 s/iter. Total: 0.3018 s/iter. ETA=0:00:17
[01/18 13:10:13] d2.evaluation.evaluator INFO: Inference done 1053/1093. Dataloading: 0.0101 s/iter. Inference: 0.1371 s/iter. Eval: 0.1545 s/iter. Total: 0.3017 s/iter. ETA=0:00:12
[01/18 13:10:18] d2.evaluation.evaluator INFO: Inference done 1072/1093. Dataloading: 0.0100 s/iter. Inference: 0.1372 s/iter. Eval: 0.1538 s/iter. Total: 0.3011 s/iter. ETA=0:00:06
[01/18 13:10:24] d2.evaluation.evaluator INFO: Inference done 1091/1093. Dataloading: 0.0100 s/iter. Inference: 0.1372 s/iter. Eval: 0.1534 s/iter. Total: 0.3006 s/iter. ETA=0:00:00
[01/18 13:10:25] d2.evaluation.evaluator INFO: Total inference time: 0:05:27.361582 (0.300884 s / iter per device, on 4 devices)
[01/18 13:10:25] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:29 (0.137152 s / iter per device, on 4 devices)
[01/18 13:10:44] d2.evaluation.sem_seg_evaluation INFO: OrderedDict([('sem_seg', {'epe': 2.2839656571653446, 'mIoU': 21.169589546239624, 'fwIoU': 44.362776739787506, 'IoU-0': nan, 'IoU-1': 95.50403118024042, 'IoU-2': 47.78259022691741, 'IoU-3': 59.71975202345193, 'IoU-4': 53.72673879443586, 'IoU-5': 47.290449113596175, 'IoU-6': 42.18981544686185, 'IoU-7': 34.31747978296331, 'IoU-8': 23.935236324233756, 'IoU-9': 35.582234051838256, 'IoU-10': 40.73079095272497, 'IoU-11': 50.496368531399746, 'IoU-12': 52.11972031216119, 'IoU-13': 51.84186439957972, 'IoU-14': 52.011048749866475, 'IoU-15': 52.37905533536637, 'IoU-16': 52.981335626349, 'IoU-17': 48.36726862233364, 'IoU-18': 49.25861443054812, 'IoU-19': 49.65030897802553, 'IoU-20': 49.76266108766237, 'IoU-21': 49.11977643735416, 'IoU-22': 50.21673371078555, 'IoU-23': 48.338179839885626, 'IoU-24': 47.45676075916571, 'IoU-25': 47.745878066137706, 'IoU-26': 46.770930915161095, 'IoU-27': 48.13406304476493, 'IoU-28': 46.639778013606, 'IoU-29': 47.88179327652652, 'IoU-30': 46.39076081233762, 'IoU-31': 47.330667058935774, 'IoU-32': 46.41421599391899, 'IoU-33': 44.923449034651405, 'IoU-34': 44.198891721274656, 'IoU-35': 45.772635164713755, 'IoU-36': 45.72429216127864, 'IoU-37': 44.69404996962333, 'IoU-38': 44.13906476481689, 'IoU-39': 43.65357475667896, 'IoU-40': 44.17511593303789, 'IoU-41': 41.92959392659358, 'IoU-42': 41.28672385181032, 'IoU-43': 40.87326424243742, 'IoU-44': 40.62869951819616, 'IoU-45': 39.32617023663978, 'IoU-46': 38.63942807870696, 'IoU-47': 38.11952285126365, 'IoU-48': 38.42289377581116, 'IoU-49': 37.59762425747547, 'IoU-50': 37.55651975629583, 'IoU-51': 36.00080691361704, 'IoU-52': 35.39260657374437, 'IoU-53': 34.860303595240424, 'IoU-54': 34.84573808508407, 'IoU-55': 33.635879084127694, 'IoU-56': 32.49546797901113, 'IoU-57': 31.54388892915656, 'IoU-58': 29.741769479071667, 'IoU-59': 28.582251337887342, 'IoU-60': 27.460115406438952, 'IoU-61': 27.194388275294244, 'IoU-62': 27.27212733446051, 'IoU-63': 26.456019732882847, 'IoU-64': 25.9421140002601, 'IoU-65': 25.07263088417076, 'IoU-66': 23.40417137978542, 'IoU-67': 23.401800431173324, 'IoU-68': 22.52642363435826, 'IoU-69': 22.489578997548605, 'IoU-70': 22.497304789517543, 'IoU-71': 20.43438651308811, 'IoU-72': 20.898260711456274, 'IoU-73': 20.80437786442301, 'IoU-74': 21.119996926259375, 'IoU-75': 20.53463015111807, 'IoU-76': 20.502257602422517, 'IoU-77': 20.21880471438267, 'IoU-78': 19.755837386934402, 'IoU-79': 19.461413492371165, 'IoU-80': 18.924330142499297, 'IoU-81': 19.465019689822203, 'IoU-82': 19.02901944055643, 'IoU-83': 18.998576057139562, 'IoU-84': 17.80946767332024, 'IoU-85': 19.092118707190973, 'IoU-86': 17.86040531316942, 'IoU-87': 17.3183464412407, 'IoU-88': 17.63812376596166, 'IoU-89': 17.629437750117148, 'IoU-90': 17.38057401136554, 'IoU-91': 16.959930624576444, 'IoU-92': 16.87539669050435, 'IoU-93': 17.224055755310587, 'IoU-94': 17.11508286787616, 'IoU-95': 17.534484969360772, 'IoU-96': 17.033135589881983, 'IoU-97': 16.92048586301929, 'IoU-98': 16.914299518896918, 'IoU-99': 15.80076274073936, 'IoU-100': 16.088451641930344, 'IoU-101': 15.20634974604416, 'IoU-102': 15.059405156823452, 'IoU-103': 14.708756180237922, 'IoU-104': 13.929148691147464, 'IoU-105': 13.810799124435752, 'IoU-106': 13.84166755785747, 'IoU-107': 14.312506694666139, 'IoU-108': 13.468006882950723, 'IoU-109': 13.66503274690343, 'IoU-110': 13.29794587639967, 'IoU-111': 12.736260919569132, 'IoU-112': 12.810558160311746, 'IoU-113': 12.863790948668704, 'IoU-114': 12.751449977732431, 'IoU-115': 12.237721836179377, 'IoU-116': 11.8975240514013, 'IoU-117': 11.889870503833663, 'IoU-118': 11.293487048045366, 'IoU-119': 12.96634542400773, 'IoU-120': 10.835863139394217, 'IoU-121': 11.169112264060859, 'IoU-122': 10.697807676441457, 'IoU-123': 10.559038261851212, 'IoU-124': 10.682819872673962, 'IoU-125': 9.945550309751626, 'IoU-126': 9.59886624188563, 'IoU-127': 9.598452312929897, 'IoU-128': 9.249053444479546, 'IoU-129': 8.522180290658277, 'IoU-130': 7.9527932455621455, 'IoU-131': 8.013485835356196, 'IoU-132': 7.7035041418256585, 'IoU-133': 7.831179629322256, 'IoU-134': 7.131413799818303, 'IoU-135': 7.30018969164556, 'IoU-136': 7.0649163849187, 'IoU-137': 6.504853775750498, 'IoU-138': 8.110619117539876, 'IoU-139': 6.599018760130929, 'IoU-140': 6.432319989953868, 'IoU-141': 6.462084113320711, 'IoU-142': 6.879977303937328, 'IoU-143': 5.707984463980705, 'IoU-144': 6.428426022665859, 'IoU-145': 6.174483037934189, 'IoU-146': 4.979395079567679, 'IoU-147': 5.825874737350404, 'IoU-148': 6.252260250069323, 'IoU-149': 5.220780583422558, 'IoU-150': 4.959216450330429, 'IoU-151': 4.319279591058369, 'IoU-152': 3.7533071480315408, 'IoU-153': 4.766669052969521, 'IoU-154': 3.3074260708645173, 'IoU-155': 3.6955246738757914, 'IoU-156': 3.7090033079999163, 'IoU-157': 3.419851904063714, 'IoU-158': 3.570693236833019, 'IoU-159': 2.566496563010163, 'IoU-160': 3.97567019845121, 'IoU-161': 4.260114487984292, 'IoU-162': 3.1852715942677197, 'IoU-163': 3.0816296140190764, 'IoU-164': 2.940472604108891, 'IoU-165': 3.000632214300767, 'IoU-166': 2.4313353417582846, 'IoU-167': 3.1570231893573837, 'IoU-168': 2.473377062095732, 'IoU-169': 2.9301777944532774, 'IoU-170': 2.2739690973816318, 'IoU-171': 1.882969402415841, 'IoU-172': 2.098104253837389, 'IoU-173': 2.522803175099817, 'IoU-174': 2.6320164132779214, 'IoU-175': 1.4657858340052625, 'IoU-176': 1.550639966760238, 'IoU-177': 2.0533674700028564, 'IoU-178': 1.5299249990342956, 'IoU-179': 2.482625273633845, 'IoU-180': 2.6845279448481385, 'IoU-181': 2.170767004341534, 'IoU-182': 1.0947310545254612, 'IoU-183': 2.102387052682882, 'IoU-184': 2.1563965557973894, 'IoU-185': 1.7691248996444549, 'IoU-186': 2.3091876781089513, 'IoU-187': 2.9026258953266253, 'IoU-188': 1.662224057666316, 'IoU-189': 2.212342840958384, 'IoU-190': 3.4200140006098425, 'IoU-191': 2.8256916963529113, 'mACC': 31.56580479143377, 'pACC': 58.74946358633342, 'ACC-0': nan, 'ACC-1': 98.64603313009694, 'ACC-2': 61.12679644881489, 'ACC-3': 74.44927914666863, 'ACC-4': 69.91982428121443, 'ACC-5': 64.19608517730015, 'ACC-6': 58.62613113685398, 'ACC-7': 48.67201942745879, 'ACC-8': 30.329003838692735, 'ACC-9': 45.2056218290496, 'ACC-10': 55.23375655055728, 'ACC-11': 65.66750588629279, 'ACC-12': 70.91456093909564, 'ACC-13': 70.12125245181042, 'ACC-14': 69.7406432791733, 'ACC-15': 69.26709185231319, 'ACC-16': 68.87057767169354, 'ACC-17': 66.15324537120595, 'ACC-18': 66.94141895438287, 'ACC-19': 66.1794716477008, 'ACC-20': 65.95715310813847, 'ACC-21': 65.99711058264968, 'ACC-22': 66.83639635084938, 'ACC-23': 65.19601795066457, 'ACC-24': 64.9151604258241, 'ACC-25': 64.40820679408297, 'ACC-26': 63.52866297240403, 'ACC-27': 64.59282548205441, 'ACC-28': 64.73201120225633, 'ACC-29': 64.33583687566176, 'ACC-30': 64.19832616955526, 'ACC-31': 63.94106035929236, 'ACC-32': 63.779357646209576, 'ACC-33': 61.870031512724765, 'ACC-34': 61.8902807764929, 'ACC-35': 62.52862233812013, 'ACC-36': 62.776117299116116, 'ACC-37': 62.11429799872321, 'ACC-38': 61.35557943200924, 'ACC-39': 60.23640140798842, 'ACC-40': 60.66466039893869, 'ACC-41': 59.54380338924764, 'ACC-42': 58.51245877209923, 'ACC-43': 57.89546383607127, 'ACC-44': 57.50189795580051, 'ACC-45': 56.24527207584643, 'ACC-46': 55.78025971139977, 'ACC-47': 55.04255104606829, 'ACC-48': 55.17079842130289, 'ACC-49': 54.87351886886728, 'ACC-50': 55.18771661520381, 'ACC-51': 52.283208263498146, 'ACC-52': 51.75635547602573, 'ACC-53': 51.65365141323042, 'ACC-54': 51.32161489842634, 'ACC-55': 50.393526153633495, 'ACC-56': 49.440261762576895, 'ACC-57': 47.821213334160916, 'ACC-58': 46.175116720375634, 'ACC-59': 44.32542124736304, 'ACC-60': 43.33252185697366, 'ACC-61': 42.56768284702338, 'ACC-62': 42.44031641971423, 'ACC-63': 41.21337434167519, 'ACC-64': 40.30872064943412, 'ACC-65': 39.435438281170015, 'ACC-66': 38.80500696843171, 'ACC-67': 37.28803068639154, 'ACC-68': 36.90030539527335, 'ACC-69': 36.237939489498984, 'ACC-70': 35.82567410201982, 'ACC-71': 34.94052341161923, 'ACC-72': 34.008821797376626, 'ACC-73': 33.605516561041895, 'ACC-74': 34.2496699721915, 'ACC-75': 34.30938291369047, 'ACC-76': 32.78927787867196, 'ACC-77': 33.431040802875074, 'ACC-78': 32.64220492025663, 'ACC-79': 32.3639335705987, 'ACC-80': 31.72915742172239, 'ACC-81': 32.044814668310956, 'ACC-82': 31.23786985071295, 'ACC-83': 31.254850402539585, 'ACC-84': 30.865781869664932, 'ACC-85': 31.407153169992867, 'ACC-86': 30.70409056870473, 'ACC-87': 29.76733869112812, 'ACC-88': 29.73614098556064, 'ACC-89': 29.868256356133355, 'ACC-90': 29.722923062814075, 'ACC-91': 28.96013527102785, 'ACC-92': 29.070236202468035, 'ACC-93': 29.497455108059018, 'ACC-94': 29.625840224205756, 'ACC-95': 29.186511746503434, 'ACC-96': 30.17103287348983, 'ACC-97': 28.665057648494756, 'ACC-98': 28.187053759938934, 'ACC-99': 27.812099132448374, 'ACC-100': 28.06978811123086, 'ACC-101': 26.146420643395217, 'ACC-102': 25.686882646125962, 'ACC-103': 25.213516576933976, 'ACC-104': 25.490150266840462, 'ACC-105': 23.57448543001994, 'ACC-106': 23.961456254390608, 'ACC-107': 24.27483257835855, 'ACC-108': 22.266553947336785, 'ACC-109': 22.618914984485805, 'ACC-110': 23.05414147849748, 'ACC-111': 22.065818186262664, 'ACC-112': 23.538926914815033, 'ACC-113': 22.147723154760975, 'ACC-114': 23.362926417193606, 'ACC-115': 22.33225083818201, 'ACC-116': 20.613677917153364, 'ACC-117': 22.086959456798734, 'ACC-118': 19.26227391645537, 'ACC-119': 24.068586516105043, 'ACC-120': 18.76719657597065, 'ACC-121': 21.835351764853904, 'ACC-122': 18.30191438572655, 'ACC-123': 18.07299242700856, 'ACC-124': 19.209994560761515, 'ACC-125': 18.90803741286086, 'ACC-126': 17.33394657510542, 'ACC-127': 17.827238830378754, 'ACC-128': 16.5329293771977, 'ACC-129': 14.67118569190096, 'ACC-130': 15.736644283528916, 'ACC-131': 14.267039628155004, 'ACC-132': 15.227821844222605, 'ACC-133': 13.772207366279504, 'ACC-134': 12.67329599310642, 'ACC-135': 13.188529730346046, 'ACC-136': 13.687380614051433, 'ACC-137': 11.376207147365665, 'ACC-138': 15.711776911947611, 'ACC-139': 11.231126301800447, 'ACC-140': 12.387959323291705, 'ACC-141': 11.196117406869437, 'ACC-142': 12.876606573798252, 'ACC-143': 10.786509988291355, 'ACC-144': 12.0557761732852, 'ACC-145': 11.877546456763344, 'ACC-146': 9.230038153115077, 'ACC-147': 10.362187703984185, 'ACC-148': 11.273217743246287, 'ACC-149': 8.881122923446535, 'ACC-150': 9.816251463140329, 'ACC-151': 7.193565267621816, 'ACC-152': 6.545662215928248, 'ACC-153': 9.378378130782902, 'ACC-154': 5.984968303339215, 'ACC-155': 6.35281009113476, 'ACC-156': 7.328563025590309, 'ACC-157': 6.1808847567755905, 'ACC-158': 6.2903023212161155, 'ACC-159': 4.602381148156864, 'ACC-160': 6.174077048364476, 'ACC-161': 6.922098310196952, 'ACC-162': 5.487933686600159, 'ACC-163': 5.409474950586012, 'ACC-164': 4.96767176456039, 'ACC-165': 4.7940615004431, 'ACC-166': 4.173053193543242, 'ACC-167': 5.312098834916714, 'ACC-168': 5.541880582319432, 'ACC-169': 4.960755790999364, 'ACC-170': 5.361883669094368, 'ACC-171': 3.2330472126654777, 'ACC-172': 4.1400896395895845, 'ACC-173': 5.582627477394422, 'ACC-174': 4.662979676819869, 'ACC-175': 2.349323970795686, 'ACC-176': 2.2204042837241698, 'ACC-177': 3.8867327712048483, 'ACC-178': 2.3715934179493967, 'ACC-179': 4.001371527813002, 'ACC-180': 5.536929797825866, 'ACC-181': 4.3666105296069695, 'ACC-182': 1.5849737851495775, 'ACC-183': 3.6357771965221977, 'ACC-184': 6.070511323838431, 'ACC-185': 3.1350322649187716, 'ACC-186': 4.321161059740848, 'ACC-187': 4.721245726573164, 'ACC-188': 4.4666276582865905, 'ACC-189': 8.261942231612673, 'ACC-190': 10.432365301780605, 'ACC-191': 8.964698205546492})])
[01/18 13:10:44] d2.engine.defaults INFO: Evaluation results for sceneflow_test in csv format:
[01/18 13:10:44] d2.evaluation.testing INFO: copypaste: Task: sem_seg
[01/18 13:10:44] d2.evaluation.testing INFO: copypaste: epe,mIoU,fwIoU,mACC,pACC
[01/18 13:10:44] d2.evaluation.testing INFO: copypaste: 2.2840,21.1696,44.3628,31.5658,58.7495
