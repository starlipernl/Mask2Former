[02/24 11:33:53] detectron2 INFO: Rank of current process: 3. World size: 4
[02/24 11:34:02] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 11:34:02] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 11:34:02] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 11:34:02] detectron2.utils.env INFO: Using a generated random seed 2678156
[02/24 11:34:12] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 1.0, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 11:34:12] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 11:34:24] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 11:34:25] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 11:34:33] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 11:34:34] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 11:34:34] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 11:34:34] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 11:34:35] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 11:34:35] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 11:34:35] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 11:34:56] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 395, in run_step
    loss_dict = self.model(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 791, in forward
    losses = self.criterion(outputs, targets)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/criterion.py", line 259, in forward
    losses.update(self.get_loss(loss, outputs, targets, indices, num_masks))
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 212, in get_loss
    return loss_map[loss](outputs, targets, indices, num_masks)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 191, in loss_segs
    semseg,
UnboundLocalError: local variable 'semseg' referenced before assignment
[02/24 11:34:56] detectron2.engine.hooks INFO: Total training time: 0:00:21 (0:00:00 on hooks)
[02/24 11:37:47] detectron2 INFO: Rank of current process: 3. World size: 4
[02/24 11:37:55] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 11:37:55] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 11:37:55] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 11:37:55] detectron2.utils.env INFO: Using a generated random seed 56076757
[02/24 11:38:03] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 1.0, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 11:38:04] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 11:38:16] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 11:38:16] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 11:38:17] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 11:38:17] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 11:38:17] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 11:38:18] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 11:38:19] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 11:38:19] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 11:38:19] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 12:10:58] detectron2.engine.hooks INFO: Overall training speed: 1100 iterations in 0:32:08 (1.7532 s / it)
[02/24 12:10:58] detectron2.engine.hooks INFO: Total training time: 0:32:14 (0:00:06 on hooks)
[02/24 12:12:19] detectron2 INFO: Rank of current process: 3. World size: 4
[02/24 12:12:26] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 12:12:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 12:12:26] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 12:12:26] detectron2.utils.env INFO: Using a generated random seed 26608385
[02/24 12:12:33] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 0.1, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 12:12:33] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 12:12:44] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 12:12:44] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 12:12:44] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 12:12:53] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 12:12:53] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 12:12:53] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 12:12:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 12:12:54] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 12:12:54] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 13:23:46] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 13:23:47] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 13:23:47] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 13:24:10] mask2former INFO: Inference done 11/1092. Dataloading: 0.0083 s/iter. Inference: 0.3720 s/iter. Eval: 0.2124 s/iter. Total: 0.5927 s/iter. ETA=0:10:40
[02/24 13:24:15] mask2former INFO: Inference done 19/1092. Dataloading: 0.0097 s/iter. Inference: 0.3830 s/iter. Eval: 0.2347 s/iter. Total: 0.6275 s/iter. ETA=0:11:13
[02/24 13:24:21] mask2former INFO: Inference done 27/1092. Dataloading: 0.0098 s/iter. Inference: 0.3896 s/iter. Eval: 0.2399 s/iter. Total: 0.6395 s/iter. ETA=0:11:21
[02/24 13:24:26] mask2former INFO: Inference done 35/1092. Dataloading: 0.0110 s/iter. Inference: 0.3843 s/iter. Eval: 0.2503 s/iter. Total: 0.6458 s/iter. ETA=0:11:22
[02/24 13:24:31] mask2former INFO: Inference done 43/1092. Dataloading: 0.0122 s/iter. Inference: 0.3855 s/iter. Eval: 0.2459 s/iter. Total: 0.6437 s/iter. ETA=0:11:15
[02/24 13:24:36] mask2former INFO: Inference done 52/1092. Dataloading: 0.0125 s/iter. Inference: 0.3869 s/iter. Eval: 0.2349 s/iter. Total: 0.6344 s/iter. ETA=0:10:59
[02/24 13:24:42] mask2former INFO: Inference done 60/1092. Dataloading: 0.0121 s/iter. Inference: 0.3873 s/iter. Eval: 0.2394 s/iter. Total: 0.6390 s/iter. ETA=0:10:59
[02/24 13:24:47] mask2former INFO: Inference done 68/1092. Dataloading: 0.0121 s/iter. Inference: 0.3857 s/iter. Eval: 0.2414 s/iter. Total: 0.6394 s/iter. ETA=0:10:54
[02/24 13:24:52] mask2former INFO: Inference done 77/1092. Dataloading: 0.0122 s/iter. Inference: 0.3828 s/iter. Eval: 0.2395 s/iter. Total: 0.6347 s/iter. ETA=0:10:44
[02/24 13:24:58] mask2former INFO: Inference done 86/1092. Dataloading: 0.0121 s/iter. Inference: 0.3816 s/iter. Eval: 0.2368 s/iter. Total: 0.6306 s/iter. ETA=0:10:34
[02/24 13:25:03] mask2former INFO: Inference done 94/1092. Dataloading: 0.0118 s/iter. Inference: 0.3819 s/iter. Eval: 0.2380 s/iter. Total: 0.6319 s/iter. ETA=0:10:30
[02/24 13:25:08] mask2former INFO: Inference done 107/1092. Dataloading: 0.0110 s/iter. Inference: 0.3674 s/iter. Eval: 0.2261 s/iter. Total: 0.6046 s/iter. ETA=0:09:55
[02/24 13:25:14] mask2former INFO: Inference done 114/1092. Dataloading: 0.0112 s/iter. Inference: 0.3732 s/iter. Eval: 0.2332 s/iter. Total: 0.6178 s/iter. ETA=0:10:04
[02/24 13:25:19] mask2former INFO: Inference done 120/1092. Dataloading: 0.0112 s/iter. Inference: 0.3801 s/iter. Eval: 0.2403 s/iter. Total: 0.6318 s/iter. ETA=0:10:14
[02/24 13:25:25] mask2former INFO: Inference done 130/1092. Dataloading: 0.0114 s/iter. Inference: 0.3752 s/iter. Eval: 0.2395 s/iter. Total: 0.6262 s/iter. ETA=0:10:02
[02/24 13:25:30] mask2former INFO: Inference done 137/1092. Dataloading: 0.0116 s/iter. Inference: 0.3790 s/iter. Eval: 0.2437 s/iter. Total: 0.6344 s/iter. ETA=0:10:05
[02/24 13:25:36] mask2former INFO: Inference done 145/1092. Dataloading: 0.0118 s/iter. Inference: 0.3790 s/iter. Eval: 0.2471 s/iter. Total: 0.6382 s/iter. ETA=0:10:04
[02/24 13:25:41] mask2former INFO: Inference done 153/1092. Dataloading: 0.0118 s/iter. Inference: 0.3785 s/iter. Eval: 0.2468 s/iter. Total: 0.6375 s/iter. ETA=0:09:58
[02/24 13:25:46] mask2former INFO: Inference done 162/1092. Dataloading: 0.0118 s/iter. Inference: 0.3775 s/iter. Eval: 0.2465 s/iter. Total: 0.6363 s/iter. ETA=0:09:51
[02/24 13:25:52] mask2former INFO: Inference done 170/1092. Dataloading: 0.0121 s/iter. Inference: 0.3796 s/iter. Eval: 0.2459 s/iter. Total: 0.6379 s/iter. ETA=0:09:48
[02/24 13:25:57] mask2former INFO: Inference done 179/1092. Dataloading: 0.0123 s/iter. Inference: 0.3790 s/iter. Eval: 0.2447 s/iter. Total: 0.6365 s/iter. ETA=0:09:41
[02/24 13:26:02] mask2former INFO: Inference done 187/1092. Dataloading: 0.0122 s/iter. Inference: 0.3792 s/iter. Eval: 0.2443 s/iter. Total: 0.6361 s/iter. ETA=0:09:35
[02/24 13:26:08] mask2former INFO: Inference done 196/1092. Dataloading: 0.0122 s/iter. Inference: 0.3784 s/iter. Eval: 0.2439 s/iter. Total: 0.6350 s/iter. ETA=0:09:28
[02/24 13:26:13] mask2former INFO: Inference done 203/1092. Dataloading: 0.0122 s/iter. Inference: 0.3800 s/iter. Eval: 0.2453 s/iter. Total: 0.6379 s/iter. ETA=0:09:27
[02/24 13:26:18] mask2former INFO: Inference done 212/1092. Dataloading: 0.0123 s/iter. Inference: 0.3789 s/iter. Eval: 0.2446 s/iter. Total: 0.6363 s/iter. ETA=0:09:19
[02/24 13:26:23] mask2former INFO: Inference done 220/1092. Dataloading: 0.0122 s/iter. Inference: 0.3794 s/iter. Eval: 0.2443 s/iter. Total: 0.6363 s/iter. ETA=0:09:14
[02/24 13:26:28] mask2former INFO: Inference done 229/1092. Dataloading: 0.0121 s/iter. Inference: 0.3791 s/iter. Eval: 0.2425 s/iter. Total: 0.6342 s/iter. ETA=0:09:07
[02/24 13:26:34] mask2former INFO: Inference done 237/1092. Dataloading: 0.0122 s/iter. Inference: 0.3792 s/iter. Eval: 0.2425 s/iter. Total: 0.6343 s/iter. ETA=0:09:02
[02/24 13:26:39] mask2former INFO: Inference done 244/1092. Dataloading: 0.0124 s/iter. Inference: 0.3812 s/iter. Eval: 0.2426 s/iter. Total: 0.6366 s/iter. ETA=0:08:59
[02/24 13:26:44] mask2former INFO: Inference done 251/1092. Dataloading: 0.0126 s/iter. Inference: 0.3821 s/iter. Eval: 0.2441 s/iter. Total: 0.6391 s/iter. ETA=0:08:57
[02/24 13:26:49] mask2former INFO: Inference done 259/1092. Dataloading: 0.0126 s/iter. Inference: 0.3830 s/iter. Eval: 0.2452 s/iter. Total: 0.6412 s/iter. ETA=0:08:54
[02/24 13:26:54] mask2former INFO: Inference done 267/1092. Dataloading: 0.0125 s/iter. Inference: 0.3825 s/iter. Eval: 0.2455 s/iter. Total: 0.6409 s/iter. ETA=0:08:48
[02/24 13:27:00] mask2former INFO: Inference done 275/1092. Dataloading: 0.0126 s/iter. Inference: 0.3826 s/iter. Eval: 0.2462 s/iter. Total: 0.6417 s/iter. ETA=0:08:44
[02/24 13:27:05] mask2former INFO: Inference done 283/1092. Dataloading: 0.0127 s/iter. Inference: 0.3814 s/iter. Eval: 0.2468 s/iter. Total: 0.6413 s/iter. ETA=0:08:38
[02/24 13:27:10] mask2former INFO: Inference done 292/1092. Dataloading: 0.0127 s/iter. Inference: 0.3805 s/iter. Eval: 0.2470 s/iter. Total: 0.6405 s/iter. ETA=0:08:32
[02/24 13:27:15] mask2former INFO: Inference done 300/1092. Dataloading: 0.0127 s/iter. Inference: 0.3801 s/iter. Eval: 0.2471 s/iter. Total: 0.6403 s/iter. ETA=0:08:27
[02/24 13:27:21] mask2former INFO: Inference done 308/1092. Dataloading: 0.0127 s/iter. Inference: 0.3809 s/iter. Eval: 0.2473 s/iter. Total: 0.6412 s/iter. ETA=0:08:22
[02/24 13:27:26] mask2former INFO: Inference done 316/1092. Dataloading: 0.0126 s/iter. Inference: 0.3816 s/iter. Eval: 0.2473 s/iter. Total: 0.6419 s/iter. ETA=0:08:18
[02/24 13:27:31] mask2former INFO: Inference done 324/1092. Dataloading: 0.0126 s/iter. Inference: 0.3823 s/iter. Eval: 0.2471 s/iter. Total: 0.6424 s/iter. ETA=0:08:13
[02/24 13:27:37] mask2former INFO: Inference done 332/1092. Dataloading: 0.0125 s/iter. Inference: 0.3824 s/iter. Eval: 0.2474 s/iter. Total: 0.6427 s/iter. ETA=0:08:08
[02/24 13:27:42] mask2former INFO: Inference done 340/1092. Dataloading: 0.0125 s/iter. Inference: 0.3824 s/iter. Eval: 0.2472 s/iter. Total: 0.6424 s/iter. ETA=0:08:03
[02/24 13:27:47] mask2former INFO: Inference done 349/1092. Dataloading: 0.0125 s/iter. Inference: 0.3818 s/iter. Eval: 0.2471 s/iter. Total: 0.6417 s/iter. ETA=0:07:56
[02/24 13:27:53] mask2former INFO: Inference done 358/1092. Dataloading: 0.0126 s/iter. Inference: 0.3818 s/iter. Eval: 0.2465 s/iter. Total: 0.6413 s/iter. ETA=0:07:50
[02/24 13:27:58] mask2former INFO: Inference done 367/1092. Dataloading: 0.0125 s/iter. Inference: 0.3812 s/iter. Eval: 0.2457 s/iter. Total: 0.6398 s/iter. ETA=0:07:43
[02/24 13:28:04] mask2former INFO: Inference done 376/1092. Dataloading: 0.0125 s/iter. Inference: 0.3812 s/iter. Eval: 0.2459 s/iter. Total: 0.6399 s/iter. ETA=0:07:38
[02/24 13:28:09] mask2former INFO: Inference done 385/1092. Dataloading: 0.0126 s/iter. Inference: 0.3806 s/iter. Eval: 0.2452 s/iter. Total: 0.6388 s/iter. ETA=0:07:31
[02/24 13:28:14] mask2former INFO: Inference done 393/1092. Dataloading: 0.0128 s/iter. Inference: 0.3805 s/iter. Eval: 0.2452 s/iter. Total: 0.6388 s/iter. ETA=0:07:26
[02/24 13:28:20] mask2former INFO: Inference done 402/1092. Dataloading: 0.0129 s/iter. Inference: 0.3802 s/iter. Eval: 0.2451 s/iter. Total: 0.6385 s/iter. ETA=0:07:20
[02/24 13:28:25] mask2former INFO: Inference done 412/1092. Dataloading: 0.0128 s/iter. Inference: 0.3792 s/iter. Eval: 0.2437 s/iter. Total: 0.6361 s/iter. ETA=0:07:12
[02/24 13:28:31] mask2former INFO: Inference done 420/1092. Dataloading: 0.0128 s/iter. Inference: 0.3793 s/iter. Eval: 0.2440 s/iter. Total: 0.6363 s/iter. ETA=0:07:07
[02/24 13:28:36] mask2former INFO: Inference done 428/1092. Dataloading: 0.0128 s/iter. Inference: 0.3797 s/iter. Eval: 0.2433 s/iter. Total: 0.6362 s/iter. ETA=0:07:02
[02/24 13:28:41] mask2former INFO: Inference done 436/1092. Dataloading: 0.0128 s/iter. Inference: 0.3796 s/iter. Eval: 0.2436 s/iter. Total: 0.6363 s/iter. ETA=0:06:57
[02/24 13:28:46] mask2former INFO: Inference done 445/1092. Dataloading: 0.0128 s/iter. Inference: 0.3788 s/iter. Eval: 0.2430 s/iter. Total: 0.6349 s/iter. ETA=0:06:50
[02/24 13:28:51] mask2former INFO: Inference done 454/1092. Dataloading: 0.0128 s/iter. Inference: 0.3791 s/iter. Eval: 0.2417 s/iter. Total: 0.6339 s/iter. ETA=0:06:44
[02/24 13:28:56] mask2former INFO: Inference done 463/1092. Dataloading: 0.0128 s/iter. Inference: 0.3786 s/iter. Eval: 0.2415 s/iter. Total: 0.6332 s/iter. ETA=0:06:38
[02/24 13:29:02] mask2former INFO: Inference done 472/1092. Dataloading: 0.0127 s/iter. Inference: 0.3781 s/iter. Eval: 0.2415 s/iter. Total: 0.6327 s/iter. ETA=0:06:32
[02/24 13:29:07] mask2former INFO: Inference done 481/1092. Dataloading: 0.0127 s/iter. Inference: 0.3780 s/iter. Eval: 0.2412 s/iter. Total: 0.6322 s/iter. ETA=0:06:26
[02/24 13:29:12] mask2former INFO: Inference done 489/1092. Dataloading: 0.0127 s/iter. Inference: 0.3782 s/iter. Eval: 0.2409 s/iter. Total: 0.6322 s/iter. ETA=0:06:21
[02/24 13:29:18] mask2former INFO: Inference done 498/1092. Dataloading: 0.0127 s/iter. Inference: 0.3780 s/iter. Eval: 0.2405 s/iter. Total: 0.6316 s/iter. ETA=0:06:15
[02/24 13:29:23] mask2former INFO: Inference done 507/1092. Dataloading: 0.0127 s/iter. Inference: 0.3775 s/iter. Eval: 0.2400 s/iter. Total: 0.6305 s/iter. ETA=0:06:08
[02/24 13:29:29] mask2former INFO: Inference done 515/1092. Dataloading: 0.0126 s/iter. Inference: 0.3782 s/iter. Eval: 0.2408 s/iter. Total: 0.6319 s/iter. ETA=0:06:04
[02/24 13:29:34] mask2former INFO: Inference done 524/1092. Dataloading: 0.0126 s/iter. Inference: 0.3782 s/iter. Eval: 0.2407 s/iter. Total: 0.6318 s/iter. ETA=0:05:58
[02/24 13:29:40] mask2former INFO: Inference done 533/1092. Dataloading: 0.0126 s/iter. Inference: 0.3776 s/iter. Eval: 0.2407 s/iter. Total: 0.6312 s/iter. ETA=0:05:52
[02/24 13:29:45] mask2former INFO: Inference done 541/1092. Dataloading: 0.0126 s/iter. Inference: 0.3784 s/iter. Eval: 0.2408 s/iter. Total: 0.6321 s/iter. ETA=0:05:48
[02/24 13:29:50] mask2former INFO: Inference done 549/1092. Dataloading: 0.0126 s/iter. Inference: 0.3787 s/iter. Eval: 0.2407 s/iter. Total: 0.6323 s/iter. ETA=0:05:43
[02/24 13:29:56] mask2former INFO: Inference done 557/1092. Dataloading: 0.0126 s/iter. Inference: 0.3788 s/iter. Eval: 0.2412 s/iter. Total: 0.6329 s/iter. ETA=0:05:38
[02/24 13:30:01] mask2former INFO: Inference done 565/1092. Dataloading: 0.0126 s/iter. Inference: 0.3787 s/iter. Eval: 0.2416 s/iter. Total: 0.6332 s/iter. ETA=0:05:33
[02/24 13:30:07] mask2former INFO: Inference done 573/1092. Dataloading: 0.0126 s/iter. Inference: 0.3793 s/iter. Eval: 0.2418 s/iter. Total: 0.6340 s/iter. ETA=0:05:29
[02/24 13:30:13] mask2former INFO: Inference done 582/1092. Dataloading: 0.0131 s/iter. Inference: 0.3792 s/iter. Eval: 0.2420 s/iter. Total: 0.6346 s/iter. ETA=0:05:23
[02/24 13:30:18] mask2former INFO: Inference done 591/1092. Dataloading: 0.0130 s/iter. Inference: 0.3789 s/iter. Eval: 0.2419 s/iter. Total: 0.6341 s/iter. ETA=0:05:17
[02/24 13:30:23] mask2former INFO: Inference done 599/1092. Dataloading: 0.0130 s/iter. Inference: 0.3790 s/iter. Eval: 0.2424 s/iter. Total: 0.6347 s/iter. ETA=0:05:12
[02/24 13:30:29] mask2former INFO: Inference done 607/1092. Dataloading: 0.0130 s/iter. Inference: 0.3788 s/iter. Eval: 0.2428 s/iter. Total: 0.6348 s/iter. ETA=0:05:07
[02/24 13:30:34] mask2former INFO: Inference done 614/1092. Dataloading: 0.0129 s/iter. Inference: 0.3792 s/iter. Eval: 0.2434 s/iter. Total: 0.6358 s/iter. ETA=0:05:03
[02/24 13:30:39] mask2former INFO: Inference done 622/1092. Dataloading: 0.0129 s/iter. Inference: 0.3790 s/iter. Eval: 0.2439 s/iter. Total: 0.6362 s/iter. ETA=0:04:59
[02/24 13:30:44] mask2former INFO: Inference done 630/1092. Dataloading: 0.0129 s/iter. Inference: 0.3787 s/iter. Eval: 0.2447 s/iter. Total: 0.6366 s/iter. ETA=0:04:54
[02/24 13:30:50] mask2former INFO: Inference done 638/1092. Dataloading: 0.0129 s/iter. Inference: 0.3790 s/iter. Eval: 0.2448 s/iter. Total: 0.6370 s/iter. ETA=0:04:49
[02/24 13:30:55] mask2former INFO: Inference done 646/1092. Dataloading: 0.0130 s/iter. Inference: 0.3791 s/iter. Eval: 0.2448 s/iter. Total: 0.6372 s/iter. ETA=0:04:44
[02/24 13:31:00] mask2former INFO: Inference done 654/1092. Dataloading: 0.0130 s/iter. Inference: 0.3794 s/iter. Eval: 0.2450 s/iter. Total: 0.6377 s/iter. ETA=0:04:39
[02/24 13:31:06] mask2former INFO: Inference done 662/1092. Dataloading: 0.0131 s/iter. Inference: 0.3797 s/iter. Eval: 0.2451 s/iter. Total: 0.6382 s/iter. ETA=0:04:34
[02/24 13:31:11] mask2former INFO: Inference done 671/1092. Dataloading: 0.0131 s/iter. Inference: 0.3790 s/iter. Eval: 0.2451 s/iter. Total: 0.6375 s/iter. ETA=0:04:28
[02/24 13:31:16] mask2former INFO: Inference done 679/1092. Dataloading: 0.0130 s/iter. Inference: 0.3793 s/iter. Eval: 0.2453 s/iter. Total: 0.6380 s/iter. ETA=0:04:23
[02/24 13:31:22] mask2former INFO: Inference done 687/1092. Dataloading: 0.0131 s/iter. Inference: 0.3794 s/iter. Eval: 0.2452 s/iter. Total: 0.6379 s/iter. ETA=0:04:18
[02/24 13:31:27] mask2former INFO: Inference done 695/1092. Dataloading: 0.0131 s/iter. Inference: 0.3799 s/iter. Eval: 0.2449 s/iter. Total: 0.6382 s/iter. ETA=0:04:13
[02/24 13:31:32] mask2former INFO: Inference done 705/1092. Dataloading: 0.0130 s/iter. Inference: 0.3792 s/iter. Eval: 0.2438 s/iter. Total: 0.6364 s/iter. ETA=0:04:06
[02/24 13:31:37] mask2former INFO: Inference done 715/1092. Dataloading: 0.0129 s/iter. Inference: 0.3783 s/iter. Eval: 0.2433 s/iter. Total: 0.6349 s/iter. ETA=0:03:59
[02/24 13:31:43] mask2former INFO: Inference done 722/1092. Dataloading: 0.0129 s/iter. Inference: 0.3789 s/iter. Eval: 0.2442 s/iter. Total: 0.6363 s/iter. ETA=0:03:55
[02/24 13:31:48] mask2former INFO: Inference done 728/1092. Dataloading: 0.0130 s/iter. Inference: 0.3803 s/iter. Eval: 0.2450 s/iter. Total: 0.6385 s/iter. ETA=0:03:52
[02/24 13:31:53] mask2former INFO: Inference done 737/1092. Dataloading: 0.0129 s/iter. Inference: 0.3801 s/iter. Eval: 0.2446 s/iter. Total: 0.6379 s/iter. ETA=0:03:46
[02/24 13:31:59] mask2former INFO: Inference done 744/1092. Dataloading: 0.0129 s/iter. Inference: 0.3808 s/iter. Eval: 0.2450 s/iter. Total: 0.6391 s/iter. ETA=0:03:42
[02/24 13:32:04] mask2former INFO: Inference done 752/1092. Dataloading: 0.0129 s/iter. Inference: 0.3807 s/iter. Eval: 0.2451 s/iter. Total: 0.6389 s/iter. ETA=0:03:37
[02/24 13:32:09] mask2former INFO: Inference done 760/1092. Dataloading: 0.0128 s/iter. Inference: 0.3807 s/iter. Eval: 0.2451 s/iter. Total: 0.6390 s/iter. ETA=0:03:32
[02/24 13:32:14] mask2former INFO: Inference done 769/1092. Dataloading: 0.0128 s/iter. Inference: 0.3803 s/iter. Eval: 0.2451 s/iter. Total: 0.6385 s/iter. ETA=0:03:26
[02/24 13:32:20] mask2former INFO: Inference done 777/1092. Dataloading: 0.0128 s/iter. Inference: 0.3808 s/iter. Eval: 0.2449 s/iter. Total: 0.6388 s/iter. ETA=0:03:21
[02/24 13:32:25] mask2former INFO: Inference done 786/1092. Dataloading: 0.0128 s/iter. Inference: 0.3807 s/iter. Eval: 0.2444 s/iter. Total: 0.6382 s/iter. ETA=0:03:15
[02/24 13:32:30] mask2former INFO: Inference done 795/1092. Dataloading: 0.0127 s/iter. Inference: 0.3809 s/iter. Eval: 0.2440 s/iter. Total: 0.6380 s/iter. ETA=0:03:09
[02/24 13:32:35] mask2former INFO: Inference done 803/1092. Dataloading: 0.0127 s/iter. Inference: 0.3806 s/iter. Eval: 0.2442 s/iter. Total: 0.6378 s/iter. ETA=0:03:04
[02/24 13:32:41] mask2former INFO: Inference done 811/1092. Dataloading: 0.0127 s/iter. Inference: 0.3809 s/iter. Eval: 0.2443 s/iter. Total: 0.6382 s/iter. ETA=0:02:59
[02/24 13:32:46] mask2former INFO: Inference done 819/1092. Dataloading: 0.0127 s/iter. Inference: 0.3810 s/iter. Eval: 0.2447 s/iter. Total: 0.6387 s/iter. ETA=0:02:54
[02/24 13:32:52] mask2former INFO: Inference done 828/1092. Dataloading: 0.0127 s/iter. Inference: 0.3808 s/iter. Eval: 0.2448 s/iter. Total: 0.6385 s/iter. ETA=0:02:48
[02/24 13:32:58] mask2former INFO: Inference done 837/1092. Dataloading: 0.0126 s/iter. Inference: 0.3805 s/iter. Eval: 0.2451 s/iter. Total: 0.6385 s/iter. ETA=0:02:42
[02/24 13:33:03] mask2former INFO: Inference done 845/1092. Dataloading: 0.0126 s/iter. Inference: 0.3806 s/iter. Eval: 0.2456 s/iter. Total: 0.6391 s/iter. ETA=0:02:37
[02/24 13:33:09] mask2former INFO: Inference done 854/1092. Dataloading: 0.0126 s/iter. Inference: 0.3806 s/iter. Eval: 0.2454 s/iter. Total: 0.6388 s/iter. ETA=0:02:32
[02/24 13:33:14] mask2former INFO: Inference done 862/1092. Dataloading: 0.0126 s/iter. Inference: 0.3807 s/iter. Eval: 0.2454 s/iter. Total: 0.6390 s/iter. ETA=0:02:26
[02/24 13:33:19] mask2former INFO: Inference done 870/1092. Dataloading: 0.0126 s/iter. Inference: 0.3809 s/iter. Eval: 0.2456 s/iter. Total: 0.6393 s/iter. ETA=0:02:21
[02/24 13:33:25] mask2former INFO: Inference done 878/1092. Dataloading: 0.0125 s/iter. Inference: 0.3808 s/iter. Eval: 0.2459 s/iter. Total: 0.6395 s/iter. ETA=0:02:16
[02/24 13:33:30] mask2former INFO: Inference done 886/1092. Dataloading: 0.0126 s/iter. Inference: 0.3810 s/iter. Eval: 0.2457 s/iter. Total: 0.6395 s/iter. ETA=0:02:11
[02/24 13:33:35] mask2former INFO: Inference done 895/1092. Dataloading: 0.0126 s/iter. Inference: 0.3808 s/iter. Eval: 0.2455 s/iter. Total: 0.6392 s/iter. ETA=0:02:05
[02/24 13:33:41] mask2former INFO: Inference done 903/1092. Dataloading: 0.0125 s/iter. Inference: 0.3807 s/iter. Eval: 0.2459 s/iter. Total: 0.6393 s/iter. ETA=0:02:00
[02/24 13:33:46] mask2former INFO: Inference done 911/1092. Dataloading: 0.0126 s/iter. Inference: 0.3806 s/iter. Eval: 0.2460 s/iter. Total: 0.6394 s/iter. ETA=0:01:55
[02/24 13:33:51] mask2former INFO: Inference done 919/1092. Dataloading: 0.0126 s/iter. Inference: 0.3807 s/iter. Eval: 0.2462 s/iter. Total: 0.6397 s/iter. ETA=0:01:50
[02/24 13:33:56] mask2former INFO: Inference done 927/1092. Dataloading: 0.0126 s/iter. Inference: 0.3807 s/iter. Eval: 0.2464 s/iter. Total: 0.6399 s/iter. ETA=0:01:45
[02/24 13:34:02] mask2former INFO: Inference done 935/1092. Dataloading: 0.0125 s/iter. Inference: 0.3805 s/iter. Eval: 0.2467 s/iter. Total: 0.6400 s/iter. ETA=0:01:40
[02/24 13:34:07] mask2former INFO: Inference done 943/1092. Dataloading: 0.0125 s/iter. Inference: 0.3807 s/iter. Eval: 0.2466 s/iter. Total: 0.6401 s/iter. ETA=0:01:35
[02/24 13:34:12] mask2former INFO: Inference done 951/1092. Dataloading: 0.0125 s/iter. Inference: 0.3805 s/iter. Eval: 0.2468 s/iter. Total: 0.6402 s/iter. ETA=0:01:30
[02/24 13:34:18] mask2former INFO: Inference done 959/1092. Dataloading: 0.0125 s/iter. Inference: 0.3805 s/iter. Eval: 0.2472 s/iter. Total: 0.6405 s/iter. ETA=0:01:25
[02/24 13:34:23] mask2former INFO: Inference done 967/1092. Dataloading: 0.0125 s/iter. Inference: 0.3804 s/iter. Eval: 0.2475 s/iter. Total: 0.6407 s/iter. ETA=0:01:20
[02/24 13:34:28] mask2former INFO: Inference done 976/1092. Dataloading: 0.0125 s/iter. Inference: 0.3801 s/iter. Eval: 0.2476 s/iter. Total: 0.6405 s/iter. ETA=0:01:14
[02/24 13:34:34] mask2former INFO: Inference done 984/1092. Dataloading: 0.0125 s/iter. Inference: 0.3801 s/iter. Eval: 0.2476 s/iter. Total: 0.6405 s/iter. ETA=0:01:09
[02/24 13:34:39] mask2former INFO: Inference done 992/1092. Dataloading: 0.0125 s/iter. Inference: 0.3798 s/iter. Eval: 0.2479 s/iter. Total: 0.6405 s/iter. ETA=0:01:04
[02/24 13:34:44] mask2former INFO: Inference done 1001/1092. Dataloading: 0.0125 s/iter. Inference: 0.3796 s/iter. Eval: 0.2480 s/iter. Total: 0.6404 s/iter. ETA=0:00:58
[02/24 13:34:50] mask2former INFO: Inference done 1009/1092. Dataloading: 0.0125 s/iter. Inference: 0.3796 s/iter. Eval: 0.2483 s/iter. Total: 0.6407 s/iter. ETA=0:00:53
[02/24 13:34:55] mask2former INFO: Inference done 1018/1092. Dataloading: 0.0126 s/iter. Inference: 0.3794 s/iter. Eval: 0.2482 s/iter. Total: 0.6405 s/iter. ETA=0:00:47
[02/24 13:35:00] mask2former INFO: Inference done 1026/1092. Dataloading: 0.0126 s/iter. Inference: 0.3794 s/iter. Eval: 0.2482 s/iter. Total: 0.6405 s/iter. ETA=0:00:42
[02/24 13:35:06] mask2former INFO: Inference done 1034/1092. Dataloading: 0.0126 s/iter. Inference: 0.3796 s/iter. Eval: 0.2481 s/iter. Total: 0.6406 s/iter. ETA=0:00:37
[02/24 13:35:11] mask2former INFO: Inference done 1043/1092. Dataloading: 0.0126 s/iter. Inference: 0.3793 s/iter. Eval: 0.2483 s/iter. Total: 0.6405 s/iter. ETA=0:00:31
[02/24 13:35:16] mask2former INFO: Inference done 1051/1092. Dataloading: 0.0126 s/iter. Inference: 0.3793 s/iter. Eval: 0.2483 s/iter. Total: 0.6404 s/iter. ETA=0:00:26
[02/24 13:35:21] mask2former INFO: Inference done 1059/1092. Dataloading: 0.0125 s/iter. Inference: 0.3793 s/iter. Eval: 0.2483 s/iter. Total: 0.6404 s/iter. ETA=0:00:21
[02/24 13:35:27] mask2former INFO: Inference done 1067/1092. Dataloading: 0.0125 s/iter. Inference: 0.3792 s/iter. Eval: 0.2484 s/iter. Total: 0.6404 s/iter. ETA=0:00:16
[02/24 13:35:32] mask2former INFO: Inference done 1075/1092. Dataloading: 0.0125 s/iter. Inference: 0.3793 s/iter. Eval: 0.2484 s/iter. Total: 0.6406 s/iter. ETA=0:00:10
[02/24 13:35:37] mask2former INFO: Inference done 1084/1092. Dataloading: 0.0125 s/iter. Inference: 0.3793 s/iter. Eval: 0.2483 s/iter. Total: 0.6404 s/iter. ETA=0:00:05
[02/24 14:47:16] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 14:47:18] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 14:47:18] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 14:47:40] mask2former INFO: Inference done 11/1092. Dataloading: 0.0153 s/iter. Inference: 0.3648 s/iter. Eval: 0.2425 s/iter. Total: 0.6227 s/iter. ETA=0:11:13
[02/24 14:47:45] mask2former INFO: Inference done 18/1092. Dataloading: 0.0173 s/iter. Inference: 0.3921 s/iter. Eval: 0.2764 s/iter. Total: 0.6858 s/iter. ETA=0:12:16
[02/24 14:47:51] mask2former INFO: Inference done 26/1092. Dataloading: 0.0183 s/iter. Inference: 0.4030 s/iter. Eval: 0.2653 s/iter. Total: 0.6869 s/iter. ETA=0:12:12
[02/24 14:47:56] mask2former INFO: Inference done 33/1092. Dataloading: 0.0166 s/iter. Inference: 0.4040 s/iter. Eval: 0.2739 s/iter. Total: 0.6946 s/iter. ETA=0:12:15
[02/24 14:48:01] mask2former INFO: Inference done 41/1092. Dataloading: 0.0161 s/iter. Inference: 0.4101 s/iter. Eval: 0.2710 s/iter. Total: 0.6975 s/iter. ETA=0:12:13
[02/24 14:48:06] mask2former INFO: Inference done 48/1092. Dataloading: 0.0163 s/iter. Inference: 0.4202 s/iter. Eval: 0.2663 s/iter. Total: 0.7031 s/iter. ETA=0:12:14
[02/24 14:48:12] mask2former INFO: Inference done 56/1092. Dataloading: 0.0161 s/iter. Inference: 0.4183 s/iter. Eval: 0.2672 s/iter. Total: 0.7019 s/iter. ETA=0:12:07
[02/24 14:48:18] mask2former INFO: Inference done 64/1092. Dataloading: 0.0156 s/iter. Inference: 0.4157 s/iter. Eval: 0.2701 s/iter. Total: 0.7017 s/iter. ETA=0:12:01
[02/24 14:48:23] mask2former INFO: Inference done 72/1092. Dataloading: 0.0149 s/iter. Inference: 0.4170 s/iter. Eval: 0.2717 s/iter. Total: 0.7039 s/iter. ETA=0:11:57
[02/24 14:48:29] mask2former INFO: Inference done 80/1092. Dataloading: 0.0152 s/iter. Inference: 0.4128 s/iter. Eval: 0.2758 s/iter. Total: 0.7040 s/iter. ETA=0:11:52
[02/24 14:48:34] mask2former INFO: Inference done 89/1092. Dataloading: 0.0150 s/iter. Inference: 0.4070 s/iter. Eval: 0.2706 s/iter. Total: 0.6929 s/iter. ETA=0:11:34
[02/24 14:48:40] mask2former INFO: Inference done 101/1092. Dataloading: 0.0138 s/iter. Inference: 0.3916 s/iter. Eval: 0.2589 s/iter. Total: 0.6646 s/iter. ETA=0:10:58
[02/24 14:48:46] mask2former INFO: Inference done 107/1092. Dataloading: 0.0139 s/iter. Inference: 0.3981 s/iter. Eval: 0.2687 s/iter. Total: 0.6808 s/iter. ETA=0:11:10
[02/24 14:48:51] mask2former INFO: Inference done 113/1092. Dataloading: 0.0138 s/iter. Inference: 0.4086 s/iter. Eval: 0.2734 s/iter. Total: 0.6961 s/iter. ETA=0:11:21
[02/24 14:48:57] mask2former INFO: Inference done 122/1092. Dataloading: 0.0142 s/iter. Inference: 0.4071 s/iter. Eval: 0.2719 s/iter. Total: 0.6934 s/iter. ETA=0:11:12
[02/24 14:49:02] mask2former INFO: Inference done 128/1092. Dataloading: 0.0140 s/iter. Inference: 0.4118 s/iter. Eval: 0.2753 s/iter. Total: 0.7013 s/iter. ETA=0:11:16
[02/24 14:49:07] mask2former INFO: Inference done 135/1092. Dataloading: 0.0139 s/iter. Inference: 0.4104 s/iter. Eval: 0.2775 s/iter. Total: 0.7021 s/iter. ETA=0:11:11
[02/24 14:49:13] mask2former INFO: Inference done 143/1092. Dataloading: 0.0138 s/iter. Inference: 0.4113 s/iter. Eval: 0.2772 s/iter. Total: 0.7026 s/iter. ETA=0:11:06
[02/24 14:49:18] mask2former INFO: Inference done 151/1092. Dataloading: 0.0137 s/iter. Inference: 0.4094 s/iter. Eval: 0.2755 s/iter. Total: 0.6989 s/iter. ETA=0:10:57
[02/24 14:49:23] mask2former INFO: Inference done 159/1092. Dataloading: 0.0140 s/iter. Inference: 0.4083 s/iter. Eval: 0.2744 s/iter. Total: 0.6969 s/iter. ETA=0:10:50
[02/24 14:49:29] mask2former INFO: Inference done 168/1092. Dataloading: 0.0139 s/iter. Inference: 0.4043 s/iter. Eval: 0.2727 s/iter. Total: 0.6911 s/iter. ETA=0:10:38
[02/24 14:49:34] mask2former INFO: Inference done 175/1092. Dataloading: 0.0138 s/iter. Inference: 0.4056 s/iter. Eval: 0.2726 s/iter. Total: 0.6923 s/iter. ETA=0:10:34
[02/24 14:49:39] mask2former INFO: Inference done 183/1092. Dataloading: 0.0136 s/iter. Inference: 0.4047 s/iter. Eval: 0.2715 s/iter. Total: 0.6900 s/iter. ETA=0:10:27
[02/24 14:49:44] mask2former INFO: Inference done 191/1092. Dataloading: 0.0136 s/iter. Inference: 0.4050 s/iter. Eval: 0.2697 s/iter. Total: 0.6885 s/iter. ETA=0:10:20
[02/24 14:49:50] mask2former INFO: Inference done 200/1092. Dataloading: 0.0136 s/iter. Inference: 0.4042 s/iter. Eval: 0.2676 s/iter. Total: 0.6856 s/iter. ETA=0:10:11
[02/24 14:49:55] mask2former INFO: Inference done 209/1092. Dataloading: 0.0136 s/iter. Inference: 0.4027 s/iter. Eval: 0.2660 s/iter. Total: 0.6825 s/iter. ETA=0:10:02
[02/24 14:50:00] mask2former INFO: Inference done 217/1092. Dataloading: 0.0134 s/iter. Inference: 0.4029 s/iter. Eval: 0.2643 s/iter. Total: 0.6808 s/iter. ETA=0:09:55
[02/24 14:50:06] mask2former INFO: Inference done 226/1092. Dataloading: 0.0133 s/iter. Inference: 0.4000 s/iter. Eval: 0.2625 s/iter. Total: 0.6760 s/iter. ETA=0:09:45
[02/24 14:50:11] mask2former INFO: Inference done 234/1092. Dataloading: 0.0132 s/iter. Inference: 0.3993 s/iter. Eval: 0.2640 s/iter. Total: 0.6768 s/iter. ETA=0:09:40
[02/24 14:50:16] mask2former INFO: Inference done 242/1092. Dataloading: 0.0133 s/iter. Inference: 0.3984 s/iter. Eval: 0.2634 s/iter. Total: 0.6753 s/iter. ETA=0:09:34
[02/24 14:50:21] mask2former INFO: Inference done 250/1092. Dataloading: 0.0133 s/iter. Inference: 0.3983 s/iter. Eval: 0.2630 s/iter. Total: 0.6748 s/iter. ETA=0:09:28
[02/24 14:50:27] mask2former INFO: Inference done 258/1092. Dataloading: 0.0134 s/iter. Inference: 0.3971 s/iter. Eval: 0.2633 s/iter. Total: 0.6740 s/iter. ETA=0:09:22
[02/24 14:50:32] mask2former INFO: Inference done 266/1092. Dataloading: 0.0133 s/iter. Inference: 0.3986 s/iter. Eval: 0.2623 s/iter. Total: 0.6745 s/iter. ETA=0:09:17
[02/24 14:50:38] mask2former INFO: Inference done 274/1092. Dataloading: 0.0134 s/iter. Inference: 0.3988 s/iter. Eval: 0.2620 s/iter. Total: 0.6745 s/iter. ETA=0:09:11
[02/24 14:50:43] mask2former INFO: Inference done 282/1092. Dataloading: 0.0135 s/iter. Inference: 0.3988 s/iter. Eval: 0.2620 s/iter. Total: 0.6746 s/iter. ETA=0:09:06
[02/24 14:50:48] mask2former INFO: Inference done 290/1092. Dataloading: 0.0135 s/iter. Inference: 0.3977 s/iter. Eval: 0.2618 s/iter. Total: 0.6732 s/iter. ETA=0:08:59
[02/24 14:50:53] mask2former INFO: Inference done 298/1092. Dataloading: 0.0134 s/iter. Inference: 0.3974 s/iter. Eval: 0.2613 s/iter. Total: 0.6723 s/iter. ETA=0:08:53
[02/24 14:50:58] mask2former INFO: Inference done 306/1092. Dataloading: 0.0134 s/iter. Inference: 0.3972 s/iter. Eval: 0.2614 s/iter. Total: 0.6721 s/iter. ETA=0:08:48
[02/24 14:51:04] mask2former INFO: Inference done 315/1092. Dataloading: 0.0135 s/iter. Inference: 0.3961 s/iter. Eval: 0.2609 s/iter. Total: 0.6707 s/iter. ETA=0:08:41
[02/24 14:51:09] mask2former INFO: Inference done 323/1092. Dataloading: 0.0134 s/iter. Inference: 0.3966 s/iter. Eval: 0.2600 s/iter. Total: 0.6701 s/iter. ETA=0:08:35
[02/24 14:51:14] mask2former INFO: Inference done 331/1092. Dataloading: 0.0132 s/iter. Inference: 0.3964 s/iter. Eval: 0.2594 s/iter. Total: 0.6693 s/iter. ETA=0:08:29
[02/24 14:51:19] mask2former INFO: Inference done 339/1092. Dataloading: 0.0131 s/iter. Inference: 0.3965 s/iter. Eval: 0.2586 s/iter. Total: 0.6685 s/iter. ETA=0:08:23
[02/24 14:51:25] mask2former INFO: Inference done 346/1092. Dataloading: 0.0131 s/iter. Inference: 0.3968 s/iter. Eval: 0.2597 s/iter. Total: 0.6699 s/iter. ETA=0:08:19
[02/24 14:51:30] mask2former INFO: Inference done 354/1092. Dataloading: 0.0130 s/iter. Inference: 0.3964 s/iter. Eval: 0.2605 s/iter. Total: 0.6702 s/iter. ETA=0:08:14
[02/24 14:51:35] mask2former INFO: Inference done 362/1092. Dataloading: 0.0130 s/iter. Inference: 0.3964 s/iter. Eval: 0.2600 s/iter. Total: 0.6697 s/iter. ETA=0:08:08
[02/24 14:51:41] mask2former INFO: Inference done 370/1092. Dataloading: 0.0129 s/iter. Inference: 0.3967 s/iter. Eval: 0.2600 s/iter. Total: 0.6699 s/iter. ETA=0:08:03
[02/24 14:51:46] mask2former INFO: Inference done 378/1092. Dataloading: 0.0130 s/iter. Inference: 0.3972 s/iter. Eval: 0.2591 s/iter. Total: 0.6696 s/iter. ETA=0:07:58
[02/24 14:51:51] mask2former INFO: Inference done 386/1092. Dataloading: 0.0129 s/iter. Inference: 0.3972 s/iter. Eval: 0.2598 s/iter. Total: 0.6702 s/iter. ETA=0:07:53
[02/24 14:51:57] mask2former INFO: Inference done 394/1092. Dataloading: 0.0131 s/iter. Inference: 0.3970 s/iter. Eval: 0.2605 s/iter. Total: 0.6708 s/iter. ETA=0:07:48
[02/24 14:52:03] mask2former INFO: Inference done 402/1092. Dataloading: 0.0131 s/iter. Inference: 0.3971 s/iter. Eval: 0.2609 s/iter. Total: 0.6713 s/iter. ETA=0:07:43
[02/24 14:52:08] mask2former INFO: Inference done 411/1092. Dataloading: 0.0132 s/iter. Inference: 0.3967 s/iter. Eval: 0.2596 s/iter. Total: 0.6697 s/iter. ETA=0:07:36
[02/24 14:52:13] mask2former INFO: Inference done 419/1092. Dataloading: 0.0131 s/iter. Inference: 0.3961 s/iter. Eval: 0.2595 s/iter. Total: 0.6690 s/iter. ETA=0:07:30
[02/24 14:52:18] mask2former INFO: Inference done 427/1092. Dataloading: 0.0132 s/iter. Inference: 0.3959 s/iter. Eval: 0.2591 s/iter. Total: 0.6684 s/iter. ETA=0:07:24
[02/24 14:52:24] mask2former INFO: Inference done 435/1092. Dataloading: 0.0132 s/iter. Inference: 0.3962 s/iter. Eval: 0.2591 s/iter. Total: 0.6687 s/iter. ETA=0:07:19
[02/24 14:52:29] mask2former INFO: Inference done 443/1092. Dataloading: 0.0131 s/iter. Inference: 0.3961 s/iter. Eval: 0.2587 s/iter. Total: 0.6681 s/iter. ETA=0:07:13
[02/24 14:52:34] mask2former INFO: Inference done 451/1092. Dataloading: 0.0130 s/iter. Inference: 0.3966 s/iter. Eval: 0.2579 s/iter. Total: 0.6678 s/iter. ETA=0:07:08
[02/24 14:52:39] mask2former INFO: Inference done 459/1092. Dataloading: 0.0130 s/iter. Inference: 0.3975 s/iter. Eval: 0.2575 s/iter. Total: 0.6682 s/iter. ETA=0:07:02
[02/24 14:52:45] mask2former INFO: Inference done 467/1092. Dataloading: 0.0130 s/iter. Inference: 0.3970 s/iter. Eval: 0.2580 s/iter. Total: 0.6682 s/iter. ETA=0:06:57
[02/24 14:52:50] mask2former INFO: Inference done 474/1092. Dataloading: 0.0130 s/iter. Inference: 0.3969 s/iter. Eval: 0.2588 s/iter. Total: 0.6690 s/iter. ETA=0:06:53
[02/24 14:52:55] mask2former INFO: Inference done 482/1092. Dataloading: 0.0131 s/iter. Inference: 0.3966 s/iter. Eval: 0.2588 s/iter. Total: 0.6688 s/iter. ETA=0:06:47
[02/24 14:53:00] mask2former INFO: Inference done 490/1092. Dataloading: 0.0130 s/iter. Inference: 0.3964 s/iter. Eval: 0.2585 s/iter. Total: 0.6682 s/iter. ETA=0:06:42
[02/24 14:53:06] mask2former INFO: Inference done 499/1092. Dataloading: 0.0130 s/iter. Inference: 0.3962 s/iter. Eval: 0.2576 s/iter. Total: 0.6671 s/iter. ETA=0:06:35
[02/24 14:53:11] mask2former INFO: Inference done 507/1092. Dataloading: 0.0129 s/iter. Inference: 0.3958 s/iter. Eval: 0.2578 s/iter. Total: 0.6668 s/iter. ETA=0:06:30
[02/24 14:53:16] mask2former INFO: Inference done 515/1092. Dataloading: 0.0129 s/iter. Inference: 0.3961 s/iter. Eval: 0.2576 s/iter. Total: 0.6669 s/iter. ETA=0:06:24
[02/24 14:53:22] mask2former INFO: Inference done 522/1092. Dataloading: 0.0128 s/iter. Inference: 0.3972 s/iter. Eval: 0.2578 s/iter. Total: 0.6681 s/iter. ETA=0:06:20
[02/24 14:53:27] mask2former INFO: Inference done 531/1092. Dataloading: 0.0129 s/iter. Inference: 0.3966 s/iter. Eval: 0.2574 s/iter. Total: 0.6671 s/iter. ETA=0:06:14
[02/24 14:53:32] mask2former INFO: Inference done 539/1092. Dataloading: 0.0128 s/iter. Inference: 0.3967 s/iter. Eval: 0.2574 s/iter. Total: 0.6672 s/iter. ETA=0:06:08
[02/24 14:53:38] mask2former INFO: Inference done 548/1092. Dataloading: 0.0128 s/iter. Inference: 0.3963 s/iter. Eval: 0.2569 s/iter. Total: 0.6663 s/iter. ETA=0:06:02
[02/24 14:53:43] mask2former INFO: Inference done 556/1092. Dataloading: 0.0127 s/iter. Inference: 0.3967 s/iter. Eval: 0.2569 s/iter. Total: 0.6666 s/iter. ETA=0:05:57
[02/24 14:53:49] mask2former INFO: Inference done 564/1092. Dataloading: 0.0128 s/iter. Inference: 0.3964 s/iter. Eval: 0.2568 s/iter. Total: 0.6663 s/iter. ETA=0:05:51
[02/24 14:53:54] mask2former INFO: Inference done 571/1092. Dataloading: 0.0128 s/iter. Inference: 0.3970 s/iter. Eval: 0.2572 s/iter. Total: 0.6673 s/iter. ETA=0:05:47
[02/24 14:53:59] mask2former INFO: Inference done 579/1092. Dataloading: 0.0127 s/iter. Inference: 0.3970 s/iter. Eval: 0.2569 s/iter. Total: 0.6669 s/iter. ETA=0:05:42
[02/24 14:54:04] mask2former INFO: Inference done 587/1092. Dataloading: 0.0127 s/iter. Inference: 0.3967 s/iter. Eval: 0.2567 s/iter. Total: 0.6664 s/iter. ETA=0:05:36
[02/24 14:54:09] mask2former INFO: Inference done 595/1092. Dataloading: 0.0127 s/iter. Inference: 0.3964 s/iter. Eval: 0.2572 s/iter. Total: 0.6667 s/iter. ETA=0:05:31
[02/24 14:54:15] mask2former INFO: Inference done 603/1092. Dataloading: 0.0127 s/iter. Inference: 0.3965 s/iter. Eval: 0.2578 s/iter. Total: 0.6673 s/iter. ETA=0:05:26
[02/24 14:54:20] mask2former INFO: Inference done 611/1092. Dataloading: 0.0127 s/iter. Inference: 0.3966 s/iter. Eval: 0.2574 s/iter. Total: 0.6671 s/iter. ETA=0:05:20
[02/24 14:54:26] mask2former INFO: Inference done 619/1092. Dataloading: 0.0127 s/iter. Inference: 0.3969 s/iter. Eval: 0.2574 s/iter. Total: 0.6673 s/iter. ETA=0:05:15
[02/24 14:54:31] mask2former INFO: Inference done 626/1092. Dataloading: 0.0127 s/iter. Inference: 0.3975 s/iter. Eval: 0.2579 s/iter. Total: 0.6684 s/iter. ETA=0:05:11
[02/24 14:54:37] mask2former INFO: Inference done 634/1092. Dataloading: 0.0126 s/iter. Inference: 0.3980 s/iter. Eval: 0.2576 s/iter. Total: 0.6686 s/iter. ETA=0:05:06
[02/24 14:54:42] mask2former INFO: Inference done 642/1092. Dataloading: 0.0126 s/iter. Inference: 0.3978 s/iter. Eval: 0.2578 s/iter. Total: 0.6685 s/iter. ETA=0:05:00
[02/24 14:54:47] mask2former INFO: Inference done 650/1092. Dataloading: 0.0126 s/iter. Inference: 0.3981 s/iter. Eval: 0.2574 s/iter. Total: 0.6684 s/iter. ETA=0:04:55
[02/24 14:54:53] mask2former INFO: Inference done 659/1092. Dataloading: 0.0125 s/iter. Inference: 0.3975 s/iter. Eval: 0.2572 s/iter. Total: 0.6675 s/iter. ETA=0:04:49
[02/24 14:54:58] mask2former INFO: Inference done 667/1092. Dataloading: 0.0125 s/iter. Inference: 0.3977 s/iter. Eval: 0.2566 s/iter. Total: 0.6671 s/iter. ETA=0:04:43
[02/24 14:55:03] mask2former INFO: Inference done 675/1092. Dataloading: 0.0126 s/iter. Inference: 0.3980 s/iter. Eval: 0.2561 s/iter. Total: 0.6670 s/iter. ETA=0:04:38
[02/24 14:55:08] mask2former INFO: Inference done 683/1092. Dataloading: 0.0127 s/iter. Inference: 0.3978 s/iter. Eval: 0.2560 s/iter. Total: 0.6668 s/iter. ETA=0:04:32
[02/24 14:55:13] mask2former INFO: Inference done 691/1092. Dataloading: 0.0127 s/iter. Inference: 0.3973 s/iter. Eval: 0.2563 s/iter. Total: 0.6667 s/iter. ETA=0:04:27
[02/24 14:55:19] mask2former INFO: Inference done 699/1092. Dataloading: 0.0127 s/iter. Inference: 0.3975 s/iter. Eval: 0.2566 s/iter. Total: 0.6672 s/iter. ETA=0:04:22
[02/24 14:55:24] mask2former INFO: Inference done 711/1092. Dataloading: 0.0126 s/iter. Inference: 0.3954 s/iter. Eval: 0.2546 s/iter. Total: 0.6630 s/iter. ETA=0:04:12
[02/24 14:55:29] mask2former INFO: Inference done 717/1092. Dataloading: 0.0127 s/iter. Inference: 0.3958 s/iter. Eval: 0.2558 s/iter. Total: 0.6646 s/iter. ETA=0:04:09
[02/24 14:55:35] mask2former INFO: Inference done 723/1092. Dataloading: 0.0127 s/iter. Inference: 0.3970 s/iter. Eval: 0.2573 s/iter. Total: 0.6674 s/iter. ETA=0:04:06
[02/24 14:55:40] mask2former INFO: Inference done 731/1092. Dataloading: 0.0126 s/iter. Inference: 0.3971 s/iter. Eval: 0.2569 s/iter. Total: 0.6670 s/iter. ETA=0:04:00
[02/24 14:55:46] mask2former INFO: Inference done 738/1092. Dataloading: 0.0126 s/iter. Inference: 0.3977 s/iter. Eval: 0.2576 s/iter. Total: 0.6683 s/iter. ETA=0:03:56
[02/24 14:55:52] mask2former INFO: Inference done 746/1092. Dataloading: 0.0126 s/iter. Inference: 0.3983 s/iter. Eval: 0.2574 s/iter. Total: 0.6687 s/iter. ETA=0:03:51
[02/24 14:55:57] mask2former INFO: Inference done 754/1092. Dataloading: 0.0126 s/iter. Inference: 0.3985 s/iter. Eval: 0.2573 s/iter. Total: 0.6688 s/iter. ETA=0:03:46
[02/24 14:56:03] mask2former INFO: Inference done 762/1092. Dataloading: 0.0126 s/iter. Inference: 0.3987 s/iter. Eval: 0.2573 s/iter. Total: 0.6690 s/iter. ETA=0:03:40
[02/24 14:56:08] mask2former INFO: Inference done 770/1092. Dataloading: 0.0126 s/iter. Inference: 0.3988 s/iter. Eval: 0.2574 s/iter. Total: 0.6692 s/iter. ETA=0:03:35
[02/24 14:56:13] mask2former INFO: Inference done 778/1092. Dataloading: 0.0126 s/iter. Inference: 0.3986 s/iter. Eval: 0.2572 s/iter. Total: 0.6688 s/iter. ETA=0:03:30
[02/24 14:56:18] mask2former INFO: Inference done 785/1092. Dataloading: 0.0126 s/iter. Inference: 0.3993 s/iter. Eval: 0.2572 s/iter. Total: 0.6695 s/iter. ETA=0:03:25
[02/24 14:56:24] mask2former INFO: Inference done 793/1092. Dataloading: 0.0126 s/iter. Inference: 0.3998 s/iter. Eval: 0.2570 s/iter. Total: 0.6698 s/iter. ETA=0:03:20
[02/24 14:56:29] mask2former INFO: Inference done 801/1092. Dataloading: 0.0126 s/iter. Inference: 0.3998 s/iter. Eval: 0.2569 s/iter. Total: 0.6696 s/iter. ETA=0:03:14
[02/24 14:56:34] mask2former INFO: Inference done 808/1092. Dataloading: 0.0126 s/iter. Inference: 0.4001 s/iter. Eval: 0.2571 s/iter. Total: 0.6702 s/iter. ETA=0:03:10
[02/24 14:56:40] mask2former INFO: Inference done 816/1092. Dataloading: 0.0126 s/iter. Inference: 0.4001 s/iter. Eval: 0.2573 s/iter. Total: 0.6703 s/iter. ETA=0:03:04
[02/24 14:56:45] mask2former INFO: Inference done 824/1092. Dataloading: 0.0126 s/iter. Inference: 0.3999 s/iter. Eval: 0.2575 s/iter. Total: 0.6703 s/iter. ETA=0:02:59
[02/24 14:56:51] mask2former INFO: Inference done 832/1092. Dataloading: 0.0125 s/iter. Inference: 0.4001 s/iter. Eval: 0.2575 s/iter. Total: 0.6705 s/iter. ETA=0:02:54
[02/24 14:56:56] mask2former INFO: Inference done 840/1092. Dataloading: 0.0125 s/iter. Inference: 0.4001 s/iter. Eval: 0.2575 s/iter. Total: 0.6704 s/iter. ETA=0:02:48
[02/24 14:57:01] mask2former INFO: Inference done 848/1092. Dataloading: 0.0125 s/iter. Inference: 0.4002 s/iter. Eval: 0.2576 s/iter. Total: 0.6707 s/iter. ETA=0:02:43
[02/24 14:57:07] mask2former INFO: Inference done 856/1092. Dataloading: 0.0125 s/iter. Inference: 0.4001 s/iter. Eval: 0.2574 s/iter. Total: 0.6703 s/iter. ETA=0:02:38
[02/24 14:57:12] mask2former INFO: Inference done 865/1092. Dataloading: 0.0124 s/iter. Inference: 0.3998 s/iter. Eval: 0.2573 s/iter. Total: 0.6699 s/iter. ETA=0:02:32
[02/24 14:57:17] mask2former INFO: Inference done 873/1092. Dataloading: 0.0124 s/iter. Inference: 0.3997 s/iter. Eval: 0.2571 s/iter. Total: 0.6696 s/iter. ETA=0:02:26
[02/24 14:57:23] mask2former INFO: Inference done 882/1092. Dataloading: 0.0124 s/iter. Inference: 0.3994 s/iter. Eval: 0.2568 s/iter. Total: 0.6688 s/iter. ETA=0:02:20
[02/24 14:57:28] mask2former INFO: Inference done 891/1092. Dataloading: 0.0123 s/iter. Inference: 0.3990 s/iter. Eval: 0.2564 s/iter. Total: 0.6680 s/iter. ETA=0:02:14
[02/24 14:57:33] mask2former INFO: Inference done 898/1092. Dataloading: 0.0123 s/iter. Inference: 0.3996 s/iter. Eval: 0.2563 s/iter. Total: 0.6686 s/iter. ETA=0:02:09
[02/24 14:57:38] mask2former INFO: Inference done 906/1092. Dataloading: 0.0123 s/iter. Inference: 0.3996 s/iter. Eval: 0.2562 s/iter. Total: 0.6685 s/iter. ETA=0:02:04
[02/24 14:57:44] mask2former INFO: Inference done 914/1092. Dataloading: 0.0123 s/iter. Inference: 0.3998 s/iter. Eval: 0.2564 s/iter. Total: 0.6688 s/iter. ETA=0:01:59
[02/24 14:57:49] mask2former INFO: Inference done 922/1092. Dataloading: 0.0123 s/iter. Inference: 0.3997 s/iter. Eval: 0.2564 s/iter. Total: 0.6687 s/iter. ETA=0:01:53
[02/24 14:57:55] mask2former INFO: Inference done 930/1092. Dataloading: 0.0123 s/iter. Inference: 0.3997 s/iter. Eval: 0.2563 s/iter. Total: 0.6686 s/iter. ETA=0:01:48
[02/24 14:58:00] mask2former INFO: Inference done 938/1092. Dataloading: 0.0123 s/iter. Inference: 0.3996 s/iter. Eval: 0.2562 s/iter. Total: 0.6684 s/iter. ETA=0:01:42
[02/24 14:58:05] mask2former INFO: Inference done 946/1092. Dataloading: 0.0122 s/iter. Inference: 0.3995 s/iter. Eval: 0.2565 s/iter. Total: 0.6686 s/iter. ETA=0:01:37
[02/24 14:58:11] mask2former INFO: Inference done 954/1092. Dataloading: 0.0123 s/iter. Inference: 0.3995 s/iter. Eval: 0.2564 s/iter. Total: 0.6685 s/iter. ETA=0:01:32
[02/24 14:58:16] mask2former INFO: Inference done 961/1092. Dataloading: 0.0123 s/iter. Inference: 0.3998 s/iter. Eval: 0.2565 s/iter. Total: 0.6688 s/iter. ETA=0:01:27
[02/24 14:58:21] mask2former INFO: Inference done 969/1092. Dataloading: 0.0122 s/iter. Inference: 0.3998 s/iter. Eval: 0.2569 s/iter. Total: 0.6693 s/iter. ETA=0:01:22
[02/24 14:58:26] mask2former INFO: Inference done 977/1092. Dataloading: 0.0122 s/iter. Inference: 0.3997 s/iter. Eval: 0.2568 s/iter. Total: 0.6691 s/iter. ETA=0:01:16
[02/24 14:58:32] mask2former INFO: Inference done 985/1092. Dataloading: 0.0122 s/iter. Inference: 0.3993 s/iter. Eval: 0.2570 s/iter. Total: 0.6689 s/iter. ETA=0:01:11
[02/24 14:58:37] mask2former INFO: Inference done 992/1092. Dataloading: 0.0123 s/iter. Inference: 0.3997 s/iter. Eval: 0.2569 s/iter. Total: 0.6692 s/iter. ETA=0:01:06
[02/24 14:58:42] mask2former INFO: Inference done 1000/1092. Dataloading: 0.0123 s/iter. Inference: 0.3996 s/iter. Eval: 0.2568 s/iter. Total: 0.6690 s/iter. ETA=0:01:01
[02/24 14:58:47] mask2former INFO: Inference done 1008/1092. Dataloading: 0.0123 s/iter. Inference: 0.3996 s/iter. Eval: 0.2567 s/iter. Total: 0.6689 s/iter. ETA=0:00:56
[02/24 14:58:52] mask2former INFO: Inference done 1016/1092. Dataloading: 0.0123 s/iter. Inference: 0.3997 s/iter. Eval: 0.2566 s/iter. Total: 0.6689 s/iter. ETA=0:00:50
[02/24 14:58:58] mask2former INFO: Inference done 1024/1092. Dataloading: 0.0123 s/iter. Inference: 0.3997 s/iter. Eval: 0.2565 s/iter. Total: 0.6689 s/iter. ETA=0:00:45
[02/24 14:59:03] mask2former INFO: Inference done 1032/1092. Dataloading: 0.0123 s/iter. Inference: 0.3998 s/iter. Eval: 0.2563 s/iter. Total: 0.6687 s/iter. ETA=0:00:40
[02/24 14:59:08] mask2former INFO: Inference done 1040/1092. Dataloading: 0.0123 s/iter. Inference: 0.3997 s/iter. Eval: 0.2565 s/iter. Total: 0.6688 s/iter. ETA=0:00:34
[02/24 14:59:14] mask2former INFO: Inference done 1048/1092. Dataloading: 0.0123 s/iter. Inference: 0.3996 s/iter. Eval: 0.2565 s/iter. Total: 0.6687 s/iter. ETA=0:00:29
[02/24 14:59:19] mask2former INFO: Inference done 1056/1092. Dataloading: 0.0123 s/iter. Inference: 0.3998 s/iter. Eval: 0.2563 s/iter. Total: 0.6687 s/iter. ETA=0:00:24
[02/24 14:59:24] mask2former INFO: Inference done 1064/1092. Dataloading: 0.0123 s/iter. Inference: 0.3996 s/iter. Eval: 0.2565 s/iter. Total: 0.6687 s/iter. ETA=0:00:18
[02/24 14:59:29] mask2former INFO: Inference done 1072/1092. Dataloading: 0.0123 s/iter. Inference: 0.3994 s/iter. Eval: 0.2566 s/iter. Total: 0.6686 s/iter. ETA=0:00:13
[02/24 14:59:35] mask2former INFO: Inference done 1080/1092. Dataloading: 0.0123 s/iter. Inference: 0.3995 s/iter. Eval: 0.2565 s/iter. Total: 0.6687 s/iter. ETA=0:00:08
[02/24 14:59:40] mask2former INFO: Inference done 1088/1092. Dataloading: 0.0123 s/iter. Inference: 0.3994 s/iter. Eval: 0.2564 s/iter. Total: 0.6684 s/iter. ETA=0:00:02
[02/24 16:20:44] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 16:20:46] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 16:20:46] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 16:21:38] mask2former INFO: Inference done 11/1092. Dataloading: 0.0265 s/iter. Inference: 0.8923 s/iter. Eval: 0.6308 s/iter. Total: 1.5496 s/iter. ETA=0:27:55
[02/24 16:21:44] mask2former INFO: Inference done 15/1092. Dataloading: 0.0245 s/iter. Inference: 0.8941 s/iter. Eval: 0.5953 s/iter. Total: 1.5180 s/iter. ETA=0:27:14
[02/24 16:21:50] mask2former INFO: Inference done 19/1092. Dataloading: 0.0240 s/iter. Inference: 0.8730 s/iter. Eval: 0.5783 s/iter. Total: 1.4784 s/iter. ETA=0:26:26
[02/24 16:21:55] mask2former INFO: Inference done 23/1092. Dataloading: 0.0236 s/iter. Inference: 0.8379 s/iter. Eval: 0.5908 s/iter. Total: 1.4547 s/iter. ETA=0:25:55
[02/24 16:22:01] mask2former INFO: Inference done 27/1092. Dataloading: 0.0249 s/iter. Inference: 0.8420 s/iter. Eval: 0.5912 s/iter. Total: 1.4601 s/iter. ETA=0:25:54
[02/24 16:22:07] mask2former INFO: Inference done 32/1092. Dataloading: 0.0241 s/iter. Inference: 0.8133 s/iter. Eval: 0.5695 s/iter. Total: 1.4090 s/iter. ETA=0:24:53
[02/24 16:22:13] mask2former INFO: Inference done 36/1092. Dataloading: 0.0231 s/iter. Inference: 0.8172 s/iter. Eval: 0.5627 s/iter. Total: 1.4050 s/iter. ETA=0:24:43
[02/24 16:22:18] mask2former INFO: Inference done 39/1092. Dataloading: 0.0240 s/iter. Inference: 0.8327 s/iter. Eval: 0.5702 s/iter. Total: 1.4287 s/iter. ETA=0:25:04
[02/24 16:22:23] mask2former INFO: Inference done 43/1092. Dataloading: 0.0241 s/iter. Inference: 0.8271 s/iter. Eval: 0.5704 s/iter. Total: 1.4233 s/iter. ETA=0:24:53
[02/24 16:22:28] mask2former INFO: Inference done 47/1092. Dataloading: 0.0235 s/iter. Inference: 0.8101 s/iter. Eval: 0.5778 s/iter. Total: 1.4130 s/iter. ETA=0:24:36
[02/24 16:22:34] mask2former INFO: Inference done 51/1092. Dataloading: 0.0233 s/iter. Inference: 0.8119 s/iter. Eval: 0.5750 s/iter. Total: 1.4117 s/iter. ETA=0:24:29
[02/24 16:22:40] mask2former INFO: Inference done 56/1092. Dataloading: 0.0225 s/iter. Inference: 0.8001 s/iter. Eval: 0.5593 s/iter. Total: 1.3833 s/iter. ETA=0:23:53
[02/24 16:22:45] mask2former INFO: Inference done 60/1092. Dataloading: 0.0223 s/iter. Inference: 0.8005 s/iter. Eval: 0.5588 s/iter. Total: 1.3828 s/iter. ETA=0:23:47
[02/24 16:22:51] mask2former INFO: Inference done 64/1092. Dataloading: 0.0231 s/iter. Inference: 0.8039 s/iter. Eval: 0.5622 s/iter. Total: 1.3906 s/iter. ETA=0:23:49
[02/24 16:22:57] mask2former INFO: Inference done 68/1092. Dataloading: 0.0228 s/iter. Inference: 0.8006 s/iter. Eval: 0.5652 s/iter. Total: 1.3898 s/iter. ETA=0:23:43
[02/24 16:23:02] mask2former INFO: Inference done 72/1092. Dataloading: 0.0230 s/iter. Inference: 0.8002 s/iter. Eval: 0.5638 s/iter. Total: 1.3882 s/iter. ETA=0:23:35
[02/24 16:23:08] mask2former INFO: Inference done 76/1092. Dataloading: 0.0227 s/iter. Inference: 0.8035 s/iter. Eval: 0.5719 s/iter. Total: 1.3993 s/iter. ETA=0:23:41
[02/24 16:23:14] mask2former INFO: Inference done 79/1092. Dataloading: 0.0232 s/iter. Inference: 0.8134 s/iter. Eval: 0.5769 s/iter. Total: 1.4147 s/iter. ETA=0:23:53
[02/24 16:23:19] mask2former INFO: Inference done 82/1092. Dataloading: 0.0234 s/iter. Inference: 0.8086 s/iter. Eval: 0.5922 s/iter. Total: 1.4253 s/iter. ETA=0:23:59
[02/24 16:23:25] mask2former INFO: Inference done 86/1092. Dataloading: 0.0235 s/iter. Inference: 0.8033 s/iter. Eval: 0.5984 s/iter. Total: 1.4265 s/iter. ETA=0:23:55
[02/24 16:23:30] mask2former INFO: Inference done 90/1092. Dataloading: 0.0236 s/iter. Inference: 0.8042 s/iter. Eval: 0.5942 s/iter. Total: 1.4234 s/iter. ETA=0:23:46
[02/24 16:23:36] mask2former INFO: Inference done 95/1092. Dataloading: 0.0229 s/iter. Inference: 0.7988 s/iter. Eval: 0.5928 s/iter. Total: 1.4158 s/iter. ETA=0:23:31
[02/24 16:23:43] mask2former INFO: Inference done 100/1092. Dataloading: 0.0230 s/iter. Inference: 0.7953 s/iter. Eval: 0.5860 s/iter. Total: 1.4057 s/iter. ETA=0:23:14
[02/24 16:23:49] mask2former INFO: Inference done 104/1092. Dataloading: 0.0230 s/iter. Inference: 0.8006 s/iter. Eval: 0.5874 s/iter. Total: 1.4124 s/iter. ETA=0:23:15
[02/24 16:23:54] mask2former INFO: Inference done 109/1092. Dataloading: 0.0227 s/iter. Inference: 0.7973 s/iter. Eval: 0.5765 s/iter. Total: 1.3977 s/iter. ETA=0:22:53
[02/24 16:24:00] mask2former INFO: Inference done 114/1092. Dataloading: 0.0223 s/iter. Inference: 0.7942 s/iter. Eval: 0.5689 s/iter. Total: 1.3866 s/iter. ETA=0:22:36
[02/24 16:24:06] mask2former INFO: Inference done 118/1092. Dataloading: 0.0226 s/iter. Inference: 0.7917 s/iter. Eval: 0.5737 s/iter. Total: 1.3892 s/iter. ETA=0:22:33
[02/24 16:24:12] mask2former INFO: Inference done 122/1092. Dataloading: 0.0228 s/iter. Inference: 0.7974 s/iter. Eval: 0.5749 s/iter. Total: 1.3963 s/iter. ETA=0:22:34
[02/24 16:24:18] mask2former INFO: Inference done 126/1092. Dataloading: 0.0228 s/iter. Inference: 0.7939 s/iter. Eval: 0.5760 s/iter. Total: 1.3940 s/iter. ETA=0:22:26
[02/24 16:24:24] mask2former INFO: Inference done 131/1092. Dataloading: 0.0232 s/iter. Inference: 0.7922 s/iter. Eval: 0.5725 s/iter. Total: 1.3890 s/iter. ETA=0:22:14
[02/24 16:24:29] mask2former INFO: Inference done 135/1092. Dataloading: 0.0232 s/iter. Inference: 0.7894 s/iter. Eval: 0.5716 s/iter. Total: 1.3852 s/iter. ETA=0:22:05
[02/24 16:24:36] mask2former INFO: Inference done 139/1092. Dataloading: 0.0230 s/iter. Inference: 0.7931 s/iter. Eval: 0.5744 s/iter. Total: 1.3917 s/iter. ETA=0:22:06
[02/24 16:24:41] mask2former INFO: Inference done 143/1092. Dataloading: 0.0230 s/iter. Inference: 0.7927 s/iter. Eval: 0.5739 s/iter. Total: 1.3909 s/iter. ETA=0:21:59
[02/24 16:24:47] mask2former INFO: Inference done 147/1092. Dataloading: 0.0228 s/iter. Inference: 0.7975 s/iter. Eval: 0.5712 s/iter. Total: 1.3927 s/iter. ETA=0:21:56
[02/24 16:24:52] mask2former INFO: Inference done 151/1092. Dataloading: 0.0228 s/iter. Inference: 0.7977 s/iter. Eval: 0.5698 s/iter. Total: 1.3916 s/iter. ETA=0:21:49
[02/24 16:24:58] mask2former INFO: Inference done 155/1092. Dataloading: 0.0229 s/iter. Inference: 0.7967 s/iter. Eval: 0.5695 s/iter. Total: 1.3903 s/iter. ETA=0:21:42
[02/24 16:25:03] mask2former INFO: Inference done 158/1092. Dataloading: 0.0231 s/iter. Inference: 0.8006 s/iter. Eval: 0.5717 s/iter. Total: 1.3967 s/iter. ETA=0:21:44
[02/24 16:25:08] mask2former INFO: Inference done 162/1092. Dataloading: 0.0233 s/iter. Inference: 0.8025 s/iter. Eval: 0.5699 s/iter. Total: 1.3969 s/iter. ETA=0:21:39
[02/24 16:25:13] mask2former INFO: Inference done 165/1092. Dataloading: 0.0233 s/iter. Inference: 0.8079 s/iter. Eval: 0.5703 s/iter. Total: 1.4026 s/iter. ETA=0:21:40
[02/24 16:25:19] mask2former INFO: Inference done 169/1092. Dataloading: 0.0234 s/iter. Inference: 0.8067 s/iter. Eval: 0.5714 s/iter. Total: 1.4027 s/iter. ETA=0:21:34
[02/24 16:25:25] mask2former INFO: Inference done 173/1092. Dataloading: 0.0234 s/iter. Inference: 0.8058 s/iter. Eval: 0.5710 s/iter. Total: 1.4016 s/iter. ETA=0:21:28
[02/24 16:25:30] mask2former INFO: Inference done 177/1092. Dataloading: 0.0239 s/iter. Inference: 0.8024 s/iter. Eval: 0.5713 s/iter. Total: 1.3990 s/iter. ETA=0:21:20
[02/24 16:25:37] mask2former INFO: Inference done 182/1092. Dataloading: 0.0240 s/iter. Inference: 0.8036 s/iter. Eval: 0.5705 s/iter. Total: 1.3994 s/iter. ETA=0:21:13
[02/24 16:25:42] mask2former INFO: Inference done 186/1092. Dataloading: 0.0242 s/iter. Inference: 0.8014 s/iter. Eval: 0.5701 s/iter. Total: 1.3970 s/iter. ETA=0:21:05
[02/24 16:25:48] mask2former INFO: Inference done 191/1092. Dataloading: 0.0239 s/iter. Inference: 0.7976 s/iter. Eval: 0.5683 s/iter. Total: 1.3911 s/iter. ETA=0:20:53
[02/24 16:25:54] mask2former INFO: Inference done 197/1092. Dataloading: 0.0239 s/iter. Inference: 0.7922 s/iter. Eval: 0.5650 s/iter. Total: 1.3823 s/iter. ETA=0:20:37
[02/24 16:26:01] mask2former INFO: Inference done 202/1092. Dataloading: 0.0236 s/iter. Inference: 0.7886 s/iter. Eval: 0.5648 s/iter. Total: 1.3783 s/iter. ETA=0:20:26
[02/24 16:26:06] mask2former INFO: Inference done 206/1092. Dataloading: 0.0237 s/iter. Inference: 0.7836 s/iter. Eval: 0.5673 s/iter. Total: 1.3759 s/iter. ETA=0:20:19
[02/24 16:26:12] mask2former INFO: Inference done 211/1092. Dataloading: 0.0236 s/iter. Inference: 0.7837 s/iter. Eval: 0.5639 s/iter. Total: 1.3726 s/iter. ETA=0:20:09
[02/24 16:26:17] mask2former INFO: Inference done 215/1092. Dataloading: 0.0236 s/iter. Inference: 0.7806 s/iter. Eval: 0.5652 s/iter. Total: 1.3708 s/iter. ETA=0:20:02
[02/24 16:26:23] mask2former INFO: Inference done 219/1092. Dataloading: 0.0235 s/iter. Inference: 0.7828 s/iter. Eval: 0.5643 s/iter. Total: 1.3719 s/iter. ETA=0:19:57
[02/24 16:26:28] mask2former INFO: Inference done 223/1092. Dataloading: 0.0236 s/iter. Inference: 0.7828 s/iter. Eval: 0.5623 s/iter. Total: 1.3700 s/iter. ETA=0:19:50
[02/24 16:26:34] mask2former INFO: Inference done 227/1092. Dataloading: 0.0235 s/iter. Inference: 0.7826 s/iter. Eval: 0.5653 s/iter. Total: 1.3727 s/iter. ETA=0:19:47
[02/24 16:26:40] mask2former INFO: Inference done 231/1092. Dataloading: 0.0238 s/iter. Inference: 0.7819 s/iter. Eval: 0.5672 s/iter. Total: 1.3742 s/iter. ETA=0:19:43
[02/24 16:26:45] mask2former INFO: Inference done 235/1092. Dataloading: 0.0238 s/iter. Inference: 0.7818 s/iter. Eval: 0.5676 s/iter. Total: 1.3746 s/iter. ETA=0:19:38
[02/24 16:26:51] mask2former INFO: Inference done 239/1092. Dataloading: 0.0239 s/iter. Inference: 0.7852 s/iter. Eval: 0.5669 s/iter. Total: 1.3774 s/iter. ETA=0:19:34
[02/24 16:26:57] mask2former INFO: Inference done 243/1092. Dataloading: 0.0245 s/iter. Inference: 0.7851 s/iter. Eval: 0.5668 s/iter. Total: 1.3779 s/iter. ETA=0:19:29
[02/24 16:27:03] mask2former INFO: Inference done 248/1092. Dataloading: 0.0245 s/iter. Inference: 0.7859 s/iter. Eval: 0.5638 s/iter. Total: 1.3756 s/iter. ETA=0:19:21
[02/24 16:27:08] mask2former INFO: Inference done 251/1092. Dataloading: 0.0245 s/iter. Inference: 0.7872 s/iter. Eval: 0.5667 s/iter. Total: 1.3799 s/iter. ETA=0:19:20
[02/24 16:27:14] mask2former INFO: Inference done 255/1092. Dataloading: 0.0246 s/iter. Inference: 0.7876 s/iter. Eval: 0.5653 s/iter. Total: 1.3790 s/iter. ETA=0:19:14
[02/24 16:27:20] mask2former INFO: Inference done 259/1092. Dataloading: 0.0247 s/iter. Inference: 0.7867 s/iter. Eval: 0.5687 s/iter. Total: 1.3815 s/iter. ETA=0:19:10
[02/24 16:27:26] mask2former INFO: Inference done 263/1092. Dataloading: 0.0246 s/iter. Inference: 0.7870 s/iter. Eval: 0.5701 s/iter. Total: 1.3831 s/iter. ETA=0:19:06
[02/24 16:27:31] mask2former INFO: Inference done 267/1092. Dataloading: 0.0247 s/iter. Inference: 0.7841 s/iter. Eval: 0.5722 s/iter. Total: 1.3824 s/iter. ETA=0:19:00
[02/24 16:27:37] mask2former INFO: Inference done 271/1092. Dataloading: 0.0249 s/iter. Inference: 0.7835 s/iter. Eval: 0.5729 s/iter. Total: 1.3827 s/iter. ETA=0:18:55
[02/24 16:27:42] mask2former INFO: Inference done 275/1092. Dataloading: 0.0250 s/iter. Inference: 0.7830 s/iter. Eval: 0.5723 s/iter. Total: 1.3818 s/iter. ETA=0:18:48
[02/24 16:27:48] mask2former INFO: Inference done 279/1092. Dataloading: 0.0250 s/iter. Inference: 0.7832 s/iter. Eval: 0.5743 s/iter. Total: 1.3839 s/iter. ETA=0:18:45
[02/24 16:27:54] mask2former INFO: Inference done 283/1092. Dataloading: 0.0249 s/iter. Inference: 0.7828 s/iter. Eval: 0.5745 s/iter. Total: 1.3836 s/iter. ETA=0:18:39
[02/24 16:27:59] mask2former INFO: Inference done 287/1092. Dataloading: 0.0249 s/iter. Inference: 0.7806 s/iter. Eval: 0.5756 s/iter. Total: 1.3825 s/iter. ETA=0:18:32
[02/24 16:28:04] mask2former INFO: Inference done 291/1092. Dataloading: 0.0252 s/iter. Inference: 0.7790 s/iter. Eval: 0.5756 s/iter. Total: 1.3812 s/iter. ETA=0:18:26
[02/24 16:28:10] mask2former INFO: Inference done 295/1092. Dataloading: 0.0251 s/iter. Inference: 0.7801 s/iter. Eval: 0.5753 s/iter. Total: 1.3819 s/iter. ETA=0:18:21
[02/24 16:28:16] mask2former INFO: Inference done 299/1092. Dataloading: 0.0251 s/iter. Inference: 0.7841 s/iter. Eval: 0.5729 s/iter. Total: 1.3836 s/iter. ETA=0:18:17
[02/24 16:28:22] mask2former INFO: Inference done 303/1092. Dataloading: 0.0251 s/iter. Inference: 0.7834 s/iter. Eval: 0.5749 s/iter. Total: 1.3848 s/iter. ETA=0:18:12
[02/24 16:28:28] mask2former INFO: Inference done 307/1092. Dataloading: 0.0251 s/iter. Inference: 0.7847 s/iter. Eval: 0.5754 s/iter. Total: 1.3867 s/iter. ETA=0:18:08
[02/24 16:28:33] mask2former INFO: Inference done 310/1092. Dataloading: 0.0252 s/iter. Inference: 0.7868 s/iter. Eval: 0.5766 s/iter. Total: 1.3900 s/iter. ETA=0:18:06
[02/24 16:28:38] mask2former INFO: Inference done 314/1092. Dataloading: 0.0251 s/iter. Inference: 0.7856 s/iter. Eval: 0.5764 s/iter. Total: 1.3884 s/iter. ETA=0:18:00
[02/24 16:28:44] mask2former INFO: Inference done 317/1092. Dataloading: 0.0251 s/iter. Inference: 0.7881 s/iter. Eval: 0.5780 s/iter. Total: 1.3926 s/iter. ETA=0:17:59
[02/24 16:28:49] mask2former INFO: Inference done 321/1092. Dataloading: 0.0250 s/iter. Inference: 0.7877 s/iter. Eval: 0.5773 s/iter. Total: 1.3914 s/iter. ETA=0:17:52
[02/24 16:28:54] mask2former INFO: Inference done 324/1092. Dataloading: 0.0250 s/iter. Inference: 0.7896 s/iter. Eval: 0.5791 s/iter. Total: 1.3951 s/iter. ETA=0:17:51
[02/24 16:29:00] mask2former INFO: Inference done 328/1092. Dataloading: 0.0250 s/iter. Inference: 0.7895 s/iter. Eval: 0.5800 s/iter. Total: 1.3959 s/iter. ETA=0:17:46
[02/24 16:29:06] mask2former INFO: Inference done 332/1092. Dataloading: 0.0249 s/iter. Inference: 0.7903 s/iter. Eval: 0.5796 s/iter. Total: 1.3962 s/iter. ETA=0:17:41
[02/24 16:29:11] mask2former INFO: Inference done 336/1092. Dataloading: 0.0250 s/iter. Inference: 0.7902 s/iter. Eval: 0.5780 s/iter. Total: 1.3945 s/iter. ETA=0:17:34
[02/24 16:29:17] mask2former INFO: Inference done 340/1092. Dataloading: 0.0251 s/iter. Inference: 0.7905 s/iter. Eval: 0.5805 s/iter. Total: 1.3974 s/iter. ETA=0:17:30
[02/24 16:29:23] mask2former INFO: Inference done 343/1092. Dataloading: 0.0255 s/iter. Inference: 0.7926 s/iter. Eval: 0.5816 s/iter. Total: 1.4011 s/iter. ETA=0:17:29
[02/24 16:29:28] mask2former INFO: Inference done 346/1092. Dataloading: 0.0256 s/iter. Inference: 0.7938 s/iter. Eval: 0.5833 s/iter. Total: 1.4040 s/iter. ETA=0:17:27
[02/24 16:29:33] mask2former INFO: Inference done 349/1092. Dataloading: 0.0255 s/iter. Inference: 0.7956 s/iter. Eval: 0.5845 s/iter. Total: 1.4069 s/iter. ETA=0:17:25
[02/24 16:29:39] mask2former INFO: Inference done 353/1092. Dataloading: 0.0254 s/iter. Inference: 0.7955 s/iter. Eval: 0.5863 s/iter. Total: 1.4085 s/iter. ETA=0:17:20
[02/24 16:29:45] mask2former INFO: Inference done 357/1092. Dataloading: 0.0255 s/iter. Inference: 0.7958 s/iter. Eval: 0.5853 s/iter. Total: 1.4078 s/iter. ETA=0:17:14
[02/24 16:29:50] mask2former INFO: Inference done 360/1092. Dataloading: 0.0255 s/iter. Inference: 0.7976 s/iter. Eval: 0.5861 s/iter. Total: 1.4104 s/iter. ETA=0:17:12
[02/24 16:29:55] mask2former INFO: Inference done 364/1092. Dataloading: 0.0257 s/iter. Inference: 0.7962 s/iter. Eval: 0.5872 s/iter. Total: 1.4103 s/iter. ETA=0:17:06
[02/24 16:30:01] mask2former INFO: Inference done 367/1092. Dataloading: 0.0257 s/iter. Inference: 0.7983 s/iter. Eval: 0.5890 s/iter. Total: 1.4143 s/iter. ETA=0:17:05
[02/24 16:30:07] mask2former INFO: Inference done 370/1092. Dataloading: 0.0258 s/iter. Inference: 0.8013 s/iter. Eval: 0.5906 s/iter. Total: 1.4189 s/iter. ETA=0:17:04
[02/24 16:30:13] mask2former INFO: Inference done 374/1092. Dataloading: 0.0258 s/iter. Inference: 0.8025 s/iter. Eval: 0.5896 s/iter. Total: 1.4192 s/iter. ETA=0:16:58
[02/24 16:30:19] mask2former INFO: Inference done 378/1092. Dataloading: 0.0259 s/iter. Inference: 0.8025 s/iter. Eval: 0.5898 s/iter. Total: 1.4195 s/iter. ETA=0:16:53
[02/24 16:30:24] mask2former INFO: Inference done 382/1092. Dataloading: 0.0260 s/iter. Inference: 0.8028 s/iter. Eval: 0.5882 s/iter. Total: 1.4183 s/iter. ETA=0:16:46
[02/24 16:30:30] mask2former INFO: Inference done 386/1092. Dataloading: 0.0262 s/iter. Inference: 0.8035 s/iter. Eval: 0.5881 s/iter. Total: 1.4192 s/iter. ETA=0:16:41
[02/24 16:30:35] mask2former INFO: Inference done 390/1092. Dataloading: 0.0262 s/iter. Inference: 0.8037 s/iter. Eval: 0.5868 s/iter. Total: 1.4180 s/iter. ETA=0:16:35
[02/24 16:30:41] mask2former INFO: Inference done 394/1092. Dataloading: 0.0262 s/iter. Inference: 0.8052 s/iter. Eval: 0.5870 s/iter. Total: 1.4197 s/iter. ETA=0:16:30
[02/24 16:30:46] mask2former INFO: Inference done 398/1092. Dataloading: 0.0261 s/iter. Inference: 0.8038 s/iter. Eval: 0.5868 s/iter. Total: 1.4180 s/iter. ETA=0:16:24
[02/24 16:30:52] mask2former INFO: Inference done 402/1092. Dataloading: 0.0261 s/iter. Inference: 0.8050 s/iter. Eval: 0.5867 s/iter. Total: 1.4192 s/iter. ETA=0:16:19
[02/24 16:30:59] mask2former INFO: Inference done 406/1092. Dataloading: 0.0261 s/iter. Inference: 0.8058 s/iter. Eval: 0.5870 s/iter. Total: 1.4203 s/iter. ETA=0:16:14
[02/24 16:31:04] mask2former INFO: Inference done 411/1092. Dataloading: 0.0261 s/iter. Inference: 0.8044 s/iter. Eval: 0.5850 s/iter. Total: 1.4168 s/iter. ETA=0:16:04
[02/24 16:31:11] mask2former INFO: Inference done 415/1092. Dataloading: 0.0262 s/iter. Inference: 0.8044 s/iter. Eval: 0.5865 s/iter. Total: 1.4184 s/iter. ETA=0:16:00
[02/24 16:31:16] mask2former INFO: Inference done 419/1092. Dataloading: 0.0261 s/iter. Inference: 0.8043 s/iter. Eval: 0.5868 s/iter. Total: 1.4185 s/iter. ETA=0:15:54
[02/24 16:31:22] mask2former INFO: Inference done 423/1092. Dataloading: 0.0261 s/iter. Inference: 0.8050 s/iter. Eval: 0.5857 s/iter. Total: 1.4181 s/iter. ETA=0:15:48
[02/24 16:31:28] mask2former INFO: Inference done 427/1092. Dataloading: 0.0261 s/iter. Inference: 0.8060 s/iter. Eval: 0.5864 s/iter. Total: 1.4198 s/iter. ETA=0:15:44
[02/24 16:31:34] mask2former INFO: Inference done 431/1092. Dataloading: 0.0262 s/iter. Inference: 0.8058 s/iter. Eval: 0.5867 s/iter. Total: 1.4200 s/iter. ETA=0:15:38
[02/24 16:31:39] mask2former INFO: Inference done 435/1092. Dataloading: 0.0262 s/iter. Inference: 0.8046 s/iter. Eval: 0.5868 s/iter. Total: 1.4188 s/iter. ETA=0:15:32
[02/24 16:31:45] mask2former INFO: Inference done 439/1092. Dataloading: 0.0262 s/iter. Inference: 0.8041 s/iter. Eval: 0.5866 s/iter. Total: 1.4181 s/iter. ETA=0:15:26
[02/24 16:31:50] mask2former INFO: Inference done 443/1092. Dataloading: 0.0262 s/iter. Inference: 0.8028 s/iter. Eval: 0.5871 s/iter. Total: 1.4174 s/iter. ETA=0:15:19
[02/24 16:31:56] mask2former INFO: Inference done 447/1092. Dataloading: 0.0261 s/iter. Inference: 0.8033 s/iter. Eval: 0.5870 s/iter. Total: 1.4177 s/iter. ETA=0:15:14
[02/24 16:32:01] mask2former INFO: Inference done 451/1092. Dataloading: 0.0263 s/iter. Inference: 0.8036 s/iter. Eval: 0.5858 s/iter. Total: 1.4170 s/iter. ETA=0:15:08
[02/24 16:32:07] mask2former INFO: Inference done 455/1092. Dataloading: 0.0263 s/iter. Inference: 0.8046 s/iter. Eval: 0.5855 s/iter. Total: 1.4176 s/iter. ETA=0:15:03
[02/24 16:32:12] mask2former INFO: Inference done 459/1092. Dataloading: 0.0262 s/iter. Inference: 0.8047 s/iter. Eval: 0.5849 s/iter. Total: 1.4171 s/iter. ETA=0:14:57
[02/24 16:32:18] mask2former INFO: Inference done 463/1092. Dataloading: 0.0262 s/iter. Inference: 0.8046 s/iter. Eval: 0.5847 s/iter. Total: 1.4168 s/iter. ETA=0:14:51
[02/24 16:32:24] mask2former INFO: Inference done 467/1092. Dataloading: 0.0262 s/iter. Inference: 0.8042 s/iter. Eval: 0.5856 s/iter. Total: 1.4173 s/iter. ETA=0:14:45
[02/24 16:32:30] mask2former INFO: Inference done 471/1092. Dataloading: 0.0262 s/iter. Inference: 0.8055 s/iter. Eval: 0.5855 s/iter. Total: 1.4185 s/iter. ETA=0:14:40
[02/24 16:32:35] mask2former INFO: Inference done 474/1092. Dataloading: 0.0263 s/iter. Inference: 0.8069 s/iter. Eval: 0.5857 s/iter. Total: 1.4202 s/iter. ETA=0:14:37
[02/24 16:32:42] mask2former INFO: Inference done 478/1092. Dataloading: 0.0263 s/iter. Inference: 0.8082 s/iter. Eval: 0.5865 s/iter. Total: 1.4223 s/iter. ETA=0:14:33
[02/24 16:32:49] mask2former INFO: Inference done 482/1092. Dataloading: 0.0262 s/iter. Inference: 0.8084 s/iter. Eval: 0.5884 s/iter. Total: 1.4245 s/iter. ETA=0:14:28
[02/24 16:32:54] mask2former INFO: Inference done 486/1092. Dataloading: 0.0262 s/iter. Inference: 0.8086 s/iter. Eval: 0.5880 s/iter. Total: 1.4242 s/iter. ETA=0:14:23
[02/24 16:33:01] mask2former INFO: Inference done 492/1092. Dataloading: 0.0263 s/iter. Inference: 0.8060 s/iter. Eval: 0.5862 s/iter. Total: 1.4200 s/iter. ETA=0:14:11
[02/24 16:33:06] mask2former INFO: Inference done 496/1092. Dataloading: 0.0263 s/iter. Inference: 0.8066 s/iter. Eval: 0.5856 s/iter. Total: 1.4199 s/iter. ETA=0:14:06
[02/24 16:33:11] mask2former INFO: Inference done 499/1092. Dataloading: 0.0263 s/iter. Inference: 0.8075 s/iter. Eval: 0.5863 s/iter. Total: 1.4216 s/iter. ETA=0:14:02
[02/24 16:33:17] mask2former INFO: Inference done 503/1092. Dataloading: 0.0265 s/iter. Inference: 0.8078 s/iter. Eval: 0.5869 s/iter. Total: 1.4226 s/iter. ETA=0:13:57
[02/24 16:33:23] mask2former INFO: Inference done 507/1092. Dataloading: 0.0264 s/iter. Inference: 0.8077 s/iter. Eval: 0.5868 s/iter. Total: 1.4224 s/iter. ETA=0:13:52
[02/24 16:33:28] mask2former INFO: Inference done 511/1092. Dataloading: 0.0265 s/iter. Inference: 0.8073 s/iter. Eval: 0.5866 s/iter. Total: 1.4218 s/iter. ETA=0:13:46
[02/24 16:33:34] mask2former INFO: Inference done 515/1092. Dataloading: 0.0264 s/iter. Inference: 0.8064 s/iter. Eval: 0.5867 s/iter. Total: 1.4210 s/iter. ETA=0:13:39
[02/24 16:33:40] mask2former INFO: Inference done 520/1092. Dataloading: 0.0262 s/iter. Inference: 0.8054 s/iter. Eval: 0.5858 s/iter. Total: 1.4189 s/iter. ETA=0:13:31
[02/24 16:33:46] mask2former INFO: Inference done 524/1092. Dataloading: 0.0263 s/iter. Inference: 0.8061 s/iter. Eval: 0.5867 s/iter. Total: 1.4206 s/iter. ETA=0:13:26
[02/24 16:33:51] mask2former INFO: Inference done 528/1092. Dataloading: 0.0263 s/iter. Inference: 0.8049 s/iter. Eval: 0.5866 s/iter. Total: 1.4194 s/iter. ETA=0:13:20
[02/24 16:33:58] mask2former INFO: Inference done 532/1092. Dataloading: 0.0263 s/iter. Inference: 0.8061 s/iter. Eval: 0.5865 s/iter. Total: 1.4204 s/iter. ETA=0:13:15
[02/24 16:34:04] mask2former INFO: Inference done 537/1092. Dataloading: 0.0262 s/iter. Inference: 0.8054 s/iter. Eval: 0.5854 s/iter. Total: 1.4185 s/iter. ETA=0:13:07
[02/24 16:34:09] mask2former INFO: Inference done 541/1092. Dataloading: 0.0263 s/iter. Inference: 0.8050 s/iter. Eval: 0.5854 s/iter. Total: 1.4183 s/iter. ETA=0:13:01
[02/24 16:34:15] mask2former INFO: Inference done 545/1092. Dataloading: 0.0263 s/iter. Inference: 0.8051 s/iter. Eval: 0.5852 s/iter. Total: 1.4181 s/iter. ETA=0:12:55
[02/24 16:34:20] mask2former INFO: Inference done 548/1092. Dataloading: 0.0263 s/iter. Inference: 0.8055 s/iter. Eval: 0.5863 s/iter. Total: 1.4196 s/iter. ETA=0:12:52
[02/24 16:34:25] mask2former INFO: Inference done 551/1092. Dataloading: 0.0263 s/iter. Inference: 0.8059 s/iter. Eval: 0.5877 s/iter. Total: 1.4214 s/iter. ETA=0:12:48
[02/24 16:34:31] mask2former INFO: Inference done 556/1092. Dataloading: 0.0266 s/iter. Inference: 0.8043 s/iter. Eval: 0.5873 s/iter. Total: 1.4198 s/iter. ETA=0:12:41
[02/24 16:34:38] mask2former INFO: Inference done 560/1092. Dataloading: 0.0269 s/iter. Inference: 0.8051 s/iter. Eval: 0.5888 s/iter. Total: 1.4224 s/iter. ETA=0:12:36
[02/24 16:34:44] mask2former INFO: Inference done 564/1092. Dataloading: 0.0270 s/iter. Inference: 0.8057 s/iter. Eval: 0.5879 s/iter. Total: 1.4222 s/iter. ETA=0:12:30
[02/24 16:34:50] mask2former INFO: Inference done 568/1092. Dataloading: 0.0269 s/iter. Inference: 0.8067 s/iter. Eval: 0.5878 s/iter. Total: 1.4230 s/iter. ETA=0:12:25
[02/24 16:34:55] mask2former INFO: Inference done 572/1092. Dataloading: 0.0269 s/iter. Inference: 0.8061 s/iter. Eval: 0.5873 s/iter. Total: 1.4218 s/iter. ETA=0:12:19
[02/24 16:35:01] mask2former INFO: Inference done 576/1092. Dataloading: 0.0270 s/iter. Inference: 0.8064 s/iter. Eval: 0.5874 s/iter. Total: 1.4224 s/iter. ETA=0:12:13
[02/24 16:35:06] mask2former INFO: Inference done 580/1092. Dataloading: 0.0271 s/iter. Inference: 0.8053 s/iter. Eval: 0.5873 s/iter. Total: 1.4213 s/iter. ETA=0:12:07
[02/24 16:35:11] mask2former INFO: Inference done 584/1092. Dataloading: 0.0270 s/iter. Inference: 0.8054 s/iter. Eval: 0.5864 s/iter. Total: 1.4204 s/iter. ETA=0:12:01
[02/24 16:35:19] mask2former INFO: Inference done 589/1092. Dataloading: 0.0269 s/iter. Inference: 0.8057 s/iter. Eval: 0.5864 s/iter. Total: 1.4205 s/iter. ETA=0:11:54
[02/24 16:35:24] mask2former INFO: Inference done 593/1092. Dataloading: 0.0268 s/iter. Inference: 0.8048 s/iter. Eval: 0.5870 s/iter. Total: 1.4202 s/iter. ETA=0:11:48
[02/24 16:35:29] mask2former INFO: Inference done 597/1092. Dataloading: 0.0268 s/iter. Inference: 0.8043 s/iter. Eval: 0.5864 s/iter. Total: 1.4191 s/iter. ETA=0:11:42
[02/24 16:35:34] mask2former INFO: Inference done 600/1092. Dataloading: 0.0268 s/iter. Inference: 0.8051 s/iter. Eval: 0.5870 s/iter. Total: 1.4205 s/iter. ETA=0:11:38
[02/24 16:35:40] mask2former INFO: Inference done 604/1092. Dataloading: 0.0269 s/iter. Inference: 0.8056 s/iter. Eval: 0.5870 s/iter. Total: 1.4211 s/iter. ETA=0:11:33
[02/24 16:35:45] mask2former INFO: Inference done 607/1092. Dataloading: 0.0269 s/iter. Inference: 0.8066 s/iter. Eval: 0.5874 s/iter. Total: 1.4224 s/iter. ETA=0:11:29
[02/24 16:35:51] mask2former INFO: Inference done 611/1092. Dataloading: 0.0269 s/iter. Inference: 0.8066 s/iter. Eval: 0.5879 s/iter. Total: 1.4229 s/iter. ETA=0:11:24
[02/24 16:35:57] mask2former INFO: Inference done 615/1092. Dataloading: 0.0271 s/iter. Inference: 0.8070 s/iter. Eval: 0.5880 s/iter. Total: 1.4236 s/iter. ETA=0:11:19
[02/24 16:36:03] mask2former INFO: Inference done 618/1092. Dataloading: 0.0271 s/iter. Inference: 0.8075 s/iter. Eval: 0.5887 s/iter. Total: 1.4249 s/iter. ETA=0:11:15
[02/24 16:36:08] mask2former INFO: Inference done 622/1092. Dataloading: 0.0271 s/iter. Inference: 0.8070 s/iter. Eval: 0.5882 s/iter. Total: 1.4239 s/iter. ETA=0:11:09
[02/24 16:36:13] mask2former INFO: Inference done 626/1092. Dataloading: 0.0271 s/iter. Inference: 0.8061 s/iter. Eval: 0.5886 s/iter. Total: 1.4233 s/iter. ETA=0:11:03
[02/24 16:36:19] mask2former INFO: Inference done 630/1092. Dataloading: 0.0270 s/iter. Inference: 0.8060 s/iter. Eval: 0.5889 s/iter. Total: 1.4234 s/iter. ETA=0:10:57
[02/24 16:36:24] mask2former INFO: Inference done 634/1092. Dataloading: 0.0269 s/iter. Inference: 0.8063 s/iter. Eval: 0.5887 s/iter. Total: 1.4234 s/iter. ETA=0:10:51
[02/24 16:36:29] mask2former INFO: Inference done 637/1092. Dataloading: 0.0269 s/iter. Inference: 0.8069 s/iter. Eval: 0.5893 s/iter. Total: 1.4246 s/iter. ETA=0:10:48
[02/24 16:36:36] mask2former INFO: Inference done 641/1092. Dataloading: 0.0270 s/iter. Inference: 0.8072 s/iter. Eval: 0.5897 s/iter. Total: 1.4254 s/iter. ETA=0:10:42
[02/24 16:36:42] mask2former INFO: Inference done 644/1092. Dataloading: 0.0270 s/iter. Inference: 0.8093 s/iter. Eval: 0.5905 s/iter. Total: 1.4283 s/iter. ETA=0:10:39
[02/24 16:36:48] mask2former INFO: Inference done 648/1092. Dataloading: 0.0270 s/iter. Inference: 0.8103 s/iter. Eval: 0.5900 s/iter. Total: 1.4288 s/iter. ETA=0:10:34
[02/24 16:36:54] mask2former INFO: Inference done 653/1092. Dataloading: 0.0269 s/iter. Inference: 0.8094 s/iter. Eval: 0.5890 s/iter. Total: 1.4270 s/iter. ETA=0:10:26
[02/24 16:36:59] mask2former INFO: Inference done 657/1092. Dataloading: 0.0269 s/iter. Inference: 0.8091 s/iter. Eval: 0.5887 s/iter. Total: 1.4262 s/iter. ETA=0:10:20
[02/24 16:37:05] mask2former INFO: Inference done 662/1092. Dataloading: 0.0270 s/iter. Inference: 0.8078 s/iter. Eval: 0.5886 s/iter. Total: 1.4249 s/iter. ETA=0:10:12
[02/24 16:37:11] mask2former INFO: Inference done 666/1092. Dataloading: 0.0271 s/iter. Inference: 0.8076 s/iter. Eval: 0.5888 s/iter. Total: 1.4251 s/iter. ETA=0:10:07
[02/24 16:37:17] mask2former INFO: Inference done 671/1092. Dataloading: 0.0271 s/iter. Inference: 0.8077 s/iter. Eval: 0.5876 s/iter. Total: 1.4239 s/iter. ETA=0:09:59
[02/24 16:37:24] mask2former INFO: Inference done 676/1092. Dataloading: 0.0270 s/iter. Inference: 0.8073 s/iter. Eval: 0.5866 s/iter. Total: 1.4225 s/iter. ETA=0:09:51
[02/24 16:37:29] mask2former INFO: Inference done 680/1092. Dataloading: 0.0270 s/iter. Inference: 0.8064 s/iter. Eval: 0.5868 s/iter. Total: 1.4218 s/iter. ETA=0:09:45
[02/24 16:37:34] mask2former INFO: Inference done 684/1092. Dataloading: 0.0270 s/iter. Inference: 0.8059 s/iter. Eval: 0.5872 s/iter. Total: 1.4217 s/iter. ETA=0:09:40
[02/24 16:37:40] mask2former INFO: Inference done 687/1092. Dataloading: 0.0270 s/iter. Inference: 0.8068 s/iter. Eval: 0.5883 s/iter. Total: 1.4237 s/iter. ETA=0:09:36
[02/24 16:37:46] mask2former INFO: Inference done 692/1092. Dataloading: 0.0270 s/iter. Inference: 0.8061 s/iter. Eval: 0.5877 s/iter. Total: 1.4224 s/iter. ETA=0:09:28
[02/24 16:37:52] mask2former INFO: Inference done 696/1092. Dataloading: 0.0271 s/iter. Inference: 0.8064 s/iter. Eval: 0.5876 s/iter. Total: 1.4228 s/iter. ETA=0:09:23
[02/24 16:37:58] mask2former INFO: Inference done 701/1092. Dataloading: 0.0273 s/iter. Inference: 0.8056 s/iter. Eval: 0.5869 s/iter. Total: 1.4213 s/iter. ETA=0:09:15
[02/24 16:38:04] mask2former INFO: Inference done 706/1092. Dataloading: 0.0272 s/iter. Inference: 0.8042 s/iter. Eval: 0.5863 s/iter. Total: 1.4193 s/iter. ETA=0:09:07
[02/24 16:38:10] mask2former INFO: Inference done 710/1092. Dataloading: 0.0271 s/iter. Inference: 0.8038 s/iter. Eval: 0.5871 s/iter. Total: 1.4196 s/iter. ETA=0:09:02
[02/24 16:38:16] mask2former INFO: Inference done 715/1092. Dataloading: 0.0271 s/iter. Inference: 0.8030 s/iter. Eval: 0.5864 s/iter. Total: 1.4181 s/iter. ETA=0:08:54
[02/24 16:38:22] mask2former INFO: Inference done 718/1092. Dataloading: 0.0271 s/iter. Inference: 0.8038 s/iter. Eval: 0.5877 s/iter. Total: 1.4202 s/iter. ETA=0:08:51
[02/24 16:38:27] mask2former INFO: Inference done 721/1092. Dataloading: 0.0271 s/iter. Inference: 0.8038 s/iter. Eval: 0.5892 s/iter. Total: 1.4217 s/iter. ETA=0:08:47
[02/24 16:38:33] mask2former INFO: Inference done 724/1092. Dataloading: 0.0271 s/iter. Inference: 0.8050 s/iter. Eval: 0.5904 s/iter. Total: 1.4242 s/iter. ETA=0:08:44
[02/24 16:38:39] mask2former INFO: Inference done 727/1092. Dataloading: 0.0272 s/iter. Inference: 0.8062 s/iter. Eval: 0.5911 s/iter. Total: 1.4261 s/iter. ETA=0:08:40
[02/24 16:38:45] mask2former INFO: Inference done 731/1092. Dataloading: 0.0272 s/iter. Inference: 0.8062 s/iter. Eval: 0.5921 s/iter. Total: 1.4272 s/iter. ETA=0:08:35
[02/24 16:38:51] mask2former INFO: Inference done 736/1092. Dataloading: 0.0272 s/iter. Inference: 0.8056 s/iter. Eval: 0.5914 s/iter. Total: 1.4258 s/iter. ETA=0:08:27
[02/24 16:38:58] mask2former INFO: Inference done 740/1092. Dataloading: 0.0273 s/iter. Inference: 0.8068 s/iter. Eval: 0.5920 s/iter. Total: 1.4276 s/iter. ETA=0:08:22
[02/24 16:39:04] mask2former INFO: Inference done 744/1092. Dataloading: 0.0273 s/iter. Inference: 0.8080 s/iter. Eval: 0.5913 s/iter. Total: 1.4281 s/iter. ETA=0:08:16
[02/24 16:39:11] mask2former INFO: Inference done 749/1092. Dataloading: 0.0272 s/iter. Inference: 0.8071 s/iter. Eval: 0.5908 s/iter. Total: 1.4267 s/iter. ETA=0:08:09
[02/24 16:39:16] mask2former INFO: Inference done 753/1092. Dataloading: 0.0271 s/iter. Inference: 0.8072 s/iter. Eval: 0.5909 s/iter. Total: 1.4268 s/iter. ETA=0:08:03
[02/24 16:39:22] mask2former INFO: Inference done 757/1092. Dataloading: 0.0272 s/iter. Inference: 0.8076 s/iter. Eval: 0.5905 s/iter. Total: 1.4269 s/iter. ETA=0:07:58
[02/24 16:39:28] mask2former INFO: Inference done 761/1092. Dataloading: 0.0272 s/iter. Inference: 0.8078 s/iter. Eval: 0.5901 s/iter. Total: 1.4268 s/iter. ETA=0:07:52
[02/24 16:39:34] mask2former INFO: Inference done 765/1092. Dataloading: 0.0273 s/iter. Inference: 0.8084 s/iter. Eval: 0.5904 s/iter. Total: 1.4276 s/iter. ETA=0:07:46
[02/24 16:39:40] mask2former INFO: Inference done 769/1092. Dataloading: 0.0273 s/iter. Inference: 0.8081 s/iter. Eval: 0.5904 s/iter. Total: 1.4274 s/iter. ETA=0:07:41
[02/24 16:39:45] mask2former INFO: Inference done 774/1092. Dataloading: 0.0273 s/iter. Inference: 0.8072 s/iter. Eval: 0.5897 s/iter. Total: 1.4258 s/iter. ETA=0:07:33
[02/24 16:39:52] mask2former INFO: Inference done 778/1092. Dataloading: 0.0273 s/iter. Inference: 0.8074 s/iter. Eval: 0.5900 s/iter. Total: 1.4263 s/iter. ETA=0:07:27
[02/24 16:39:57] mask2former INFO: Inference done 782/1092. Dataloading: 0.0274 s/iter. Inference: 0.8069 s/iter. Eval: 0.5904 s/iter. Total: 1.4262 s/iter. ETA=0:07:22
[02/24 16:40:03] mask2former INFO: Inference done 787/1092. Dataloading: 0.0273 s/iter. Inference: 0.8066 s/iter. Eval: 0.5894 s/iter. Total: 1.4249 s/iter. ETA=0:07:14
[02/24 16:40:09] mask2former INFO: Inference done 791/1092. Dataloading: 0.0273 s/iter. Inference: 0.8062 s/iter. Eval: 0.5895 s/iter. Total: 1.4246 s/iter. ETA=0:07:08
[02/24 16:40:15] mask2former INFO: Inference done 796/1092. Dataloading: 0.0273 s/iter. Inference: 0.8053 s/iter. Eval: 0.5888 s/iter. Total: 1.4229 s/iter. ETA=0:07:01
[02/24 16:40:20] mask2former INFO: Inference done 799/1092. Dataloading: 0.0273 s/iter. Inference: 0.8057 s/iter. Eval: 0.5896 s/iter. Total: 1.4243 s/iter. ETA=0:06:57
[02/24 16:40:26] mask2former INFO: Inference done 803/1092. Dataloading: 0.0273 s/iter. Inference: 0.8058 s/iter. Eval: 0.5904 s/iter. Total: 1.4251 s/iter. ETA=0:06:51
[02/24 16:40:33] mask2former INFO: Inference done 807/1092. Dataloading: 0.0274 s/iter. Inference: 0.8068 s/iter. Eval: 0.5902 s/iter. Total: 1.4260 s/iter. ETA=0:06:46
[02/24 16:40:38] mask2former INFO: Inference done 811/1092. Dataloading: 0.0274 s/iter. Inference: 0.8069 s/iter. Eval: 0.5902 s/iter. Total: 1.4260 s/iter. ETA=0:06:40
[02/24 16:40:44] mask2former INFO: Inference done 815/1092. Dataloading: 0.0274 s/iter. Inference: 0.8067 s/iter. Eval: 0.5901 s/iter. Total: 1.4258 s/iter. ETA=0:06:34
[02/24 16:40:50] mask2former INFO: Inference done 819/1092. Dataloading: 0.0274 s/iter. Inference: 0.8070 s/iter. Eval: 0.5899 s/iter. Total: 1.4259 s/iter. ETA=0:06:29
[02/24 16:40:55] mask2former INFO: Inference done 822/1092. Dataloading: 0.0274 s/iter. Inference: 0.8082 s/iter. Eval: 0.5902 s/iter. Total: 1.4274 s/iter. ETA=0:06:25
[02/24 16:41:00] mask2former INFO: Inference done 826/1092. Dataloading: 0.0274 s/iter. Inference: 0.8076 s/iter. Eval: 0.5902 s/iter. Total: 1.4268 s/iter. ETA=0:06:19
[02/24 16:41:06] mask2former INFO: Inference done 830/1092. Dataloading: 0.0273 s/iter. Inference: 0.8074 s/iter. Eval: 0.5900 s/iter. Total: 1.4262 s/iter. ETA=0:06:13
[02/24 16:41:11] mask2former INFO: Inference done 834/1092. Dataloading: 0.0273 s/iter. Inference: 0.8069 s/iter. Eval: 0.5898 s/iter. Total: 1.4256 s/iter. ETA=0:06:07
[02/24 16:41:18] mask2former INFO: Inference done 838/1092. Dataloading: 0.0273 s/iter. Inference: 0.8079 s/iter. Eval: 0.5903 s/iter. Total: 1.4272 s/iter. ETA=0:06:02
[02/24 16:41:24] mask2former INFO: Inference done 842/1092. Dataloading: 0.0273 s/iter. Inference: 0.8082 s/iter. Eval: 0.5909 s/iter. Total: 1.4280 s/iter. ETA=0:05:56
[02/24 16:41:31] mask2former INFO: Inference done 846/1092. Dataloading: 0.0273 s/iter. Inference: 0.8084 s/iter. Eval: 0.5915 s/iter. Total: 1.4288 s/iter. ETA=0:05:51
[02/24 16:41:37] mask2former INFO: Inference done 850/1092. Dataloading: 0.0273 s/iter. Inference: 0.8086 s/iter. Eval: 0.5921 s/iter. Total: 1.4296 s/iter. ETA=0:05:45
[02/24 16:41:43] mask2former INFO: Inference done 853/1092. Dataloading: 0.0273 s/iter. Inference: 0.8092 s/iter. Eval: 0.5936 s/iter. Total: 1.4317 s/iter. ETA=0:05:42
[02/24 16:41:49] mask2former INFO: Inference done 857/1092. Dataloading: 0.0274 s/iter. Inference: 0.8090 s/iter. Eval: 0.5938 s/iter. Total: 1.4318 s/iter. ETA=0:05:36
[02/24 16:41:55] mask2former INFO: Inference done 861/1092. Dataloading: 0.0273 s/iter. Inference: 0.8094 s/iter. Eval: 0.5937 s/iter. Total: 1.4320 s/iter. ETA=0:05:30
[02/24 16:42:01] mask2former INFO: Inference done 866/1092. Dataloading: 0.0273 s/iter. Inference: 0.8091 s/iter. Eval: 0.5931 s/iter. Total: 1.4311 s/iter. ETA=0:05:23
[02/24 16:42:08] mask2former INFO: Inference done 870/1092. Dataloading: 0.0274 s/iter. Inference: 0.8092 s/iter. Eval: 0.5937 s/iter. Total: 1.4319 s/iter. ETA=0:05:17
[02/24 16:42:13] mask2former INFO: Inference done 874/1092. Dataloading: 0.0273 s/iter. Inference: 0.8084 s/iter. Eval: 0.5938 s/iter. Total: 1.4311 s/iter. ETA=0:05:11
[02/24 16:42:19] mask2former INFO: Inference done 879/1092. Dataloading: 0.0273 s/iter. Inference: 0.8077 s/iter. Eval: 0.5937 s/iter. Total: 1.4304 s/iter. ETA=0:05:04
[02/24 16:42:26] mask2former INFO: Inference done 883/1092. Dataloading: 0.0273 s/iter. Inference: 0.8080 s/iter. Eval: 0.5942 s/iter. Total: 1.4311 s/iter. ETA=0:04:59
[02/24 16:42:32] mask2former INFO: Inference done 887/1092. Dataloading: 0.0274 s/iter. Inference: 0.8083 s/iter. Eval: 0.5944 s/iter. Total: 1.4317 s/iter. ETA=0:04:53
[02/24 16:42:38] mask2former INFO: Inference done 891/1092. Dataloading: 0.0274 s/iter. Inference: 0.8083 s/iter. Eval: 0.5946 s/iter. Total: 1.4319 s/iter. ETA=0:04:47
[02/24 16:42:44] mask2former INFO: Inference done 896/1092. Dataloading: 0.0274 s/iter. Inference: 0.8075 s/iter. Eval: 0.5939 s/iter. Total: 1.4305 s/iter. ETA=0:04:40
[02/24 16:42:49] mask2former INFO: Inference done 900/1092. Dataloading: 0.0274 s/iter. Inference: 0.8073 s/iter. Eval: 0.5940 s/iter. Total: 1.4304 s/iter. ETA=0:04:34
[02/24 16:42:55] mask2former INFO: Inference done 905/1092. Dataloading: 0.0274 s/iter. Inference: 0.8066 s/iter. Eval: 0.5931 s/iter. Total: 1.4287 s/iter. ETA=0:04:27
[02/24 16:43:01] mask2former INFO: Inference done 910/1092. Dataloading: 0.0274 s/iter. Inference: 0.8064 s/iter. Eval: 0.5925 s/iter. Total: 1.4280 s/iter. ETA=0:04:19
[02/24 16:43:07] mask2former INFO: Inference done 915/1092. Dataloading: 0.0274 s/iter. Inference: 0.8053 s/iter. Eval: 0.5920 s/iter. Total: 1.4264 s/iter. ETA=0:04:12
[02/24 16:43:13] mask2former INFO: Inference done 919/1092. Dataloading: 0.0274 s/iter. Inference: 0.8055 s/iter. Eval: 0.5921 s/iter. Total: 1.4266 s/iter. ETA=0:04:06
[02/24 16:43:18] mask2former INFO: Inference done 923/1092. Dataloading: 0.0274 s/iter. Inference: 0.8053 s/iter. Eval: 0.5918 s/iter. Total: 1.4261 s/iter. ETA=0:04:01
[02/24 16:43:24] mask2former INFO: Inference done 928/1092. Dataloading: 0.0273 s/iter. Inference: 0.8049 s/iter. Eval: 0.5912 s/iter. Total: 1.4250 s/iter. ETA=0:03:53
[02/24 16:43:30] mask2former INFO: Inference done 932/1092. Dataloading: 0.0273 s/iter. Inference: 0.8055 s/iter. Eval: 0.5910 s/iter. Total: 1.4253 s/iter. ETA=0:03:48
[02/24 16:43:36] mask2former INFO: Inference done 936/1092. Dataloading: 0.0272 s/iter. Inference: 0.8055 s/iter. Eval: 0.5910 s/iter. Total: 1.4253 s/iter. ETA=0:03:42
[02/24 16:43:42] mask2former INFO: Inference done 940/1092. Dataloading: 0.0272 s/iter. Inference: 0.8052 s/iter. Eval: 0.5915 s/iter. Total: 1.4255 s/iter. ETA=0:03:36
[02/24 16:43:48] mask2former INFO: Inference done 944/1092. Dataloading: 0.0272 s/iter. Inference: 0.8055 s/iter. Eval: 0.5917 s/iter. Total: 1.4259 s/iter. ETA=0:03:31
[02/24 16:43:53] mask2former INFO: Inference done 949/1092. Dataloading: 0.0271 s/iter. Inference: 0.8041 s/iter. Eval: 0.5912 s/iter. Total: 1.4241 s/iter. ETA=0:03:23
[02/24 16:43:59] mask2former INFO: Inference done 953/1092. Dataloading: 0.0271 s/iter. Inference: 0.8032 s/iter. Eval: 0.5915 s/iter. Total: 1.4235 s/iter. ETA=0:03:17
[02/24 16:44:05] mask2former INFO: Inference done 957/1092. Dataloading: 0.0271 s/iter. Inference: 0.8029 s/iter. Eval: 0.5924 s/iter. Total: 1.4241 s/iter. ETA=0:03:12
[02/24 16:44:11] mask2former INFO: Inference done 962/1092. Dataloading: 0.0271 s/iter. Inference: 0.8026 s/iter. Eval: 0.5917 s/iter. Total: 1.4230 s/iter. ETA=0:03:04
[02/24 16:44:16] mask2former INFO: Inference done 966/1092. Dataloading: 0.0271 s/iter. Inference: 0.8022 s/iter. Eval: 0.5916 s/iter. Total: 1.4225 s/iter. ETA=0:02:59
[02/24 16:44:22] mask2former INFO: Inference done 971/1092. Dataloading: 0.0271 s/iter. Inference: 0.8010 s/iter. Eval: 0.5912 s/iter. Total: 1.4209 s/iter. ETA=0:02:51
[02/24 16:44:28] mask2former INFO: Inference done 976/1092. Dataloading: 0.0271 s/iter. Inference: 0.8007 s/iter. Eval: 0.5909 s/iter. Total: 1.4203 s/iter. ETA=0:02:44
[02/24 16:44:34] mask2former INFO: Inference done 980/1092. Dataloading: 0.0271 s/iter. Inference: 0.8004 s/iter. Eval: 0.5911 s/iter. Total: 1.4202 s/iter. ETA=0:02:39
[02/24 16:44:40] mask2former INFO: Inference done 984/1092. Dataloading: 0.0270 s/iter. Inference: 0.8005 s/iter. Eval: 0.5913 s/iter. Total: 1.4204 s/iter. ETA=0:02:33
[02/24 16:44:45] mask2former INFO: Inference done 989/1092. Dataloading: 0.0270 s/iter. Inference: 0.7994 s/iter. Eval: 0.5910 s/iter. Total: 1.4191 s/iter. ETA=0:02:26
[02/24 16:44:51] mask2former INFO: Inference done 994/1092. Dataloading: 0.0270 s/iter. Inference: 0.7988 s/iter. Eval: 0.5900 s/iter. Total: 1.4175 s/iter. ETA=0:02:18
[02/24 16:44:56] mask2former INFO: Inference done 998/1092. Dataloading: 0.0270 s/iter. Inference: 0.7986 s/iter. Eval: 0.5897 s/iter. Total: 1.4170 s/iter. ETA=0:02:13
[02/24 16:45:02] mask2former INFO: Inference done 1002/1092. Dataloading: 0.0270 s/iter. Inference: 0.7989 s/iter. Eval: 0.5900 s/iter. Total: 1.4176 s/iter. ETA=0:02:07
[02/24 16:45:08] mask2former INFO: Inference done 1006/1092. Dataloading: 0.0271 s/iter. Inference: 0.7987 s/iter. Eval: 0.5900 s/iter. Total: 1.4175 s/iter. ETA=0:02:01
[02/24 16:45:14] mask2former INFO: Inference done 1010/1092. Dataloading: 0.0271 s/iter. Inference: 0.7988 s/iter. Eval: 0.5901 s/iter. Total: 1.4177 s/iter. ETA=0:01:56
[02/24 16:45:19] mask2former INFO: Inference done 1014/1092. Dataloading: 0.0272 s/iter. Inference: 0.7983 s/iter. Eval: 0.5902 s/iter. Total: 1.4174 s/iter. ETA=0:01:50
[02/24 16:45:25] mask2former INFO: Inference done 1018/1092. Dataloading: 0.0272 s/iter. Inference: 0.7981 s/iter. Eval: 0.5910 s/iter. Total: 1.4180 s/iter. ETA=0:01:44
[02/24 16:45:32] mask2former INFO: Inference done 1022/1092. Dataloading: 0.0272 s/iter. Inference: 0.7988 s/iter. Eval: 0.5907 s/iter. Total: 1.4184 s/iter. ETA=0:01:39
[02/24 16:45:38] mask2former INFO: Inference done 1028/1092. Dataloading: 0.0272 s/iter. Inference: 0.7979 s/iter. Eval: 0.5892 s/iter. Total: 1.4160 s/iter. ETA=0:01:30
[02/24 16:45:43] mask2former INFO: Inference done 1033/1092. Dataloading: 0.0272 s/iter. Inference: 0.7968 s/iter. Eval: 0.5883 s/iter. Total: 1.4140 s/iter. ETA=0:01:23
[02/24 16:45:49] mask2former INFO: Inference done 1037/1092. Dataloading: 0.0272 s/iter. Inference: 0.7973 s/iter. Eval: 0.5886 s/iter. Total: 1.4147 s/iter. ETA=0:01:17
[02/24 16:45:56] mask2former INFO: Inference done 1041/1092. Dataloading: 0.0272 s/iter. Inference: 0.7979 s/iter. Eval: 0.5890 s/iter. Total: 1.4158 s/iter. ETA=0:01:12
[02/24 16:46:02] mask2former INFO: Inference done 1045/1092. Dataloading: 0.0272 s/iter. Inference: 0.7980 s/iter. Eval: 0.5891 s/iter. Total: 1.4160 s/iter. ETA=0:01:06
[02/24 16:46:07] mask2former INFO: Inference done 1049/1092. Dataloading: 0.0272 s/iter. Inference: 0.7981 s/iter. Eval: 0.5891 s/iter. Total: 1.4160 s/iter. ETA=0:01:00
[02/24 16:46:14] mask2former INFO: Inference done 1053/1092. Dataloading: 0.0272 s/iter. Inference: 0.7991 s/iter. Eval: 0.5894 s/iter. Total: 1.4173 s/iter. ETA=0:00:55
[02/24 16:46:21] mask2former INFO: Inference done 1058/1092. Dataloading: 0.0271 s/iter. Inference: 0.7990 s/iter. Eval: 0.5891 s/iter. Total: 1.4169 s/iter. ETA=0:00:48
[02/24 16:46:26] mask2former INFO: Inference done 1061/1092. Dataloading: 0.0271 s/iter. Inference: 0.7996 s/iter. Eval: 0.5895 s/iter. Total: 1.4179 s/iter. ETA=0:00:43
[02/24 16:46:33] mask2former INFO: Inference done 1065/1092. Dataloading: 0.0271 s/iter. Inference: 0.7993 s/iter. Eval: 0.5904 s/iter. Total: 1.4184 s/iter. ETA=0:00:38
[02/24 16:46:39] mask2former INFO: Inference done 1069/1092. Dataloading: 0.0271 s/iter. Inference: 0.7995 s/iter. Eval: 0.5906 s/iter. Total: 1.4188 s/iter. ETA=0:00:32
[02/24 16:46:44] mask2former INFO: Inference done 1073/1092. Dataloading: 0.0271 s/iter. Inference: 0.7991 s/iter. Eval: 0.5905 s/iter. Total: 1.4183 s/iter. ETA=0:00:26
[02/24 16:46:49] mask2former INFO: Inference done 1077/1092. Dataloading: 0.0270 s/iter. Inference: 0.7992 s/iter. Eval: 0.5900 s/iter. Total: 1.4179 s/iter. ETA=0:00:21
[02/24 16:46:55] mask2former INFO: Inference done 1082/1092. Dataloading: 0.0270 s/iter. Inference: 0.7987 s/iter. Eval: 0.5894 s/iter. Total: 1.4168 s/iter. ETA=0:00:14
[02/24 16:47:01] mask2former INFO: Inference done 1086/1092. Dataloading: 0.0271 s/iter. Inference: 0.7987 s/iter. Eval: 0.5896 s/iter. Total: 1.4170 s/iter. ETA=0:00:08
[02/24 16:47:06] mask2former INFO: Inference done 1090/1092. Dataloading: 0.0270 s/iter. Inference: 0.7986 s/iter. Eval: 0.5896 s/iter. Total: 1.4168 s/iter. ETA=0:00:02
