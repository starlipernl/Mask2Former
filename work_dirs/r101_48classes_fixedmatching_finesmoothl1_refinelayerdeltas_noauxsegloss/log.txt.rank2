[02/24 11:33:53] detectron2 INFO: Rank of current process: 2. World size: 4
[02/24 11:34:03] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 11:34:03] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 11:34:03] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 11:34:03] detectron2.utils.env INFO: Using a generated random seed 3804012
[02/24 11:34:12] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 1.0, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 11:34:13] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 11:34:25] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 11:34:25] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 11:34:29] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 11:34:29] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 11:34:29] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 11:34:30] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 11:34:35] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 11:34:35] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 11:34:35] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 11:34:56] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 395, in run_step
    loss_dict = self.model(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 791, in forward
    losses = self.criterion(outputs, targets)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/criterion.py", line 259, in forward
    losses.update(self.get_loss(loss, outputs, targets, indices, num_masks))
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 212, in get_loss
    return loss_map[loss](outputs, targets, indices, num_masks)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 191, in loss_segs
    semseg,
UnboundLocalError: local variable 'semseg' referenced before assignment
[02/24 11:34:56] detectron2.engine.hooks INFO: Total training time: 0:00:21 (0:00:00 on hooks)
[02/24 11:37:47] detectron2 INFO: Rank of current process: 2. World size: 4
[02/24 11:37:53] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 11:37:53] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 11:37:53] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 11:37:53] detectron2.utils.env INFO: Using a generated random seed 53320708
[02/24 11:37:58] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 1.0, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 11:37:58] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 11:38:08] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 11:38:16] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 11:38:16] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 11:38:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 11:38:17] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 11:38:17] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 11:38:18] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 11:38:18] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 11:38:18] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 12:10:58] detectron2.engine.hooks INFO: Overall training speed: 1100 iterations in 0:32:10 (1.7546 s / it)
[02/24 12:10:58] detectron2.engine.hooks INFO: Total training time: 0:32:14 (0:00:04 on hooks)
[02/24 12:12:19] detectron2 INFO: Rank of current process: 2. World size: 4
[02/24 12:12:26] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 12:12:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 12:12:26] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 12:12:26] detectron2.utils.env INFO: Using a generated random seed 26648756
[02/24 12:12:33] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 0.1, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 12:12:34] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 12:12:44] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 12:12:44] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 12:12:44] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 12:12:53] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 12:12:53] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 12:12:53] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 12:12:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 12:12:54] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 12:12:54] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 13:23:46] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 13:23:47] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 13:23:47] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 13:24:10] mask2former INFO: Inference done 11/1092. Dataloading: 0.0063 s/iter. Inference: 0.3293 s/iter. Eval: 0.2762 s/iter. Total: 0.6118 s/iter. ETA=0:11:01
[02/24 13:24:15] mask2former INFO: Inference done 19/1092. Dataloading: 0.0088 s/iter. Inference: 0.3339 s/iter. Eval: 0.2769 s/iter. Total: 0.6198 s/iter. ETA=0:11:05
[02/24 13:24:20] mask2former INFO: Inference done 27/1092. Dataloading: 0.0092 s/iter. Inference: 0.3476 s/iter. Eval: 0.2735 s/iter. Total: 0.6305 s/iter. ETA=0:11:11
[02/24 13:24:25] mask2former INFO: Inference done 35/1092. Dataloading: 0.0100 s/iter. Inference: 0.3627 s/iter. Eval: 0.2622 s/iter. Total: 0.6351 s/iter. ETA=0:11:11
[02/24 13:24:30] mask2former INFO: Inference done 43/1092. Dataloading: 0.0114 s/iter. Inference: 0.3716 s/iter. Eval: 0.2566 s/iter. Total: 0.6398 s/iter. ETA=0:11:11
[02/24 13:24:36] mask2former INFO: Inference done 52/1092. Dataloading: 0.0122 s/iter. Inference: 0.3666 s/iter. Eval: 0.2513 s/iter. Total: 0.6303 s/iter. ETA=0:10:55
[02/24 13:24:41] mask2former INFO: Inference done 60/1092. Dataloading: 0.0120 s/iter. Inference: 0.3685 s/iter. Eval: 0.2516 s/iter. Total: 0.6323 s/iter. ETA=0:10:52
[02/24 13:24:46] mask2former INFO: Inference done 69/1092. Dataloading: 0.0118 s/iter. Inference: 0.3640 s/iter. Eval: 0.2501 s/iter. Total: 0.6261 s/iter. ETA=0:10:40
[02/24 13:24:51] mask2former INFO: Inference done 78/1092. Dataloading: 0.0113 s/iter. Inference: 0.3657 s/iter. Eval: 0.2457 s/iter. Total: 0.6230 s/iter. ETA=0:10:31
[02/24 13:24:57] mask2former INFO: Inference done 87/1092. Dataloading: 0.0109 s/iter. Inference: 0.3638 s/iter. Eval: 0.2424 s/iter. Total: 0.6173 s/iter. ETA=0:10:20
[02/24 13:25:02] mask2former INFO: Inference done 96/1092. Dataloading: 0.0109 s/iter. Inference: 0.3640 s/iter. Eval: 0.2410 s/iter. Total: 0.6161 s/iter. ETA=0:10:13
[02/24 13:25:07] mask2former INFO: Inference done 108/1092. Dataloading: 0.0108 s/iter. Inference: 0.3538 s/iter. Eval: 0.2292 s/iter. Total: 0.5940 s/iter. ETA=0:09:44
[02/24 13:25:13] mask2former INFO: Inference done 116/1092. Dataloading: 0.0105 s/iter. Inference: 0.3606 s/iter. Eval: 0.2328 s/iter. Total: 0.6042 s/iter. ETA=0:09:49
[02/24 13:25:18] mask2former INFO: Inference done 122/1092. Dataloading: 0.0110 s/iter. Inference: 0.3680 s/iter. Eval: 0.2382 s/iter. Total: 0.6174 s/iter. ETA=0:09:58
[02/24 13:25:23] mask2former INFO: Inference done 131/1092. Dataloading: 0.0111 s/iter. Inference: 0.3674 s/iter. Eval: 0.2363 s/iter. Total: 0.6150 s/iter. ETA=0:09:50
[02/24 13:25:29] mask2former INFO: Inference done 138/1092. Dataloading: 0.0116 s/iter. Inference: 0.3682 s/iter. Eval: 0.2423 s/iter. Total: 0.6223 s/iter. ETA=0:09:53
[02/24 13:25:34] mask2former INFO: Inference done 146/1092. Dataloading: 0.0118 s/iter. Inference: 0.3706 s/iter. Eval: 0.2441 s/iter. Total: 0.6267 s/iter. ETA=0:09:52
[02/24 13:25:40] mask2former INFO: Inference done 154/1092. Dataloading: 0.0116 s/iter. Inference: 0.3724 s/iter. Eval: 0.2445 s/iter. Total: 0.6289 s/iter. ETA=0:09:49
[02/24 13:25:45] mask2former INFO: Inference done 162/1092. Dataloading: 0.0119 s/iter. Inference: 0.3727 s/iter. Eval: 0.2460 s/iter. Total: 0.6309 s/iter. ETA=0:09:46
[02/24 13:25:51] mask2former INFO: Inference done 171/1092. Dataloading: 0.0117 s/iter. Inference: 0.3718 s/iter. Eval: 0.2463 s/iter. Total: 0.6302 s/iter. ETA=0:09:40
[02/24 13:25:56] mask2former INFO: Inference done 180/1092. Dataloading: 0.0118 s/iter. Inference: 0.3715 s/iter. Eval: 0.2447 s/iter. Total: 0.6287 s/iter. ETA=0:09:33
[02/24 13:26:01] mask2former INFO: Inference done 189/1092. Dataloading: 0.0118 s/iter. Inference: 0.3724 s/iter. Eval: 0.2430 s/iter. Total: 0.6278 s/iter. ETA=0:09:26
[02/24 13:26:07] mask2former INFO: Inference done 197/1092. Dataloading: 0.0122 s/iter. Inference: 0.3725 s/iter. Eval: 0.2434 s/iter. Total: 0.6287 s/iter. ETA=0:09:22
[02/24 13:26:12] mask2former INFO: Inference done 205/1092. Dataloading: 0.0121 s/iter. Inference: 0.3721 s/iter. Eval: 0.2440 s/iter. Total: 0.6288 s/iter. ETA=0:09:17
[02/24 13:26:17] mask2former INFO: Inference done 213/1092. Dataloading: 0.0120 s/iter. Inference: 0.3721 s/iter. Eval: 0.2456 s/iter. Total: 0.6303 s/iter. ETA=0:09:14
[02/24 13:26:22] mask2former INFO: Inference done 221/1092. Dataloading: 0.0120 s/iter. Inference: 0.3727 s/iter. Eval: 0.2464 s/iter. Total: 0.6317 s/iter. ETA=0:09:10
[02/24 13:26:28] mask2former INFO: Inference done 230/1092. Dataloading: 0.0119 s/iter. Inference: 0.3713 s/iter. Eval: 0.2467 s/iter. Total: 0.6305 s/iter. ETA=0:09:03
[02/24 13:26:33] mask2former INFO: Inference done 238/1092. Dataloading: 0.0119 s/iter. Inference: 0.3718 s/iter. Eval: 0.2469 s/iter. Total: 0.6312 s/iter. ETA=0:08:59
[02/24 13:26:38] mask2former INFO: Inference done 246/1092. Dataloading: 0.0118 s/iter. Inference: 0.3720 s/iter. Eval: 0.2468 s/iter. Total: 0.6312 s/iter. ETA=0:08:53
[02/24 13:26:43] mask2former INFO: Inference done 254/1092. Dataloading: 0.0119 s/iter. Inference: 0.3731 s/iter. Eval: 0.2468 s/iter. Total: 0.6324 s/iter. ETA=0:08:49
[02/24 13:26:48] mask2former INFO: Inference done 262/1092. Dataloading: 0.0118 s/iter. Inference: 0.3728 s/iter. Eval: 0.2472 s/iter. Total: 0.6324 s/iter. ETA=0:08:44
[02/24 13:26:54] mask2former INFO: Inference done 270/1092. Dataloading: 0.0119 s/iter. Inference: 0.3728 s/iter. Eval: 0.2479 s/iter. Total: 0.6331 s/iter. ETA=0:08:40
[02/24 13:26:59] mask2former INFO: Inference done 278/1092. Dataloading: 0.0120 s/iter. Inference: 0.3734 s/iter. Eval: 0.2478 s/iter. Total: 0.6338 s/iter. ETA=0:08:35
[02/24 13:27:04] mask2former INFO: Inference done 286/1092. Dataloading: 0.0122 s/iter. Inference: 0.3744 s/iter. Eval: 0.2467 s/iter. Total: 0.6338 s/iter. ETA=0:08:30
[02/24 13:27:10] mask2former INFO: Inference done 295/1092. Dataloading: 0.0122 s/iter. Inference: 0.3742 s/iter. Eval: 0.2466 s/iter. Total: 0.6335 s/iter. ETA=0:08:24
[02/24 13:27:15] mask2former INFO: Inference done 304/1092. Dataloading: 0.0122 s/iter. Inference: 0.3745 s/iter. Eval: 0.2468 s/iter. Total: 0.6339 s/iter. ETA=0:08:19
[02/24 13:27:21] mask2former INFO: Inference done 313/1092. Dataloading: 0.0123 s/iter. Inference: 0.3747 s/iter. Eval: 0.2459 s/iter. Total: 0.6335 s/iter. ETA=0:08:13
[02/24 13:27:26] mask2former INFO: Inference done 321/1092. Dataloading: 0.0122 s/iter. Inference: 0.3748 s/iter. Eval: 0.2459 s/iter. Total: 0.6334 s/iter. ETA=0:08:08
[02/24 13:27:31] mask2former INFO: Inference done 329/1092. Dataloading: 0.0123 s/iter. Inference: 0.3758 s/iter. Eval: 0.2455 s/iter. Total: 0.6341 s/iter. ETA=0:08:03
[02/24 13:27:37] mask2former INFO: Inference done 337/1092. Dataloading: 0.0123 s/iter. Inference: 0.3766 s/iter. Eval: 0.2453 s/iter. Total: 0.6346 s/iter. ETA=0:07:59
[02/24 13:27:42] mask2former INFO: Inference done 345/1092. Dataloading: 0.0123 s/iter. Inference: 0.3770 s/iter. Eval: 0.2451 s/iter. Total: 0.6348 s/iter. ETA=0:07:54
[02/24 13:27:47] mask2former INFO: Inference done 354/1092. Dataloading: 0.0124 s/iter. Inference: 0.3766 s/iter. Eval: 0.2451 s/iter. Total: 0.6345 s/iter. ETA=0:07:48
[02/24 13:27:53] mask2former INFO: Inference done 362/1092. Dataloading: 0.0123 s/iter. Inference: 0.3771 s/iter. Eval: 0.2452 s/iter. Total: 0.6351 s/iter. ETA=0:07:43
[02/24 13:27:58] mask2former INFO: Inference done 370/1092. Dataloading: 0.0124 s/iter. Inference: 0.3774 s/iter. Eval: 0.2450 s/iter. Total: 0.6352 s/iter. ETA=0:07:38
[02/24 13:28:03] mask2former INFO: Inference done 378/1092. Dataloading: 0.0123 s/iter. Inference: 0.3776 s/iter. Eval: 0.2452 s/iter. Total: 0.6355 s/iter. ETA=0:07:33
[02/24 13:28:08] mask2former INFO: Inference done 386/1092. Dataloading: 0.0123 s/iter. Inference: 0.3780 s/iter. Eval: 0.2459 s/iter. Total: 0.6366 s/iter. ETA=0:07:29
[02/24 13:28:14] mask2former INFO: Inference done 395/1092. Dataloading: 0.0123 s/iter. Inference: 0.3776 s/iter. Eval: 0.2455 s/iter. Total: 0.6358 s/iter. ETA=0:07:23
[02/24 13:28:19] mask2former INFO: Inference done 404/1092. Dataloading: 0.0123 s/iter. Inference: 0.3771 s/iter. Eval: 0.2451 s/iter. Total: 0.6348 s/iter. ETA=0:07:16
[02/24 13:28:24] mask2former INFO: Inference done 412/1092. Dataloading: 0.0124 s/iter. Inference: 0.3769 s/iter. Eval: 0.2451 s/iter. Total: 0.6348 s/iter. ETA=0:07:11
[02/24 13:28:29] mask2former INFO: Inference done 420/1092. Dataloading: 0.0124 s/iter. Inference: 0.3771 s/iter. Eval: 0.2450 s/iter. Total: 0.6350 s/iter. ETA=0:07:06
[02/24 13:28:35] mask2former INFO: Inference done 429/1092. Dataloading: 0.0124 s/iter. Inference: 0.3770 s/iter. Eval: 0.2448 s/iter. Total: 0.6346 s/iter. ETA=0:07:00
[02/24 13:28:41] mask2former INFO: Inference done 438/1092. Dataloading: 0.0124 s/iter. Inference: 0.3769 s/iter. Eval: 0.2448 s/iter. Total: 0.6345 s/iter. ETA=0:06:54
[02/24 13:28:46] mask2former INFO: Inference done 446/1092. Dataloading: 0.0123 s/iter. Inference: 0.3771 s/iter. Eval: 0.2446 s/iter. Total: 0.6345 s/iter. ETA=0:06:49
[02/24 13:28:51] mask2former INFO: Inference done 455/1092. Dataloading: 0.0123 s/iter. Inference: 0.3772 s/iter. Eval: 0.2434 s/iter. Total: 0.6333 s/iter. ETA=0:06:43
[02/24 13:28:56] mask2former INFO: Inference done 464/1092. Dataloading: 0.0123 s/iter. Inference: 0.3771 s/iter. Eval: 0.2428 s/iter. Total: 0.6326 s/iter. ETA=0:06:37
[02/24 13:29:02] mask2former INFO: Inference done 473/1092. Dataloading: 0.0122 s/iter. Inference: 0.3769 s/iter. Eval: 0.2423 s/iter. Total: 0.6317 s/iter. ETA=0:06:31
[02/24 13:29:07] mask2former INFO: Inference done 482/1092. Dataloading: 0.0121 s/iter. Inference: 0.3766 s/iter. Eval: 0.2425 s/iter. Total: 0.6316 s/iter. ETA=0:06:25
[02/24 13:29:12] mask2former INFO: Inference done 490/1092. Dataloading: 0.0121 s/iter. Inference: 0.3768 s/iter. Eval: 0.2429 s/iter. Total: 0.6321 s/iter. ETA=0:06:20
[02/24 13:29:18] mask2former INFO: Inference done 498/1092. Dataloading: 0.0121 s/iter. Inference: 0.3773 s/iter. Eval: 0.2428 s/iter. Total: 0.6327 s/iter. ETA=0:06:15
[02/24 13:29:23] mask2former INFO: Inference done 506/1092. Dataloading: 0.0122 s/iter. Inference: 0.3774 s/iter. Eval: 0.2429 s/iter. Total: 0.6329 s/iter. ETA=0:06:10
[02/24 13:29:28] mask2former INFO: Inference done 514/1092. Dataloading: 0.0122 s/iter. Inference: 0.3777 s/iter. Eval: 0.2427 s/iter. Total: 0.6330 s/iter. ETA=0:06:05
[02/24 13:29:33] mask2former INFO: Inference done 522/1092. Dataloading: 0.0122 s/iter. Inference: 0.3777 s/iter. Eval: 0.2432 s/iter. Total: 0.6335 s/iter. ETA=0:06:01
[02/24 13:29:39] mask2former INFO: Inference done 530/1092. Dataloading: 0.0121 s/iter. Inference: 0.3777 s/iter. Eval: 0.2436 s/iter. Total: 0.6338 s/iter. ETA=0:05:56
[02/24 13:29:44] mask2former INFO: Inference done 538/1092. Dataloading: 0.0122 s/iter. Inference: 0.3776 s/iter. Eval: 0.2441 s/iter. Total: 0.6343 s/iter. ETA=0:05:51
[02/24 13:29:49] mask2former INFO: Inference done 546/1092. Dataloading: 0.0123 s/iter. Inference: 0.3776 s/iter. Eval: 0.2445 s/iter. Total: 0.6348 s/iter. ETA=0:05:46
[02/24 13:29:54] mask2former INFO: Inference done 554/1092. Dataloading: 0.0123 s/iter. Inference: 0.3776 s/iter. Eval: 0.2445 s/iter. Total: 0.6348 s/iter. ETA=0:05:41
[02/24 13:30:00] mask2former INFO: Inference done 562/1092. Dataloading: 0.0123 s/iter. Inference: 0.3776 s/iter. Eval: 0.2448 s/iter. Total: 0.6350 s/iter. ETA=0:05:36
[02/24 13:30:05] mask2former INFO: Inference done 570/1092. Dataloading: 0.0123 s/iter. Inference: 0.3776 s/iter. Eval: 0.2451 s/iter. Total: 0.6354 s/iter. ETA=0:05:31
[02/24 13:30:10] mask2former INFO: Inference done 580/1092. Dataloading: 0.0123 s/iter. Inference: 0.3767 s/iter. Eval: 0.2445 s/iter. Total: 0.6337 s/iter. ETA=0:05:24
[02/24 13:30:16] mask2former INFO: Inference done 589/1092. Dataloading: 0.0122 s/iter. Inference: 0.3761 s/iter. Eval: 0.2444 s/iter. Total: 0.6331 s/iter. ETA=0:05:18
[02/24 13:30:21] mask2former INFO: Inference done 597/1092. Dataloading: 0.0122 s/iter. Inference: 0.3765 s/iter. Eval: 0.2448 s/iter. Total: 0.6339 s/iter. ETA=0:05:13
[02/24 13:30:26] mask2former INFO: Inference done 605/1092. Dataloading: 0.0123 s/iter. Inference: 0.3766 s/iter. Eval: 0.2449 s/iter. Total: 0.6341 s/iter. ETA=0:05:08
[02/24 13:30:32] mask2former INFO: Inference done 613/1092. Dataloading: 0.0123 s/iter. Inference: 0.3765 s/iter. Eval: 0.2453 s/iter. Total: 0.6343 s/iter. ETA=0:05:03
[02/24 13:30:37] mask2former INFO: Inference done 621/1092. Dataloading: 0.0123 s/iter. Inference: 0.3770 s/iter. Eval: 0.2456 s/iter. Total: 0.6352 s/iter. ETA=0:04:59
[02/24 13:30:42] mask2former INFO: Inference done 629/1092. Dataloading: 0.0123 s/iter. Inference: 0.3770 s/iter. Eval: 0.2457 s/iter. Total: 0.6354 s/iter. ETA=0:04:54
[02/24 13:30:48] mask2former INFO: Inference done 638/1092. Dataloading: 0.0126 s/iter. Inference: 0.3763 s/iter. Eval: 0.2461 s/iter. Total: 0.6353 s/iter. ETA=0:04:48
[02/24 13:30:53] mask2former INFO: Inference done 646/1092. Dataloading: 0.0126 s/iter. Inference: 0.3767 s/iter. Eval: 0.2462 s/iter. Total: 0.6358 s/iter. ETA=0:04:43
[02/24 13:30:59] mask2former INFO: Inference done 654/1092. Dataloading: 0.0126 s/iter. Inference: 0.3769 s/iter. Eval: 0.2461 s/iter. Total: 0.6359 s/iter. ETA=0:04:38
[02/24 13:31:04] mask2former INFO: Inference done 662/1092. Dataloading: 0.0125 s/iter. Inference: 0.3769 s/iter. Eval: 0.2462 s/iter. Total: 0.6359 s/iter. ETA=0:04:33
[02/24 13:31:09] mask2former INFO: Inference done 670/1092. Dataloading: 0.0126 s/iter. Inference: 0.3774 s/iter. Eval: 0.2459 s/iter. Total: 0.6362 s/iter. ETA=0:04:28
[02/24 13:31:14] mask2former INFO: Inference done 678/1092. Dataloading: 0.0126 s/iter. Inference: 0.3773 s/iter. Eval: 0.2461 s/iter. Total: 0.6363 s/iter. ETA=0:04:23
[02/24 13:31:20] mask2former INFO: Inference done 686/1092. Dataloading: 0.0126 s/iter. Inference: 0.3775 s/iter. Eval: 0.2466 s/iter. Total: 0.6371 s/iter. ETA=0:04:18
[02/24 13:31:25] mask2former INFO: Inference done 695/1092. Dataloading: 0.0126 s/iter. Inference: 0.3775 s/iter. Eval: 0.2464 s/iter. Total: 0.6369 s/iter. ETA=0:04:12
[02/24 13:31:31] mask2former INFO: Inference done 703/1092. Dataloading: 0.0126 s/iter. Inference: 0.3776 s/iter. Eval: 0.2465 s/iter. Total: 0.6370 s/iter. ETA=0:04:07
[02/24 13:31:36] mask2former INFO: Inference done 715/1092. Dataloading: 0.0126 s/iter. Inference: 0.3759 s/iter. Eval: 0.2446 s/iter. Total: 0.6334 s/iter. ETA=0:03:58
[02/24 13:31:41] mask2former INFO: Inference done 720/1092. Dataloading: 0.0126 s/iter. Inference: 0.3771 s/iter. Eval: 0.2459 s/iter. Total: 0.6359 s/iter. ETA=0:03:56
[02/24 13:31:46] mask2former INFO: Inference done 726/1092. Dataloading: 0.0126 s/iter. Inference: 0.3782 s/iter. Eval: 0.2467 s/iter. Total: 0.6377 s/iter. ETA=0:03:53
[02/24 13:31:51] mask2former INFO: Inference done 736/1092. Dataloading: 0.0126 s/iter. Inference: 0.3776 s/iter. Eval: 0.2462 s/iter. Total: 0.6367 s/iter. ETA=0:03:46
[02/24 13:31:57] mask2former INFO: Inference done 743/1092. Dataloading: 0.0127 s/iter. Inference: 0.3784 s/iter. Eval: 0.2471 s/iter. Total: 0.6385 s/iter. ETA=0:03:42
[02/24 13:32:02] mask2former INFO: Inference done 750/1092. Dataloading: 0.0127 s/iter. Inference: 0.3790 s/iter. Eval: 0.2474 s/iter. Total: 0.6394 s/iter. ETA=0:03:38
[02/24 13:32:08] mask2former INFO: Inference done 759/1092. Dataloading: 0.0126 s/iter. Inference: 0.3788 s/iter. Eval: 0.2471 s/iter. Total: 0.6388 s/iter. ETA=0:03:32
[02/24 13:32:13] mask2former INFO: Inference done 768/1092. Dataloading: 0.0126 s/iter. Inference: 0.3785 s/iter. Eval: 0.2467 s/iter. Total: 0.6381 s/iter. ETA=0:03:26
[02/24 13:32:18] mask2former INFO: Inference done 776/1092. Dataloading: 0.0126 s/iter. Inference: 0.3782 s/iter. Eval: 0.2469 s/iter. Total: 0.6380 s/iter. ETA=0:03:21
[02/24 13:32:23] mask2former INFO: Inference done 785/1092. Dataloading: 0.0126 s/iter. Inference: 0.3781 s/iter. Eval: 0.2467 s/iter. Total: 0.6377 s/iter. ETA=0:03:15
[02/24 13:32:28] mask2former INFO: Inference done 793/1092. Dataloading: 0.0125 s/iter. Inference: 0.3780 s/iter. Eval: 0.2469 s/iter. Total: 0.6377 s/iter. ETA=0:03:10
[02/24 13:32:34] mask2former INFO: Inference done 802/1092. Dataloading: 0.0125 s/iter. Inference: 0.3780 s/iter. Eval: 0.2467 s/iter. Total: 0.6375 s/iter. ETA=0:03:04
[02/24 13:32:39] mask2former INFO: Inference done 810/1092. Dataloading: 0.0125 s/iter. Inference: 0.3783 s/iter. Eval: 0.2466 s/iter. Total: 0.6377 s/iter. ETA=0:02:59
[02/24 13:32:45] mask2former INFO: Inference done 818/1092. Dataloading: 0.0125 s/iter. Inference: 0.3785 s/iter. Eval: 0.2467 s/iter. Total: 0.6380 s/iter. ETA=0:02:54
[02/24 13:32:50] mask2former INFO: Inference done 826/1092. Dataloading: 0.0125 s/iter. Inference: 0.3784 s/iter. Eval: 0.2468 s/iter. Total: 0.6380 s/iter. ETA=0:02:49
[02/24 13:32:55] mask2former INFO: Inference done 834/1092. Dataloading: 0.0125 s/iter. Inference: 0.3787 s/iter. Eval: 0.2467 s/iter. Total: 0.6382 s/iter. ETA=0:02:44
[02/24 13:33:00] mask2former INFO: Inference done 842/1092. Dataloading: 0.0125 s/iter. Inference: 0.3785 s/iter. Eval: 0.2471 s/iter. Total: 0.6384 s/iter. ETA=0:02:39
[02/24 13:33:06] mask2former INFO: Inference done 850/1092. Dataloading: 0.0125 s/iter. Inference: 0.3787 s/iter. Eval: 0.2471 s/iter. Total: 0.6387 s/iter. ETA=0:02:34
[02/24 13:33:11] mask2former INFO: Inference done 859/1092. Dataloading: 0.0125 s/iter. Inference: 0.3785 s/iter. Eval: 0.2471 s/iter. Total: 0.6384 s/iter. ETA=0:02:28
[02/24 13:33:16] mask2former INFO: Inference done 867/1092. Dataloading: 0.0126 s/iter. Inference: 0.3783 s/iter. Eval: 0.2472 s/iter. Total: 0.6384 s/iter. ETA=0:02:23
[02/24 13:33:21] mask2former INFO: Inference done 875/1092. Dataloading: 0.0126 s/iter. Inference: 0.3781 s/iter. Eval: 0.2474 s/iter. Total: 0.6383 s/iter. ETA=0:02:18
[02/24 13:33:27] mask2former INFO: Inference done 884/1092. Dataloading: 0.0125 s/iter. Inference: 0.3777 s/iter. Eval: 0.2477 s/iter. Total: 0.6382 s/iter. ETA=0:02:12
[02/24 13:33:32] mask2former INFO: Inference done 892/1092. Dataloading: 0.0126 s/iter. Inference: 0.3774 s/iter. Eval: 0.2479 s/iter. Total: 0.6381 s/iter. ETA=0:02:07
[02/24 13:33:37] mask2former INFO: Inference done 901/1092. Dataloading: 0.0125 s/iter. Inference: 0.3772 s/iter. Eval: 0.2478 s/iter. Total: 0.6378 s/iter. ETA=0:02:01
[02/24 13:33:42] mask2former INFO: Inference done 909/1092. Dataloading: 0.0125 s/iter. Inference: 0.3771 s/iter. Eval: 0.2479 s/iter. Total: 0.6378 s/iter. ETA=0:01:56
[02/24 13:33:48] mask2former INFO: Inference done 917/1092. Dataloading: 0.0125 s/iter. Inference: 0.3768 s/iter. Eval: 0.2482 s/iter. Total: 0.6379 s/iter. ETA=0:01:51
[02/24 13:33:53] mask2former INFO: Inference done 926/1092. Dataloading: 0.0125 s/iter. Inference: 0.3767 s/iter. Eval: 0.2484 s/iter. Total: 0.6379 s/iter. ETA=0:01:45
[02/24 13:33:59] mask2former INFO: Inference done 935/1092. Dataloading: 0.0125 s/iter. Inference: 0.3764 s/iter. Eval: 0.2481 s/iter. Total: 0.6373 s/iter. ETA=0:01:40
[02/24 13:34:04] mask2former INFO: Inference done 943/1092. Dataloading: 0.0125 s/iter. Inference: 0.3762 s/iter. Eval: 0.2486 s/iter. Total: 0.6376 s/iter. ETA=0:01:35
[02/24 13:34:09] mask2former INFO: Inference done 951/1092. Dataloading: 0.0125 s/iter. Inference: 0.3762 s/iter. Eval: 0.2487 s/iter. Total: 0.6377 s/iter. ETA=0:01:29
[02/24 13:34:14] mask2former INFO: Inference done 959/1092. Dataloading: 0.0125 s/iter. Inference: 0.3763 s/iter. Eval: 0.2487 s/iter. Total: 0.6378 s/iter. ETA=0:01:24
[02/24 13:34:20] mask2former INFO: Inference done 967/1092. Dataloading: 0.0125 s/iter. Inference: 0.3764 s/iter. Eval: 0.2490 s/iter. Total: 0.6382 s/iter. ETA=0:01:19
[02/24 13:34:25] mask2former INFO: Inference done 975/1092. Dataloading: 0.0125 s/iter. Inference: 0.3768 s/iter. Eval: 0.2488 s/iter. Total: 0.6384 s/iter. ETA=0:01:14
[02/24 13:34:30] mask2former INFO: Inference done 984/1092. Dataloading: 0.0124 s/iter. Inference: 0.3764 s/iter. Eval: 0.2488 s/iter. Total: 0.6380 s/iter. ETA=0:01:08
[02/24 13:34:36] mask2former INFO: Inference done 992/1092. Dataloading: 0.0125 s/iter. Inference: 0.3764 s/iter. Eval: 0.2489 s/iter. Total: 0.6381 s/iter. ETA=0:01:03
[02/24 13:34:41] mask2former INFO: Inference done 1000/1092. Dataloading: 0.0125 s/iter. Inference: 0.3765 s/iter. Eval: 0.2491 s/iter. Total: 0.6384 s/iter. ETA=0:00:58
[02/24 13:34:47] mask2former INFO: Inference done 1008/1092. Dataloading: 0.0125 s/iter. Inference: 0.3765 s/iter. Eval: 0.2494 s/iter. Total: 0.6387 s/iter. ETA=0:00:53
[02/24 13:34:52] mask2former INFO: Inference done 1016/1092. Dataloading: 0.0125 s/iter. Inference: 0.3766 s/iter. Eval: 0.2494 s/iter. Total: 0.6389 s/iter. ETA=0:00:48
[02/24 13:34:57] mask2former INFO: Inference done 1024/1092. Dataloading: 0.0125 s/iter. Inference: 0.3768 s/iter. Eval: 0.2498 s/iter. Total: 0.6394 s/iter. ETA=0:00:43
[02/24 13:35:03] mask2former INFO: Inference done 1032/1092. Dataloading: 0.0125 s/iter. Inference: 0.3770 s/iter. Eval: 0.2501 s/iter. Total: 0.6399 s/iter. ETA=0:00:38
[02/24 13:35:08] mask2former INFO: Inference done 1040/1092. Dataloading: 0.0125 s/iter. Inference: 0.3773 s/iter. Eval: 0.2500 s/iter. Total: 0.6401 s/iter. ETA=0:00:33
[02/24 13:35:14] mask2former INFO: Inference done 1049/1092. Dataloading: 0.0125 s/iter. Inference: 0.3771 s/iter. Eval: 0.2500 s/iter. Total: 0.6399 s/iter. ETA=0:00:27
[02/24 13:35:20] mask2former INFO: Inference done 1057/1092. Dataloading: 0.0125 s/iter. Inference: 0.3774 s/iter. Eval: 0.2502 s/iter. Total: 0.6404 s/iter. ETA=0:00:22
[02/24 13:35:25] mask2former INFO: Inference done 1066/1092. Dataloading: 0.0125 s/iter. Inference: 0.3772 s/iter. Eval: 0.2500 s/iter. Total: 0.6400 s/iter. ETA=0:00:16
[02/24 13:35:30] mask2former INFO: Inference done 1074/1092. Dataloading: 0.0125 s/iter. Inference: 0.3771 s/iter. Eval: 0.2500 s/iter. Total: 0.6399 s/iter. ETA=0:00:11
[02/24 13:35:35] mask2former INFO: Inference done 1083/1092. Dataloading: 0.0124 s/iter. Inference: 0.3769 s/iter. Eval: 0.2499 s/iter. Total: 0.6396 s/iter. ETA=0:00:05
[02/24 13:35:40] mask2former INFO: Inference done 1091/1092. Dataloading: 0.0124 s/iter. Inference: 0.3769 s/iter. Eval: 0.2500 s/iter. Total: 0.6395 s/iter. ETA=0:00:00
[02/24 14:47:16] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 14:47:17] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 14:47:17] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 14:47:42] mask2former INFO: Inference done 11/1092. Dataloading: 0.0071 s/iter. Inference: 0.3691 s/iter. Eval: 0.2593 s/iter. Total: 0.6355 s/iter. ETA=0:11:27
[02/24 14:47:47] mask2former INFO: Inference done 18/1092. Dataloading: 0.0085 s/iter. Inference: 0.3789 s/iter. Eval: 0.2965 s/iter. Total: 0.6840 s/iter. ETA=0:12:14
[02/24 14:47:52] mask2former INFO: Inference done 25/1092. Dataloading: 0.0098 s/iter. Inference: 0.3975 s/iter. Eval: 0.2954 s/iter. Total: 0.7028 s/iter. ETA=0:12:29
[02/24 14:47:57] mask2former INFO: Inference done 33/1092. Dataloading: 0.0122 s/iter. Inference: 0.3940 s/iter. Eval: 0.2772 s/iter. Total: 0.6835 s/iter. ETA=0:12:03
[02/24 14:48:02] mask2former INFO: Inference done 41/1092. Dataloading: 0.0130 s/iter. Inference: 0.3941 s/iter. Eval: 0.2756 s/iter. Total: 0.6829 s/iter. ETA=0:11:57
[02/24 14:48:07] mask2former INFO: Inference done 48/1092. Dataloading: 0.0132 s/iter. Inference: 0.4008 s/iter. Eval: 0.2742 s/iter. Total: 0.6883 s/iter. ETA=0:11:58
[02/24 14:48:13] mask2former INFO: Inference done 55/1092. Dataloading: 0.0137 s/iter. Inference: 0.4049 s/iter. Eval: 0.2765 s/iter. Total: 0.6952 s/iter. ETA=0:12:00
[02/24 14:48:18] mask2former INFO: Inference done 63/1092. Dataloading: 0.0132 s/iter. Inference: 0.4052 s/iter. Eval: 0.2755 s/iter. Total: 0.6941 s/iter. ETA=0:11:54
[02/24 14:48:24] mask2former INFO: Inference done 71/1092. Dataloading: 0.0126 s/iter. Inference: 0.4072 s/iter. Eval: 0.2745 s/iter. Total: 0.6944 s/iter. ETA=0:11:49
[02/24 14:48:29] mask2former INFO: Inference done 79/1092. Dataloading: 0.0125 s/iter. Inference: 0.4066 s/iter. Eval: 0.2743 s/iter. Total: 0.6935 s/iter. ETA=0:11:42
[02/24 14:48:34] mask2former INFO: Inference done 87/1092. Dataloading: 0.0125 s/iter. Inference: 0.4028 s/iter. Eval: 0.2717 s/iter. Total: 0.6872 s/iter. ETA=0:11:30
[02/24 14:48:39] mask2former INFO: Inference done 99/1092. Dataloading: 0.0119 s/iter. Inference: 0.3871 s/iter. Eval: 0.2557 s/iter. Total: 0.6548 s/iter. ETA=0:10:50
[02/24 14:48:46] mask2former INFO: Inference done 105/1092. Dataloading: 0.0129 s/iter. Inference: 0.4006 s/iter. Eval: 0.2633 s/iter. Total: 0.6769 s/iter. ETA=0:11:08
[02/24 14:48:51] mask2former INFO: Inference done 110/1092. Dataloading: 0.0131 s/iter. Inference: 0.4096 s/iter. Eval: 0.2715 s/iter. Total: 0.6944 s/iter. ETA=0:11:21
[02/24 14:48:56] mask2former INFO: Inference done 118/1092. Dataloading: 0.0128 s/iter. Inference: 0.4063 s/iter. Eval: 0.2712 s/iter. Total: 0.6906 s/iter. ETA=0:11:12
[02/24 14:49:01] mask2former INFO: Inference done 123/1092. Dataloading: 0.0130 s/iter. Inference: 0.4143 s/iter. Eval: 0.2762 s/iter. Total: 0.7038 s/iter. ETA=0:11:21
[02/24 14:49:06] mask2former INFO: Inference done 130/1092. Dataloading: 0.0133 s/iter. Inference: 0.4151 s/iter. Eval: 0.2786 s/iter. Total: 0.7073 s/iter. ETA=0:11:20
[02/24 14:49:12] mask2former INFO: Inference done 138/1092. Dataloading: 0.0131 s/iter. Inference: 0.4152 s/iter. Eval: 0.2767 s/iter. Total: 0.7052 s/iter. ETA=0:11:12
[02/24 14:49:17] mask2former INFO: Inference done 146/1092. Dataloading: 0.0129 s/iter. Inference: 0.4152 s/iter. Eval: 0.2767 s/iter. Total: 0.7051 s/iter. ETA=0:11:06
[02/24 14:49:23] mask2former INFO: Inference done 154/1092. Dataloading: 0.0128 s/iter. Inference: 0.4158 s/iter. Eval: 0.2748 s/iter. Total: 0.7036 s/iter. ETA=0:11:00
[02/24 14:49:28] mask2former INFO: Inference done 163/1092. Dataloading: 0.0127 s/iter. Inference: 0.4143 s/iter. Eval: 0.2716 s/iter. Total: 0.6988 s/iter. ETA=0:10:49
[02/24 14:49:34] mask2former INFO: Inference done 171/1092. Dataloading: 0.0127 s/iter. Inference: 0.4125 s/iter. Eval: 0.2711 s/iter. Total: 0.6966 s/iter. ETA=0:10:41
[02/24 14:49:39] mask2former INFO: Inference done 179/1092. Dataloading: 0.0125 s/iter. Inference: 0.4110 s/iter. Eval: 0.2705 s/iter. Total: 0.6943 s/iter. ETA=0:10:33
[02/24 14:49:44] mask2former INFO: Inference done 187/1092. Dataloading: 0.0124 s/iter. Inference: 0.4117 s/iter. Eval: 0.2691 s/iter. Total: 0.6934 s/iter. ETA=0:10:27
[02/24 14:49:50] mask2former INFO: Inference done 196/1092. Dataloading: 0.0127 s/iter. Inference: 0.4103 s/iter. Eval: 0.2675 s/iter. Total: 0.6907 s/iter. ETA=0:10:18
[02/24 14:49:55] mask2former INFO: Inference done 204/1092. Dataloading: 0.0129 s/iter. Inference: 0.4093 s/iter. Eval: 0.2664 s/iter. Total: 0.6889 s/iter. ETA=0:10:11
[02/24 14:50:00] mask2former INFO: Inference done 212/1092. Dataloading: 0.0129 s/iter. Inference: 0.4077 s/iter. Eval: 0.2665 s/iter. Total: 0.6875 s/iter. ETA=0:10:04
[02/24 14:50:06] mask2former INFO: Inference done 221/1092. Dataloading: 0.0131 s/iter. Inference: 0.4064 s/iter. Eval: 0.2659 s/iter. Total: 0.6855 s/iter. ETA=0:09:57
[02/24 14:50:11] mask2former INFO: Inference done 229/1092. Dataloading: 0.0130 s/iter. Inference: 0.4067 s/iter. Eval: 0.2649 s/iter. Total: 0.6849 s/iter. ETA=0:09:51
[02/24 14:50:17] mask2former INFO: Inference done 237/1092. Dataloading: 0.0130 s/iter. Inference: 0.4064 s/iter. Eval: 0.2656 s/iter. Total: 0.6853 s/iter. ETA=0:09:45
[02/24 14:50:22] mask2former INFO: Inference done 246/1092. Dataloading: 0.0129 s/iter. Inference: 0.4051 s/iter. Eval: 0.2647 s/iter. Total: 0.6830 s/iter. ETA=0:09:37
[02/24 14:50:28] mask2former INFO: Inference done 254/1092. Dataloading: 0.0129 s/iter. Inference: 0.4041 s/iter. Eval: 0.2644 s/iter. Total: 0.6817 s/iter. ETA=0:09:31
[02/24 14:50:33] mask2former INFO: Inference done 262/1092. Dataloading: 0.0130 s/iter. Inference: 0.4027 s/iter. Eval: 0.2649 s/iter. Total: 0.6809 s/iter. ETA=0:09:25
[02/24 14:50:38] mask2former INFO: Inference done 270/1092. Dataloading: 0.0131 s/iter. Inference: 0.4030 s/iter. Eval: 0.2639 s/iter. Total: 0.6802 s/iter. ETA=0:09:19
[02/24 14:50:44] mask2former INFO: Inference done 278/1092. Dataloading: 0.0130 s/iter. Inference: 0.4030 s/iter. Eval: 0.2639 s/iter. Total: 0.6801 s/iter. ETA=0:09:13
[02/24 14:50:49] mask2former INFO: Inference done 286/1092. Dataloading: 0.0130 s/iter. Inference: 0.4030 s/iter. Eval: 0.2648 s/iter. Total: 0.6810 s/iter. ETA=0:09:08
[02/24 14:50:55] mask2former INFO: Inference done 294/1092. Dataloading: 0.0129 s/iter. Inference: 0.4035 s/iter. Eval: 0.2646 s/iter. Total: 0.6813 s/iter. ETA=0:09:03
[02/24 14:51:00] mask2former INFO: Inference done 302/1092. Dataloading: 0.0128 s/iter. Inference: 0.4036 s/iter. Eval: 0.2647 s/iter. Total: 0.6814 s/iter. ETA=0:08:58
[02/24 14:51:06] mask2former INFO: Inference done 310/1092. Dataloading: 0.0127 s/iter. Inference: 0.4032 s/iter. Eval: 0.2645 s/iter. Total: 0.6808 s/iter. ETA=0:08:52
[02/24 14:51:11] mask2former INFO: Inference done 319/1092. Dataloading: 0.0127 s/iter. Inference: 0.4022 s/iter. Eval: 0.2635 s/iter. Total: 0.6788 s/iter. ETA=0:08:44
[02/24 14:51:17] mask2former INFO: Inference done 328/1092. Dataloading: 0.0127 s/iter. Inference: 0.4008 s/iter. Eval: 0.2635 s/iter. Total: 0.6773 s/iter. ETA=0:08:37
[02/24 14:51:22] mask2former INFO: Inference done 336/1092. Dataloading: 0.0127 s/iter. Inference: 0.4004 s/iter. Eval: 0.2633 s/iter. Total: 0.6766 s/iter. ETA=0:08:31
[02/24 14:51:27] mask2former INFO: Inference done 344/1092. Dataloading: 0.0127 s/iter. Inference: 0.3999 s/iter. Eval: 0.2632 s/iter. Total: 0.6761 s/iter. ETA=0:08:25
[02/24 14:51:32] mask2former INFO: Inference done 352/1092. Dataloading: 0.0126 s/iter. Inference: 0.3996 s/iter. Eval: 0.2631 s/iter. Total: 0.6756 s/iter. ETA=0:08:19
[02/24 14:51:37] mask2former INFO: Inference done 360/1092. Dataloading: 0.0126 s/iter. Inference: 0.3988 s/iter. Eval: 0.2630 s/iter. Total: 0.6747 s/iter. ETA=0:08:13
[02/24 14:51:43] mask2former INFO: Inference done 368/1092. Dataloading: 0.0125 s/iter. Inference: 0.3988 s/iter. Eval: 0.2638 s/iter. Total: 0.6754 s/iter. ETA=0:08:09
[02/24 14:51:49] mask2former INFO: Inference done 376/1092. Dataloading: 0.0125 s/iter. Inference: 0.3996 s/iter. Eval: 0.2636 s/iter. Total: 0.6761 s/iter. ETA=0:08:04
[02/24 14:51:54] mask2former INFO: Inference done 384/1092. Dataloading: 0.0125 s/iter. Inference: 0.3996 s/iter. Eval: 0.2639 s/iter. Total: 0.6763 s/iter. ETA=0:07:58
[02/24 14:52:00] mask2former INFO: Inference done 392/1092. Dataloading: 0.0126 s/iter. Inference: 0.3999 s/iter. Eval: 0.2635 s/iter. Total: 0.6763 s/iter. ETA=0:07:53
[02/24 14:52:05] mask2former INFO: Inference done 400/1092. Dataloading: 0.0125 s/iter. Inference: 0.3994 s/iter. Eval: 0.2635 s/iter. Total: 0.6757 s/iter. ETA=0:07:47
[02/24 14:52:10] mask2former INFO: Inference done 409/1092. Dataloading: 0.0125 s/iter. Inference: 0.3988 s/iter. Eval: 0.2624 s/iter. Total: 0.6740 s/iter. ETA=0:07:40
[02/24 14:52:15] mask2former INFO: Inference done 417/1092. Dataloading: 0.0124 s/iter. Inference: 0.3985 s/iter. Eval: 0.2621 s/iter. Total: 0.6733 s/iter. ETA=0:07:34
[02/24 14:52:21] mask2former INFO: Inference done 425/1092. Dataloading: 0.0125 s/iter. Inference: 0.3985 s/iter. Eval: 0.2618 s/iter. Total: 0.6731 s/iter. ETA=0:07:28
[02/24 14:52:26] mask2former INFO: Inference done 433/1092. Dataloading: 0.0125 s/iter. Inference: 0.3981 s/iter. Eval: 0.2615 s/iter. Total: 0.6724 s/iter. ETA=0:07:23
[02/24 14:52:31] mask2former INFO: Inference done 441/1092. Dataloading: 0.0125 s/iter. Inference: 0.3982 s/iter. Eval: 0.2612 s/iter. Total: 0.6723 s/iter. ETA=0:07:17
[02/24 14:52:36] mask2former INFO: Inference done 450/1092. Dataloading: 0.0125 s/iter. Inference: 0.3976 s/iter. Eval: 0.2604 s/iter. Total: 0.6708 s/iter. ETA=0:07:10
[02/24 14:52:41] mask2former INFO: Inference done 458/1092. Dataloading: 0.0125 s/iter. Inference: 0.3976 s/iter. Eval: 0.2596 s/iter. Total: 0.6700 s/iter. ETA=0:07:04
[02/24 14:52:47] mask2former INFO: Inference done 466/1092. Dataloading: 0.0124 s/iter. Inference: 0.3974 s/iter. Eval: 0.2597 s/iter. Total: 0.6698 s/iter. ETA=0:06:59
[02/24 14:52:52] mask2former INFO: Inference done 475/1092. Dataloading: 0.0124 s/iter. Inference: 0.3974 s/iter. Eval: 0.2585 s/iter. Total: 0.6685 s/iter. ETA=0:06:52
[02/24 14:52:57] mask2former INFO: Inference done 483/1092. Dataloading: 0.0124 s/iter. Inference: 0.3967 s/iter. Eval: 0.2592 s/iter. Total: 0.6686 s/iter. ETA=0:06:47
[02/24 14:53:03] mask2former INFO: Inference done 491/1092. Dataloading: 0.0123 s/iter. Inference: 0.3967 s/iter. Eval: 0.2592 s/iter. Total: 0.6685 s/iter. ETA=0:06:41
[02/24 14:53:08] mask2former INFO: Inference done 499/1092. Dataloading: 0.0123 s/iter. Inference: 0.3958 s/iter. Eval: 0.2595 s/iter. Total: 0.6679 s/iter. ETA=0:06:36
[02/24 14:53:13] mask2former INFO: Inference done 506/1092. Dataloading: 0.0124 s/iter. Inference: 0.3970 s/iter. Eval: 0.2593 s/iter. Total: 0.6689 s/iter. ETA=0:06:31
[02/24 14:53:19] mask2former INFO: Inference done 514/1092. Dataloading: 0.0123 s/iter. Inference: 0.3978 s/iter. Eval: 0.2595 s/iter. Total: 0.6699 s/iter. ETA=0:06:27
[02/24 14:53:24] mask2former INFO: Inference done 522/1092. Dataloading: 0.0123 s/iter. Inference: 0.3979 s/iter. Eval: 0.2596 s/iter. Total: 0.6701 s/iter. ETA=0:06:21
[02/24 14:53:30] mask2former INFO: Inference done 530/1092. Dataloading: 0.0123 s/iter. Inference: 0.3981 s/iter. Eval: 0.2593 s/iter. Total: 0.6700 s/iter. ETA=0:06:16
[02/24 14:53:35] mask2former INFO: Inference done 539/1092. Dataloading: 0.0122 s/iter. Inference: 0.3971 s/iter. Eval: 0.2589 s/iter. Total: 0.6685 s/iter. ETA=0:06:09
[02/24 14:53:40] mask2former INFO: Inference done 547/1092. Dataloading: 0.0122 s/iter. Inference: 0.3974 s/iter. Eval: 0.2584 s/iter. Total: 0.6683 s/iter. ETA=0:06:04
[02/24 14:53:45] mask2former INFO: Inference done 555/1092. Dataloading: 0.0121 s/iter. Inference: 0.3970 s/iter. Eval: 0.2584 s/iter. Total: 0.6679 s/iter. ETA=0:05:58
[02/24 14:53:50] mask2former INFO: Inference done 563/1092. Dataloading: 0.0121 s/iter. Inference: 0.3966 s/iter. Eval: 0.2585 s/iter. Total: 0.6675 s/iter. ETA=0:05:53
[02/24 14:53:56] mask2former INFO: Inference done 571/1092. Dataloading: 0.0121 s/iter. Inference: 0.3969 s/iter. Eval: 0.2583 s/iter. Total: 0.6675 s/iter. ETA=0:05:47
[02/24 14:54:01] mask2former INFO: Inference done 579/1092. Dataloading: 0.0122 s/iter. Inference: 0.3972 s/iter. Eval: 0.2579 s/iter. Total: 0.6676 s/iter. ETA=0:05:42
[02/24 14:54:07] mask2former INFO: Inference done 587/1092. Dataloading: 0.0122 s/iter. Inference: 0.3976 s/iter. Eval: 0.2578 s/iter. Total: 0.6680 s/iter. ETA=0:05:37
[02/24 14:54:12] mask2former INFO: Inference done 595/1092. Dataloading: 0.0122 s/iter. Inference: 0.3976 s/iter. Eval: 0.2579 s/iter. Total: 0.6680 s/iter. ETA=0:05:32
[02/24 14:54:17] mask2former INFO: Inference done 603/1092. Dataloading: 0.0122 s/iter. Inference: 0.3974 s/iter. Eval: 0.2577 s/iter. Total: 0.6678 s/iter. ETA=0:05:26
[02/24 14:54:23] mask2former INFO: Inference done 611/1092. Dataloading: 0.0123 s/iter. Inference: 0.3970 s/iter. Eval: 0.2583 s/iter. Total: 0.6679 s/iter. ETA=0:05:21
[02/24 14:54:28] mask2former INFO: Inference done 619/1092. Dataloading: 0.0123 s/iter. Inference: 0.3971 s/iter. Eval: 0.2585 s/iter. Total: 0.6682 s/iter. ETA=0:05:16
[02/24 14:54:33] mask2former INFO: Inference done 627/1092. Dataloading: 0.0122 s/iter. Inference: 0.3972 s/iter. Eval: 0.2584 s/iter. Total: 0.6681 s/iter. ETA=0:05:10
[02/24 14:54:39] mask2former INFO: Inference done 635/1092. Dataloading: 0.0121 s/iter. Inference: 0.3974 s/iter. Eval: 0.2585 s/iter. Total: 0.6684 s/iter. ETA=0:05:05
[02/24 14:54:44] mask2former INFO: Inference done 643/1092. Dataloading: 0.0122 s/iter. Inference: 0.3974 s/iter. Eval: 0.2580 s/iter. Total: 0.6680 s/iter. ETA=0:04:59
[02/24 14:54:49] mask2former INFO: Inference done 651/1092. Dataloading: 0.0123 s/iter. Inference: 0.3972 s/iter. Eval: 0.2578 s/iter. Total: 0.6676 s/iter. ETA=0:04:54
[02/24 14:54:54] mask2former INFO: Inference done 659/1092. Dataloading: 0.0123 s/iter. Inference: 0.3977 s/iter. Eval: 0.2573 s/iter. Total: 0.6676 s/iter. ETA=0:04:49
[02/24 14:55:00] mask2former INFO: Inference done 666/1092. Dataloading: 0.0123 s/iter. Inference: 0.3979 s/iter. Eval: 0.2580 s/iter. Total: 0.6685 s/iter. ETA=0:04:44
[02/24 14:55:05] mask2former INFO: Inference done 674/1092. Dataloading: 0.0123 s/iter. Inference: 0.3980 s/iter. Eval: 0.2577 s/iter. Total: 0.6684 s/iter. ETA=0:04:39
[02/24 14:55:11] mask2former INFO: Inference done 682/1092. Dataloading: 0.0124 s/iter. Inference: 0.3981 s/iter. Eval: 0.2579 s/iter. Total: 0.6687 s/iter. ETA=0:04:34
[02/24 14:55:16] mask2former INFO: Inference done 690/1092. Dataloading: 0.0124 s/iter. Inference: 0.3980 s/iter. Eval: 0.2579 s/iter. Total: 0.6686 s/iter. ETA=0:04:28
[02/24 14:55:21] mask2former INFO: Inference done 700/1092. Dataloading: 0.0123 s/iter. Inference: 0.3969 s/iter. Eval: 0.2570 s/iter. Total: 0.6666 s/iter. ETA=0:04:21
[02/24 14:55:27] mask2former INFO: Inference done 710/1092. Dataloading: 0.0123 s/iter. Inference: 0.3957 s/iter. Eval: 0.2564 s/iter. Total: 0.6648 s/iter. ETA=0:04:13
[02/24 14:55:32] mask2former INFO: Inference done 716/1092. Dataloading: 0.0124 s/iter. Inference: 0.3965 s/iter. Eval: 0.2575 s/iter. Total: 0.6668 s/iter. ETA=0:04:10
[02/24 14:55:37] mask2former INFO: Inference done 722/1092. Dataloading: 0.0124 s/iter. Inference: 0.3974 s/iter. Eval: 0.2584 s/iter. Total: 0.6685 s/iter. ETA=0:04:07
[02/24 14:55:43] mask2former INFO: Inference done 732/1092. Dataloading: 0.0123 s/iter. Inference: 0.3970 s/iter. Eval: 0.2580 s/iter. Total: 0.6676 s/iter. ETA=0:04:00
[02/24 14:55:48] mask2former INFO: Inference done 739/1092. Dataloading: 0.0124 s/iter. Inference: 0.3972 s/iter. Eval: 0.2582 s/iter. Total: 0.6681 s/iter. ETA=0:03:55
[02/24 14:55:53] mask2former INFO: Inference done 747/1092. Dataloading: 0.0124 s/iter. Inference: 0.3971 s/iter. Eval: 0.2579 s/iter. Total: 0.6677 s/iter. ETA=0:03:50
[02/24 14:55:58] mask2former INFO: Inference done 755/1092. Dataloading: 0.0124 s/iter. Inference: 0.3968 s/iter. Eval: 0.2578 s/iter. Total: 0.6674 s/iter. ETA=0:03:44
[02/24 14:56:04] mask2former INFO: Inference done 763/1092. Dataloading: 0.0123 s/iter. Inference: 0.3969 s/iter. Eval: 0.2575 s/iter. Total: 0.6672 s/iter. ETA=0:03:39
[02/24 14:56:09] mask2former INFO: Inference done 771/1092. Dataloading: 0.0124 s/iter. Inference: 0.3965 s/iter. Eval: 0.2575 s/iter. Total: 0.6667 s/iter. ETA=0:03:34
[02/24 14:56:14] mask2former INFO: Inference done 779/1092. Dataloading: 0.0124 s/iter. Inference: 0.3967 s/iter. Eval: 0.2571 s/iter. Total: 0.6666 s/iter. ETA=0:03:28
[02/24 14:56:19] mask2former INFO: Inference done 787/1092. Dataloading: 0.0124 s/iter. Inference: 0.3968 s/iter. Eval: 0.2569 s/iter. Total: 0.6666 s/iter. ETA=0:03:23
[02/24 14:56:24] mask2former INFO: Inference done 794/1092. Dataloading: 0.0124 s/iter. Inference: 0.3971 s/iter. Eval: 0.2573 s/iter. Total: 0.6672 s/iter. ETA=0:03:18
[02/24 14:56:29] mask2former INFO: Inference done 802/1092. Dataloading: 0.0124 s/iter. Inference: 0.3969 s/iter. Eval: 0.2572 s/iter. Total: 0.6669 s/iter. ETA=0:03:13
[02/24 14:56:35] mask2former INFO: Inference done 810/1092. Dataloading: 0.0124 s/iter. Inference: 0.3970 s/iter. Eval: 0.2573 s/iter. Total: 0.6670 s/iter. ETA=0:03:08
[02/24 14:56:40] mask2former INFO: Inference done 818/1092. Dataloading: 0.0123 s/iter. Inference: 0.3970 s/iter. Eval: 0.2572 s/iter. Total: 0.6670 s/iter. ETA=0:03:02
[02/24 14:56:46] mask2former INFO: Inference done 827/1092. Dataloading: 0.0124 s/iter. Inference: 0.3970 s/iter. Eval: 0.2569 s/iter. Total: 0.6666 s/iter. ETA=0:02:56
[02/24 14:56:51] mask2former INFO: Inference done 834/1092. Dataloading: 0.0124 s/iter. Inference: 0.3972 s/iter. Eval: 0.2572 s/iter. Total: 0.6673 s/iter. ETA=0:02:52
[02/24 14:56:56] mask2former INFO: Inference done 842/1092. Dataloading: 0.0125 s/iter. Inference: 0.3976 s/iter. Eval: 0.2569 s/iter. Total: 0.6673 s/iter. ETA=0:02:46
[02/24 14:57:02] mask2former INFO: Inference done 850/1092. Dataloading: 0.0125 s/iter. Inference: 0.3976 s/iter. Eval: 0.2568 s/iter. Total: 0.6673 s/iter. ETA=0:02:41
[02/24 14:57:07] mask2former INFO: Inference done 858/1092. Dataloading: 0.0124 s/iter. Inference: 0.3977 s/iter. Eval: 0.2565 s/iter. Total: 0.6670 s/iter. ETA=0:02:36
[02/24 14:57:12] mask2former INFO: Inference done 866/1092. Dataloading: 0.0124 s/iter. Inference: 0.3977 s/iter. Eval: 0.2563 s/iter. Total: 0.6668 s/iter. ETA=0:02:30
[02/24 14:57:17] mask2former INFO: Inference done 874/1092. Dataloading: 0.0124 s/iter. Inference: 0.3975 s/iter. Eval: 0.2564 s/iter. Total: 0.6667 s/iter. ETA=0:02:25
[02/24 14:57:22] mask2former INFO: Inference done 884/1092. Dataloading: 0.0124 s/iter. Inference: 0.3967 s/iter. Eval: 0.2556 s/iter. Total: 0.6651 s/iter. ETA=0:02:18
[02/24 14:57:28] mask2former INFO: Inference done 892/1092. Dataloading: 0.0123 s/iter. Inference: 0.3969 s/iter. Eval: 0.2553 s/iter. Total: 0.6649 s/iter. ETA=0:02:12
[02/24 14:57:33] mask2former INFO: Inference done 900/1092. Dataloading: 0.0123 s/iter. Inference: 0.3973 s/iter. Eval: 0.2551 s/iter. Total: 0.6651 s/iter. ETA=0:02:07
[02/24 14:57:39] mask2former INFO: Inference done 908/1092. Dataloading: 0.0123 s/iter. Inference: 0.3970 s/iter. Eval: 0.2556 s/iter. Total: 0.6653 s/iter. ETA=0:02:02
[02/24 14:57:44] mask2former INFO: Inference done 916/1092. Dataloading: 0.0123 s/iter. Inference: 0.3971 s/iter. Eval: 0.2555 s/iter. Total: 0.6653 s/iter. ETA=0:01:57
[02/24 14:57:49] mask2former INFO: Inference done 924/1092. Dataloading: 0.0123 s/iter. Inference: 0.3971 s/iter. Eval: 0.2556 s/iter. Total: 0.6653 s/iter. ETA=0:01:51
[02/24 14:57:55] mask2former INFO: Inference done 932/1092. Dataloading: 0.0124 s/iter. Inference: 0.3973 s/iter. Eval: 0.2554 s/iter. Total: 0.6654 s/iter. ETA=0:01:46
[02/24 14:58:00] mask2former INFO: Inference done 940/1092. Dataloading: 0.0124 s/iter. Inference: 0.3969 s/iter. Eval: 0.2556 s/iter. Total: 0.6653 s/iter. ETA=0:01:41
[02/24 14:58:05] mask2former INFO: Inference done 948/1092. Dataloading: 0.0124 s/iter. Inference: 0.3969 s/iter. Eval: 0.2554 s/iter. Total: 0.6651 s/iter. ETA=0:01:35
[02/24 14:58:10] mask2former INFO: Inference done 955/1092. Dataloading: 0.0124 s/iter. Inference: 0.3971 s/iter. Eval: 0.2556 s/iter. Total: 0.6655 s/iter. ETA=0:01:31
[02/24 14:58:16] mask2former INFO: Inference done 963/1092. Dataloading: 0.0124 s/iter. Inference: 0.3971 s/iter. Eval: 0.2559 s/iter. Total: 0.6657 s/iter. ETA=0:01:25
[02/24 14:58:21] mask2former INFO: Inference done 971/1092. Dataloading: 0.0124 s/iter. Inference: 0.3971 s/iter. Eval: 0.2558 s/iter. Total: 0.6656 s/iter. ETA=0:01:20
[02/24 14:58:26] mask2former INFO: Inference done 978/1092. Dataloading: 0.0123 s/iter. Inference: 0.3974 s/iter. Eval: 0.2560 s/iter. Total: 0.6661 s/iter. ETA=0:01:15
[02/24 14:58:31] mask2former INFO: Inference done 986/1092. Dataloading: 0.0123 s/iter. Inference: 0.3972 s/iter. Eval: 0.2561 s/iter. Total: 0.6661 s/iter. ETA=0:01:10
[02/24 14:58:36] mask2former INFO: Inference done 994/1092. Dataloading: 0.0123 s/iter. Inference: 0.3973 s/iter. Eval: 0.2560 s/iter. Total: 0.6659 s/iter. ETA=0:01:05
[02/24 14:58:42] mask2former INFO: Inference done 1001/1092. Dataloading: 0.0124 s/iter. Inference: 0.3975 s/iter. Eval: 0.2562 s/iter. Total: 0.6664 s/iter. ETA=0:01:00
[02/24 14:58:47] mask2former INFO: Inference done 1008/1092. Dataloading: 0.0124 s/iter. Inference: 0.3977 s/iter. Eval: 0.2562 s/iter. Total: 0.6667 s/iter. ETA=0:00:56
[02/24 14:58:52] mask2former INFO: Inference done 1015/1092. Dataloading: 0.0124 s/iter. Inference: 0.3979 s/iter. Eval: 0.2564 s/iter. Total: 0.6671 s/iter. ETA=0:00:51
[02/24 14:58:57] mask2former INFO: Inference done 1023/1092. Dataloading: 0.0124 s/iter. Inference: 0.3979 s/iter. Eval: 0.2562 s/iter. Total: 0.6669 s/iter. ETA=0:00:46
[02/24 14:59:02] mask2former INFO: Inference done 1031/1092. Dataloading: 0.0124 s/iter. Inference: 0.3981 s/iter. Eval: 0.2561 s/iter. Total: 0.6670 s/iter. ETA=0:00:40
[02/24 14:59:08] mask2former INFO: Inference done 1039/1092. Dataloading: 0.0124 s/iter. Inference: 0.3980 s/iter. Eval: 0.2563 s/iter. Total: 0.6671 s/iter. ETA=0:00:35
[02/24 14:59:13] mask2former INFO: Inference done 1047/1092. Dataloading: 0.0124 s/iter. Inference: 0.3981 s/iter. Eval: 0.2564 s/iter. Total: 0.6673 s/iter. ETA=0:00:30
[02/24 14:59:19] mask2former INFO: Inference done 1055/1092. Dataloading: 0.0124 s/iter. Inference: 0.3981 s/iter. Eval: 0.2565 s/iter. Total: 0.6674 s/iter. ETA=0:00:24
[02/24 14:59:24] mask2former INFO: Inference done 1063/1092. Dataloading: 0.0124 s/iter. Inference: 0.3979 s/iter. Eval: 0.2565 s/iter. Total: 0.6672 s/iter. ETA=0:00:19
[02/24 14:59:29] mask2former INFO: Inference done 1071/1092. Dataloading: 0.0124 s/iter. Inference: 0.3978 s/iter. Eval: 0.2565 s/iter. Total: 0.6671 s/iter. ETA=0:00:14
[02/24 14:59:34] mask2former INFO: Inference done 1079/1092. Dataloading: 0.0124 s/iter. Inference: 0.3975 s/iter. Eval: 0.2565 s/iter. Total: 0.6668 s/iter. ETA=0:00:08
[02/24 14:59:39] mask2former INFO: Inference done 1086/1092. Dataloading: 0.0124 s/iter. Inference: 0.3978 s/iter. Eval: 0.2565 s/iter. Total: 0.6671 s/iter. ETA=0:00:04
[02/24 16:20:44] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 16:20:47] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 16:20:48] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 16:21:42] mask2former INFO: Inference done 11/1092. Dataloading: 0.0186 s/iter. Inference: 0.9173 s/iter. Eval: 0.5333 s/iter. Total: 1.4693 s/iter. ETA=0:26:28
[02/24 16:21:47] mask2former INFO: Inference done 15/1092. Dataloading: 0.0163 s/iter. Inference: 0.8406 s/iter. Eval: 0.5411 s/iter. Total: 1.3980 s/iter. ETA=0:25:05
[02/24 16:21:54] mask2former INFO: Inference done 20/1092. Dataloading: 0.0148 s/iter. Inference: 0.8098 s/iter. Eval: 0.5344 s/iter. Total: 1.3592 s/iter. ETA=0:24:17
[02/24 16:22:00] mask2former INFO: Inference done 24/1092. Dataloading: 0.0194 s/iter. Inference: 0.8073 s/iter. Eval: 0.5455 s/iter. Total: 1.3744 s/iter. ETA=0:24:27
[02/24 16:22:05] mask2former INFO: Inference done 28/1092. Dataloading: 0.0211 s/iter. Inference: 0.8155 s/iter. Eval: 0.5290 s/iter. Total: 1.3675 s/iter. ETA=0:24:15
[02/24 16:22:11] mask2former INFO: Inference done 33/1092. Dataloading: 0.0217 s/iter. Inference: 0.8049 s/iter. Eval: 0.5185 s/iter. Total: 1.3467 s/iter. ETA=0:23:46
[02/24 16:22:16] mask2former INFO: Inference done 36/1092. Dataloading: 0.0219 s/iter. Inference: 0.8204 s/iter. Eval: 0.5408 s/iter. Total: 1.3846 s/iter. ETA=0:24:22
[02/24 16:22:22] mask2former INFO: Inference done 40/1092. Dataloading: 0.0225 s/iter. Inference: 0.8245 s/iter. Eval: 0.5442 s/iter. Total: 1.3933 s/iter. ETA=0:24:25
[02/24 16:22:27] mask2former INFO: Inference done 44/1092. Dataloading: 0.0223 s/iter. Inference: 0.8249 s/iter. Eval: 0.5366 s/iter. Total: 1.3856 s/iter. ETA=0:24:12
[02/24 16:22:33] mask2former INFO: Inference done 48/1092. Dataloading: 0.0220 s/iter. Inference: 0.8170 s/iter. Eval: 0.5353 s/iter. Total: 1.3759 s/iter. ETA=0:23:56
[02/24 16:22:39] mask2former INFO: Inference done 53/1092. Dataloading: 0.0213 s/iter. Inference: 0.8097 s/iter. Eval: 0.5265 s/iter. Total: 1.3594 s/iter. ETA=0:23:32
[02/24 16:22:44] mask2former INFO: Inference done 57/1092. Dataloading: 0.0215 s/iter. Inference: 0.8085 s/iter. Eval: 0.5267 s/iter. Total: 1.3585 s/iter. ETA=0:23:26
[02/24 16:22:50] mask2former INFO: Inference done 61/1092. Dataloading: 0.0212 s/iter. Inference: 0.8102 s/iter. Eval: 0.5322 s/iter. Total: 1.3652 s/iter. ETA=0:23:27
[02/24 16:22:55] mask2former INFO: Inference done 65/1092. Dataloading: 0.0205 s/iter. Inference: 0.8124 s/iter. Eval: 0.5233 s/iter. Total: 1.3579 s/iter. ETA=0:23:14
[02/24 16:23:01] mask2former INFO: Inference done 69/1092. Dataloading: 0.0210 s/iter. Inference: 0.8215 s/iter. Eval: 0.5280 s/iter. Total: 1.3720 s/iter. ETA=0:23:23
[02/24 16:23:07] mask2former INFO: Inference done 73/1092. Dataloading: 0.0214 s/iter. Inference: 0.8292 s/iter. Eval: 0.5294 s/iter. Total: 1.3815 s/iter. ETA=0:23:27
[02/24 16:23:13] mask2former INFO: Inference done 77/1092. Dataloading: 0.0213 s/iter. Inference: 0.8290 s/iter. Eval: 0.5335 s/iter. Total: 1.3851 s/iter. ETA=0:23:25
[02/24 16:23:19] mask2former INFO: Inference done 81/1092. Dataloading: 0.0219 s/iter. Inference: 0.8286 s/iter. Eval: 0.5402 s/iter. Total: 1.3921 s/iter. ETA=0:23:27
[02/24 16:23:25] mask2former INFO: Inference done 85/1092. Dataloading: 0.0220 s/iter. Inference: 0.8255 s/iter. Eval: 0.5472 s/iter. Total: 1.3959 s/iter. ETA=0:23:25
[02/24 16:23:31] mask2former INFO: Inference done 89/1092. Dataloading: 0.0225 s/iter. Inference: 0.8263 s/iter. Eval: 0.5502 s/iter. Total: 1.4002 s/iter. ETA=0:23:24
[02/24 16:23:38] mask2former INFO: Inference done 93/1092. Dataloading: 0.0226 s/iter. Inference: 0.8393 s/iter. Eval: 0.5549 s/iter. Total: 1.4181 s/iter. ETA=0:23:36
[02/24 16:23:44] mask2former INFO: Inference done 97/1092. Dataloading: 0.0233 s/iter. Inference: 0.8335 s/iter. Eval: 0.5597 s/iter. Total: 1.4177 s/iter. ETA=0:23:30
[02/24 16:23:50] mask2former INFO: Inference done 101/1092. Dataloading: 0.0238 s/iter. Inference: 0.8276 s/iter. Eval: 0.5684 s/iter. Total: 1.4210 s/iter. ETA=0:23:28
[02/24 16:23:55] mask2former INFO: Inference done 106/1092. Dataloading: 0.0237 s/iter. Inference: 0.8195 s/iter. Eval: 0.5621 s/iter. Total: 1.4064 s/iter. ETA=0:23:06
[02/24 16:24:01] mask2former INFO: Inference done 111/1092. Dataloading: 0.0232 s/iter. Inference: 0.8117 s/iter. Eval: 0.5591 s/iter. Total: 1.3951 s/iter. ETA=0:22:48
[02/24 16:24:08] mask2former INFO: Inference done 115/1092. Dataloading: 0.0230 s/iter. Inference: 0.8184 s/iter. Eval: 0.5610 s/iter. Total: 1.4034 s/iter. ETA=0:22:51
[02/24 16:24:15] mask2former INFO: Inference done 119/1092. Dataloading: 0.0236 s/iter. Inference: 0.8249 s/iter. Eval: 0.5645 s/iter. Total: 1.4140 s/iter. ETA=0:22:55
[02/24 16:24:20] mask2former INFO: Inference done 123/1092. Dataloading: 0.0236 s/iter. Inference: 0.8250 s/iter. Eval: 0.5637 s/iter. Total: 1.4133 s/iter. ETA=0:22:49
[02/24 16:24:26] mask2former INFO: Inference done 127/1092. Dataloading: 0.0235 s/iter. Inference: 0.8236 s/iter. Eval: 0.5633 s/iter. Total: 1.4114 s/iter. ETA=0:22:41
[02/24 16:24:32] mask2former INFO: Inference done 131/1092. Dataloading: 0.0235 s/iter. Inference: 0.8240 s/iter. Eval: 0.5657 s/iter. Total: 1.4141 s/iter. ETA=0:22:38
[02/24 16:24:38] mask2former INFO: Inference done 135/1092. Dataloading: 0.0242 s/iter. Inference: 0.8233 s/iter. Eval: 0.5704 s/iter. Total: 1.4188 s/iter. ETA=0:22:37
[02/24 16:24:43] mask2former INFO: Inference done 138/1092. Dataloading: 0.0245 s/iter. Inference: 0.8276 s/iter. Eval: 0.5733 s/iter. Total: 1.4263 s/iter. ETA=0:22:40
[02/24 16:24:48] mask2former INFO: Inference done 142/1092. Dataloading: 0.0249 s/iter. Inference: 0.8270 s/iter. Eval: 0.5705 s/iter. Total: 1.4233 s/iter. ETA=0:22:32
[02/24 16:24:54] mask2former INFO: Inference done 146/1092. Dataloading: 0.0248 s/iter. Inference: 0.8246 s/iter. Eval: 0.5717 s/iter. Total: 1.4220 s/iter. ETA=0:22:25
[02/24 16:24:59] mask2former INFO: Inference done 150/1092. Dataloading: 0.0247 s/iter. Inference: 0.8214 s/iter. Eval: 0.5723 s/iter. Total: 1.4193 s/iter. ETA=0:22:17
[02/24 16:25:05] mask2former INFO: Inference done 154/1092. Dataloading: 0.0247 s/iter. Inference: 0.8207 s/iter. Eval: 0.5732 s/iter. Total: 1.4195 s/iter. ETA=0:22:11
[02/24 16:25:11] mask2former INFO: Inference done 158/1092. Dataloading: 0.0244 s/iter. Inference: 0.8224 s/iter. Eval: 0.5743 s/iter. Total: 1.4221 s/iter. ETA=0:22:08
[02/24 16:25:16] mask2former INFO: Inference done 162/1092. Dataloading: 0.0243 s/iter. Inference: 0.8205 s/iter. Eval: 0.5740 s/iter. Total: 1.4197 s/iter. ETA=0:22:00
[02/24 16:25:21] mask2former INFO: Inference done 166/1092. Dataloading: 0.0243 s/iter. Inference: 0.8212 s/iter. Eval: 0.5702 s/iter. Total: 1.4167 s/iter. ETA=0:21:51
[02/24 16:25:27] mask2former INFO: Inference done 170/1092. Dataloading: 0.0243 s/iter. Inference: 0.8214 s/iter. Eval: 0.5718 s/iter. Total: 1.4185 s/iter. ETA=0:21:47
[02/24 16:25:33] mask2former INFO: Inference done 174/1092. Dataloading: 0.0243 s/iter. Inference: 0.8204 s/iter. Eval: 0.5704 s/iter. Total: 1.4162 s/iter. ETA=0:21:40
[02/24 16:25:38] mask2former INFO: Inference done 177/1092. Dataloading: 0.0244 s/iter. Inference: 0.8230 s/iter. Eval: 0.5724 s/iter. Total: 1.4213 s/iter. ETA=0:21:40
[02/24 16:25:43] mask2former INFO: Inference done 181/1092. Dataloading: 0.0246 s/iter. Inference: 0.8226 s/iter. Eval: 0.5713 s/iter. Total: 1.4199 s/iter. ETA=0:21:33
[02/24 16:25:49] mask2former INFO: Inference done 187/1092. Dataloading: 0.0241 s/iter. Inference: 0.8141 s/iter. Eval: 0.5646 s/iter. Total: 1.4043 s/iter. ETA=0:21:10
[02/24 16:25:55] mask2former INFO: Inference done 192/1092. Dataloading: 0.0241 s/iter. Inference: 0.8107 s/iter. Eval: 0.5619 s/iter. Total: 1.3980 s/iter. ETA=0:20:58
[02/24 16:26:01] mask2former INFO: Inference done 196/1092. Dataloading: 0.0241 s/iter. Inference: 0.8112 s/iter. Eval: 0.5635 s/iter. Total: 1.4001 s/iter. ETA=0:20:54
[02/24 16:26:08] mask2former INFO: Inference done 201/1092. Dataloading: 0.0239 s/iter. Inference: 0.8096 s/iter. Eval: 0.5644 s/iter. Total: 1.3992 s/iter. ETA=0:20:46
[02/24 16:26:14] mask2former INFO: Inference done 206/1092. Dataloading: 0.0240 s/iter. Inference: 0.8072 s/iter. Eval: 0.5620 s/iter. Total: 1.3944 s/iter. ETA=0:20:35
[02/24 16:26:19] mask2former INFO: Inference done 210/1092. Dataloading: 0.0239 s/iter. Inference: 0.8067 s/iter. Eval: 0.5615 s/iter. Total: 1.3933 s/iter. ETA=0:20:28
[02/24 16:26:25] mask2former INFO: Inference done 215/1092. Dataloading: 0.0237 s/iter. Inference: 0.8042 s/iter. Eval: 0.5575 s/iter. Total: 1.3867 s/iter. ETA=0:20:16
[02/24 16:26:30] mask2former INFO: Inference done 219/1092. Dataloading: 0.0235 s/iter. Inference: 0.8039 s/iter. Eval: 0.5565 s/iter. Total: 1.3853 s/iter. ETA=0:20:09
[02/24 16:26:35] mask2former INFO: Inference done 222/1092. Dataloading: 0.0235 s/iter. Inference: 0.8084 s/iter. Eval: 0.5587 s/iter. Total: 1.3920 s/iter. ETA=0:20:11
[02/24 16:26:42] mask2former INFO: Inference done 227/1092. Dataloading: 0.0235 s/iter. Inference: 0.8065 s/iter. Eval: 0.5568 s/iter. Total: 1.3881 s/iter. ETA=0:20:00
[02/24 16:26:47] mask2former INFO: Inference done 231/1092. Dataloading: 0.0236 s/iter. Inference: 0.8074 s/iter. Eval: 0.5560 s/iter. Total: 1.3884 s/iter. ETA=0:19:55
[02/24 16:26:52] mask2former INFO: Inference done 234/1092. Dataloading: 0.0235 s/iter. Inference: 0.8094 s/iter. Eval: 0.5580 s/iter. Total: 1.3923 s/iter. ETA=0:19:54
[02/24 16:26:58] mask2former INFO: Inference done 239/1092. Dataloading: 0.0235 s/iter. Inference: 0.8061 s/iter. Eval: 0.5575 s/iter. Total: 1.3885 s/iter. ETA=0:19:44
[02/24 16:27:04] mask2former INFO: Inference done 243/1092. Dataloading: 0.0234 s/iter. Inference: 0.8048 s/iter. Eval: 0.5589 s/iter. Total: 1.3885 s/iter. ETA=0:19:38
[02/24 16:27:09] mask2former INFO: Inference done 246/1092. Dataloading: 0.0236 s/iter. Inference: 0.8083 s/iter. Eval: 0.5608 s/iter. Total: 1.3941 s/iter. ETA=0:19:39
[02/24 16:27:15] mask2former INFO: Inference done 250/1092. Dataloading: 0.0240 s/iter. Inference: 0.8091 s/iter. Eval: 0.5601 s/iter. Total: 1.3947 s/iter. ETA=0:19:34
[02/24 16:27:21] mask2former INFO: Inference done 254/1092. Dataloading: 0.0242 s/iter. Inference: 0.8086 s/iter. Eval: 0.5609 s/iter. Total: 1.3950 s/iter. ETA=0:19:29
[02/24 16:27:28] mask2former INFO: Inference done 258/1092. Dataloading: 0.0242 s/iter. Inference: 0.8093 s/iter. Eval: 0.5652 s/iter. Total: 1.4001 s/iter. ETA=0:19:27
[02/24 16:27:33] mask2former INFO: Inference done 261/1092. Dataloading: 0.0246 s/iter. Inference: 0.8124 s/iter. Eval: 0.5671 s/iter. Total: 1.4054 s/iter. ETA=0:19:27
[02/24 16:27:39] mask2former INFO: Inference done 265/1092. Dataloading: 0.0246 s/iter. Inference: 0.8141 s/iter. Eval: 0.5660 s/iter. Total: 1.4061 s/iter. ETA=0:19:22
[02/24 16:27:44] mask2former INFO: Inference done 269/1092. Dataloading: 0.0245 s/iter. Inference: 0.8140 s/iter. Eval: 0.5651 s/iter. Total: 1.4050 s/iter. ETA=0:19:16
[02/24 16:27:50] mask2former INFO: Inference done 273/1092. Dataloading: 0.0245 s/iter. Inference: 0.8139 s/iter. Eval: 0.5655 s/iter. Total: 1.4055 s/iter. ETA=0:19:11
[02/24 16:27:55] mask2former INFO: Inference done 276/1092. Dataloading: 0.0247 s/iter. Inference: 0.8171 s/iter. Eval: 0.5658 s/iter. Total: 1.4094 s/iter. ETA=0:19:10
[02/24 16:28:00] mask2former INFO: Inference done 280/1092. Dataloading: 0.0247 s/iter. Inference: 0.8145 s/iter. Eval: 0.5663 s/iter. Total: 1.4071 s/iter. ETA=0:19:02
[02/24 16:28:06] mask2former INFO: Inference done 284/1092. Dataloading: 0.0246 s/iter. Inference: 0.8154 s/iter. Eval: 0.5659 s/iter. Total: 1.4076 s/iter. ETA=0:18:57
[02/24 16:28:12] mask2former INFO: Inference done 288/1092. Dataloading: 0.0247 s/iter. Inference: 0.8136 s/iter. Eval: 0.5690 s/iter. Total: 1.4090 s/iter. ETA=0:18:52
[02/24 16:28:17] mask2former INFO: Inference done 291/1092. Dataloading: 0.0250 s/iter. Inference: 0.8165 s/iter. Eval: 0.5696 s/iter. Total: 1.4127 s/iter. ETA=0:18:51
[02/24 16:28:23] mask2former INFO: Inference done 296/1092. Dataloading: 0.0249 s/iter. Inference: 0.8125 s/iter. Eval: 0.5679 s/iter. Total: 1.4073 s/iter. ETA=0:18:40
[02/24 16:28:28] mask2former INFO: Inference done 299/1092. Dataloading: 0.0252 s/iter. Inference: 0.8145 s/iter. Eval: 0.5683 s/iter. Total: 1.4100 s/iter. ETA=0:18:38
[02/24 16:28:34] mask2former INFO: Inference done 303/1092. Dataloading: 0.0254 s/iter. Inference: 0.8143 s/iter. Eval: 0.5709 s/iter. Total: 1.4126 s/iter. ETA=0:18:34
[02/24 16:28:40] mask2former INFO: Inference done 306/1092. Dataloading: 0.0254 s/iter. Inference: 0.8155 s/iter. Eval: 0.5738 s/iter. Total: 1.4167 s/iter. ETA=0:18:33
[02/24 16:28:45] mask2former INFO: Inference done 309/1092. Dataloading: 0.0257 s/iter. Inference: 0.8161 s/iter. Eval: 0.5763 s/iter. Total: 1.4201 s/iter. ETA=0:18:31
[02/24 16:28:51] mask2former INFO: Inference done 313/1092. Dataloading: 0.0258 s/iter. Inference: 0.8173 s/iter. Eval: 0.5771 s/iter. Total: 1.4222 s/iter. ETA=0:18:27
[02/24 16:28:58] mask2former INFO: Inference done 317/1092. Dataloading: 0.0258 s/iter. Inference: 0.8182 s/iter. Eval: 0.5779 s/iter. Total: 1.4238 s/iter. ETA=0:18:23
[02/24 16:29:03] mask2former INFO: Inference done 321/1092. Dataloading: 0.0258 s/iter. Inference: 0.8169 s/iter. Eval: 0.5779 s/iter. Total: 1.4225 s/iter. ETA=0:18:16
[02/24 16:29:09] mask2former INFO: Inference done 325/1092. Dataloading: 0.0259 s/iter. Inference: 0.8170 s/iter. Eval: 0.5800 s/iter. Total: 1.4248 s/iter. ETA=0:18:12
[02/24 16:29:16] mask2former INFO: Inference done 329/1092. Dataloading: 0.0260 s/iter. Inference: 0.8193 s/iter. Eval: 0.5791 s/iter. Total: 1.4263 s/iter. ETA=0:18:08
[02/24 16:29:21] mask2former INFO: Inference done 333/1092. Dataloading: 0.0261 s/iter. Inference: 0.8196 s/iter. Eval: 0.5785 s/iter. Total: 1.4262 s/iter. ETA=0:18:02
[02/24 16:29:27] mask2former INFO: Inference done 337/1092. Dataloading: 0.0261 s/iter. Inference: 0.8210 s/iter. Eval: 0.5778 s/iter. Total: 1.4269 s/iter. ETA=0:17:57
[02/24 16:29:33] mask2former INFO: Inference done 341/1092. Dataloading: 0.0261 s/iter. Inference: 0.8206 s/iter. Eval: 0.5792 s/iter. Total: 1.4278 s/iter. ETA=0:17:52
[02/24 16:29:39] mask2former INFO: Inference done 345/1092. Dataloading: 0.0261 s/iter. Inference: 0.8204 s/iter. Eval: 0.5813 s/iter. Total: 1.4297 s/iter. ETA=0:17:47
[02/24 16:29:45] mask2former INFO: Inference done 349/1092. Dataloading: 0.0261 s/iter. Inference: 0.8204 s/iter. Eval: 0.5814 s/iter. Total: 1.4298 s/iter. ETA=0:17:42
[02/24 16:29:51] mask2former INFO: Inference done 352/1092. Dataloading: 0.0260 s/iter. Inference: 0.8221 s/iter. Eval: 0.5826 s/iter. Total: 1.4327 s/iter. ETA=0:17:40
[02/24 16:29:57] mask2former INFO: Inference done 355/1092. Dataloading: 0.0260 s/iter. Inference: 0.8245 s/iter. Eval: 0.5857 s/iter. Total: 1.4381 s/iter. ETA=0:17:39
[02/24 16:30:02] mask2former INFO: Inference done 358/1092. Dataloading: 0.0261 s/iter. Inference: 0.8242 s/iter. Eval: 0.5883 s/iter. Total: 1.4406 s/iter. ETA=0:17:37
[02/24 16:30:08] mask2former INFO: Inference done 362/1092. Dataloading: 0.0262 s/iter. Inference: 0.8235 s/iter. Eval: 0.5886 s/iter. Total: 1.4403 s/iter. ETA=0:17:31
[02/24 16:30:15] mask2former INFO: Inference done 366/1092. Dataloading: 0.0262 s/iter. Inference: 0.8254 s/iter. Eval: 0.5908 s/iter. Total: 1.4444 s/iter. ETA=0:17:28
[02/24 16:30:21] mask2former INFO: Inference done 371/1092. Dataloading: 0.0261 s/iter. Inference: 0.8245 s/iter. Eval: 0.5879 s/iter. Total: 1.4404 s/iter. ETA=0:17:18
[02/24 16:30:27] mask2former INFO: Inference done 375/1092. Dataloading: 0.0261 s/iter. Inference: 0.8234 s/iter. Eval: 0.5898 s/iter. Total: 1.4413 s/iter. ETA=0:17:13
[02/24 16:30:33] mask2former INFO: Inference done 379/1092. Dataloading: 0.0262 s/iter. Inference: 0.8223 s/iter. Eval: 0.5917 s/iter. Total: 1.4420 s/iter. ETA=0:17:08
[02/24 16:30:38] mask2former INFO: Inference done 383/1092. Dataloading: 0.0261 s/iter. Inference: 0.8219 s/iter. Eval: 0.5914 s/iter. Total: 1.4413 s/iter. ETA=0:17:01
[02/24 16:30:44] mask2former INFO: Inference done 387/1092. Dataloading: 0.0261 s/iter. Inference: 0.8218 s/iter. Eval: 0.5914 s/iter. Total: 1.4412 s/iter. ETA=0:16:56
[02/24 16:30:49] mask2former INFO: Inference done 390/1092. Dataloading: 0.0264 s/iter. Inference: 0.8216 s/iter. Eval: 0.5936 s/iter. Total: 1.4434 s/iter. ETA=0:16:53
[02/24 16:30:54] mask2former INFO: Inference done 393/1092. Dataloading: 0.0263 s/iter. Inference: 0.8214 s/iter. Eval: 0.5960 s/iter. Total: 1.4456 s/iter. ETA=0:16:50
[02/24 16:31:00] mask2former INFO: Inference done 397/1092. Dataloading: 0.0262 s/iter. Inference: 0.8202 s/iter. Eval: 0.5960 s/iter. Total: 1.4444 s/iter. ETA=0:16:43
[02/24 16:31:06] mask2former INFO: Inference done 401/1092. Dataloading: 0.0262 s/iter. Inference: 0.8209 s/iter. Eval: 0.5965 s/iter. Total: 1.4456 s/iter. ETA=0:16:38
[02/24 16:31:12] mask2former INFO: Inference done 406/1092. Dataloading: 0.0261 s/iter. Inference: 0.8203 s/iter. Eval: 0.5949 s/iter. Total: 1.4432 s/iter. ETA=0:16:30
[02/24 16:31:19] mask2former INFO: Inference done 410/1092. Dataloading: 0.0262 s/iter. Inference: 0.8220 s/iter. Eval: 0.5956 s/iter. Total: 1.4457 s/iter. ETA=0:16:25
[02/24 16:31:25] mask2former INFO: Inference done 414/1092. Dataloading: 0.0261 s/iter. Inference: 0.8228 s/iter. Eval: 0.5947 s/iter. Total: 1.4455 s/iter. ETA=0:16:20
[02/24 16:31:31] mask2former INFO: Inference done 418/1092. Dataloading: 0.0261 s/iter. Inference: 0.8232 s/iter. Eval: 0.5966 s/iter. Total: 1.4478 s/iter. ETA=0:16:15
[02/24 16:31:37] mask2former INFO: Inference done 421/1092. Dataloading: 0.0262 s/iter. Inference: 0.8241 s/iter. Eval: 0.5979 s/iter. Total: 1.4501 s/iter. ETA=0:16:12
[02/24 16:31:43] mask2former INFO: Inference done 425/1092. Dataloading: 0.0263 s/iter. Inference: 0.8260 s/iter. Eval: 0.5983 s/iter. Total: 1.4525 s/iter. ETA=0:16:08
[02/24 16:31:49] mask2former INFO: Inference done 429/1092. Dataloading: 0.0262 s/iter. Inference: 0.8265 s/iter. Eval: 0.5980 s/iter. Total: 1.4526 s/iter. ETA=0:16:03
[02/24 16:31:55] mask2former INFO: Inference done 433/1092. Dataloading: 0.0263 s/iter. Inference: 0.8259 s/iter. Eval: 0.5978 s/iter. Total: 1.4518 s/iter. ETA=0:15:56
[02/24 16:32:00] mask2former INFO: Inference done 437/1092. Dataloading: 0.0263 s/iter. Inference: 0.8250 s/iter. Eval: 0.5979 s/iter. Total: 1.4510 s/iter. ETA=0:15:50
[02/24 16:32:06] mask2former INFO: Inference done 441/1092. Dataloading: 0.0263 s/iter. Inference: 0.8252 s/iter. Eval: 0.5975 s/iter. Total: 1.4509 s/iter. ETA=0:15:44
[02/24 16:32:12] mask2former INFO: Inference done 445/1092. Dataloading: 0.0263 s/iter. Inference: 0.8251 s/iter. Eval: 0.5971 s/iter. Total: 1.4503 s/iter. ETA=0:15:38
[02/24 16:32:18] mask2former INFO: Inference done 449/1092. Dataloading: 0.0263 s/iter. Inference: 0.8261 s/iter. Eval: 0.5979 s/iter. Total: 1.4522 s/iter. ETA=0:15:33
[02/24 16:32:23] mask2former INFO: Inference done 453/1092. Dataloading: 0.0263 s/iter. Inference: 0.8261 s/iter. Eval: 0.5968 s/iter. Total: 1.4510 s/iter. ETA=0:15:27
[02/24 16:32:29] mask2former INFO: Inference done 458/1092. Dataloading: 0.0261 s/iter. Inference: 0.8259 s/iter. Eval: 0.5941 s/iter. Total: 1.4480 s/iter. ETA=0:15:18
[02/24 16:32:36] mask2former INFO: Inference done 462/1092. Dataloading: 0.0262 s/iter. Inference: 0.8265 s/iter. Eval: 0.5943 s/iter. Total: 1.4489 s/iter. ETA=0:15:12
[02/24 16:32:41] mask2former INFO: Inference done 466/1092. Dataloading: 0.0262 s/iter. Inference: 0.8260 s/iter. Eval: 0.5947 s/iter. Total: 1.4488 s/iter. ETA=0:15:06
[02/24 16:32:48] mask2former INFO: Inference done 471/1092. Dataloading: 0.0262 s/iter. Inference: 0.8251 s/iter. Eval: 0.5939 s/iter. Total: 1.4471 s/iter. ETA=0:14:58
[02/24 16:32:54] mask2former INFO: Inference done 475/1092. Dataloading: 0.0262 s/iter. Inference: 0.8261 s/iter. Eval: 0.5937 s/iter. Total: 1.4479 s/iter. ETA=0:14:53
[02/24 16:33:00] mask2former INFO: Inference done 480/1092. Dataloading: 0.0263 s/iter. Inference: 0.8245 s/iter. Eval: 0.5930 s/iter. Total: 1.4456 s/iter. ETA=0:14:44
[02/24 16:33:06] mask2former INFO: Inference done 484/1092. Dataloading: 0.0263 s/iter. Inference: 0.8240 s/iter. Eval: 0.5935 s/iter. Total: 1.4456 s/iter. ETA=0:14:38
[02/24 16:33:12] mask2former INFO: Inference done 488/1092. Dataloading: 0.0263 s/iter. Inference: 0.8244 s/iter. Eval: 0.5938 s/iter. Total: 1.4464 s/iter. ETA=0:14:33
[02/24 16:33:18] mask2former INFO: Inference done 491/1092. Dataloading: 0.0263 s/iter. Inference: 0.8253 s/iter. Eval: 0.5958 s/iter. Total: 1.4491 s/iter. ETA=0:14:30
[02/24 16:33:23] mask2former INFO: Inference done 495/1092. Dataloading: 0.0262 s/iter. Inference: 0.8256 s/iter. Eval: 0.5951 s/iter. Total: 1.4487 s/iter. ETA=0:14:24
[02/24 16:33:29] mask2former INFO: Inference done 498/1092. Dataloading: 0.0262 s/iter. Inference: 0.8268 s/iter. Eval: 0.5956 s/iter. Total: 1.4505 s/iter. ETA=0:14:21
[02/24 16:33:34] mask2former INFO: Inference done 502/1092. Dataloading: 0.0262 s/iter. Inference: 0.8261 s/iter. Eval: 0.5949 s/iter. Total: 1.4490 s/iter. ETA=0:14:14
[02/24 16:33:40] mask2former INFO: Inference done 506/1092. Dataloading: 0.0262 s/iter. Inference: 0.8266 s/iter. Eval: 0.5950 s/iter. Total: 1.4497 s/iter. ETA=0:14:09
[02/24 16:33:45] mask2former INFO: Inference done 510/1092. Dataloading: 0.0261 s/iter. Inference: 0.8268 s/iter. Eval: 0.5948 s/iter. Total: 1.4497 s/iter. ETA=0:14:03
[02/24 16:33:51] mask2former INFO: Inference done 513/1092. Dataloading: 0.0262 s/iter. Inference: 0.8284 s/iter. Eval: 0.5962 s/iter. Total: 1.4527 s/iter. ETA=0:14:01
[02/24 16:33:57] mask2former INFO: Inference done 517/1092. Dataloading: 0.0263 s/iter. Inference: 0.8283 s/iter. Eval: 0.5959 s/iter. Total: 1.4525 s/iter. ETA=0:13:55
[02/24 16:34:03] mask2former INFO: Inference done 522/1092. Dataloading: 0.0261 s/iter. Inference: 0.8267 s/iter. Eval: 0.5954 s/iter. Total: 1.4502 s/iter. ETA=0:13:46
[02/24 16:34:10] mask2former INFO: Inference done 526/1092. Dataloading: 0.0261 s/iter. Inference: 0.8278 s/iter. Eval: 0.5963 s/iter. Total: 1.4521 s/iter. ETA=0:13:41
[02/24 16:34:16] mask2former INFO: Inference done 530/1092. Dataloading: 0.0261 s/iter. Inference: 0.8277 s/iter. Eval: 0.5962 s/iter. Total: 1.4520 s/iter. ETA=0:13:36
[02/24 16:34:22] mask2former INFO: Inference done 534/1092. Dataloading: 0.0260 s/iter. Inference: 0.8286 s/iter. Eval: 0.5968 s/iter. Total: 1.4534 s/iter. ETA=0:13:30
[02/24 16:34:28] mask2former INFO: Inference done 538/1092. Dataloading: 0.0261 s/iter. Inference: 0.8275 s/iter. Eval: 0.5969 s/iter. Total: 1.4524 s/iter. ETA=0:13:24
[02/24 16:34:34] mask2former INFO: Inference done 542/1092. Dataloading: 0.0261 s/iter. Inference: 0.8279 s/iter. Eval: 0.5974 s/iter. Total: 1.4534 s/iter. ETA=0:13:19
[02/24 16:34:39] mask2former INFO: Inference done 545/1092. Dataloading: 0.0263 s/iter. Inference: 0.8291 s/iter. Eval: 0.5972 s/iter. Total: 1.4546 s/iter. ETA=0:13:15
[02/24 16:34:44] mask2former INFO: Inference done 548/1092. Dataloading: 0.0263 s/iter. Inference: 0.8300 s/iter. Eval: 0.5980 s/iter. Total: 1.4562 s/iter. ETA=0:13:12
[02/24 16:34:50] mask2former INFO: Inference done 552/1092. Dataloading: 0.0264 s/iter. Inference: 0.8305 s/iter. Eval: 0.5982 s/iter. Total: 1.4571 s/iter. ETA=0:13:06
[02/24 16:34:57] mask2former INFO: Inference done 556/1092. Dataloading: 0.0264 s/iter. Inference: 0.8310 s/iter. Eval: 0.5994 s/iter. Total: 1.4588 s/iter. ETA=0:13:01
[02/24 16:35:02] mask2former INFO: Inference done 560/1092. Dataloading: 0.0264 s/iter. Inference: 0.8310 s/iter. Eval: 0.5982 s/iter. Total: 1.4575 s/iter. ETA=0:12:55
[02/24 16:35:08] mask2former INFO: Inference done 563/1092. Dataloading: 0.0266 s/iter. Inference: 0.8315 s/iter. Eval: 0.5991 s/iter. Total: 1.4591 s/iter. ETA=0:12:51
[02/24 16:35:15] mask2former INFO: Inference done 567/1092. Dataloading: 0.0265 s/iter. Inference: 0.8327 s/iter. Eval: 0.6003 s/iter. Total: 1.4615 s/iter. ETA=0:12:47
[02/24 16:35:21] mask2former INFO: Inference done 571/1092. Dataloading: 0.0265 s/iter. Inference: 0.8335 s/iter. Eval: 0.6005 s/iter. Total: 1.4624 s/iter. ETA=0:12:41
[02/24 16:35:27] mask2former INFO: Inference done 574/1092. Dataloading: 0.0265 s/iter. Inference: 0.8353 s/iter. Eval: 0.6011 s/iter. Total: 1.4649 s/iter. ETA=0:12:38
[02/24 16:35:32] mask2former INFO: Inference done 578/1092. Dataloading: 0.0266 s/iter. Inference: 0.8349 s/iter. Eval: 0.6009 s/iter. Total: 1.4643 s/iter. ETA=0:12:32
[02/24 16:35:39] mask2former INFO: Inference done 582/1092. Dataloading: 0.0266 s/iter. Inference: 0.8359 s/iter. Eval: 0.6007 s/iter. Total: 1.4650 s/iter. ETA=0:12:27
[02/24 16:35:45] mask2former INFO: Inference done 586/1092. Dataloading: 0.0266 s/iter. Inference: 0.8359 s/iter. Eval: 0.6009 s/iter. Total: 1.4653 s/iter. ETA=0:12:21
[02/24 16:35:51] mask2former INFO: Inference done 590/1092. Dataloading: 0.0266 s/iter. Inference: 0.8364 s/iter. Eval: 0.6008 s/iter. Total: 1.4656 s/iter. ETA=0:12:15
[02/24 16:35:58] mask2former INFO: Inference done 595/1092. Dataloading: 0.0265 s/iter. Inference: 0.8367 s/iter. Eval: 0.5998 s/iter. Total: 1.4648 s/iter. ETA=0:12:07
[02/24 16:36:03] mask2former INFO: Inference done 598/1092. Dataloading: 0.0264 s/iter. Inference: 0.8371 s/iter. Eval: 0.6008 s/iter. Total: 1.4662 s/iter. ETA=0:12:04
[02/24 16:36:08] mask2former INFO: Inference done 602/1092. Dataloading: 0.0264 s/iter. Inference: 0.8371 s/iter. Eval: 0.5995 s/iter. Total: 1.4648 s/iter. ETA=0:11:57
[02/24 16:36:14] mask2former INFO: Inference done 606/1092. Dataloading: 0.0264 s/iter. Inference: 0.8372 s/iter. Eval: 0.5996 s/iter. Total: 1.4650 s/iter. ETA=0:11:52
[02/24 16:36:19] mask2former INFO: Inference done 610/1092. Dataloading: 0.0264 s/iter. Inference: 0.8369 s/iter. Eval: 0.5992 s/iter. Total: 1.4643 s/iter. ETA=0:11:45
[02/24 16:36:25] mask2former INFO: Inference done 613/1092. Dataloading: 0.0264 s/iter. Inference: 0.8385 s/iter. Eval: 0.5993 s/iter. Total: 1.4661 s/iter. ETA=0:11:42
[02/24 16:36:31] mask2former INFO: Inference done 617/1092. Dataloading: 0.0265 s/iter. Inference: 0.8386 s/iter. Eval: 0.5992 s/iter. Total: 1.4661 s/iter. ETA=0:11:36
[02/24 16:36:37] mask2former INFO: Inference done 621/1092. Dataloading: 0.0268 s/iter. Inference: 0.8397 s/iter. Eval: 0.5994 s/iter. Total: 1.4677 s/iter. ETA=0:11:31
[02/24 16:36:44] mask2former INFO: Inference done 625/1092. Dataloading: 0.0269 s/iter. Inference: 0.8401 s/iter. Eval: 0.5995 s/iter. Total: 1.4682 s/iter. ETA=0:11:25
[02/24 16:36:50] mask2former INFO: Inference done 629/1092. Dataloading: 0.0269 s/iter. Inference: 0.8401 s/iter. Eval: 0.5999 s/iter. Total: 1.4687 s/iter. ETA=0:11:20
[02/24 16:36:55] mask2former INFO: Inference done 633/1092. Dataloading: 0.0269 s/iter. Inference: 0.8398 s/iter. Eval: 0.5995 s/iter. Total: 1.4679 s/iter. ETA=0:11:13
[02/24 16:37:02] mask2former INFO: Inference done 638/1092. Dataloading: 0.0269 s/iter. Inference: 0.8385 s/iter. Eval: 0.6004 s/iter. Total: 1.4677 s/iter. ETA=0:11:06
[02/24 16:37:07] mask2former INFO: Inference done 642/1092. Dataloading: 0.0269 s/iter. Inference: 0.8382 s/iter. Eval: 0.5994 s/iter. Total: 1.4663 s/iter. ETA=0:10:59
[02/24 16:37:13] mask2former INFO: Inference done 646/1092. Dataloading: 0.0269 s/iter. Inference: 0.8372 s/iter. Eval: 0.5993 s/iter. Total: 1.4652 s/iter. ETA=0:10:53
[02/24 16:37:19] mask2former INFO: Inference done 650/1092. Dataloading: 0.0269 s/iter. Inference: 0.8373 s/iter. Eval: 0.5994 s/iter. Total: 1.4654 s/iter. ETA=0:10:47
[02/24 16:37:25] mask2former INFO: Inference done 655/1092. Dataloading: 0.0268 s/iter. Inference: 0.8367 s/iter. Eval: 0.5985 s/iter. Total: 1.4639 s/iter. ETA=0:10:39
[02/24 16:37:30] mask2former INFO: Inference done 659/1092. Dataloading: 0.0268 s/iter. Inference: 0.8368 s/iter. Eval: 0.5981 s/iter. Total: 1.4633 s/iter. ETA=0:10:33
[02/24 16:37:36] mask2former INFO: Inference done 663/1092. Dataloading: 0.0267 s/iter. Inference: 0.8376 s/iter. Eval: 0.5974 s/iter. Total: 1.4635 s/iter. ETA=0:10:27
[02/24 16:37:42] mask2former INFO: Inference done 667/1092. Dataloading: 0.0267 s/iter. Inference: 0.8378 s/iter. Eval: 0.5966 s/iter. Total: 1.4629 s/iter. ETA=0:10:21
[02/24 16:37:48] mask2former INFO: Inference done 672/1092. Dataloading: 0.0267 s/iter. Inference: 0.8361 s/iter. Eval: 0.5960 s/iter. Total: 1.4605 s/iter. ETA=0:10:13
[02/24 16:37:53] mask2former INFO: Inference done 676/1092. Dataloading: 0.0266 s/iter. Inference: 0.8350 s/iter. Eval: 0.5959 s/iter. Total: 1.4592 s/iter. ETA=0:10:07
[02/24 16:37:58] mask2former INFO: Inference done 680/1092. Dataloading: 0.0266 s/iter. Inference: 0.8347 s/iter. Eval: 0.5954 s/iter. Total: 1.4584 s/iter. ETA=0:10:00
[02/24 16:38:04] mask2former INFO: Inference done 684/1092. Dataloading: 0.0268 s/iter. Inference: 0.8349 s/iter. Eval: 0.5959 s/iter. Total: 1.4593 s/iter. ETA=0:09:55
[02/24 16:38:10] mask2former INFO: Inference done 688/1092. Dataloading: 0.0268 s/iter. Inference: 0.8353 s/iter. Eval: 0.5957 s/iter. Total: 1.4595 s/iter. ETA=0:09:49
[02/24 16:38:15] mask2former INFO: Inference done 691/1092. Dataloading: 0.0268 s/iter. Inference: 0.8358 s/iter. Eval: 0.5963 s/iter. Total: 1.4605 s/iter. ETA=0:09:45
[02/24 16:38:21] mask2former INFO: Inference done 695/1092. Dataloading: 0.0267 s/iter. Inference: 0.8361 s/iter. Eval: 0.5963 s/iter. Total: 1.4608 s/iter. ETA=0:09:39
[02/24 16:38:27] mask2former INFO: Inference done 698/1092. Dataloading: 0.0267 s/iter. Inference: 0.8376 s/iter. Eval: 0.5968 s/iter. Total: 1.4628 s/iter. ETA=0:09:36
[02/24 16:38:33] mask2former INFO: Inference done 703/1092. Dataloading: 0.0269 s/iter. Inference: 0.8363 s/iter. Eval: 0.5961 s/iter. Total: 1.4610 s/iter. ETA=0:09:28
[02/24 16:38:39] mask2former INFO: Inference done 707/1092. Dataloading: 0.0270 s/iter. Inference: 0.8358 s/iter. Eval: 0.5971 s/iter. Total: 1.4616 s/iter. ETA=0:09:22
[02/24 16:38:45] mask2former INFO: Inference done 710/1092. Dataloading: 0.0269 s/iter. Inference: 0.8368 s/iter. Eval: 0.5978 s/iter. Total: 1.4634 s/iter. ETA=0:09:19
[02/24 16:38:50] mask2former INFO: Inference done 714/1092. Dataloading: 0.0269 s/iter. Inference: 0.8363 s/iter. Eval: 0.5975 s/iter. Total: 1.4625 s/iter. ETA=0:09:12
[02/24 16:38:57] mask2former INFO: Inference done 718/1092. Dataloading: 0.0269 s/iter. Inference: 0.8368 s/iter. Eval: 0.5982 s/iter. Total: 1.4636 s/iter. ETA=0:09:07
[02/24 16:39:03] mask2former INFO: Inference done 722/1092. Dataloading: 0.0268 s/iter. Inference: 0.8367 s/iter. Eval: 0.5992 s/iter. Total: 1.4645 s/iter. ETA=0:09:01
[02/24 16:39:10] mask2former INFO: Inference done 727/1092. Dataloading: 0.0268 s/iter. Inference: 0.8357 s/iter. Eval: 0.5989 s/iter. Total: 1.4632 s/iter. ETA=0:08:54
[02/24 16:39:15] mask2former INFO: Inference done 731/1092. Dataloading: 0.0268 s/iter. Inference: 0.8352 s/iter. Eval: 0.5987 s/iter. Total: 1.4625 s/iter. ETA=0:08:47
[02/24 16:39:21] mask2former INFO: Inference done 735/1092. Dataloading: 0.0269 s/iter. Inference: 0.8356 s/iter. Eval: 0.5985 s/iter. Total: 1.4627 s/iter. ETA=0:08:42
[02/24 16:39:27] mask2former INFO: Inference done 739/1092. Dataloading: 0.0269 s/iter. Inference: 0.8358 s/iter. Eval: 0.5982 s/iter. Total: 1.4627 s/iter. ETA=0:08:36
[02/24 16:39:33] mask2former INFO: Inference done 743/1092. Dataloading: 0.0269 s/iter. Inference: 0.8369 s/iter. Eval: 0.5977 s/iter. Total: 1.4632 s/iter. ETA=0:08:30
[02/24 16:39:39] mask2former INFO: Inference done 747/1092. Dataloading: 0.0269 s/iter. Inference: 0.8371 s/iter. Eval: 0.5975 s/iter. Total: 1.4632 s/iter. ETA=0:08:24
[02/24 16:39:45] mask2former INFO: Inference done 751/1092. Dataloading: 0.0268 s/iter. Inference: 0.8374 s/iter. Eval: 0.5976 s/iter. Total: 1.4636 s/iter. ETA=0:08:19
[02/24 16:39:51] mask2former INFO: Inference done 754/1092. Dataloading: 0.0268 s/iter. Inference: 0.8387 s/iter. Eval: 0.5977 s/iter. Total: 1.4649 s/iter. ETA=0:08:15
[02/24 16:39:57] mask2former INFO: Inference done 758/1092. Dataloading: 0.0267 s/iter. Inference: 0.8397 s/iter. Eval: 0.5972 s/iter. Total: 1.4654 s/iter. ETA=0:08:09
[02/24 16:40:03] mask2former INFO: Inference done 763/1092. Dataloading: 0.0267 s/iter. Inference: 0.8391 s/iter. Eval: 0.5969 s/iter. Total: 1.4644 s/iter. ETA=0:08:01
[02/24 16:40:09] mask2former INFO: Inference done 768/1092. Dataloading: 0.0266 s/iter. Inference: 0.8380 s/iter. Eval: 0.5962 s/iter. Total: 1.4626 s/iter. ETA=0:07:53
[02/24 16:40:15] mask2former INFO: Inference done 772/1092. Dataloading: 0.0266 s/iter. Inference: 0.8377 s/iter. Eval: 0.5960 s/iter. Total: 1.4621 s/iter. ETA=0:07:47
[02/24 16:40:20] mask2former INFO: Inference done 775/1092. Dataloading: 0.0267 s/iter. Inference: 0.8386 s/iter. Eval: 0.5959 s/iter. Total: 1.4630 s/iter. ETA=0:07:43
[02/24 16:40:26] mask2former INFO: Inference done 779/1092. Dataloading: 0.0267 s/iter. Inference: 0.8388 s/iter. Eval: 0.5963 s/iter. Total: 1.4635 s/iter. ETA=0:07:38
[02/24 16:40:33] mask2former INFO: Inference done 784/1092. Dataloading: 0.0266 s/iter. Inference: 0.8383 s/iter. Eval: 0.5961 s/iter. Total: 1.4628 s/iter. ETA=0:07:30
[02/24 16:40:39] mask2former INFO: Inference done 788/1092. Dataloading: 0.0266 s/iter. Inference: 0.8381 s/iter. Eval: 0.5965 s/iter. Total: 1.4629 s/iter. ETA=0:07:24
[02/24 16:40:44] mask2former INFO: Inference done 792/1092. Dataloading: 0.0267 s/iter. Inference: 0.8376 s/iter. Eval: 0.5965 s/iter. Total: 1.4625 s/iter. ETA=0:07:18
[02/24 16:40:50] mask2former INFO: Inference done 796/1092. Dataloading: 0.0266 s/iter. Inference: 0.8373 s/iter. Eval: 0.5969 s/iter. Total: 1.4626 s/iter. ETA=0:07:12
[02/24 16:40:57] mask2former INFO: Inference done 801/1092. Dataloading: 0.0267 s/iter. Inference: 0.8368 s/iter. Eval: 0.5967 s/iter. Total: 1.4619 s/iter. ETA=0:07:05
[02/24 16:41:02] mask2former INFO: Inference done 804/1092. Dataloading: 0.0267 s/iter. Inference: 0.8374 s/iter. Eval: 0.5972 s/iter. Total: 1.4630 s/iter. ETA=0:07:01
[02/24 16:41:08] mask2former INFO: Inference done 807/1092. Dataloading: 0.0267 s/iter. Inference: 0.8386 s/iter. Eval: 0.5974 s/iter. Total: 1.4645 s/iter. ETA=0:06:57
[02/24 16:41:13] mask2former INFO: Inference done 811/1092. Dataloading: 0.0268 s/iter. Inference: 0.8382 s/iter. Eval: 0.5972 s/iter. Total: 1.4639 s/iter. ETA=0:06:51
[02/24 16:41:19] mask2former INFO: Inference done 815/1092. Dataloading: 0.0267 s/iter. Inference: 0.8376 s/iter. Eval: 0.5974 s/iter. Total: 1.4634 s/iter. ETA=0:06:45
[02/24 16:41:25] mask2former INFO: Inference done 819/1092. Dataloading: 0.0267 s/iter. Inference: 0.8379 s/iter. Eval: 0.5978 s/iter. Total: 1.4640 s/iter. ETA=0:06:39
[02/24 16:41:31] mask2former INFO: Inference done 823/1092. Dataloading: 0.0269 s/iter. Inference: 0.8379 s/iter. Eval: 0.5976 s/iter. Total: 1.4640 s/iter. ETA=0:06:33
[02/24 16:41:36] mask2former INFO: Inference done 827/1092. Dataloading: 0.0270 s/iter. Inference: 0.8373 s/iter. Eval: 0.5970 s/iter. Total: 1.4630 s/iter. ETA=0:06:27
[02/24 16:41:41] mask2former INFO: Inference done 831/1092. Dataloading: 0.0269 s/iter. Inference: 0.8369 s/iter. Eval: 0.5967 s/iter. Total: 1.4623 s/iter. ETA=0:06:21
[02/24 16:41:47] mask2former INFO: Inference done 835/1092. Dataloading: 0.0269 s/iter. Inference: 0.8374 s/iter. Eval: 0.5966 s/iter. Total: 1.4626 s/iter. ETA=0:06:15
[02/24 16:41:53] mask2former INFO: Inference done 839/1092. Dataloading: 0.0269 s/iter. Inference: 0.8372 s/iter. Eval: 0.5963 s/iter. Total: 1.4621 s/iter. ETA=0:06:09
[02/24 16:41:58] mask2former INFO: Inference done 843/1092. Dataloading: 0.0270 s/iter. Inference: 0.8367 s/iter. Eval: 0.5962 s/iter. Total: 1.4616 s/iter. ETA=0:06:03
[02/24 16:42:03] mask2former INFO: Inference done 847/1092. Dataloading: 0.0270 s/iter. Inference: 0.8363 s/iter. Eval: 0.5958 s/iter. Total: 1.4607 s/iter. ETA=0:05:57
[02/24 16:42:09] mask2former INFO: Inference done 851/1092. Dataloading: 0.0270 s/iter. Inference: 0.8364 s/iter. Eval: 0.5957 s/iter. Total: 1.4607 s/iter. ETA=0:05:52
[02/24 16:42:15] mask2former INFO: Inference done 856/1092. Dataloading: 0.0269 s/iter. Inference: 0.8354 s/iter. Eval: 0.5951 s/iter. Total: 1.4591 s/iter. ETA=0:05:44
[02/24 16:42:21] mask2former INFO: Inference done 860/1092. Dataloading: 0.0270 s/iter. Inference: 0.8348 s/iter. Eval: 0.5953 s/iter. Total: 1.4588 s/iter. ETA=0:05:38
[02/24 16:42:26] mask2former INFO: Inference done 863/1092. Dataloading: 0.0270 s/iter. Inference: 0.8352 s/iter. Eval: 0.5962 s/iter. Total: 1.4601 s/iter. ETA=0:05:34
[02/24 16:42:32] mask2former INFO: Inference done 868/1092. Dataloading: 0.0269 s/iter. Inference: 0.8343 s/iter. Eval: 0.5951 s/iter. Total: 1.4581 s/iter. ETA=0:05:26
[02/24 16:42:38] mask2former INFO: Inference done 872/1092. Dataloading: 0.0270 s/iter. Inference: 0.8344 s/iter. Eval: 0.5951 s/iter. Total: 1.4582 s/iter. ETA=0:05:20
[02/24 16:42:44] mask2former INFO: Inference done 876/1092. Dataloading: 0.0270 s/iter. Inference: 0.8350 s/iter. Eval: 0.5954 s/iter. Total: 1.4590 s/iter. ETA=0:05:15
[02/24 16:42:50] mask2former INFO: Inference done 881/1092. Dataloading: 0.0270 s/iter. Inference: 0.8345 s/iter. Eval: 0.5945 s/iter. Total: 1.4576 s/iter. ETA=0:05:07
[02/24 16:42:56] mask2former INFO: Inference done 885/1092. Dataloading: 0.0269 s/iter. Inference: 0.8342 s/iter. Eval: 0.5943 s/iter. Total: 1.4570 s/iter. ETA=0:05:01
[02/24 16:43:01] mask2former INFO: Inference done 889/1092. Dataloading: 0.0270 s/iter. Inference: 0.8342 s/iter. Eval: 0.5942 s/iter. Total: 1.4571 s/iter. ETA=0:04:55
[02/24 16:43:07] mask2former INFO: Inference done 894/1092. Dataloading: 0.0269 s/iter. Inference: 0.8332 s/iter. Eval: 0.5937 s/iter. Total: 1.4555 s/iter. ETA=0:04:48
[02/24 16:43:13] mask2former INFO: Inference done 897/1092. Dataloading: 0.0269 s/iter. Inference: 0.8328 s/iter. Eval: 0.5950 s/iter. Total: 1.4564 s/iter. ETA=0:04:43
[02/24 16:43:19] mask2former INFO: Inference done 901/1092. Dataloading: 0.0269 s/iter. Inference: 0.8335 s/iter. Eval: 0.5950 s/iter. Total: 1.4570 s/iter. ETA=0:04:38
[02/24 16:43:25] mask2former INFO: Inference done 906/1092. Dataloading: 0.0268 s/iter. Inference: 0.8334 s/iter. Eval: 0.5942 s/iter. Total: 1.4561 s/iter. ETA=0:04:30
[02/24 16:43:31] mask2former INFO: Inference done 910/1092. Dataloading: 0.0269 s/iter. Inference: 0.8337 s/iter. Eval: 0.5942 s/iter. Total: 1.4564 s/iter. ETA=0:04:25
[02/24 16:43:37] mask2former INFO: Inference done 913/1092. Dataloading: 0.0268 s/iter. Inference: 0.8344 s/iter. Eval: 0.5945 s/iter. Total: 1.4573 s/iter. ETA=0:04:20
[02/24 16:43:42] mask2former INFO: Inference done 917/1092. Dataloading: 0.0269 s/iter. Inference: 0.8339 s/iter. Eval: 0.5944 s/iter. Total: 1.4568 s/iter. ETA=0:04:14
[02/24 16:43:47] mask2former INFO: Inference done 921/1092. Dataloading: 0.0269 s/iter. Inference: 0.8335 s/iter. Eval: 0.5941 s/iter. Total: 1.4561 s/iter. ETA=0:04:08
[02/24 16:43:53] mask2former INFO: Inference done 925/1092. Dataloading: 0.0268 s/iter. Inference: 0.8337 s/iter. Eval: 0.5943 s/iter. Total: 1.4565 s/iter. ETA=0:04:03
[02/24 16:43:59] mask2former INFO: Inference done 929/1092. Dataloading: 0.0268 s/iter. Inference: 0.8340 s/iter. Eval: 0.5937 s/iter. Total: 1.4562 s/iter. ETA=0:03:57
[02/24 16:44:04] mask2former INFO: Inference done 933/1092. Dataloading: 0.0268 s/iter. Inference: 0.8333 s/iter. Eval: 0.5940 s/iter. Total: 1.4557 s/iter. ETA=0:03:51
[02/24 16:44:10] mask2former INFO: Inference done 937/1092. Dataloading: 0.0269 s/iter. Inference: 0.8330 s/iter. Eval: 0.5941 s/iter. Total: 1.4556 s/iter. ETA=0:03:45
[02/24 16:44:16] mask2former INFO: Inference done 941/1092. Dataloading: 0.0269 s/iter. Inference: 0.8331 s/iter. Eval: 0.5944 s/iter. Total: 1.4561 s/iter. ETA=0:03:39
[02/24 16:44:22] mask2former INFO: Inference done 945/1092. Dataloading: 0.0269 s/iter. Inference: 0.8331 s/iter. Eval: 0.5945 s/iter. Total: 1.4562 s/iter. ETA=0:03:34
[02/24 16:44:27] mask2former INFO: Inference done 948/1092. Dataloading: 0.0269 s/iter. Inference: 0.8339 s/iter. Eval: 0.5946 s/iter. Total: 1.4571 s/iter. ETA=0:03:29
[02/24 16:44:33] mask2former INFO: Inference done 952/1092. Dataloading: 0.0271 s/iter. Inference: 0.8341 s/iter. Eval: 0.5942 s/iter. Total: 1.4570 s/iter. ETA=0:03:23
[02/24 16:44:39] mask2former INFO: Inference done 957/1092. Dataloading: 0.0270 s/iter. Inference: 0.8333 s/iter. Eval: 0.5937 s/iter. Total: 1.4557 s/iter. ETA=0:03:16
[02/24 16:44:45] mask2former INFO: Inference done 961/1092. Dataloading: 0.0270 s/iter. Inference: 0.8328 s/iter. Eval: 0.5939 s/iter. Total: 1.4553 s/iter. ETA=0:03:10
[02/24 16:44:50] mask2former INFO: Inference done 965/1092. Dataloading: 0.0270 s/iter. Inference: 0.8326 s/iter. Eval: 0.5941 s/iter. Total: 1.4553 s/iter. ETA=0:03:04
[02/24 16:44:56] mask2former INFO: Inference done 969/1092. Dataloading: 0.0270 s/iter. Inference: 0.8322 s/iter. Eval: 0.5937 s/iter. Total: 1.4545 s/iter. ETA=0:02:58
[02/24 16:45:01] mask2former INFO: Inference done 972/1092. Dataloading: 0.0269 s/iter. Inference: 0.8328 s/iter. Eval: 0.5942 s/iter. Total: 1.4556 s/iter. ETA=0:02:54
[02/24 16:45:07] mask2former INFO: Inference done 976/1092. Dataloading: 0.0270 s/iter. Inference: 0.8332 s/iter. Eval: 0.5943 s/iter. Total: 1.4561 s/iter. ETA=0:02:48
[02/24 16:45:14] mask2former INFO: Inference done 980/1092. Dataloading: 0.0270 s/iter. Inference: 0.8340 s/iter. Eval: 0.5941 s/iter. Total: 1.4567 s/iter. ETA=0:02:43
[02/24 16:45:20] mask2former INFO: Inference done 984/1092. Dataloading: 0.0270 s/iter. Inference: 0.8346 s/iter. Eval: 0.5943 s/iter. Total: 1.4575 s/iter. ETA=0:02:37
[02/24 16:45:27] mask2former INFO: Inference done 988/1092. Dataloading: 0.0269 s/iter. Inference: 0.8349 s/iter. Eval: 0.5947 s/iter. Total: 1.4582 s/iter. ETA=0:02:31
[02/24 16:45:34] mask2former INFO: Inference done 993/1092. Dataloading: 0.0269 s/iter. Inference: 0.8344 s/iter. Eval: 0.5948 s/iter. Total: 1.4577 s/iter. ETA=0:02:24
[02/24 16:45:40] mask2former INFO: Inference done 998/1092. Dataloading: 0.0269 s/iter. Inference: 0.8337 s/iter. Eval: 0.5941 s/iter. Total: 1.4564 s/iter. ETA=0:02:16
[02/24 16:45:45] mask2former INFO: Inference done 1002/1092. Dataloading: 0.0269 s/iter. Inference: 0.8336 s/iter. Eval: 0.5940 s/iter. Total: 1.4562 s/iter. ETA=0:02:11
[02/24 16:45:52] mask2former INFO: Inference done 1006/1092. Dataloading: 0.0269 s/iter. Inference: 0.8337 s/iter. Eval: 0.5944 s/iter. Total: 1.4567 s/iter. ETA=0:02:05
[02/24 16:45:57] mask2former INFO: Inference done 1010/1092. Dataloading: 0.0268 s/iter. Inference: 0.8338 s/iter. Eval: 0.5940 s/iter. Total: 1.4563 s/iter. ETA=0:01:59
[02/24 16:46:02] mask2former INFO: Inference done 1014/1092. Dataloading: 0.0268 s/iter. Inference: 0.8334 s/iter. Eval: 0.5939 s/iter. Total: 1.4558 s/iter. ETA=0:01:53
[02/24 16:46:09] mask2former INFO: Inference done 1018/1092. Dataloading: 0.0268 s/iter. Inference: 0.8334 s/iter. Eval: 0.5943 s/iter. Total: 1.4562 s/iter. ETA=0:01:47
[02/24 16:46:15] mask2former INFO: Inference done 1022/1092. Dataloading: 0.0269 s/iter. Inference: 0.8334 s/iter. Eval: 0.5945 s/iter. Total: 1.4564 s/iter. ETA=0:01:41
[02/24 16:46:20] mask2former INFO: Inference done 1025/1092. Dataloading: 0.0268 s/iter. Inference: 0.8344 s/iter. Eval: 0.5949 s/iter. Total: 1.4578 s/iter. ETA=0:01:37
[02/24 16:46:26] mask2former INFO: Inference done 1029/1092. Dataloading: 0.0268 s/iter. Inference: 0.8344 s/iter. Eval: 0.5946 s/iter. Total: 1.4575 s/iter. ETA=0:01:31
[02/24 16:46:31] mask2former INFO: Inference done 1032/1092. Dataloading: 0.0268 s/iter. Inference: 0.8348 s/iter. Eval: 0.5949 s/iter. Total: 1.4582 s/iter. ETA=0:01:27
[02/24 16:46:37] mask2former INFO: Inference done 1036/1092. Dataloading: 0.0268 s/iter. Inference: 0.8347 s/iter. Eval: 0.5948 s/iter. Total: 1.4579 s/iter. ETA=0:01:21
[02/24 16:46:42] mask2former INFO: Inference done 1040/1092. Dataloading: 0.0268 s/iter. Inference: 0.8344 s/iter. Eval: 0.5945 s/iter. Total: 1.4573 s/iter. ETA=0:01:15
[02/24 16:46:47] mask2former INFO: Inference done 1044/1092. Dataloading: 0.0268 s/iter. Inference: 0.8342 s/iter. Eval: 0.5941 s/iter. Total: 1.4567 s/iter. ETA=0:01:09
[02/24 16:46:52] mask2former INFO: Inference done 1048/1092. Dataloading: 0.0267 s/iter. Inference: 0.8337 s/iter. Eval: 0.5941 s/iter. Total: 1.4562 s/iter. ETA=0:01:04
[02/24 16:46:57] mask2former INFO: Inference done 1052/1092. Dataloading: 0.0267 s/iter. Inference: 0.8335 s/iter. Eval: 0.5937 s/iter. Total: 1.4555 s/iter. ETA=0:00:58
[02/24 16:47:02] mask2former INFO: Inference done 1056/1092. Dataloading: 0.0267 s/iter. Inference: 0.8330 s/iter. Eval: 0.5935 s/iter. Total: 1.4548 s/iter. ETA=0:00:52
[02/24 16:47:08] mask2former INFO: Inference done 1060/1092. Dataloading: 0.0267 s/iter. Inference: 0.8326 s/iter. Eval: 0.5935 s/iter. Total: 1.4544 s/iter. ETA=0:00:46
[02/24 16:47:13] mask2former INFO: Inference done 1064/1092. Dataloading: 0.0266 s/iter. Inference: 0.8327 s/iter. Eval: 0.5931 s/iter. Total: 1.4541 s/iter. ETA=0:00:40
[02/24 16:47:18] mask2former INFO: Inference done 1069/1092. Dataloading: 0.0266 s/iter. Inference: 0.8315 s/iter. Eval: 0.5923 s/iter. Total: 1.4520 s/iter. ETA=0:00:33
[02/24 16:47:23] mask2former INFO: Inference done 1073/1092. Dataloading: 0.0266 s/iter. Inference: 0.8307 s/iter. Eval: 0.5924 s/iter. Total: 1.4513 s/iter. ETA=0:00:27
[02/24 16:47:28] mask2former INFO: Inference done 1077/1092. Dataloading: 0.0265 s/iter. Inference: 0.8304 s/iter. Eval: 0.5920 s/iter. Total: 1.4506 s/iter. ETA=0:00:21
[02/24 16:47:34] mask2former INFO: Inference done 1082/1092. Dataloading: 0.0265 s/iter. Inference: 0.8295 s/iter. Eval: 0.5915 s/iter. Total: 1.4491 s/iter. ETA=0:00:14
[02/24 16:47:39] mask2former INFO: Inference done 1086/1092. Dataloading: 0.0265 s/iter. Inference: 0.8292 s/iter. Eval: 0.5912 s/iter. Total: 1.4486 s/iter. ETA=0:00:08
[02/24 16:47:44] mask2former INFO: Inference done 1090/1092. Dataloading: 0.0264 s/iter. Inference: 0.8291 s/iter. Eval: 0.5908 s/iter. Total: 1.4479 s/iter. ETA=0:00:02
[02/24 19:01:32] detectron2.engine.hooks INFO: Overall training speed: 9729 iterations in 5:49:50 (2.1575 s / it)
[02/24 19:01:32] detectron2.engine.hooks INFO: Total training time: 6:48:13 (0:58:22 on hooks)
