[02/24 11:33:53] detectron2 INFO: Rank of current process: 1. World size: 4
[02/24 11:34:00] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 11:34:00] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 11:34:00] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 11:34:00] detectron2.utils.env INFO: Using a generated random seed 343755
[02/24 11:34:06] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 1.0, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 11:34:06] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 11:34:20] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 11:34:25] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 11:34:26] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 11:34:29] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 11:34:29] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 11:34:29] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 11:34:34] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 11:34:34] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 11:34:34] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 11:34:57] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 395, in run_step
    loss_dict = self.model(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 791, in forward
    losses = self.criterion(outputs, targets)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/criterion.py", line 259, in forward
    losses.update(self.get_loss(loss, outputs, targets, indices, num_masks))
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 212, in get_loss
    return loss_map[loss](outputs, targets, indices, num_masks)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 191, in loss_segs
    semseg,
UnboundLocalError: local variable 'semseg' referenced before assignment
[02/24 11:34:57] detectron2.engine.hooks INFO: Total training time: 0:00:22 (0:00:00 on hooks)
[02/24 11:37:47] detectron2 INFO: Rank of current process: 1. World size: 4
[02/24 11:37:55] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 11:37:55] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 11:37:55] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 11:37:55] detectron2.utils.env INFO: Using a generated random seed 55910311
[02/24 11:38:04] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 1.0, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 11:38:04] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 11:38:15] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 11:38:16] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 11:38:16] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 11:38:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 11:38:17] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 11:38:17] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 11:38:18] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 11:38:18] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 11:38:18] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 12:10:58] detectron2.engine.hooks INFO: Overall training speed: 1100 iterations in 0:32:08 (1.7534 s / it)
[02/24 12:10:58] detectron2.engine.hooks INFO: Total training time: 0:32:14 (0:00:06 on hooks)
[02/24 12:12:19] detectron2 INFO: Rank of current process: 1. World size: 4
[02/24 12:12:26] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/24 12:12:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65530', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/24 12:12:26] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/24 12:12:26] detectron2.utils.env INFO: Using a generated random seed 26312642
[02/24 12:12:33] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (refinement_layer): DispRefineLayer(
    (conv2d_feature): Conv2d(
      257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
      (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (residual_atrous_blocks): ModuleList(
      (0): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(2, 2)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (2): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(4, 4)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (3): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same, dilation=(8, 8)
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (4): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (5): BasicBlock(
        (conv1): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
    )
    (conv2d_out): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 1.0, 'loss_ce': 0.0, 'loss_dice': 1.0, 'loss_seg': 0.1, 'loss_mask_0': 1.0, 'loss_ce_0': 0.0, 'loss_dice_0': 1.0, 'loss_mask_1': 1.0, 'loss_ce_1': 0.0, 'loss_dice_1': 1.0, 'loss_mask_2': 1.0, 'loss_ce_2': 0.0, 'loss_dice_2': 1.0, 'loss_mask_3': 1.0, 'loss_ce_3': 0.0, 'loss_dice_3': 1.0, 'loss_mask_4': 1.0, 'loss_ce_4': 0.0, 'loss_dice_4': 1.0, 'loss_mask_5': 1.0, 'loss_ce_5': 0.0, 'loss_dice_5': 1.0, 'loss_mask_6': 1.0, 'loss_ce_6': 0.0, 'loss_dice_6': 1.0, 'loss_mask_7': 1.0, 'loss_ce_7': 0.0, 'loss_dice_7': 1.0, 'loss_mask_8': 1.0, 'loss_ce_8': 0.0, 'loss_dice_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/24 12:12:33] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/24 12:12:44] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/24 12:12:44] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/24 12:12:44] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/24 12:12:53] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/24 12:12:53] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/24 12:12:53] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/24 12:12:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34mrefinement_layer.conv2d_feature.norm.{bias, weight}[0m
[34mrefinement_layer.conv2d_feature.{bias, weight}[0m
[34mrefinement_layer.conv2d_out.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.0.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.1.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.2.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.3.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.4.conv1.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.norm.{bias, weight}[0m
[34mrefinement_layer.residual_atrous_blocks.5.conv1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/24 12:12:54] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/24 12:12:54] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/24 13:23:46] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 13:23:47] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 13:23:47] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 13:24:09] mask2former INFO: Inference done 11/1093. Dataloading: 0.0091 s/iter. Inference: 0.4174 s/iter. Eval: 0.2012 s/iter. Total: 0.6277 s/iter. ETA=0:11:19
[02/24 13:24:15] mask2former INFO: Inference done 20/1093. Dataloading: 0.0105 s/iter. Inference: 0.3800 s/iter. Eval: 0.2196 s/iter. Total: 0.6102 s/iter. ETA=0:10:54
[02/24 13:24:20] mask2former INFO: Inference done 29/1093. Dataloading: 0.0125 s/iter. Inference: 0.3679 s/iter. Eval: 0.2339 s/iter. Total: 0.6144 s/iter. ETA=0:10:53
[02/24 13:24:26] mask2former INFO: Inference done 37/1093. Dataloading: 0.0125 s/iter. Inference: 0.3745 s/iter. Eval: 0.2483 s/iter. Total: 0.6355 s/iter. ETA=0:11:11
[02/24 13:24:32] mask2former INFO: Inference done 46/1093. Dataloading: 0.0127 s/iter. Inference: 0.3739 s/iter. Eval: 0.2476 s/iter. Total: 0.6344 s/iter. ETA=0:11:04
[02/24 13:24:37] mask2former INFO: Inference done 54/1093. Dataloading: 0.0125 s/iter. Inference: 0.3797 s/iter. Eval: 0.2429 s/iter. Total: 0.6352 s/iter. ETA=0:10:59
[02/24 13:24:42] mask2former INFO: Inference done 61/1093. Dataloading: 0.0126 s/iter. Inference: 0.3866 s/iter. Eval: 0.2499 s/iter. Total: 0.6493 s/iter. ETA=0:11:10
[02/24 13:24:47] mask2former INFO: Inference done 70/1093. Dataloading: 0.0123 s/iter. Inference: 0.3806 s/iter. Eval: 0.2447 s/iter. Total: 0.6377 s/iter. ETA=0:10:52
[02/24 13:24:52] mask2former INFO: Inference done 79/1093. Dataloading: 0.0122 s/iter. Inference: 0.3793 s/iter. Eval: 0.2412 s/iter. Total: 0.6328 s/iter. ETA=0:10:41
[02/24 13:24:58] mask2former INFO: Inference done 88/1093. Dataloading: 0.0119 s/iter. Inference: 0.3750 s/iter. Eval: 0.2399 s/iter. Total: 0.6269 s/iter. ETA=0:10:29
[02/24 13:25:03] mask2former INFO: Inference done 97/1093. Dataloading: 0.0122 s/iter. Inference: 0.3711 s/iter. Eval: 0.2396 s/iter. Total: 0.6230 s/iter. ETA=0:10:20
[02/24 13:25:08] mask2former INFO: Inference done 110/1093. Dataloading: 0.0116 s/iter. Inference: 0.3575 s/iter. Eval: 0.2291 s/iter. Total: 0.5985 s/iter. ETA=0:09:48
[02/24 13:25:14] mask2former INFO: Inference done 117/1093. Dataloading: 0.0121 s/iter. Inference: 0.3626 s/iter. Eval: 0.2365 s/iter. Total: 0.6113 s/iter. ETA=0:09:56
[02/24 13:25:20] mask2former INFO: Inference done 123/1093. Dataloading: 0.0122 s/iter. Inference: 0.3724 s/iter. Eval: 0.2421 s/iter. Total: 0.6269 s/iter. ETA=0:10:08
[02/24 13:25:25] mask2former INFO: Inference done 132/1093. Dataloading: 0.0120 s/iter. Inference: 0.3717 s/iter. Eval: 0.2389 s/iter. Total: 0.6227 s/iter. ETA=0:09:58
[02/24 13:25:30] mask2former INFO: Inference done 139/1093. Dataloading: 0.0120 s/iter. Inference: 0.3751 s/iter. Eval: 0.2429 s/iter. Total: 0.6302 s/iter. ETA=0:10:01
[02/24 13:25:36] mask2former INFO: Inference done 147/1093. Dataloading: 0.0119 s/iter. Inference: 0.3760 s/iter. Eval: 0.2448 s/iter. Total: 0.6329 s/iter. ETA=0:09:58
[02/24 13:25:41] mask2former INFO: Inference done 155/1093. Dataloading: 0.0119 s/iter. Inference: 0.3763 s/iter. Eval: 0.2459 s/iter. Total: 0.6343 s/iter. ETA=0:09:54
[02/24 13:25:46] mask2former INFO: Inference done 164/1093. Dataloading: 0.0119 s/iter. Inference: 0.3752 s/iter. Eval: 0.2450 s/iter. Total: 0.6323 s/iter. ETA=0:09:47
[02/24 13:25:52] mask2former INFO: Inference done 173/1093. Dataloading: 0.0117 s/iter. Inference: 0.3739 s/iter. Eval: 0.2455 s/iter. Total: 0.6313 s/iter. ETA=0:09:40
[02/24 13:25:57] mask2former INFO: Inference done 182/1093. Dataloading: 0.0116 s/iter. Inference: 0.3738 s/iter. Eval: 0.2444 s/iter. Total: 0.6300 s/iter. ETA=0:09:33
[02/24 13:26:02] mask2former INFO: Inference done 190/1093. Dataloading: 0.0116 s/iter. Inference: 0.3739 s/iter. Eval: 0.2457 s/iter. Total: 0.6314 s/iter. ETA=0:09:30
[02/24 13:26:08] mask2former INFO: Inference done 198/1093. Dataloading: 0.0116 s/iter. Inference: 0.3737 s/iter. Eval: 0.2466 s/iter. Total: 0.6320 s/iter. ETA=0:09:25
[02/24 13:26:13] mask2former INFO: Inference done 206/1093. Dataloading: 0.0115 s/iter. Inference: 0.3742 s/iter. Eval: 0.2473 s/iter. Total: 0.6332 s/iter. ETA=0:09:21
[02/24 13:26:18] mask2former INFO: Inference done 214/1093. Dataloading: 0.0115 s/iter. Inference: 0.3745 s/iter. Eval: 0.2483 s/iter. Total: 0.6344 s/iter. ETA=0:09:17
[02/24 13:26:23] mask2former INFO: Inference done 222/1093. Dataloading: 0.0117 s/iter. Inference: 0.3744 s/iter. Eval: 0.2483 s/iter. Total: 0.6346 s/iter. ETA=0:09:12
[02/24 13:26:29] mask2former INFO: Inference done 230/1093. Dataloading: 0.0117 s/iter. Inference: 0.3740 s/iter. Eval: 0.2505 s/iter. Total: 0.6364 s/iter. ETA=0:09:09
[02/24 13:26:34] mask2former INFO: Inference done 238/1093. Dataloading: 0.0117 s/iter. Inference: 0.3746 s/iter. Eval: 0.2519 s/iter. Total: 0.6383 s/iter. ETA=0:09:05
[02/24 13:26:40] mask2former INFO: Inference done 246/1093. Dataloading: 0.0117 s/iter. Inference: 0.3740 s/iter. Eval: 0.2527 s/iter. Total: 0.6385 s/iter. ETA=0:09:00
[02/24 13:26:45] mask2former INFO: Inference done 255/1093. Dataloading: 0.0116 s/iter. Inference: 0.3732 s/iter. Eval: 0.2526 s/iter. Total: 0.6376 s/iter. ETA=0:08:54
[02/24 13:26:50] mask2former INFO: Inference done 263/1093. Dataloading: 0.0115 s/iter. Inference: 0.3736 s/iter. Eval: 0.2533 s/iter. Total: 0.6386 s/iter. ETA=0:08:50
[02/24 13:26:56] mask2former INFO: Inference done 272/1093. Dataloading: 0.0114 s/iter. Inference: 0.3726 s/iter. Eval: 0.2531 s/iter. Total: 0.6372 s/iter. ETA=0:08:43
[02/24 13:27:01] mask2former INFO: Inference done 280/1093. Dataloading: 0.0114 s/iter. Inference: 0.3725 s/iter. Eval: 0.2536 s/iter. Total: 0.6377 s/iter. ETA=0:08:38
[02/24 13:27:06] mask2former INFO: Inference done 288/1093. Dataloading: 0.0115 s/iter. Inference: 0.3737 s/iter. Eval: 0.2532 s/iter. Total: 0.6386 s/iter. ETA=0:08:34
[02/24 13:27:12] mask2former INFO: Inference done 296/1093. Dataloading: 0.0114 s/iter. Inference: 0.3743 s/iter. Eval: 0.2532 s/iter. Total: 0.6391 s/iter. ETA=0:08:29
[02/24 13:27:17] mask2former INFO: Inference done 305/1093. Dataloading: 0.0114 s/iter. Inference: 0.3739 s/iter. Eval: 0.2528 s/iter. Total: 0.6383 s/iter. ETA=0:08:23
[02/24 13:27:22] mask2former INFO: Inference done 313/1093. Dataloading: 0.0115 s/iter. Inference: 0.3752 s/iter. Eval: 0.2522 s/iter. Total: 0.6390 s/iter. ETA=0:08:18
[02/24 13:27:28] mask2former INFO: Inference done 322/1093. Dataloading: 0.0114 s/iter. Inference: 0.3747 s/iter. Eval: 0.2513 s/iter. Total: 0.6375 s/iter. ETA=0:08:11
[02/24 13:27:33] mask2former INFO: Inference done 331/1093. Dataloading: 0.0114 s/iter. Inference: 0.3741 s/iter. Eval: 0.2512 s/iter. Total: 0.6369 s/iter. ETA=0:08:05
[02/24 13:27:39] mask2former INFO: Inference done 339/1093. Dataloading: 0.0114 s/iter. Inference: 0.3747 s/iter. Eval: 0.2512 s/iter. Total: 0.6375 s/iter. ETA=0:08:00
[02/24 13:27:44] mask2former INFO: Inference done 347/1093. Dataloading: 0.0115 s/iter. Inference: 0.3748 s/iter. Eval: 0.2517 s/iter. Total: 0.6382 s/iter. ETA=0:07:56
[02/24 13:27:50] mask2former INFO: Inference done 355/1093. Dataloading: 0.0116 s/iter. Inference: 0.3758 s/iter. Eval: 0.2523 s/iter. Total: 0.6398 s/iter. ETA=0:07:52
[02/24 13:27:55] mask2former INFO: Inference done 363/1093. Dataloading: 0.0115 s/iter. Inference: 0.3766 s/iter. Eval: 0.2526 s/iter. Total: 0.6409 s/iter. ETA=0:07:47
[02/24 13:28:00] mask2former INFO: Inference done 371/1093. Dataloading: 0.0114 s/iter. Inference: 0.3773 s/iter. Eval: 0.2523 s/iter. Total: 0.6412 s/iter. ETA=0:07:42
[02/24 13:28:05] mask2former INFO: Inference done 379/1093. Dataloading: 0.0114 s/iter. Inference: 0.3777 s/iter. Eval: 0.2518 s/iter. Total: 0.6411 s/iter. ETA=0:07:37
[02/24 13:28:11] mask2former INFO: Inference done 388/1093. Dataloading: 0.0116 s/iter. Inference: 0.3774 s/iter. Eval: 0.2517 s/iter. Total: 0.6409 s/iter. ETA=0:07:31
[02/24 13:28:16] mask2former INFO: Inference done 396/1093. Dataloading: 0.0116 s/iter. Inference: 0.3773 s/iter. Eval: 0.2518 s/iter. Total: 0.6410 s/iter. ETA=0:07:26
[02/24 13:28:22] mask2former INFO: Inference done 405/1093. Dataloading: 0.0116 s/iter. Inference: 0.3769 s/iter. Eval: 0.2519 s/iter. Total: 0.6405 s/iter. ETA=0:07:20
[02/24 13:28:27] mask2former INFO: Inference done 413/1093. Dataloading: 0.0117 s/iter. Inference: 0.3768 s/iter. Eval: 0.2520 s/iter. Total: 0.6406 s/iter. ETA=0:07:15
[02/24 13:28:32] mask2former INFO: Inference done 421/1093. Dataloading: 0.0117 s/iter. Inference: 0.3767 s/iter. Eval: 0.2519 s/iter. Total: 0.6405 s/iter. ETA=0:07:10
[02/24 13:28:37] mask2former INFO: Inference done 430/1093. Dataloading: 0.0117 s/iter. Inference: 0.3760 s/iter. Eval: 0.2511 s/iter. Total: 0.6390 s/iter. ETA=0:07:03
[02/24 13:28:43] mask2former INFO: Inference done 438/1093. Dataloading: 0.0116 s/iter. Inference: 0.3771 s/iter. Eval: 0.2515 s/iter. Total: 0.6404 s/iter. ETA=0:06:59
[02/24 13:28:49] mask2former INFO: Inference done 447/1093. Dataloading: 0.0116 s/iter. Inference: 0.3771 s/iter. Eval: 0.2512 s/iter. Total: 0.6401 s/iter. ETA=0:06:53
[02/24 13:28:54] mask2former INFO: Inference done 455/1093. Dataloading: 0.0116 s/iter. Inference: 0.3774 s/iter. Eval: 0.2510 s/iter. Total: 0.6402 s/iter. ETA=0:06:48
[02/24 13:28:59] mask2former INFO: Inference done 463/1093. Dataloading: 0.0117 s/iter. Inference: 0.3775 s/iter. Eval: 0.2510 s/iter. Total: 0.6404 s/iter. ETA=0:06:43
[02/24 13:29:05] mask2former INFO: Inference done 472/1093. Dataloading: 0.0116 s/iter. Inference: 0.3774 s/iter. Eval: 0.2510 s/iter. Total: 0.6402 s/iter. ETA=0:06:37
[02/24 13:29:10] mask2former INFO: Inference done 480/1093. Dataloading: 0.0116 s/iter. Inference: 0.3775 s/iter. Eval: 0.2508 s/iter. Total: 0.6402 s/iter. ETA=0:06:32
[02/24 13:29:15] mask2former INFO: Inference done 488/1093. Dataloading: 0.0117 s/iter. Inference: 0.3781 s/iter. Eval: 0.2509 s/iter. Total: 0.6409 s/iter. ETA=0:06:27
[02/24 13:29:21] mask2former INFO: Inference done 497/1093. Dataloading: 0.0116 s/iter. Inference: 0.3783 s/iter. Eval: 0.2503 s/iter. Total: 0.6405 s/iter. ETA=0:06:21
[02/24 13:29:26] mask2former INFO: Inference done 505/1093. Dataloading: 0.0116 s/iter. Inference: 0.3780 s/iter. Eval: 0.2504 s/iter. Total: 0.6403 s/iter. ETA=0:06:16
[02/24 13:29:31] mask2former INFO: Inference done 513/1093. Dataloading: 0.0117 s/iter. Inference: 0.3781 s/iter. Eval: 0.2508 s/iter. Total: 0.6408 s/iter. ETA=0:06:11
[02/24 13:29:36] mask2former INFO: Inference done 521/1093. Dataloading: 0.0117 s/iter. Inference: 0.3781 s/iter. Eval: 0.2508 s/iter. Total: 0.6409 s/iter. ETA=0:06:06
[02/24 13:29:42] mask2former INFO: Inference done 529/1093. Dataloading: 0.0118 s/iter. Inference: 0.3781 s/iter. Eval: 0.2515 s/iter. Total: 0.6416 s/iter. ETA=0:06:01
[02/24 13:29:47] mask2former INFO: Inference done 537/1093. Dataloading: 0.0117 s/iter. Inference: 0.3780 s/iter. Eval: 0.2515 s/iter. Total: 0.6414 s/iter. ETA=0:05:56
[02/24 13:29:52] mask2former INFO: Inference done 545/1093. Dataloading: 0.0118 s/iter. Inference: 0.3785 s/iter. Eval: 0.2517 s/iter. Total: 0.6421 s/iter. ETA=0:05:51
[02/24 13:29:58] mask2former INFO: Inference done 554/1093. Dataloading: 0.0117 s/iter. Inference: 0.3781 s/iter. Eval: 0.2512 s/iter. Total: 0.6413 s/iter. ETA=0:05:45
[02/24 13:30:03] mask2former INFO: Inference done 562/1093. Dataloading: 0.0117 s/iter. Inference: 0.3783 s/iter. Eval: 0.2520 s/iter. Total: 0.6422 s/iter. ETA=0:05:41
[02/24 13:30:08] mask2former INFO: Inference done 570/1093. Dataloading: 0.0117 s/iter. Inference: 0.3783 s/iter. Eval: 0.2520 s/iter. Total: 0.6422 s/iter. ETA=0:05:35
[02/24 13:30:14] mask2former INFO: Inference done 579/1093. Dataloading: 0.0117 s/iter. Inference: 0.3782 s/iter. Eval: 0.2515 s/iter. Total: 0.6416 s/iter. ETA=0:05:29
[02/24 13:30:19] mask2former INFO: Inference done 587/1093. Dataloading: 0.0117 s/iter. Inference: 0.3782 s/iter. Eval: 0.2515 s/iter. Total: 0.6416 s/iter. ETA=0:05:24
[02/24 13:30:25] mask2former INFO: Inference done 596/1093. Dataloading: 0.0117 s/iter. Inference: 0.3781 s/iter. Eval: 0.2512 s/iter. Total: 0.6413 s/iter. ETA=0:05:18
[02/24 13:30:30] mask2former INFO: Inference done 604/1093. Dataloading: 0.0117 s/iter. Inference: 0.3784 s/iter. Eval: 0.2517 s/iter. Total: 0.6420 s/iter. ETA=0:05:13
[02/24 13:30:35] mask2former INFO: Inference done 612/1093. Dataloading: 0.0117 s/iter. Inference: 0.3782 s/iter. Eval: 0.2517 s/iter. Total: 0.6418 s/iter. ETA=0:05:08
[02/24 13:30:41] mask2former INFO: Inference done 620/1093. Dataloading: 0.0117 s/iter. Inference: 0.3788 s/iter. Eval: 0.2516 s/iter. Total: 0.6423 s/iter. ETA=0:05:03
[02/24 13:30:46] mask2former INFO: Inference done 629/1093. Dataloading: 0.0116 s/iter. Inference: 0.3785 s/iter. Eval: 0.2509 s/iter. Total: 0.6412 s/iter. ETA=0:04:57
[02/24 13:30:51] mask2former INFO: Inference done 637/1093. Dataloading: 0.0117 s/iter. Inference: 0.3787 s/iter. Eval: 0.2510 s/iter. Total: 0.6417 s/iter. ETA=0:04:52
[02/24 13:30:56] mask2former INFO: Inference done 645/1093. Dataloading: 0.0118 s/iter. Inference: 0.3786 s/iter. Eval: 0.2510 s/iter. Total: 0.6416 s/iter. ETA=0:04:47
[02/24 13:31:01] mask2former INFO: Inference done 653/1093. Dataloading: 0.0118 s/iter. Inference: 0.3786 s/iter. Eval: 0.2511 s/iter. Total: 0.6417 s/iter. ETA=0:04:42
[02/24 13:31:07] mask2former INFO: Inference done 661/1093. Dataloading: 0.0118 s/iter. Inference: 0.3787 s/iter. Eval: 0.2513 s/iter. Total: 0.6420 s/iter. ETA=0:04:37
[02/24 13:31:12] mask2former INFO: Inference done 669/1093. Dataloading: 0.0118 s/iter. Inference: 0.3788 s/iter. Eval: 0.2516 s/iter. Total: 0.6425 s/iter. ETA=0:04:32
[02/24 13:31:18] mask2former INFO: Inference done 677/1093. Dataloading: 0.0118 s/iter. Inference: 0.3792 s/iter. Eval: 0.2516 s/iter. Total: 0.6428 s/iter. ETA=0:04:27
[02/24 13:31:23] mask2former INFO: Inference done 685/1093. Dataloading: 0.0117 s/iter. Inference: 0.3790 s/iter. Eval: 0.2517 s/iter. Total: 0.6426 s/iter. ETA=0:04:22
[02/24 13:31:28] mask2former INFO: Inference done 694/1093. Dataloading: 0.0118 s/iter. Inference: 0.3787 s/iter. Eval: 0.2516 s/iter. Total: 0.6423 s/iter. ETA=0:04:16
[02/24 13:31:33] mask2former INFO: Inference done 706/1093. Dataloading: 0.0117 s/iter. Inference: 0.3770 s/iter. Eval: 0.2499 s/iter. Total: 0.6388 s/iter. ETA=0:04:07
[02/24 13:31:39] mask2former INFO: Inference done 715/1093. Dataloading: 0.0117 s/iter. Inference: 0.3768 s/iter. Eval: 0.2504 s/iter. Total: 0.6392 s/iter. ETA=0:04:01
[02/24 13:31:45] mask2former INFO: Inference done 722/1093. Dataloading: 0.0117 s/iter. Inference: 0.3774 s/iter. Eval: 0.2514 s/iter. Total: 0.6407 s/iter. ETA=0:03:57
[02/24 13:31:50] mask2former INFO: Inference done 731/1093. Dataloading: 0.0117 s/iter. Inference: 0.3771 s/iter. Eval: 0.2512 s/iter. Total: 0.6403 s/iter. ETA=0:03:51
[02/24 13:31:56] mask2former INFO: Inference done 738/1093. Dataloading: 0.0117 s/iter. Inference: 0.3780 s/iter. Eval: 0.2518 s/iter. Total: 0.6418 s/iter. ETA=0:03:47
[02/24 13:32:01] mask2former INFO: Inference done 745/1093. Dataloading: 0.0118 s/iter. Inference: 0.3788 s/iter. Eval: 0.2520 s/iter. Total: 0.6428 s/iter. ETA=0:03:43
[02/24 13:32:06] mask2former INFO: Inference done 754/1093. Dataloading: 0.0118 s/iter. Inference: 0.3783 s/iter. Eval: 0.2517 s/iter. Total: 0.6420 s/iter. ETA=0:03:37
[02/24 13:32:12] mask2former INFO: Inference done 763/1093. Dataloading: 0.0118 s/iter. Inference: 0.3782 s/iter. Eval: 0.2515 s/iter. Total: 0.6418 s/iter. ETA=0:03:31
[02/24 13:32:17] mask2former INFO: Inference done 771/1093. Dataloading: 0.0118 s/iter. Inference: 0.3785 s/iter. Eval: 0.2512 s/iter. Total: 0.6416 s/iter. ETA=0:03:26
[02/24 13:32:22] mask2former INFO: Inference done 779/1093. Dataloading: 0.0118 s/iter. Inference: 0.3783 s/iter. Eval: 0.2514 s/iter. Total: 0.6417 s/iter. ETA=0:03:21
[02/24 13:32:28] mask2former INFO: Inference done 788/1093. Dataloading: 0.0118 s/iter. Inference: 0.3780 s/iter. Eval: 0.2515 s/iter. Total: 0.6415 s/iter. ETA=0:03:15
[02/24 13:32:33] mask2former INFO: Inference done 797/1093. Dataloading: 0.0118 s/iter. Inference: 0.3777 s/iter. Eval: 0.2514 s/iter. Total: 0.6411 s/iter. ETA=0:03:09
[02/24 13:32:38] mask2former INFO: Inference done 804/1093. Dataloading: 0.0118 s/iter. Inference: 0.3779 s/iter. Eval: 0.2519 s/iter. Total: 0.6418 s/iter. ETA=0:03:05
[02/24 13:32:44] mask2former INFO: Inference done 813/1093. Dataloading: 0.0118 s/iter. Inference: 0.3776 s/iter. Eval: 0.2516 s/iter. Total: 0.6413 s/iter. ETA=0:02:59
[02/24 13:32:49] mask2former INFO: Inference done 822/1093. Dataloading: 0.0118 s/iter. Inference: 0.3776 s/iter. Eval: 0.2512 s/iter. Total: 0.6408 s/iter. ETA=0:02:53
[02/24 13:32:54] mask2former INFO: Inference done 830/1093. Dataloading: 0.0118 s/iter. Inference: 0.3778 s/iter. Eval: 0.2509 s/iter. Total: 0.6407 s/iter. ETA=0:02:48
[02/24 13:33:00] mask2former INFO: Inference done 838/1093. Dataloading: 0.0118 s/iter. Inference: 0.3780 s/iter. Eval: 0.2509 s/iter. Total: 0.6410 s/iter. ETA=0:02:43
[02/24 13:33:05] mask2former INFO: Inference done 846/1093. Dataloading: 0.0118 s/iter. Inference: 0.3780 s/iter. Eval: 0.2509 s/iter. Total: 0.6409 s/iter. ETA=0:02:38
[02/24 13:33:10] mask2former INFO: Inference done 854/1093. Dataloading: 0.0117 s/iter. Inference: 0.3778 s/iter. Eval: 0.2510 s/iter. Total: 0.6408 s/iter. ETA=0:02:33
[02/24 13:33:15] mask2former INFO: Inference done 862/1093. Dataloading: 0.0118 s/iter. Inference: 0.3776 s/iter. Eval: 0.2510 s/iter. Total: 0.6407 s/iter. ETA=0:02:27
[02/24 13:33:20] mask2former INFO: Inference done 870/1093. Dataloading: 0.0118 s/iter. Inference: 0.3777 s/iter. Eval: 0.2508 s/iter. Total: 0.6405 s/iter. ETA=0:02:22
[02/24 13:33:25] mask2former INFO: Inference done 879/1093. Dataloading: 0.0118 s/iter. Inference: 0.3775 s/iter. Eval: 0.2506 s/iter. Total: 0.6402 s/iter. ETA=0:02:17
[02/24 13:33:30] mask2former INFO: Inference done 888/1093. Dataloading: 0.0118 s/iter. Inference: 0.3772 s/iter. Eval: 0.2505 s/iter. Total: 0.6397 s/iter. ETA=0:02:11
[02/24 13:33:36] mask2former INFO: Inference done 897/1093. Dataloading: 0.0118 s/iter. Inference: 0.3771 s/iter. Eval: 0.2502 s/iter. Total: 0.6393 s/iter. ETA=0:02:05
[02/24 13:33:41] mask2former INFO: Inference done 905/1093. Dataloading: 0.0118 s/iter. Inference: 0.3772 s/iter. Eval: 0.2500 s/iter. Total: 0.6392 s/iter. ETA=0:02:00
[02/24 13:33:46] mask2former INFO: Inference done 913/1093. Dataloading: 0.0118 s/iter. Inference: 0.3774 s/iter. Eval: 0.2499 s/iter. Total: 0.6393 s/iter. ETA=0:01:55
[02/24 13:33:51] mask2former INFO: Inference done 921/1093. Dataloading: 0.0117 s/iter. Inference: 0.3775 s/iter. Eval: 0.2499 s/iter. Total: 0.6394 s/iter. ETA=0:01:49
[02/24 13:33:57] mask2former INFO: Inference done 930/1093. Dataloading: 0.0117 s/iter. Inference: 0.3775 s/iter. Eval: 0.2496 s/iter. Total: 0.6391 s/iter. ETA=0:01:44
[02/24 13:34:02] mask2former INFO: Inference done 938/1093. Dataloading: 0.0117 s/iter. Inference: 0.3775 s/iter. Eval: 0.2495 s/iter. Total: 0.6390 s/iter. ETA=0:01:39
[02/24 13:34:07] mask2former INFO: Inference done 946/1093. Dataloading: 0.0117 s/iter. Inference: 0.3776 s/iter. Eval: 0.2496 s/iter. Total: 0.6391 s/iter. ETA=0:01:33
[02/24 13:34:12] mask2former INFO: Inference done 955/1093. Dataloading: 0.0117 s/iter. Inference: 0.3773 s/iter. Eval: 0.2493 s/iter. Total: 0.6385 s/iter. ETA=0:01:28
[02/24 13:34:17] mask2former INFO: Inference done 963/1093. Dataloading: 0.0117 s/iter. Inference: 0.3774 s/iter. Eval: 0.2493 s/iter. Total: 0.6386 s/iter. ETA=0:01:23
[02/24 13:34:23] mask2former INFO: Inference done 972/1093. Dataloading: 0.0116 s/iter. Inference: 0.3773 s/iter. Eval: 0.2491 s/iter. Total: 0.6382 s/iter. ETA=0:01:17
[02/24 13:34:28] mask2former INFO: Inference done 980/1093. Dataloading: 0.0116 s/iter. Inference: 0.3774 s/iter. Eval: 0.2491 s/iter. Total: 0.6384 s/iter. ETA=0:01:12
[02/24 13:34:33] mask2former INFO: Inference done 988/1093. Dataloading: 0.0116 s/iter. Inference: 0.3775 s/iter. Eval: 0.2491 s/iter. Total: 0.6384 s/iter. ETA=0:01:07
[02/24 13:34:38] mask2former INFO: Inference done 996/1093. Dataloading: 0.0116 s/iter. Inference: 0.3776 s/iter. Eval: 0.2491 s/iter. Total: 0.6386 s/iter. ETA=0:01:01
[02/24 13:34:44] mask2former INFO: Inference done 1003/1093. Dataloading: 0.0116 s/iter. Inference: 0.3781 s/iter. Eval: 0.2494 s/iter. Total: 0.6393 s/iter. ETA=0:00:57
[02/24 13:34:49] mask2former INFO: Inference done 1011/1093. Dataloading: 0.0116 s/iter. Inference: 0.3781 s/iter. Eval: 0.2495 s/iter. Total: 0.6394 s/iter. ETA=0:00:52
[02/24 13:34:54] mask2former INFO: Inference done 1019/1093. Dataloading: 0.0115 s/iter. Inference: 0.3780 s/iter. Eval: 0.2495 s/iter. Total: 0.6393 s/iter. ETA=0:00:47
[02/24 13:34:59] mask2former INFO: Inference done 1027/1093. Dataloading: 0.0115 s/iter. Inference: 0.3781 s/iter. Eval: 0.2497 s/iter. Total: 0.6395 s/iter. ETA=0:00:42
[02/24 13:35:05] mask2former INFO: Inference done 1035/1093. Dataloading: 0.0116 s/iter. Inference: 0.3782 s/iter. Eval: 0.2499 s/iter. Total: 0.6399 s/iter. ETA=0:00:37
[02/24 13:35:10] mask2former INFO: Inference done 1044/1093. Dataloading: 0.0115 s/iter. Inference: 0.3780 s/iter. Eval: 0.2497 s/iter. Total: 0.6395 s/iter. ETA=0:00:31
[02/24 13:35:16] mask2former INFO: Inference done 1053/1093. Dataloading: 0.0116 s/iter. Inference: 0.3781 s/iter. Eval: 0.2494 s/iter. Total: 0.6393 s/iter. ETA=0:00:25
[02/24 13:35:21] mask2former INFO: Inference done 1061/1093. Dataloading: 0.0115 s/iter. Inference: 0.3783 s/iter. Eval: 0.2495 s/iter. Total: 0.6396 s/iter. ETA=0:00:20
[02/24 13:35:26] mask2former INFO: Inference done 1070/1093. Dataloading: 0.0115 s/iter. Inference: 0.3780 s/iter. Eval: 0.2492 s/iter. Total: 0.6390 s/iter. ETA=0:00:14
[02/24 13:35:31] mask2former INFO: Inference done 1079/1093. Dataloading: 0.0116 s/iter. Inference: 0.3778 s/iter. Eval: 0.2488 s/iter. Total: 0.6384 s/iter. ETA=0:00:08
[02/24 13:35:37] mask2former INFO: Inference done 1088/1093. Dataloading: 0.0116 s/iter. Inference: 0.3774 s/iter. Eval: 0.2489 s/iter. Total: 0.6382 s/iter. ETA=0:00:03
[02/24 14:47:16] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 14:47:17] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 14:47:17] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 14:47:41] mask2former INFO: Inference done 11/1093. Dataloading: 0.0061 s/iter. Inference: 0.3786 s/iter. Eval: 0.2897 s/iter. Total: 0.6745 s/iter. ETA=0:12:09
[02/24 14:47:47] mask2former INFO: Inference done 18/1093. Dataloading: 0.0090 s/iter. Inference: 0.4234 s/iter. Eval: 0.2791 s/iter. Total: 0.7117 s/iter. ETA=0:12:45
[02/24 14:47:52] mask2former INFO: Inference done 26/1093. Dataloading: 0.0119 s/iter. Inference: 0.4033 s/iter. Eval: 0.2830 s/iter. Total: 0.6984 s/iter. ETA=0:12:25
[02/24 14:47:57] mask2former INFO: Inference done 34/1093. Dataloading: 0.0110 s/iter. Inference: 0.3875 s/iter. Eval: 0.2802 s/iter. Total: 0.6788 s/iter. ETA=0:11:58
[02/24 14:48:02] mask2former INFO: Inference done 42/1093. Dataloading: 0.0110 s/iter. Inference: 0.3871 s/iter. Eval: 0.2726 s/iter. Total: 0.6708 s/iter. ETA=0:11:44
[02/24 14:48:08] mask2former INFO: Inference done 50/1093. Dataloading: 0.0117 s/iter. Inference: 0.3851 s/iter. Eval: 0.2751 s/iter. Total: 0.6721 s/iter. ETA=0:11:40
[02/24 14:48:13] mask2former INFO: Inference done 57/1093. Dataloading: 0.0121 s/iter. Inference: 0.3878 s/iter. Eval: 0.2803 s/iter. Total: 0.6803 s/iter. ETA=0:11:44
[02/24 14:48:18] mask2former INFO: Inference done 66/1093. Dataloading: 0.0119 s/iter. Inference: 0.3827 s/iter. Eval: 0.2771 s/iter. Total: 0.6718 s/iter. ETA=0:11:29
[02/24 14:48:24] mask2former INFO: Inference done 74/1093. Dataloading: 0.0121 s/iter. Inference: 0.3855 s/iter. Eval: 0.2735 s/iter. Total: 0.6713 s/iter. ETA=0:11:24
[02/24 14:48:29] mask2former INFO: Inference done 82/1093. Dataloading: 0.0133 s/iter. Inference: 0.3896 s/iter. Eval: 0.2700 s/iter. Total: 0.6731 s/iter. ETA=0:11:20
[02/24 14:48:34] mask2former INFO: Inference done 90/1093. Dataloading: 0.0128 s/iter. Inference: 0.3902 s/iter. Eval: 0.2674 s/iter. Total: 0.6707 s/iter. ETA=0:11:12
[02/24 14:48:39] mask2former INFO: Inference done 102/1093. Dataloading: 0.0122 s/iter. Inference: 0.3739 s/iter. Eval: 0.2532 s/iter. Total: 0.6396 s/iter. ETA=0:10:33
[02/24 14:48:44] mask2former INFO: Inference done 108/1093. Dataloading: 0.0132 s/iter. Inference: 0.3793 s/iter. Eval: 0.2598 s/iter. Total: 0.6525 s/iter. ETA=0:10:42
[02/24 14:48:50] mask2former INFO: Inference done 113/1093. Dataloading: 0.0133 s/iter. Inference: 0.3925 s/iter. Eval: 0.2636 s/iter. Total: 0.6698 s/iter. ETA=0:10:56
[02/24 14:48:55] mask2former INFO: Inference done 121/1093. Dataloading: 0.0134 s/iter. Inference: 0.3926 s/iter. Eval: 0.2631 s/iter. Total: 0.6695 s/iter. ETA=0:10:50
[02/24 14:49:00] mask2former INFO: Inference done 127/1093. Dataloading: 0.0135 s/iter. Inference: 0.3982 s/iter. Eval: 0.2681 s/iter. Total: 0.6801 s/iter. ETA=0:10:56
[02/24 14:49:05] mask2former INFO: Inference done 134/1093. Dataloading: 0.0138 s/iter. Inference: 0.3995 s/iter. Eval: 0.2690 s/iter. Total: 0.6828 s/iter. ETA=0:10:54
[02/24 14:49:10] mask2former INFO: Inference done 141/1093. Dataloading: 0.0137 s/iter. Inference: 0.3992 s/iter. Eval: 0.2722 s/iter. Total: 0.6856 s/iter. ETA=0:10:52
[02/24 14:49:16] mask2former INFO: Inference done 149/1093. Dataloading: 0.0134 s/iter. Inference: 0.3981 s/iter. Eval: 0.2738 s/iter. Total: 0.6857 s/iter. ETA=0:10:47
[02/24 14:49:21] mask2former INFO: Inference done 157/1093. Dataloading: 0.0135 s/iter. Inference: 0.3968 s/iter. Eval: 0.2743 s/iter. Total: 0.6849 s/iter. ETA=0:10:41
[02/24 14:49:27] mask2former INFO: Inference done 165/1093. Dataloading: 0.0134 s/iter. Inference: 0.3955 s/iter. Eval: 0.2737 s/iter. Total: 0.6830 s/iter. ETA=0:10:33
[02/24 14:49:32] mask2former INFO: Inference done 173/1093. Dataloading: 0.0132 s/iter. Inference: 0.3955 s/iter. Eval: 0.2729 s/iter. Total: 0.6819 s/iter. ETA=0:10:27
[02/24 14:49:37] mask2former INFO: Inference done 181/1093. Dataloading: 0.0132 s/iter. Inference: 0.3967 s/iter. Eval: 0.2715 s/iter. Total: 0.6816 s/iter. ETA=0:10:21
[02/24 14:49:42] mask2former INFO: Inference done 188/1093. Dataloading: 0.0135 s/iter. Inference: 0.3981 s/iter. Eval: 0.2719 s/iter. Total: 0.6839 s/iter. ETA=0:10:18
[02/24 14:49:48] mask2former INFO: Inference done 196/1093. Dataloading: 0.0133 s/iter. Inference: 0.3970 s/iter. Eval: 0.2723 s/iter. Total: 0.6830 s/iter. ETA=0:10:12
[02/24 14:49:53] mask2former INFO: Inference done 204/1093. Dataloading: 0.0131 s/iter. Inference: 0.3975 s/iter. Eval: 0.2712 s/iter. Total: 0.6822 s/iter. ETA=0:10:06
[02/24 14:49:58] mask2former INFO: Inference done 212/1093. Dataloading: 0.0130 s/iter. Inference: 0.3957 s/iter. Eval: 0.2716 s/iter. Total: 0.6807 s/iter. ETA=0:09:59
[02/24 14:50:03] mask2former INFO: Inference done 221/1093. Dataloading: 0.0129 s/iter. Inference: 0.3926 s/iter. Eval: 0.2699 s/iter. Total: 0.6757 s/iter. ETA=0:09:49
[02/24 14:50:09] mask2former INFO: Inference done 230/1093. Dataloading: 0.0129 s/iter. Inference: 0.3905 s/iter. Eval: 0.2702 s/iter. Total: 0.6739 s/iter. ETA=0:09:41
[02/24 14:50:14] mask2former INFO: Inference done 238/1093. Dataloading: 0.0129 s/iter. Inference: 0.3901 s/iter. Eval: 0.2711 s/iter. Total: 0.6745 s/iter. ETA=0:09:36
[02/24 14:50:20] mask2former INFO: Inference done 246/1093. Dataloading: 0.0128 s/iter. Inference: 0.3900 s/iter. Eval: 0.2702 s/iter. Total: 0.6733 s/iter. ETA=0:09:30
[02/24 14:50:25] mask2former INFO: Inference done 253/1093. Dataloading: 0.0128 s/iter. Inference: 0.3910 s/iter. Eval: 0.2704 s/iter. Total: 0.6745 s/iter. ETA=0:09:26
[02/24 14:50:30] mask2former INFO: Inference done 262/1093. Dataloading: 0.0127 s/iter. Inference: 0.3900 s/iter. Eval: 0.2691 s/iter. Total: 0.6721 s/iter. ETA=0:09:18
[02/24 14:50:35] mask2former INFO: Inference done 269/1093. Dataloading: 0.0129 s/iter. Inference: 0.3914 s/iter. Eval: 0.2691 s/iter. Total: 0.6737 s/iter. ETA=0:09:15
[02/24 14:50:41] mask2former INFO: Inference done 277/1093. Dataloading: 0.0131 s/iter. Inference: 0.3921 s/iter. Eval: 0.2687 s/iter. Total: 0.6741 s/iter. ETA=0:09:10
[02/24 14:50:46] mask2former INFO: Inference done 286/1093. Dataloading: 0.0130 s/iter. Inference: 0.3911 s/iter. Eval: 0.2680 s/iter. Total: 0.6725 s/iter. ETA=0:09:02
[02/24 14:50:52] mask2former INFO: Inference done 294/1093. Dataloading: 0.0130 s/iter. Inference: 0.3919 s/iter. Eval: 0.2678 s/iter. Total: 0.6730 s/iter. ETA=0:08:57
[02/24 14:50:57] mask2former INFO: Inference done 301/1093. Dataloading: 0.0130 s/iter. Inference: 0.3929 s/iter. Eval: 0.2685 s/iter. Total: 0.6747 s/iter. ETA=0:08:54
[02/24 14:51:02] mask2former INFO: Inference done 309/1093. Dataloading: 0.0130 s/iter. Inference: 0.3930 s/iter. Eval: 0.2684 s/iter. Total: 0.6746 s/iter. ETA=0:08:48
[02/24 14:51:07] mask2former INFO: Inference done 317/1093. Dataloading: 0.0131 s/iter. Inference: 0.3924 s/iter. Eval: 0.2677 s/iter. Total: 0.6735 s/iter. ETA=0:08:42
[02/24 14:51:13] mask2former INFO: Inference done 324/1093. Dataloading: 0.0130 s/iter. Inference: 0.3924 s/iter. Eval: 0.2693 s/iter. Total: 0.6750 s/iter. ETA=0:08:39
[02/24 14:51:18] mask2former INFO: Inference done 333/1093. Dataloading: 0.0131 s/iter. Inference: 0.3920 s/iter. Eval: 0.2682 s/iter. Total: 0.6736 s/iter. ETA=0:08:31
[02/24 14:51:23] mask2former INFO: Inference done 340/1093. Dataloading: 0.0131 s/iter. Inference: 0.3922 s/iter. Eval: 0.2689 s/iter. Total: 0.6745 s/iter. ETA=0:08:27
[02/24 14:51:28] mask2former INFO: Inference done 348/1093. Dataloading: 0.0130 s/iter. Inference: 0.3925 s/iter. Eval: 0.2681 s/iter. Total: 0.6739 s/iter. ETA=0:08:22
[02/24 14:51:34] mask2former INFO: Inference done 356/1093. Dataloading: 0.0130 s/iter. Inference: 0.3934 s/iter. Eval: 0.2674 s/iter. Total: 0.6742 s/iter. ETA=0:08:16
[02/24 14:51:39] mask2former INFO: Inference done 363/1093. Dataloading: 0.0130 s/iter. Inference: 0.3937 s/iter. Eval: 0.2680 s/iter. Total: 0.6751 s/iter. ETA=0:08:12
[02/24 14:51:45] mask2former INFO: Inference done 372/1093. Dataloading: 0.0130 s/iter. Inference: 0.3932 s/iter. Eval: 0.2672 s/iter. Total: 0.6737 s/iter. ETA=0:08:05
[02/24 14:51:50] mask2former INFO: Inference done 380/1093. Dataloading: 0.0130 s/iter. Inference: 0.3936 s/iter. Eval: 0.2673 s/iter. Total: 0.6742 s/iter. ETA=0:08:00
[02/24 14:51:56] mask2former INFO: Inference done 388/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2679 s/iter. Total: 0.6746 s/iter. ETA=0:07:55
[02/24 14:52:01] mask2former INFO: Inference done 396/1093. Dataloading: 0.0130 s/iter. Inference: 0.3940 s/iter. Eval: 0.2668 s/iter. Total: 0.6741 s/iter. ETA=0:07:49
[02/24 14:52:06] mask2former INFO: Inference done 404/1093. Dataloading: 0.0130 s/iter. Inference: 0.3939 s/iter. Eval: 0.2671 s/iter. Total: 0.6743 s/iter. ETA=0:07:44
[02/24 14:52:12] mask2former INFO: Inference done 411/1093. Dataloading: 0.0130 s/iter. Inference: 0.3948 s/iter. Eval: 0.2674 s/iter. Total: 0.6755 s/iter. ETA=0:07:40
[02/24 14:52:17] mask2former INFO: Inference done 419/1093. Dataloading: 0.0129 s/iter. Inference: 0.3948 s/iter. Eval: 0.2679 s/iter. Total: 0.6758 s/iter. ETA=0:07:35
[02/24 14:52:22] mask2former INFO: Inference done 427/1093. Dataloading: 0.0128 s/iter. Inference: 0.3942 s/iter. Eval: 0.2681 s/iter. Total: 0.6754 s/iter. ETA=0:07:29
[02/24 14:52:27] mask2former INFO: Inference done 435/1093. Dataloading: 0.0129 s/iter. Inference: 0.3939 s/iter. Eval: 0.2677 s/iter. Total: 0.6747 s/iter. ETA=0:07:23
[02/24 14:52:33] mask2former INFO: Inference done 443/1093. Dataloading: 0.0128 s/iter. Inference: 0.3943 s/iter. Eval: 0.2674 s/iter. Total: 0.6748 s/iter. ETA=0:07:18
[02/24 14:52:38] mask2former INFO: Inference done 451/1093. Dataloading: 0.0129 s/iter. Inference: 0.3943 s/iter. Eval: 0.2677 s/iter. Total: 0.6752 s/iter. ETA=0:07:13
[02/24 14:52:44] mask2former INFO: Inference done 459/1093. Dataloading: 0.0128 s/iter. Inference: 0.3948 s/iter. Eval: 0.2671 s/iter. Total: 0.6750 s/iter. ETA=0:07:07
[02/24 14:52:49] mask2former INFO: Inference done 466/1093. Dataloading: 0.0128 s/iter. Inference: 0.3950 s/iter. Eval: 0.2678 s/iter. Total: 0.6759 s/iter. ETA=0:07:03
[02/24 14:52:54] mask2former INFO: Inference done 474/1093. Dataloading: 0.0128 s/iter. Inference: 0.3951 s/iter. Eval: 0.2682 s/iter. Total: 0.6764 s/iter. ETA=0:06:58
[02/24 14:53:00] mask2former INFO: Inference done 482/1093. Dataloading: 0.0128 s/iter. Inference: 0.3956 s/iter. Eval: 0.2677 s/iter. Total: 0.6765 s/iter. ETA=0:06:53
[02/24 14:53:05] mask2former INFO: Inference done 490/1093. Dataloading: 0.0127 s/iter. Inference: 0.3960 s/iter. Eval: 0.2676 s/iter. Total: 0.6766 s/iter. ETA=0:06:48
[02/24 14:53:11] mask2former INFO: Inference done 498/1093. Dataloading: 0.0127 s/iter. Inference: 0.3960 s/iter. Eval: 0.2670 s/iter. Total: 0.6760 s/iter. ETA=0:06:42
[02/24 14:53:16] mask2former INFO: Inference done 506/1093. Dataloading: 0.0127 s/iter. Inference: 0.3954 s/iter. Eval: 0.2673 s/iter. Total: 0.6757 s/iter. ETA=0:06:36
[02/24 14:53:21] mask2former INFO: Inference done 513/1093. Dataloading: 0.0128 s/iter. Inference: 0.3961 s/iter. Eval: 0.2672 s/iter. Total: 0.6763 s/iter. ETA=0:06:32
[02/24 14:53:26] mask2former INFO: Inference done 520/1093. Dataloading: 0.0129 s/iter. Inference: 0.3965 s/iter. Eval: 0.2675 s/iter. Total: 0.6772 s/iter. ETA=0:06:28
[02/24 14:53:31] mask2former INFO: Inference done 529/1093. Dataloading: 0.0129 s/iter. Inference: 0.3959 s/iter. Eval: 0.2667 s/iter. Total: 0.6758 s/iter. ETA=0:06:21
[02/24 14:53:36] mask2former INFO: Inference done 537/1093. Dataloading: 0.0128 s/iter. Inference: 0.3951 s/iter. Eval: 0.2669 s/iter. Total: 0.6751 s/iter. ETA=0:06:15
[02/24 14:53:42] mask2former INFO: Inference done 545/1093. Dataloading: 0.0129 s/iter. Inference: 0.3947 s/iter. Eval: 0.2670 s/iter. Total: 0.6749 s/iter. ETA=0:06:09
[02/24 14:53:47] mask2former INFO: Inference done 553/1093. Dataloading: 0.0129 s/iter. Inference: 0.3946 s/iter. Eval: 0.2667 s/iter. Total: 0.6744 s/iter. ETA=0:06:04
[02/24 14:53:52] mask2former INFO: Inference done 560/1093. Dataloading: 0.0129 s/iter. Inference: 0.3949 s/iter. Eval: 0.2671 s/iter. Total: 0.6751 s/iter. ETA=0:05:59
[02/24 14:53:57] mask2former INFO: Inference done 568/1093. Dataloading: 0.0128 s/iter. Inference: 0.3948 s/iter. Eval: 0.2669 s/iter. Total: 0.6747 s/iter. ETA=0:05:54
[02/24 14:54:03] mask2former INFO: Inference done 576/1093. Dataloading: 0.0128 s/iter. Inference: 0.3949 s/iter. Eval: 0.2672 s/iter. Total: 0.6751 s/iter. ETA=0:05:49
[02/24 14:54:08] mask2former INFO: Inference done 584/1093. Dataloading: 0.0128 s/iter. Inference: 0.3945 s/iter. Eval: 0.2675 s/iter. Total: 0.6750 s/iter. ETA=0:05:43
[02/24 14:54:13] mask2former INFO: Inference done 592/1093. Dataloading: 0.0128 s/iter. Inference: 0.3941 s/iter. Eval: 0.2676 s/iter. Total: 0.6748 s/iter. ETA=0:05:38
[02/24 14:54:19] mask2former INFO: Inference done 601/1093. Dataloading: 0.0129 s/iter. Inference: 0.3938 s/iter. Eval: 0.2671 s/iter. Total: 0.6741 s/iter. ETA=0:05:31
[02/24 14:54:24] mask2former INFO: Inference done 609/1093. Dataloading: 0.0128 s/iter. Inference: 0.3939 s/iter. Eval: 0.2666 s/iter. Total: 0.6736 s/iter. ETA=0:05:26
[02/24 14:54:29] mask2former INFO: Inference done 617/1093. Dataloading: 0.0128 s/iter. Inference: 0.3936 s/iter. Eval: 0.2668 s/iter. Total: 0.6734 s/iter. ETA=0:05:20
[02/24 14:54:35] mask2former INFO: Inference done 625/1093. Dataloading: 0.0129 s/iter. Inference: 0.3940 s/iter. Eval: 0.2662 s/iter. Total: 0.6733 s/iter. ETA=0:05:15
[02/24 14:54:40] mask2former INFO: Inference done 633/1093. Dataloading: 0.0128 s/iter. Inference: 0.3944 s/iter. Eval: 0.2659 s/iter. Total: 0.6734 s/iter. ETA=0:05:09
[02/24 14:54:45] mask2former INFO: Inference done 641/1093. Dataloading: 0.0129 s/iter. Inference: 0.3946 s/iter. Eval: 0.2655 s/iter. Total: 0.6732 s/iter. ETA=0:05:04
[02/24 14:54:51] mask2former INFO: Inference done 649/1093. Dataloading: 0.0129 s/iter. Inference: 0.3941 s/iter. Eval: 0.2657 s/iter. Total: 0.6730 s/iter. ETA=0:04:58
[02/24 14:54:56] mask2former INFO: Inference done 657/1093. Dataloading: 0.0130 s/iter. Inference: 0.3937 s/iter. Eval: 0.2660 s/iter. Total: 0.6729 s/iter. ETA=0:04:53
[02/24 14:55:01] mask2former INFO: Inference done 665/1093. Dataloading: 0.0130 s/iter. Inference: 0.3938 s/iter. Eval: 0.2656 s/iter. Total: 0.6727 s/iter. ETA=0:04:47
[02/24 14:55:06] mask2former INFO: Inference done 673/1093. Dataloading: 0.0130 s/iter. Inference: 0.3934 s/iter. Eval: 0.2655 s/iter. Total: 0.6721 s/iter. ETA=0:04:42
[02/24 14:55:11] mask2former INFO: Inference done 681/1093. Dataloading: 0.0130 s/iter. Inference: 0.3929 s/iter. Eval: 0.2654 s/iter. Total: 0.6715 s/iter. ETA=0:04:36
[02/24 14:55:17] mask2former INFO: Inference done 689/1093. Dataloading: 0.0130 s/iter. Inference: 0.3930 s/iter. Eval: 0.2657 s/iter. Total: 0.6720 s/iter. ETA=0:04:31
[02/24 14:55:22] mask2former INFO: Inference done 700/1093. Dataloading: 0.0130 s/iter. Inference: 0.3916 s/iter. Eval: 0.2640 s/iter. Total: 0.6688 s/iter. ETA=0:04:22
[02/24 14:55:27] mask2former INFO: Inference done 709/1093. Dataloading: 0.0129 s/iter. Inference: 0.3912 s/iter. Eval: 0.2636 s/iter. Total: 0.6680 s/iter. ETA=0:04:16
[02/24 14:55:33] mask2former INFO: Inference done 715/1093. Dataloading: 0.0130 s/iter. Inference: 0.3926 s/iter. Eval: 0.2645 s/iter. Total: 0.6703 s/iter. ETA=0:04:13
[02/24 14:55:39] mask2former INFO: Inference done 722/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2647 s/iter. Total: 0.6713 s/iter. ETA=0:04:09
[02/24 14:55:44] mask2former INFO: Inference done 730/1093. Dataloading: 0.0131 s/iter. Inference: 0.3929 s/iter. Eval: 0.2647 s/iter. Total: 0.6710 s/iter. ETA=0:04:03
[02/24 14:55:49] mask2former INFO: Inference done 737/1093. Dataloading: 0.0132 s/iter. Inference: 0.3934 s/iter. Eval: 0.2652 s/iter. Total: 0.6720 s/iter. ETA=0:03:59
[02/24 14:55:54] mask2former INFO: Inference done 745/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2652 s/iter. Total: 0.6719 s/iter. ETA=0:03:53
[02/24 14:55:59] mask2former INFO: Inference done 753/1093. Dataloading: 0.0131 s/iter. Inference: 0.3932 s/iter. Eval: 0.2648 s/iter. Total: 0.6714 s/iter. ETA=0:03:48
[02/24 14:56:05] mask2former INFO: Inference done 762/1093. Dataloading: 0.0131 s/iter. Inference: 0.3931 s/iter. Eval: 0.2643 s/iter. Total: 0.6708 s/iter. ETA=0:03:42
[02/24 14:56:11] mask2former INFO: Inference done 770/1093. Dataloading: 0.0131 s/iter. Inference: 0.3934 s/iter. Eval: 0.2646 s/iter. Total: 0.6713 s/iter. ETA=0:03:36
[02/24 14:56:16] mask2former INFO: Inference done 778/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2647 s/iter. Total: 0.6714 s/iter. ETA=0:03:31
[02/24 14:56:21] mask2former INFO: Inference done 786/1093. Dataloading: 0.0131 s/iter. Inference: 0.3932 s/iter. Eval: 0.2646 s/iter. Total: 0.6712 s/iter. ETA=0:03:26
[02/24 14:56:27] mask2former INFO: Inference done 794/1093. Dataloading: 0.0131 s/iter. Inference: 0.3937 s/iter. Eval: 0.2645 s/iter. Total: 0.6716 s/iter. ETA=0:03:20
[02/24 14:56:32] mask2former INFO: Inference done 802/1093. Dataloading: 0.0131 s/iter. Inference: 0.3938 s/iter. Eval: 0.2643 s/iter. Total: 0.6715 s/iter. ETA=0:03:15
[02/24 14:56:38] mask2former INFO: Inference done 810/1093. Dataloading: 0.0131 s/iter. Inference: 0.3935 s/iter. Eval: 0.2645 s/iter. Total: 0.6714 s/iter. ETA=0:03:10
[02/24 14:56:43] mask2former INFO: Inference done 819/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2642 s/iter. Total: 0.6709 s/iter. ETA=0:03:03
[02/24 14:56:49] mask2former INFO: Inference done 827/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2641 s/iter. Total: 0.6708 s/iter. ETA=0:02:58
[02/24 14:56:54] mask2former INFO: Inference done 835/1093. Dataloading: 0.0130 s/iter. Inference: 0.3932 s/iter. Eval: 0.2643 s/iter. Total: 0.6708 s/iter. ETA=0:02:53
[02/24 14:56:59] mask2former INFO: Inference done 843/1093. Dataloading: 0.0131 s/iter. Inference: 0.3930 s/iter. Eval: 0.2643 s/iter. Total: 0.6706 s/iter. ETA=0:02:47
[02/24 14:57:05] mask2former INFO: Inference done 851/1093. Dataloading: 0.0131 s/iter. Inference: 0.3932 s/iter. Eval: 0.2642 s/iter. Total: 0.6707 s/iter. ETA=0:02:42
[02/24 14:57:10] mask2former INFO: Inference done 860/1093. Dataloading: 0.0131 s/iter. Inference: 0.3930 s/iter. Eval: 0.2638 s/iter. Total: 0.6701 s/iter. ETA=0:02:36
[02/24 14:57:16] mask2former INFO: Inference done 868/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2635 s/iter. Total: 0.6701 s/iter. ETA=0:02:30
[02/24 14:57:21] mask2former INFO: Inference done 876/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2634 s/iter. Total: 0.6700 s/iter. ETA=0:02:25
[02/24 14:57:26] mask2former INFO: Inference done 885/1093. Dataloading: 0.0130 s/iter. Inference: 0.3930 s/iter. Eval: 0.2627 s/iter. Total: 0.6690 s/iter. ETA=0:02:19
[02/24 14:57:31] mask2former INFO: Inference done 893/1093. Dataloading: 0.0130 s/iter. Inference: 0.3929 s/iter. Eval: 0.2627 s/iter. Total: 0.6689 s/iter. ETA=0:02:13
[02/24 14:57:36] mask2former INFO: Inference done 901/1093. Dataloading: 0.0130 s/iter. Inference: 0.3927 s/iter. Eval: 0.2625 s/iter. Total: 0.6685 s/iter. ETA=0:02:08
[02/24 14:57:42] mask2former INFO: Inference done 910/1093. Dataloading: 0.0130 s/iter. Inference: 0.3925 s/iter. Eval: 0.2624 s/iter. Total: 0.6682 s/iter. ETA=0:02:02
[02/24 14:57:47] mask2former INFO: Inference done 918/1093. Dataloading: 0.0130 s/iter. Inference: 0.3927 s/iter. Eval: 0.2623 s/iter. Total: 0.6682 s/iter. ETA=0:01:56
[02/24 14:57:53] mask2former INFO: Inference done 927/1093. Dataloading: 0.0130 s/iter. Inference: 0.3925 s/iter. Eval: 0.2621 s/iter. Total: 0.6679 s/iter. ETA=0:01:50
[02/24 14:57:58] mask2former INFO: Inference done 935/1093. Dataloading: 0.0130 s/iter. Inference: 0.3924 s/iter. Eval: 0.2620 s/iter. Total: 0.6676 s/iter. ETA=0:01:45
[02/24 14:58:04] mask2former INFO: Inference done 943/1093. Dataloading: 0.0130 s/iter. Inference: 0.3923 s/iter. Eval: 0.2621 s/iter. Total: 0.6677 s/iter. ETA=0:01:40
[02/24 14:58:09] mask2former INFO: Inference done 951/1093. Dataloading: 0.0131 s/iter. Inference: 0.3926 s/iter. Eval: 0.2621 s/iter. Total: 0.6680 s/iter. ETA=0:01:34
[02/24 14:58:15] mask2former INFO: Inference done 958/1093. Dataloading: 0.0132 s/iter. Inference: 0.3928 s/iter. Eval: 0.2624 s/iter. Total: 0.6688 s/iter. ETA=0:01:30
[02/24 14:58:20] mask2former INFO: Inference done 966/1093. Dataloading: 0.0132 s/iter. Inference: 0.3929 s/iter. Eval: 0.2624 s/iter. Total: 0.6689 s/iter. ETA=0:01:24
[02/24 14:58:26] mask2former INFO: Inference done 975/1093. Dataloading: 0.0132 s/iter. Inference: 0.3930 s/iter. Eval: 0.2621 s/iter. Total: 0.6686 s/iter. ETA=0:01:18
[02/24 14:58:31] mask2former INFO: Inference done 983/1093. Dataloading: 0.0132 s/iter. Inference: 0.3933 s/iter. Eval: 0.2620 s/iter. Total: 0.6687 s/iter. ETA=0:01:13
[02/24 14:58:37] mask2former INFO: Inference done 991/1093. Dataloading: 0.0132 s/iter. Inference: 0.3933 s/iter. Eval: 0.2622 s/iter. Total: 0.6689 s/iter. ETA=0:01:08
[02/24 14:58:42] mask2former INFO: Inference done 999/1093. Dataloading: 0.0131 s/iter. Inference: 0.3935 s/iter. Eval: 0.2622 s/iter. Total: 0.6692 s/iter. ETA=0:01:02
[02/24 14:58:47] mask2former INFO: Inference done 1006/1093. Dataloading: 0.0131 s/iter. Inference: 0.3938 s/iter. Eval: 0.2623 s/iter. Total: 0.6695 s/iter. ETA=0:00:58
[02/24 14:58:53] mask2former INFO: Inference done 1015/1093. Dataloading: 0.0131 s/iter. Inference: 0.3937 s/iter. Eval: 0.2621 s/iter. Total: 0.6691 s/iter. ETA=0:00:52
[02/24 14:58:58] mask2former INFO: Inference done 1023/1093. Dataloading: 0.0130 s/iter. Inference: 0.3937 s/iter. Eval: 0.2621 s/iter. Total: 0.6692 s/iter. ETA=0:00:46
[02/24 14:59:03] mask2former INFO: Inference done 1031/1093. Dataloading: 0.0131 s/iter. Inference: 0.3935 s/iter. Eval: 0.2620 s/iter. Total: 0.6688 s/iter. ETA=0:00:41
[02/24 14:59:09] mask2former INFO: Inference done 1039/1093. Dataloading: 0.0130 s/iter. Inference: 0.3935 s/iter. Eval: 0.2620 s/iter. Total: 0.6688 s/iter. ETA=0:00:36
[02/24 14:59:14] mask2former INFO: Inference done 1047/1093. Dataloading: 0.0131 s/iter. Inference: 0.3937 s/iter. Eval: 0.2620 s/iter. Total: 0.6690 s/iter. ETA=0:00:30
[02/24 14:59:20] mask2former INFO: Inference done 1055/1093. Dataloading: 0.0131 s/iter. Inference: 0.3937 s/iter. Eval: 0.2619 s/iter. Total: 0.6690 s/iter. ETA=0:00:25
[02/24 14:59:25] mask2former INFO: Inference done 1063/1093. Dataloading: 0.0131 s/iter. Inference: 0.3934 s/iter. Eval: 0.2619 s/iter. Total: 0.6687 s/iter. ETA=0:00:20
[02/24 14:59:30] mask2former INFO: Inference done 1072/1093. Dataloading: 0.0131 s/iter. Inference: 0.3931 s/iter. Eval: 0.2616 s/iter. Total: 0.6681 s/iter. ETA=0:00:14
[02/24 14:59:35] mask2former INFO: Inference done 1080/1093. Dataloading: 0.0131 s/iter. Inference: 0.3933 s/iter. Eval: 0.2614 s/iter. Total: 0.6681 s/iter. ETA=0:00:08
[02/24 14:59:41] mask2former INFO: Inference done 1088/1093. Dataloading: 0.0131 s/iter. Inference: 0.3935 s/iter. Eval: 0.2615 s/iter. Total: 0.6684 s/iter. ETA=0:00:03
[02/24 16:20:44] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/24 16:20:47] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/24 16:20:47] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/24 16:21:37] mask2former INFO: Inference done 11/1093. Dataloading: 0.0243 s/iter. Inference: 0.8155 s/iter. Eval: 0.8294 s/iter. Total: 1.6692 s/iter. ETA=0:30:06
[02/24 16:21:43] mask2former INFO: Inference done 15/1093. Dataloading: 0.0226 s/iter. Inference: 0.7989 s/iter. Eval: 0.7764 s/iter. Total: 1.5987 s/iter. ETA=0:28:43
[02/24 16:21:48] mask2former INFO: Inference done 19/1093. Dataloading: 0.0204 s/iter. Inference: 0.7449 s/iter. Eval: 0.7354 s/iter. Total: 1.5028 s/iter. ETA=0:26:53
[02/24 16:21:54] mask2former INFO: Inference done 23/1093. Dataloading: 0.0193 s/iter. Inference: 0.7609 s/iter. Eval: 0.6921 s/iter. Total: 1.4743 s/iter. ETA=0:26:17
[02/24 16:21:59] mask2former INFO: Inference done 27/1093. Dataloading: 0.0187 s/iter. Inference: 0.7806 s/iter. Eval: 0.6618 s/iter. Total: 1.4628 s/iter. ETA=0:25:59
[02/24 16:22:05] mask2former INFO: Inference done 30/1093. Dataloading: 0.0203 s/iter. Inference: 0.8269 s/iter. Eval: 0.6756 s/iter. Total: 1.5242 s/iter. ETA=0:27:00
[02/24 16:22:11] mask2former INFO: Inference done 34/1093. Dataloading: 0.0199 s/iter. Inference: 0.8360 s/iter. Eval: 0.6731 s/iter. Total: 1.5307 s/iter. ETA=0:27:00
[02/24 16:22:17] mask2former INFO: Inference done 39/1093. Dataloading: 0.0192 s/iter. Inference: 0.8252 s/iter. Eval: 0.6304 s/iter. Total: 1.4762 s/iter. ETA=0:25:55
[02/24 16:22:23] mask2former INFO: Inference done 44/1093. Dataloading: 0.0189 s/iter. Inference: 0.8042 s/iter. Eval: 0.6114 s/iter. Total: 1.4359 s/iter. ETA=0:25:06
[02/24 16:22:29] mask2former INFO: Inference done 47/1093. Dataloading: 0.0194 s/iter. Inference: 0.8183 s/iter. Eval: 0.6301 s/iter. Total: 1.4690 s/iter. ETA=0:25:36
[02/24 16:22:34] mask2former INFO: Inference done 52/1093. Dataloading: 0.0199 s/iter. Inference: 0.8129 s/iter. Eval: 0.6007 s/iter. Total: 1.4346 s/iter. ETA=0:24:53
[02/24 16:22:40] mask2former INFO: Inference done 57/1093. Dataloading: 0.0213 s/iter. Inference: 0.7961 s/iter. Eval: 0.5794 s/iter. Total: 1.3979 s/iter. ETA=0:24:08
[02/24 16:22:46] mask2former INFO: Inference done 61/1093. Dataloading: 0.0210 s/iter. Inference: 0.7951 s/iter. Eval: 0.5878 s/iter. Total: 1.4049 s/iter. ETA=0:24:09
[02/24 16:22:52] mask2former INFO: Inference done 65/1093. Dataloading: 0.0216 s/iter. Inference: 0.7920 s/iter. Eval: 0.5958 s/iter. Total: 1.4104 s/iter. ETA=0:24:09
[02/24 16:22:57] mask2former INFO: Inference done 68/1093. Dataloading: 0.0214 s/iter. Inference: 0.8024 s/iter. Eval: 0.6027 s/iter. Total: 1.4274 s/iter. ETA=0:24:23
[02/24 16:23:03] mask2former INFO: Inference done 72/1093. Dataloading: 0.0218 s/iter. Inference: 0.8028 s/iter. Eval: 0.6076 s/iter. Total: 1.4331 s/iter. ETA=0:24:23
[02/24 16:23:10] mask2former INFO: Inference done 76/1093. Dataloading: 0.0218 s/iter. Inference: 0.8108 s/iter. Eval: 0.6114 s/iter. Total: 1.4448 s/iter. ETA=0:24:29
[02/24 16:23:15] mask2former INFO: Inference done 80/1093. Dataloading: 0.0221 s/iter. Inference: 0.8061 s/iter. Eval: 0.6109 s/iter. Total: 1.4400 s/iter. ETA=0:24:18
[02/24 16:23:20] mask2former INFO: Inference done 84/1093. Dataloading: 0.0221 s/iter. Inference: 0.8020 s/iter. Eval: 0.6075 s/iter. Total: 1.4325 s/iter. ETA=0:24:05
[02/24 16:23:26] mask2former INFO: Inference done 88/1093. Dataloading: 0.0223 s/iter. Inference: 0.8080 s/iter. Eval: 0.6059 s/iter. Total: 1.4370 s/iter. ETA=0:24:04
[02/24 16:23:33] mask2former INFO: Inference done 92/1093. Dataloading: 0.0222 s/iter. Inference: 0.8181 s/iter. Eval: 0.6127 s/iter. Total: 1.4538 s/iter. ETA=0:24:15
[02/24 16:23:40] mask2former INFO: Inference done 97/1093. Dataloading: 0.0231 s/iter. Inference: 0.8155 s/iter. Eval: 0.6062 s/iter. Total: 1.4456 s/iter. ETA=0:23:59
[02/24 16:23:45] mask2former INFO: Inference done 101/1093. Dataloading: 0.0232 s/iter. Inference: 0.8108 s/iter. Eval: 0.6047 s/iter. Total: 1.4395 s/iter. ETA=0:23:47
[02/24 16:23:50] mask2former INFO: Inference done 105/1093. Dataloading: 0.0228 s/iter. Inference: 0.8112 s/iter. Eval: 0.5992 s/iter. Total: 1.4339 s/iter. ETA=0:23:36
[02/24 16:23:56] mask2former INFO: Inference done 109/1093. Dataloading: 0.0233 s/iter. Inference: 0.8107 s/iter. Eval: 0.5955 s/iter. Total: 1.4302 s/iter. ETA=0:23:27
[02/24 16:24:02] mask2former INFO: Inference done 114/1093. Dataloading: 0.0230 s/iter. Inference: 0.8059 s/iter. Eval: 0.5914 s/iter. Total: 1.4210 s/iter. ETA=0:23:11
[02/24 16:24:08] mask2former INFO: Inference done 118/1093. Dataloading: 0.0228 s/iter. Inference: 0.8099 s/iter. Eval: 0.5896 s/iter. Total: 1.4233 s/iter. ETA=0:23:07
[02/24 16:24:14] mask2former INFO: Inference done 123/1093. Dataloading: 0.0228 s/iter. Inference: 0.8075 s/iter. Eval: 0.5837 s/iter. Total: 1.4149 s/iter. ETA=0:22:52
[02/24 16:24:19] mask2former INFO: Inference done 128/1093. Dataloading: 0.0228 s/iter. Inference: 0.7987 s/iter. Eval: 0.5762 s/iter. Total: 1.3985 s/iter. ETA=0:22:29
[02/24 16:24:24] mask2former INFO: Inference done 132/1093. Dataloading: 0.0230 s/iter. Inference: 0.7954 s/iter. Eval: 0.5760 s/iter. Total: 1.3952 s/iter. ETA=0:22:20
[02/24 16:24:30] mask2former INFO: Inference done 135/1093. Dataloading: 0.0229 s/iter. Inference: 0.7979 s/iter. Eval: 0.5837 s/iter. Total: 1.4054 s/iter. ETA=0:22:26
[02/24 16:24:35] mask2former INFO: Inference done 139/1093. Dataloading: 0.0228 s/iter. Inference: 0.7966 s/iter. Eval: 0.5844 s/iter. Total: 1.4048 s/iter. ETA=0:22:20
[02/24 16:24:41] mask2former INFO: Inference done 143/1093. Dataloading: 0.0232 s/iter. Inference: 0.7999 s/iter. Eval: 0.5845 s/iter. Total: 1.4085 s/iter. ETA=0:22:18
[02/24 16:24:47] mask2former INFO: Inference done 147/1093. Dataloading: 0.0232 s/iter. Inference: 0.7987 s/iter. Eval: 0.5833 s/iter. Total: 1.4062 s/iter. ETA=0:22:10
[02/24 16:24:52] mask2former INFO: Inference done 150/1093. Dataloading: 0.0234 s/iter. Inference: 0.8050 s/iter. Eval: 0.5849 s/iter. Total: 1.4142 s/iter. ETA=0:22:13
[02/24 16:24:58] mask2former INFO: Inference done 154/1093. Dataloading: 0.0234 s/iter. Inference: 0.8049 s/iter. Eval: 0.5872 s/iter. Total: 1.4164 s/iter. ETA=0:22:10
[02/24 16:25:04] mask2former INFO: Inference done 159/1093. Dataloading: 0.0236 s/iter. Inference: 0.7996 s/iter. Eval: 0.5851 s/iter. Total: 1.4093 s/iter. ETA=0:21:56
[02/24 16:25:09] mask2former INFO: Inference done 163/1093. Dataloading: 0.0236 s/iter. Inference: 0.7997 s/iter. Eval: 0.5822 s/iter. Total: 1.4065 s/iter. ETA=0:21:48
[02/24 16:25:15] mask2former INFO: Inference done 167/1093. Dataloading: 0.0235 s/iter. Inference: 0.7984 s/iter. Eval: 0.5860 s/iter. Total: 1.4088 s/iter. ETA=0:21:44
[02/24 16:25:20] mask2former INFO: Inference done 171/1093. Dataloading: 0.0236 s/iter. Inference: 0.7969 s/iter. Eval: 0.5851 s/iter. Total: 1.4066 s/iter. ETA=0:21:36
[02/24 16:25:26] mask2former INFO: Inference done 175/1093. Dataloading: 0.0236 s/iter. Inference: 0.7966 s/iter. Eval: 0.5869 s/iter. Total: 1.4081 s/iter. ETA=0:21:32
[02/24 16:25:32] mask2former INFO: Inference done 178/1093. Dataloading: 0.0236 s/iter. Inference: 0.8020 s/iter. Eval: 0.5877 s/iter. Total: 1.4143 s/iter. ETA=0:21:34
[02/24 16:25:37] mask2former INFO: Inference done 181/1093. Dataloading: 0.0237 s/iter. Inference: 0.8029 s/iter. Eval: 0.5912 s/iter. Total: 1.4188 s/iter. ETA=0:21:33
[02/24 16:25:42] mask2former INFO: Inference done 185/1093. Dataloading: 0.0239 s/iter. Inference: 0.8026 s/iter. Eval: 0.5888 s/iter. Total: 1.4163 s/iter. ETA=0:21:25
[02/24 16:25:48] mask2former INFO: Inference done 190/1093. Dataloading: 0.0236 s/iter. Inference: 0.7987 s/iter. Eval: 0.5850 s/iter. Total: 1.4082 s/iter. ETA=0:21:11
[02/24 16:25:53] mask2former INFO: Inference done 195/1093. Dataloading: 0.0235 s/iter. Inference: 0.7931 s/iter. Eval: 0.5811 s/iter. Total: 1.3986 s/iter. ETA=0:20:55
[02/24 16:25:58] mask2former INFO: Inference done 200/1093. Dataloading: 0.0233 s/iter. Inference: 0.7879 s/iter. Eval: 0.5800 s/iter. Total: 1.3923 s/iter. ETA=0:20:43
[02/24 16:26:04] mask2former INFO: Inference done 204/1093. Dataloading: 0.0235 s/iter. Inference: 0.7860 s/iter. Eval: 0.5796 s/iter. Total: 1.3902 s/iter. ETA=0:20:35
[02/24 16:26:09] mask2former INFO: Inference done 208/1093. Dataloading: 0.0234 s/iter. Inference: 0.7867 s/iter. Eval: 0.5791 s/iter. Total: 1.3904 s/iter. ETA=0:20:30
[02/24 16:26:15] mask2former INFO: Inference done 212/1093. Dataloading: 0.0236 s/iter. Inference: 0.7871 s/iter. Eval: 0.5810 s/iter. Total: 1.3928 s/iter. ETA=0:20:27
[02/24 16:26:21] mask2former INFO: Inference done 216/1093. Dataloading: 0.0235 s/iter. Inference: 0.7890 s/iter. Eval: 0.5801 s/iter. Total: 1.3937 s/iter. ETA=0:20:22
[02/24 16:26:27] mask2former INFO: Inference done 221/1093. Dataloading: 0.0239 s/iter. Inference: 0.7860 s/iter. Eval: 0.5791 s/iter. Total: 1.3901 s/iter. ETA=0:20:12
[02/24 16:26:32] mask2former INFO: Inference done 224/1093. Dataloading: 0.0240 s/iter. Inference: 0.7889 s/iter. Eval: 0.5809 s/iter. Total: 1.3949 s/iter. ETA=0:20:12
[02/24 16:26:38] mask2former INFO: Inference done 228/1093. Dataloading: 0.0244 s/iter. Inference: 0.7892 s/iter. Eval: 0.5799 s/iter. Total: 1.3947 s/iter. ETA=0:20:06
[02/24 16:26:44] mask2former INFO: Inference done 232/1093. Dataloading: 0.0249 s/iter. Inference: 0.7902 s/iter. Eval: 0.5800 s/iter. Total: 1.3963 s/iter. ETA=0:20:02
[02/24 16:26:49] mask2former INFO: Inference done 236/1093. Dataloading: 0.0250 s/iter. Inference: 0.7904 s/iter. Eval: 0.5780 s/iter. Total: 1.3947 s/iter. ETA=0:19:55
[02/24 16:26:55] mask2former INFO: Inference done 240/1093. Dataloading: 0.0247 s/iter. Inference: 0.7900 s/iter. Eval: 0.5800 s/iter. Total: 1.3959 s/iter. ETA=0:19:50
[02/24 16:27:02] mask2former INFO: Inference done 245/1093. Dataloading: 0.0247 s/iter. Inference: 0.7892 s/iter. Eval: 0.5790 s/iter. Total: 1.3941 s/iter. ETA=0:19:42
[02/24 16:27:09] mask2former INFO: Inference done 250/1093. Dataloading: 0.0246 s/iter. Inference: 0.7892 s/iter. Eval: 0.5794 s/iter. Total: 1.3944 s/iter. ETA=0:19:35
[02/24 16:27:14] mask2former INFO: Inference done 254/1093. Dataloading: 0.0245 s/iter. Inference: 0.7907 s/iter. Eval: 0.5789 s/iter. Total: 1.3952 s/iter. ETA=0:19:30
[02/24 16:27:20] mask2former INFO: Inference done 258/1093. Dataloading: 0.0245 s/iter. Inference: 0.7914 s/iter. Eval: 0.5790 s/iter. Total: 1.3961 s/iter. ETA=0:19:25
[02/24 16:27:26] mask2former INFO: Inference done 261/1093. Dataloading: 0.0247 s/iter. Inference: 0.7932 s/iter. Eval: 0.5821 s/iter. Total: 1.4011 s/iter. ETA=0:19:25
[02/24 16:27:32] mask2former INFO: Inference done 265/1093. Dataloading: 0.0254 s/iter. Inference: 0.7933 s/iter. Eval: 0.5823 s/iter. Total: 1.4022 s/iter. ETA=0:19:21
[02/24 16:27:37] mask2former INFO: Inference done 269/1093. Dataloading: 0.0254 s/iter. Inference: 0.7917 s/iter. Eval: 0.5820 s/iter. Total: 1.4003 s/iter. ETA=0:19:13
[02/24 16:27:43] mask2former INFO: Inference done 273/1093. Dataloading: 0.0254 s/iter. Inference: 0.7932 s/iter. Eval: 0.5830 s/iter. Total: 1.4030 s/iter. ETA=0:19:10
[02/24 16:27:48] mask2former INFO: Inference done 276/1093. Dataloading: 0.0255 s/iter. Inference: 0.7955 s/iter. Eval: 0.5845 s/iter. Total: 1.4068 s/iter. ETA=0:19:09
[02/24 16:27:54] mask2former INFO: Inference done 280/1093. Dataloading: 0.0259 s/iter. Inference: 0.7967 s/iter. Eval: 0.5848 s/iter. Total: 1.4088 s/iter. ETA=0:19:05
[02/24 16:28:00] mask2former INFO: Inference done 284/1093. Dataloading: 0.0258 s/iter. Inference: 0.7976 s/iter. Eval: 0.5829 s/iter. Total: 1.4077 s/iter. ETA=0:18:58
[02/24 16:28:05] mask2former INFO: Inference done 288/1093. Dataloading: 0.0259 s/iter. Inference: 0.7973 s/iter. Eval: 0.5823 s/iter. Total: 1.4069 s/iter. ETA=0:18:52
[02/24 16:28:11] mask2former INFO: Inference done 292/1093. Dataloading: 0.0259 s/iter. Inference: 0.7978 s/iter. Eval: 0.5810 s/iter. Total: 1.4061 s/iter. ETA=0:18:46
[02/24 16:28:16] mask2former INFO: Inference done 295/1093. Dataloading: 0.0259 s/iter. Inference: 0.8011 s/iter. Eval: 0.5832 s/iter. Total: 1.4117 s/iter. ETA=0:18:46
[02/24 16:28:22] mask2former INFO: Inference done 298/1093. Dataloading: 0.0258 s/iter. Inference: 0.8033 s/iter. Eval: 0.5844 s/iter. Total: 1.4149 s/iter. ETA=0:18:44
[02/24 16:28:27] mask2former INFO: Inference done 301/1093. Dataloading: 0.0261 s/iter. Inference: 0.8053 s/iter. Eval: 0.5848 s/iter. Total: 1.4176 s/iter. ETA=0:18:42
[02/24 16:28:33] mask2former INFO: Inference done 305/1093. Dataloading: 0.0264 s/iter. Inference: 0.8062 s/iter. Eval: 0.5862 s/iter. Total: 1.4202 s/iter. ETA=0:18:39
[02/24 16:28:38] mask2former INFO: Inference done 308/1093. Dataloading: 0.0263 s/iter. Inference: 0.8077 s/iter. Eval: 0.5875 s/iter. Total: 1.4230 s/iter. ETA=0:18:37
[02/24 16:28:44] mask2former INFO: Inference done 312/1093. Dataloading: 0.0262 s/iter. Inference: 0.8071 s/iter. Eval: 0.5879 s/iter. Total: 1.4226 s/iter. ETA=0:18:31
[02/24 16:28:49] mask2former INFO: Inference done 316/1093. Dataloading: 0.0263 s/iter. Inference: 0.8068 s/iter. Eval: 0.5870 s/iter. Total: 1.4215 s/iter. ETA=0:18:24
[02/24 16:28:54] mask2former INFO: Inference done 319/1093. Dataloading: 0.0262 s/iter. Inference: 0.8083 s/iter. Eval: 0.5879 s/iter. Total: 1.4238 s/iter. ETA=0:18:22
[02/24 16:29:01] mask2former INFO: Inference done 323/1093. Dataloading: 0.0265 s/iter. Inference: 0.8094 s/iter. Eval: 0.5899 s/iter. Total: 1.4272 s/iter. ETA=0:18:18
[02/24 16:29:07] mask2former INFO: Inference done 328/1093. Dataloading: 0.0267 s/iter. Inference: 0.8084 s/iter. Eval: 0.5876 s/iter. Total: 1.4240 s/iter. ETA=0:18:09
[02/24 16:29:12] mask2former INFO: Inference done 331/1093. Dataloading: 0.0266 s/iter. Inference: 0.8107 s/iter. Eval: 0.5889 s/iter. Total: 1.4276 s/iter. ETA=0:18:07
[02/24 16:29:18] mask2former INFO: Inference done 335/1093. Dataloading: 0.0266 s/iter. Inference: 0.8098 s/iter. Eval: 0.5898 s/iter. Total: 1.4275 s/iter. ETA=0:18:02
[02/24 16:29:24] mask2former INFO: Inference done 339/1093. Dataloading: 0.0265 s/iter. Inference: 0.8108 s/iter. Eval: 0.5883 s/iter. Total: 1.4270 s/iter. ETA=0:17:55
[02/24 16:29:29] mask2former INFO: Inference done 342/1093. Dataloading: 0.0264 s/iter. Inference: 0.8106 s/iter. Eval: 0.5916 s/iter. Total: 1.4300 s/iter. ETA=0:17:53
[02/24 16:29:35] mask2former INFO: Inference done 346/1093. Dataloading: 0.0265 s/iter. Inference: 0.8121 s/iter. Eval: 0.5921 s/iter. Total: 1.4322 s/iter. ETA=0:17:49
[02/24 16:29:41] mask2former INFO: Inference done 350/1093. Dataloading: 0.0265 s/iter. Inference: 0.8115 s/iter. Eval: 0.5922 s/iter. Total: 1.4317 s/iter. ETA=0:17:43
[02/24 16:29:47] mask2former INFO: Inference done 353/1093. Dataloading: 0.0269 s/iter. Inference: 0.8137 s/iter. Eval: 0.5948 s/iter. Total: 1.4368 s/iter. ETA=0:17:43
[02/24 16:29:53] mask2former INFO: Inference done 357/1093. Dataloading: 0.0268 s/iter. Inference: 0.8130 s/iter. Eval: 0.5974 s/iter. Total: 1.4386 s/iter. ETA=0:17:38
[02/24 16:29:59] mask2former INFO: Inference done 360/1093. Dataloading: 0.0268 s/iter. Inference: 0.8159 s/iter. Eval: 0.5980 s/iter. Total: 1.4421 s/iter. ETA=0:17:37
[02/24 16:30:05] mask2former INFO: Inference done 364/1093. Dataloading: 0.0270 s/iter. Inference: 0.8174 s/iter. Eval: 0.5969 s/iter. Total: 1.4427 s/iter. ETA=0:17:31
[02/24 16:30:10] mask2former INFO: Inference done 367/1093. Dataloading: 0.0270 s/iter. Inference: 0.8205 s/iter. Eval: 0.5969 s/iter. Total: 1.4457 s/iter. ETA=0:17:29
[02/24 16:30:16] mask2former INFO: Inference done 371/1093. Dataloading: 0.0269 s/iter. Inference: 0.8204 s/iter. Eval: 0.5965 s/iter. Total: 1.4452 s/iter. ETA=0:17:23
[02/24 16:30:22] mask2former INFO: Inference done 375/1093. Dataloading: 0.0270 s/iter. Inference: 0.8207 s/iter. Eval: 0.5965 s/iter. Total: 1.4455 s/iter. ETA=0:17:17
[02/24 16:30:28] mask2former INFO: Inference done 380/1093. Dataloading: 0.0269 s/iter. Inference: 0.8185 s/iter. Eval: 0.5955 s/iter. Total: 1.4423 s/iter. ETA=0:17:08
[02/24 16:30:33] mask2former INFO: Inference done 383/1093. Dataloading: 0.0268 s/iter. Inference: 0.8202 s/iter. Eval: 0.5968 s/iter. Total: 1.4453 s/iter. ETA=0:17:06
[02/24 16:30:39] mask2former INFO: Inference done 387/1093. Dataloading: 0.0268 s/iter. Inference: 0.8204 s/iter. Eval: 0.5966 s/iter. Total: 1.4452 s/iter. ETA=0:17:00
[02/24 16:30:45] mask2former INFO: Inference done 391/1093. Dataloading: 0.0268 s/iter. Inference: 0.8206 s/iter. Eval: 0.5973 s/iter. Total: 1.4460 s/iter. ETA=0:16:55
[02/24 16:30:51] mask2former INFO: Inference done 394/1093. Dataloading: 0.0268 s/iter. Inference: 0.8233 s/iter. Eval: 0.5989 s/iter. Total: 1.4505 s/iter. ETA=0:16:53
[02/24 16:30:57] mask2former INFO: Inference done 398/1093. Dataloading: 0.0268 s/iter. Inference: 0.8231 s/iter. Eval: 0.5985 s/iter. Total: 1.4497 s/iter. ETA=0:16:47
[02/24 16:31:03] mask2former INFO: Inference done 402/1093. Dataloading: 0.0268 s/iter. Inference: 0.8240 s/iter. Eval: 0.5993 s/iter. Total: 1.4514 s/iter. ETA=0:16:42
[02/24 16:31:09] mask2former INFO: Inference done 406/1093. Dataloading: 0.0269 s/iter. Inference: 0.8238 s/iter. Eval: 0.5985 s/iter. Total: 1.4506 s/iter. ETA=0:16:36
[02/24 16:31:15] mask2former INFO: Inference done 410/1093. Dataloading: 0.0269 s/iter. Inference: 0.8243 s/iter. Eval: 0.5982 s/iter. Total: 1.4508 s/iter. ETA=0:16:30
[02/24 16:31:20] mask2former INFO: Inference done 413/1093. Dataloading: 0.0271 s/iter. Inference: 0.8249 s/iter. Eval: 0.5996 s/iter. Total: 1.4530 s/iter. ETA=0:16:28
[02/24 16:31:25] mask2former INFO: Inference done 416/1093. Dataloading: 0.0273 s/iter. Inference: 0.8259 s/iter. Eval: 0.6009 s/iter. Total: 1.4554 s/iter. ETA=0:16:25
[02/24 16:31:31] mask2former INFO: Inference done 420/1093. Dataloading: 0.0273 s/iter. Inference: 0.8264 s/iter. Eval: 0.6004 s/iter. Total: 1.4555 s/iter. ETA=0:16:19
[02/24 16:31:38] mask2former INFO: Inference done 424/1093. Dataloading: 0.0274 s/iter. Inference: 0.8280 s/iter. Eval: 0.6014 s/iter. Total: 1.4582 s/iter. ETA=0:16:15
[02/24 16:31:45] mask2former INFO: Inference done 428/1093. Dataloading: 0.0274 s/iter. Inference: 0.8281 s/iter. Eval: 0.6032 s/iter. Total: 1.4602 s/iter. ETA=0:16:11
[02/24 16:31:50] mask2former INFO: Inference done 432/1093. Dataloading: 0.0274 s/iter. Inference: 0.8271 s/iter. Eval: 0.6036 s/iter. Total: 1.4595 s/iter. ETA=0:16:04
[02/24 16:31:56] mask2former INFO: Inference done 435/1093. Dataloading: 0.0273 s/iter. Inference: 0.8282 s/iter. Eval: 0.6047 s/iter. Total: 1.4617 s/iter. ETA=0:16:01
[02/24 16:32:01] mask2former INFO: Inference done 439/1093. Dataloading: 0.0274 s/iter. Inference: 0.8276 s/iter. Eval: 0.6033 s/iter. Total: 1.4597 s/iter. ETA=0:15:54
[02/24 16:32:06] mask2former INFO: Inference done 442/1093. Dataloading: 0.0274 s/iter. Inference: 0.8280 s/iter. Eval: 0.6047 s/iter. Total: 1.4615 s/iter. ETA=0:15:51
[02/24 16:32:11] mask2former INFO: Inference done 446/1093. Dataloading: 0.0275 s/iter. Inference: 0.8276 s/iter. Eval: 0.6034 s/iter. Total: 1.4599 s/iter. ETA=0:15:44
[02/24 16:32:17] mask2former INFO: Inference done 450/1093. Dataloading: 0.0274 s/iter. Inference: 0.8279 s/iter. Eval: 0.6039 s/iter. Total: 1.4606 s/iter. ETA=0:15:39
[02/24 16:32:22] mask2former INFO: Inference done 454/1093. Dataloading: 0.0272 s/iter. Inference: 0.8270 s/iter. Eval: 0.6033 s/iter. Total: 1.4590 s/iter. ETA=0:15:32
[02/24 16:32:28] mask2former INFO: Inference done 457/1093. Dataloading: 0.0275 s/iter. Inference: 0.8284 s/iter. Eval: 0.6047 s/iter. Total: 1.4621 s/iter. ETA=0:15:29
[02/24 16:32:33] mask2former INFO: Inference done 460/1093. Dataloading: 0.0275 s/iter. Inference: 0.8299 s/iter. Eval: 0.6054 s/iter. Total: 1.4643 s/iter. ETA=0:15:26
[02/24 16:32:38] mask2former INFO: Inference done 464/1093. Dataloading: 0.0276 s/iter. Inference: 0.8294 s/iter. Eval: 0.6042 s/iter. Total: 1.4627 s/iter. ETA=0:15:20
[02/24 16:32:44] mask2former INFO: Inference done 468/1093. Dataloading: 0.0276 s/iter. Inference: 0.8290 s/iter. Eval: 0.6043 s/iter. Total: 1.4624 s/iter. ETA=0:15:14
[02/24 16:32:50] mask2former INFO: Inference done 472/1093. Dataloading: 0.0277 s/iter. Inference: 0.8282 s/iter. Eval: 0.6050 s/iter. Total: 1.4625 s/iter. ETA=0:15:08
[02/24 16:32:56] mask2former INFO: Inference done 476/1093. Dataloading: 0.0278 s/iter. Inference: 0.8282 s/iter. Eval: 0.6048 s/iter. Total: 1.4624 s/iter. ETA=0:15:02
[02/24 16:33:02] mask2former INFO: Inference done 481/1093. Dataloading: 0.0277 s/iter. Inference: 0.8266 s/iter. Eval: 0.6038 s/iter. Total: 1.4597 s/iter. ETA=0:14:53
[02/24 16:33:08] mask2former INFO: Inference done 485/1093. Dataloading: 0.0277 s/iter. Inference: 0.8265 s/iter. Eval: 0.6040 s/iter. Total: 1.4598 s/iter. ETA=0:14:47
[02/24 16:33:14] mask2former INFO: Inference done 489/1093. Dataloading: 0.0276 s/iter. Inference: 0.8275 s/iter. Eval: 0.6046 s/iter. Total: 1.4613 s/iter. ETA=0:14:42
[02/24 16:33:20] mask2former INFO: Inference done 493/1093. Dataloading: 0.0277 s/iter. Inference: 0.8272 s/iter. Eval: 0.6041 s/iter. Total: 1.4606 s/iter. ETA=0:14:36
[02/24 16:33:25] mask2former INFO: Inference done 497/1093. Dataloading: 0.0276 s/iter. Inference: 0.8274 s/iter. Eval: 0.6037 s/iter. Total: 1.4604 s/iter. ETA=0:14:30
[02/24 16:33:32] mask2former INFO: Inference done 501/1093. Dataloading: 0.0275 s/iter. Inference: 0.8284 s/iter. Eval: 0.6033 s/iter. Total: 1.4608 s/iter. ETA=0:14:24
[02/24 16:33:37] mask2former INFO: Inference done 504/1093. Dataloading: 0.0277 s/iter. Inference: 0.8296 s/iter. Eval: 0.6045 s/iter. Total: 1.4634 s/iter. ETA=0:14:21
[02/24 16:33:43] mask2former INFO: Inference done 508/1093. Dataloading: 0.0276 s/iter. Inference: 0.8288 s/iter. Eval: 0.6049 s/iter. Total: 1.4629 s/iter. ETA=0:14:15
[02/24 16:33:50] mask2former INFO: Inference done 512/1093. Dataloading: 0.0276 s/iter. Inference: 0.8301 s/iter. Eval: 0.6053 s/iter. Total: 1.4645 s/iter. ETA=0:14:10
[02/24 16:33:55] mask2former INFO: Inference done 516/1093. Dataloading: 0.0276 s/iter. Inference: 0.8302 s/iter. Eval: 0.6051 s/iter. Total: 1.4645 s/iter. ETA=0:14:05
[02/24 16:34:00] mask2former INFO: Inference done 520/1093. Dataloading: 0.0276 s/iter. Inference: 0.8296 s/iter. Eval: 0.6043 s/iter. Total: 1.4631 s/iter. ETA=0:13:58
[02/24 16:34:06] mask2former INFO: Inference done 523/1093. Dataloading: 0.0275 s/iter. Inference: 0.8302 s/iter. Eval: 0.6053 s/iter. Total: 1.4646 s/iter. ETA=0:13:54
[02/24 16:34:11] mask2former INFO: Inference done 527/1093. Dataloading: 0.0275 s/iter. Inference: 0.8301 s/iter. Eval: 0.6045 s/iter. Total: 1.4637 s/iter. ETA=0:13:48
[02/24 16:34:18] mask2former INFO: Inference done 531/1093. Dataloading: 0.0274 s/iter. Inference: 0.8299 s/iter. Eval: 0.6062 s/iter. Total: 1.4650 s/iter. ETA=0:13:43
[02/24 16:34:23] mask2former INFO: Inference done 535/1093. Dataloading: 0.0273 s/iter. Inference: 0.8302 s/iter. Eval: 0.6058 s/iter. Total: 1.4649 s/iter. ETA=0:13:37
[02/24 16:34:29] mask2former INFO: Inference done 539/1093. Dataloading: 0.0273 s/iter. Inference: 0.8302 s/iter. Eval: 0.6060 s/iter. Total: 1.4652 s/iter. ETA=0:13:31
[02/24 16:34:34] mask2former INFO: Inference done 542/1093. Dataloading: 0.0274 s/iter. Inference: 0.8311 s/iter. Eval: 0.6064 s/iter. Total: 1.4665 s/iter. ETA=0:13:28
[02/24 16:34:40] mask2former INFO: Inference done 546/1093. Dataloading: 0.0273 s/iter. Inference: 0.8307 s/iter. Eval: 0.6060 s/iter. Total: 1.4656 s/iter. ETA=0:13:21
[02/24 16:34:46] mask2former INFO: Inference done 550/1093. Dataloading: 0.0274 s/iter. Inference: 0.8306 s/iter. Eval: 0.6064 s/iter. Total: 1.4660 s/iter. ETA=0:13:16
[02/24 16:34:51] mask2former INFO: Inference done 554/1093. Dataloading: 0.0273 s/iter. Inference: 0.8305 s/iter. Eval: 0.6055 s/iter. Total: 1.4649 s/iter. ETA=0:13:09
[02/24 16:34:57] mask2former INFO: Inference done 559/1093. Dataloading: 0.0272 s/iter. Inference: 0.8292 s/iter. Eval: 0.6045 s/iter. Total: 1.4625 s/iter. ETA=0:13:00
[02/24 16:35:02] mask2former INFO: Inference done 562/1093. Dataloading: 0.0272 s/iter. Inference: 0.8302 s/iter. Eval: 0.6047 s/iter. Total: 1.4636 s/iter. ETA=0:12:57
[02/24 16:35:08] mask2former INFO: Inference done 566/1093. Dataloading: 0.0271 s/iter. Inference: 0.8298 s/iter. Eval: 0.6047 s/iter. Total: 1.4632 s/iter. ETA=0:12:51
[02/24 16:35:13] mask2former INFO: Inference done 569/1093. Dataloading: 0.0271 s/iter. Inference: 0.8305 s/iter. Eval: 0.6053 s/iter. Total: 1.4645 s/iter. ETA=0:12:47
[02/24 16:35:18] mask2former INFO: Inference done 572/1093. Dataloading: 0.0271 s/iter. Inference: 0.8307 s/iter. Eval: 0.6062 s/iter. Total: 1.4657 s/iter. ETA=0:12:43
[02/24 16:35:23] mask2former INFO: Inference done 575/1093. Dataloading: 0.0271 s/iter. Inference: 0.8309 s/iter. Eval: 0.6073 s/iter. Total: 1.4670 s/iter. ETA=0:12:39
[02/24 16:35:29] mask2former INFO: Inference done 578/1093. Dataloading: 0.0272 s/iter. Inference: 0.8319 s/iter. Eval: 0.6094 s/iter. Total: 1.4701 s/iter. ETA=0:12:37
[02/24 16:35:36] mask2former INFO: Inference done 582/1093. Dataloading: 0.0273 s/iter. Inference: 0.8323 s/iter. Eval: 0.6099 s/iter. Total: 1.4710 s/iter. ETA=0:12:31
[02/24 16:35:41] mask2former INFO: Inference done 586/1093. Dataloading: 0.0275 s/iter. Inference: 0.8321 s/iter. Eval: 0.6091 s/iter. Total: 1.4703 s/iter. ETA=0:12:25
[02/24 16:35:47] mask2former INFO: Inference done 590/1093. Dataloading: 0.0274 s/iter. Inference: 0.8318 s/iter. Eval: 0.6088 s/iter. Total: 1.4697 s/iter. ETA=0:12:19
[02/24 16:35:53] mask2former INFO: Inference done 594/1093. Dataloading: 0.0275 s/iter. Inference: 0.8315 s/iter. Eval: 0.6092 s/iter. Total: 1.4698 s/iter. ETA=0:12:13
[02/24 16:35:58] mask2former INFO: Inference done 597/1093. Dataloading: 0.0276 s/iter. Inference: 0.8321 s/iter. Eval: 0.6096 s/iter. Total: 1.4710 s/iter. ETA=0:12:09
[02/24 16:36:03] mask2former INFO: Inference done 601/1093. Dataloading: 0.0275 s/iter. Inference: 0.8318 s/iter. Eval: 0.6089 s/iter. Total: 1.4699 s/iter. ETA=0:12:03
[02/24 16:36:09] mask2former INFO: Inference done 606/1093. Dataloading: 0.0274 s/iter. Inference: 0.8310 s/iter. Eval: 0.6075 s/iter. Total: 1.4676 s/iter. ETA=0:11:54
[02/24 16:36:14] mask2former INFO: Inference done 610/1093. Dataloading: 0.0274 s/iter. Inference: 0.8309 s/iter. Eval: 0.6069 s/iter. Total: 1.4668 s/iter. ETA=0:11:48
[02/24 16:36:21] mask2former INFO: Inference done 615/1093. Dataloading: 0.0273 s/iter. Inference: 0.8310 s/iter. Eval: 0.6053 s/iter. Total: 1.4652 s/iter. ETA=0:11:40
[02/24 16:36:26] mask2former INFO: Inference done 619/1093. Dataloading: 0.0274 s/iter. Inference: 0.8313 s/iter. Eval: 0.6041 s/iter. Total: 1.4645 s/iter. ETA=0:11:34
[02/24 16:36:31] mask2former INFO: Inference done 622/1093. Dataloading: 0.0273 s/iter. Inference: 0.8311 s/iter. Eval: 0.6054 s/iter. Total: 1.4655 s/iter. ETA=0:11:30
[02/24 16:36:37] mask2former INFO: Inference done 626/1093. Dataloading: 0.0273 s/iter. Inference: 0.8312 s/iter. Eval: 0.6054 s/iter. Total: 1.4655 s/iter. ETA=0:11:24
[02/24 16:36:44] mask2former INFO: Inference done 630/1093. Dataloading: 0.0273 s/iter. Inference: 0.8325 s/iter. Eval: 0.6055 s/iter. Total: 1.4670 s/iter. ETA=0:11:19
[02/24 16:36:49] mask2former INFO: Inference done 633/1093. Dataloading: 0.0273 s/iter. Inference: 0.8338 s/iter. Eval: 0.6053 s/iter. Total: 1.4681 s/iter. ETA=0:11:15
[02/24 16:36:55] mask2former INFO: Inference done 638/1093. Dataloading: 0.0272 s/iter. Inference: 0.8332 s/iter. Eval: 0.6046 s/iter. Total: 1.4667 s/iter. ETA=0:11:07
[02/24 16:37:01] mask2former INFO: Inference done 642/1093. Dataloading: 0.0272 s/iter. Inference: 0.8329 s/iter. Eval: 0.6051 s/iter. Total: 1.4670 s/iter. ETA=0:11:01
[02/24 16:37:07] mask2former INFO: Inference done 646/1093. Dataloading: 0.0273 s/iter. Inference: 0.8332 s/iter. Eval: 0.6044 s/iter. Total: 1.4665 s/iter. ETA=0:10:55
[02/24 16:37:12] mask2former INFO: Inference done 650/1093. Dataloading: 0.0273 s/iter. Inference: 0.8317 s/iter. Eval: 0.6049 s/iter. Total: 1.4656 s/iter. ETA=0:10:49
[02/24 16:37:18] mask2former INFO: Inference done 654/1093. Dataloading: 0.0272 s/iter. Inference: 0.8314 s/iter. Eval: 0.6047 s/iter. Total: 1.4650 s/iter. ETA=0:10:43
[02/24 16:37:23] mask2former INFO: Inference done 659/1093. Dataloading: 0.0273 s/iter. Inference: 0.8300 s/iter. Eval: 0.6030 s/iter. Total: 1.4619 s/iter. ETA=0:10:34
[02/24 16:37:28] mask2former INFO: Inference done 663/1093. Dataloading: 0.0274 s/iter. Inference: 0.8291 s/iter. Eval: 0.6029 s/iter. Total: 1.4610 s/iter. ETA=0:10:28
[02/24 16:37:34] mask2former INFO: Inference done 667/1093. Dataloading: 0.0273 s/iter. Inference: 0.8293 s/iter. Eval: 0.6032 s/iter. Total: 1.4615 s/iter. ETA=0:10:22
[02/24 16:37:40] mask2former INFO: Inference done 670/1093. Dataloading: 0.0273 s/iter. Inference: 0.8300 s/iter. Eval: 0.6036 s/iter. Total: 1.4626 s/iter. ETA=0:10:18
[02/24 16:37:46] mask2former INFO: Inference done 674/1093. Dataloading: 0.0273 s/iter. Inference: 0.8303 s/iter. Eval: 0.6042 s/iter. Total: 1.4635 s/iter. ETA=0:10:13
[02/24 16:37:51] mask2former INFO: Inference done 678/1093. Dataloading: 0.0273 s/iter. Inference: 0.8300 s/iter. Eval: 0.6038 s/iter. Total: 1.4627 s/iter. ETA=0:10:07
[02/24 16:37:57] mask2former INFO: Inference done 683/1093. Dataloading: 0.0272 s/iter. Inference: 0.8290 s/iter. Eval: 0.6024 s/iter. Total: 1.4602 s/iter. ETA=0:09:58
[02/24 16:38:02] mask2former INFO: Inference done 687/1093. Dataloading: 0.0272 s/iter. Inference: 0.8286 s/iter. Eval: 0.6018 s/iter. Total: 1.4592 s/iter. ETA=0:09:52
[02/24 16:38:08] mask2former INFO: Inference done 691/1093. Dataloading: 0.0271 s/iter. Inference: 0.8282 s/iter. Eval: 0.6017 s/iter. Total: 1.4586 s/iter. ETA=0:09:46
[02/24 16:38:13] mask2former INFO: Inference done 695/1093. Dataloading: 0.0271 s/iter. Inference: 0.8278 s/iter. Eval: 0.6013 s/iter. Total: 1.4578 s/iter. ETA=0:09:40
[02/24 16:38:20] mask2former INFO: Inference done 699/1093. Dataloading: 0.0273 s/iter. Inference: 0.8288 s/iter. Eval: 0.6016 s/iter. Total: 1.4594 s/iter. ETA=0:09:34
[02/24 16:38:26] mask2former INFO: Inference done 703/1093. Dataloading: 0.0274 s/iter. Inference: 0.8288 s/iter. Eval: 0.6016 s/iter. Total: 1.4594 s/iter. ETA=0:09:29
[02/24 16:38:32] mask2former INFO: Inference done 707/1093. Dataloading: 0.0273 s/iter. Inference: 0.8294 s/iter. Eval: 0.6017 s/iter. Total: 1.4600 s/iter. ETA=0:09:23
[02/24 16:38:39] mask2former INFO: Inference done 711/1093. Dataloading: 0.0273 s/iter. Inference: 0.8300 s/iter. Eval: 0.6023 s/iter. Total: 1.4612 s/iter. ETA=0:09:18
[02/24 16:38:45] mask2former INFO: Inference done 715/1093. Dataloading: 0.0274 s/iter. Inference: 0.8296 s/iter. Eval: 0.6031 s/iter. Total: 1.4617 s/iter. ETA=0:09:12
[02/24 16:38:51] mask2former INFO: Inference done 720/1093. Dataloading: 0.0273 s/iter. Inference: 0.8287 s/iter. Eval: 0.6020 s/iter. Total: 1.4597 s/iter. ETA=0:09:04
[02/24 16:38:56] mask2former INFO: Inference done 724/1093. Dataloading: 0.0273 s/iter. Inference: 0.8280 s/iter. Eval: 0.6017 s/iter. Total: 1.4587 s/iter. ETA=0:08:58
[02/24 16:39:02] mask2former INFO: Inference done 728/1093. Dataloading: 0.0273 s/iter. Inference: 0.8288 s/iter. Eval: 0.6013 s/iter. Total: 1.4591 s/iter. ETA=0:08:52
[02/24 16:39:07] mask2former INFO: Inference done 732/1093. Dataloading: 0.0273 s/iter. Inference: 0.8284 s/iter. Eval: 0.6009 s/iter. Total: 1.4582 s/iter. ETA=0:08:46
[02/24 16:39:13] mask2former INFO: Inference done 736/1093. Dataloading: 0.0273 s/iter. Inference: 0.8283 s/iter. Eval: 0.6007 s/iter. Total: 1.4579 s/iter. ETA=0:08:40
[02/24 16:39:18] mask2former INFO: Inference done 740/1093. Dataloading: 0.0272 s/iter. Inference: 0.8281 s/iter. Eval: 0.6003 s/iter. Total: 1.4573 s/iter. ETA=0:08:34
[02/24 16:39:25] mask2former INFO: Inference done 744/1093. Dataloading: 0.0272 s/iter. Inference: 0.8284 s/iter. Eval: 0.6008 s/iter. Total: 1.4581 s/iter. ETA=0:08:28
[02/24 16:39:30] mask2former INFO: Inference done 749/1093. Dataloading: 0.0272 s/iter. Inference: 0.8273 s/iter. Eval: 0.5994 s/iter. Total: 1.4555 s/iter. ETA=0:08:20
[02/24 16:39:35] mask2former INFO: Inference done 753/1093. Dataloading: 0.0271 s/iter. Inference: 0.8275 s/iter. Eval: 0.5987 s/iter. Total: 1.4550 s/iter. ETA=0:08:14
[02/24 16:39:41] mask2former INFO: Inference done 758/1093. Dataloading: 0.0270 s/iter. Inference: 0.8269 s/iter. Eval: 0.5979 s/iter. Total: 1.4535 s/iter. ETA=0:08:06
[02/24 16:39:47] mask2former INFO: Inference done 762/1093. Dataloading: 0.0269 s/iter. Inference: 0.8269 s/iter. Eval: 0.5981 s/iter. Total: 1.4536 s/iter. ETA=0:08:01
[02/24 16:39:54] mask2former INFO: Inference done 766/1093. Dataloading: 0.0269 s/iter. Inference: 0.8275 s/iter. Eval: 0.5989 s/iter. Total: 1.4549 s/iter. ETA=0:07:55
[02/24 16:40:00] mask2former INFO: Inference done 770/1093. Dataloading: 0.0269 s/iter. Inference: 0.8272 s/iter. Eval: 0.5986 s/iter. Total: 1.4545 s/iter. ETA=0:07:49
[02/24 16:40:05] mask2former INFO: Inference done 773/1093. Dataloading: 0.0270 s/iter. Inference: 0.8282 s/iter. Eval: 0.5988 s/iter. Total: 1.4557 s/iter. ETA=0:07:45
[02/24 16:40:10] mask2former INFO: Inference done 777/1093. Dataloading: 0.0270 s/iter. Inference: 0.8279 s/iter. Eval: 0.5987 s/iter. Total: 1.4553 s/iter. ETA=0:07:39
[02/24 16:40:17] mask2former INFO: Inference done 781/1093. Dataloading: 0.0269 s/iter. Inference: 0.8283 s/iter. Eval: 0.5989 s/iter. Total: 1.4558 s/iter. ETA=0:07:34
[02/24 16:40:22] mask2former INFO: Inference done 785/1093. Dataloading: 0.0270 s/iter. Inference: 0.8277 s/iter. Eval: 0.5987 s/iter. Total: 1.4551 s/iter. ETA=0:07:28
[02/24 16:40:28] mask2former INFO: Inference done 788/1093. Dataloading: 0.0270 s/iter. Inference: 0.8282 s/iter. Eval: 0.5997 s/iter. Total: 1.4567 s/iter. ETA=0:07:24
[02/24 16:40:33] mask2former INFO: Inference done 791/1093. Dataloading: 0.0271 s/iter. Inference: 0.8287 s/iter. Eval: 0.6009 s/iter. Total: 1.4584 s/iter. ETA=0:07:20
[02/24 16:40:39] mask2former INFO: Inference done 794/1093. Dataloading: 0.0271 s/iter. Inference: 0.8295 s/iter. Eval: 0.6015 s/iter. Total: 1.4597 s/iter. ETA=0:07:16
[02/24 16:40:45] mask2former INFO: Inference done 798/1093. Dataloading: 0.0271 s/iter. Inference: 0.8294 s/iter. Eval: 0.6017 s/iter. Total: 1.4599 s/iter. ETA=0:07:10
[02/24 16:40:50] mask2former INFO: Inference done 802/1093. Dataloading: 0.0270 s/iter. Inference: 0.8295 s/iter. Eval: 0.6014 s/iter. Total: 1.4597 s/iter. ETA=0:07:04
[02/24 16:40:56] mask2former INFO: Inference done 806/1093. Dataloading: 0.0271 s/iter. Inference: 0.8290 s/iter. Eval: 0.6012 s/iter. Total: 1.4590 s/iter. ETA=0:06:58
[02/24 16:41:01] mask2former INFO: Inference done 810/1093. Dataloading: 0.0270 s/iter. Inference: 0.8286 s/iter. Eval: 0.6009 s/iter. Total: 1.4581 s/iter. ETA=0:06:52
[02/24 16:41:06] mask2former INFO: Inference done 813/1093. Dataloading: 0.0271 s/iter. Inference: 0.8294 s/iter. Eval: 0.6008 s/iter. Total: 1.4589 s/iter. ETA=0:06:48
[02/24 16:41:12] mask2former INFO: Inference done 816/1093. Dataloading: 0.0271 s/iter. Inference: 0.8300 s/iter. Eval: 0.6020 s/iter. Total: 1.4608 s/iter. ETA=0:06:44
[02/24 16:41:18] mask2former INFO: Inference done 820/1093. Dataloading: 0.0273 s/iter. Inference: 0.8298 s/iter. Eval: 0.6022 s/iter. Total: 1.4610 s/iter. ETA=0:06:38
[02/24 16:41:23] mask2former INFO: Inference done 824/1093. Dataloading: 0.0273 s/iter. Inference: 0.8300 s/iter. Eval: 0.6016 s/iter. Total: 1.4606 s/iter. ETA=0:06:32
[02/24 16:41:29] mask2former INFO: Inference done 828/1093. Dataloading: 0.0273 s/iter. Inference: 0.8303 s/iter. Eval: 0.6017 s/iter. Total: 1.4610 s/iter. ETA=0:06:27
[02/24 16:41:35] mask2former INFO: Inference done 831/1093. Dataloading: 0.0272 s/iter. Inference: 0.8303 s/iter. Eval: 0.6027 s/iter. Total: 1.4619 s/iter. ETA=0:06:23
[02/24 16:41:40] mask2former INFO: Inference done 835/1093. Dataloading: 0.0272 s/iter. Inference: 0.8306 s/iter. Eval: 0.6023 s/iter. Total: 1.4617 s/iter. ETA=0:06:17
[02/24 16:41:45] mask2former INFO: Inference done 839/1093. Dataloading: 0.0272 s/iter. Inference: 0.8303 s/iter. Eval: 0.6018 s/iter. Total: 1.4609 s/iter. ETA=0:06:11
[02/24 16:41:51] mask2former INFO: Inference done 843/1093. Dataloading: 0.0271 s/iter. Inference: 0.8301 s/iter. Eval: 0.6018 s/iter. Total: 1.4606 s/iter. ETA=0:06:05
[02/24 16:41:56] mask2former INFO: Inference done 847/1093. Dataloading: 0.0271 s/iter. Inference: 0.8299 s/iter. Eval: 0.6014 s/iter. Total: 1.4600 s/iter. ETA=0:05:59
[02/24 16:42:02] mask2former INFO: Inference done 852/1093. Dataloading: 0.0270 s/iter. Inference: 0.8284 s/iter. Eval: 0.6007 s/iter. Total: 1.4577 s/iter. ETA=0:05:51
[02/24 16:42:08] mask2former INFO: Inference done 856/1093. Dataloading: 0.0270 s/iter. Inference: 0.8290 s/iter. Eval: 0.6001 s/iter. Total: 1.4578 s/iter. ETA=0:05:45
[02/24 16:42:14] mask2former INFO: Inference done 861/1093. Dataloading: 0.0270 s/iter. Inference: 0.8289 s/iter. Eval: 0.5996 s/iter. Total: 1.4571 s/iter. ETA=0:05:38
[02/24 16:42:20] mask2former INFO: Inference done 865/1093. Dataloading: 0.0270 s/iter. Inference: 0.8291 s/iter. Eval: 0.5995 s/iter. Total: 1.4572 s/iter. ETA=0:05:32
[02/24 16:42:26] mask2former INFO: Inference done 869/1093. Dataloading: 0.0270 s/iter. Inference: 0.8287 s/iter. Eval: 0.5994 s/iter. Total: 1.4567 s/iter. ETA=0:05:26
[02/24 16:42:32] mask2former INFO: Inference done 873/1093. Dataloading: 0.0269 s/iter. Inference: 0.8282 s/iter. Eval: 0.6007 s/iter. Total: 1.4575 s/iter. ETA=0:05:20
[02/24 16:42:38] mask2former INFO: Inference done 877/1093. Dataloading: 0.0269 s/iter. Inference: 0.8283 s/iter. Eval: 0.6002 s/iter. Total: 1.4571 s/iter. ETA=0:05:14
[02/24 16:42:44] mask2former INFO: Inference done 882/1093. Dataloading: 0.0269 s/iter. Inference: 0.8277 s/iter. Eval: 0.5994 s/iter. Total: 1.4556 s/iter. ETA=0:05:07
[02/24 16:42:49] mask2former INFO: Inference done 886/1093. Dataloading: 0.0269 s/iter. Inference: 0.8281 s/iter. Eval: 0.5990 s/iter. Total: 1.4556 s/iter. ETA=0:05:01
[02/24 16:42:54] mask2former INFO: Inference done 890/1093. Dataloading: 0.0268 s/iter. Inference: 0.8278 s/iter. Eval: 0.5985 s/iter. Total: 1.4548 s/iter. ETA=0:04:55
[02/24 16:43:00] mask2former INFO: Inference done 894/1093. Dataloading: 0.0268 s/iter. Inference: 0.8278 s/iter. Eval: 0.5982 s/iter. Total: 1.4545 s/iter. ETA=0:04:49
[02/24 16:43:05] mask2former INFO: Inference done 898/1093. Dataloading: 0.0268 s/iter. Inference: 0.8271 s/iter. Eval: 0.5982 s/iter. Total: 1.4537 s/iter. ETA=0:04:43
[02/24 16:43:11] mask2former INFO: Inference done 903/1093. Dataloading: 0.0268 s/iter. Inference: 0.8261 s/iter. Eval: 0.5978 s/iter. Total: 1.4523 s/iter. ETA=0:04:35
[02/24 16:43:16] mask2former INFO: Inference done 906/1093. Dataloading: 0.0267 s/iter. Inference: 0.8268 s/iter. Eval: 0.5981 s/iter. Total: 1.4533 s/iter. ETA=0:04:31
[02/24 16:43:23] mask2former INFO: Inference done 910/1093. Dataloading: 0.0267 s/iter. Inference: 0.8278 s/iter. Eval: 0.5977 s/iter. Total: 1.4538 s/iter. ETA=0:04:26
[02/24 16:43:29] mask2former INFO: Inference done 915/1093. Dataloading: 0.0267 s/iter. Inference: 0.8274 s/iter. Eval: 0.5970 s/iter. Total: 1.4526 s/iter. ETA=0:04:18
[02/24 16:43:35] mask2former INFO: Inference done 919/1093. Dataloading: 0.0266 s/iter. Inference: 0.8272 s/iter. Eval: 0.5973 s/iter. Total: 1.4527 s/iter. ETA=0:04:12
[02/24 16:43:40] mask2former INFO: Inference done 923/1093. Dataloading: 0.0266 s/iter. Inference: 0.8267 s/iter. Eval: 0.5970 s/iter. Total: 1.4519 s/iter. ETA=0:04:06
[02/24 16:43:46] mask2former INFO: Inference done 928/1093. Dataloading: 0.0266 s/iter. Inference: 0.8264 s/iter. Eval: 0.5961 s/iter. Total: 1.4506 s/iter. ETA=0:03:59
[02/24 16:43:52] mask2former INFO: Inference done 932/1093. Dataloading: 0.0266 s/iter. Inference: 0.8262 s/iter. Eval: 0.5962 s/iter. Total: 1.4505 s/iter. ETA=0:03:53
[02/24 16:43:57] mask2former INFO: Inference done 936/1093. Dataloading: 0.0265 s/iter. Inference: 0.8263 s/iter. Eval: 0.5955 s/iter. Total: 1.4500 s/iter. ETA=0:03:47
[02/24 16:44:03] mask2former INFO: Inference done 940/1093. Dataloading: 0.0265 s/iter. Inference: 0.8263 s/iter. Eval: 0.5956 s/iter. Total: 1.4500 s/iter. ETA=0:03:41
[02/24 16:44:08] mask2former INFO: Inference done 944/1093. Dataloading: 0.0265 s/iter. Inference: 0.8263 s/iter. Eval: 0.5952 s/iter. Total: 1.4496 s/iter. ETA=0:03:35
[02/24 16:44:14] mask2former INFO: Inference done 949/1093. Dataloading: 0.0264 s/iter. Inference: 0.8253 s/iter. Eval: 0.5948 s/iter. Total: 1.4481 s/iter. ETA=0:03:28
[02/24 16:44:20] mask2former INFO: Inference done 954/1093. Dataloading: 0.0264 s/iter. Inference: 0.8250 s/iter. Eval: 0.5941 s/iter. Total: 1.4471 s/iter. ETA=0:03:21
[02/24 16:44:25] mask2former INFO: Inference done 958/1093. Dataloading: 0.0264 s/iter. Inference: 0.8246 s/iter. Eval: 0.5938 s/iter. Total: 1.4464 s/iter. ETA=0:03:15
[02/24 16:44:31] mask2former INFO: Inference done 962/1093. Dataloading: 0.0264 s/iter. Inference: 0.8240 s/iter. Eval: 0.5939 s/iter. Total: 1.4459 s/iter. ETA=0:03:09
[02/24 16:44:36] mask2former INFO: Inference done 966/1093. Dataloading: 0.0264 s/iter. Inference: 0.8240 s/iter. Eval: 0.5937 s/iter. Total: 1.4457 s/iter. ETA=0:03:03
[02/24 16:44:42] mask2former INFO: Inference done 970/1093. Dataloading: 0.0264 s/iter. Inference: 0.8234 s/iter. Eval: 0.5941 s/iter. Total: 1.4455 s/iter. ETA=0:02:57
[02/24 16:44:47] mask2former INFO: Inference done 973/1093. Dataloading: 0.0263 s/iter. Inference: 0.8240 s/iter. Eval: 0.5944 s/iter. Total: 1.4463 s/iter. ETA=0:02:53
[02/24 16:44:53] mask2former INFO: Inference done 977/1093. Dataloading: 0.0263 s/iter. Inference: 0.8241 s/iter. Eval: 0.5944 s/iter. Total: 1.4464 s/iter. ETA=0:02:47
[02/24 16:44:58] mask2former INFO: Inference done 981/1093. Dataloading: 0.0263 s/iter. Inference: 0.8238 s/iter. Eval: 0.5944 s/iter. Total: 1.4461 s/iter. ETA=0:02:41
[02/24 16:45:04] mask2former INFO: Inference done 985/1093. Dataloading: 0.0263 s/iter. Inference: 0.8231 s/iter. Eval: 0.5947 s/iter. Total: 1.4457 s/iter. ETA=0:02:36
[02/24 16:45:09] mask2former INFO: Inference done 989/1093. Dataloading: 0.0263 s/iter. Inference: 0.8230 s/iter. Eval: 0.5943 s/iter. Total: 1.4452 s/iter. ETA=0:02:30
[02/24 16:45:15] mask2former INFO: Inference done 994/1093. Dataloading: 0.0263 s/iter. Inference: 0.8226 s/iter. Eval: 0.5939 s/iter. Total: 1.4444 s/iter. ETA=0:02:22
[02/24 16:45:21] mask2former INFO: Inference done 998/1093. Dataloading: 0.0262 s/iter. Inference: 0.8229 s/iter. Eval: 0.5937 s/iter. Total: 1.4445 s/iter. ETA=0:02:17
[02/24 16:45:27] mask2former INFO: Inference done 1002/1093. Dataloading: 0.0262 s/iter. Inference: 0.8227 s/iter. Eval: 0.5937 s/iter. Total: 1.4443 s/iter. ETA=0:02:11
[02/24 16:45:33] mask2former INFO: Inference done 1006/1093. Dataloading: 0.0262 s/iter. Inference: 0.8228 s/iter. Eval: 0.5939 s/iter. Total: 1.4445 s/iter. ETA=0:02:05
[02/24 16:45:39] mask2former INFO: Inference done 1010/1093. Dataloading: 0.0262 s/iter. Inference: 0.8226 s/iter. Eval: 0.5940 s/iter. Total: 1.4445 s/iter. ETA=0:01:59
[02/24 16:45:44] mask2former INFO: Inference done 1014/1093. Dataloading: 0.0263 s/iter. Inference: 0.8228 s/iter. Eval: 0.5936 s/iter. Total: 1.4444 s/iter. ETA=0:01:54
[02/24 16:45:51] mask2former INFO: Inference done 1018/1093. Dataloading: 0.0262 s/iter. Inference: 0.8230 s/iter. Eval: 0.5941 s/iter. Total: 1.4449 s/iter. ETA=0:01:48
[02/24 16:45:56] mask2former INFO: Inference done 1022/1093. Dataloading: 0.0262 s/iter. Inference: 0.8233 s/iter. Eval: 0.5938 s/iter. Total: 1.4449 s/iter. ETA=0:01:42
[02/24 16:46:02] mask2former INFO: Inference done 1026/1093. Dataloading: 0.0262 s/iter. Inference: 0.8238 s/iter. Eval: 0.5931 s/iter. Total: 1.4448 s/iter. ETA=0:01:36
[02/24 16:46:08] mask2former INFO: Inference done 1030/1093. Dataloading: 0.0262 s/iter. Inference: 0.8235 s/iter. Eval: 0.5937 s/iter. Total: 1.4451 s/iter. ETA=0:01:31
[02/24 16:46:14] mask2former INFO: Inference done 1034/1093. Dataloading: 0.0262 s/iter. Inference: 0.8234 s/iter. Eval: 0.5941 s/iter. Total: 1.4454 s/iter. ETA=0:01:25
[02/24 16:46:20] mask2former INFO: Inference done 1038/1093. Dataloading: 0.0262 s/iter. Inference: 0.8233 s/iter. Eval: 0.5940 s/iter. Total: 1.4452 s/iter. ETA=0:01:19
[02/24 16:46:27] mask2former INFO: Inference done 1042/1093. Dataloading: 0.0263 s/iter. Inference: 0.8243 s/iter. Eval: 0.5942 s/iter. Total: 1.4465 s/iter. ETA=0:01:13
[02/24 16:46:33] mask2former INFO: Inference done 1046/1093. Dataloading: 0.0263 s/iter. Inference: 0.8245 s/iter. Eval: 0.5942 s/iter. Total: 1.4467 s/iter. ETA=0:01:07
[02/24 16:46:39] mask2former INFO: Inference done 1050/1093. Dataloading: 0.0263 s/iter. Inference: 0.8253 s/iter. Eval: 0.5937 s/iter. Total: 1.4469 s/iter. ETA=0:01:02
[02/24 16:46:45] mask2former INFO: Inference done 1054/1093. Dataloading: 0.0263 s/iter. Inference: 0.8253 s/iter. Eval: 0.5939 s/iter. Total: 1.4472 s/iter. ETA=0:00:56
[02/24 16:46:50] mask2former INFO: Inference done 1058/1093. Dataloading: 0.0263 s/iter. Inference: 0.8251 s/iter. Eval: 0.5936 s/iter. Total: 1.4467 s/iter. ETA=0:00:50
[02/24 16:46:56] mask2former INFO: Inference done 1062/1093. Dataloading: 0.0263 s/iter. Inference: 0.8249 s/iter. Eval: 0.5933 s/iter. Total: 1.4461 s/iter. ETA=0:00:44
[02/24 16:47:01] mask2former INFO: Inference done 1066/1093. Dataloading: 0.0263 s/iter. Inference: 0.8250 s/iter. Eval: 0.5933 s/iter. Total: 1.4462 s/iter. ETA=0:00:39
[02/24 16:47:08] mask2former INFO: Inference done 1071/1093. Dataloading: 0.0263 s/iter. Inference: 0.8249 s/iter. Eval: 0.5928 s/iter. Total: 1.4457 s/iter. ETA=0:00:31
[02/24 16:47:14] mask2former INFO: Inference done 1075/1093. Dataloading: 0.0264 s/iter. Inference: 0.8253 s/iter. Eval: 0.5922 s/iter. Total: 1.4455 s/iter. ETA=0:00:26
[02/24 16:47:20] mask2former INFO: Inference done 1079/1093. Dataloading: 0.0263 s/iter. Inference: 0.8254 s/iter. Eval: 0.5921 s/iter. Total: 1.4456 s/iter. ETA=0:00:20
[02/24 16:47:26] mask2former INFO: Inference done 1083/1093. Dataloading: 0.0264 s/iter. Inference: 0.8260 s/iter. Eval: 0.5920 s/iter. Total: 1.4460 s/iter. ETA=0:00:14
[02/24 16:47:32] mask2former INFO: Inference done 1088/1093. Dataloading: 0.0263 s/iter. Inference: 0.8256 s/iter. Eval: 0.5916 s/iter. Total: 1.4452 s/iter. ETA=0:00:07
[02/24 16:47:38] mask2former INFO: Inference done 1092/1093. Dataloading: 0.0263 s/iter. Inference: 0.8254 s/iter. Eval: 0.5915 s/iter. Total: 1.4449 s/iter. ETA=0:00:01
