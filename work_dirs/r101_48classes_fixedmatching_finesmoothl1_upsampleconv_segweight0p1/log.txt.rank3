[02/17 22:40:03] detectron2 INFO: Rank of current process: 3. World size: 4
[02/17 22:40:07] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 22:40:07] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 22:40:07] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 22:40:08] detectron2.utils.env INFO: Using a generated random seed 8154686
[02/17 22:40:13] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 5.0, 'loss_ce': 0.0, 'loss_dice': 5.0, 'loss_seg': 0.1, 'loss_mask_0': 5.0, 'loss_ce_0': 0.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_ce_1': 0.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_ce_2': 0.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_ce_3': 0.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_ce_4': 0.0, 'loss_dice_4': 5.0, 'loss_mask_5': 5.0, 'loss_ce_5': 0.0, 'loss_dice_5': 5.0, 'loss_mask_6': 5.0, 'loss_ce_6': 0.0, 'loss_dice_6': 5.0, 'loss_mask_7': 5.0, 'loss_ce_7': 0.0, 'loss_dice_7': 5.0, 'loss_mask_8': 5.0, 'loss_ce_8': 0.0, 'loss_dice_8': 5.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (upsampler): UpsampleMasks(
    (conv2d): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
[02/17 22:40:13] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 22:40:18] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 22:40:19] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 22:40:19] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 22:40:19] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 22:40:19] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 22:40:19] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 22:40:20] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mupsampler.conv2d.{bias, weight}[0m
[02/17 22:40:20] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 22:40:20] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 22:46:51] detectron2.engine.hooks INFO: Overall training speed: 240 iterations in 0:06:15 (1.5634 s / it)
[02/17 22:46:51] detectron2.engine.hooks INFO: Total training time: 0:06:15 (0:00:00 on hooks)
[02/17 22:47:28] detectron2 INFO: Rank of current process: 3. World size: 4
[02/17 22:47:33] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 22:47:33] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 22:47:33] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 22:47:33] detectron2.utils.env INFO: Using a generated random seed 33169102
[02/17 22:47:38] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 5.0, 'loss_ce': 0.0, 'loss_dice': 5.0, 'loss_seg': 0.1, 'loss_mask_0': 5.0, 'loss_ce_0': 0.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_ce_1': 0.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_ce_2': 0.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_ce_3': 0.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_ce_4': 0.0, 'loss_dice_4': 5.0, 'loss_mask_5': 5.0, 'loss_ce_5': 0.0, 'loss_dice_5': 5.0, 'loss_mask_6': 5.0, 'loss_ce_6': 0.0, 'loss_dice_6': 5.0, 'loss_mask_7': 5.0, 'loss_ce_7': 0.0, 'loss_dice_7': 5.0, 'loss_mask_8': 5.0, 'loss_ce_8': 0.0, 'loss_dice_8': 5.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (upsampler): UpsampleMasks(
    (conv2d): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
[02/17 22:47:38] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 22:47:44] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 22:47:44] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 22:47:44] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 22:47:44] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 22:47:44] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 22:47:44] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 22:47:45] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mupsampler.conv2d.{bias, weight}[0m
[02/17 22:47:45] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 22:47:45] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 22:48:34] detectron2.engine.hooks INFO: Overall training speed: 15 iterations in 0:00:31 (2.1031 s / it)
[02/17 22:48:34] detectron2.engine.hooks INFO: Total training time: 0:00:31 (0:00:00 on hooks)
[02/17 22:48:54] detectron2 INFO: Rank of current process: 3. World size: 4
[02/17 22:48:59] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 22:48:59] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 22:48:59] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 22:49:00] detectron2.utils.env INFO: Using a generated random seed 81596
[02/17 22:49:03] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 5.0, 'loss_ce': 0.0, 'loss_dice': 5.0, 'loss_seg': 0.1, 'loss_mask_0': 5.0, 'loss_ce_0': 0.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_ce_1': 0.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_ce_2': 0.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_ce_3': 0.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_ce_4': 0.0, 'loss_dice_4': 5.0, 'loss_mask_5': 5.0, 'loss_ce_5': 0.0, 'loss_dice_5': 5.0, 'loss_mask_6': 5.0, 'loss_ce_6': 0.0, 'loss_dice_6': 5.0, 'loss_mask_7': 5.0, 'loss_ce_7': 0.0, 'loss_dice_7': 5.0, 'loss_mask_8': 5.0, 'loss_ce_8': 0.0, 'loss_dice_8': 5.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (upsampler): UpsampleMasks(
    (conv2d): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
[02/17 22:49:03] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 22:49:09] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 22:49:09] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 22:49:09] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 22:49:09] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 22:49:09] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 22:49:10] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 22:49:10] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mupsampler.conv2d.{bias, weight}[0m
[02/17 22:49:10] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 22:49:10] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 22:53:51] detectron2.engine.hooks INFO: Overall training speed: 146 iterations in 0:04:23 (1.8042 s / it)
[02/17 22:53:51] detectron2.engine.hooks INFO: Total training time: 0:04:23 (0:00:00 on hooks)
[02/17 22:54:21] detectron2 INFO: Rank of current process: 3. World size: 4
[02/17 22:54:25] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 22:54:25] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 22:54:25] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 22:54:25] detectron2.utils.env INFO: Using a generated random seed 25643184
[02/17 22:54:28] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 5.0, 'loss_ce': 0.0, 'loss_dice': 5.0, 'loss_seg': 0.1, 'loss_mask_0': 5.0, 'loss_ce_0': 0.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_ce_1': 0.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_ce_2': 0.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_ce_3': 0.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_ce_4': 0.0, 'loss_dice_4': 5.0, 'loss_mask_5': 5.0, 'loss_ce_5': 0.0, 'loss_dice_5': 5.0, 'loss_mask_6': 5.0, 'loss_ce_6': 0.0, 'loss_dice_6': 5.0, 'loss_mask_7': 5.0, 'loss_ce_7': 0.0, 'loss_dice_7': 5.0, 'loss_mask_8': 5.0, 'loss_ce_8': 0.0, 'loss_dice_8': 5.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (upsampler): UpsampleMasks(
    (conv2d): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
[02/17 22:54:28] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 22:54:34] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 22:54:36] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 22:54:36] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 22:54:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 22:54:36] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 22:54:37] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 22:54:38] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mupsampler.conv2d.{bias, weight}[0m
[02/17 22:54:38] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 22:54:38] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 22:55:33] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 22:55:34] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 22:55:34] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 22:55:47] mask2former INFO: Inference done 11/1092. Dataloading: 0.0042 s/iter. Inference: 0.2620 s/iter. Eval: 0.0557 s/iter. Total: 0.3220 s/iter. ETA=0:05:48
[02/17 22:55:52] mask2former INFO: Inference done 27/1092. Dataloading: 0.0055 s/iter. Inference: 0.2772 s/iter. Eval: 0.0474 s/iter. Total: 0.3302 s/iter. ETA=0:05:51
[02/17 22:55:57] mask2former INFO: Inference done 43/1092. Dataloading: 0.0059 s/iter. Inference: 0.2744 s/iter. Eval: 0.0443 s/iter. Total: 0.3248 s/iter. ETA=0:05:40
[02/17 22:56:03] mask2former INFO: Inference done 59/1092. Dataloading: 0.0063 s/iter. Inference: 0.2745 s/iter. Eval: 0.0431 s/iter. Total: 0.3240 s/iter. ETA=0:05:34
[02/17 22:56:08] mask2former INFO: Inference done 74/1092. Dataloading: 0.0065 s/iter. Inference: 0.2748 s/iter. Eval: 0.0450 s/iter. Total: 0.3263 s/iter. ETA=0:05:32
[02/17 22:56:13] mask2former INFO: Inference done 89/1092. Dataloading: 0.0069 s/iter. Inference: 0.2764 s/iter. Eval: 0.0456 s/iter. Total: 0.3290 s/iter. ETA=0:05:30
[02/17 22:56:18] mask2former INFO: Inference done 104/1092. Dataloading: 0.0069 s/iter. Inference: 0.2786 s/iter. Eval: 0.0472 s/iter. Total: 0.3327 s/iter. ETA=0:05:28
[02/17 22:56:23] mask2former INFO: Inference done 119/1092. Dataloading: 0.0070 s/iter. Inference: 0.2768 s/iter. Eval: 0.0493 s/iter. Total: 0.3333 s/iter. ETA=0:05:24
[02/17 22:56:28] mask2former INFO: Inference done 135/1092. Dataloading: 0.0070 s/iter. Inference: 0.2760 s/iter. Eval: 0.0480 s/iter. Total: 0.3311 s/iter. ETA=0:05:16
[02/17 22:56:30] detectron2.engine.hooks INFO: Overall training speed: 17 iterations in 0:00:38 (2.2675 s / it)
[02/17 22:56:30] detectron2.engine.hooks INFO: Total training time: 0:01:35 (0:00:57 on hooks)
[02/17 22:56:58] detectron2 INFO: Rank of current process: 3. World size: 4
[02/17 22:57:03] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 22:57:03] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 22:57:03] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 22:57:03] detectron2.utils.env INFO: Using a generated random seed 3663951
[02/17 22:57:09] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 5.0, 'loss_ce': 0.0, 'loss_dice': 5.0, 'loss_seg': 0.1, 'loss_mask_0': 5.0, 'loss_ce_0': 0.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_ce_1': 0.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_ce_2': 0.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_ce_3': 0.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_ce_4': 0.0, 'loss_dice_4': 5.0, 'loss_mask_5': 5.0, 'loss_ce_5': 0.0, 'loss_dice_5': 5.0, 'loss_mask_6': 5.0, 'loss_ce_6': 0.0, 'loss_dice_6': 5.0, 'loss_mask_7': 5.0, 'loss_ce_7': 0.0, 'loss_dice_7': 5.0, 'loss_mask_8': 5.0, 'loss_ce_8': 0.0, 'loss_dice_8': 5.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (upsampler): UpsampleMasks(
    (conv2d): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
[02/17 22:57:09] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 22:57:14] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 22:57:14] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 22:57:15] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 22:57:15] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 22:57:15] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 22:57:15] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 22:57:16] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mupsampler.conv2d.{bias, weight}[0m
[02/17 22:57:16] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 22:57:16] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 22:58:08] detectron2.engine.hooks INFO: Overall training speed: 15 iterations in 0:00:33 (2.2646 s / it)
[02/17 22:58:08] detectron2.engine.hooks INFO: Total training time: 0:00:33 (0:00:00 on hooks)
[02/17 23:05:11] detectron2 INFO: Rank of current process: 3. World size: 4
[02/17 23:05:16] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 23:05:16] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 23:05:16] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 23:05:16] detectron2.utils.env INFO: Using a generated random seed 16276163
[02/17 23:05:20] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 5.0, 'loss_ce': 0.0, 'loss_dice': 5.0, 'loss_seg': 0.1, 'loss_mask_0': 5.0, 'loss_ce_0': 0.0, 'loss_dice_0': 5.0, 'loss_mask_1': 5.0, 'loss_ce_1': 0.0, 'loss_dice_1': 5.0, 'loss_mask_2': 5.0, 'loss_ce_2': 0.0, 'loss_dice_2': 5.0, 'loss_mask_3': 5.0, 'loss_ce_3': 0.0, 'loss_dice_3': 5.0, 'loss_mask_4': 5.0, 'loss_ce_4': 0.0, 'loss_dice_4': 5.0, 'loss_mask_5': 5.0, 'loss_ce_5': 0.0, 'loss_dice_5': 5.0, 'loss_mask_6': 5.0, 'loss_ce_6': 0.0, 'loss_dice_6': 5.0, 'loss_mask_7': 5.0, 'loss_ce_7': 0.0, 'loss_dice_7': 5.0, 'loss_mask_8': 5.0, 'loss_ce_8': 0.0, 'loss_dice_8': 5.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (upsampler): UpsampleMasks(
    (conv2d): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
[02/17 23:05:20] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 23:05:26] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 23:05:27] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 23:05:27] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 23:05:27] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 23:05:27] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 23:05:28] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 23:05:29] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[34mupsampler.conv2d.{bias, weight}[0m
[02/17 23:05:29] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 23:05:29] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/18 00:32:40] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 00:32:40] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 00:32:40] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 00:33:00] mask2former INFO: Inference done 11/1092. Dataloading: 0.0050 s/iter. Inference: 0.3159 s/iter. Eval: 0.1216 s/iter. Total: 0.4425 s/iter. ETA=0:07:58
[02/18 00:33:05] mask2former INFO: Inference done 20/1092. Dataloading: 0.0072 s/iter. Inference: 0.3374 s/iter. Eval: 0.1896 s/iter. Total: 0.5343 s/iter. ETA=0:09:32
[02/18 00:33:10] mask2former INFO: Inference done 30/1092. Dataloading: 0.0076 s/iter. Inference: 0.3373 s/iter. Eval: 0.1808 s/iter. Total: 0.5258 s/iter. ETA=0:09:18
[02/18 00:33:16] mask2former INFO: Inference done 43/1092. Dataloading: 0.0114 s/iter. Inference: 0.3119 s/iter. Eval: 0.1627 s/iter. Total: 0.4861 s/iter. ETA=0:08:29
[02/18 00:33:21] mask2former INFO: Inference done 55/1092. Dataloading: 0.0106 s/iter. Inference: 0.3014 s/iter. Eval: 0.1581 s/iter. Total: 0.4701 s/iter. ETA=0:08:07
[02/18 00:33:26] mask2former INFO: Inference done 68/1092. Dataloading: 0.0096 s/iter. Inference: 0.2894 s/iter. Eval: 0.1564 s/iter. Total: 0.4555 s/iter. ETA=0:07:46
[02/18 00:33:31] mask2former INFO: Inference done 81/1092. Dataloading: 0.0093 s/iter. Inference: 0.2854 s/iter. Eval: 0.1543 s/iter. Total: 0.4491 s/iter. ETA=0:07:34
[02/18 00:33:37] mask2former INFO: Inference done 95/1092. Dataloading: 0.0089 s/iter. Inference: 0.2806 s/iter. Eval: 0.1495 s/iter. Total: 0.4391 s/iter. ETA=0:07:17
[02/18 00:33:42] mask2former INFO: Inference done 108/1092. Dataloading: 0.0086 s/iter. Inference: 0.2769 s/iter. Eval: 0.1476 s/iter. Total: 0.4331 s/iter. ETA=0:07:06
[02/18 00:33:47] mask2former INFO: Inference done 121/1092. Dataloading: 0.0084 s/iter. Inference: 0.2750 s/iter. Eval: 0.1477 s/iter. Total: 0.4312 s/iter. ETA=0:06:58
[02/18 00:33:52] mask2former INFO: Inference done 133/1092. Dataloading: 0.0082 s/iter. Inference: 0.2740 s/iter. Eval: 0.1477 s/iter. Total: 0.4301 s/iter. ETA=0:06:52
[02/18 00:33:57] mask2former INFO: Inference done 145/1092. Dataloading: 0.0082 s/iter. Inference: 0.2732 s/iter. Eval: 0.1476 s/iter. Total: 0.4291 s/iter. ETA=0:06:46
[02/18 00:34:03] mask2former INFO: Inference done 158/1092. Dataloading: 0.0081 s/iter. Inference: 0.2721 s/iter. Eval: 0.1466 s/iter. Total: 0.4269 s/iter. ETA=0:06:38
[02/18 00:34:08] mask2former INFO: Inference done 171/1092. Dataloading: 0.0080 s/iter. Inference: 0.2701 s/iter. Eval: 0.1460 s/iter. Total: 0.4242 s/iter. ETA=0:06:30
[02/18 00:34:13] mask2former INFO: Inference done 183/1092. Dataloading: 0.0080 s/iter. Inference: 0.2703 s/iter. Eval: 0.1457 s/iter. Total: 0.4241 s/iter. ETA=0:06:25
[02/18 00:34:18] mask2former INFO: Inference done 196/1092. Dataloading: 0.0079 s/iter. Inference: 0.2684 s/iter. Eval: 0.1461 s/iter. Total: 0.4225 s/iter. ETA=0:06:18
[02/18 00:34:23] mask2former INFO: Inference done 208/1092. Dataloading: 0.0080 s/iter. Inference: 0.2692 s/iter. Eval: 0.1463 s/iter. Total: 0.4236 s/iter. ETA=0:06:14
[02/18 00:34:29] mask2former INFO: Inference done 220/1092. Dataloading: 0.0080 s/iter. Inference: 0.2703 s/iter. Eval: 0.1459 s/iter. Total: 0.4244 s/iter. ETA=0:06:10
[02/18 00:34:34] mask2former INFO: Inference done 232/1092. Dataloading: 0.0079 s/iter. Inference: 0.2726 s/iter. Eval: 0.1442 s/iter. Total: 0.4248 s/iter. ETA=0:06:05
[02/18 00:34:39] mask2former INFO: Inference done 243/1092. Dataloading: 0.0079 s/iter. Inference: 0.2743 s/iter. Eval: 0.1440 s/iter. Total: 0.4263 s/iter. ETA=0:06:01
[02/18 00:34:44] mask2former INFO: Inference done 255/1092. Dataloading: 0.0079 s/iter. Inference: 0.2739 s/iter. Eval: 0.1441 s/iter. Total: 0.4261 s/iter. ETA=0:05:56
[02/18 00:34:49] mask2former INFO: Inference done 268/1092. Dataloading: 0.0079 s/iter. Inference: 0.2728 s/iter. Eval: 0.1439 s/iter. Total: 0.4247 s/iter. ETA=0:05:49
[02/18 00:34:54] mask2former INFO: Inference done 280/1092. Dataloading: 0.0078 s/iter. Inference: 0.2731 s/iter. Eval: 0.1438 s/iter. Total: 0.4247 s/iter. ETA=0:05:44
[02/18 00:34:59] mask2former INFO: Inference done 292/1092. Dataloading: 0.0078 s/iter. Inference: 0.2722 s/iter. Eval: 0.1446 s/iter. Total: 0.4247 s/iter. ETA=0:05:39
[02/18 00:35:04] mask2former INFO: Inference done 304/1092. Dataloading: 0.0079 s/iter. Inference: 0.2726 s/iter. Eval: 0.1447 s/iter. Total: 0.4252 s/iter. ETA=0:05:35
[02/18 00:35:10] mask2former INFO: Inference done 316/1092. Dataloading: 0.0080 s/iter. Inference: 0.2730 s/iter. Eval: 0.1447 s/iter. Total: 0.4257 s/iter. ETA=0:05:30
[02/18 00:35:15] mask2former INFO: Inference done 329/1092. Dataloading: 0.0079 s/iter. Inference: 0.2719 s/iter. Eval: 0.1447 s/iter. Total: 0.4247 s/iter. ETA=0:05:24
[02/18 00:35:20] mask2former INFO: Inference done 342/1092. Dataloading: 0.0080 s/iter. Inference: 0.2714 s/iter. Eval: 0.1448 s/iter. Total: 0.4243 s/iter. ETA=0:05:18
[02/18 00:35:26] mask2former INFO: Inference done 355/1092. Dataloading: 0.0081 s/iter. Inference: 0.2707 s/iter. Eval: 0.1446 s/iter. Total: 0.4235 s/iter. ETA=0:05:12
[02/18 00:35:31] mask2former INFO: Inference done 367/1092. Dataloading: 0.0081 s/iter. Inference: 0.2702 s/iter. Eval: 0.1450 s/iter. Total: 0.4235 s/iter. ETA=0:05:07
[02/18 00:35:36] mask2former INFO: Inference done 380/1092. Dataloading: 0.0081 s/iter. Inference: 0.2696 s/iter. Eval: 0.1444 s/iter. Total: 0.4222 s/iter. ETA=0:05:00
[02/18 00:35:41] mask2former INFO: Inference done 393/1092. Dataloading: 0.0081 s/iter. Inference: 0.2689 s/iter. Eval: 0.1443 s/iter. Total: 0.4214 s/iter. ETA=0:04:54
[02/18 00:35:46] mask2former INFO: Inference done 406/1092. Dataloading: 0.0081 s/iter. Inference: 0.2688 s/iter. Eval: 0.1440 s/iter. Total: 0.4210 s/iter. ETA=0:04:48
[02/18 00:35:51] mask2former INFO: Inference done 419/1092. Dataloading: 0.0080 s/iter. Inference: 0.2680 s/iter. Eval: 0.1443 s/iter. Total: 0.4204 s/iter. ETA=0:04:42
[02/18 00:35:56] mask2former INFO: Inference done 432/1092. Dataloading: 0.0079 s/iter. Inference: 0.2678 s/iter. Eval: 0.1438 s/iter. Total: 0.4196 s/iter. ETA=0:04:36
[02/18 00:36:02] mask2former INFO: Inference done 445/1092. Dataloading: 0.0079 s/iter. Inference: 0.2672 s/iter. Eval: 0.1436 s/iter. Total: 0.4188 s/iter. ETA=0:04:30
[02/18 00:36:07] mask2former INFO: Inference done 458/1092. Dataloading: 0.0079 s/iter. Inference: 0.2672 s/iter. Eval: 0.1433 s/iter. Total: 0.4185 s/iter. ETA=0:04:25
[02/18 00:36:12] mask2former INFO: Inference done 470/1092. Dataloading: 0.0078 s/iter. Inference: 0.2671 s/iter. Eval: 0.1435 s/iter. Total: 0.4185 s/iter. ETA=0:04:20
[02/18 00:36:17] mask2former INFO: Inference done 482/1092. Dataloading: 0.0078 s/iter. Inference: 0.2670 s/iter. Eval: 0.1437 s/iter. Total: 0.4186 s/iter. ETA=0:04:15
[02/18 00:36:22] mask2former INFO: Inference done 495/1092. Dataloading: 0.0078 s/iter. Inference: 0.2671 s/iter. Eval: 0.1431 s/iter. Total: 0.4180 s/iter. ETA=0:04:09
[02/18 00:36:27] mask2former INFO: Inference done 508/1092. Dataloading: 0.0077 s/iter. Inference: 0.2672 s/iter. Eval: 0.1425 s/iter. Total: 0.4175 s/iter. ETA=0:04:03
[02/18 00:36:33] mask2former INFO: Inference done 520/1092. Dataloading: 0.0077 s/iter. Inference: 0.2677 s/iter. Eval: 0.1429 s/iter. Total: 0.4185 s/iter. ETA=0:03:59
[02/18 00:36:38] mask2former INFO: Inference done 532/1092. Dataloading: 0.0078 s/iter. Inference: 0.2681 s/iter. Eval: 0.1429 s/iter. Total: 0.4188 s/iter. ETA=0:03:54
[02/18 00:36:43] mask2former INFO: Inference done 544/1092. Dataloading: 0.0078 s/iter. Inference: 0.2689 s/iter. Eval: 0.1426 s/iter. Total: 0.4194 s/iter. ETA=0:03:49
[02/18 00:36:48] mask2former INFO: Inference done 556/1092. Dataloading: 0.0077 s/iter. Inference: 0.2688 s/iter. Eval: 0.1428 s/iter. Total: 0.4195 s/iter. ETA=0:03:44
[02/18 00:36:54] mask2former INFO: Inference done 569/1092. Dataloading: 0.0077 s/iter. Inference: 0.2687 s/iter. Eval: 0.1423 s/iter. Total: 0.4189 s/iter. ETA=0:03:39
[02/18 00:36:59] mask2former INFO: Inference done 582/1092. Dataloading: 0.0077 s/iter. Inference: 0.2687 s/iter. Eval: 0.1420 s/iter. Total: 0.4185 s/iter. ETA=0:03:33
[02/18 00:37:04] mask2former INFO: Inference done 594/1092. Dataloading: 0.0077 s/iter. Inference: 0.2690 s/iter. Eval: 0.1423 s/iter. Total: 0.4191 s/iter. ETA=0:03:28
[02/18 00:37:09] mask2former INFO: Inference done 607/1092. Dataloading: 0.0078 s/iter. Inference: 0.2684 s/iter. Eval: 0.1421 s/iter. Total: 0.4184 s/iter. ETA=0:03:22
[02/18 00:37:14] mask2former INFO: Inference done 620/1092. Dataloading: 0.0077 s/iter. Inference: 0.2679 s/iter. Eval: 0.1422 s/iter. Total: 0.4180 s/iter. ETA=0:03:17
[02/18 00:37:19] mask2former INFO: Inference done 633/1092. Dataloading: 0.0077 s/iter. Inference: 0.2674 s/iter. Eval: 0.1422 s/iter. Total: 0.4174 s/iter. ETA=0:03:11
[02/18 00:37:25] mask2former INFO: Inference done 646/1092. Dataloading: 0.0077 s/iter. Inference: 0.2667 s/iter. Eval: 0.1427 s/iter. Total: 0.4172 s/iter. ETA=0:03:06
[02/18 00:37:30] mask2former INFO: Inference done 658/1092. Dataloading: 0.0078 s/iter. Inference: 0.2667 s/iter. Eval: 0.1429 s/iter. Total: 0.4175 s/iter. ETA=0:03:01
[02/18 00:37:35] mask2former INFO: Inference done 668/1092. Dataloading: 0.0078 s/iter. Inference: 0.2683 s/iter. Eval: 0.1427 s/iter. Total: 0.4189 s/iter. ETA=0:02:57
[02/18 00:37:40] mask2former INFO: Inference done 678/1092. Dataloading: 0.0078 s/iter. Inference: 0.2700 s/iter. Eval: 0.1426 s/iter. Total: 0.4204 s/iter. ETA=0:02:54
[02/18 00:37:46] mask2former INFO: Inference done 688/1092. Dataloading: 0.0078 s/iter. Inference: 0.2717 s/iter. Eval: 0.1426 s/iter. Total: 0.4222 s/iter. ETA=0:02:50
[02/18 00:37:51] mask2former INFO: Inference done 698/1092. Dataloading: 0.0078 s/iter. Inference: 0.2731 s/iter. Eval: 0.1428 s/iter. Total: 0.4238 s/iter. ETA=0:02:46
[02/18 00:37:56] mask2former INFO: Inference done 708/1092. Dataloading: 0.0078 s/iter. Inference: 0.2746 s/iter. Eval: 0.1430 s/iter. Total: 0.4255 s/iter. ETA=0:02:43
[02/18 00:38:01] mask2former INFO: Inference done 718/1092. Dataloading: 0.0077 s/iter. Inference: 0.2760 s/iter. Eval: 0.1427 s/iter. Total: 0.4266 s/iter. ETA=0:02:39
[02/18 00:38:07] mask2former INFO: Inference done 727/1092. Dataloading: 0.0079 s/iter. Inference: 0.2776 s/iter. Eval: 0.1426 s/iter. Total: 0.4283 s/iter. ETA=0:02:36
[02/18 00:38:12] mask2former INFO: Inference done 737/1092. Dataloading: 0.0079 s/iter. Inference: 0.2790 s/iter. Eval: 0.1427 s/iter. Total: 0.4297 s/iter. ETA=0:02:32
[02/18 00:38:17] mask2former INFO: Inference done 747/1092. Dataloading: 0.0079 s/iter. Inference: 0.2803 s/iter. Eval: 0.1427 s/iter. Total: 0.4310 s/iter. ETA=0:02:28
[02/18 00:38:23] mask2former INFO: Inference done 757/1092. Dataloading: 0.0079 s/iter. Inference: 0.2816 s/iter. Eval: 0.1429 s/iter. Total: 0.4325 s/iter. ETA=0:02:24
[02/18 00:38:28] mask2former INFO: Inference done 767/1092. Dataloading: 0.0079 s/iter. Inference: 0.2832 s/iter. Eval: 0.1429 s/iter. Total: 0.4341 s/iter. ETA=0:02:21
[02/18 00:38:34] mask2former INFO: Inference done 776/1092. Dataloading: 0.0079 s/iter. Inference: 0.2848 s/iter. Eval: 0.1433 s/iter. Total: 0.4361 s/iter. ETA=0:02:17
[02/18 00:38:39] mask2former INFO: Inference done 786/1092. Dataloading: 0.0079 s/iter. Inference: 0.2864 s/iter. Eval: 0.1432 s/iter. Total: 0.4376 s/iter. ETA=0:02:13
[02/18 00:38:44] mask2former INFO: Inference done 796/1092. Dataloading: 0.0079 s/iter. Inference: 0.2879 s/iter. Eval: 0.1429 s/iter. Total: 0.4389 s/iter. ETA=0:02:09
[02/18 00:38:49] mask2former INFO: Inference done 804/1092. Dataloading: 0.0080 s/iter. Inference: 0.2894 s/iter. Eval: 0.1433 s/iter. Total: 0.4408 s/iter. ETA=0:02:06
[02/18 00:38:55] mask2former INFO: Inference done 814/1092. Dataloading: 0.0079 s/iter. Inference: 0.2904 s/iter. Eval: 0.1435 s/iter. Total: 0.4420 s/iter. ETA=0:02:02
[02/18 00:39:00] mask2former INFO: Inference done 823/1092. Dataloading: 0.0079 s/iter. Inference: 0.2915 s/iter. Eval: 0.1438 s/iter. Total: 0.4433 s/iter. ETA=0:01:59
[02/18 00:39:05] mask2former INFO: Inference done 832/1092. Dataloading: 0.0079 s/iter. Inference: 0.2928 s/iter. Eval: 0.1438 s/iter. Total: 0.4446 s/iter. ETA=0:01:55
[02/18 00:39:10] mask2former INFO: Inference done 841/1092. Dataloading: 0.0079 s/iter. Inference: 0.2940 s/iter. Eval: 0.1438 s/iter. Total: 0.4458 s/iter. ETA=0:01:51
[02/18 00:39:16] mask2former INFO: Inference done 851/1092. Dataloading: 0.0079 s/iter. Inference: 0.2953 s/iter. Eval: 0.1437 s/iter. Total: 0.4471 s/iter. ETA=0:01:47
[02/18 00:39:21] mask2former INFO: Inference done 861/1092. Dataloading: 0.0079 s/iter. Inference: 0.2965 s/iter. Eval: 0.1439 s/iter. Total: 0.4483 s/iter. ETA=0:01:43
[02/18 00:39:27] mask2former INFO: Inference done 870/1092. Dataloading: 0.0079 s/iter. Inference: 0.2979 s/iter. Eval: 0.1441 s/iter. Total: 0.4499 s/iter. ETA=0:01:39
[02/18 00:39:32] mask2former INFO: Inference done 880/1092. Dataloading: 0.0078 s/iter. Inference: 0.2989 s/iter. Eval: 0.1442 s/iter. Total: 0.4511 s/iter. ETA=0:01:35
[02/18 00:39:38] mask2former INFO: Inference done 890/1092. Dataloading: 0.0078 s/iter. Inference: 0.3002 s/iter. Eval: 0.1442 s/iter. Total: 0.4523 s/iter. ETA=0:01:31
[02/18 00:39:43] mask2former INFO: Inference done 899/1092. Dataloading: 0.0078 s/iter. Inference: 0.3011 s/iter. Eval: 0.1445 s/iter. Total: 0.4536 s/iter. ETA=0:01:27
[02/18 00:39:48] mask2former INFO: Inference done 908/1092. Dataloading: 0.0078 s/iter. Inference: 0.3022 s/iter. Eval: 0.1447 s/iter. Total: 0.4548 s/iter. ETA=0:01:23
[02/18 00:39:53] mask2former INFO: Inference done 918/1092. Dataloading: 0.0078 s/iter. Inference: 0.3031 s/iter. Eval: 0.1446 s/iter. Total: 0.4557 s/iter. ETA=0:01:19
[02/18 00:39:59] mask2former INFO: Inference done 927/1092. Dataloading: 0.0078 s/iter. Inference: 0.3044 s/iter. Eval: 0.1447 s/iter. Total: 0.4570 s/iter. ETA=0:01:15
[02/18 00:40:04] mask2former INFO: Inference done 936/1092. Dataloading: 0.0078 s/iter. Inference: 0.3056 s/iter. Eval: 0.1447 s/iter. Total: 0.4582 s/iter. ETA=0:01:11
[02/18 00:40:09] mask2former INFO: Inference done 945/1092. Dataloading: 0.0078 s/iter. Inference: 0.3066 s/iter. Eval: 0.1448 s/iter. Total: 0.4594 s/iter. ETA=0:01:07
[02/18 00:40:14] mask2former INFO: Inference done 954/1092. Dataloading: 0.0078 s/iter. Inference: 0.3075 s/iter. Eval: 0.1449 s/iter. Total: 0.4604 s/iter. ETA=0:01:03
[02/18 00:40:20] mask2former INFO: Inference done 964/1092. Dataloading: 0.0078 s/iter. Inference: 0.3084 s/iter. Eval: 0.1450 s/iter. Total: 0.4613 s/iter. ETA=0:00:59
[02/18 00:40:25] mask2former INFO: Inference done 974/1092. Dataloading: 0.0078 s/iter. Inference: 0.3092 s/iter. Eval: 0.1450 s/iter. Total: 0.4621 s/iter. ETA=0:00:54
[02/18 00:40:30] mask2former INFO: Inference done 983/1092. Dataloading: 0.0078 s/iter. Inference: 0.3101 s/iter. Eval: 0.1450 s/iter. Total: 0.4631 s/iter. ETA=0:00:50
[02/18 00:40:35] mask2former INFO: Inference done 991/1092. Dataloading: 0.0079 s/iter. Inference: 0.3109 s/iter. Eval: 0.1455 s/iter. Total: 0.4644 s/iter. ETA=0:00:46
[02/18 00:40:40] mask2former INFO: Inference done 1000/1092. Dataloading: 0.0078 s/iter. Inference: 0.3118 s/iter. Eval: 0.1455 s/iter. Total: 0.4653 s/iter. ETA=0:00:42
[02/18 00:40:46] mask2former INFO: Inference done 1010/1092. Dataloading: 0.0078 s/iter. Inference: 0.3127 s/iter. Eval: 0.1453 s/iter. Total: 0.4660 s/iter. ETA=0:00:38
[02/18 00:40:51] mask2former INFO: Inference done 1019/1092. Dataloading: 0.0078 s/iter. Inference: 0.3136 s/iter. Eval: 0.1454 s/iter. Total: 0.4670 s/iter. ETA=0:00:34
[02/18 00:40:56] mask2former INFO: Inference done 1029/1092. Dataloading: 0.0078 s/iter. Inference: 0.3145 s/iter. Eval: 0.1452 s/iter. Total: 0.4676 s/iter. ETA=0:00:29
[02/18 00:41:01] mask2former INFO: Inference done 1039/1092. Dataloading: 0.0078 s/iter. Inference: 0.3152 s/iter. Eval: 0.1450 s/iter. Total: 0.4682 s/iter. ETA=0:00:24
[02/18 00:41:07] mask2former INFO: Inference done 1049/1092. Dataloading: 0.0078 s/iter. Inference: 0.3161 s/iter. Eval: 0.1449 s/iter. Total: 0.4689 s/iter. ETA=0:00:20
[02/18 00:41:12] mask2former INFO: Inference done 1059/1092. Dataloading: 0.0078 s/iter. Inference: 0.3168 s/iter. Eval: 0.1446 s/iter. Total: 0.4693 s/iter. ETA=0:00:15
[02/18 00:41:17] mask2former INFO: Inference done 1069/1092. Dataloading: 0.0077 s/iter. Inference: 0.3175 s/iter. Eval: 0.1443 s/iter. Total: 0.4697 s/iter. ETA=0:00:10
[02/18 00:41:22] mask2former INFO: Inference done 1079/1092. Dataloading: 0.0077 s/iter. Inference: 0.3183 s/iter. Eval: 0.1441 s/iter. Total: 0.4702 s/iter. ETA=0:00:06
[02/18 00:41:28] mask2former INFO: Inference done 1089/1092. Dataloading: 0.0077 s/iter. Inference: 0.3192 s/iter. Eval: 0.1438 s/iter. Total: 0.4708 s/iter. ETA=0:00:01
[02/18 03:18:44] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 03:18:46] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 03:18:46] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 03:19:06] mask2former INFO: Inference done 11/1092. Dataloading: 0.0045 s/iter. Inference: 0.4552 s/iter. Eval: 0.1421 s/iter. Total: 0.6018 s/iter. ETA=0:10:50
[02/18 03:19:11] mask2former INFO: Inference done 19/1092. Dataloading: 0.0082 s/iter. Inference: 0.4782 s/iter. Eval: 0.1557 s/iter. Total: 0.6422 s/iter. ETA=0:11:29
[02/18 03:19:17] mask2former INFO: Inference done 28/1092. Dataloading: 0.0074 s/iter. Inference: 0.4587 s/iter. Eval: 0.1606 s/iter. Total: 0.6268 s/iter. ETA=0:11:06
[02/18 03:19:22] mask2former INFO: Inference done 36/1092. Dataloading: 0.0079 s/iter. Inference: 0.4576 s/iter. Eval: 0.1631 s/iter. Total: 0.6287 s/iter. ETA=0:11:03
[02/18 03:19:27] mask2former INFO: Inference done 44/1092. Dataloading: 0.0096 s/iter. Inference: 0.4631 s/iter. Eval: 0.1608 s/iter. Total: 0.6335 s/iter. ETA=0:11:03
[02/18 03:19:32] mask2former INFO: Inference done 52/1092. Dataloading: 0.0092 s/iter. Inference: 0.4588 s/iter. Eval: 0.1640 s/iter. Total: 0.6321 s/iter. ETA=0:10:57
[02/18 03:19:37] mask2former INFO: Inference done 60/1092. Dataloading: 0.0094 s/iter. Inference: 0.4547 s/iter. Eval: 0.1669 s/iter. Total: 0.6311 s/iter. ETA=0:10:51
[02/18 03:19:42] mask2former INFO: Inference done 68/1092. Dataloading: 0.0090 s/iter. Inference: 0.4516 s/iter. Eval: 0.1715 s/iter. Total: 0.6322 s/iter. ETA=0:10:47
[02/18 03:19:47] mask2former INFO: Inference done 75/1092. Dataloading: 0.0098 s/iter. Inference: 0.4554 s/iter. Eval: 0.1766 s/iter. Total: 0.6419 s/iter. ETA=0:10:52
[02/18 03:19:53] mask2former INFO: Inference done 83/1092. Dataloading: 0.0110 s/iter. Inference: 0.4546 s/iter. Eval: 0.1824 s/iter. Total: 0.6481 s/iter. ETA=0:10:53
[02/18 03:19:58] mask2former INFO: Inference done 92/1092. Dataloading: 0.0107 s/iter. Inference: 0.4521 s/iter. Eval: 0.1808 s/iter. Total: 0.6437 s/iter. ETA=0:10:43
[02/18 03:20:04] mask2former INFO: Inference done 101/1092. Dataloading: 0.0104 s/iter. Inference: 0.4516 s/iter. Eval: 0.1790 s/iter. Total: 0.6411 s/iter. ETA=0:10:35
[02/18 03:20:09] mask2former INFO: Inference done 109/1092. Dataloading: 0.0104 s/iter. Inference: 0.4506 s/iter. Eval: 0.1803 s/iter. Total: 0.6414 s/iter. ETA=0:10:30
[02/18 03:20:15] mask2former INFO: Inference done 117/1092. Dataloading: 0.0102 s/iter. Inference: 0.4512 s/iter. Eval: 0.1822 s/iter. Total: 0.6438 s/iter. ETA=0:10:27
[02/18 03:20:20] mask2former INFO: Inference done 125/1092. Dataloading: 0.0103 s/iter. Inference: 0.4495 s/iter. Eval: 0.1829 s/iter. Total: 0.6429 s/iter. ETA=0:10:21
[02/18 03:20:25] mask2former INFO: Inference done 134/1092. Dataloading: 0.0101 s/iter. Inference: 0.4492 s/iter. Eval: 0.1813 s/iter. Total: 0.6408 s/iter. ETA=0:10:13
[02/18 03:20:31] mask2former INFO: Inference done 142/1092. Dataloading: 0.0103 s/iter. Inference: 0.4518 s/iter. Eval: 0.1804 s/iter. Total: 0.6427 s/iter. ETA=0:10:10
[02/18 03:20:36] mask2former INFO: Inference done 150/1092. Dataloading: 0.0102 s/iter. Inference: 0.4515 s/iter. Eval: 0.1814 s/iter. Total: 0.6432 s/iter. ETA=0:10:05
[02/18 03:20:41] mask2former INFO: Inference done 158/1092. Dataloading: 0.0099 s/iter. Inference: 0.4512 s/iter. Eval: 0.1830 s/iter. Total: 0.6442 s/iter. ETA=0:10:01
[02/18 03:20:46] mask2former INFO: Inference done 165/1092. Dataloading: 0.0100 s/iter. Inference: 0.4525 s/iter. Eval: 0.1851 s/iter. Total: 0.6478 s/iter. ETA=0:10:00
[02/18 03:20:51] mask2former INFO: Inference done 172/1092. Dataloading: 0.0102 s/iter. Inference: 0.4549 s/iter. Eval: 0.1863 s/iter. Total: 0.6516 s/iter. ETA=0:09:59
[02/18 03:20:57] mask2former INFO: Inference done 180/1092. Dataloading: 0.0102 s/iter. Inference: 0.4557 s/iter. Eval: 0.1860 s/iter. Total: 0.6520 s/iter. ETA=0:09:54
[02/18 03:21:02] mask2former INFO: Inference done 188/1092. Dataloading: 0.0102 s/iter. Inference: 0.4566 s/iter. Eval: 0.1877 s/iter. Total: 0.6546 s/iter. ETA=0:09:51
[02/18 03:21:08] mask2former INFO: Inference done 196/1092. Dataloading: 0.0102 s/iter. Inference: 0.4565 s/iter. Eval: 0.1884 s/iter. Total: 0.6553 s/iter. ETA=0:09:47
[02/18 03:21:13] mask2former INFO: Inference done 205/1092. Dataloading: 0.0102 s/iter. Inference: 0.4554 s/iter. Eval: 0.1881 s/iter. Total: 0.6538 s/iter. ETA=0:09:39
[02/18 03:21:18] mask2former INFO: Inference done 214/1092. Dataloading: 0.0101 s/iter. Inference: 0.4543 s/iter. Eval: 0.1861 s/iter. Total: 0.6506 s/iter. ETA=0:09:31
[02/18 03:21:24] mask2former INFO: Inference done 222/1092. Dataloading: 0.0100 s/iter. Inference: 0.4538 s/iter. Eval: 0.1872 s/iter. Total: 0.6512 s/iter. ETA=0:09:26
[02/18 03:21:29] mask2former INFO: Inference done 230/1092. Dataloading: 0.0101 s/iter. Inference: 0.4539 s/iter. Eval: 0.1879 s/iter. Total: 0.6520 s/iter. ETA=0:09:21
[02/18 03:21:34] mask2former INFO: Inference done 238/1092. Dataloading: 0.0101 s/iter. Inference: 0.4536 s/iter. Eval: 0.1882 s/iter. Total: 0.6521 s/iter. ETA=0:09:16
[02/18 03:21:40] mask2former INFO: Inference done 246/1092. Dataloading: 0.0100 s/iter. Inference: 0.4536 s/iter. Eval: 0.1880 s/iter. Total: 0.6518 s/iter. ETA=0:09:11
[02/18 03:21:45] mask2former INFO: Inference done 255/1092. Dataloading: 0.0100 s/iter. Inference: 0.4518 s/iter. Eval: 0.1875 s/iter. Total: 0.6495 s/iter. ETA=0:09:03
[02/18 03:21:50] mask2former INFO: Inference done 263/1092. Dataloading: 0.0099 s/iter. Inference: 0.4524 s/iter. Eval: 0.1877 s/iter. Total: 0.6501 s/iter. ETA=0:08:58
[02/18 03:21:55] mask2former INFO: Inference done 271/1092. Dataloading: 0.0099 s/iter. Inference: 0.4522 s/iter. Eval: 0.1873 s/iter. Total: 0.6496 s/iter. ETA=0:08:53
[02/18 03:22:01] mask2former INFO: Inference done 279/1092. Dataloading: 0.0099 s/iter. Inference: 0.4530 s/iter. Eval: 0.1875 s/iter. Total: 0.6506 s/iter. ETA=0:08:48
[02/18 03:22:06] mask2former INFO: Inference done 287/1092. Dataloading: 0.0099 s/iter. Inference: 0.4535 s/iter. Eval: 0.1879 s/iter. Total: 0.6515 s/iter. ETA=0:08:44
[02/18 03:22:12] mask2former INFO: Inference done 295/1092. Dataloading: 0.0098 s/iter. Inference: 0.4538 s/iter. Eval: 0.1884 s/iter. Total: 0.6522 s/iter. ETA=0:08:39
[02/18 03:22:17] mask2former INFO: Inference done 303/1092. Dataloading: 0.0101 s/iter. Inference: 0.4548 s/iter. Eval: 0.1882 s/iter. Total: 0.6532 s/iter. ETA=0:08:35
[02/18 03:22:22] mask2former INFO: Inference done 311/1092. Dataloading: 0.0102 s/iter. Inference: 0.4545 s/iter. Eval: 0.1884 s/iter. Total: 0.6532 s/iter. ETA=0:08:30
[02/18 03:22:28] mask2former INFO: Inference done 319/1092. Dataloading: 0.0101 s/iter. Inference: 0.4548 s/iter. Eval: 0.1885 s/iter. Total: 0.6536 s/iter. ETA=0:08:25
[02/18 03:22:33] mask2former INFO: Inference done 326/1092. Dataloading: 0.0101 s/iter. Inference: 0.4557 s/iter. Eval: 0.1898 s/iter. Total: 0.6557 s/iter. ETA=0:08:22
[02/18 03:22:38] mask2former INFO: Inference done 333/1092. Dataloading: 0.0102 s/iter. Inference: 0.4568 s/iter. Eval: 0.1908 s/iter. Total: 0.6579 s/iter. ETA=0:08:19
[02/18 03:22:44] mask2former INFO: Inference done 342/1092. Dataloading: 0.0102 s/iter. Inference: 0.4558 s/iter. Eval: 0.1908 s/iter. Total: 0.6569 s/iter. ETA=0:08:12
[02/18 03:22:49] mask2former INFO: Inference done 351/1092. Dataloading: 0.0101 s/iter. Inference: 0.4555 s/iter. Eval: 0.1902 s/iter. Total: 0.6559 s/iter. ETA=0:08:06
[02/18 03:22:55] mask2former INFO: Inference done 360/1092. Dataloading: 0.0101 s/iter. Inference: 0.4556 s/iter. Eval: 0.1891 s/iter. Total: 0.6549 s/iter. ETA=0:07:59
[02/18 03:23:00] mask2former INFO: Inference done 368/1092. Dataloading: 0.0100 s/iter. Inference: 0.4558 s/iter. Eval: 0.1893 s/iter. Total: 0.6554 s/iter. ETA=0:07:54
[02/18 03:23:05] mask2former INFO: Inference done 376/1092. Dataloading: 0.0100 s/iter. Inference: 0.4559 s/iter. Eval: 0.1889 s/iter. Total: 0.6549 s/iter. ETA=0:07:48
[02/18 03:23:10] mask2former INFO: Inference done 383/1092. Dataloading: 0.0099 s/iter. Inference: 0.4565 s/iter. Eval: 0.1895 s/iter. Total: 0.6561 s/iter. ETA=0:07:45
[02/18 03:23:16] mask2former INFO: Inference done 392/1092. Dataloading: 0.0100 s/iter. Inference: 0.4562 s/iter. Eval: 0.1888 s/iter. Total: 0.6552 s/iter. ETA=0:07:38
[02/18 03:23:22] mask2former INFO: Inference done 400/1092. Dataloading: 0.0099 s/iter. Inference: 0.4566 s/iter. Eval: 0.1891 s/iter. Total: 0.6558 s/iter. ETA=0:07:33
[02/18 03:23:27] mask2former INFO: Inference done 410/1092. Dataloading: 0.0099 s/iter. Inference: 0.4549 s/iter. Eval: 0.1878 s/iter. Total: 0.6528 s/iter. ETA=0:07:25
[02/18 03:23:32] mask2former INFO: Inference done 419/1092. Dataloading: 0.0098 s/iter. Inference: 0.4541 s/iter. Eval: 0.1875 s/iter. Total: 0.6516 s/iter. ETA=0:07:18
[02/18 03:23:37] mask2former INFO: Inference done 427/1092. Dataloading: 0.0098 s/iter. Inference: 0.4540 s/iter. Eval: 0.1876 s/iter. Total: 0.6516 s/iter. ETA=0:07:13
[02/18 03:23:43] mask2former INFO: Inference done 436/1092. Dataloading: 0.0098 s/iter. Inference: 0.4539 s/iter. Eval: 0.1870 s/iter. Total: 0.6509 s/iter. ETA=0:07:06
[02/18 03:23:49] mask2former INFO: Inference done 445/1092. Dataloading: 0.0098 s/iter. Inference: 0.4539 s/iter. Eval: 0.1862 s/iter. Total: 0.6501 s/iter. ETA=0:07:00
[02/18 03:23:54] mask2former INFO: Inference done 454/1092. Dataloading: 0.0098 s/iter. Inference: 0.4540 s/iter. Eval: 0.1853 s/iter. Total: 0.6493 s/iter. ETA=0:06:54
[02/18 03:23:59] mask2former INFO: Inference done 462/1092. Dataloading: 0.0098 s/iter. Inference: 0.4540 s/iter. Eval: 0.1859 s/iter. Total: 0.6500 s/iter. ETA=0:06:49
[02/18 03:24:05] mask2former INFO: Inference done 471/1092. Dataloading: 0.0099 s/iter. Inference: 0.4537 s/iter. Eval: 0.1856 s/iter. Total: 0.6494 s/iter. ETA=0:06:43
[02/18 03:24:11] mask2former INFO: Inference done 480/1092. Dataloading: 0.0099 s/iter. Inference: 0.4537 s/iter. Eval: 0.1850 s/iter. Total: 0.6488 s/iter. ETA=0:06:37
[02/18 03:24:16] mask2former INFO: Inference done 488/1092. Dataloading: 0.0098 s/iter. Inference: 0.4541 s/iter. Eval: 0.1847 s/iter. Total: 0.6489 s/iter. ETA=0:06:31
[02/18 03:24:21] mask2former INFO: Inference done 497/1092. Dataloading: 0.0099 s/iter. Inference: 0.4541 s/iter. Eval: 0.1841 s/iter. Total: 0.6484 s/iter. ETA=0:06:25
[02/18 03:24:27] mask2former INFO: Inference done 505/1092. Dataloading: 0.0099 s/iter. Inference: 0.4541 s/iter. Eval: 0.1841 s/iter. Total: 0.6483 s/iter. ETA=0:06:20
[02/18 03:24:32] mask2former INFO: Inference done 513/1092. Dataloading: 0.0099 s/iter. Inference: 0.4544 s/iter. Eval: 0.1841 s/iter. Total: 0.6486 s/iter. ETA=0:06:15
[02/18 03:24:37] mask2former INFO: Inference done 522/1092. Dataloading: 0.0098 s/iter. Inference: 0.4541 s/iter. Eval: 0.1838 s/iter. Total: 0.6480 s/iter. ETA=0:06:09
[02/18 03:24:43] mask2former INFO: Inference done 531/1092. Dataloading: 0.0098 s/iter. Inference: 0.4542 s/iter. Eval: 0.1833 s/iter. Total: 0.6475 s/iter. ETA=0:06:03
[02/18 03:24:48] mask2former INFO: Inference done 539/1092. Dataloading: 0.0098 s/iter. Inference: 0.4545 s/iter. Eval: 0.1830 s/iter. Total: 0.6474 s/iter. ETA=0:05:58
[02/18 03:24:54] mask2former INFO: Inference done 548/1092. Dataloading: 0.0097 s/iter. Inference: 0.4540 s/iter. Eval: 0.1827 s/iter. Total: 0.6466 s/iter. ETA=0:05:51
[02/18 03:24:59] mask2former INFO: Inference done 557/1092. Dataloading: 0.0097 s/iter. Inference: 0.4535 s/iter. Eval: 0.1825 s/iter. Total: 0.6459 s/iter. ETA=0:05:45
[02/18 03:25:04] mask2former INFO: Inference done 565/1092. Dataloading: 0.0098 s/iter. Inference: 0.4536 s/iter. Eval: 0.1820 s/iter. Total: 0.6456 s/iter. ETA=0:05:40
[02/18 03:25:09] mask2former INFO: Inference done 573/1092. Dataloading: 0.0097 s/iter. Inference: 0.4540 s/iter. Eval: 0.1821 s/iter. Total: 0.6460 s/iter. ETA=0:05:35
[02/18 03:25:15] mask2former INFO: Inference done 582/1092. Dataloading: 0.0098 s/iter. Inference: 0.4539 s/iter. Eval: 0.1816 s/iter. Total: 0.6455 s/iter. ETA=0:05:29
[02/18 03:25:21] mask2former INFO: Inference done 591/1092. Dataloading: 0.0097 s/iter. Inference: 0.4541 s/iter. Eval: 0.1814 s/iter. Total: 0.6454 s/iter. ETA=0:05:23
[02/18 03:25:26] mask2former INFO: Inference done 600/1092. Dataloading: 0.0097 s/iter. Inference: 0.4536 s/iter. Eval: 0.1810 s/iter. Total: 0.6445 s/iter. ETA=0:05:17
[02/18 03:25:32] mask2former INFO: Inference done 609/1092. Dataloading: 0.0097 s/iter. Inference: 0.4534 s/iter. Eval: 0.1809 s/iter. Total: 0.6441 s/iter. ETA=0:05:11
[02/18 03:25:37] mask2former INFO: Inference done 618/1092. Dataloading: 0.0096 s/iter. Inference: 0.4537 s/iter. Eval: 0.1803 s/iter. Total: 0.6438 s/iter. ETA=0:05:05
[02/18 03:25:42] mask2former INFO: Inference done 626/1092. Dataloading: 0.0096 s/iter. Inference: 0.4535 s/iter. Eval: 0.1805 s/iter. Total: 0.6438 s/iter. ETA=0:05:00
[02/18 03:25:48] mask2former INFO: Inference done 634/1092. Dataloading: 0.0096 s/iter. Inference: 0.4535 s/iter. Eval: 0.1810 s/iter. Total: 0.6443 s/iter. ETA=0:04:55
[02/18 03:25:53] mask2former INFO: Inference done 642/1092. Dataloading: 0.0096 s/iter. Inference: 0.4535 s/iter. Eval: 0.1808 s/iter. Total: 0.6441 s/iter. ETA=0:04:49
[02/18 03:25:58] mask2former INFO: Inference done 649/1092. Dataloading: 0.0096 s/iter. Inference: 0.4539 s/iter. Eval: 0.1812 s/iter. Total: 0.6449 s/iter. ETA=0:04:45
[02/18 03:26:03] mask2former INFO: Inference done 657/1092. Dataloading: 0.0096 s/iter. Inference: 0.4538 s/iter. Eval: 0.1812 s/iter. Total: 0.6448 s/iter. ETA=0:04:40
[02/18 03:26:09] mask2former INFO: Inference done 666/1092. Dataloading: 0.0096 s/iter. Inference: 0.4535 s/iter. Eval: 0.1813 s/iter. Total: 0.6446 s/iter. ETA=0:04:34
[02/18 03:26:14] mask2former INFO: Inference done 674/1092. Dataloading: 0.0096 s/iter. Inference: 0.4536 s/iter. Eval: 0.1813 s/iter. Total: 0.6446 s/iter. ETA=0:04:29
[02/18 03:26:19] mask2former INFO: Inference done 683/1092. Dataloading: 0.0096 s/iter. Inference: 0.4533 s/iter. Eval: 0.1811 s/iter. Total: 0.6443 s/iter. ETA=0:04:23
[02/18 03:26:25] mask2former INFO: Inference done 692/1092. Dataloading: 0.0096 s/iter. Inference: 0.4529 s/iter. Eval: 0.1811 s/iter. Total: 0.6439 s/iter. ETA=0:04:17
[02/18 03:26:30] mask2former INFO: Inference done 701/1092. Dataloading: 0.0096 s/iter. Inference: 0.4527 s/iter. Eval: 0.1808 s/iter. Total: 0.6433 s/iter. ETA=0:04:11
[02/18 03:26:36] mask2former INFO: Inference done 709/1092. Dataloading: 0.0096 s/iter. Inference: 0.4527 s/iter. Eval: 0.1810 s/iter. Total: 0.6437 s/iter. ETA=0:04:06
[02/18 03:26:41] mask2former INFO: Inference done 717/1092. Dataloading: 0.0096 s/iter. Inference: 0.4529 s/iter. Eval: 0.1814 s/iter. Total: 0.6442 s/iter. ETA=0:04:01
[02/18 03:26:47] mask2former INFO: Inference done 725/1092. Dataloading: 0.0097 s/iter. Inference: 0.4530 s/iter. Eval: 0.1819 s/iter. Total: 0.6449 s/iter. ETA=0:03:56
[02/18 03:26:52] mask2former INFO: Inference done 733/1092. Dataloading: 0.0097 s/iter. Inference: 0.4532 s/iter. Eval: 0.1821 s/iter. Total: 0.6453 s/iter. ETA=0:03:51
[02/18 03:26:58] mask2former INFO: Inference done 742/1092. Dataloading: 0.0097 s/iter. Inference: 0.4530 s/iter. Eval: 0.1819 s/iter. Total: 0.6449 s/iter. ETA=0:03:45
[02/18 03:27:03] mask2former INFO: Inference done 750/1092. Dataloading: 0.0096 s/iter. Inference: 0.4528 s/iter. Eval: 0.1821 s/iter. Total: 0.6448 s/iter. ETA=0:03:40
[02/18 03:27:08] mask2former INFO: Inference done 758/1092. Dataloading: 0.0096 s/iter. Inference: 0.4526 s/iter. Eval: 0.1821 s/iter. Total: 0.6447 s/iter. ETA=0:03:35
[02/18 03:27:13] mask2former INFO: Inference done 767/1092. Dataloading: 0.0097 s/iter. Inference: 0.4522 s/iter. Eval: 0.1818 s/iter. Total: 0.6440 s/iter. ETA=0:03:29
[02/18 03:27:18] mask2former INFO: Inference done 776/1092. Dataloading: 0.0097 s/iter. Inference: 0.4520 s/iter. Eval: 0.1813 s/iter. Total: 0.6433 s/iter. ETA=0:03:23
[02/18 03:27:24] mask2former INFO: Inference done 784/1092. Dataloading: 0.0099 s/iter. Inference: 0.4520 s/iter. Eval: 0.1813 s/iter. Total: 0.6436 s/iter. ETA=0:03:18
[02/18 03:27:29] mask2former INFO: Inference done 793/1092. Dataloading: 0.0099 s/iter. Inference: 0.4518 s/iter. Eval: 0.1811 s/iter. Total: 0.6430 s/iter. ETA=0:03:12
[02/18 03:27:35] mask2former INFO: Inference done 802/1092. Dataloading: 0.0099 s/iter. Inference: 0.4517 s/iter. Eval: 0.1810 s/iter. Total: 0.6429 s/iter. ETA=0:03:06
[02/18 03:27:40] mask2former INFO: Inference done 810/1092. Dataloading: 0.0098 s/iter. Inference: 0.4517 s/iter. Eval: 0.1810 s/iter. Total: 0.6428 s/iter. ETA=0:03:01
[02/18 03:27:46] mask2former INFO: Inference done 819/1092. Dataloading: 0.0098 s/iter. Inference: 0.4513 s/iter. Eval: 0.1813 s/iter. Total: 0.6427 s/iter. ETA=0:02:55
[02/18 03:27:51] mask2former INFO: Inference done 828/1092. Dataloading: 0.0098 s/iter. Inference: 0.4512 s/iter. Eval: 0.1808 s/iter. Total: 0.6421 s/iter. ETA=0:02:49
[02/18 03:27:56] mask2former INFO: Inference done 836/1092. Dataloading: 0.0099 s/iter. Inference: 0.4515 s/iter. Eval: 0.1806 s/iter. Total: 0.6422 s/iter. ETA=0:02:44
[02/18 03:28:01] mask2former INFO: Inference done 844/1092. Dataloading: 0.0099 s/iter. Inference: 0.4515 s/iter. Eval: 0.1806 s/iter. Total: 0.6422 s/iter. ETA=0:02:39
[02/18 03:28:07] mask2former INFO: Inference done 852/1092. Dataloading: 0.0098 s/iter. Inference: 0.4514 s/iter. Eval: 0.1808 s/iter. Total: 0.6423 s/iter. ETA=0:02:34
[02/18 03:28:12] mask2former INFO: Inference done 861/1092. Dataloading: 0.0098 s/iter. Inference: 0.4511 s/iter. Eval: 0.1806 s/iter. Total: 0.6418 s/iter. ETA=0:02:28
[02/18 03:28:18] mask2former INFO: Inference done 870/1092. Dataloading: 0.0098 s/iter. Inference: 0.4512 s/iter. Eval: 0.1808 s/iter. Total: 0.6421 s/iter. ETA=0:02:22
[02/18 03:28:23] mask2former INFO: Inference done 878/1092. Dataloading: 0.0099 s/iter. Inference: 0.4512 s/iter. Eval: 0.1808 s/iter. Total: 0.6422 s/iter. ETA=0:02:17
[02/18 03:28:28] mask2former INFO: Inference done 887/1092. Dataloading: 0.0099 s/iter. Inference: 0.4510 s/iter. Eval: 0.1805 s/iter. Total: 0.6416 s/iter. ETA=0:02:11
[02/18 03:28:34] mask2former INFO: Inference done 895/1092. Dataloading: 0.0099 s/iter. Inference: 0.4513 s/iter. Eval: 0.1806 s/iter. Total: 0.6420 s/iter. ETA=0:02:06
[02/18 03:28:39] mask2former INFO: Inference done 903/1092. Dataloading: 0.0099 s/iter. Inference: 0.4515 s/iter. Eval: 0.1804 s/iter. Total: 0.6421 s/iter. ETA=0:02:01
[02/18 03:28:44] mask2former INFO: Inference done 911/1092. Dataloading: 0.0099 s/iter. Inference: 0.4513 s/iter. Eval: 0.1807 s/iter. Total: 0.6422 s/iter. ETA=0:01:56
[02/18 03:28:49] mask2former INFO: Inference done 919/1092. Dataloading: 0.0099 s/iter. Inference: 0.4514 s/iter. Eval: 0.1806 s/iter. Total: 0.6422 s/iter. ETA=0:01:51
[02/18 03:28:55] mask2former INFO: Inference done 927/1092. Dataloading: 0.0099 s/iter. Inference: 0.4515 s/iter. Eval: 0.1807 s/iter. Total: 0.6424 s/iter. ETA=0:01:46
[02/18 03:29:00] mask2former INFO: Inference done 935/1092. Dataloading: 0.0099 s/iter. Inference: 0.4513 s/iter. Eval: 0.1813 s/iter. Total: 0.6428 s/iter. ETA=0:01:40
[02/18 03:29:06] mask2former INFO: Inference done 943/1092. Dataloading: 0.0099 s/iter. Inference: 0.4515 s/iter. Eval: 0.1815 s/iter. Total: 0.6431 s/iter. ETA=0:01:35
[02/18 03:29:11] mask2former INFO: Inference done 952/1092. Dataloading: 0.0099 s/iter. Inference: 0.4513 s/iter. Eval: 0.1813 s/iter. Total: 0.6428 s/iter. ETA=0:01:29
[02/18 03:29:16] mask2former INFO: Inference done 959/1092. Dataloading: 0.0099 s/iter. Inference: 0.4516 s/iter. Eval: 0.1816 s/iter. Total: 0.6435 s/iter. ETA=0:01:25
[02/18 03:29:21] mask2former INFO: Inference done 967/1092. Dataloading: 0.0100 s/iter. Inference: 0.4518 s/iter. Eval: 0.1815 s/iter. Total: 0.6435 s/iter. ETA=0:01:20
[02/18 03:29:27] mask2former INFO: Inference done 975/1092. Dataloading: 0.0099 s/iter. Inference: 0.4517 s/iter. Eval: 0.1816 s/iter. Total: 0.6435 s/iter. ETA=0:01:15
[02/18 03:29:32] mask2former INFO: Inference done 983/1092. Dataloading: 0.0099 s/iter. Inference: 0.4518 s/iter. Eval: 0.1816 s/iter. Total: 0.6436 s/iter. ETA=0:01:10
[02/18 03:29:37] mask2former INFO: Inference done 991/1092. Dataloading: 0.0099 s/iter. Inference: 0.4521 s/iter. Eval: 0.1814 s/iter. Total: 0.6437 s/iter. ETA=0:01:05
[02/18 03:29:42] mask2former INFO: Inference done 999/1092. Dataloading: 0.0100 s/iter. Inference: 0.4521 s/iter. Eval: 0.1813 s/iter. Total: 0.6436 s/iter. ETA=0:00:59
[02/18 03:29:48] mask2former INFO: Inference done 1007/1092. Dataloading: 0.0100 s/iter. Inference: 0.4522 s/iter. Eval: 0.1815 s/iter. Total: 0.6439 s/iter. ETA=0:00:54
[02/18 03:29:53] mask2former INFO: Inference done 1015/1092. Dataloading: 0.0100 s/iter. Inference: 0.4521 s/iter. Eval: 0.1814 s/iter. Total: 0.6438 s/iter. ETA=0:00:49
[02/18 03:29:58] mask2former INFO: Inference done 1023/1092. Dataloading: 0.0100 s/iter. Inference: 0.4520 s/iter. Eval: 0.1817 s/iter. Total: 0.6440 s/iter. ETA=0:00:44
[02/18 03:30:03] mask2former INFO: Inference done 1032/1092. Dataloading: 0.0100 s/iter. Inference: 0.4518 s/iter. Eval: 0.1814 s/iter. Total: 0.6435 s/iter. ETA=0:00:38
[02/18 03:30:09] mask2former INFO: Inference done 1040/1092. Dataloading: 0.0100 s/iter. Inference: 0.4520 s/iter. Eval: 0.1814 s/iter. Total: 0.6437 s/iter. ETA=0:00:33
[02/18 03:30:14] mask2former INFO: Inference done 1049/1092. Dataloading: 0.0100 s/iter. Inference: 0.4516 s/iter. Eval: 0.1812 s/iter. Total: 0.6431 s/iter. ETA=0:00:27
[02/18 03:30:19] mask2former INFO: Inference done 1059/1092. Dataloading: 0.0100 s/iter. Inference: 0.4512 s/iter. Eval: 0.1808 s/iter. Total: 0.6422 s/iter. ETA=0:00:21
[02/18 03:30:25] mask2former INFO: Inference done 1067/1092. Dataloading: 0.0099 s/iter. Inference: 0.4513 s/iter. Eval: 0.1810 s/iter. Total: 0.6425 s/iter. ETA=0:00:16
[02/18 03:30:30] mask2former INFO: Inference done 1075/1092. Dataloading: 0.0099 s/iter. Inference: 0.4514 s/iter. Eval: 0.1812 s/iter. Total: 0.6428 s/iter. ETA=0:00:10
[02/18 03:30:36] mask2former INFO: Inference done 1083/1092. Dataloading: 0.0100 s/iter. Inference: 0.4515 s/iter. Eval: 0.1815 s/iter. Total: 0.6433 s/iter. ETA=0:00:05
[02/18 03:30:41] mask2former INFO: Inference done 1089/1092. Dataloading: 0.0100 s/iter. Inference: 0.4522 s/iter. Eval: 0.1822 s/iter. Total: 0.6447 s/iter. ETA=0:00:01
[02/18 04:42:56] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 04:42:57] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 04:42:57] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 04:43:11] mask2former INFO: Inference done 11/1092. Dataloading: 0.0054 s/iter. Inference: 0.2364 s/iter. Eval: 0.1193 s/iter. Total: 0.3611 s/iter. ETA=0:06:30
[02/18 04:43:16] mask2former INFO: Inference done 25/1092. Dataloading: 0.0051 s/iter. Inference: 0.2399 s/iter. Eval: 0.1159 s/iter. Total: 0.3609 s/iter. ETA=0:06:25
[02/18 04:43:21] mask2former INFO: Inference done 38/1092. Dataloading: 0.0054 s/iter. Inference: 0.2427 s/iter. Eval: 0.1242 s/iter. Total: 0.3724 s/iter. ETA=0:06:32
[02/18 04:43:27] mask2former INFO: Inference done 52/1092. Dataloading: 0.0058 s/iter. Inference: 0.2463 s/iter. Eval: 0.1201 s/iter. Total: 0.3723 s/iter. ETA=0:06:27
[02/18 04:43:32] mask2former INFO: Inference done 66/1092. Dataloading: 0.0059 s/iter. Inference: 0.2452 s/iter. Eval: 0.1204 s/iter. Total: 0.3716 s/iter. ETA=0:06:21
[02/18 04:43:37] mask2former INFO: Inference done 80/1092. Dataloading: 0.0058 s/iter. Inference: 0.2451 s/iter. Eval: 0.1203 s/iter. Total: 0.3714 s/iter. ETA=0:06:15
[02/18 04:43:42] mask2former INFO: Inference done 94/1092. Dataloading: 0.0062 s/iter. Inference: 0.2420 s/iter. Eval: 0.1215 s/iter. Total: 0.3698 s/iter. ETA=0:06:09
[02/18 04:43:47] mask2former INFO: Inference done 107/1092. Dataloading: 0.0062 s/iter. Inference: 0.2431 s/iter. Eval: 0.1224 s/iter. Total: 0.3718 s/iter. ETA=0:06:06
[02/18 04:43:52] mask2former INFO: Inference done 121/1092. Dataloading: 0.0065 s/iter. Inference: 0.2422 s/iter. Eval: 0.1221 s/iter. Total: 0.3708 s/iter. ETA=0:06:00
[02/18 04:43:57] mask2former INFO: Inference done 134/1092. Dataloading: 0.0072 s/iter. Inference: 0.2437 s/iter. Eval: 0.1217 s/iter. Total: 0.3727 s/iter. ETA=0:05:57
[02/18 04:44:02] mask2former INFO: Inference done 148/1092. Dataloading: 0.0071 s/iter. Inference: 0.2440 s/iter. Eval: 0.1217 s/iter. Total: 0.3729 s/iter. ETA=0:05:52
[02/18 04:44:08] mask2former INFO: Inference done 162/1092. Dataloading: 0.0070 s/iter. Inference: 0.2436 s/iter. Eval: 0.1221 s/iter. Total: 0.3728 s/iter. ETA=0:05:46
[02/18 04:44:13] mask2former INFO: Inference done 176/1092. Dataloading: 0.0069 s/iter. Inference: 0.2430 s/iter. Eval: 0.1225 s/iter. Total: 0.3725 s/iter. ETA=0:05:41
[02/18 04:44:18] mask2former INFO: Inference done 189/1092. Dataloading: 0.0069 s/iter. Inference: 0.2438 s/iter. Eval: 0.1234 s/iter. Total: 0.3743 s/iter. ETA=0:05:37
[02/18 04:44:23] mask2former INFO: Inference done 203/1092. Dataloading: 0.0069 s/iter. Inference: 0.2431 s/iter. Eval: 0.1239 s/iter. Total: 0.3740 s/iter. ETA=0:05:32
[02/18 04:44:28] mask2former INFO: Inference done 217/1092. Dataloading: 0.0068 s/iter. Inference: 0.2433 s/iter. Eval: 0.1230 s/iter. Total: 0.3731 s/iter. ETA=0:05:26
[02/18 04:44:33] mask2former INFO: Inference done 231/1092. Dataloading: 0.0067 s/iter. Inference: 0.2427 s/iter. Eval: 0.1233 s/iter. Total: 0.3728 s/iter. ETA=0:05:20
[02/18 04:44:39] mask2former INFO: Inference done 244/1092. Dataloading: 0.0068 s/iter. Inference: 0.2432 s/iter. Eval: 0.1241 s/iter. Total: 0.3741 s/iter. ETA=0:05:17
[02/18 04:44:44] mask2former INFO: Inference done 259/1092. Dataloading: 0.0067 s/iter. Inference: 0.2423 s/iter. Eval: 0.1235 s/iter. Total: 0.3726 s/iter. ETA=0:05:10
[02/18 04:44:49] mask2former INFO: Inference done 273/1092. Dataloading: 0.0066 s/iter. Inference: 0.2424 s/iter. Eval: 0.1229 s/iter. Total: 0.3721 s/iter. ETA=0:05:04
[02/18 04:44:54] mask2former INFO: Inference done 287/1092. Dataloading: 0.0067 s/iter. Inference: 0.2423 s/iter. Eval: 0.1227 s/iter. Total: 0.3718 s/iter. ETA=0:04:59
[02/18 04:44:59] mask2former INFO: Inference done 300/1092. Dataloading: 0.0066 s/iter. Inference: 0.2434 s/iter. Eval: 0.1230 s/iter. Total: 0.3732 s/iter. ETA=0:04:55
[02/18 04:45:04] mask2former INFO: Inference done 314/1092. Dataloading: 0.0066 s/iter. Inference: 0.2435 s/iter. Eval: 0.1229 s/iter. Total: 0.3731 s/iter. ETA=0:04:50
[02/18 04:45:10] mask2former INFO: Inference done 329/1092. Dataloading: 0.0066 s/iter. Inference: 0.2429 s/iter. Eval: 0.1224 s/iter. Total: 0.3719 s/iter. ETA=0:04:43
[02/18 04:45:15] mask2former INFO: Inference done 343/1092. Dataloading: 0.0066 s/iter. Inference: 0.2431 s/iter. Eval: 0.1225 s/iter. Total: 0.3722 s/iter. ETA=0:04:38
[02/18 04:45:20] mask2former INFO: Inference done 357/1092. Dataloading: 0.0065 s/iter. Inference: 0.2428 s/iter. Eval: 0.1226 s/iter. Total: 0.3719 s/iter. ETA=0:04:33
[02/18 04:45:25] mask2former INFO: Inference done 370/1092. Dataloading: 0.0065 s/iter. Inference: 0.2434 s/iter. Eval: 0.1225 s/iter. Total: 0.3724 s/iter. ETA=0:04:28
[02/18 04:45:30] mask2former INFO: Inference done 385/1092. Dataloading: 0.0065 s/iter. Inference: 0.2434 s/iter. Eval: 0.1219 s/iter. Total: 0.3719 s/iter. ETA=0:04:22
[02/18 04:45:35] mask2former INFO: Inference done 399/1092. Dataloading: 0.0064 s/iter. Inference: 0.2427 s/iter. Eval: 0.1222 s/iter. Total: 0.3714 s/iter. ETA=0:04:17
[02/18 04:45:41] mask2former INFO: Inference done 414/1092. Dataloading: 0.0065 s/iter. Inference: 0.2424 s/iter. Eval: 0.1218 s/iter. Total: 0.3707 s/iter. ETA=0:04:11
[02/18 04:45:46] mask2former INFO: Inference done 428/1092. Dataloading: 0.0064 s/iter. Inference: 0.2428 s/iter. Eval: 0.1216 s/iter. Total: 0.3709 s/iter. ETA=0:04:06
[02/18 04:45:51] mask2former INFO: Inference done 443/1092. Dataloading: 0.0064 s/iter. Inference: 0.2428 s/iter. Eval: 0.1210 s/iter. Total: 0.3703 s/iter. ETA=0:04:00
[02/18 04:45:57] mask2former INFO: Inference done 457/1092. Dataloading: 0.0064 s/iter. Inference: 0.2432 s/iter. Eval: 0.1209 s/iter. Total: 0.3706 s/iter. ETA=0:03:55
[02/18 04:46:02] mask2former INFO: Inference done 471/1092. Dataloading: 0.0064 s/iter. Inference: 0.2432 s/iter. Eval: 0.1208 s/iter. Total: 0.3704 s/iter. ETA=0:03:50
[02/18 04:46:07] mask2former INFO: Inference done 484/1092. Dataloading: 0.0063 s/iter. Inference: 0.2434 s/iter. Eval: 0.1210 s/iter. Total: 0.3708 s/iter. ETA=0:03:45
[02/18 04:46:12] mask2former INFO: Inference done 498/1092. Dataloading: 0.0064 s/iter. Inference: 0.2436 s/iter. Eval: 0.1206 s/iter. Total: 0.3707 s/iter. ETA=0:03:40
[02/18 04:46:17] mask2former INFO: Inference done 512/1092. Dataloading: 0.0064 s/iter. Inference: 0.2437 s/iter. Eval: 0.1206 s/iter. Total: 0.3707 s/iter. ETA=0:03:35
[02/18 04:46:22] mask2former INFO: Inference done 526/1092. Dataloading: 0.0064 s/iter. Inference: 0.2436 s/iter. Eval: 0.1209 s/iter. Total: 0.3710 s/iter. ETA=0:03:29
[02/18 04:46:28] mask2former INFO: Inference done 540/1092. Dataloading: 0.0064 s/iter. Inference: 0.2435 s/iter. Eval: 0.1209 s/iter. Total: 0.3709 s/iter. ETA=0:03:24
[02/18 04:46:33] mask2former INFO: Inference done 554/1092. Dataloading: 0.0064 s/iter. Inference: 0.2434 s/iter. Eval: 0.1210 s/iter. Total: 0.3709 s/iter. ETA=0:03:19
[02/18 04:46:38] mask2former INFO: Inference done 568/1092. Dataloading: 0.0064 s/iter. Inference: 0.2432 s/iter. Eval: 0.1212 s/iter. Total: 0.3708 s/iter. ETA=0:03:14
[02/18 04:46:43] mask2former INFO: Inference done 582/1092. Dataloading: 0.0064 s/iter. Inference: 0.2432 s/iter. Eval: 0.1212 s/iter. Total: 0.3709 s/iter. ETA=0:03:09
[02/18 04:46:48] mask2former INFO: Inference done 596/1092. Dataloading: 0.0064 s/iter. Inference: 0.2433 s/iter. Eval: 0.1209 s/iter. Total: 0.3706 s/iter. ETA=0:03:03
[02/18 04:46:53] mask2former INFO: Inference done 611/1092. Dataloading: 0.0064 s/iter. Inference: 0.2430 s/iter. Eval: 0.1207 s/iter. Total: 0.3701 s/iter. ETA=0:02:58
[02/18 04:46:58] mask2former INFO: Inference done 625/1092. Dataloading: 0.0064 s/iter. Inference: 0.2428 s/iter. Eval: 0.1206 s/iter. Total: 0.3698 s/iter. ETA=0:02:52
[02/18 04:47:04] mask2former INFO: Inference done 639/1092. Dataloading: 0.0063 s/iter. Inference: 0.2427 s/iter. Eval: 0.1206 s/iter. Total: 0.3697 s/iter. ETA=0:02:47
[02/18 04:47:09] mask2former INFO: Inference done 653/1092. Dataloading: 0.0063 s/iter. Inference: 0.2429 s/iter. Eval: 0.1206 s/iter. Total: 0.3699 s/iter. ETA=0:02:42
[02/18 04:47:14] mask2former INFO: Inference done 667/1092. Dataloading: 0.0063 s/iter. Inference: 0.2430 s/iter. Eval: 0.1203 s/iter. Total: 0.3698 s/iter. ETA=0:02:37
[02/18 04:47:19] mask2former INFO: Inference done 681/1092. Dataloading: 0.0063 s/iter. Inference: 0.2435 s/iter. Eval: 0.1200 s/iter. Total: 0.3699 s/iter. ETA=0:02:32
[02/18 04:47:24] mask2former INFO: Inference done 695/1092. Dataloading: 0.0063 s/iter. Inference: 0.2433 s/iter. Eval: 0.1199 s/iter. Total: 0.3696 s/iter. ETA=0:02:26
[02/18 04:47:29] mask2former INFO: Inference done 709/1092. Dataloading: 0.0063 s/iter. Inference: 0.2436 s/iter. Eval: 0.1197 s/iter. Total: 0.3696 s/iter. ETA=0:02:21
[02/18 04:47:35] mask2former INFO: Inference done 724/1092. Dataloading: 0.0063 s/iter. Inference: 0.2433 s/iter. Eval: 0.1195 s/iter. Total: 0.3692 s/iter. ETA=0:02:15
[02/18 04:47:40] mask2former INFO: Inference done 738/1092. Dataloading: 0.0062 s/iter. Inference: 0.2435 s/iter. Eval: 0.1193 s/iter. Total: 0.3692 s/iter. ETA=0:02:10
[02/18 04:47:45] mask2former INFO: Inference done 752/1092. Dataloading: 0.0063 s/iter. Inference: 0.2435 s/iter. Eval: 0.1194 s/iter. Total: 0.3692 s/iter. ETA=0:02:05
[02/18 04:47:50] mask2former INFO: Inference done 767/1092. Dataloading: 0.0063 s/iter. Inference: 0.2431 s/iter. Eval: 0.1194 s/iter. Total: 0.3688 s/iter. ETA=0:01:59
[02/18 04:47:55] mask2former INFO: Inference done 781/1092. Dataloading: 0.0063 s/iter. Inference: 0.2433 s/iter. Eval: 0.1192 s/iter. Total: 0.3689 s/iter. ETA=0:01:54
[02/18 04:48:01] mask2former INFO: Inference done 796/1092. Dataloading: 0.0063 s/iter. Inference: 0.2436 s/iter. Eval: 0.1188 s/iter. Total: 0.3688 s/iter. ETA=0:01:49
[02/18 04:48:06] mask2former INFO: Inference done 810/1092. Dataloading: 0.0062 s/iter. Inference: 0.2435 s/iter. Eval: 0.1189 s/iter. Total: 0.3687 s/iter. ETA=0:01:43
[02/18 04:48:11] mask2former INFO: Inference done 824/1092. Dataloading: 0.0062 s/iter. Inference: 0.2436 s/iter. Eval: 0.1189 s/iter. Total: 0.3689 s/iter. ETA=0:01:38
[02/18 04:48:16] mask2former INFO: Inference done 838/1092. Dataloading: 0.0063 s/iter. Inference: 0.2436 s/iter. Eval: 0.1187 s/iter. Total: 0.3687 s/iter. ETA=0:01:33
[02/18 04:48:21] mask2former INFO: Inference done 852/1092. Dataloading: 0.0062 s/iter. Inference: 0.2435 s/iter. Eval: 0.1188 s/iter. Total: 0.3686 s/iter. ETA=0:01:28
[02/18 04:48:26] mask2former INFO: Inference done 866/1092. Dataloading: 0.0062 s/iter. Inference: 0.2433 s/iter. Eval: 0.1189 s/iter. Total: 0.3685 s/iter. ETA=0:01:23
[02/18 04:48:31] mask2former INFO: Inference done 879/1092. Dataloading: 0.0062 s/iter. Inference: 0.2434 s/iter. Eval: 0.1190 s/iter. Total: 0.3687 s/iter. ETA=0:01:18
[02/18 04:48:37] mask2former INFO: Inference done 893/1092. Dataloading: 0.0062 s/iter. Inference: 0.2437 s/iter. Eval: 0.1189 s/iter. Total: 0.3690 s/iter. ETA=0:01:13
[02/18 04:48:42] mask2former INFO: Inference done 906/1092. Dataloading: 0.0062 s/iter. Inference: 0.2440 s/iter. Eval: 0.1189 s/iter. Total: 0.3693 s/iter. ETA=0:01:08
[02/18 04:48:47] mask2former INFO: Inference done 919/1092. Dataloading: 0.0062 s/iter. Inference: 0.2443 s/iter. Eval: 0.1189 s/iter. Total: 0.3696 s/iter. ETA=0:01:03
[02/18 04:48:52] mask2former INFO: Inference done 933/1092. Dataloading: 0.0062 s/iter. Inference: 0.2444 s/iter. Eval: 0.1187 s/iter. Total: 0.3694 s/iter. ETA=0:00:58
[02/18 04:48:57] mask2former INFO: Inference done 947/1092. Dataloading: 0.0062 s/iter. Inference: 0.2445 s/iter. Eval: 0.1184 s/iter. Total: 0.3693 s/iter. ETA=0:00:53
[02/18 04:49:02] mask2former INFO: Inference done 961/1092. Dataloading: 0.0062 s/iter. Inference: 0.2444 s/iter. Eval: 0.1186 s/iter. Total: 0.3693 s/iter. ETA=0:00:48
[02/18 04:49:08] mask2former INFO: Inference done 974/1092. Dataloading: 0.0063 s/iter. Inference: 0.2449 s/iter. Eval: 0.1189 s/iter. Total: 0.3702 s/iter. ETA=0:00:43
[02/18 04:49:13] mask2former INFO: Inference done 980/1092. Dataloading: 0.0063 s/iter. Inference: 0.2464 s/iter. Eval: 0.1203 s/iter. Total: 0.3731 s/iter. ETA=0:00:41
[02/18 04:49:18] mask2former INFO: Inference done 987/1092. Dataloading: 0.0065 s/iter. Inference: 0.2477 s/iter. Eval: 0.1214 s/iter. Total: 0.3757 s/iter. ETA=0:00:39
[02/18 04:49:23] mask2former INFO: Inference done 999/1092. Dataloading: 0.0065 s/iter. Inference: 0.2484 s/iter. Eval: 0.1214 s/iter. Total: 0.3764 s/iter. ETA=0:00:35
[02/18 04:49:29] mask2former INFO: Inference done 1008/1092. Dataloading: 0.0066 s/iter. Inference: 0.2489 s/iter. Eval: 0.1226 s/iter. Total: 0.3783 s/iter. ETA=0:00:31
[02/18 04:49:34] mask2former INFO: Inference done 1019/1092. Dataloading: 0.0067 s/iter. Inference: 0.2496 s/iter. Eval: 0.1228 s/iter. Total: 0.3791 s/iter. ETA=0:00:27
[02/18 04:49:39] mask2former INFO: Inference done 1034/1092. Dataloading: 0.0067 s/iter. Inference: 0.2494 s/iter. Eval: 0.1225 s/iter. Total: 0.3787 s/iter. ETA=0:00:21
[02/18 04:49:44] mask2former INFO: Inference done 1049/1092. Dataloading: 0.0067 s/iter. Inference: 0.2489 s/iter. Eval: 0.1225 s/iter. Total: 0.3782 s/iter. ETA=0:00:16
[02/18 04:49:49] mask2former INFO: Inference done 1062/1092. Dataloading: 0.0067 s/iter. Inference: 0.2492 s/iter. Eval: 0.1225 s/iter. Total: 0.3784 s/iter. ETA=0:00:11
[02/18 04:49:54] mask2former INFO: Inference done 1076/1092. Dataloading: 0.0066 s/iter. Inference: 0.2491 s/iter. Eval: 0.1223 s/iter. Total: 0.3782 s/iter. ETA=0:00:06
[02/18 04:49:59] mask2former INFO: Inference done 1090/1092. Dataloading: 0.0066 s/iter. Inference: 0.2491 s/iter. Eval: 0.1221 s/iter. Total: 0.3779 s/iter. ETA=0:00:00
[02/18 05:59:23] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 05:59:23] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 05:59:23] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 05:59:38] mask2former INFO: Inference done 11/1092. Dataloading: 0.0051 s/iter. Inference: 0.2104 s/iter. Eval: 0.1194 s/iter. Total: 0.3350 s/iter. ETA=0:06:02
[02/18 05:59:43] mask2former INFO: Inference done 25/1092. Dataloading: 0.0048 s/iter. Inference: 0.2304 s/iter. Eval: 0.1179 s/iter. Total: 0.3532 s/iter. ETA=0:06:16
[02/18 05:59:48] mask2former INFO: Inference done 40/1092. Dataloading: 0.0050 s/iter. Inference: 0.2283 s/iter. Eval: 0.1122 s/iter. Total: 0.3455 s/iter. ETA=0:06:03
[02/18 05:59:53] mask2former INFO: Inference done 55/1092. Dataloading: 0.0050 s/iter. Inference: 0.2312 s/iter. Eval: 0.1090 s/iter. Total: 0.3453 s/iter. ETA=0:05:58
[02/18 05:59:58] mask2former INFO: Inference done 69/1092. Dataloading: 0.0051 s/iter. Inference: 0.2325 s/iter. Eval: 0.1115 s/iter. Total: 0.3493 s/iter. ETA=0:05:57
[02/18 06:00:03] mask2former INFO: Inference done 83/1092. Dataloading: 0.0051 s/iter. Inference: 0.2317 s/iter. Eval: 0.1141 s/iter. Total: 0.3511 s/iter. ETA=0:05:54
[02/18 06:00:08] mask2former INFO: Inference done 97/1092. Dataloading: 0.0052 s/iter. Inference: 0.2328 s/iter. Eval: 0.1146 s/iter. Total: 0.3528 s/iter. ETA=0:05:51
[02/18 06:00:14] mask2former INFO: Inference done 112/1092. Dataloading: 0.0052 s/iter. Inference: 0.2323 s/iter. Eval: 0.1156 s/iter. Total: 0.3534 s/iter. ETA=0:05:46
[02/18 06:00:19] mask2former INFO: Inference done 127/1092. Dataloading: 0.0052 s/iter. Inference: 0.2331 s/iter. Eval: 0.1140 s/iter. Total: 0.3526 s/iter. ETA=0:05:40
[02/18 06:00:24] mask2former INFO: Inference done 142/1092. Dataloading: 0.0053 s/iter. Inference: 0.2325 s/iter. Eval: 0.1152 s/iter. Total: 0.3532 s/iter. ETA=0:05:35
[02/18 06:00:29] mask2former INFO: Inference done 157/1092. Dataloading: 0.0052 s/iter. Inference: 0.2315 s/iter. Eval: 0.1154 s/iter. Total: 0.3522 s/iter. ETA=0:05:29
[02/18 06:00:34] mask2former INFO: Inference done 171/1092. Dataloading: 0.0052 s/iter. Inference: 0.2325 s/iter. Eval: 0.1155 s/iter. Total: 0.3534 s/iter. ETA=0:05:25
[02/18 06:00:40] mask2former INFO: Inference done 186/1092. Dataloading: 0.0052 s/iter. Inference: 0.2326 s/iter. Eval: 0.1154 s/iter. Total: 0.3533 s/iter. ETA=0:05:20
[02/18 06:00:45] mask2former INFO: Inference done 201/1092. Dataloading: 0.0053 s/iter. Inference: 0.2324 s/iter. Eval: 0.1152 s/iter. Total: 0.3530 s/iter. ETA=0:05:14
[02/18 06:00:50] mask2former INFO: Inference done 216/1092. Dataloading: 0.0052 s/iter. Inference: 0.2327 s/iter. Eval: 0.1145 s/iter. Total: 0.3526 s/iter. ETA=0:05:08
[02/18 06:00:56] mask2former INFO: Inference done 231/1092. Dataloading: 0.0052 s/iter. Inference: 0.2328 s/iter. Eval: 0.1147 s/iter. Total: 0.3529 s/iter. ETA=0:05:03
[02/18 06:01:01] mask2former INFO: Inference done 245/1092. Dataloading: 0.0053 s/iter. Inference: 0.2336 s/iter. Eval: 0.1155 s/iter. Total: 0.3545 s/iter. ETA=0:05:00
[02/18 06:01:06] mask2former INFO: Inference done 260/1092. Dataloading: 0.0053 s/iter. Inference: 0.2328 s/iter. Eval: 0.1157 s/iter. Total: 0.3539 s/iter. ETA=0:04:54
[02/18 06:01:11] mask2former INFO: Inference done 275/1092. Dataloading: 0.0053 s/iter. Inference: 0.2324 s/iter. Eval: 0.1155 s/iter. Total: 0.3533 s/iter. ETA=0:04:48
[02/18 06:01:16] mask2former INFO: Inference done 290/1092. Dataloading: 0.0053 s/iter. Inference: 0.2328 s/iter. Eval: 0.1145 s/iter. Total: 0.3526 s/iter. ETA=0:04:42
[02/18 06:01:21] mask2former INFO: Inference done 305/1092. Dataloading: 0.0053 s/iter. Inference: 0.2324 s/iter. Eval: 0.1144 s/iter. Total: 0.3522 s/iter. ETA=0:04:37
[02/18 06:01:27] mask2former INFO: Inference done 319/1092. Dataloading: 0.0053 s/iter. Inference: 0.2338 s/iter. Eval: 0.1142 s/iter. Total: 0.3534 s/iter. ETA=0:04:33
[02/18 06:01:32] mask2former INFO: Inference done 333/1092. Dataloading: 0.0052 s/iter. Inference: 0.2345 s/iter. Eval: 0.1139 s/iter. Total: 0.3537 s/iter. ETA=0:04:28
[02/18 06:01:37] mask2former INFO: Inference done 348/1092. Dataloading: 0.0052 s/iter. Inference: 0.2342 s/iter. Eval: 0.1135 s/iter. Total: 0.3531 s/iter. ETA=0:04:22
[02/18 06:01:42] mask2former INFO: Inference done 363/1092. Dataloading: 0.0052 s/iter. Inference: 0.2342 s/iter. Eval: 0.1131 s/iter. Total: 0.3527 s/iter. ETA=0:04:17
[02/18 06:01:47] mask2former INFO: Inference done 377/1092. Dataloading: 0.0053 s/iter. Inference: 0.2345 s/iter. Eval: 0.1131 s/iter. Total: 0.3530 s/iter. ETA=0:04:12
[02/18 06:01:52] mask2former INFO: Inference done 391/1092. Dataloading: 0.0052 s/iter. Inference: 0.2350 s/iter. Eval: 0.1136 s/iter. Total: 0.3540 s/iter. ETA=0:04:08
[02/18 06:01:58] mask2former INFO: Inference done 406/1092. Dataloading: 0.0052 s/iter. Inference: 0.2348 s/iter. Eval: 0.1135 s/iter. Total: 0.3537 s/iter. ETA=0:04:02
[02/18 06:02:03] mask2former INFO: Inference done 421/1092. Dataloading: 0.0052 s/iter. Inference: 0.2347 s/iter. Eval: 0.1129 s/iter. Total: 0.3530 s/iter. ETA=0:03:56
[02/18 06:02:08] mask2former INFO: Inference done 437/1092. Dataloading: 0.0052 s/iter. Inference: 0.2344 s/iter. Eval: 0.1125 s/iter. Total: 0.3522 s/iter. ETA=0:03:50
[02/18 06:02:13] mask2former INFO: Inference done 450/1092. Dataloading: 0.0052 s/iter. Inference: 0.2348 s/iter. Eval: 0.1136 s/iter. Total: 0.3537 s/iter. ETA=0:03:47
[02/18 06:02:18] mask2former INFO: Inference done 457/1092. Dataloading: 0.0053 s/iter. Inference: 0.2383 s/iter. Eval: 0.1160 s/iter. Total: 0.3598 s/iter. ETA=0:03:48
[02/18 06:02:24] mask2former INFO: Inference done 463/1092. Dataloading: 0.0054 s/iter. Inference: 0.2423 s/iter. Eval: 0.1190 s/iter. Total: 0.3668 s/iter. ETA=0:03:50
[02/18 06:02:29] mask2former INFO: Inference done 476/1092. Dataloading: 0.0054 s/iter. Inference: 0.2429 s/iter. Eval: 0.1192 s/iter. Total: 0.3676 s/iter. ETA=0:03:46
[02/18 06:02:34] mask2former INFO: Inference done 486/1092. Dataloading: 0.0054 s/iter. Inference: 0.2446 s/iter. Eval: 0.1202 s/iter. Total: 0.3704 s/iter. ETA=0:03:44
[02/18 06:02:39] mask2former INFO: Inference done 499/1092. Dataloading: 0.0055 s/iter. Inference: 0.2451 s/iter. Eval: 0.1204 s/iter. Total: 0.3711 s/iter. ETA=0:03:40
[02/18 06:02:44] mask2former INFO: Inference done 513/1092. Dataloading: 0.0055 s/iter. Inference: 0.2451 s/iter. Eval: 0.1207 s/iter. Total: 0.3713 s/iter. ETA=0:03:34
[02/18 06:02:49] mask2former INFO: Inference done 527/1092. Dataloading: 0.0055 s/iter. Inference: 0.2450 s/iter. Eval: 0.1204 s/iter. Total: 0.3710 s/iter. ETA=0:03:29
[02/18 06:02:55] mask2former INFO: Inference done 542/1092. Dataloading: 0.0055 s/iter. Inference: 0.2448 s/iter. Eval: 0.1202 s/iter. Total: 0.3706 s/iter. ETA=0:03:23
[02/18 06:03:00] mask2former INFO: Inference done 556/1092. Dataloading: 0.0054 s/iter. Inference: 0.2451 s/iter. Eval: 0.1198 s/iter. Total: 0.3705 s/iter. ETA=0:03:18
[02/18 06:03:05] mask2former INFO: Inference done 570/1092. Dataloading: 0.0054 s/iter. Inference: 0.2449 s/iter. Eval: 0.1199 s/iter. Total: 0.3703 s/iter. ETA=0:03:13
[02/18 06:03:10] mask2former INFO: Inference done 585/1092. Dataloading: 0.0054 s/iter. Inference: 0.2446 s/iter. Eval: 0.1197 s/iter. Total: 0.3698 s/iter. ETA=0:03:07
[02/18 06:03:15] mask2former INFO: Inference done 598/1092. Dataloading: 0.0054 s/iter. Inference: 0.2451 s/iter. Eval: 0.1197 s/iter. Total: 0.3703 s/iter. ETA=0:03:02
[02/18 06:03:21] mask2former INFO: Inference done 612/1092. Dataloading: 0.0055 s/iter. Inference: 0.2451 s/iter. Eval: 0.1197 s/iter. Total: 0.3703 s/iter. ETA=0:02:57
[02/18 06:03:26] mask2former INFO: Inference done 627/1092. Dataloading: 0.0055 s/iter. Inference: 0.2446 s/iter. Eval: 0.1200 s/iter. Total: 0.3701 s/iter. ETA=0:02:52
[02/18 06:03:31] mask2former INFO: Inference done 641/1092. Dataloading: 0.0054 s/iter. Inference: 0.2444 s/iter. Eval: 0.1202 s/iter. Total: 0.3702 s/iter. ETA=0:02:46
[02/18 06:03:37] mask2former INFO: Inference done 655/1092. Dataloading: 0.0054 s/iter. Inference: 0.2444 s/iter. Eval: 0.1204 s/iter. Total: 0.3704 s/iter. ETA=0:02:41
[02/18 06:03:42] mask2former INFO: Inference done 670/1092. Dataloading: 0.0055 s/iter. Inference: 0.2443 s/iter. Eval: 0.1202 s/iter. Total: 0.3700 s/iter. ETA=0:02:36
[02/18 06:03:47] mask2former INFO: Inference done 684/1092. Dataloading: 0.0055 s/iter. Inference: 0.2444 s/iter. Eval: 0.1203 s/iter. Total: 0.3704 s/iter. ETA=0:02:31
[02/18 06:03:53] mask2former INFO: Inference done 698/1092. Dataloading: 0.0055 s/iter. Inference: 0.2444 s/iter. Eval: 0.1206 s/iter. Total: 0.3705 s/iter. ETA=0:02:25
[02/18 06:03:58] mask2former INFO: Inference done 712/1092. Dataloading: 0.0055 s/iter. Inference: 0.2445 s/iter. Eval: 0.1203 s/iter. Total: 0.3704 s/iter. ETA=0:02:20
[02/18 06:04:03] mask2former INFO: Inference done 727/1092. Dataloading: 0.0055 s/iter. Inference: 0.2441 s/iter. Eval: 0.1201 s/iter. Total: 0.3698 s/iter. ETA=0:02:14
[02/18 06:04:08] mask2former INFO: Inference done 742/1092. Dataloading: 0.0055 s/iter. Inference: 0.2438 s/iter. Eval: 0.1198 s/iter. Total: 0.3692 s/iter. ETA=0:02:09
[02/18 06:04:13] mask2former INFO: Inference done 756/1092. Dataloading: 0.0055 s/iter. Inference: 0.2437 s/iter. Eval: 0.1198 s/iter. Total: 0.3691 s/iter. ETA=0:02:04
[02/18 06:04:18] mask2former INFO: Inference done 771/1092. Dataloading: 0.0055 s/iter. Inference: 0.2434 s/iter. Eval: 0.1197 s/iter. Total: 0.3687 s/iter. ETA=0:01:58
[02/18 06:04:23] mask2former INFO: Inference done 785/1092. Dataloading: 0.0055 s/iter. Inference: 0.2434 s/iter. Eval: 0.1196 s/iter. Total: 0.3685 s/iter. ETA=0:01:53
[02/18 06:04:28] mask2former INFO: Inference done 800/1092. Dataloading: 0.0055 s/iter. Inference: 0.2433 s/iter. Eval: 0.1190 s/iter. Total: 0.3679 s/iter. ETA=0:01:47
[02/18 06:04:34] mask2former INFO: Inference done 815/1092. Dataloading: 0.0055 s/iter. Inference: 0.2431 s/iter. Eval: 0.1189 s/iter. Total: 0.3676 s/iter. ETA=0:01:41
[02/18 06:04:39] mask2former INFO: Inference done 830/1092. Dataloading: 0.0055 s/iter. Inference: 0.2431 s/iter. Eval: 0.1187 s/iter. Total: 0.3673 s/iter. ETA=0:01:36
[02/18 06:04:44] mask2former INFO: Inference done 844/1092. Dataloading: 0.0055 s/iter. Inference: 0.2430 s/iter. Eval: 0.1189 s/iter. Total: 0.3674 s/iter. ETA=0:01:31
[02/18 06:04:49] mask2former INFO: Inference done 858/1092. Dataloading: 0.0055 s/iter. Inference: 0.2429 s/iter. Eval: 0.1190 s/iter. Total: 0.3674 s/iter. ETA=0:01:25
[02/18 06:04:54] mask2former INFO: Inference done 872/1092. Dataloading: 0.0055 s/iter. Inference: 0.2429 s/iter. Eval: 0.1191 s/iter. Total: 0.3675 s/iter. ETA=0:01:20
[02/18 06:05:00] mask2former INFO: Inference done 887/1092. Dataloading: 0.0055 s/iter. Inference: 0.2425 s/iter. Eval: 0.1190 s/iter. Total: 0.3670 s/iter. ETA=0:01:15
[02/18 06:05:05] mask2former INFO: Inference done 901/1092. Dataloading: 0.0055 s/iter. Inference: 0.2425 s/iter. Eval: 0.1189 s/iter. Total: 0.3669 s/iter. ETA=0:01:10
[02/18 06:05:10] mask2former INFO: Inference done 915/1092. Dataloading: 0.0055 s/iter. Inference: 0.2425 s/iter. Eval: 0.1188 s/iter. Total: 0.3668 s/iter. ETA=0:01:04
[02/18 06:05:15] mask2former INFO: Inference done 929/1092. Dataloading: 0.0055 s/iter. Inference: 0.2423 s/iter. Eval: 0.1189 s/iter. Total: 0.3667 s/iter. ETA=0:00:59
[02/18 06:05:20] mask2former INFO: Inference done 944/1092. Dataloading: 0.0055 s/iter. Inference: 0.2421 s/iter. Eval: 0.1187 s/iter. Total: 0.3664 s/iter. ETA=0:00:54
[02/18 06:05:25] mask2former INFO: Inference done 959/1092. Dataloading: 0.0055 s/iter. Inference: 0.2420 s/iter. Eval: 0.1186 s/iter. Total: 0.3662 s/iter. ETA=0:00:48
[02/18 06:05:30] mask2former INFO: Inference done 974/1092. Dataloading: 0.0055 s/iter. Inference: 0.2417 s/iter. Eval: 0.1184 s/iter. Total: 0.3657 s/iter. ETA=0:00:43
[02/18 06:05:36] mask2former INFO: Inference done 989/1092. Dataloading: 0.0055 s/iter. Inference: 0.2417 s/iter. Eval: 0.1182 s/iter. Total: 0.3655 s/iter. ETA=0:00:37
[02/18 06:05:41] mask2former INFO: Inference done 1004/1092. Dataloading: 0.0055 s/iter. Inference: 0.2415 s/iter. Eval: 0.1182 s/iter. Total: 0.3652 s/iter. ETA=0:00:32
[02/18 06:05:46] mask2former INFO: Inference done 1018/1092. Dataloading: 0.0055 s/iter. Inference: 0.2415 s/iter. Eval: 0.1182 s/iter. Total: 0.3653 s/iter. ETA=0:00:27
[02/18 06:05:51] mask2former INFO: Inference done 1033/1092. Dataloading: 0.0055 s/iter. Inference: 0.2413 s/iter. Eval: 0.1181 s/iter. Total: 0.3650 s/iter. ETA=0:00:21
[02/18 06:05:56] mask2former INFO: Inference done 1048/1092. Dataloading: 0.0055 s/iter. Inference: 0.2410 s/iter. Eval: 0.1181 s/iter. Total: 0.3646 s/iter. ETA=0:00:16
[02/18 06:06:01] mask2former INFO: Inference done 1063/1092. Dataloading: 0.0055 s/iter. Inference: 0.2409 s/iter. Eval: 0.1180 s/iter. Total: 0.3644 s/iter. ETA=0:00:10
[02/18 06:06:06] mask2former INFO: Inference done 1078/1092. Dataloading: 0.0055 s/iter. Inference: 0.2408 s/iter. Eval: 0.1177 s/iter. Total: 0.3641 s/iter. ETA=0:00:05
[02/18 07:14:46] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 07:14:47] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 07:14:47] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 07:15:00] mask2former INFO: Inference done 11/1092. Dataloading: 0.0043 s/iter. Inference: 0.2408 s/iter. Eval: 0.1123 s/iter. Total: 0.3574 s/iter. ETA=0:06:26
[02/18 07:15:05] mask2former INFO: Inference done 26/1092. Dataloading: 0.0046 s/iter. Inference: 0.2330 s/iter. Eval: 0.1035 s/iter. Total: 0.3411 s/iter. ETA=0:06:03
[02/18 07:15:10] mask2former INFO: Inference done 37/1092. Dataloading: 0.0059 s/iter. Inference: 0.2523 s/iter. Eval: 0.1306 s/iter. Total: 0.3889 s/iter. ETA=0:06:50
[02/18 07:15:15] mask2former INFO: Inference done 44/1092. Dataloading: 0.0073 s/iter. Inference: 0.2852 s/iter. Eval: 0.1606 s/iter. Total: 0.4531 s/iter. ETA=0:07:54
[02/18 07:15:20] mask2former INFO: Inference done 51/1092. Dataloading: 0.0088 s/iter. Inference: 0.3151 s/iter. Eval: 0.1714 s/iter. Total: 0.4954 s/iter. ETA=0:08:35
[02/18 07:15:26] mask2former INFO: Inference done 65/1092. Dataloading: 0.0082 s/iter. Inference: 0.2999 s/iter. Eval: 0.1620 s/iter. Total: 0.4703 s/iter. ETA=0:08:02
[02/18 07:15:31] mask2former INFO: Inference done 75/1092. Dataloading: 0.0088 s/iter. Inference: 0.2982 s/iter. Eval: 0.1692 s/iter. Total: 0.4764 s/iter. ETA=0:08:04
[02/18 07:15:36] mask2former INFO: Inference done 89/1092. Dataloading: 0.0083 s/iter. Inference: 0.2897 s/iter. Eval: 0.1614 s/iter. Total: 0.4595 s/iter. ETA=0:07:40
[02/18 07:15:41] mask2former INFO: Inference done 104/1092. Dataloading: 0.0077 s/iter. Inference: 0.2795 s/iter. Eval: 0.1547 s/iter. Total: 0.4420 s/iter. ETA=0:07:16
[02/18 07:15:47] mask2former INFO: Inference done 119/1092. Dataloading: 0.0074 s/iter. Inference: 0.2731 s/iter. Eval: 0.1500 s/iter. Total: 0.4305 s/iter. ETA=0:06:58
[02/18 07:15:52] mask2former INFO: Inference done 133/1092. Dataloading: 0.0074 s/iter. Inference: 0.2697 s/iter. Eval: 0.1461 s/iter. Total: 0.4233 s/iter. ETA=0:06:45
[02/18 07:15:57] mask2former INFO: Inference done 148/1092. Dataloading: 0.0071 s/iter. Inference: 0.2650 s/iter. Eval: 0.1435 s/iter. Total: 0.4157 s/iter. ETA=0:06:32
[02/18 07:16:02] mask2former INFO: Inference done 162/1092. Dataloading: 0.0070 s/iter. Inference: 0.2626 s/iter. Eval: 0.1411 s/iter. Total: 0.4109 s/iter. ETA=0:06:22
[02/18 07:16:08] mask2former INFO: Inference done 176/1092. Dataloading: 0.0070 s/iter. Inference: 0.2608 s/iter. Eval: 0.1404 s/iter. Total: 0.4082 s/iter. ETA=0:06:13
[02/18 07:16:13] mask2former INFO: Inference done 190/1092. Dataloading: 0.0068 s/iter. Inference: 0.2590 s/iter. Eval: 0.1390 s/iter. Total: 0.4049 s/iter. ETA=0:06:05
[02/18 07:16:18] mask2former INFO: Inference done 204/1092. Dataloading: 0.0067 s/iter. Inference: 0.2573 s/iter. Eval: 0.1380 s/iter. Total: 0.4022 s/iter. ETA=0:05:57
[02/18 07:16:23] mask2former INFO: Inference done 218/1092. Dataloading: 0.0066 s/iter. Inference: 0.2566 s/iter. Eval: 0.1366 s/iter. Total: 0.3999 s/iter. ETA=0:05:49
[02/18 07:16:28] mask2former INFO: Inference done 232/1092. Dataloading: 0.0066 s/iter. Inference: 0.2556 s/iter. Eval: 0.1358 s/iter. Total: 0.3980 s/iter. ETA=0:05:42
[02/18 07:16:33] mask2former INFO: Inference done 246/1092. Dataloading: 0.0065 s/iter. Inference: 0.2545 s/iter. Eval: 0.1358 s/iter. Total: 0.3969 s/iter. ETA=0:05:35
[02/18 07:16:39] mask2former INFO: Inference done 260/1092. Dataloading: 0.0065 s/iter. Inference: 0.2538 s/iter. Eval: 0.1350 s/iter. Total: 0.3954 s/iter. ETA=0:05:28
[02/18 07:16:44] mask2former INFO: Inference done 274/1092. Dataloading: 0.0065 s/iter. Inference: 0.2527 s/iter. Eval: 0.1343 s/iter. Total: 0.3936 s/iter. ETA=0:05:21
[02/18 07:16:49] mask2former INFO: Inference done 288/1092. Dataloading: 0.0065 s/iter. Inference: 0.2517 s/iter. Eval: 0.1339 s/iter. Total: 0.3922 s/iter. ETA=0:05:15
[02/18 07:16:54] mask2former INFO: Inference done 303/1092. Dataloading: 0.0064 s/iter. Inference: 0.2506 s/iter. Eval: 0.1329 s/iter. Total: 0.3900 s/iter. ETA=0:05:07
[02/18 07:16:59] mask2former INFO: Inference done 318/1092. Dataloading: 0.0063 s/iter. Inference: 0.2498 s/iter. Eval: 0.1320 s/iter. Total: 0.3882 s/iter. ETA=0:05:00
[02/18 07:17:04] mask2former INFO: Inference done 332/1092. Dataloading: 0.0063 s/iter. Inference: 0.2497 s/iter. Eval: 0.1310 s/iter. Total: 0.3871 s/iter. ETA=0:04:54
[02/18 07:17:09] mask2former INFO: Inference done 346/1092. Dataloading: 0.0062 s/iter. Inference: 0.2493 s/iter. Eval: 0.1303 s/iter. Total: 0.3859 s/iter. ETA=0:04:47
[02/18 07:17:15] mask2former INFO: Inference done 359/1092. Dataloading: 0.0063 s/iter. Inference: 0.2501 s/iter. Eval: 0.1301 s/iter. Total: 0.3865 s/iter. ETA=0:04:43
[02/18 07:17:20] mask2former INFO: Inference done 374/1092. Dataloading: 0.0062 s/iter. Inference: 0.2498 s/iter. Eval: 0.1290 s/iter. Total: 0.3850 s/iter. ETA=0:04:36
[02/18 07:17:25] mask2former INFO: Inference done 388/1092. Dataloading: 0.0062 s/iter. Inference: 0.2492 s/iter. Eval: 0.1290 s/iter. Total: 0.3845 s/iter. ETA=0:04:30
[02/18 07:17:30] mask2former INFO: Inference done 402/1092. Dataloading: 0.0062 s/iter. Inference: 0.2487 s/iter. Eval: 0.1290 s/iter. Total: 0.3839 s/iter. ETA=0:04:24
[02/18 07:17:35] mask2former INFO: Inference done 416/1092. Dataloading: 0.0062 s/iter. Inference: 0.2485 s/iter. Eval: 0.1284 s/iter. Total: 0.3831 s/iter. ETA=0:04:19
[02/18 07:17:40] mask2former INFO: Inference done 431/1092. Dataloading: 0.0062 s/iter. Inference: 0.2481 s/iter. Eval: 0.1277 s/iter. Total: 0.3820 s/iter. ETA=0:04:12
[02/18 07:17:46] mask2former INFO: Inference done 446/1092. Dataloading: 0.0061 s/iter. Inference: 0.2478 s/iter. Eval: 0.1268 s/iter. Total: 0.3808 s/iter. ETA=0:04:06
[02/18 07:17:51] mask2former INFO: Inference done 461/1092. Dataloading: 0.0061 s/iter. Inference: 0.2472 s/iter. Eval: 0.1259 s/iter. Total: 0.3793 s/iter. ETA=0:03:59
[02/18 07:17:56] mask2former INFO: Inference done 475/1092. Dataloading: 0.0061 s/iter. Inference: 0.2471 s/iter. Eval: 0.1257 s/iter. Total: 0.3790 s/iter. ETA=0:03:53
[02/18 07:18:01] mask2former INFO: Inference done 489/1092. Dataloading: 0.0060 s/iter. Inference: 0.2468 s/iter. Eval: 0.1258 s/iter. Total: 0.3788 s/iter. ETA=0:03:48
[02/18 07:18:06] mask2former INFO: Inference done 504/1092. Dataloading: 0.0060 s/iter. Inference: 0.2463 s/iter. Eval: 0.1254 s/iter. Total: 0.3778 s/iter. ETA=0:03:42
[02/18 07:18:12] mask2former INFO: Inference done 517/1092. Dataloading: 0.0060 s/iter. Inference: 0.2467 s/iter. Eval: 0.1258 s/iter. Total: 0.3786 s/iter. ETA=0:03:37
[02/18 07:18:17] mask2former INFO: Inference done 532/1092. Dataloading: 0.0060 s/iter. Inference: 0.2467 s/iter. Eval: 0.1252 s/iter. Total: 0.3779 s/iter. ETA=0:03:31
[02/18 07:18:22] mask2former INFO: Inference done 547/1092. Dataloading: 0.0059 s/iter. Inference: 0.2464 s/iter. Eval: 0.1249 s/iter. Total: 0.3774 s/iter. ETA=0:03:25
[02/18 07:18:27] mask2former INFO: Inference done 560/1092. Dataloading: 0.0059 s/iter. Inference: 0.2465 s/iter. Eval: 0.1252 s/iter. Total: 0.3778 s/iter. ETA=0:03:20
[02/18 07:18:32] mask2former INFO: Inference done 574/1092. Dataloading: 0.0060 s/iter. Inference: 0.2464 s/iter. Eval: 0.1248 s/iter. Total: 0.3773 s/iter. ETA=0:03:15
[02/18 07:18:38] mask2former INFO: Inference done 589/1092. Dataloading: 0.0060 s/iter. Inference: 0.2462 s/iter. Eval: 0.1245 s/iter. Total: 0.3768 s/iter. ETA=0:03:09
[02/18 07:18:43] mask2former INFO: Inference done 603/1092. Dataloading: 0.0060 s/iter. Inference: 0.2460 s/iter. Eval: 0.1244 s/iter. Total: 0.3765 s/iter. ETA=0:03:04
[02/18 07:18:48] mask2former INFO: Inference done 618/1092. Dataloading: 0.0059 s/iter. Inference: 0.2460 s/iter. Eval: 0.1240 s/iter. Total: 0.3760 s/iter. ETA=0:02:58
[02/18 07:18:53] mask2former INFO: Inference done 633/1092. Dataloading: 0.0059 s/iter. Inference: 0.2458 s/iter. Eval: 0.1234 s/iter. Total: 0.3753 s/iter. ETA=0:02:52
[02/18 07:18:59] mask2former INFO: Inference done 647/1092. Dataloading: 0.0060 s/iter. Inference: 0.2457 s/iter. Eval: 0.1233 s/iter. Total: 0.3751 s/iter. ETA=0:02:46
[02/18 07:19:04] mask2former INFO: Inference done 662/1092. Dataloading: 0.0059 s/iter. Inference: 0.2457 s/iter. Eval: 0.1230 s/iter. Total: 0.3747 s/iter. ETA=0:02:41
[02/18 07:19:09] mask2former INFO: Inference done 676/1092. Dataloading: 0.0060 s/iter. Inference: 0.2457 s/iter. Eval: 0.1229 s/iter. Total: 0.3746 s/iter. ETA=0:02:35
[02/18 07:19:14] mask2former INFO: Inference done 690/1092. Dataloading: 0.0060 s/iter. Inference: 0.2454 s/iter. Eval: 0.1228 s/iter. Total: 0.3743 s/iter. ETA=0:02:30
[02/18 07:19:19] mask2former INFO: Inference done 704/1092. Dataloading: 0.0059 s/iter. Inference: 0.2453 s/iter. Eval: 0.1229 s/iter. Total: 0.3742 s/iter. ETA=0:02:25
[02/18 07:19:24] mask2former INFO: Inference done 718/1092. Dataloading: 0.0059 s/iter. Inference: 0.2451 s/iter. Eval: 0.1229 s/iter. Total: 0.3740 s/iter. ETA=0:02:19
[02/18 07:19:29] mask2former INFO: Inference done 732/1092. Dataloading: 0.0059 s/iter. Inference: 0.2449 s/iter. Eval: 0.1228 s/iter. Total: 0.3737 s/iter. ETA=0:02:14
[02/18 07:19:34] mask2former INFO: Inference done 747/1092. Dataloading: 0.0059 s/iter. Inference: 0.2447 s/iter. Eval: 0.1223 s/iter. Total: 0.3730 s/iter. ETA=0:02:08
[02/18 07:19:40] mask2former INFO: Inference done 761/1092. Dataloading: 0.0059 s/iter. Inference: 0.2446 s/iter. Eval: 0.1224 s/iter. Total: 0.3729 s/iter. ETA=0:02:03
[02/18 07:19:45] mask2former INFO: Inference done 776/1092. Dataloading: 0.0059 s/iter. Inference: 0.2444 s/iter. Eval: 0.1222 s/iter. Total: 0.3726 s/iter. ETA=0:01:57
[02/18 07:19:50] mask2former INFO: Inference done 791/1092. Dataloading: 0.0059 s/iter. Inference: 0.2442 s/iter. Eval: 0.1219 s/iter. Total: 0.3720 s/iter. ETA=0:01:51
[02/18 07:19:55] mask2former INFO: Inference done 806/1092. Dataloading: 0.0059 s/iter. Inference: 0.2441 s/iter. Eval: 0.1216 s/iter. Total: 0.3717 s/iter. ETA=0:01:46
[02/18 07:20:01] mask2former INFO: Inference done 821/1092. Dataloading: 0.0058 s/iter. Inference: 0.2439 s/iter. Eval: 0.1213 s/iter. Total: 0.3711 s/iter. ETA=0:01:40
[02/18 07:20:06] mask2former INFO: Inference done 836/1092. Dataloading: 0.0058 s/iter. Inference: 0.2437 s/iter. Eval: 0.1212 s/iter. Total: 0.3707 s/iter. ETA=0:01:34
[02/18 07:20:11] mask2former INFO: Inference done 850/1092. Dataloading: 0.0058 s/iter. Inference: 0.2435 s/iter. Eval: 0.1212 s/iter. Total: 0.3706 s/iter. ETA=0:01:29
[02/18 07:20:16] mask2former INFO: Inference done 863/1092. Dataloading: 0.0058 s/iter. Inference: 0.2435 s/iter. Eval: 0.1215 s/iter. Total: 0.3709 s/iter. ETA=0:01:24
[02/18 07:20:21] mask2former INFO: Inference done 878/1092. Dataloading: 0.0058 s/iter. Inference: 0.2431 s/iter. Eval: 0.1214 s/iter. Total: 0.3703 s/iter. ETA=0:01:19
[02/18 07:20:26] mask2former INFO: Inference done 892/1092. Dataloading: 0.0058 s/iter. Inference: 0.2429 s/iter. Eval: 0.1213 s/iter. Total: 0.3701 s/iter. ETA=0:01:14
[02/18 07:20:31] mask2former INFO: Inference done 906/1092. Dataloading: 0.0058 s/iter. Inference: 0.2428 s/iter. Eval: 0.1213 s/iter. Total: 0.3700 s/iter. ETA=0:01:08
[02/18 07:20:36] mask2former INFO: Inference done 920/1092. Dataloading: 0.0058 s/iter. Inference: 0.2428 s/iter. Eval: 0.1214 s/iter. Total: 0.3701 s/iter. ETA=0:01:03
[02/18 07:20:41] mask2former INFO: Inference done 934/1092. Dataloading: 0.0057 s/iter. Inference: 0.2428 s/iter. Eval: 0.1214 s/iter. Total: 0.3700 s/iter. ETA=0:00:58
[02/18 07:20:47] mask2former INFO: Inference done 949/1092. Dataloading: 0.0057 s/iter. Inference: 0.2426 s/iter. Eval: 0.1213 s/iter. Total: 0.3698 s/iter. ETA=0:00:52
[02/18 07:20:52] mask2former INFO: Inference done 963/1092. Dataloading: 0.0057 s/iter. Inference: 0.2427 s/iter. Eval: 0.1213 s/iter. Total: 0.3698 s/iter. ETA=0:00:47
[02/18 07:20:57] mask2former INFO: Inference done 978/1092. Dataloading: 0.0057 s/iter. Inference: 0.2427 s/iter. Eval: 0.1210 s/iter. Total: 0.3695 s/iter. ETA=0:00:42
[02/18 07:21:02] mask2former INFO: Inference done 992/1092. Dataloading: 0.0057 s/iter. Inference: 0.2426 s/iter. Eval: 0.1212 s/iter. Total: 0.3696 s/iter. ETA=0:00:36
[02/18 07:21:08] mask2former INFO: Inference done 1006/1092. Dataloading: 0.0057 s/iter. Inference: 0.2425 s/iter. Eval: 0.1212 s/iter. Total: 0.3695 s/iter. ETA=0:00:31
[02/18 07:21:13] mask2former INFO: Inference done 1020/1092. Dataloading: 0.0057 s/iter. Inference: 0.2425 s/iter. Eval: 0.1211 s/iter. Total: 0.3694 s/iter. ETA=0:00:26
[02/18 07:21:18] mask2former INFO: Inference done 1035/1092. Dataloading: 0.0057 s/iter. Inference: 0.2425 s/iter. Eval: 0.1209 s/iter. Total: 0.3692 s/iter. ETA=0:00:21
[02/18 07:21:23] mask2former INFO: Inference done 1049/1092. Dataloading: 0.0057 s/iter. Inference: 0.2424 s/iter. Eval: 0.1210 s/iter. Total: 0.3691 s/iter. ETA=0:00:15
[02/18 07:21:28] mask2former INFO: Inference done 1063/1092. Dataloading: 0.0057 s/iter. Inference: 0.2424 s/iter. Eval: 0.1208 s/iter. Total: 0.3691 s/iter. ETA=0:00:10
[02/18 07:21:33] mask2former INFO: Inference done 1077/1092. Dataloading: 0.0057 s/iter. Inference: 0.2424 s/iter. Eval: 0.1208 s/iter. Total: 0.3690 s/iter. ETA=0:00:05
[02/18 07:21:38] mask2former INFO: Inference done 1092/1092. Dataloading: 0.0057 s/iter. Inference: 0.2422 s/iter. Eval: 0.1206 s/iter. Total: 0.3685 s/iter. ETA=0:00:00
[02/18 08:30:07] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 08:30:08] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 08:30:08] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 08:30:21] mask2former INFO: Inference done 11/1092. Dataloading: 0.0034 s/iter. Inference: 0.2537 s/iter. Eval: 0.1257 s/iter. Total: 0.3828 s/iter. ETA=0:06:53
[02/18 08:30:26] mask2former INFO: Inference done 25/1092. Dataloading: 0.0045 s/iter. Inference: 0.2383 s/iter. Eval: 0.1239 s/iter. Total: 0.3667 s/iter. ETA=0:06:31
[02/18 08:30:32] mask2former INFO: Inference done 40/1092. Dataloading: 0.0048 s/iter. Inference: 0.2378 s/iter. Eval: 0.1171 s/iter. Total: 0.3597 s/iter. ETA=0:06:18
[02/18 08:30:37] mask2former INFO: Inference done 55/1092. Dataloading: 0.0049 s/iter. Inference: 0.2394 s/iter. Eval: 0.1136 s/iter. Total: 0.3580 s/iter. ETA=0:06:11
[02/18 08:30:42] mask2former INFO: Inference done 70/1092. Dataloading: 0.0050 s/iter. Inference: 0.2355 s/iter. Eval: 0.1157 s/iter. Total: 0.3562 s/iter. ETA=0:06:04
[02/18 08:30:47] mask2former INFO: Inference done 85/1092. Dataloading: 0.0051 s/iter. Inference: 0.2357 s/iter. Eval: 0.1151 s/iter. Total: 0.3560 s/iter. ETA=0:05:58
[02/18 08:30:52] mask2former INFO: Inference done 99/1092. Dataloading: 0.0053 s/iter. Inference: 0.2361 s/iter. Eval: 0.1150 s/iter. Total: 0.3565 s/iter. ETA=0:05:54
[02/18 08:30:58] mask2former INFO: Inference done 114/1092. Dataloading: 0.0052 s/iter. Inference: 0.2349 s/iter. Eval: 0.1158 s/iter. Total: 0.3560 s/iter. ETA=0:05:48
[02/18 08:31:03] mask2former INFO: Inference done 128/1092. Dataloading: 0.0052 s/iter. Inference: 0.2352 s/iter. Eval: 0.1157 s/iter. Total: 0.3562 s/iter. ETA=0:05:43
[02/18 08:31:08] mask2former INFO: Inference done 142/1092. Dataloading: 0.0053 s/iter. Inference: 0.2358 s/iter. Eval: 0.1158 s/iter. Total: 0.3569 s/iter. ETA=0:05:39
[02/18 08:31:13] mask2former INFO: Inference done 156/1092. Dataloading: 0.0052 s/iter. Inference: 0.2356 s/iter. Eval: 0.1170 s/iter. Total: 0.3579 s/iter. ETA=0:05:34
[02/18 08:31:18] mask2former INFO: Inference done 169/1092. Dataloading: 0.0052 s/iter. Inference: 0.2369 s/iter. Eval: 0.1182 s/iter. Total: 0.3604 s/iter. ETA=0:05:32
[02/18 08:31:23] mask2former INFO: Inference done 182/1092. Dataloading: 0.0053 s/iter. Inference: 0.2379 s/iter. Eval: 0.1189 s/iter. Total: 0.3622 s/iter. ETA=0:05:29
[02/18 08:31:28] mask2former INFO: Inference done 196/1092. Dataloading: 0.0055 s/iter. Inference: 0.2391 s/iter. Eval: 0.1189 s/iter. Total: 0.3636 s/iter. ETA=0:05:25
[02/18 08:31:34] mask2former INFO: Inference done 211/1092. Dataloading: 0.0054 s/iter. Inference: 0.2386 s/iter. Eval: 0.1183 s/iter. Total: 0.3624 s/iter. ETA=0:05:19
[02/18 08:31:39] mask2former INFO: Inference done 226/1092. Dataloading: 0.0054 s/iter. Inference: 0.2388 s/iter. Eval: 0.1178 s/iter. Total: 0.3621 s/iter. ETA=0:05:13
[02/18 08:31:44] mask2former INFO: Inference done 240/1092. Dataloading: 0.0055 s/iter. Inference: 0.2384 s/iter. Eval: 0.1184 s/iter. Total: 0.3624 s/iter. ETA=0:05:08
[02/18 08:31:49] mask2former INFO: Inference done 254/1092. Dataloading: 0.0055 s/iter. Inference: 0.2382 s/iter. Eval: 0.1187 s/iter. Total: 0.3625 s/iter. ETA=0:05:03
[02/18 08:31:55] mask2former INFO: Inference done 268/1092. Dataloading: 0.0055 s/iter. Inference: 0.2388 s/iter. Eval: 0.1189 s/iter. Total: 0.3633 s/iter. ETA=0:04:59
[02/18 08:32:00] mask2former INFO: Inference done 282/1092. Dataloading: 0.0055 s/iter. Inference: 0.2391 s/iter. Eval: 0.1186 s/iter. Total: 0.3634 s/iter. ETA=0:04:54
[02/18 08:32:05] mask2former INFO: Inference done 297/1092. Dataloading: 0.0055 s/iter. Inference: 0.2384 s/iter. Eval: 0.1188 s/iter. Total: 0.3628 s/iter. ETA=0:04:48
[02/18 08:32:10] mask2former INFO: Inference done 311/1092. Dataloading: 0.0055 s/iter. Inference: 0.2387 s/iter. Eval: 0.1188 s/iter. Total: 0.3631 s/iter. ETA=0:04:43
[02/18 08:32:15] mask2former INFO: Inference done 325/1092. Dataloading: 0.0055 s/iter. Inference: 0.2386 s/iter. Eval: 0.1190 s/iter. Total: 0.3631 s/iter. ETA=0:04:38
[02/18 08:32:20] mask2former INFO: Inference done 339/1092. Dataloading: 0.0056 s/iter. Inference: 0.2390 s/iter. Eval: 0.1186 s/iter. Total: 0.3632 s/iter. ETA=0:04:33
[02/18 08:32:26] mask2former INFO: Inference done 353/1092. Dataloading: 0.0056 s/iter. Inference: 0.2394 s/iter. Eval: 0.1187 s/iter. Total: 0.3638 s/iter. ETA=0:04:28
[02/18 08:32:31] mask2former INFO: Inference done 367/1092. Dataloading: 0.0056 s/iter. Inference: 0.2396 s/iter. Eval: 0.1190 s/iter. Total: 0.3642 s/iter. ETA=0:04:24
[02/18 08:32:36] mask2former INFO: Inference done 382/1092. Dataloading: 0.0056 s/iter. Inference: 0.2394 s/iter. Eval: 0.1183 s/iter. Total: 0.3633 s/iter. ETA=0:04:17
[02/18 08:32:41] mask2former INFO: Inference done 396/1092. Dataloading: 0.0056 s/iter. Inference: 0.2394 s/iter. Eval: 0.1187 s/iter. Total: 0.3637 s/iter. ETA=0:04:13
[02/18 08:32:46] mask2former INFO: Inference done 410/1092. Dataloading: 0.0056 s/iter. Inference: 0.2398 s/iter. Eval: 0.1184 s/iter. Total: 0.3639 s/iter. ETA=0:04:08
[02/18 08:32:51] mask2former INFO: Inference done 424/1092. Dataloading: 0.0056 s/iter. Inference: 0.2399 s/iter. Eval: 0.1183 s/iter. Total: 0.3639 s/iter. ETA=0:04:03
[02/18 08:32:56] mask2former INFO: Inference done 438/1092. Dataloading: 0.0056 s/iter. Inference: 0.2400 s/iter. Eval: 0.1181 s/iter. Total: 0.3638 s/iter. ETA=0:03:57
[02/18 08:33:02] mask2former INFO: Inference done 453/1092. Dataloading: 0.0055 s/iter. Inference: 0.2397 s/iter. Eval: 0.1180 s/iter. Total: 0.3633 s/iter. ETA=0:03:52
[02/18 08:33:07] mask2former INFO: Inference done 467/1092. Dataloading: 0.0056 s/iter. Inference: 0.2400 s/iter. Eval: 0.1181 s/iter. Total: 0.3637 s/iter. ETA=0:03:47
[02/18 08:33:12] mask2former INFO: Inference done 481/1092. Dataloading: 0.0055 s/iter. Inference: 0.2400 s/iter. Eval: 0.1183 s/iter. Total: 0.3639 s/iter. ETA=0:03:42
[02/18 08:33:17] mask2former INFO: Inference done 496/1092. Dataloading: 0.0055 s/iter. Inference: 0.2402 s/iter. Eval: 0.1177 s/iter. Total: 0.3636 s/iter. ETA=0:03:36
[02/18 08:33:22] mask2former INFO: Inference done 509/1092. Dataloading: 0.0056 s/iter. Inference: 0.2404 s/iter. Eval: 0.1181 s/iter. Total: 0.3641 s/iter. ETA=0:03:32
[02/18 08:33:28] mask2former INFO: Inference done 523/1092. Dataloading: 0.0055 s/iter. Inference: 0.2403 s/iter. Eval: 0.1182 s/iter. Total: 0.3641 s/iter. ETA=0:03:27
[02/18 08:33:33] mask2former INFO: Inference done 537/1092. Dataloading: 0.0055 s/iter. Inference: 0.2405 s/iter. Eval: 0.1182 s/iter. Total: 0.3643 s/iter. ETA=0:03:22
[02/18 08:33:38] mask2former INFO: Inference done 552/1092. Dataloading: 0.0055 s/iter. Inference: 0.2402 s/iter. Eval: 0.1178 s/iter. Total: 0.3636 s/iter. ETA=0:03:16
[02/18 08:33:43] mask2former INFO: Inference done 567/1092. Dataloading: 0.0055 s/iter. Inference: 0.2401 s/iter. Eval: 0.1179 s/iter. Total: 0.3636 s/iter. ETA=0:03:10
[02/18 08:33:48] mask2former INFO: Inference done 581/1092. Dataloading: 0.0055 s/iter. Inference: 0.2401 s/iter. Eval: 0.1181 s/iter. Total: 0.3637 s/iter. ETA=0:03:05
[02/18 08:33:54] mask2former INFO: Inference done 595/1092. Dataloading: 0.0055 s/iter. Inference: 0.2400 s/iter. Eval: 0.1184 s/iter. Total: 0.3640 s/iter. ETA=0:03:00
[02/18 08:33:59] mask2former INFO: Inference done 609/1092. Dataloading: 0.0055 s/iter. Inference: 0.2402 s/iter. Eval: 0.1184 s/iter. Total: 0.3642 s/iter. ETA=0:02:55
[02/18 08:34:04] mask2former INFO: Inference done 623/1092. Dataloading: 0.0055 s/iter. Inference: 0.2405 s/iter. Eval: 0.1183 s/iter. Total: 0.3644 s/iter. ETA=0:02:50
[02/18 08:34:09] mask2former INFO: Inference done 637/1092. Dataloading: 0.0055 s/iter. Inference: 0.2405 s/iter. Eval: 0.1182 s/iter. Total: 0.3642 s/iter. ETA=0:02:45
[02/18 08:34:14] mask2former INFO: Inference done 652/1092. Dataloading: 0.0055 s/iter. Inference: 0.2403 s/iter. Eval: 0.1181 s/iter. Total: 0.3640 s/iter. ETA=0:02:40
[02/18 08:34:20] mask2former INFO: Inference done 666/1092. Dataloading: 0.0054 s/iter. Inference: 0.2403 s/iter. Eval: 0.1182 s/iter. Total: 0.3640 s/iter. ETA=0:02:35
[02/18 08:34:25] mask2former INFO: Inference done 680/1092. Dataloading: 0.0055 s/iter. Inference: 0.2404 s/iter. Eval: 0.1184 s/iter. Total: 0.3644 s/iter. ETA=0:02:30
[02/18 08:34:30] mask2former INFO: Inference done 694/1092. Dataloading: 0.0055 s/iter. Inference: 0.2405 s/iter. Eval: 0.1184 s/iter. Total: 0.3645 s/iter. ETA=0:02:25
[02/18 08:34:35] mask2former INFO: Inference done 709/1092. Dataloading: 0.0054 s/iter. Inference: 0.2402 s/iter. Eval: 0.1183 s/iter. Total: 0.3640 s/iter. ETA=0:02:19
[02/18 08:34:40] mask2former INFO: Inference done 723/1092. Dataloading: 0.0054 s/iter. Inference: 0.2399 s/iter. Eval: 0.1185 s/iter. Total: 0.3639 s/iter. ETA=0:02:14
[02/18 08:34:45] mask2former INFO: Inference done 738/1092. Dataloading: 0.0055 s/iter. Inference: 0.2399 s/iter. Eval: 0.1181 s/iter. Total: 0.3635 s/iter. ETA=0:02:08
[02/18 08:34:51] mask2former INFO: Inference done 753/1092. Dataloading: 0.0054 s/iter. Inference: 0.2397 s/iter. Eval: 0.1181 s/iter. Total: 0.3633 s/iter. ETA=0:02:03
[02/18 08:34:56] mask2former INFO: Inference done 767/1092. Dataloading: 0.0054 s/iter. Inference: 0.2397 s/iter. Eval: 0.1182 s/iter. Total: 0.3634 s/iter. ETA=0:01:58
[02/18 08:35:01] mask2former INFO: Inference done 781/1092. Dataloading: 0.0054 s/iter. Inference: 0.2397 s/iter. Eval: 0.1182 s/iter. Total: 0.3634 s/iter. ETA=0:01:53
[02/18 08:35:06] mask2former INFO: Inference done 796/1092. Dataloading: 0.0054 s/iter. Inference: 0.2396 s/iter. Eval: 0.1179 s/iter. Total: 0.3631 s/iter. ETA=0:01:47
[02/18 08:35:11] mask2former INFO: Inference done 810/1092. Dataloading: 0.0054 s/iter. Inference: 0.2394 s/iter. Eval: 0.1181 s/iter. Total: 0.3630 s/iter. ETA=0:01:42
[02/18 08:35:16] mask2former INFO: Inference done 825/1092. Dataloading: 0.0054 s/iter. Inference: 0.2392 s/iter. Eval: 0.1180 s/iter. Total: 0.3627 s/iter. ETA=0:01:36
[02/18 08:35:21] mask2former INFO: Inference done 839/1092. Dataloading: 0.0054 s/iter. Inference: 0.2392 s/iter. Eval: 0.1180 s/iter. Total: 0.3627 s/iter. ETA=0:01:31
[02/18 08:35:27] mask2former INFO: Inference done 854/1092. Dataloading: 0.0054 s/iter. Inference: 0.2391 s/iter. Eval: 0.1180 s/iter. Total: 0.3625 s/iter. ETA=0:01:26
[02/18 08:35:32] mask2former INFO: Inference done 868/1092. Dataloading: 0.0054 s/iter. Inference: 0.2390 s/iter. Eval: 0.1179 s/iter. Total: 0.3625 s/iter. ETA=0:01:21
[02/18 08:35:37] mask2former INFO: Inference done 883/1092. Dataloading: 0.0054 s/iter. Inference: 0.2390 s/iter. Eval: 0.1179 s/iter. Total: 0.3623 s/iter. ETA=0:01:15
[02/18 08:35:42] mask2former INFO: Inference done 897/1092. Dataloading: 0.0054 s/iter. Inference: 0.2390 s/iter. Eval: 0.1179 s/iter. Total: 0.3624 s/iter. ETA=0:01:10
[02/18 08:35:47] mask2former INFO: Inference done 911/1092. Dataloading: 0.0054 s/iter. Inference: 0.2390 s/iter. Eval: 0.1180 s/iter. Total: 0.3624 s/iter. ETA=0:01:05
[02/18 08:35:52] mask2former INFO: Inference done 925/1092. Dataloading: 0.0054 s/iter. Inference: 0.2389 s/iter. Eval: 0.1179 s/iter. Total: 0.3624 s/iter. ETA=0:01:00
[02/18 08:35:57] mask2former INFO: Inference done 939/1092. Dataloading: 0.0054 s/iter. Inference: 0.2388 s/iter. Eval: 0.1181 s/iter. Total: 0.3624 s/iter. ETA=0:00:55
[02/18 08:36:03] mask2former INFO: Inference done 954/1092. Dataloading: 0.0054 s/iter. Inference: 0.2387 s/iter. Eval: 0.1179 s/iter. Total: 0.3621 s/iter. ETA=0:00:49
[02/18 08:36:08] mask2former INFO: Inference done 969/1092. Dataloading: 0.0054 s/iter. Inference: 0.2384 s/iter. Eval: 0.1178 s/iter. Total: 0.3617 s/iter. ETA=0:00:44
[02/18 08:36:13] mask2former INFO: Inference done 984/1092. Dataloading: 0.0054 s/iter. Inference: 0.2383 s/iter. Eval: 0.1177 s/iter. Total: 0.3615 s/iter. ETA=0:00:39
[02/18 08:36:18] mask2former INFO: Inference done 998/1092. Dataloading: 0.0054 s/iter. Inference: 0.2384 s/iter. Eval: 0.1177 s/iter. Total: 0.3615 s/iter. ETA=0:00:33
[02/18 08:36:23] mask2former INFO: Inference done 1012/1092. Dataloading: 0.0054 s/iter. Inference: 0.2386 s/iter. Eval: 0.1177 s/iter. Total: 0.3618 s/iter. ETA=0:00:28
[02/18 08:36:29] mask2former INFO: Inference done 1028/1092. Dataloading: 0.0054 s/iter. Inference: 0.2384 s/iter. Eval: 0.1174 s/iter. Total: 0.3614 s/iter. ETA=0:00:23
[02/18 08:36:34] mask2former INFO: Inference done 1043/1092. Dataloading: 0.0054 s/iter. Inference: 0.2382 s/iter. Eval: 0.1174 s/iter. Total: 0.3611 s/iter. ETA=0:00:17
[02/18 08:36:39] mask2former INFO: Inference done 1057/1092. Dataloading: 0.0054 s/iter. Inference: 0.2381 s/iter. Eval: 0.1176 s/iter. Total: 0.3613 s/iter. ETA=0:00:12
[02/18 08:36:44] mask2former INFO: Inference done 1072/1092. Dataloading: 0.0054 s/iter. Inference: 0.2381 s/iter. Eval: 0.1176 s/iter. Total: 0.3612 s/iter. ETA=0:00:07
[02/18 08:36:49] mask2former INFO: Inference done 1087/1092. Dataloading: 0.0054 s/iter. Inference: 0.2379 s/iter. Eval: 0.1175 s/iter. Total: 0.3609 s/iter. ETA=0:00:01
[02/18 09:45:59] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 09:45:59] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 09:45:59] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 09:46:13] mask2former INFO: Inference done 11/1092. Dataloading: 0.0041 s/iter. Inference: 0.2505 s/iter. Eval: 0.1286 s/iter. Total: 0.3833 s/iter. ETA=0:06:54
[02/18 09:46:18] mask2former INFO: Inference done 26/1092. Dataloading: 0.0058 s/iter. Inference: 0.2373 s/iter. Eval: 0.1177 s/iter. Total: 0.3610 s/iter. ETA=0:06:24
[02/18 09:46:24] mask2former INFO: Inference done 40/1092. Dataloading: 0.0055 s/iter. Inference: 0.2417 s/iter. Eval: 0.1157 s/iter. Total: 0.3630 s/iter. ETA=0:06:21
[02/18 09:46:29] mask2former INFO: Inference done 55/1092. Dataloading: 0.0055 s/iter. Inference: 0.2388 s/iter. Eval: 0.1163 s/iter. Total: 0.3607 s/iter. ETA=0:06:13
[02/18 09:46:34] mask2former INFO: Inference done 69/1092. Dataloading: 0.0053 s/iter. Inference: 0.2388 s/iter. Eval: 0.1167 s/iter. Total: 0.3609 s/iter. ETA=0:06:09
[02/18 09:46:39] mask2former INFO: Inference done 83/1092. Dataloading: 0.0052 s/iter. Inference: 0.2399 s/iter. Eval: 0.1173 s/iter. Total: 0.3625 s/iter. ETA=0:06:05
[02/18 09:46:44] mask2former INFO: Inference done 97/1092. Dataloading: 0.0052 s/iter. Inference: 0.2406 s/iter. Eval: 0.1166 s/iter. Total: 0.3625 s/iter. ETA=0:06:00
[02/18 09:46:49] mask2former INFO: Inference done 112/1092. Dataloading: 0.0052 s/iter. Inference: 0.2386 s/iter. Eval: 0.1165 s/iter. Total: 0.3604 s/iter. ETA=0:05:53
[02/18 09:46:55] mask2former INFO: Inference done 127/1092. Dataloading: 0.0052 s/iter. Inference: 0.2378 s/iter. Eval: 0.1163 s/iter. Total: 0.3593 s/iter. ETA=0:05:46
[02/18 09:47:00] mask2former INFO: Inference done 141/1092. Dataloading: 0.0052 s/iter. Inference: 0.2379 s/iter. Eval: 0.1170 s/iter. Total: 0.3602 s/iter. ETA=0:05:42
[02/18 09:47:05] mask2former INFO: Inference done 156/1092. Dataloading: 0.0052 s/iter. Inference: 0.2375 s/iter. Eval: 0.1171 s/iter. Total: 0.3598 s/iter. ETA=0:05:36
[02/18 09:47:10] mask2former INFO: Inference done 170/1092. Dataloading: 0.0052 s/iter. Inference: 0.2377 s/iter. Eval: 0.1172 s/iter. Total: 0.3602 s/iter. ETA=0:05:32
[02/18 09:47:15] mask2former INFO: Inference done 184/1092. Dataloading: 0.0052 s/iter. Inference: 0.2377 s/iter. Eval: 0.1176 s/iter. Total: 0.3606 s/iter. ETA=0:05:27
[02/18 09:47:20] mask2former INFO: Inference done 198/1092. Dataloading: 0.0052 s/iter. Inference: 0.2376 s/iter. Eval: 0.1176 s/iter. Total: 0.3605 s/iter. ETA=0:05:22
[02/18 09:47:26] mask2former INFO: Inference done 213/1092. Dataloading: 0.0052 s/iter. Inference: 0.2372 s/iter. Eval: 0.1169 s/iter. Total: 0.3593 s/iter. ETA=0:05:15
[02/18 09:47:31] mask2former INFO: Inference done 228/1092. Dataloading: 0.0052 s/iter. Inference: 0.2371 s/iter. Eval: 0.1164 s/iter. Total: 0.3587 s/iter. ETA=0:05:09
[02/18 09:47:36] mask2former INFO: Inference done 243/1092. Dataloading: 0.0052 s/iter. Inference: 0.2374 s/iter. Eval: 0.1163 s/iter. Total: 0.3591 s/iter. ETA=0:05:04
[02/18 09:47:42] mask2former INFO: Inference done 257/1092. Dataloading: 0.0052 s/iter. Inference: 0.2371 s/iter. Eval: 0.1178 s/iter. Total: 0.3603 s/iter. ETA=0:05:00
[02/18 09:47:47] mask2former INFO: Inference done 272/1092. Dataloading: 0.0052 s/iter. Inference: 0.2373 s/iter. Eval: 0.1173 s/iter. Total: 0.3599 s/iter. ETA=0:04:55
[02/18 09:47:52] mask2former INFO: Inference done 286/1092. Dataloading: 0.0053 s/iter. Inference: 0.2377 s/iter. Eval: 0.1171 s/iter. Total: 0.3601 s/iter. ETA=0:04:50
[02/18 09:47:57] mask2former INFO: Inference done 301/1092. Dataloading: 0.0053 s/iter. Inference: 0.2371 s/iter. Eval: 0.1168 s/iter. Total: 0.3592 s/iter. ETA=0:04:44
[02/18 09:48:02] mask2former INFO: Inference done 316/1092. Dataloading: 0.0052 s/iter. Inference: 0.2369 s/iter. Eval: 0.1163 s/iter. Total: 0.3585 s/iter. ETA=0:04:38
[02/18 09:48:07] mask2former INFO: Inference done 329/1092. Dataloading: 0.0052 s/iter. Inference: 0.2379 s/iter. Eval: 0.1168 s/iter. Total: 0.3600 s/iter. ETA=0:04:34
[02/18 09:48:13] mask2former INFO: Inference done 344/1092. Dataloading: 0.0052 s/iter. Inference: 0.2373 s/iter. Eval: 0.1167 s/iter. Total: 0.3593 s/iter. ETA=0:04:28
[02/18 09:48:18] mask2former INFO: Inference done 358/1092. Dataloading: 0.0052 s/iter. Inference: 0.2374 s/iter. Eval: 0.1172 s/iter. Total: 0.3599 s/iter. ETA=0:04:24
[02/18 09:48:23] mask2former INFO: Inference done 372/1092. Dataloading: 0.0053 s/iter. Inference: 0.2379 s/iter. Eval: 0.1173 s/iter. Total: 0.3606 s/iter. ETA=0:04:19
[02/18 09:48:28] mask2former INFO: Inference done 386/1092. Dataloading: 0.0054 s/iter. Inference: 0.2379 s/iter. Eval: 0.1172 s/iter. Total: 0.3606 s/iter. ETA=0:04:14
[02/18 09:48:33] mask2former INFO: Inference done 400/1092. Dataloading: 0.0053 s/iter. Inference: 0.2383 s/iter. Eval: 0.1169 s/iter. Total: 0.3606 s/iter. ETA=0:04:09
[02/18 09:48:39] mask2former INFO: Inference done 416/1092. Dataloading: 0.0053 s/iter. Inference: 0.2381 s/iter. Eval: 0.1161 s/iter. Total: 0.3596 s/iter. ETA=0:04:03
[02/18 09:48:44] mask2former INFO: Inference done 431/1092. Dataloading: 0.0053 s/iter. Inference: 0.2375 s/iter. Eval: 0.1160 s/iter. Total: 0.3589 s/iter. ETA=0:03:57
[02/18 09:48:49] mask2former INFO: Inference done 445/1092. Dataloading: 0.0053 s/iter. Inference: 0.2376 s/iter. Eval: 0.1158 s/iter. Total: 0.3589 s/iter. ETA=0:03:52
[02/18 09:48:54] mask2former INFO: Inference done 459/1092. Dataloading: 0.0053 s/iter. Inference: 0.2384 s/iter. Eval: 0.1154 s/iter. Total: 0.3592 s/iter. ETA=0:03:47
[02/18 09:48:59] mask2former INFO: Inference done 473/1092. Dataloading: 0.0053 s/iter. Inference: 0.2385 s/iter. Eval: 0.1156 s/iter. Total: 0.3595 s/iter. ETA=0:03:42
[02/18 09:49:04] mask2former INFO: Inference done 487/1092. Dataloading: 0.0053 s/iter. Inference: 0.2385 s/iter. Eval: 0.1160 s/iter. Total: 0.3598 s/iter. ETA=0:03:37
[02/18 09:49:09] mask2former INFO: Inference done 501/1092. Dataloading: 0.0053 s/iter. Inference: 0.2388 s/iter. Eval: 0.1156 s/iter. Total: 0.3598 s/iter. ETA=0:03:32
[02/18 09:49:14] mask2former INFO: Inference done 516/1092. Dataloading: 0.0053 s/iter. Inference: 0.2385 s/iter. Eval: 0.1154 s/iter. Total: 0.3593 s/iter. ETA=0:03:26
[02/18 09:49:20] mask2former INFO: Inference done 531/1092. Dataloading: 0.0053 s/iter. Inference: 0.2381 s/iter. Eval: 0.1156 s/iter. Total: 0.3591 s/iter. ETA=0:03:21
[02/18 09:49:25] mask2former INFO: Inference done 546/1092. Dataloading: 0.0053 s/iter. Inference: 0.2380 s/iter. Eval: 0.1153 s/iter. Total: 0.3587 s/iter. ETA=0:03:15
[02/18 09:49:30] mask2former INFO: Inference done 560/1092. Dataloading: 0.0053 s/iter. Inference: 0.2379 s/iter. Eval: 0.1157 s/iter. Total: 0.3590 s/iter. ETA=0:03:10
[02/18 09:49:35] mask2former INFO: Inference done 574/1092. Dataloading: 0.0053 s/iter. Inference: 0.2379 s/iter. Eval: 0.1158 s/iter. Total: 0.3591 s/iter. ETA=0:03:06
[02/18 09:49:40] mask2former INFO: Inference done 588/1092. Dataloading: 0.0053 s/iter. Inference: 0.2382 s/iter. Eval: 0.1158 s/iter. Total: 0.3594 s/iter. ETA=0:03:01
[02/18 09:49:45] mask2former INFO: Inference done 602/1092. Dataloading: 0.0053 s/iter. Inference: 0.2380 s/iter. Eval: 0.1159 s/iter. Total: 0.3594 s/iter. ETA=0:02:56
[02/18 09:49:51] mask2former INFO: Inference done 617/1092. Dataloading: 0.0053 s/iter. Inference: 0.2378 s/iter. Eval: 0.1158 s/iter. Total: 0.3590 s/iter. ETA=0:02:50
[02/18 09:49:56] mask2former INFO: Inference done 631/1092. Dataloading: 0.0053 s/iter. Inference: 0.2378 s/iter. Eval: 0.1161 s/iter. Total: 0.3592 s/iter. ETA=0:02:45
[02/18 09:50:01] mask2former INFO: Inference done 645/1092. Dataloading: 0.0053 s/iter. Inference: 0.2380 s/iter. Eval: 0.1159 s/iter. Total: 0.3593 s/iter. ETA=0:02:40
[02/18 09:50:06] mask2former INFO: Inference done 659/1092. Dataloading: 0.0054 s/iter. Inference: 0.2380 s/iter. Eval: 0.1160 s/iter. Total: 0.3594 s/iter. ETA=0:02:35
[02/18 09:50:11] mask2former INFO: Inference done 674/1092. Dataloading: 0.0054 s/iter. Inference: 0.2380 s/iter. Eval: 0.1160 s/iter. Total: 0.3594 s/iter. ETA=0:02:30
[02/18 09:50:16] mask2former INFO: Inference done 689/1092. Dataloading: 0.0054 s/iter. Inference: 0.2378 s/iter. Eval: 0.1159 s/iter. Total: 0.3591 s/iter. ETA=0:02:24
[02/18 09:50:22] mask2former INFO: Inference done 703/1092. Dataloading: 0.0053 s/iter. Inference: 0.2379 s/iter. Eval: 0.1159 s/iter. Total: 0.3592 s/iter. ETA=0:02:19
[02/18 09:50:27] mask2former INFO: Inference done 718/1092. Dataloading: 0.0053 s/iter. Inference: 0.2378 s/iter. Eval: 0.1159 s/iter. Total: 0.3591 s/iter. ETA=0:02:14
[02/18 09:50:32] mask2former INFO: Inference done 733/1092. Dataloading: 0.0054 s/iter. Inference: 0.2377 s/iter. Eval: 0.1157 s/iter. Total: 0.3589 s/iter. ETA=0:02:08
[02/18 09:50:38] mask2former INFO: Inference done 748/1092. Dataloading: 0.0054 s/iter. Inference: 0.2377 s/iter. Eval: 0.1157 s/iter. Total: 0.3589 s/iter. ETA=0:02:03
[02/18 09:50:43] mask2former INFO: Inference done 762/1092. Dataloading: 0.0053 s/iter. Inference: 0.2378 s/iter. Eval: 0.1159 s/iter. Total: 0.3591 s/iter. ETA=0:01:58
[02/18 09:50:48] mask2former INFO: Inference done 776/1092. Dataloading: 0.0054 s/iter. Inference: 0.2376 s/iter. Eval: 0.1161 s/iter. Total: 0.3591 s/iter. ETA=0:01:53
[02/18 09:50:53] mask2former INFO: Inference done 791/1092. Dataloading: 0.0053 s/iter. Inference: 0.2375 s/iter. Eval: 0.1157 s/iter. Total: 0.3587 s/iter. ETA=0:01:47
[02/18 09:50:58] mask2former INFO: Inference done 805/1092. Dataloading: 0.0053 s/iter. Inference: 0.2375 s/iter. Eval: 0.1157 s/iter. Total: 0.3586 s/iter. ETA=0:01:42
[02/18 09:51:03] mask2former INFO: Inference done 820/1092. Dataloading: 0.0053 s/iter. Inference: 0.2373 s/iter. Eval: 0.1156 s/iter. Total: 0.3583 s/iter. ETA=0:01:37
[02/18 09:51:08] mask2former INFO: Inference done 835/1092. Dataloading: 0.0053 s/iter. Inference: 0.2371 s/iter. Eval: 0.1156 s/iter. Total: 0.3581 s/iter. ETA=0:01:32
[02/18 09:51:13] mask2former INFO: Inference done 850/1092. Dataloading: 0.0053 s/iter. Inference: 0.2369 s/iter. Eval: 0.1157 s/iter. Total: 0.3579 s/iter. ETA=0:01:26
[02/18 09:51:18] mask2former INFO: Inference done 864/1092. Dataloading: 0.0053 s/iter. Inference: 0.2367 s/iter. Eval: 0.1159 s/iter. Total: 0.3579 s/iter. ETA=0:01:21
[02/18 09:51:23] mask2former INFO: Inference done 878/1092. Dataloading: 0.0053 s/iter. Inference: 0.2366 s/iter. Eval: 0.1161 s/iter. Total: 0.3580 s/iter. ETA=0:01:16
[02/18 09:51:28] mask2former INFO: Inference done 892/1092. Dataloading: 0.0053 s/iter. Inference: 0.2368 s/iter. Eval: 0.1159 s/iter. Total: 0.3581 s/iter. ETA=0:01:11
[02/18 09:51:34] mask2former INFO: Inference done 907/1092. Dataloading: 0.0053 s/iter. Inference: 0.2368 s/iter. Eval: 0.1158 s/iter. Total: 0.3581 s/iter. ETA=0:01:06
[02/18 09:51:39] mask2former INFO: Inference done 921/1092. Dataloading: 0.0053 s/iter. Inference: 0.2368 s/iter. Eval: 0.1160 s/iter. Total: 0.3582 s/iter. ETA=0:01:01
[02/18 09:51:44] mask2former INFO: Inference done 935/1092. Dataloading: 0.0053 s/iter. Inference: 0.2369 s/iter. Eval: 0.1160 s/iter. Total: 0.3583 s/iter. ETA=0:00:56
[02/18 09:51:49] mask2former INFO: Inference done 951/1092. Dataloading: 0.0053 s/iter. Inference: 0.2367 s/iter. Eval: 0.1159 s/iter. Total: 0.3580 s/iter. ETA=0:00:50
[02/18 09:51:55] mask2former INFO: Inference done 966/1092. Dataloading: 0.0053 s/iter. Inference: 0.2365 s/iter. Eval: 0.1159 s/iter. Total: 0.3577 s/iter. ETA=0:00:45
[02/18 09:52:00] mask2former INFO: Inference done 980/1092. Dataloading: 0.0053 s/iter. Inference: 0.2364 s/iter. Eval: 0.1160 s/iter. Total: 0.3578 s/iter. ETA=0:00:40
[02/18 09:52:05] mask2former INFO: Inference done 994/1092. Dataloading: 0.0053 s/iter. Inference: 0.2365 s/iter. Eval: 0.1161 s/iter. Total: 0.3579 s/iter. ETA=0:00:35
[02/18 09:52:10] mask2former INFO: Inference done 1009/1092. Dataloading: 0.0053 s/iter. Inference: 0.2365 s/iter. Eval: 0.1158 s/iter. Total: 0.3577 s/iter. ETA=0:00:29
[02/18 09:52:15] mask2former INFO: Inference done 1024/1092. Dataloading: 0.0053 s/iter. Inference: 0.2365 s/iter. Eval: 0.1157 s/iter. Total: 0.3576 s/iter. ETA=0:00:24
[02/18 09:52:20] mask2former INFO: Inference done 1038/1092. Dataloading: 0.0053 s/iter. Inference: 0.2366 s/iter. Eval: 0.1156 s/iter. Total: 0.3576 s/iter. ETA=0:00:19
[02/18 09:52:25] mask2former INFO: Inference done 1052/1092. Dataloading: 0.0053 s/iter. Inference: 0.2366 s/iter. Eval: 0.1157 s/iter. Total: 0.3577 s/iter. ETA=0:00:14
[02/18 09:52:31] mask2former INFO: Inference done 1067/1092. Dataloading: 0.0053 s/iter. Inference: 0.2366 s/iter. Eval: 0.1157 s/iter. Total: 0.3576 s/iter. ETA=0:00:08
[02/18 09:52:36] mask2former INFO: Inference done 1081/1092. Dataloading: 0.0053 s/iter. Inference: 0.2367 s/iter. Eval: 0.1156 s/iter. Total: 0.3577 s/iter. ETA=0:00:03
[02/18 11:01:55] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 11:01:55] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 11:01:55] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 11:02:09] mask2former INFO: Inference done 11/1092. Dataloading: 0.0040 s/iter. Inference: 0.2571 s/iter. Eval: 0.1095 s/iter. Total: 0.3706 s/iter. ETA=0:06:40
[02/18 11:02:14] mask2former INFO: Inference done 25/1092. Dataloading: 0.0051 s/iter. Inference: 0.2566 s/iter. Eval: 0.1147 s/iter. Total: 0.3764 s/iter. ETA=0:06:41
[02/18 11:02:20] mask2former INFO: Inference done 39/1092. Dataloading: 0.0063 s/iter. Inference: 0.2527 s/iter. Eval: 0.1179 s/iter. Total: 0.3769 s/iter. ETA=0:06:36
[02/18 11:02:25] mask2former INFO: Inference done 53/1092. Dataloading: 0.0062 s/iter. Inference: 0.2549 s/iter. Eval: 0.1173 s/iter. Total: 0.3785 s/iter. ETA=0:06:33
[02/18 11:02:30] mask2former INFO: Inference done 68/1092. Dataloading: 0.0059 s/iter. Inference: 0.2517 s/iter. Eval: 0.1150 s/iter. Total: 0.3726 s/iter. ETA=0:06:21
[02/18 11:02:35] mask2former INFO: Inference done 82/1092. Dataloading: 0.0057 s/iter. Inference: 0.2494 s/iter. Eval: 0.1151 s/iter. Total: 0.3703 s/iter. ETA=0:06:13
[02/18 11:02:41] mask2former INFO: Inference done 97/1092. Dataloading: 0.0056 s/iter. Inference: 0.2463 s/iter. Eval: 0.1156 s/iter. Total: 0.3676 s/iter. ETA=0:06:05
[02/18 11:02:46] mask2former INFO: Inference done 112/1092. Dataloading: 0.0054 s/iter. Inference: 0.2451 s/iter. Eval: 0.1150 s/iter. Total: 0.3657 s/iter. ETA=0:05:58
[02/18 11:02:51] mask2former INFO: Inference done 125/1092. Dataloading: 0.0055 s/iter. Inference: 0.2482 s/iter. Eval: 0.1161 s/iter. Total: 0.3698 s/iter. ETA=0:05:57
[02/18 11:02:57] mask2former INFO: Inference done 140/1092. Dataloading: 0.0055 s/iter. Inference: 0.2464 s/iter. Eval: 0.1160 s/iter. Total: 0.3680 s/iter. ETA=0:05:50
[02/18 11:03:02] mask2former INFO: Inference done 154/1092. Dataloading: 0.0055 s/iter. Inference: 0.2464 s/iter. Eval: 0.1161 s/iter. Total: 0.3681 s/iter. ETA=0:05:45
[02/18 11:03:07] mask2former INFO: Inference done 167/1092. Dataloading: 0.0055 s/iter. Inference: 0.2474 s/iter. Eval: 0.1170 s/iter. Total: 0.3700 s/iter. ETA=0:05:42
[02/18 11:03:12] mask2former INFO: Inference done 181/1092. Dataloading: 0.0055 s/iter. Inference: 0.2472 s/iter. Eval: 0.1172 s/iter. Total: 0.3700 s/iter. ETA=0:05:37
[02/18 11:03:17] mask2former INFO: Inference done 195/1092. Dataloading: 0.0055 s/iter. Inference: 0.2469 s/iter. Eval: 0.1178 s/iter. Total: 0.3702 s/iter. ETA=0:05:32
[02/18 11:03:23] mask2former INFO: Inference done 208/1092. Dataloading: 0.0055 s/iter. Inference: 0.2480 s/iter. Eval: 0.1193 s/iter. Total: 0.3728 s/iter. ETA=0:05:29
[02/18 11:03:28] mask2former INFO: Inference done 223/1092. Dataloading: 0.0054 s/iter. Inference: 0.2469 s/iter. Eval: 0.1191 s/iter. Total: 0.3715 s/iter. ETA=0:05:22
[02/18 11:03:33] mask2former INFO: Inference done 238/1092. Dataloading: 0.0054 s/iter. Inference: 0.2458 s/iter. Eval: 0.1184 s/iter. Total: 0.3698 s/iter. ETA=0:05:15
[02/18 11:03:38] mask2former INFO: Inference done 252/1092. Dataloading: 0.0055 s/iter. Inference: 0.2452 s/iter. Eval: 0.1188 s/iter. Total: 0.3696 s/iter. ETA=0:05:10
[02/18 11:03:43] mask2former INFO: Inference done 267/1092. Dataloading: 0.0054 s/iter. Inference: 0.2446 s/iter. Eval: 0.1181 s/iter. Total: 0.3682 s/iter. ETA=0:05:03
[02/18 11:03:49] mask2former INFO: Inference done 282/1092. Dataloading: 0.0054 s/iter. Inference: 0.2438 s/iter. Eval: 0.1178 s/iter. Total: 0.3671 s/iter. ETA=0:04:57
[02/18 11:03:54] mask2former INFO: Inference done 296/1092. Dataloading: 0.0054 s/iter. Inference: 0.2441 s/iter. Eval: 0.1180 s/iter. Total: 0.3677 s/iter. ETA=0:04:52
[02/18 11:03:59] mask2former INFO: Inference done 310/1092. Dataloading: 0.0055 s/iter. Inference: 0.2438 s/iter. Eval: 0.1181 s/iter. Total: 0.3675 s/iter. ETA=0:04:47
[02/18 11:04:04] mask2former INFO: Inference done 324/1092. Dataloading: 0.0055 s/iter. Inference: 0.2439 s/iter. Eval: 0.1177 s/iter. Total: 0.3672 s/iter. ETA=0:04:41
[02/18 11:04:09] mask2former INFO: Inference done 338/1092. Dataloading: 0.0055 s/iter. Inference: 0.2441 s/iter. Eval: 0.1172 s/iter. Total: 0.3668 s/iter. ETA=0:04:36
[02/18 11:04:14] mask2former INFO: Inference done 352/1092. Dataloading: 0.0055 s/iter. Inference: 0.2436 s/iter. Eval: 0.1174 s/iter. Total: 0.3666 s/iter. ETA=0:04:31
[02/18 11:04:19] mask2former INFO: Inference done 367/1092. Dataloading: 0.0055 s/iter. Inference: 0.2429 s/iter. Eval: 0.1169 s/iter. Total: 0.3653 s/iter. ETA=0:04:24
[02/18 11:04:25] mask2former INFO: Inference done 382/1092. Dataloading: 0.0055 s/iter. Inference: 0.2424 s/iter. Eval: 0.1169 s/iter. Total: 0.3648 s/iter. ETA=0:04:19
[02/18 11:04:30] mask2former INFO: Inference done 396/1092. Dataloading: 0.0055 s/iter. Inference: 0.2424 s/iter. Eval: 0.1171 s/iter. Total: 0.3651 s/iter. ETA=0:04:14
[02/18 11:04:35] mask2former INFO: Inference done 411/1092. Dataloading: 0.0055 s/iter. Inference: 0.2425 s/iter. Eval: 0.1165 s/iter. Total: 0.3645 s/iter. ETA=0:04:08
[02/18 11:04:40] mask2former INFO: Inference done 425/1092. Dataloading: 0.0055 s/iter. Inference: 0.2427 s/iter. Eval: 0.1164 s/iter. Total: 0.3647 s/iter. ETA=0:04:03
[02/18 11:04:45] mask2former INFO: Inference done 439/1092. Dataloading: 0.0055 s/iter. Inference: 0.2424 s/iter. Eval: 0.1166 s/iter. Total: 0.3645 s/iter. ETA=0:03:58
[02/18 11:04:50] mask2former INFO: Inference done 454/1092. Dataloading: 0.0055 s/iter. Inference: 0.2423 s/iter. Eval: 0.1162 s/iter. Total: 0.3641 s/iter. ETA=0:03:52
[02/18 11:04:56] mask2former INFO: Inference done 469/1092. Dataloading: 0.0055 s/iter. Inference: 0.2421 s/iter. Eval: 0.1160 s/iter. Total: 0.3637 s/iter. ETA=0:03:46
[02/18 11:05:01] mask2former INFO: Inference done 483/1092. Dataloading: 0.0055 s/iter. Inference: 0.2423 s/iter. Eval: 0.1162 s/iter. Total: 0.3641 s/iter. ETA=0:03:41
[02/18 11:05:06] mask2former INFO: Inference done 498/1092. Dataloading: 0.0055 s/iter. Inference: 0.2421 s/iter. Eval: 0.1157 s/iter. Total: 0.3634 s/iter. ETA=0:03:35
[02/18 11:05:11] mask2former INFO: Inference done 512/1092. Dataloading: 0.0055 s/iter. Inference: 0.2424 s/iter. Eval: 0.1159 s/iter. Total: 0.3639 s/iter. ETA=0:03:31
[02/18 11:05:17] mask2former INFO: Inference done 526/1092. Dataloading: 0.0055 s/iter. Inference: 0.2427 s/iter. Eval: 0.1160 s/iter. Total: 0.3643 s/iter. ETA=0:03:26
[02/18 11:05:22] mask2former INFO: Inference done 542/1092. Dataloading: 0.0055 s/iter. Inference: 0.2422 s/iter. Eval: 0.1157 s/iter. Total: 0.3635 s/iter. ETA=0:03:19
[02/18 11:05:27] mask2former INFO: Inference done 556/1092. Dataloading: 0.0055 s/iter. Inference: 0.2424 s/iter. Eval: 0.1157 s/iter. Total: 0.3637 s/iter. ETA=0:03:14
[02/18 11:05:33] mask2former INFO: Inference done 571/1092. Dataloading: 0.0054 s/iter. Inference: 0.2423 s/iter. Eval: 0.1155 s/iter. Total: 0.3634 s/iter. ETA=0:03:09
[02/18 11:05:38] mask2former INFO: Inference done 586/1092. Dataloading: 0.0054 s/iter. Inference: 0.2421 s/iter. Eval: 0.1154 s/iter. Total: 0.3631 s/iter. ETA=0:03:03
[02/18 11:05:43] mask2former INFO: Inference done 601/1092. Dataloading: 0.0054 s/iter. Inference: 0.2421 s/iter. Eval: 0.1153 s/iter. Total: 0.3629 s/iter. ETA=0:02:58
[02/18 11:05:48] mask2former INFO: Inference done 616/1092. Dataloading: 0.0055 s/iter. Inference: 0.2420 s/iter. Eval: 0.1150 s/iter. Total: 0.3625 s/iter. ETA=0:02:52
[02/18 11:05:54] mask2former INFO: Inference done 631/1092. Dataloading: 0.0054 s/iter. Inference: 0.2418 s/iter. Eval: 0.1149 s/iter. Total: 0.3623 s/iter. ETA=0:02:47
[02/18 11:05:59] mask2former INFO: Inference done 645/1092. Dataloading: 0.0054 s/iter. Inference: 0.2418 s/iter. Eval: 0.1151 s/iter. Total: 0.3624 s/iter. ETA=0:02:41
[02/18 11:06:04] mask2former INFO: Inference done 659/1092. Dataloading: 0.0054 s/iter. Inference: 0.2417 s/iter. Eval: 0.1152 s/iter. Total: 0.3624 s/iter. ETA=0:02:36
[02/18 11:06:09] mask2former INFO: Inference done 674/1092. Dataloading: 0.0054 s/iter. Inference: 0.2415 s/iter. Eval: 0.1149 s/iter. Total: 0.3619 s/iter. ETA=0:02:31
[02/18 11:06:14] mask2former INFO: Inference done 688/1092. Dataloading: 0.0054 s/iter. Inference: 0.2417 s/iter. Eval: 0.1148 s/iter. Total: 0.3620 s/iter. ETA=0:02:26
[02/18 11:06:19] mask2former INFO: Inference done 703/1092. Dataloading: 0.0054 s/iter. Inference: 0.2415 s/iter. Eval: 0.1147 s/iter. Total: 0.3618 s/iter. ETA=0:02:20
[02/18 11:06:25] mask2former INFO: Inference done 718/1092. Dataloading: 0.0054 s/iter. Inference: 0.2414 s/iter. Eval: 0.1147 s/iter. Total: 0.3616 s/iter. ETA=0:02:15
[02/18 11:06:30] mask2former INFO: Inference done 732/1092. Dataloading: 0.0054 s/iter. Inference: 0.2411 s/iter. Eval: 0.1150 s/iter. Total: 0.3615 s/iter. ETA=0:02:10
[02/18 11:06:35] mask2former INFO: Inference done 746/1092. Dataloading: 0.0054 s/iter. Inference: 0.2412 s/iter. Eval: 0.1148 s/iter. Total: 0.3615 s/iter. ETA=0:02:05
[02/18 11:06:40] mask2former INFO: Inference done 761/1092. Dataloading: 0.0054 s/iter. Inference: 0.2409 s/iter. Eval: 0.1150 s/iter. Total: 0.3614 s/iter. ETA=0:01:59
[02/18 11:06:45] mask2former INFO: Inference done 775/1092. Dataloading: 0.0054 s/iter. Inference: 0.2409 s/iter. Eval: 0.1150 s/iter. Total: 0.3614 s/iter. ETA=0:01:54
[02/18 11:06:50] mask2former INFO: Inference done 790/1092. Dataloading: 0.0055 s/iter. Inference: 0.2409 s/iter. Eval: 0.1148 s/iter. Total: 0.3612 s/iter. ETA=0:01:49
[02/18 11:06:56] mask2former INFO: Inference done 804/1092. Dataloading: 0.0055 s/iter. Inference: 0.2411 s/iter. Eval: 0.1146 s/iter. Total: 0.3613 s/iter. ETA=0:01:44
[02/18 11:07:01] mask2former INFO: Inference done 819/1092. Dataloading: 0.0055 s/iter. Inference: 0.2408 s/iter. Eval: 0.1145 s/iter. Total: 0.3609 s/iter. ETA=0:01:38
[02/18 11:07:06] mask2former INFO: Inference done 834/1092. Dataloading: 0.0054 s/iter. Inference: 0.2405 s/iter. Eval: 0.1145 s/iter. Total: 0.3605 s/iter. ETA=0:01:33
[02/18 11:07:11] mask2former INFO: Inference done 849/1092. Dataloading: 0.0054 s/iter. Inference: 0.2404 s/iter. Eval: 0.1146 s/iter. Total: 0.3604 s/iter. ETA=0:01:27
[02/18 11:07:16] mask2former INFO: Inference done 864/1092. Dataloading: 0.0054 s/iter. Inference: 0.2403 s/iter. Eval: 0.1145 s/iter. Total: 0.3603 s/iter. ETA=0:01:22
[02/18 11:07:22] mask2former INFO: Inference done 879/1092. Dataloading: 0.0054 s/iter. Inference: 0.2402 s/iter. Eval: 0.1145 s/iter. Total: 0.3602 s/iter. ETA=0:01:16
[02/18 11:07:27] mask2former INFO: Inference done 893/1092. Dataloading: 0.0054 s/iter. Inference: 0.2402 s/iter. Eval: 0.1145 s/iter. Total: 0.3602 s/iter. ETA=0:01:11
[02/18 11:07:32] mask2former INFO: Inference done 907/1092. Dataloading: 0.0054 s/iter. Inference: 0.2404 s/iter. Eval: 0.1146 s/iter. Total: 0.3605 s/iter. ETA=0:01:06
[02/18 11:07:37] mask2former INFO: Inference done 922/1092. Dataloading: 0.0054 s/iter. Inference: 0.2401 s/iter. Eval: 0.1146 s/iter. Total: 0.3602 s/iter. ETA=0:01:01
[02/18 11:07:42] mask2former INFO: Inference done 937/1092. Dataloading: 0.0054 s/iter. Inference: 0.2399 s/iter. Eval: 0.1147 s/iter. Total: 0.3600 s/iter. ETA=0:00:55
[02/18 11:07:47] mask2former INFO: Inference done 951/1092. Dataloading: 0.0054 s/iter. Inference: 0.2399 s/iter. Eval: 0.1146 s/iter. Total: 0.3600 s/iter. ETA=0:00:50
[02/18 11:07:53] mask2former INFO: Inference done 965/1092. Dataloading: 0.0054 s/iter. Inference: 0.2399 s/iter. Eval: 0.1148 s/iter. Total: 0.3601 s/iter. ETA=0:00:45
[02/18 11:07:58] mask2former INFO: Inference done 979/1092. Dataloading: 0.0056 s/iter. Inference: 0.2400 s/iter. Eval: 0.1148 s/iter. Total: 0.3605 s/iter. ETA=0:00:40
[02/18 11:08:03] mask2former INFO: Inference done 994/1092. Dataloading: 0.0056 s/iter. Inference: 0.2399 s/iter. Eval: 0.1147 s/iter. Total: 0.3602 s/iter. ETA=0:00:35
[02/18 11:08:08] mask2former INFO: Inference done 1008/1092. Dataloading: 0.0056 s/iter. Inference: 0.2398 s/iter. Eval: 0.1147 s/iter. Total: 0.3602 s/iter. ETA=0:00:30
[02/18 11:08:14] mask2former INFO: Inference done 1023/1092. Dataloading: 0.0056 s/iter. Inference: 0.2399 s/iter. Eval: 0.1146 s/iter. Total: 0.3601 s/iter. ETA=0:00:24
[02/18 11:08:19] mask2former INFO: Inference done 1037/1092. Dataloading: 0.0056 s/iter. Inference: 0.2399 s/iter. Eval: 0.1147 s/iter. Total: 0.3602 s/iter. ETA=0:00:19
[02/18 11:08:24] mask2former INFO: Inference done 1051/1092. Dataloading: 0.0056 s/iter. Inference: 0.2398 s/iter. Eval: 0.1147 s/iter. Total: 0.3602 s/iter. ETA=0:00:14
[02/18 11:08:29] mask2former INFO: Inference done 1065/1092. Dataloading: 0.0056 s/iter. Inference: 0.2400 s/iter. Eval: 0.1147 s/iter. Total: 0.3605 s/iter. ETA=0:00:09
[02/18 11:08:34] mask2former INFO: Inference done 1079/1092. Dataloading: 0.0056 s/iter. Inference: 0.2401 s/iter. Eval: 0.1147 s/iter. Total: 0.3605 s/iter. ETA=0:00:04
[02/18 12:11:35] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 12:11:35] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 12:11:35] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 12:11:48] mask2former INFO: Inference done 11/1092. Dataloading: 0.0047 s/iter. Inference: 0.2276 s/iter. Eval: 0.1170 s/iter. Total: 0.3494 s/iter. ETA=0:06:17
[02/18 12:11:54] mask2former INFO: Inference done 25/1092. Dataloading: 0.0052 s/iter. Inference: 0.2407 s/iter. Eval: 0.1229 s/iter. Total: 0.3689 s/iter. ETA=0:06:33
[02/18 12:11:59] mask2former INFO: Inference done 40/1092. Dataloading: 0.0052 s/iter. Inference: 0.2370 s/iter. Eval: 0.1169 s/iter. Total: 0.3592 s/iter. ETA=0:06:17
[02/18 12:12:04] mask2former INFO: Inference done 55/1092. Dataloading: 0.0050 s/iter. Inference: 0.2356 s/iter. Eval: 0.1143 s/iter. Total: 0.3550 s/iter. ETA=0:06:08
[02/18 12:12:09] mask2former INFO: Inference done 69/1092. Dataloading: 0.0049 s/iter. Inference: 0.2375 s/iter. Eval: 0.1159 s/iter. Total: 0.3584 s/iter. ETA=0:06:06
[02/18 12:12:14] mask2former INFO: Inference done 83/1092. Dataloading: 0.0048 s/iter. Inference: 0.2383 s/iter. Eval: 0.1155 s/iter. Total: 0.3587 s/iter. ETA=0:06:01
[02/18 12:12:19] mask2former INFO: Inference done 98/1092. Dataloading: 0.0048 s/iter. Inference: 0.2363 s/iter. Eval: 0.1153 s/iter. Total: 0.3565 s/iter. ETA=0:05:54
[02/18 12:12:25] mask2former INFO: Inference done 113/1092. Dataloading: 0.0048 s/iter. Inference: 0.2351 s/iter. Eval: 0.1161 s/iter. Total: 0.3560 s/iter. ETA=0:05:48
[02/18 12:12:30] mask2former INFO: Inference done 127/1092. Dataloading: 0.0050 s/iter. Inference: 0.2366 s/iter. Eval: 0.1165 s/iter. Total: 0.3581 s/iter. ETA=0:05:45
[02/18 12:12:35] mask2former INFO: Inference done 142/1092. Dataloading: 0.0050 s/iter. Inference: 0.2362 s/iter. Eval: 0.1161 s/iter. Total: 0.3573 s/iter. ETA=0:05:39
[02/18 12:12:40] mask2former INFO: Inference done 156/1092. Dataloading: 0.0050 s/iter. Inference: 0.2375 s/iter. Eval: 0.1160 s/iter. Total: 0.3586 s/iter. ETA=0:05:35
[02/18 12:12:46] mask2former INFO: Inference done 171/1092. Dataloading: 0.0050 s/iter. Inference: 0.2363 s/iter. Eval: 0.1159 s/iter. Total: 0.3573 s/iter. ETA=0:05:29
[02/18 12:12:51] mask2former INFO: Inference done 185/1092. Dataloading: 0.0050 s/iter. Inference: 0.2365 s/iter. Eval: 0.1167 s/iter. Total: 0.3582 s/iter. ETA=0:05:24
[02/18 12:12:56] mask2former INFO: Inference done 199/1092. Dataloading: 0.0051 s/iter. Inference: 0.2371 s/iter. Eval: 0.1172 s/iter. Total: 0.3595 s/iter. ETA=0:05:21
[02/18 12:13:01] mask2former INFO: Inference done 214/1092. Dataloading: 0.0051 s/iter. Inference: 0.2369 s/iter. Eval: 0.1166 s/iter. Total: 0.3587 s/iter. ETA=0:05:14
[02/18 12:13:06] mask2former INFO: Inference done 228/1092. Dataloading: 0.0051 s/iter. Inference: 0.2373 s/iter. Eval: 0.1168 s/iter. Total: 0.3593 s/iter. ETA=0:05:10
[02/18 12:13:12] mask2former INFO: Inference done 242/1092. Dataloading: 0.0052 s/iter. Inference: 0.2372 s/iter. Eval: 0.1178 s/iter. Total: 0.3604 s/iter. ETA=0:05:06
[02/18 12:13:17] mask2former INFO: Inference done 256/1092. Dataloading: 0.0053 s/iter. Inference: 0.2376 s/iter. Eval: 0.1182 s/iter. Total: 0.3613 s/iter. ETA=0:05:02
[02/18 12:13:22] mask2former INFO: Inference done 271/1092. Dataloading: 0.0053 s/iter. Inference: 0.2373 s/iter. Eval: 0.1176 s/iter. Total: 0.3603 s/iter. ETA=0:04:55
[02/18 12:13:27] mask2former INFO: Inference done 286/1092. Dataloading: 0.0052 s/iter. Inference: 0.2378 s/iter. Eval: 0.1169 s/iter. Total: 0.3600 s/iter. ETA=0:04:50
[02/18 12:13:33] mask2former INFO: Inference done 301/1092. Dataloading: 0.0052 s/iter. Inference: 0.2374 s/iter. Eval: 0.1162 s/iter. Total: 0.3589 s/iter. ETA=0:04:43
[02/18 12:13:38] mask2former INFO: Inference done 316/1092. Dataloading: 0.0053 s/iter. Inference: 0.2373 s/iter. Eval: 0.1156 s/iter. Total: 0.3582 s/iter. ETA=0:04:38
[02/18 12:13:43] mask2former INFO: Inference done 331/1092. Dataloading: 0.0052 s/iter. Inference: 0.2372 s/iter. Eval: 0.1155 s/iter. Total: 0.3580 s/iter. ETA=0:04:32
[02/18 12:13:48] mask2former INFO: Inference done 346/1092. Dataloading: 0.0052 s/iter. Inference: 0.2370 s/iter. Eval: 0.1148 s/iter. Total: 0.3572 s/iter. ETA=0:04:26
[02/18 12:13:53] mask2former INFO: Inference done 360/1092. Dataloading: 0.0052 s/iter. Inference: 0.2373 s/iter. Eval: 0.1146 s/iter. Total: 0.3572 s/iter. ETA=0:04:21
[02/18 12:13:58] mask2former INFO: Inference done 375/1092. Dataloading: 0.0052 s/iter. Inference: 0.2372 s/iter. Eval: 0.1144 s/iter. Total: 0.3569 s/iter. ETA=0:04:15
[02/18 12:14:03] mask2former INFO: Inference done 390/1092. Dataloading: 0.0052 s/iter. Inference: 0.2368 s/iter. Eval: 0.1143 s/iter. Total: 0.3564 s/iter. ETA=0:04:10
[02/18 12:14:08] mask2former INFO: Inference done 404/1092. Dataloading: 0.0052 s/iter. Inference: 0.2366 s/iter. Eval: 0.1145 s/iter. Total: 0.3564 s/iter. ETA=0:04:05
[02/18 12:14:14] mask2former INFO: Inference done 418/1092. Dataloading: 0.0052 s/iter. Inference: 0.2370 s/iter. Eval: 0.1144 s/iter. Total: 0.3567 s/iter. ETA=0:04:00
[02/18 12:14:19] mask2former INFO: Inference done 432/1092. Dataloading: 0.0052 s/iter. Inference: 0.2373 s/iter. Eval: 0.1145 s/iter. Total: 0.3571 s/iter. ETA=0:03:55
[02/18 12:14:24] mask2former INFO: Inference done 446/1092. Dataloading: 0.0053 s/iter. Inference: 0.2372 s/iter. Eval: 0.1148 s/iter. Total: 0.3575 s/iter. ETA=0:03:50
[02/18 12:14:29] mask2former INFO: Inference done 459/1092. Dataloading: 0.0054 s/iter. Inference: 0.2381 s/iter. Eval: 0.1152 s/iter. Total: 0.3587 s/iter. ETA=0:03:47
[02/18 12:14:34] mask2former INFO: Inference done 473/1092. Dataloading: 0.0053 s/iter. Inference: 0.2382 s/iter. Eval: 0.1151 s/iter. Total: 0.3588 s/iter. ETA=0:03:42
[02/18 12:14:39] mask2former INFO: Inference done 488/1092. Dataloading: 0.0053 s/iter. Inference: 0.2380 s/iter. Eval: 0.1150 s/iter. Total: 0.3584 s/iter. ETA=0:03:36
[02/18 12:14:45] mask2former INFO: Inference done 503/1092. Dataloading: 0.0053 s/iter. Inference: 0.2384 s/iter. Eval: 0.1146 s/iter. Total: 0.3584 s/iter. ETA=0:03:31
[02/18 12:14:50] mask2former INFO: Inference done 517/1092. Dataloading: 0.0053 s/iter. Inference: 0.2386 s/iter. Eval: 0.1148 s/iter. Total: 0.3588 s/iter. ETA=0:03:26
[02/18 12:14:55] mask2former INFO: Inference done 532/1092. Dataloading: 0.0053 s/iter. Inference: 0.2384 s/iter. Eval: 0.1149 s/iter. Total: 0.3586 s/iter. ETA=0:03:20
[02/18 12:15:00] mask2former INFO: Inference done 546/1092. Dataloading: 0.0053 s/iter. Inference: 0.2384 s/iter. Eval: 0.1150 s/iter. Total: 0.3588 s/iter. ETA=0:03:15
[02/18 12:15:05] mask2former INFO: Inference done 561/1092. Dataloading: 0.0053 s/iter. Inference: 0.2379 s/iter. Eval: 0.1149 s/iter. Total: 0.3582 s/iter. ETA=0:03:10
[02/18 12:15:11] mask2former INFO: Inference done 576/1092. Dataloading: 0.0052 s/iter. Inference: 0.2379 s/iter. Eval: 0.1149 s/iter. Total: 0.3582 s/iter. ETA=0:03:04
[02/18 12:15:16] mask2former INFO: Inference done 591/1092. Dataloading: 0.0052 s/iter. Inference: 0.2376 s/iter. Eval: 0.1147 s/iter. Total: 0.3576 s/iter. ETA=0:02:59
[02/18 12:15:21] mask2former INFO: Inference done 606/1092. Dataloading: 0.0053 s/iter. Inference: 0.2374 s/iter. Eval: 0.1147 s/iter. Total: 0.3574 s/iter. ETA=0:02:53
[02/18 12:15:26] mask2former INFO: Inference done 620/1092. Dataloading: 0.0053 s/iter. Inference: 0.2376 s/iter. Eval: 0.1147 s/iter. Total: 0.3577 s/iter. ETA=0:02:48
[02/18 12:15:32] mask2former INFO: Inference done 635/1092. Dataloading: 0.0053 s/iter. Inference: 0.2376 s/iter. Eval: 0.1148 s/iter. Total: 0.3578 s/iter. ETA=0:02:43
[02/18 12:15:37] mask2former INFO: Inference done 649/1092. Dataloading: 0.0053 s/iter. Inference: 0.2375 s/iter. Eval: 0.1152 s/iter. Total: 0.3581 s/iter. ETA=0:02:38
[02/18 12:15:42] mask2former INFO: Inference done 664/1092. Dataloading: 0.0053 s/iter. Inference: 0.2375 s/iter. Eval: 0.1152 s/iter. Total: 0.3581 s/iter. ETA=0:02:33
[02/18 12:15:48] mask2former INFO: Inference done 679/1092. Dataloading: 0.0053 s/iter. Inference: 0.2377 s/iter. Eval: 0.1151 s/iter. Total: 0.3582 s/iter. ETA=0:02:27
[02/18 12:15:53] mask2former INFO: Inference done 693/1092. Dataloading: 0.0053 s/iter. Inference: 0.2377 s/iter. Eval: 0.1151 s/iter. Total: 0.3583 s/iter. ETA=0:02:22
[02/18 12:15:58] mask2former INFO: Inference done 707/1092. Dataloading: 0.0053 s/iter. Inference: 0.2377 s/iter. Eval: 0.1155 s/iter. Total: 0.3586 s/iter. ETA=0:02:18
[02/18 12:16:03] mask2former INFO: Inference done 721/1092. Dataloading: 0.0054 s/iter. Inference: 0.2380 s/iter. Eval: 0.1156 s/iter. Total: 0.3590 s/iter. ETA=0:02:13
[02/18 12:16:09] mask2former INFO: Inference done 737/1092. Dataloading: 0.0053 s/iter. Inference: 0.2377 s/iter. Eval: 0.1152 s/iter. Total: 0.3584 s/iter. ETA=0:02:07
[02/18 12:16:14] mask2former INFO: Inference done 751/1092. Dataloading: 0.0054 s/iter. Inference: 0.2379 s/iter. Eval: 0.1153 s/iter. Total: 0.3586 s/iter. ETA=0:02:02
[02/18 12:16:19] mask2former INFO: Inference done 766/1092. Dataloading: 0.0054 s/iter. Inference: 0.2375 s/iter. Eval: 0.1152 s/iter. Total: 0.3582 s/iter. ETA=0:01:56
[02/18 12:16:24] mask2former INFO: Inference done 780/1092. Dataloading: 0.0055 s/iter. Inference: 0.2376 s/iter. Eval: 0.1150 s/iter. Total: 0.3582 s/iter. ETA=0:01:51
[02/18 12:16:29] mask2former INFO: Inference done 795/1092. Dataloading: 0.0056 s/iter. Inference: 0.2377 s/iter. Eval: 0.1146 s/iter. Total: 0.3579 s/iter. ETA=0:01:46
[02/18 12:16:34] mask2former INFO: Inference done 810/1092. Dataloading: 0.0055 s/iter. Inference: 0.2377 s/iter. Eval: 0.1146 s/iter. Total: 0.3579 s/iter. ETA=0:01:40
[02/18 12:16:40] mask2former INFO: Inference done 824/1092. Dataloading: 0.0056 s/iter. Inference: 0.2379 s/iter. Eval: 0.1147 s/iter. Total: 0.3583 s/iter. ETA=0:01:36
[02/18 12:16:45] mask2former INFO: Inference done 839/1092. Dataloading: 0.0056 s/iter. Inference: 0.2374 s/iter. Eval: 0.1148 s/iter. Total: 0.3579 s/iter. ETA=0:01:30
[02/18 12:16:50] mask2former INFO: Inference done 854/1092. Dataloading: 0.0056 s/iter. Inference: 0.2374 s/iter. Eval: 0.1148 s/iter. Total: 0.3579 s/iter. ETA=0:01:25
[02/18 12:16:55] mask2former INFO: Inference done 869/1092. Dataloading: 0.0056 s/iter. Inference: 0.2372 s/iter. Eval: 0.1149 s/iter. Total: 0.3578 s/iter. ETA=0:01:19
[02/18 12:17:01] mask2former INFO: Inference done 884/1092. Dataloading: 0.0056 s/iter. Inference: 0.2373 s/iter. Eval: 0.1147 s/iter. Total: 0.3577 s/iter. ETA=0:01:14
[02/18 12:17:06] mask2former INFO: Inference done 898/1092. Dataloading: 0.0056 s/iter. Inference: 0.2373 s/iter. Eval: 0.1148 s/iter. Total: 0.3578 s/iter. ETA=0:01:09
[02/18 12:17:11] mask2former INFO: Inference done 913/1092. Dataloading: 0.0055 s/iter. Inference: 0.2371 s/iter. Eval: 0.1149 s/iter. Total: 0.3576 s/iter. ETA=0:01:04
[02/18 12:17:16] mask2former INFO: Inference done 927/1092. Dataloading: 0.0055 s/iter. Inference: 0.2370 s/iter. Eval: 0.1151 s/iter. Total: 0.3578 s/iter. ETA=0:00:59
[02/18 12:17:22] mask2former INFO: Inference done 942/1092. Dataloading: 0.0055 s/iter. Inference: 0.2370 s/iter. Eval: 0.1151 s/iter. Total: 0.3578 s/iter. ETA=0:00:53
[02/18 12:17:27] mask2former INFO: Inference done 956/1092. Dataloading: 0.0055 s/iter. Inference: 0.2372 s/iter. Eval: 0.1151 s/iter. Total: 0.3579 s/iter. ETA=0:00:48
[02/18 12:17:32] mask2former INFO: Inference done 970/1092. Dataloading: 0.0055 s/iter. Inference: 0.2372 s/iter. Eval: 0.1151 s/iter. Total: 0.3580 s/iter. ETA=0:00:43
[02/18 12:17:37] mask2former INFO: Inference done 985/1092. Dataloading: 0.0055 s/iter. Inference: 0.2371 s/iter. Eval: 0.1149 s/iter. Total: 0.3576 s/iter. ETA=0:00:38
[02/18 12:17:42] mask2former INFO: Inference done 1000/1092. Dataloading: 0.0055 s/iter. Inference: 0.2371 s/iter. Eval: 0.1149 s/iter. Total: 0.3576 s/iter. ETA=0:00:32
[02/18 12:17:47] mask2former INFO: Inference done 1015/1092. Dataloading: 0.0055 s/iter. Inference: 0.2372 s/iter. Eval: 0.1148 s/iter. Total: 0.3576 s/iter. ETA=0:00:27
[02/18 12:17:53] mask2former INFO: Inference done 1030/1092. Dataloading: 0.0055 s/iter. Inference: 0.2372 s/iter. Eval: 0.1145 s/iter. Total: 0.3574 s/iter. ETA=0:00:22
[02/18 12:17:58] mask2former INFO: Inference done 1045/1092. Dataloading: 0.0055 s/iter. Inference: 0.2371 s/iter. Eval: 0.1144 s/iter. Total: 0.3571 s/iter. ETA=0:00:16
[02/18 12:18:03] mask2former INFO: Inference done 1059/1092. Dataloading: 0.0055 s/iter. Inference: 0.2370 s/iter. Eval: 0.1145 s/iter. Total: 0.3572 s/iter. ETA=0:00:11
[02/18 12:18:08] mask2former INFO: Inference done 1073/1092. Dataloading: 0.0055 s/iter. Inference: 0.2373 s/iter. Eval: 0.1145 s/iter. Total: 0.3574 s/iter. ETA=0:00:06
[02/18 12:18:13] mask2former INFO: Inference done 1088/1092. Dataloading: 0.0055 s/iter. Inference: 0.2372 s/iter. Eval: 0.1144 s/iter. Total: 0.3572 s/iter. ETA=0:00:01
[02/18 13:26:12] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 13:26:13] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 13:26:13] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 13:26:27] mask2former INFO: Inference done 11/1092. Dataloading: 0.0038 s/iter. Inference: 0.2458 s/iter. Eval: 0.1408 s/iter. Total: 0.3904 s/iter. ETA=0:07:02
[02/18 13:26:32] mask2former INFO: Inference done 25/1092. Dataloading: 0.0047 s/iter. Inference: 0.2494 s/iter. Eval: 0.1147 s/iter. Total: 0.3689 s/iter. ETA=0:06:33
[02/18 13:26:37] mask2former INFO: Inference done 40/1092. Dataloading: 0.0046 s/iter. Inference: 0.2381 s/iter. Eval: 0.1144 s/iter. Total: 0.3572 s/iter. ETA=0:06:15
[02/18 13:26:42] mask2former INFO: Inference done 55/1092. Dataloading: 0.0046 s/iter. Inference: 0.2339 s/iter. Eval: 0.1121 s/iter. Total: 0.3507 s/iter. ETA=0:06:03
[02/18 13:26:47] mask2former INFO: Inference done 70/1092. Dataloading: 0.0050 s/iter. Inference: 0.2345 s/iter. Eval: 0.1121 s/iter. Total: 0.3516 s/iter. ETA=0:05:59
[02/18 13:26:53] mask2former INFO: Inference done 84/1092. Dataloading: 0.0051 s/iter. Inference: 0.2371 s/iter. Eval: 0.1126 s/iter. Total: 0.3548 s/iter. ETA=0:05:57
[02/18 13:26:58] mask2former INFO: Inference done 98/1092. Dataloading: 0.0050 s/iter. Inference: 0.2355 s/iter. Eval: 0.1147 s/iter. Total: 0.3554 s/iter. ETA=0:05:53
[02/18 13:27:03] mask2former INFO: Inference done 112/1092. Dataloading: 0.0050 s/iter. Inference: 0.2346 s/iter. Eval: 0.1159 s/iter. Total: 0.3557 s/iter. ETA=0:05:48
[02/18 13:27:08] mask2former INFO: Inference done 127/1092. Dataloading: 0.0051 s/iter. Inference: 0.2345 s/iter. Eval: 0.1161 s/iter. Total: 0.3558 s/iter. ETA=0:05:43
[02/18 13:27:13] mask2former INFO: Inference done 142/1092. Dataloading: 0.0053 s/iter. Inference: 0.2352 s/iter. Eval: 0.1154 s/iter. Total: 0.3560 s/iter. ETA=0:05:38
[02/18 13:27:18] mask2former INFO: Inference done 156/1092. Dataloading: 0.0052 s/iter. Inference: 0.2359 s/iter. Eval: 0.1153 s/iter. Total: 0.3565 s/iter. ETA=0:05:33
[02/18 13:27:24] mask2former INFO: Inference done 170/1092. Dataloading: 0.0053 s/iter. Inference: 0.2369 s/iter. Eval: 0.1155 s/iter. Total: 0.3578 s/iter. ETA=0:05:29
[02/18 13:27:29] mask2former INFO: Inference done 184/1092. Dataloading: 0.0053 s/iter. Inference: 0.2365 s/iter. Eval: 0.1159 s/iter. Total: 0.3578 s/iter. ETA=0:05:24
[02/18 13:27:34] mask2former INFO: Inference done 198/1092. Dataloading: 0.0054 s/iter. Inference: 0.2366 s/iter. Eval: 0.1158 s/iter. Total: 0.3579 s/iter. ETA=0:05:19
[02/18 13:27:39] mask2former INFO: Inference done 213/1092. Dataloading: 0.0054 s/iter. Inference: 0.2360 s/iter. Eval: 0.1157 s/iter. Total: 0.3572 s/iter. ETA=0:05:13
[02/18 13:27:44] mask2former INFO: Inference done 227/1092. Dataloading: 0.0054 s/iter. Inference: 0.2363 s/iter. Eval: 0.1155 s/iter. Total: 0.3573 s/iter. ETA=0:05:09
[02/18 13:27:49] mask2former INFO: Inference done 242/1092. Dataloading: 0.0054 s/iter. Inference: 0.2362 s/iter. Eval: 0.1158 s/iter. Total: 0.3575 s/iter. ETA=0:05:03
[02/18 13:27:54] mask2former INFO: Inference done 255/1092. Dataloading: 0.0055 s/iter. Inference: 0.2366 s/iter. Eval: 0.1170 s/iter. Total: 0.3592 s/iter. ETA=0:05:00
[02/18 13:27:59] mask2former INFO: Inference done 270/1092. Dataloading: 0.0054 s/iter. Inference: 0.2359 s/iter. Eval: 0.1167 s/iter. Total: 0.3582 s/iter. ETA=0:04:54
[02/18 13:28:05] mask2former INFO: Inference done 284/1092. Dataloading: 0.0054 s/iter. Inference: 0.2365 s/iter. Eval: 0.1163 s/iter. Total: 0.3583 s/iter. ETA=0:04:49
[02/18 13:28:10] mask2former INFO: Inference done 297/1092. Dataloading: 0.0055 s/iter. Inference: 0.2379 s/iter. Eval: 0.1172 s/iter. Total: 0.3606 s/iter. ETA=0:04:46
[02/18 13:28:15] mask2former INFO: Inference done 311/1092. Dataloading: 0.0055 s/iter. Inference: 0.2382 s/iter. Eval: 0.1171 s/iter. Total: 0.3609 s/iter. ETA=0:04:41
[02/18 13:28:20] mask2former INFO: Inference done 326/1092. Dataloading: 0.0055 s/iter. Inference: 0.2383 s/iter. Eval: 0.1164 s/iter. Total: 0.3602 s/iter. ETA=0:04:35
[02/18 13:28:25] mask2former INFO: Inference done 340/1092. Dataloading: 0.0054 s/iter. Inference: 0.2386 s/iter. Eval: 0.1163 s/iter. Total: 0.3604 s/iter. ETA=0:04:31
[02/18 13:28:30] mask2former INFO: Inference done 355/1092. Dataloading: 0.0054 s/iter. Inference: 0.2379 s/iter. Eval: 0.1162 s/iter. Total: 0.3596 s/iter. ETA=0:04:25
[02/18 13:28:36] mask2former INFO: Inference done 369/1092. Dataloading: 0.0054 s/iter. Inference: 0.2379 s/iter. Eval: 0.1165 s/iter. Total: 0.3600 s/iter. ETA=0:04:20
[02/18 13:28:41] mask2former INFO: Inference done 383/1092. Dataloading: 0.0055 s/iter. Inference: 0.2380 s/iter. Eval: 0.1170 s/iter. Total: 0.3605 s/iter. ETA=0:04:15
[02/18 13:28:46] mask2former INFO: Inference done 398/1092. Dataloading: 0.0055 s/iter. Inference: 0.2377 s/iter. Eval: 0.1167 s/iter. Total: 0.3601 s/iter. ETA=0:04:09
[02/18 13:28:51] mask2former INFO: Inference done 413/1092. Dataloading: 0.0055 s/iter. Inference: 0.2377 s/iter. Eval: 0.1161 s/iter. Total: 0.3594 s/iter. ETA=0:04:04
[02/18 13:28:56] mask2former INFO: Inference done 427/1092. Dataloading: 0.0055 s/iter. Inference: 0.2380 s/iter. Eval: 0.1164 s/iter. Total: 0.3600 s/iter. ETA=0:03:59
[02/18 13:29:02] mask2former INFO: Inference done 441/1092. Dataloading: 0.0055 s/iter. Inference: 0.2379 s/iter. Eval: 0.1168 s/iter. Total: 0.3603 s/iter. ETA=0:03:54
[02/18 13:29:07] mask2former INFO: Inference done 455/1092. Dataloading: 0.0055 s/iter. Inference: 0.2382 s/iter. Eval: 0.1165 s/iter. Total: 0.3603 s/iter. ETA=0:03:49
[02/18 13:29:12] mask2former INFO: Inference done 469/1092. Dataloading: 0.0055 s/iter. Inference: 0.2385 s/iter. Eval: 0.1163 s/iter. Total: 0.3604 s/iter. ETA=0:03:44
[02/18 13:29:17] mask2former INFO: Inference done 484/1092. Dataloading: 0.0055 s/iter. Inference: 0.2382 s/iter. Eval: 0.1163 s/iter. Total: 0.3601 s/iter. ETA=0:03:38
[02/18 13:29:22] mask2former INFO: Inference done 499/1092. Dataloading: 0.0055 s/iter. Inference: 0.2383 s/iter. Eval: 0.1160 s/iter. Total: 0.3598 s/iter. ETA=0:03:33
[02/18 13:29:28] mask2former INFO: Inference done 513/1092. Dataloading: 0.0055 s/iter. Inference: 0.2384 s/iter. Eval: 0.1164 s/iter. Total: 0.3604 s/iter. ETA=0:03:28
[02/18 13:29:33] mask2former INFO: Inference done 528/1092. Dataloading: 0.0055 s/iter. Inference: 0.2381 s/iter. Eval: 0.1163 s/iter. Total: 0.3600 s/iter. ETA=0:03:23
[02/18 13:29:38] mask2former INFO: Inference done 542/1092. Dataloading: 0.0055 s/iter. Inference: 0.2383 s/iter. Eval: 0.1163 s/iter. Total: 0.3602 s/iter. ETA=0:03:18
[02/18 13:29:43] mask2former INFO: Inference done 557/1092. Dataloading: 0.0054 s/iter. Inference: 0.2379 s/iter. Eval: 0.1162 s/iter. Total: 0.3596 s/iter. ETA=0:03:12
[02/18 13:29:48] mask2former INFO: Inference done 571/1092. Dataloading: 0.0054 s/iter. Inference: 0.2376 s/iter. Eval: 0.1166 s/iter. Total: 0.3598 s/iter. ETA=0:03:07
[02/18 13:29:53] mask2former INFO: Inference done 586/1092. Dataloading: 0.0055 s/iter. Inference: 0.2375 s/iter. Eval: 0.1165 s/iter. Total: 0.3596 s/iter. ETA=0:03:01
[02/18 13:29:59] mask2former INFO: Inference done 600/1092. Dataloading: 0.0055 s/iter. Inference: 0.2376 s/iter. Eval: 0.1166 s/iter. Total: 0.3598 s/iter. ETA=0:02:57
[02/18 13:30:04] mask2former INFO: Inference done 615/1092. Dataloading: 0.0055 s/iter. Inference: 0.2376 s/iter. Eval: 0.1163 s/iter. Total: 0.3595 s/iter. ETA=0:02:51
[02/18 13:30:09] mask2former INFO: Inference done 630/1092. Dataloading: 0.0055 s/iter. Inference: 0.2374 s/iter. Eval: 0.1162 s/iter. Total: 0.3592 s/iter. ETA=0:02:45
[02/18 13:30:14] mask2former INFO: Inference done 645/1092. Dataloading: 0.0054 s/iter. Inference: 0.2371 s/iter. Eval: 0.1162 s/iter. Total: 0.3589 s/iter. ETA=0:02:40
[02/18 13:30:19] mask2former INFO: Inference done 660/1092. Dataloading: 0.0055 s/iter. Inference: 0.2369 s/iter. Eval: 0.1159 s/iter. Total: 0.3583 s/iter. ETA=0:02:34
[02/18 13:30:24] mask2former INFO: Inference done 674/1092. Dataloading: 0.0055 s/iter. Inference: 0.2372 s/iter. Eval: 0.1158 s/iter. Total: 0.3585 s/iter. ETA=0:02:29
[02/18 13:30:29] mask2former INFO: Inference done 688/1092. Dataloading: 0.0055 s/iter. Inference: 0.2372 s/iter. Eval: 0.1158 s/iter. Total: 0.3586 s/iter. ETA=0:02:24
[02/18 13:30:35] mask2former INFO: Inference done 702/1092. Dataloading: 0.0054 s/iter. Inference: 0.2374 s/iter. Eval: 0.1158 s/iter. Total: 0.3587 s/iter. ETA=0:02:19
[02/18 13:30:40] mask2former INFO: Inference done 718/1092. Dataloading: 0.0054 s/iter. Inference: 0.2369 s/iter. Eval: 0.1155 s/iter. Total: 0.3580 s/iter. ETA=0:02:13
[02/18 13:30:45] mask2former INFO: Inference done 732/1092. Dataloading: 0.0054 s/iter. Inference: 0.2369 s/iter. Eval: 0.1157 s/iter. Total: 0.3581 s/iter. ETA=0:02:08
[02/18 13:30:50] mask2former INFO: Inference done 747/1092. Dataloading: 0.0054 s/iter. Inference: 0.2370 s/iter. Eval: 0.1156 s/iter. Total: 0.3581 s/iter. ETA=0:02:03
[02/18 13:30:56] mask2former INFO: Inference done 761/1092. Dataloading: 0.0054 s/iter. Inference: 0.2373 s/iter. Eval: 0.1156 s/iter. Total: 0.3584 s/iter. ETA=0:01:58
[02/18 13:31:01] mask2former INFO: Inference done 776/1092. Dataloading: 0.0054 s/iter. Inference: 0.2373 s/iter. Eval: 0.1157 s/iter. Total: 0.3585 s/iter. ETA=0:01:53
[02/18 13:31:06] mask2former INFO: Inference done 791/1092. Dataloading: 0.0054 s/iter. Inference: 0.2375 s/iter. Eval: 0.1155 s/iter. Total: 0.3584 s/iter. ETA=0:01:47
[02/18 13:31:11] mask2former INFO: Inference done 805/1092. Dataloading: 0.0054 s/iter. Inference: 0.2377 s/iter. Eval: 0.1154 s/iter. Total: 0.3585 s/iter. ETA=0:01:42
[02/18 13:31:16] mask2former INFO: Inference done 819/1092. Dataloading: 0.0054 s/iter. Inference: 0.2378 s/iter. Eval: 0.1152 s/iter. Total: 0.3585 s/iter. ETA=0:01:37
[02/18 13:31:21] mask2former INFO: Inference done 833/1092. Dataloading: 0.0054 s/iter. Inference: 0.2378 s/iter. Eval: 0.1153 s/iter. Total: 0.3586 s/iter. ETA=0:01:32
[02/18 13:31:27] mask2former INFO: Inference done 848/1092. Dataloading: 0.0054 s/iter. Inference: 0.2374 s/iter. Eval: 0.1155 s/iter. Total: 0.3584 s/iter. ETA=0:01:27
[02/18 13:31:32] mask2former INFO: Inference done 862/1092. Dataloading: 0.0054 s/iter. Inference: 0.2373 s/iter. Eval: 0.1158 s/iter. Total: 0.3586 s/iter. ETA=0:01:22
[02/18 13:31:37] mask2former INFO: Inference done 876/1092. Dataloading: 0.0054 s/iter. Inference: 0.2374 s/iter. Eval: 0.1157 s/iter. Total: 0.3586 s/iter. ETA=0:01:17
[02/18 13:31:42] mask2former INFO: Inference done 890/1092. Dataloading: 0.0054 s/iter. Inference: 0.2375 s/iter. Eval: 0.1157 s/iter. Total: 0.3586 s/iter. ETA=0:01:12
[02/18 13:31:47] mask2former INFO: Inference done 905/1092. Dataloading: 0.0054 s/iter. Inference: 0.2373 s/iter. Eval: 0.1156 s/iter. Total: 0.3585 s/iter. ETA=0:01:07
[02/18 13:31:52] mask2former INFO: Inference done 919/1092. Dataloading: 0.0054 s/iter. Inference: 0.2374 s/iter. Eval: 0.1157 s/iter. Total: 0.3586 s/iter. ETA=0:01:02
[02/18 13:31:58] mask2former INFO: Inference done 933/1092. Dataloading: 0.0054 s/iter. Inference: 0.2376 s/iter. Eval: 0.1158 s/iter. Total: 0.3589 s/iter. ETA=0:00:57
[02/18 13:32:03] mask2former INFO: Inference done 947/1092. Dataloading: 0.0054 s/iter. Inference: 0.2379 s/iter. Eval: 0.1158 s/iter. Total: 0.3591 s/iter. ETA=0:00:52
[02/18 13:32:08] mask2former INFO: Inference done 962/1092. Dataloading: 0.0054 s/iter. Inference: 0.2377 s/iter. Eval: 0.1157 s/iter. Total: 0.3589 s/iter. ETA=0:00:46
[02/18 13:32:13] mask2former INFO: Inference done 977/1092. Dataloading: 0.0054 s/iter. Inference: 0.2376 s/iter. Eval: 0.1156 s/iter. Total: 0.3586 s/iter. ETA=0:00:41
[02/18 13:32:18] mask2former INFO: Inference done 991/1092. Dataloading: 0.0054 s/iter. Inference: 0.2375 s/iter. Eval: 0.1158 s/iter. Total: 0.3588 s/iter. ETA=0:00:36
[02/18 13:32:24] mask2former INFO: Inference done 1005/1092. Dataloading: 0.0054 s/iter. Inference: 0.2377 s/iter. Eval: 0.1158 s/iter. Total: 0.3590 s/iter. ETA=0:00:31
[02/18 13:32:29] mask2former INFO: Inference done 1019/1092. Dataloading: 0.0054 s/iter. Inference: 0.2375 s/iter. Eval: 0.1161 s/iter. Total: 0.3591 s/iter. ETA=0:00:26
[02/18 13:32:34] mask2former INFO: Inference done 1034/1092. Dataloading: 0.0054 s/iter. Inference: 0.2376 s/iter. Eval: 0.1159 s/iter. Total: 0.3590 s/iter. ETA=0:00:20
[02/18 13:32:39] mask2former INFO: Inference done 1048/1092. Dataloading: 0.0054 s/iter. Inference: 0.2375 s/iter. Eval: 0.1159 s/iter. Total: 0.3590 s/iter. ETA=0:00:15
[02/18 13:32:44] mask2former INFO: Inference done 1063/1092. Dataloading: 0.0054 s/iter. Inference: 0.2374 s/iter. Eval: 0.1158 s/iter. Total: 0.3588 s/iter. ETA=0:00:10
[02/18 13:32:49] mask2former INFO: Inference done 1078/1092. Dataloading: 0.0054 s/iter. Inference: 0.2375 s/iter. Eval: 0.1158 s/iter. Total: 0.3587 s/iter. ETA=0:00:05
[02/18 13:32:55] mask2former INFO: Inference done 1092/1092. Dataloading: 0.0054 s/iter. Inference: 0.2376 s/iter. Eval: 0.1159 s/iter. Total: 0.3590 s/iter. ETA=0:00:00
[02/18 15:06:28] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 15:06:29] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 15:06:29] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 15:06:45] mask2former INFO: Inference done 11/1092. Dataloading: 0.0032 s/iter. Inference: 0.3576 s/iter. Eval: 0.1114 s/iter. Total: 0.4722 s/iter. ETA=0:08:30
[02/18 15:06:50] mask2former INFO: Inference done 22/1092. Dataloading: 0.0042 s/iter. Inference: 0.3585 s/iter. Eval: 0.1242 s/iter. Total: 0.4870 s/iter. ETA=0:08:41
[02/18 15:06:56] mask2former INFO: Inference done 34/1092. Dataloading: 0.0044 s/iter. Inference: 0.3535 s/iter. Eval: 0.1152 s/iter. Total: 0.4731 s/iter. ETA=0:08:20
[02/18 15:07:01] mask2former INFO: Inference done 45/1092. Dataloading: 0.0046 s/iter. Inference: 0.3531 s/iter. Eval: 0.1218 s/iter. Total: 0.4795 s/iter. ETA=0:08:22
[02/18 15:07:07] mask2former INFO: Inference done 56/1092. Dataloading: 0.0048 s/iter. Inference: 0.3535 s/iter. Eval: 0.1220 s/iter. Total: 0.4804 s/iter. ETA=0:08:17
[02/18 15:07:12] mask2former INFO: Inference done 66/1092. Dataloading: 0.0048 s/iter. Inference: 0.3552 s/iter. Eval: 0.1235 s/iter. Total: 0.4837 s/iter. ETA=0:08:16
[02/18 15:07:17] mask2former INFO: Inference done 76/1092. Dataloading: 0.0049 s/iter. Inference: 0.3583 s/iter. Eval: 0.1239 s/iter. Total: 0.4872 s/iter. ETA=0:08:14
[02/18 15:07:22] mask2former INFO: Inference done 86/1092. Dataloading: 0.0049 s/iter. Inference: 0.3622 s/iter. Eval: 0.1233 s/iter. Total: 0.4904 s/iter. ETA=0:08:13
[02/18 15:07:27] mask2former INFO: Inference done 97/1092. Dataloading: 0.0052 s/iter. Inference: 0.3629 s/iter. Eval: 0.1219 s/iter. Total: 0.4901 s/iter. ETA=0:08:07
[02/18 15:07:33] mask2former INFO: Inference done 108/1092. Dataloading: 0.0059 s/iter. Inference: 0.3625 s/iter. Eval: 0.1221 s/iter. Total: 0.4906 s/iter. ETA=0:08:02
[02/18 15:07:38] mask2former INFO: Inference done 119/1092. Dataloading: 0.0058 s/iter. Inference: 0.3620 s/iter. Eval: 0.1227 s/iter. Total: 0.4906 s/iter. ETA=0:07:57
[02/18 15:07:43] mask2former INFO: Inference done 130/1092. Dataloading: 0.0058 s/iter. Inference: 0.3610 s/iter. Eval: 0.1228 s/iter. Total: 0.4898 s/iter. ETA=0:07:51
[02/18 15:07:49] mask2former INFO: Inference done 141/1092. Dataloading: 0.0057 s/iter. Inference: 0.3602 s/iter. Eval: 0.1220 s/iter. Total: 0.4880 s/iter. ETA=0:07:44
[02/18 15:07:54] mask2former INFO: Inference done 152/1092. Dataloading: 0.0057 s/iter. Inference: 0.3600 s/iter. Eval: 0.1222 s/iter. Total: 0.4879 s/iter. ETA=0:07:38
[02/18 15:07:59] mask2former INFO: Inference done 163/1092. Dataloading: 0.0055 s/iter. Inference: 0.3601 s/iter. Eval: 0.1203 s/iter. Total: 0.4861 s/iter. ETA=0:07:31
[02/18 15:08:04] mask2former INFO: Inference done 174/1092. Dataloading: 0.0055 s/iter. Inference: 0.3594 s/iter. Eval: 0.1210 s/iter. Total: 0.4860 s/iter. ETA=0:07:26
[02/18 15:08:10] mask2former INFO: Inference done 185/1092. Dataloading: 0.0054 s/iter. Inference: 0.3593 s/iter. Eval: 0.1215 s/iter. Total: 0.4863 s/iter. ETA=0:07:21
[02/18 15:08:15] mask2former INFO: Inference done 195/1092. Dataloading: 0.0054 s/iter. Inference: 0.3600 s/iter. Eval: 0.1222 s/iter. Total: 0.4877 s/iter. ETA=0:07:17
[02/18 15:08:20] mask2former INFO: Inference done 206/1092. Dataloading: 0.0054 s/iter. Inference: 0.3603 s/iter. Eval: 0.1219 s/iter. Total: 0.4877 s/iter. ETA=0:07:12
[02/18 15:08:25] mask2former INFO: Inference done 217/1092. Dataloading: 0.0053 s/iter. Inference: 0.3607 s/iter. Eval: 0.1209 s/iter. Total: 0.4870 s/iter. ETA=0:07:06
[02/18 15:08:31] mask2former INFO: Inference done 228/1092. Dataloading: 0.0053 s/iter. Inference: 0.3605 s/iter. Eval: 0.1208 s/iter. Total: 0.4867 s/iter. ETA=0:07:00
[02/18 15:08:36] mask2former INFO: Inference done 239/1092. Dataloading: 0.0053 s/iter. Inference: 0.3604 s/iter. Eval: 0.1198 s/iter. Total: 0.4856 s/iter. ETA=0:06:54
[02/18 15:08:41] mask2former INFO: Inference done 250/1092. Dataloading: 0.0054 s/iter. Inference: 0.3604 s/iter. Eval: 0.1199 s/iter. Total: 0.4858 s/iter. ETA=0:06:49
[02/18 15:08:46] mask2former INFO: Inference done 261/1092. Dataloading: 0.0054 s/iter. Inference: 0.3605 s/iter. Eval: 0.1198 s/iter. Total: 0.4858 s/iter. ETA=0:06:43
[02/18 15:08:52] mask2former INFO: Inference done 272/1092. Dataloading: 0.0054 s/iter. Inference: 0.3598 s/iter. Eval: 0.1193 s/iter. Total: 0.4846 s/iter. ETA=0:06:37
[02/18 15:08:57] mask2former INFO: Inference done 283/1092. Dataloading: 0.0054 s/iter. Inference: 0.3593 s/iter. Eval: 0.1197 s/iter. Total: 0.4845 s/iter. ETA=0:06:31
[02/18 15:09:02] mask2former INFO: Inference done 294/1092. Dataloading: 0.0054 s/iter. Inference: 0.3598 s/iter. Eval: 0.1196 s/iter. Total: 0.4848 s/iter. ETA=0:06:26
[02/18 15:09:08] mask2former INFO: Inference done 305/1092. Dataloading: 0.0054 s/iter. Inference: 0.3595 s/iter. Eval: 0.1197 s/iter. Total: 0.4847 s/iter. ETA=0:06:21
[02/18 15:09:13] mask2former INFO: Inference done 316/1092. Dataloading: 0.0054 s/iter. Inference: 0.3588 s/iter. Eval: 0.1198 s/iter. Total: 0.4842 s/iter. ETA=0:06:15
[02/18 15:09:18] mask2former INFO: Inference done 327/1092. Dataloading: 0.0055 s/iter. Inference: 0.3590 s/iter. Eval: 0.1197 s/iter. Total: 0.4843 s/iter. ETA=0:06:10
[02/18 15:09:23] mask2former INFO: Inference done 338/1092. Dataloading: 0.0054 s/iter. Inference: 0.3593 s/iter. Eval: 0.1192 s/iter. Total: 0.4841 s/iter. ETA=0:06:04
[02/18 15:09:29] mask2former INFO: Inference done 349/1092. Dataloading: 0.0054 s/iter. Inference: 0.3594 s/iter. Eval: 0.1191 s/iter. Total: 0.4841 s/iter. ETA=0:05:59
[02/18 15:09:34] mask2former INFO: Inference done 360/1092. Dataloading: 0.0055 s/iter. Inference: 0.3600 s/iter. Eval: 0.1187 s/iter. Total: 0.4843 s/iter. ETA=0:05:54
[02/18 15:09:39] mask2former INFO: Inference done 371/1092. Dataloading: 0.0055 s/iter. Inference: 0.3602 s/iter. Eval: 0.1183 s/iter. Total: 0.4841 s/iter. ETA=0:05:49
[02/18 15:09:45] mask2former INFO: Inference done 382/1092. Dataloading: 0.0055 s/iter. Inference: 0.3603 s/iter. Eval: 0.1185 s/iter. Total: 0.4844 s/iter. ETA=0:05:43
[02/18 15:09:50] mask2former INFO: Inference done 393/1092. Dataloading: 0.0055 s/iter. Inference: 0.3600 s/iter. Eval: 0.1185 s/iter. Total: 0.4841 s/iter. ETA=0:05:38
[02/18 15:09:55] mask2former INFO: Inference done 404/1092. Dataloading: 0.0054 s/iter. Inference: 0.3599 s/iter. Eval: 0.1183 s/iter. Total: 0.4838 s/iter. ETA=0:05:32
[02/18 15:10:00] mask2former INFO: Inference done 415/1092. Dataloading: 0.0055 s/iter. Inference: 0.3603 s/iter. Eval: 0.1179 s/iter. Total: 0.4837 s/iter. ETA=0:05:27
[02/18 15:10:06] mask2former INFO: Inference done 426/1092. Dataloading: 0.0055 s/iter. Inference: 0.3601 s/iter. Eval: 0.1185 s/iter. Total: 0.4842 s/iter. ETA=0:05:22
[02/18 15:10:12] mask2former INFO: Inference done 438/1092. Dataloading: 0.0055 s/iter. Inference: 0.3599 s/iter. Eval: 0.1182 s/iter. Total: 0.4837 s/iter. ETA=0:05:16
[02/18 15:10:17] mask2former INFO: Inference done 449/1092. Dataloading: 0.0055 s/iter. Inference: 0.3602 s/iter. Eval: 0.1181 s/iter. Total: 0.4838 s/iter. ETA=0:05:11
[02/18 15:10:22] mask2former INFO: Inference done 460/1092. Dataloading: 0.0055 s/iter. Inference: 0.3606 s/iter. Eval: 0.1175 s/iter. Total: 0.4837 s/iter. ETA=0:05:05
[02/18 15:10:28] mask2former INFO: Inference done 471/1092. Dataloading: 0.0055 s/iter. Inference: 0.3606 s/iter. Eval: 0.1175 s/iter. Total: 0.4836 s/iter. ETA=0:05:00
[02/18 15:10:33] mask2former INFO: Inference done 481/1092. Dataloading: 0.0054 s/iter. Inference: 0.3607 s/iter. Eval: 0.1178 s/iter. Total: 0.4841 s/iter. ETA=0:04:55
[02/18 15:10:38] mask2former INFO: Inference done 492/1092. Dataloading: 0.0054 s/iter. Inference: 0.3605 s/iter. Eval: 0.1173 s/iter. Total: 0.4834 s/iter. ETA=0:04:50
[02/18 15:10:43] mask2former INFO: Inference done 503/1092. Dataloading: 0.0054 s/iter. Inference: 0.3607 s/iter. Eval: 0.1171 s/iter. Total: 0.4833 s/iter. ETA=0:04:44
[02/18 15:10:48] mask2former INFO: Inference done 513/1092. Dataloading: 0.0054 s/iter. Inference: 0.3608 s/iter. Eval: 0.1175 s/iter. Total: 0.4838 s/iter. ETA=0:04:40
[02/18 15:10:53] mask2former INFO: Inference done 523/1092. Dataloading: 0.0054 s/iter. Inference: 0.3611 s/iter. Eval: 0.1178 s/iter. Total: 0.4844 s/iter. ETA=0:04:35
[02/18 15:10:58] mask2former INFO: Inference done 534/1092. Dataloading: 0.0054 s/iter. Inference: 0.3612 s/iter. Eval: 0.1177 s/iter. Total: 0.4844 s/iter. ETA=0:04:30
[02/18 15:11:03] mask2former INFO: Inference done 545/1092. Dataloading: 0.0054 s/iter. Inference: 0.3609 s/iter. Eval: 0.1174 s/iter. Total: 0.4838 s/iter. ETA=0:04:24
[02/18 15:11:09] mask2former INFO: Inference done 556/1092. Dataloading: 0.0054 s/iter. Inference: 0.3608 s/iter. Eval: 0.1172 s/iter. Total: 0.4835 s/iter. ETA=0:04:19
[02/18 15:11:14] mask2former INFO: Inference done 567/1092. Dataloading: 0.0053 s/iter. Inference: 0.3606 s/iter. Eval: 0.1173 s/iter. Total: 0.4834 s/iter. ETA=0:04:13
[02/18 15:11:19] mask2former INFO: Inference done 578/1092. Dataloading: 0.0054 s/iter. Inference: 0.3605 s/iter. Eval: 0.1176 s/iter. Total: 0.4836 s/iter. ETA=0:04:08
[02/18 15:11:25] mask2former INFO: Inference done 589/1092. Dataloading: 0.0054 s/iter. Inference: 0.3605 s/iter. Eval: 0.1174 s/iter. Total: 0.4835 s/iter. ETA=0:04:03
[02/18 15:11:30] mask2former INFO: Inference done 600/1092. Dataloading: 0.0054 s/iter. Inference: 0.3604 s/iter. Eval: 0.1175 s/iter. Total: 0.4835 s/iter. ETA=0:03:57
[02/18 15:11:35] mask2former INFO: Inference done 611/1092. Dataloading: 0.0055 s/iter. Inference: 0.3604 s/iter. Eval: 0.1174 s/iter. Total: 0.4834 s/iter. ETA=0:03:52
[02/18 15:11:41] mask2former INFO: Inference done 622/1092. Dataloading: 0.0055 s/iter. Inference: 0.3606 s/iter. Eval: 0.1174 s/iter. Total: 0.4836 s/iter. ETA=0:03:47
[02/18 15:11:46] mask2former INFO: Inference done 632/1092. Dataloading: 0.0055 s/iter. Inference: 0.3608 s/iter. Eval: 0.1176 s/iter. Total: 0.4840 s/iter. ETA=0:03:42
[02/18 15:11:51] mask2former INFO: Inference done 642/1092. Dataloading: 0.0055 s/iter. Inference: 0.3610 s/iter. Eval: 0.1177 s/iter. Total: 0.4843 s/iter. ETA=0:03:37
[02/18 15:11:56] mask2former INFO: Inference done 653/1092. Dataloading: 0.0054 s/iter. Inference: 0.3609 s/iter. Eval: 0.1176 s/iter. Total: 0.4841 s/iter. ETA=0:03:32
[02/18 15:12:01] mask2former INFO: Inference done 664/1092. Dataloading: 0.0054 s/iter. Inference: 0.3608 s/iter. Eval: 0.1178 s/iter. Total: 0.4841 s/iter. ETA=0:03:27
[02/18 15:12:07] mask2former INFO: Inference done 675/1092. Dataloading: 0.0054 s/iter. Inference: 0.3611 s/iter. Eval: 0.1177 s/iter. Total: 0.4844 s/iter. ETA=0:03:21
[02/18 15:12:12] mask2former INFO: Inference done 686/1092. Dataloading: 0.0054 s/iter. Inference: 0.3614 s/iter. Eval: 0.1174 s/iter. Total: 0.4843 s/iter. ETA=0:03:16
[02/18 15:12:17] mask2former INFO: Inference done 696/1092. Dataloading: 0.0054 s/iter. Inference: 0.3613 s/iter. Eval: 0.1179 s/iter. Total: 0.4847 s/iter. ETA=0:03:11
[02/18 15:12:22] mask2former INFO: Inference done 707/1092. Dataloading: 0.0054 s/iter. Inference: 0.3612 s/iter. Eval: 0.1179 s/iter. Total: 0.4847 s/iter. ETA=0:03:06
[02/18 15:12:28] mask2former INFO: Inference done 717/1092. Dataloading: 0.0054 s/iter. Inference: 0.3611 s/iter. Eval: 0.1186 s/iter. Total: 0.4853 s/iter. ETA=0:03:01
[02/18 15:12:33] mask2former INFO: Inference done 728/1092. Dataloading: 0.0054 s/iter. Inference: 0.3611 s/iter. Eval: 0.1187 s/iter. Total: 0.4853 s/iter. ETA=0:02:56
[02/18 15:12:38] mask2former INFO: Inference done 740/1092. Dataloading: 0.0054 s/iter. Inference: 0.3608 s/iter. Eval: 0.1184 s/iter. Total: 0.4848 s/iter. ETA=0:02:50
[02/18 15:12:44] mask2former INFO: Inference done 752/1092. Dataloading: 0.0054 s/iter. Inference: 0.3607 s/iter. Eval: 0.1183 s/iter. Total: 0.4845 s/iter. ETA=0:02:44
[02/18 15:12:50] mask2former INFO: Inference done 763/1092. Dataloading: 0.0054 s/iter. Inference: 0.3608 s/iter. Eval: 0.1184 s/iter. Total: 0.4848 s/iter. ETA=0:02:39
[02/18 15:12:55] mask2former INFO: Inference done 774/1092. Dataloading: 0.0054 s/iter. Inference: 0.3607 s/iter. Eval: 0.1185 s/iter. Total: 0.4847 s/iter. ETA=0:02:34
[02/18 15:13:00] mask2former INFO: Inference done 785/1092. Dataloading: 0.0054 s/iter. Inference: 0.3606 s/iter. Eval: 0.1186 s/iter. Total: 0.4847 s/iter. ETA=0:02:28
[02/18 15:13:05] mask2former INFO: Inference done 796/1092. Dataloading: 0.0054 s/iter. Inference: 0.3607 s/iter. Eval: 0.1183 s/iter. Total: 0.4845 s/iter. ETA=0:02:23
[02/18 15:13:11] mask2former INFO: Inference done 806/1092. Dataloading: 0.0054 s/iter. Inference: 0.3610 s/iter. Eval: 0.1183 s/iter. Total: 0.4849 s/iter. ETA=0:02:18
[02/18 15:13:16] mask2former INFO: Inference done 817/1092. Dataloading: 0.0054 s/iter. Inference: 0.3613 s/iter. Eval: 0.1182 s/iter. Total: 0.4850 s/iter. ETA=0:02:13
[02/18 15:13:21] mask2former INFO: Inference done 827/1092. Dataloading: 0.0054 s/iter. Inference: 0.3615 s/iter. Eval: 0.1184 s/iter. Total: 0.4854 s/iter. ETA=0:02:08
[02/18 15:13:27] mask2former INFO: Inference done 839/1092. Dataloading: 0.0054 s/iter. Inference: 0.3612 s/iter. Eval: 0.1182 s/iter. Total: 0.4849 s/iter. ETA=0:02:02
[02/18 15:13:32] mask2former INFO: Inference done 850/1092. Dataloading: 0.0054 s/iter. Inference: 0.3611 s/iter. Eval: 0.1182 s/iter. Total: 0.4849 s/iter. ETA=0:01:57
[02/18 15:13:37] mask2former INFO: Inference done 861/1092. Dataloading: 0.0054 s/iter. Inference: 0.3612 s/iter. Eval: 0.1180 s/iter. Total: 0.4847 s/iter. ETA=0:01:51
[02/18 15:13:43] mask2former INFO: Inference done 872/1092. Dataloading: 0.0054 s/iter. Inference: 0.3611 s/iter. Eval: 0.1183 s/iter. Total: 0.4849 s/iter. ETA=0:01:46
[02/18 15:13:48] mask2former INFO: Inference done 883/1092. Dataloading: 0.0054 s/iter. Inference: 0.3609 s/iter. Eval: 0.1182 s/iter. Total: 0.4846 s/iter. ETA=0:01:41
[02/18 15:13:53] mask2former INFO: Inference done 894/1092. Dataloading: 0.0054 s/iter. Inference: 0.3608 s/iter. Eval: 0.1183 s/iter. Total: 0.4847 s/iter. ETA=0:01:35
[02/18 15:13:58] mask2former INFO: Inference done 904/1092. Dataloading: 0.0054 s/iter. Inference: 0.3609 s/iter. Eval: 0.1184 s/iter. Total: 0.4849 s/iter. ETA=0:01:31
[02/18 15:14:03] mask2former INFO: Inference done 915/1092. Dataloading: 0.0054 s/iter. Inference: 0.3607 s/iter. Eval: 0.1185 s/iter. Total: 0.4848 s/iter. ETA=0:01:25
[02/18 15:14:08] mask2former INFO: Inference done 926/1092. Dataloading: 0.0054 s/iter. Inference: 0.3604 s/iter. Eval: 0.1185 s/iter. Total: 0.4845 s/iter. ETA=0:01:20
[02/18 15:14:13] mask2former INFO: Inference done 937/1092. Dataloading: 0.0054 s/iter. Inference: 0.3603 s/iter. Eval: 0.1184 s/iter. Total: 0.4842 s/iter. ETA=0:01:15
[02/18 15:14:19] mask2former INFO: Inference done 948/1092. Dataloading: 0.0054 s/iter. Inference: 0.3604 s/iter. Eval: 0.1183 s/iter. Total: 0.4842 s/iter. ETA=0:01:09
[02/18 15:14:24] mask2former INFO: Inference done 959/1092. Dataloading: 0.0054 s/iter. Inference: 0.3602 s/iter. Eval: 0.1183 s/iter. Total: 0.4840 s/iter. ETA=0:01:04
[02/18 15:14:29] mask2former INFO: Inference done 973/1092. Dataloading: 0.0054 s/iter. Inference: 0.3585 s/iter. Eval: 0.1183 s/iter. Total: 0.4823 s/iter. ETA=0:00:57
[02/18 15:14:34] mask2former INFO: Inference done 987/1092. Dataloading: 0.0054 s/iter. Inference: 0.3569 s/iter. Eval: 0.1181 s/iter. Total: 0.4805 s/iter. ETA=0:00:50
[02/18 15:14:39] mask2former INFO: Inference done 1001/1092. Dataloading: 0.0054 s/iter. Inference: 0.3553 s/iter. Eval: 0.1180 s/iter. Total: 0.4788 s/iter. ETA=0:00:43
[02/18 15:14:44] mask2former INFO: Inference done 1016/1092. Dataloading: 0.0054 s/iter. Inference: 0.3535 s/iter. Eval: 0.1179 s/iter. Total: 0.4769 s/iter. ETA=0:00:36
[02/18 15:14:50] mask2former INFO: Inference done 1031/1092. Dataloading: 0.0054 s/iter. Inference: 0.3518 s/iter. Eval: 0.1178 s/iter. Total: 0.4752 s/iter. ETA=0:00:28
[02/18 15:14:55] mask2former INFO: Inference done 1046/1092. Dataloading: 0.0054 s/iter. Inference: 0.3500 s/iter. Eval: 0.1178 s/iter. Total: 0.4734 s/iter. ETA=0:00:21
[02/18 15:15:00] mask2former INFO: Inference done 1059/1092. Dataloading: 0.0054 s/iter. Inference: 0.3488 s/iter. Eval: 0.1180 s/iter. Total: 0.4723 s/iter. ETA=0:00:15
[02/18 15:15:05] mask2former INFO: Inference done 1074/1092. Dataloading: 0.0054 s/iter. Inference: 0.3470 s/iter. Eval: 0.1179 s/iter. Total: 0.4704 s/iter. ETA=0:00:08
[02/18 15:15:10] mask2former INFO: Inference done 1089/1092. Dataloading: 0.0054 s/iter. Inference: 0.3455 s/iter. Eval: 0.1178 s/iter. Total: 0.4688 s/iter. ETA=0:00:01
[02/18 17:22:57] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/18 17:22:58] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/18 17:22:58] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/18 17:23:12] mask2former INFO: Inference done 11/1092. Dataloading: 0.0030 s/iter. Inference: 0.3526 s/iter. Eval: 0.1081 s/iter. Total: 0.4638 s/iter. ETA=0:08:21
[02/18 17:23:18] mask2former INFO: Inference done 22/1092. Dataloading: 0.0038 s/iter. Inference: 0.3646 s/iter. Eval: 0.1124 s/iter. Total: 0.4809 s/iter. ETA=0:08:34
[02/18 17:23:23] mask2former INFO: Inference done 32/1092. Dataloading: 0.0056 s/iter. Inference: 0.3735 s/iter. Eval: 0.1142 s/iter. Total: 0.4934 s/iter. ETA=0:08:43
[02/18 17:23:28] mask2former INFO: Inference done 43/1092. Dataloading: 0.0053 s/iter. Inference: 0.3640 s/iter. Eval: 0.1189 s/iter. Total: 0.4883 s/iter. ETA=0:08:32
[02/18 17:23:33] mask2former INFO: Inference done 54/1092. Dataloading: 0.0051 s/iter. Inference: 0.3617 s/iter. Eval: 0.1155 s/iter. Total: 0.4823 s/iter. ETA=0:08:20
[02/18 17:23:39] mask2former INFO: Inference done 65/1092. Dataloading: 0.0050 s/iter. Inference: 0.3631 s/iter. Eval: 0.1163 s/iter. Total: 0.4845 s/iter. ETA=0:08:17
[02/18 17:23:44] mask2former INFO: Inference done 76/1092. Dataloading: 0.0049 s/iter. Inference: 0.3643 s/iter. Eval: 0.1168 s/iter. Total: 0.4861 s/iter. ETA=0:08:13
[02/18 17:23:50] mask2former INFO: Inference done 87/1092. Dataloading: 0.0048 s/iter. Inference: 0.3656 s/iter. Eval: 0.1157 s/iter. Total: 0.4862 s/iter. ETA=0:08:08
[02/18 17:23:55] mask2former INFO: Inference done 98/1092. Dataloading: 0.0048 s/iter. Inference: 0.3645 s/iter. Eval: 0.1157 s/iter. Total: 0.4851 s/iter. ETA=0:08:02
[02/18 17:24:00] mask2former INFO: Inference done 109/1092. Dataloading: 0.0048 s/iter. Inference: 0.3639 s/iter. Eval: 0.1166 s/iter. Total: 0.4853 s/iter. ETA=0:07:57
[02/18 17:24:05] mask2former INFO: Inference done 119/1092. Dataloading: 0.0050 s/iter. Inference: 0.3641 s/iter. Eval: 0.1184 s/iter. Total: 0.4875 s/iter. ETA=0:07:54
[02/18 17:24:11] mask2former INFO: Inference done 130/1092. Dataloading: 0.0050 s/iter. Inference: 0.3635 s/iter. Eval: 0.1179 s/iter. Total: 0.4864 s/iter. ETA=0:07:47
[02/18 17:24:16] mask2former INFO: Inference done 141/1092. Dataloading: 0.0049 s/iter. Inference: 0.3642 s/iter. Eval: 0.1169 s/iter. Total: 0.4861 s/iter. ETA=0:07:42
[02/18 17:24:21] mask2former INFO: Inference done 152/1092. Dataloading: 0.0050 s/iter. Inference: 0.3641 s/iter. Eval: 0.1169 s/iter. Total: 0.4860 s/iter. ETA=0:07:36
[02/18 17:24:26] mask2former INFO: Inference done 163/1092. Dataloading: 0.0049 s/iter. Inference: 0.3632 s/iter. Eval: 0.1174 s/iter. Total: 0.4857 s/iter. ETA=0:07:31
[02/18 17:24:32] mask2former INFO: Inference done 174/1092. Dataloading: 0.0050 s/iter. Inference: 0.3634 s/iter. Eval: 0.1180 s/iter. Total: 0.4865 s/iter. ETA=0:07:26
[02/18 17:24:37] mask2former INFO: Inference done 184/1092. Dataloading: 0.0050 s/iter. Inference: 0.3647 s/iter. Eval: 0.1181 s/iter. Total: 0.4879 s/iter. ETA=0:07:23
[02/18 17:24:42] mask2former INFO: Inference done 194/1092. Dataloading: 0.0050 s/iter. Inference: 0.3648 s/iter. Eval: 0.1187 s/iter. Total: 0.4886 s/iter. ETA=0:07:18
[02/18 17:24:47] mask2former INFO: Inference done 205/1092. Dataloading: 0.0051 s/iter. Inference: 0.3642 s/iter. Eval: 0.1182 s/iter. Total: 0.4875 s/iter. ETA=0:07:12
[02/18 17:24:52] mask2former INFO: Inference done 216/1092. Dataloading: 0.0050 s/iter. Inference: 0.3638 s/iter. Eval: 0.1178 s/iter. Total: 0.4867 s/iter. ETA=0:07:06
[02/18 17:24:58] mask2former INFO: Inference done 227/1092. Dataloading: 0.0050 s/iter. Inference: 0.3633 s/iter. Eval: 0.1174 s/iter. Total: 0.4858 s/iter. ETA=0:07:00
[02/18 17:25:03] mask2former INFO: Inference done 238/1092. Dataloading: 0.0051 s/iter. Inference: 0.3630 s/iter. Eval: 0.1177 s/iter. Total: 0.4858 s/iter. ETA=0:06:54
[02/18 17:25:08] mask2former INFO: Inference done 249/1092. Dataloading: 0.0051 s/iter. Inference: 0.3629 s/iter. Eval: 0.1177 s/iter. Total: 0.4858 s/iter. ETA=0:06:49
[02/18 17:25:13] mask2former INFO: Inference done 259/1092. Dataloading: 0.0051 s/iter. Inference: 0.3629 s/iter. Eval: 0.1183 s/iter. Total: 0.4863 s/iter. ETA=0:06:45
[02/18 17:25:18] mask2former INFO: Inference done 270/1092. Dataloading: 0.0051 s/iter. Inference: 0.3624 s/iter. Eval: 0.1179 s/iter. Total: 0.4854 s/iter. ETA=0:06:39
[02/18 17:25:24] mask2former INFO: Inference done 281/1092. Dataloading: 0.0052 s/iter. Inference: 0.3619 s/iter. Eval: 0.1178 s/iter. Total: 0.4850 s/iter. ETA=0:06:33
[02/18 17:25:29] mask2former INFO: Inference done 292/1092. Dataloading: 0.0052 s/iter. Inference: 0.3622 s/iter. Eval: 0.1179 s/iter. Total: 0.4854 s/iter. ETA=0:06:28
[02/18 17:25:34] mask2former INFO: Inference done 303/1092. Dataloading: 0.0052 s/iter. Inference: 0.3619 s/iter. Eval: 0.1180 s/iter. Total: 0.4852 s/iter. ETA=0:06:22
[02/18 17:25:40] mask2former INFO: Inference done 314/1092. Dataloading: 0.0052 s/iter. Inference: 0.3618 s/iter. Eval: 0.1182 s/iter. Total: 0.4853 s/iter. ETA=0:06:17
[02/18 17:25:45] mask2former INFO: Inference done 325/1092. Dataloading: 0.0052 s/iter. Inference: 0.3618 s/iter. Eval: 0.1182 s/iter. Total: 0.4853 s/iter. ETA=0:06:12
[02/18 17:25:50] mask2former INFO: Inference done 335/1092. Dataloading: 0.0052 s/iter. Inference: 0.3619 s/iter. Eval: 0.1189 s/iter. Total: 0.4861 s/iter. ETA=0:06:07
[02/18 17:25:55] mask2former INFO: Inference done 345/1092. Dataloading: 0.0052 s/iter. Inference: 0.3619 s/iter. Eval: 0.1195 s/iter. Total: 0.4867 s/iter. ETA=0:06:03
[02/18 17:26:00] mask2former INFO: Inference done 356/1092. Dataloading: 0.0052 s/iter. Inference: 0.3613 s/iter. Eval: 0.1198 s/iter. Total: 0.4864 s/iter. ETA=0:05:57
[02/18 17:26:06] mask2former INFO: Inference done 367/1092. Dataloading: 0.0052 s/iter. Inference: 0.3613 s/iter. Eval: 0.1196 s/iter. Total: 0.4862 s/iter. ETA=0:05:52
[02/18 17:26:11] mask2former INFO: Inference done 378/1092. Dataloading: 0.0052 s/iter. Inference: 0.3613 s/iter. Eval: 0.1192 s/iter. Total: 0.4857 s/iter. ETA=0:05:46
[02/18 17:26:16] mask2former INFO: Inference done 389/1092. Dataloading: 0.0052 s/iter. Inference: 0.3610 s/iter. Eval: 0.1191 s/iter. Total: 0.4853 s/iter. ETA=0:05:41
[02/18 17:26:21] mask2former INFO: Inference done 400/1092. Dataloading: 0.0052 s/iter. Inference: 0.3609 s/iter. Eval: 0.1192 s/iter. Total: 0.4853 s/iter. ETA=0:05:35
[02/18 17:26:27] mask2former INFO: Inference done 411/1092. Dataloading: 0.0052 s/iter. Inference: 0.3612 s/iter. Eval: 0.1190 s/iter. Total: 0.4855 s/iter. ETA=0:05:30
[02/18 17:26:32] mask2former INFO: Inference done 422/1092. Dataloading: 0.0052 s/iter. Inference: 0.3608 s/iter. Eval: 0.1190 s/iter. Total: 0.4851 s/iter. ETA=0:05:25
[02/18 17:26:37] mask2former INFO: Inference done 433/1092. Dataloading: 0.0052 s/iter. Inference: 0.3612 s/iter. Eval: 0.1188 s/iter. Total: 0.4852 s/iter. ETA=0:05:19
[02/18 17:26:42] mask2former INFO: Inference done 444/1092. Dataloading: 0.0052 s/iter. Inference: 0.3607 s/iter. Eval: 0.1187 s/iter. Total: 0.4847 s/iter. ETA=0:05:14
[02/18 17:26:48] mask2former INFO: Inference done 455/1092. Dataloading: 0.0052 s/iter. Inference: 0.3609 s/iter. Eval: 0.1181 s/iter. Total: 0.4843 s/iter. ETA=0:05:08
[02/18 17:26:53] mask2former INFO: Inference done 466/1092. Dataloading: 0.0052 s/iter. Inference: 0.3606 s/iter. Eval: 0.1181 s/iter. Total: 0.4840 s/iter. ETA=0:05:02
[02/18 17:26:58] mask2former INFO: Inference done 477/1092. Dataloading: 0.0052 s/iter. Inference: 0.3601 s/iter. Eval: 0.1185 s/iter. Total: 0.4838 s/iter. ETA=0:04:57
[02/18 17:27:03] mask2former INFO: Inference done 488/1092. Dataloading: 0.0052 s/iter. Inference: 0.3600 s/iter. Eval: 0.1183 s/iter. Total: 0.4836 s/iter. ETA=0:04:52
[02/18 17:27:09] mask2former INFO: Inference done 500/1092. Dataloading: 0.0052 s/iter. Inference: 0.3595 s/iter. Eval: 0.1181 s/iter. Total: 0.4829 s/iter. ETA=0:04:45
[02/18 17:27:14] mask2former INFO: Inference done 511/1092. Dataloading: 0.0052 s/iter. Inference: 0.3599 s/iter. Eval: 0.1179 s/iter. Total: 0.4831 s/iter. ETA=0:04:40
[02/18 17:27:19] mask2former INFO: Inference done 522/1092. Dataloading: 0.0052 s/iter. Inference: 0.3597 s/iter. Eval: 0.1177 s/iter. Total: 0.4826 s/iter. ETA=0:04:35
[02/18 17:27:24] mask2former INFO: Inference done 532/1092. Dataloading: 0.0052 s/iter. Inference: 0.3599 s/iter. Eval: 0.1180 s/iter. Total: 0.4831 s/iter. ETA=0:04:30
[02/18 17:27:30] mask2former INFO: Inference done 543/1092. Dataloading: 0.0052 s/iter. Inference: 0.3601 s/iter. Eval: 0.1176 s/iter. Total: 0.4831 s/iter. ETA=0:04:25
[02/18 17:27:35] mask2former INFO: Inference done 554/1092. Dataloading: 0.0052 s/iter. Inference: 0.3597 s/iter. Eval: 0.1176 s/iter. Total: 0.4827 s/iter. ETA=0:04:19
[02/18 17:27:40] mask2former INFO: Inference done 564/1092. Dataloading: 0.0053 s/iter. Inference: 0.3599 s/iter. Eval: 0.1178 s/iter. Total: 0.4830 s/iter. ETA=0:04:15
[02/18 17:27:45] mask2former INFO: Inference done 575/1092. Dataloading: 0.0053 s/iter. Inference: 0.3595 s/iter. Eval: 0.1180 s/iter. Total: 0.4829 s/iter. ETA=0:04:09
[02/18 17:27:50] mask2former INFO: Inference done 586/1092. Dataloading: 0.0053 s/iter. Inference: 0.3593 s/iter. Eval: 0.1178 s/iter. Total: 0.4824 s/iter. ETA=0:04:04
[02/18 17:27:55] mask2former INFO: Inference done 597/1092. Dataloading: 0.0053 s/iter. Inference: 0.3596 s/iter. Eval: 0.1176 s/iter. Total: 0.4826 s/iter. ETA=0:03:58
[02/18 17:28:01] mask2former INFO: Inference done 609/1092. Dataloading: 0.0053 s/iter. Inference: 0.3593 s/iter. Eval: 0.1174 s/iter. Total: 0.4821 s/iter. ETA=0:03:52
[02/18 17:28:06] mask2former INFO: Inference done 621/1092. Dataloading: 0.0053 s/iter. Inference: 0.3589 s/iter. Eval: 0.1171 s/iter. Total: 0.4813 s/iter. ETA=0:03:46
[02/18 17:28:12] mask2former INFO: Inference done 632/1092. Dataloading: 0.0053 s/iter. Inference: 0.3587 s/iter. Eval: 0.1174 s/iter. Total: 0.4815 s/iter. ETA=0:03:41
[02/18 17:28:17] mask2former INFO: Inference done 643/1092. Dataloading: 0.0053 s/iter. Inference: 0.3589 s/iter. Eval: 0.1174 s/iter. Total: 0.4817 s/iter. ETA=0:03:36
[02/18 17:28:22] mask2former INFO: Inference done 654/1092. Dataloading: 0.0053 s/iter. Inference: 0.3586 s/iter. Eval: 0.1178 s/iter. Total: 0.4817 s/iter. ETA=0:03:31
[02/18 17:28:28] mask2former INFO: Inference done 665/1092. Dataloading: 0.0053 s/iter. Inference: 0.3588 s/iter. Eval: 0.1175 s/iter. Total: 0.4816 s/iter. ETA=0:03:25
[02/18 17:28:33] mask2former INFO: Inference done 676/1092. Dataloading: 0.0053 s/iter. Inference: 0.3586 s/iter. Eval: 0.1176 s/iter. Total: 0.4815 s/iter. ETA=0:03:20
[02/18 17:28:38] mask2former INFO: Inference done 686/1092. Dataloading: 0.0053 s/iter. Inference: 0.3588 s/iter. Eval: 0.1177 s/iter. Total: 0.4819 s/iter. ETA=0:03:15
[02/18 17:28:43] mask2former INFO: Inference done 697/1092. Dataloading: 0.0053 s/iter. Inference: 0.3585 s/iter. Eval: 0.1176 s/iter. Total: 0.4815 s/iter. ETA=0:03:10
[02/18 17:28:48] mask2former INFO: Inference done 708/1092. Dataloading: 0.0053 s/iter. Inference: 0.3585 s/iter. Eval: 0.1176 s/iter. Total: 0.4815 s/iter. ETA=0:03:04
[02/18 17:28:53] mask2former INFO: Inference done 719/1092. Dataloading: 0.0053 s/iter. Inference: 0.3586 s/iter. Eval: 0.1175 s/iter. Total: 0.4815 s/iter. ETA=0:02:59
[02/18 17:28:59] mask2former INFO: Inference done 729/1092. Dataloading: 0.0053 s/iter. Inference: 0.3590 s/iter. Eval: 0.1176 s/iter. Total: 0.4819 s/iter. ETA=0:02:54
[02/18 17:29:04] mask2former INFO: Inference done 740/1092. Dataloading: 0.0053 s/iter. Inference: 0.3589 s/iter. Eval: 0.1175 s/iter. Total: 0.4817 s/iter. ETA=0:02:49
[02/18 17:29:09] mask2former INFO: Inference done 750/1092. Dataloading: 0.0053 s/iter. Inference: 0.3591 s/iter. Eval: 0.1176 s/iter. Total: 0.4820 s/iter. ETA=0:02:44
[02/18 17:29:14] mask2former INFO: Inference done 761/1092. Dataloading: 0.0053 s/iter. Inference: 0.3589 s/iter. Eval: 0.1177 s/iter. Total: 0.4820 s/iter. ETA=0:02:39
[02/18 17:29:19] mask2former INFO: Inference done 772/1092. Dataloading: 0.0053 s/iter. Inference: 0.3589 s/iter. Eval: 0.1178 s/iter. Total: 0.4821 s/iter. ETA=0:02:34
[02/18 17:29:25] mask2former INFO: Inference done 783/1092. Dataloading: 0.0053 s/iter. Inference: 0.3588 s/iter. Eval: 0.1177 s/iter. Total: 0.4819 s/iter. ETA=0:02:28
[02/18 17:29:30] mask2former INFO: Inference done 795/1092. Dataloading: 0.0053 s/iter. Inference: 0.3589 s/iter. Eval: 0.1171 s/iter. Total: 0.4814 s/iter. ETA=0:02:22
[02/18 17:29:35] mask2former INFO: Inference done 806/1092. Dataloading: 0.0053 s/iter. Inference: 0.3591 s/iter. Eval: 0.1170 s/iter. Total: 0.4815 s/iter. ETA=0:02:17
[02/18 17:29:41] mask2former INFO: Inference done 817/1092. Dataloading: 0.0053 s/iter. Inference: 0.3592 s/iter. Eval: 0.1168 s/iter. Total: 0.4814 s/iter. ETA=0:02:12
[02/18 17:29:46] mask2former INFO: Inference done 824/1092. Dataloading: 0.0054 s/iter. Inference: 0.3599 s/iter. Eval: 0.1187 s/iter. Total: 0.4841 s/iter. ETA=0:02:09
[02/18 17:29:51] mask2former INFO: Inference done 830/1092. Dataloading: 0.0054 s/iter. Inference: 0.3615 s/iter. Eval: 0.1197 s/iter. Total: 0.4868 s/iter. ETA=0:02:07
[02/18 17:29:56] mask2former INFO: Inference done 837/1092. Dataloading: 0.0054 s/iter. Inference: 0.3627 s/iter. Eval: 0.1206 s/iter. Total: 0.4888 s/iter. ETA=0:02:04
[02/18 17:30:02] mask2former INFO: Inference done 848/1092. Dataloading: 0.0054 s/iter. Inference: 0.3627 s/iter. Eval: 0.1208 s/iter. Total: 0.4890 s/iter. ETA=0:01:59
[02/18 17:30:07] mask2former INFO: Inference done 856/1092. Dataloading: 0.0055 s/iter. Inference: 0.3634 s/iter. Eval: 0.1215 s/iter. Total: 0.4905 s/iter. ETA=0:01:55
[02/18 17:30:12] mask2former INFO: Inference done 866/1092. Dataloading: 0.0055 s/iter. Inference: 0.3634 s/iter. Eval: 0.1220 s/iter. Total: 0.4910 s/iter. ETA=0:01:50
[02/18 17:30:18] mask2former INFO: Inference done 877/1092. Dataloading: 0.0055 s/iter. Inference: 0.3633 s/iter. Eval: 0.1221 s/iter. Total: 0.4910 s/iter. ETA=0:01:45
[02/18 17:30:23] mask2former INFO: Inference done 888/1092. Dataloading: 0.0055 s/iter. Inference: 0.3632 s/iter. Eval: 0.1221 s/iter. Total: 0.4909 s/iter. ETA=0:01:40
[02/18 17:30:28] mask2former INFO: Inference done 899/1092. Dataloading: 0.0055 s/iter. Inference: 0.3632 s/iter. Eval: 0.1220 s/iter. Total: 0.4907 s/iter. ETA=0:01:34
[02/18 17:30:34] mask2former INFO: Inference done 910/1092. Dataloading: 0.0055 s/iter. Inference: 0.3630 s/iter. Eval: 0.1222 s/iter. Total: 0.4907 s/iter. ETA=0:01:29
[02/18 17:30:39] mask2former INFO: Inference done 921/1092. Dataloading: 0.0055 s/iter. Inference: 0.3629 s/iter. Eval: 0.1222 s/iter. Total: 0.4907 s/iter. ETA=0:01:23
[02/18 17:30:45] mask2former INFO: Inference done 932/1092. Dataloading: 0.0055 s/iter. Inference: 0.3628 s/iter. Eval: 0.1223 s/iter. Total: 0.4907 s/iter. ETA=0:01:18
[02/18 17:30:50] mask2former INFO: Inference done 943/1092. Dataloading: 0.0055 s/iter. Inference: 0.3627 s/iter. Eval: 0.1221 s/iter. Total: 0.4904 s/iter. ETA=0:01:13
[02/18 17:30:55] mask2former INFO: Inference done 954/1092. Dataloading: 0.0055 s/iter. Inference: 0.3626 s/iter. Eval: 0.1222 s/iter. Total: 0.4903 s/iter. ETA=0:01:07
[02/18 17:31:00] mask2former INFO: Inference done 964/1092. Dataloading: 0.0055 s/iter. Inference: 0.3628 s/iter. Eval: 0.1223 s/iter. Total: 0.4906 s/iter. ETA=0:01:02
[02/18 17:31:05] mask2former INFO: Inference done 975/1092. Dataloading: 0.0055 s/iter. Inference: 0.3627 s/iter. Eval: 0.1221 s/iter. Total: 0.4905 s/iter. ETA=0:00:57
[02/18 17:31:11] mask2former INFO: Inference done 986/1092. Dataloading: 0.0055 s/iter. Inference: 0.3629 s/iter. Eval: 0.1220 s/iter. Total: 0.4905 s/iter. ETA=0:00:51
[02/18 17:31:16] mask2former INFO: Inference done 997/1092. Dataloading: 0.0055 s/iter. Inference: 0.3628 s/iter. Eval: 0.1220 s/iter. Total: 0.4904 s/iter. ETA=0:00:46
[02/18 17:31:21] mask2former INFO: Inference done 1007/1092. Dataloading: 0.0055 s/iter. Inference: 0.3630 s/iter. Eval: 0.1222 s/iter. Total: 0.4908 s/iter. ETA=0:00:41
[02/18 17:31:27] mask2former INFO: Inference done 1018/1092. Dataloading: 0.0055 s/iter. Inference: 0.3629 s/iter. Eval: 0.1223 s/iter. Total: 0.4908 s/iter. ETA=0:00:36
[02/18 17:31:32] mask2former INFO: Inference done 1029/1092. Dataloading: 0.0055 s/iter. Inference: 0.3629 s/iter. Eval: 0.1223 s/iter. Total: 0.4908 s/iter. ETA=0:00:30
[02/18 17:31:37] mask2former INFO: Inference done 1039/1092. Dataloading: 0.0055 s/iter. Inference: 0.3631 s/iter. Eval: 0.1222 s/iter. Total: 0.4909 s/iter. ETA=0:00:26
[02/18 17:31:43] mask2former INFO: Inference done 1050/1092. Dataloading: 0.0055 s/iter. Inference: 0.3630 s/iter. Eval: 0.1222 s/iter. Total: 0.4908 s/iter. ETA=0:00:20
[02/18 17:31:48] mask2former INFO: Inference done 1061/1092. Dataloading: 0.0055 s/iter. Inference: 0.3631 s/iter. Eval: 0.1221 s/iter. Total: 0.4908 s/iter. ETA=0:00:15
[02/18 17:31:53] mask2former INFO: Inference done 1072/1092. Dataloading: 0.0055 s/iter. Inference: 0.3632 s/iter. Eval: 0.1220 s/iter. Total: 0.4907 s/iter. ETA=0:00:09
[02/18 17:31:59] mask2former INFO: Inference done 1083/1092. Dataloading: 0.0055 s/iter. Inference: 0.3630 s/iter. Eval: 0.1220 s/iter. Total: 0.4906 s/iter. ETA=0:00:04
[02/18 17:32:51] detectron2.engine.hooks INFO: Overall training speed: 30000 iterations in 16:44:44 (2.0095 s / it)
[02/18 17:32:51] detectron2.engine.hooks INFO: Total training time: 18:27:05 (1:42:21 on hooks)
