[02/17 01:53:59] detectron2 INFO: Rank of current process: 2. World size: 4
[02/17 01:54:06] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 01:54:06] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 01:54:06] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 01:54:06] detectron2.utils.env INFO: Using a generated random seed 6377008
[02/17 01:54:09] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 0.0, 'loss_ce': 0.0, 'loss_dice': 0.0, 'loss_seg': 1.0, 'loss_mask_0': 0.0, 'loss_ce_0': 0.0, 'loss_dice_0': 0.0, 'loss_seg_0': 1.0, 'loss_mask_1': 0.0, 'loss_ce_1': 0.0, 'loss_dice_1': 0.0, 'loss_seg_1': 1.0, 'loss_mask_2': 0.0, 'loss_ce_2': 0.0, 'loss_dice_2': 0.0, 'loss_seg_2': 1.0, 'loss_mask_3': 0.0, 'loss_ce_3': 0.0, 'loss_dice_3': 0.0, 'loss_seg_3': 1.0, 'loss_mask_4': 0.0, 'loss_ce_4': 0.0, 'loss_dice_4': 0.0, 'loss_seg_4': 1.0, 'loss_mask_5': 0.0, 'loss_ce_5': 0.0, 'loss_dice_5': 0.0, 'loss_seg_5': 1.0, 'loss_mask_6': 0.0, 'loss_ce_6': 0.0, 'loss_dice_6': 0.0, 'loss_seg_6': 1.0, 'loss_mask_7': 0.0, 'loss_ce_7': 0.0, 'loss_dice_7': 0.0, 'loss_seg_7': 1.0, 'loss_mask_8': 0.0, 'loss_ce_8': 0.0, 'loss_dice_8': 0.0, 'loss_seg_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/17 01:54:09] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 01:54:18] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 01:54:20] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 01:54:20] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 01:54:20] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 01:54:21] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 01:54:21] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 01:54:22] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/17 01:54:22] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 01:54:22] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 01:54:36] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 395, in run_step
    loss_dict = self.model(data)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 799, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 672, in forward
    losses = self.criterion(outputs, targets)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nstarli/Mask2Former/mask2former/modeling/criterion.py", line 266, in forward
    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_masks)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 181, in get_loss
    return loss_map[loss](outputs, targets, indices, num_masks)
  File "/home/nstarli/Mask2Former/mask2former/maskformer_model_stereo.py", line 167, in loss_segs
    semseg = (mask_pred * classes).sum(1)
RuntimeError: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 2; 31.75 GiB total capacity; 11.38 GiB already allocated; 596.75 MiB free; 11.84 GiB reserved in total by PyTorch)
[02/17 01:54:36] detectron2.engine.hooks INFO: Total training time: 0:00:13 (0:00:00 on hooks)
[02/17 01:55:29] detectron2 INFO: Rank of current process: 2. World size: 4
[02/17 01:55:35] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 01:55:35] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 01:55:35] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 01:55:35] detectron2.utils.env INFO: Using a generated random seed 35184726
[02/17 01:55:40] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 0.0, 'loss_ce': 0.0, 'loss_dice': 0.0, 'loss_seg': 1.0, 'loss_mask_0': 0.0, 'loss_ce_0': 0.0, 'loss_dice_0': 0.0, 'loss_seg_0': 1.0, 'loss_mask_1': 0.0, 'loss_ce_1': 0.0, 'loss_dice_1': 0.0, 'loss_seg_1': 1.0, 'loss_mask_2': 0.0, 'loss_ce_2': 0.0, 'loss_dice_2': 0.0, 'loss_seg_2': 1.0, 'loss_mask_3': 0.0, 'loss_ce_3': 0.0, 'loss_dice_3': 0.0, 'loss_seg_3': 1.0, 'loss_mask_4': 0.0, 'loss_ce_4': 0.0, 'loss_dice_4': 0.0, 'loss_seg_4': 1.0, 'loss_mask_5': 0.0, 'loss_ce_5': 0.0, 'loss_dice_5': 0.0, 'loss_seg_5': 1.0, 'loss_mask_6': 0.0, 'loss_ce_6': 0.0, 'loss_dice_6': 0.0, 'loss_seg_6': 1.0, 'loss_mask_7': 0.0, 'loss_ce_7': 0.0, 'loss_dice_7': 0.0, 'loss_seg_7': 1.0, 'loss_mask_8': 0.0, 'loss_ce_8': 0.0, 'loss_dice_8': 0.0, 'loss_seg_8': 1.0}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/17 01:55:40] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 01:55:46] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 01:55:46] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 01:55:46] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 01:55:46] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 01:55:46] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 01:55:46] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 01:55:47] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/17 01:55:47] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 01:55:47] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 01:56:02] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/home/nstarli/detectron2/detectron2/engine/train_loop.py", line 403, in run_step
    self.grad_scaler.scale(losses).backward()
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Found dtype Long but expected Float
[02/17 01:56:02] detectron2.engine.hooks INFO: Total training time: 0:00:14 (0:00:00 on hooks)
[02/17 01:59:21] detectron2 INFO: Rank of current process: 2. World size: 4
[02/17 01:59:25] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.21.5
detectron2              0.6 @/home/nstarli/detectron2/detectron2
Compiler                GCC 7.5
CUDA compiler           CUDA 11.5
detectron2 arch flags   7.0
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2,3             Tesla V100-SXM2-32GB (arch=7.0)
Driver version          495.29.05
CUDA_HOME               /usr/local/cuda-11
Pillow                  8.4.0
torchvision             0.10.0 @/home/nstarli/anaconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20211023
iopath                  0.1.9
cv2                     4.5.4
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[02/17 01:59:25] detectron2 INFO: Command line arguments: Namespace(config_file='configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml', dist_url='tcp://127.0.0.1:65510', eval_only=False, machine_rank=0, num_gpus=4, num_machines=1, opts=[], resume=False)
[02/17 01:59:25] detectron2 INFO: Contents of args.config_file=configs/sceneflow/semantic-segmentation/maskformer2stereo_R101_bs16_90k.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmaskformer2stereo_R50_bs16_90k.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m  [39m[38;5;186m"[39m[38;5;186mcheckpoints/R-101.pkl[39m[38;5;186m"[39m[38;5;15m [39m[38;5;242m#"/home/nstarli/Mask2Former/work_dirs/r101_48classes_fixedmatching/model_final.pth"[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m101[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mbasic[39m[38;5;186m"[39m[38;5;15m  [39m[38;5;242m# not used[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mSyncBN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m][39m[38;5;15m  [39m[38;5;242m# not used[39m

[02/17 01:59:25] detectron2.utils.env INFO: Using a generated random seed 25621728
[02/17 01:59:30] detectron2.engine.defaults INFO: Model:
MaskFormerStereo(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (6): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (7): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (8): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (9): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (10): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (11): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (12): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (13): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (14): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (15): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (16): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (17): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (18): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (19): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (20): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (21): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (22): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(48, 256)
      (query_embed): Embedding(48, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=49, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterionStereo
      matcher: Matcher FixedMatcher
      losses: ['labels', 'masks', 'segs']
      weight_dict: {'loss_mask': 5.0, 'loss_ce': 0.0, 'loss_dice': 5.0, 'loss_seg': 0.001, 'loss_mask_0': 5.0, 'loss_ce_0': 0.0, 'loss_dice_0': 5.0, 'loss_seg_0': 0.001, 'loss_mask_1': 5.0, 'loss_ce_1': 0.0, 'loss_dice_1': 5.0, 'loss_seg_1': 0.001, 'loss_mask_2': 5.0, 'loss_ce_2': 0.0, 'loss_dice_2': 5.0, 'loss_seg_2': 0.001, 'loss_mask_3': 5.0, 'loss_ce_3': 0.0, 'loss_dice_3': 5.0, 'loss_seg_3': 0.001, 'loss_mask_4': 5.0, 'loss_ce_4': 0.0, 'loss_dice_4': 5.0, 'loss_seg_4': 0.001, 'loss_mask_5': 5.0, 'loss_ce_5': 0.0, 'loss_dice_5': 5.0, 'loss_seg_5': 0.001, 'loss_mask_6': 5.0, 'loss_ce_6': 0.0, 'loss_dice_6': 5.0, 'loss_seg_6': 0.001, 'loss_mask_7': 5.0, 'loss_ce_7': 0.0, 'loss_dice_7': 5.0, 'loss_seg_7': 0.001, 'loss_mask_8': 5.0, 'loss_ce_8': 0.0, 'loss_dice_8': 5.0, 'loss_seg_8': 0.001}
      num_classes: 48
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[02/17 01:59:30] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in training: [RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[256, 512], single_category_max_area=1.0, ignored_category=0)]
[02/17 01:59:35] detectron2.data.build INFO: Using training sampler TrainingSampler
[02/17 01:59:37] detectron2.data.common INFO: Serializing 35454 elements to byte tensors and concatenating them all ...
[02/17 01:59:37] detectron2.data.common INFO: Serialized dataset takes 10.76 MiB
[02/17 01:59:38] fvcore.common.checkpoint INFO: [Checkpointer] Loading from checkpoints/R-101.pkl ...
[02/17 01:59:38] detectron2.checkpoint.c2_model_loading INFO: Renaming Caffe2 weights ......
[02/17 01:59:38] detectron2.checkpoint.c2_model_loading INFO: Following weights matched with submodule backbone:
| Names in Model    | Names in Checkpoint       | Shapes                                          |
|:------------------|:--------------------------|:------------------------------------------------|
| res2.0.conv1.*    | res2_0_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,1,1)             |
| res2.0.conv2.*    | res2_0_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.0.conv3.*    | res2_0_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.0.shortcut.* | res2_0_branch1_{bn_*,w}   | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.1.conv1.*    | res2_1_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.1.conv2.*    | res2_1_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.1.conv3.*    | res2_1_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res2.2.conv1.*    | res2_2_branch2a_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,256,1,1)            |
| res2.2.conv2.*    | res2_2_branch2b_{bn_*,w}  | (64,) (64,) (64,) (64,) (64,64,3,3)             |
| res2.2.conv3.*    | res2_2_branch2c_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,64,1,1)        |
| res3.0.conv1.*    | res3_0_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,256,1,1)       |
| res3.0.conv2.*    | res3_0_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.0.conv3.*    | res3_0_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.0.shortcut.* | res3_0_branch1_{bn_*,w}   | (512,) (512,) (512,) (512,) (512,256,1,1)       |
| res3.1.conv1.*    | res3_1_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.1.conv2.*    | res3_1_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.1.conv3.*    | res3_1_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.2.conv1.*    | res3_2_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.2.conv2.*    | res3_2_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.2.conv3.*    | res3_2_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res3.3.conv1.*    | res3_3_branch2a_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,512,1,1)       |
| res3.3.conv2.*    | res3_3_branch2b_{bn_*,w}  | (128,) (128,) (128,) (128,) (128,128,3,3)       |
| res3.3.conv3.*    | res3_3_branch2c_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,128,1,1)       |
| res4.0.conv1.*    | res4_0_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,512,1,1)       |
| res4.0.conv2.*    | res4_0_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.0.conv3.*    | res4_0_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.0.shortcut.* | res4_0_branch1_{bn_*,w}   | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |
| res4.1.conv1.*    | res4_1_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.1.conv2.*    | res4_1_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.1.conv3.*    | res4_1_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.10.conv1.*   | res4_10_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.10.conv2.*   | res4_10_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.10.conv3.*   | res4_10_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.11.conv1.*   | res4_11_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.11.conv2.*   | res4_11_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.11.conv3.*   | res4_11_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.12.conv1.*   | res4_12_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.12.conv2.*   | res4_12_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.12.conv3.*   | res4_12_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.13.conv1.*   | res4_13_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.13.conv2.*   | res4_13_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.13.conv3.*   | res4_13_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.14.conv1.*   | res4_14_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.14.conv2.*   | res4_14_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.14.conv3.*   | res4_14_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.15.conv1.*   | res4_15_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.15.conv2.*   | res4_15_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.15.conv3.*   | res4_15_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.16.conv1.*   | res4_16_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.16.conv2.*   | res4_16_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.16.conv3.*   | res4_16_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.17.conv1.*   | res4_17_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.17.conv2.*   | res4_17_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.17.conv3.*   | res4_17_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.18.conv1.*   | res4_18_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.18.conv2.*   | res4_18_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.18.conv3.*   | res4_18_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.19.conv1.*   | res4_19_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.19.conv2.*   | res4_19_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.19.conv3.*   | res4_19_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.2.conv1.*    | res4_2_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.2.conv2.*    | res4_2_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.2.conv3.*    | res4_2_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.20.conv1.*   | res4_20_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.20.conv2.*   | res4_20_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.20.conv3.*   | res4_20_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.21.conv1.*   | res4_21_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.21.conv2.*   | res4_21_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.21.conv3.*   | res4_21_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.22.conv1.*   | res4_22_branch2a_{bn_*,w} | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.22.conv2.*   | res4_22_branch2b_{bn_*,w} | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.22.conv3.*   | res4_22_branch2c_{bn_*,w} | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.3.conv1.*    | res4_3_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.3.conv2.*    | res4_3_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.3.conv3.*    | res4_3_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.4.conv1.*    | res4_4_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.4.conv2.*    | res4_4_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.4.conv3.*    | res4_4_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.5.conv1.*    | res4_5_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.5.conv2.*    | res4_5_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.5.conv3.*    | res4_5_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.6.conv1.*    | res4_6_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.6.conv2.*    | res4_6_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.6.conv3.*    | res4_6_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.7.conv1.*    | res4_7_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.7.conv2.*    | res4_7_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.7.conv3.*    | res4_7_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.8.conv1.*    | res4_8_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.8.conv2.*    | res4_8_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.8.conv3.*    | res4_8_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res4.9.conv1.*    | res4_9_branch2a_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,1024,1,1)      |
| res4.9.conv2.*    | res4_9_branch2b_{bn_*,w}  | (256,) (256,) (256,) (256,) (256,256,3,3)       |
| res4.9.conv3.*    | res4_9_branch2c_{bn_*,w}  | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |
| res5.0.conv1.*    | res5_0_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,1024,1,1)      |
| res5.0.conv2.*    | res5_0_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.0.conv3.*    | res5_0_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.0.shortcut.* | res5_0_branch1_{bn_*,w}   | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |
| res5.1.conv1.*    | res5_1_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.1.conv2.*    | res5_1_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.1.conv3.*    | res5_1_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| res5.2.conv1.*    | res5_2_branch2a_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,2048,1,1)      |
| res5.2.conv2.*    | res5_2_branch2b_{bn_*,w}  | (512,) (512,) (512,) (512,) (512,512,3,3)       |
| res5.2.conv3.*    | res5_2_branch2c_{bn_*,w}  | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |
| stem.conv1.norm.* | res_conv1_bn_*            | (64,) (64,) (64,) (64,)                         |
| stem.conv1.weight | conv1_w                   | (64, 3, 7, 7)                                   |
[02/17 01:59:39] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.adapter_1.weight[0m
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.layer_1.weight[0m
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}[0m
[34msem_seg_head.pixel_decoder.transformer.level_embed[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[34msem_seg_head.predictor.decoder_norm.{bias, weight}[0m
[34msem_seg_head.predictor.level_embed.weight[0m
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}[0m
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}[0m
[34msem_seg_head.predictor.query_embed.weight[0m
[34msem_seg_head.predictor.query_feat.weight[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}[0m
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}[0m
[02/17 01:59:39] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mfc1000.{bias, weight}[0m
[02/17 01:59:39] detectron2.engine.train_loop INFO: Starting training from iteration 0
[02/17 03:51:40] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 03:51:40] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 03:51:40] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 03:51:55] mask2former INFO: Inference done 11/1092. Dataloading: 0.0040 s/iter. Inference: 0.2647 s/iter. Eval: 0.2030 s/iter. Total: 0.4716 s/iter. ETA=0:08:29
[02/17 03:52:01] mask2former INFO: Inference done 23/1092. Dataloading: 0.0069 s/iter. Inference: 0.2726 s/iter. Eval: 0.1696 s/iter. Total: 0.4493 s/iter. ETA=0:08:00
[02/17 03:52:06] mask2former INFO: Inference done 34/1092. Dataloading: 0.0084 s/iter. Inference: 0.2855 s/iter. Eval: 0.1655 s/iter. Total: 0.4594 s/iter. ETA=0:08:06
[02/17 03:52:11] mask2former INFO: Inference done 45/1092. Dataloading: 0.0080 s/iter. Inference: 0.2903 s/iter. Eval: 0.1614 s/iter. Total: 0.4598 s/iter. ETA=0:08:01
[02/17 03:52:16] mask2former INFO: Inference done 56/1092. Dataloading: 0.0082 s/iter. Inference: 0.2962 s/iter. Eval: 0.1601 s/iter. Total: 0.4646 s/iter. ETA=0:08:01
[02/17 03:52:21] mask2former INFO: Inference done 67/1092. Dataloading: 0.0083 s/iter. Inference: 0.2964 s/iter. Eval: 0.1593 s/iter. Total: 0.4640 s/iter. ETA=0:07:55
[02/17 03:52:27] mask2former INFO: Inference done 78/1092. Dataloading: 0.0084 s/iter. Inference: 0.3009 s/iter. Eval: 0.1585 s/iter. Total: 0.4679 s/iter. ETA=0:07:54
[02/17 03:52:32] mask2former INFO: Inference done 90/1092. Dataloading: 0.0081 s/iter. Inference: 0.2972 s/iter. Eval: 0.1584 s/iter. Total: 0.4638 s/iter. ETA=0:07:44
[02/17 03:52:37] mask2former INFO: Inference done 101/1092. Dataloading: 0.0078 s/iter. Inference: 0.2997 s/iter. Eval: 0.1557 s/iter. Total: 0.4633 s/iter. ETA=0:07:39
[02/17 03:52:42] mask2former INFO: Inference done 112/1092. Dataloading: 0.0077 s/iter. Inference: 0.3007 s/iter. Eval: 0.1569 s/iter. Total: 0.4654 s/iter. ETA=0:07:36
[02/17 03:52:47] mask2former INFO: Inference done 123/1092. Dataloading: 0.0075 s/iter. Inference: 0.3014 s/iter. Eval: 0.1569 s/iter. Total: 0.4660 s/iter. ETA=0:07:31
[02/17 03:52:53] mask2former INFO: Inference done 136/1092. Dataloading: 0.0075 s/iter. Inference: 0.2972 s/iter. Eval: 0.1552 s/iter. Total: 0.4599 s/iter. ETA=0:07:19
[02/17 03:52:58] mask2former INFO: Inference done 148/1092. Dataloading: 0.0074 s/iter. Inference: 0.2954 s/iter. Eval: 0.1556 s/iter. Total: 0.4585 s/iter. ETA=0:07:12
[02/17 03:53:03] mask2former INFO: Inference done 160/1092. Dataloading: 0.0075 s/iter. Inference: 0.2944 s/iter. Eval: 0.1551 s/iter. Total: 0.4572 s/iter. ETA=0:07:06
[02/17 03:53:08] mask2former INFO: Inference done 171/1092. Dataloading: 0.0078 s/iter. Inference: 0.2945 s/iter. Eval: 0.1555 s/iter. Total: 0.4579 s/iter. ETA=0:07:01
[02/17 03:53:14] mask2former INFO: Inference done 182/1092. Dataloading: 0.0077 s/iter. Inference: 0.2942 s/iter. Eval: 0.1570 s/iter. Total: 0.4590 s/iter. ETA=0:06:57
[02/17 03:53:19] mask2former INFO: Inference done 193/1092. Dataloading: 0.0077 s/iter. Inference: 0.2947 s/iter. Eval: 0.1567 s/iter. Total: 0.4593 s/iter. ETA=0:06:52
[02/17 03:53:24] mask2former INFO: Inference done 205/1092. Dataloading: 0.0076 s/iter. Inference: 0.2935 s/iter. Eval: 0.1561 s/iter. Total: 0.4574 s/iter. ETA=0:06:45
[02/17 03:53:29] mask2former INFO: Inference done 216/1092. Dataloading: 0.0077 s/iter. Inference: 0.2931 s/iter. Eval: 0.1565 s/iter. Total: 0.4574 s/iter. ETA=0:06:40
[02/17 03:53:34] mask2former INFO: Inference done 228/1092. Dataloading: 0.0077 s/iter. Inference: 0.2929 s/iter. Eval: 0.1567 s/iter. Total: 0.4574 s/iter. ETA=0:06:35
[02/17 03:53:40] mask2former INFO: Inference done 240/1092. Dataloading: 0.0075 s/iter. Inference: 0.2924 s/iter. Eval: 0.1565 s/iter. Total: 0.4565 s/iter. ETA=0:06:28
[02/17 03:53:45] mask2former INFO: Inference done 252/1092. Dataloading: 0.0075 s/iter. Inference: 0.2935 s/iter. Eval: 0.1553 s/iter. Total: 0.4565 s/iter. ETA=0:06:23
[02/17 03:53:51] mask2former INFO: Inference done 264/1092. Dataloading: 0.0074 s/iter. Inference: 0.2937 s/iter. Eval: 0.1546 s/iter. Total: 0.4559 s/iter. ETA=0:06:17
[02/17 03:53:56] mask2former INFO: Inference done 275/1092. Dataloading: 0.0074 s/iter. Inference: 0.2939 s/iter. Eval: 0.1545 s/iter. Total: 0.4560 s/iter. ETA=0:06:12
[02/17 03:54:01] mask2former INFO: Inference done 287/1092. Dataloading: 0.0074 s/iter. Inference: 0.2940 s/iter. Eval: 0.1541 s/iter. Total: 0.4556 s/iter. ETA=0:06:06
[02/17 03:54:06] mask2former INFO: Inference done 298/1092. Dataloading: 0.0074 s/iter. Inference: 0.2939 s/iter. Eval: 0.1542 s/iter. Total: 0.4557 s/iter. ETA=0:06:01
[02/17 03:54:11] mask2former INFO: Inference done 310/1092. Dataloading: 0.0073 s/iter. Inference: 0.2935 s/iter. Eval: 0.1544 s/iter. Total: 0.4554 s/iter. ETA=0:05:56
[02/17 03:54:17] mask2former INFO: Inference done 321/1092. Dataloading: 0.0074 s/iter. Inference: 0.2938 s/iter. Eval: 0.1550 s/iter. Total: 0.4563 s/iter. ETA=0:05:51
[02/17 03:54:22] mask2former INFO: Inference done 333/1092. Dataloading: 0.0074 s/iter. Inference: 0.2932 s/iter. Eval: 0.1554 s/iter. Total: 0.4562 s/iter. ETA=0:05:46
[02/17 03:54:27] mask2former INFO: Inference done 345/1092. Dataloading: 0.0073 s/iter. Inference: 0.2929 s/iter. Eval: 0.1553 s/iter. Total: 0.4556 s/iter. ETA=0:05:40
[02/17 03:54:33] mask2former INFO: Inference done 357/1092. Dataloading: 0.0073 s/iter. Inference: 0.2928 s/iter. Eval: 0.1551 s/iter. Total: 0.4554 s/iter. ETA=0:05:34
[02/17 03:54:38] mask2former INFO: Inference done 368/1092. Dataloading: 0.0074 s/iter. Inference: 0.2935 s/iter. Eval: 0.1546 s/iter. Total: 0.4556 s/iter. ETA=0:05:29
[02/17 03:54:43] mask2former INFO: Inference done 379/1092. Dataloading: 0.0074 s/iter. Inference: 0.2936 s/iter. Eval: 0.1545 s/iter. Total: 0.4556 s/iter. ETA=0:05:24
[02/17 03:54:48] mask2former INFO: Inference done 390/1092. Dataloading: 0.0074 s/iter. Inference: 0.2933 s/iter. Eval: 0.1549 s/iter. Total: 0.4557 s/iter. ETA=0:05:19
[02/17 03:54:53] mask2former INFO: Inference done 402/1092. Dataloading: 0.0073 s/iter. Inference: 0.2934 s/iter. Eval: 0.1547 s/iter. Total: 0.4555 s/iter. ETA=0:05:14
[02/17 03:54:59] mask2former INFO: Inference done 414/1092. Dataloading: 0.0073 s/iter. Inference: 0.2934 s/iter. Eval: 0.1547 s/iter. Total: 0.4555 s/iter. ETA=0:05:08
[02/17 03:55:04] mask2former INFO: Inference done 425/1092. Dataloading: 0.0073 s/iter. Inference: 0.2930 s/iter. Eval: 0.1555 s/iter. Total: 0.4559 s/iter. ETA=0:05:04
[02/17 03:55:09] mask2former INFO: Inference done 437/1092. Dataloading: 0.0073 s/iter. Inference: 0.2927 s/iter. Eval: 0.1551 s/iter. Total: 0.4552 s/iter. ETA=0:04:58
[02/17 03:55:14] mask2former INFO: Inference done 448/1092. Dataloading: 0.0073 s/iter. Inference: 0.2933 s/iter. Eval: 0.1549 s/iter. Total: 0.4555 s/iter. ETA=0:04:53
[02/17 03:55:20] mask2former INFO: Inference done 459/1092. Dataloading: 0.0072 s/iter. Inference: 0.2934 s/iter. Eval: 0.1553 s/iter. Total: 0.4561 s/iter. ETA=0:04:48
[02/17 03:55:25] mask2former INFO: Inference done 470/1092. Dataloading: 0.0072 s/iter. Inference: 0.2942 s/iter. Eval: 0.1549 s/iter. Total: 0.4565 s/iter. ETA=0:04:43
[02/17 03:55:30] mask2former INFO: Inference done 481/1092. Dataloading: 0.0073 s/iter. Inference: 0.2945 s/iter. Eval: 0.1551 s/iter. Total: 0.4570 s/iter. ETA=0:04:39
[02/17 03:55:35] mask2former INFO: Inference done 493/1092. Dataloading: 0.0073 s/iter. Inference: 0.2940 s/iter. Eval: 0.1550 s/iter. Total: 0.4564 s/iter. ETA=0:04:33
[02/17 03:55:40] mask2former INFO: Inference done 504/1092. Dataloading: 0.0073 s/iter. Inference: 0.2943 s/iter. Eval: 0.1550 s/iter. Total: 0.4567 s/iter. ETA=0:04:28
[02/17 03:55:45] mask2former INFO: Inference done 515/1092. Dataloading: 0.0072 s/iter. Inference: 0.2945 s/iter. Eval: 0.1548 s/iter. Total: 0.4566 s/iter. ETA=0:04:23
[02/17 03:55:50] mask2former INFO: Inference done 526/1092. Dataloading: 0.0073 s/iter. Inference: 0.2944 s/iter. Eval: 0.1550 s/iter. Total: 0.4569 s/iter. ETA=0:04:18
[02/17 03:55:56] mask2former INFO: Inference done 538/1092. Dataloading: 0.0073 s/iter. Inference: 0.2943 s/iter. Eval: 0.1547 s/iter. Total: 0.4565 s/iter. ETA=0:04:12
[02/17 03:56:01] mask2former INFO: Inference done 548/1092. Dataloading: 0.0073 s/iter. Inference: 0.2951 s/iter. Eval: 0.1548 s/iter. Total: 0.4574 s/iter. ETA=0:04:08
[02/17 03:56:06] mask2former INFO: Inference done 559/1092. Dataloading: 0.0074 s/iter. Inference: 0.2952 s/iter. Eval: 0.1546 s/iter. Total: 0.4574 s/iter. ETA=0:04:03
[02/17 03:56:11] mask2former INFO: Inference done 570/1092. Dataloading: 0.0074 s/iter. Inference: 0.2954 s/iter. Eval: 0.1546 s/iter. Total: 0.4576 s/iter. ETA=0:03:58
[02/17 03:56:16] mask2former INFO: Inference done 581/1092. Dataloading: 0.0074 s/iter. Inference: 0.2954 s/iter. Eval: 0.1548 s/iter. Total: 0.4578 s/iter. ETA=0:03:53
[02/17 03:56:22] mask2former INFO: Inference done 593/1092. Dataloading: 0.0074 s/iter. Inference: 0.2952 s/iter. Eval: 0.1549 s/iter. Total: 0.4576 s/iter. ETA=0:03:48
[02/17 03:56:27] mask2former INFO: Inference done 605/1092. Dataloading: 0.0074 s/iter. Inference: 0.2949 s/iter. Eval: 0.1545 s/iter. Total: 0.4569 s/iter. ETA=0:03:42
[02/17 03:56:32] mask2former INFO: Inference done 616/1092. Dataloading: 0.0074 s/iter. Inference: 0.2950 s/iter. Eval: 0.1544 s/iter. Total: 0.4570 s/iter. ETA=0:03:37
[02/17 03:56:37] mask2former INFO: Inference done 627/1092. Dataloading: 0.0074 s/iter. Inference: 0.2952 s/iter. Eval: 0.1544 s/iter. Total: 0.4571 s/iter. ETA=0:03:32
[02/17 03:56:42] mask2former INFO: Inference done 638/1092. Dataloading: 0.0074 s/iter. Inference: 0.2958 s/iter. Eval: 0.1541 s/iter. Total: 0.4574 s/iter. ETA=0:03:27
[02/17 03:56:47] mask2former INFO: Inference done 650/1092. Dataloading: 0.0075 s/iter. Inference: 0.2955 s/iter. Eval: 0.1540 s/iter. Total: 0.4571 s/iter. ETA=0:03:22
[02/17 03:56:52] mask2former INFO: Inference done 661/1092. Dataloading: 0.0075 s/iter. Inference: 0.2954 s/iter. Eval: 0.1542 s/iter. Total: 0.4572 s/iter. ETA=0:03:17
[02/17 03:56:57] mask2former INFO: Inference done 672/1092. Dataloading: 0.0075 s/iter. Inference: 0.2953 s/iter. Eval: 0.1543 s/iter. Total: 0.4572 s/iter. ETA=0:03:12
[02/17 03:57:02] mask2former INFO: Inference done 685/1092. Dataloading: 0.0074 s/iter. Inference: 0.2944 s/iter. Eval: 0.1539 s/iter. Total: 0.4559 s/iter. ETA=0:03:05
[02/17 03:57:08] mask2former INFO: Inference done 699/1092. Dataloading: 0.0074 s/iter. Inference: 0.2933 s/iter. Eval: 0.1532 s/iter. Total: 0.4540 s/iter. ETA=0:02:58
[02/17 03:57:13] mask2former INFO: Inference done 711/1092. Dataloading: 0.0073 s/iter. Inference: 0.2932 s/iter. Eval: 0.1528 s/iter. Total: 0.4535 s/iter. ETA=0:02:52
[02/17 03:57:18] mask2former INFO: Inference done 718/1092. Dataloading: 0.0074 s/iter. Inference: 0.2947 s/iter. Eval: 0.1545 s/iter. Total: 0.4568 s/iter. ETA=0:02:50
[02/17 03:57:24] mask2former INFO: Inference done 724/1092. Dataloading: 0.0075 s/iter. Inference: 0.2969 s/iter. Eval: 0.1561 s/iter. Total: 0.4607 s/iter. ETA=0:02:49
[02/17 03:57:29] mask2former INFO: Inference done 735/1092. Dataloading: 0.0075 s/iter. Inference: 0.2972 s/iter. Eval: 0.1562 s/iter. Total: 0.4610 s/iter. ETA=0:02:44
[02/17 03:57:34] mask2former INFO: Inference done 743/1092. Dataloading: 0.0076 s/iter. Inference: 0.2980 s/iter. Eval: 0.1576 s/iter. Total: 0.4633 s/iter. ETA=0:02:41
[02/17 03:57:40] mask2former INFO: Inference done 753/1092. Dataloading: 0.0076 s/iter. Inference: 0.2986 s/iter. Eval: 0.1580 s/iter. Total: 0.4643 s/iter. ETA=0:02:37
[02/17 03:57:45] mask2former INFO: Inference done 764/1092. Dataloading: 0.0076 s/iter. Inference: 0.2985 s/iter. Eval: 0.1580 s/iter. Total: 0.4643 s/iter. ETA=0:02:32
[02/17 03:57:50] mask2former INFO: Inference done 776/1092. Dataloading: 0.0076 s/iter. Inference: 0.2985 s/iter. Eval: 0.1578 s/iter. Total: 0.4640 s/iter. ETA=0:02:26
[02/17 03:57:55] mask2former INFO: Inference done 787/1092. Dataloading: 0.0076 s/iter. Inference: 0.2986 s/iter. Eval: 0.1578 s/iter. Total: 0.4641 s/iter. ETA=0:02:21
[02/17 03:58:00] mask2former INFO: Inference done 797/1092. Dataloading: 0.0075 s/iter. Inference: 0.2992 s/iter. Eval: 0.1577 s/iter. Total: 0.4646 s/iter. ETA=0:02:17
[02/17 03:58:06] mask2former INFO: Inference done 808/1092. Dataloading: 0.0075 s/iter. Inference: 0.2996 s/iter. Eval: 0.1577 s/iter. Total: 0.4649 s/iter. ETA=0:02:12
[02/17 03:58:11] mask2former INFO: Inference done 819/1092. Dataloading: 0.0075 s/iter. Inference: 0.2997 s/iter. Eval: 0.1580 s/iter. Total: 0.4653 s/iter. ETA=0:02:07
[02/17 03:58:16] mask2former INFO: Inference done 830/1092. Dataloading: 0.0075 s/iter. Inference: 0.2998 s/iter. Eval: 0.1580 s/iter. Total: 0.4655 s/iter. ETA=0:02:01
[02/17 03:58:22] mask2former INFO: Inference done 841/1092. Dataloading: 0.0075 s/iter. Inference: 0.3000 s/iter. Eval: 0.1578 s/iter. Total: 0.4654 s/iter. ETA=0:01:56
[02/17 03:58:27] mask2former INFO: Inference done 852/1092. Dataloading: 0.0075 s/iter. Inference: 0.2999 s/iter. Eval: 0.1578 s/iter. Total: 0.4654 s/iter. ETA=0:01:51
[02/17 03:58:32] mask2former INFO: Inference done 863/1092. Dataloading: 0.0075 s/iter. Inference: 0.2998 s/iter. Eval: 0.1579 s/iter. Total: 0.4654 s/iter. ETA=0:01:46
[02/17 03:58:37] mask2former INFO: Inference done 874/1092. Dataloading: 0.0075 s/iter. Inference: 0.2997 s/iter. Eval: 0.1579 s/iter. Total: 0.4652 s/iter. ETA=0:01:41
[02/17 03:58:42] mask2former INFO: Inference done 885/1092. Dataloading: 0.0075 s/iter. Inference: 0.2997 s/iter. Eval: 0.1583 s/iter. Total: 0.4657 s/iter. ETA=0:01:36
[02/17 03:58:48] mask2former INFO: Inference done 897/1092. Dataloading: 0.0075 s/iter. Inference: 0.2995 s/iter. Eval: 0.1585 s/iter. Total: 0.4656 s/iter. ETA=0:01:30
[02/17 03:58:53] mask2former INFO: Inference done 908/1092. Dataloading: 0.0075 s/iter. Inference: 0.2996 s/iter. Eval: 0.1586 s/iter. Total: 0.4659 s/iter. ETA=0:01:25
[02/17 03:58:58] mask2former INFO: Inference done 920/1092. Dataloading: 0.0075 s/iter. Inference: 0.2994 s/iter. Eval: 0.1585 s/iter. Total: 0.4655 s/iter. ETA=0:01:20
[02/17 03:59:04] mask2former INFO: Inference done 930/1092. Dataloading: 0.0075 s/iter. Inference: 0.2998 s/iter. Eval: 0.1587 s/iter. Total: 0.4661 s/iter. ETA=0:01:15
[02/17 03:59:09] mask2former INFO: Inference done 941/1092. Dataloading: 0.0075 s/iter. Inference: 0.2997 s/iter. Eval: 0.1587 s/iter. Total: 0.4661 s/iter. ETA=0:01:10
[02/17 03:59:14] mask2former INFO: Inference done 952/1092. Dataloading: 0.0074 s/iter. Inference: 0.2998 s/iter. Eval: 0.1588 s/iter. Total: 0.4662 s/iter. ETA=0:01:05
[02/17 03:59:19] mask2former INFO: Inference done 963/1092. Dataloading: 0.0074 s/iter. Inference: 0.3000 s/iter. Eval: 0.1587 s/iter. Total: 0.4662 s/iter. ETA=0:01:00
[02/17 03:59:24] mask2former INFO: Inference done 974/1092. Dataloading: 0.0074 s/iter. Inference: 0.2998 s/iter. Eval: 0.1589 s/iter. Total: 0.4662 s/iter. ETA=0:00:55
[02/17 03:59:30] mask2former INFO: Inference done 987/1092. Dataloading: 0.0074 s/iter. Inference: 0.2994 s/iter. Eval: 0.1585 s/iter. Total: 0.4655 s/iter. ETA=0:00:48
[02/17 03:59:35] mask2former INFO: Inference done 997/1092. Dataloading: 0.0074 s/iter. Inference: 0.2999 s/iter. Eval: 0.1584 s/iter. Total: 0.4658 s/iter. ETA=0:00:44
[02/17 03:59:40] mask2former INFO: Inference done 1009/1092. Dataloading: 0.0074 s/iter. Inference: 0.2998 s/iter. Eval: 0.1582 s/iter. Total: 0.4655 s/iter. ETA=0:00:38
[02/17 03:59:45] mask2former INFO: Inference done 1022/1092. Dataloading: 0.0074 s/iter. Inference: 0.2993 s/iter. Eval: 0.1578 s/iter. Total: 0.4647 s/iter. ETA=0:00:32
[02/17 03:59:50] mask2former INFO: Inference done 1036/1092. Dataloading: 0.0074 s/iter. Inference: 0.2987 s/iter. Eval: 0.1573 s/iter. Total: 0.4636 s/iter. ETA=0:00:25
[02/17 03:59:55] mask2former INFO: Inference done 1049/1092. Dataloading: 0.0074 s/iter. Inference: 0.2980 s/iter. Eval: 0.1571 s/iter. Total: 0.4627 s/iter. ETA=0:00:19
[02/17 04:00:01] mask2former INFO: Inference done 1061/1092. Dataloading: 0.0073 s/iter. Inference: 0.2979 s/iter. Eval: 0.1568 s/iter. Total: 0.4622 s/iter. ETA=0:00:14
[02/17 04:00:06] mask2former INFO: Inference done 1074/1092. Dataloading: 0.0073 s/iter. Inference: 0.2974 s/iter. Eval: 0.1567 s/iter. Total: 0.4615 s/iter. ETA=0:00:08
[02/17 04:00:11] mask2former INFO: Inference done 1087/1092. Dataloading: 0.0073 s/iter. Inference: 0.2968 s/iter. Eval: 0.1563 s/iter. Total: 0.4606 s/iter. ETA=0:00:02
[02/17 05:52:47] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 05:52:47] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 05:52:47] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 05:53:03] mask2former INFO: Inference done 11/1092. Dataloading: 0.0040 s/iter. Inference: 0.2803 s/iter. Eval: 0.1502 s/iter. Total: 0.4346 s/iter. ETA=0:07:49
[02/17 05:53:08] mask2former INFO: Inference done 24/1092. Dataloading: 0.0060 s/iter. Inference: 0.2707 s/iter. Eval: 0.1431 s/iter. Total: 0.4199 s/iter. ETA=0:07:28
[02/17 05:53:13] mask2former INFO: Inference done 35/1092. Dataloading: 0.0067 s/iter. Inference: 0.2801 s/iter. Eval: 0.1460 s/iter. Total: 0.4329 s/iter. ETA=0:07:37
[02/17 05:53:19] mask2former INFO: Inference done 46/1092. Dataloading: 0.0077 s/iter. Inference: 0.2883 s/iter. Eval: 0.1537 s/iter. Total: 0.4499 s/iter. ETA=0:07:50
[02/17 05:53:24] mask2former INFO: Inference done 56/1092. Dataloading: 0.0076 s/iter. Inference: 0.2937 s/iter. Eval: 0.1586 s/iter. Total: 0.4600 s/iter. ETA=0:07:56
[02/17 05:53:29] mask2former INFO: Inference done 67/1092. Dataloading: 0.0073 s/iter. Inference: 0.2955 s/iter. Eval: 0.1580 s/iter. Total: 0.4608 s/iter. ETA=0:07:52
[02/17 05:53:34] mask2former INFO: Inference done 78/1092. Dataloading: 0.0073 s/iter. Inference: 0.2981 s/iter. Eval: 0.1584 s/iter. Total: 0.4639 s/iter. ETA=0:07:50
[02/17 05:53:39] mask2former INFO: Inference done 89/1092. Dataloading: 0.0071 s/iter. Inference: 0.2986 s/iter. Eval: 0.1595 s/iter. Total: 0.4653 s/iter. ETA=0:07:46
[02/17 05:53:44] mask2former INFO: Inference done 99/1092. Dataloading: 0.0075 s/iter. Inference: 0.3008 s/iter. Eval: 0.1631 s/iter. Total: 0.4715 s/iter. ETA=0:07:48
[02/17 05:53:50] mask2former INFO: Inference done 110/1092. Dataloading: 0.0074 s/iter. Inference: 0.3020 s/iter. Eval: 0.1635 s/iter. Total: 0.4731 s/iter. ETA=0:07:44
[02/17 05:53:55] mask2former INFO: Inference done 121/1092. Dataloading: 0.0073 s/iter. Inference: 0.3013 s/iter. Eval: 0.1634 s/iter. Total: 0.4720 s/iter. ETA=0:07:38
[02/17 05:54:00] mask2former INFO: Inference done 133/1092. Dataloading: 0.0071 s/iter. Inference: 0.3001 s/iter. Eval: 0.1608 s/iter. Total: 0.4682 s/iter. ETA=0:07:28
[02/17 05:54:05] mask2former INFO: Inference done 144/1092. Dataloading: 0.0070 s/iter. Inference: 0.3012 s/iter. Eval: 0.1593 s/iter. Total: 0.4677 s/iter. ETA=0:07:23
[02/17 05:54:11] mask2former INFO: Inference done 155/1092. Dataloading: 0.0069 s/iter. Inference: 0.3023 s/iter. Eval: 0.1610 s/iter. Total: 0.4703 s/iter. ETA=0:07:20
[02/17 05:54:16] mask2former INFO: Inference done 165/1092. Dataloading: 0.0069 s/iter. Inference: 0.3042 s/iter. Eval: 0.1613 s/iter. Total: 0.4725 s/iter. ETA=0:07:17
[02/17 05:54:21] mask2former INFO: Inference done 177/1092. Dataloading: 0.0068 s/iter. Inference: 0.3036 s/iter. Eval: 0.1601 s/iter. Total: 0.4706 s/iter. ETA=0:07:10
[02/17 05:54:26] mask2former INFO: Inference done 188/1092. Dataloading: 0.0067 s/iter. Inference: 0.3041 s/iter. Eval: 0.1591 s/iter. Total: 0.4701 s/iter. ETA=0:07:04
[02/17 05:54:31] mask2former INFO: Inference done 199/1092. Dataloading: 0.0067 s/iter. Inference: 0.3049 s/iter. Eval: 0.1587 s/iter. Total: 0.4704 s/iter. ETA=0:07:00
[02/17 05:54:37] mask2former INFO: Inference done 210/1092. Dataloading: 0.0067 s/iter. Inference: 0.3067 s/iter. Eval: 0.1584 s/iter. Total: 0.4719 s/iter. ETA=0:06:56
[02/17 05:54:42] mask2former INFO: Inference done 221/1092. Dataloading: 0.0069 s/iter. Inference: 0.3059 s/iter. Eval: 0.1584 s/iter. Total: 0.4713 s/iter. ETA=0:06:50
[02/17 05:54:47] mask2former INFO: Inference done 232/1092. Dataloading: 0.0069 s/iter. Inference: 0.3055 s/iter. Eval: 0.1580 s/iter. Total: 0.4706 s/iter. ETA=0:06:44
[02/17 05:54:52] mask2former INFO: Inference done 242/1092. Dataloading: 0.0069 s/iter. Inference: 0.3065 s/iter. Eval: 0.1586 s/iter. Total: 0.4721 s/iter. ETA=0:06:41
[02/17 05:54:57] mask2former INFO: Inference done 253/1092. Dataloading: 0.0068 s/iter. Inference: 0.3069 s/iter. Eval: 0.1586 s/iter. Total: 0.4723 s/iter. ETA=0:06:36
[02/17 05:55:02] mask2former INFO: Inference done 264/1092. Dataloading: 0.0068 s/iter. Inference: 0.3073 s/iter. Eval: 0.1582 s/iter. Total: 0.4724 s/iter. ETA=0:06:31
[02/17 05:55:08] mask2former INFO: Inference done 276/1092. Dataloading: 0.0068 s/iter. Inference: 0.3056 s/iter. Eval: 0.1581 s/iter. Total: 0.4706 s/iter. ETA=0:06:24
[02/17 05:55:13] mask2former INFO: Inference done 288/1092. Dataloading: 0.0068 s/iter. Inference: 0.3050 s/iter. Eval: 0.1573 s/iter. Total: 0.4692 s/iter. ETA=0:06:17
[02/17 05:55:18] mask2former INFO: Inference done 299/1092. Dataloading: 0.0069 s/iter. Inference: 0.3045 s/iter. Eval: 0.1573 s/iter. Total: 0.4687 s/iter. ETA=0:06:11
[02/17 05:55:23] mask2former INFO: Inference done 311/1092. Dataloading: 0.0068 s/iter. Inference: 0.3038 s/iter. Eval: 0.1574 s/iter. Total: 0.4681 s/iter. ETA=0:06:05
[02/17 05:55:29] mask2former INFO: Inference done 323/1092. Dataloading: 0.0068 s/iter. Inference: 0.3035 s/iter. Eval: 0.1563 s/iter. Total: 0.4667 s/iter. ETA=0:05:58
[02/17 05:55:34] mask2former INFO: Inference done 335/1092. Dataloading: 0.0069 s/iter. Inference: 0.3023 s/iter. Eval: 0.1557 s/iter. Total: 0.4649 s/iter. ETA=0:05:51
[02/17 05:55:39] mask2former INFO: Inference done 347/1092. Dataloading: 0.0069 s/iter. Inference: 0.3012 s/iter. Eval: 0.1555 s/iter. Total: 0.4636 s/iter. ETA=0:05:45
[02/17 05:55:44] mask2former INFO: Inference done 358/1092. Dataloading: 0.0068 s/iter. Inference: 0.3020 s/iter. Eval: 0.1553 s/iter. Total: 0.4642 s/iter. ETA=0:05:40
[02/17 05:55:49] mask2former INFO: Inference done 370/1092. Dataloading: 0.0069 s/iter. Inference: 0.3020 s/iter. Eval: 0.1547 s/iter. Total: 0.4637 s/iter. ETA=0:05:34
[02/17 05:55:55] mask2former INFO: Inference done 381/1092. Dataloading: 0.0069 s/iter. Inference: 0.3025 s/iter. Eval: 0.1547 s/iter. Total: 0.4643 s/iter. ETA=0:05:30
[02/17 05:56:00] mask2former INFO: Inference done 393/1092. Dataloading: 0.0069 s/iter. Inference: 0.3020 s/iter. Eval: 0.1548 s/iter. Total: 0.4638 s/iter. ETA=0:05:24
[02/17 05:56:05] mask2former INFO: Inference done 404/1092. Dataloading: 0.0069 s/iter. Inference: 0.3020 s/iter. Eval: 0.1548 s/iter. Total: 0.4638 s/iter. ETA=0:05:19
[02/17 05:56:10] mask2former INFO: Inference done 415/1092. Dataloading: 0.0070 s/iter. Inference: 0.3021 s/iter. Eval: 0.1547 s/iter. Total: 0.4639 s/iter. ETA=0:05:14
[02/17 05:56:16] mask2former INFO: Inference done 426/1092. Dataloading: 0.0070 s/iter. Inference: 0.3025 s/iter. Eval: 0.1551 s/iter. Total: 0.4647 s/iter. ETA=0:05:09
[02/17 05:56:21] mask2former INFO: Inference done 437/1092. Dataloading: 0.0070 s/iter. Inference: 0.3026 s/iter. Eval: 0.1552 s/iter. Total: 0.4649 s/iter. ETA=0:05:04
[02/17 05:56:26] mask2former INFO: Inference done 449/1092. Dataloading: 0.0070 s/iter. Inference: 0.3026 s/iter. Eval: 0.1547 s/iter. Total: 0.4644 s/iter. ETA=0:04:58
[02/17 05:56:32] mask2former INFO: Inference done 461/1092. Dataloading: 0.0069 s/iter. Inference: 0.3023 s/iter. Eval: 0.1544 s/iter. Total: 0.4638 s/iter. ETA=0:04:52
[02/17 05:56:37] mask2former INFO: Inference done 472/1092. Dataloading: 0.0069 s/iter. Inference: 0.3030 s/iter. Eval: 0.1538 s/iter. Total: 0.4638 s/iter. ETA=0:04:47
[02/17 05:56:42] mask2former INFO: Inference done 483/1092. Dataloading: 0.0069 s/iter. Inference: 0.3029 s/iter. Eval: 0.1537 s/iter. Total: 0.4636 s/iter. ETA=0:04:42
[02/17 05:56:47] mask2former INFO: Inference done 494/1092. Dataloading: 0.0070 s/iter. Inference: 0.3025 s/iter. Eval: 0.1539 s/iter. Total: 0.4635 s/iter. ETA=0:04:37
[02/17 05:56:52] mask2former INFO: Inference done 505/1092. Dataloading: 0.0070 s/iter. Inference: 0.3022 s/iter. Eval: 0.1543 s/iter. Total: 0.4636 s/iter. ETA=0:04:32
[02/17 05:56:57] mask2former INFO: Inference done 516/1092. Dataloading: 0.0070 s/iter. Inference: 0.3025 s/iter. Eval: 0.1544 s/iter. Total: 0.4641 s/iter. ETA=0:04:27
[02/17 05:57:02] mask2former INFO: Inference done 527/1092. Dataloading: 0.0070 s/iter. Inference: 0.3023 s/iter. Eval: 0.1546 s/iter. Total: 0.4641 s/iter. ETA=0:04:22
[02/17 05:57:08] mask2former INFO: Inference done 539/1092. Dataloading: 0.0070 s/iter. Inference: 0.3021 s/iter. Eval: 0.1545 s/iter. Total: 0.4637 s/iter. ETA=0:04:16
[02/17 05:57:13] mask2former INFO: Inference done 550/1092. Dataloading: 0.0070 s/iter. Inference: 0.3020 s/iter. Eval: 0.1549 s/iter. Total: 0.4641 s/iter. ETA=0:04:11
[02/17 05:57:18] mask2former INFO: Inference done 561/1092. Dataloading: 0.0070 s/iter. Inference: 0.3021 s/iter. Eval: 0.1552 s/iter. Total: 0.4645 s/iter. ETA=0:04:06
[02/17 05:57:24] mask2former INFO: Inference done 572/1092. Dataloading: 0.0071 s/iter. Inference: 0.3022 s/iter. Eval: 0.1556 s/iter. Total: 0.4651 s/iter. ETA=0:04:01
[02/17 05:57:29] mask2former INFO: Inference done 584/1092. Dataloading: 0.0071 s/iter. Inference: 0.3022 s/iter. Eval: 0.1553 s/iter. Total: 0.4648 s/iter. ETA=0:03:56
[02/17 05:57:34] mask2former INFO: Inference done 595/1092. Dataloading: 0.0071 s/iter. Inference: 0.3021 s/iter. Eval: 0.1555 s/iter. Total: 0.4648 s/iter. ETA=0:03:51
[02/17 05:57:40] mask2former INFO: Inference done 606/1092. Dataloading: 0.0071 s/iter. Inference: 0.3023 s/iter. Eval: 0.1556 s/iter. Total: 0.4651 s/iter. ETA=0:03:46
[02/17 05:57:45] mask2former INFO: Inference done 617/1092. Dataloading: 0.0071 s/iter. Inference: 0.3027 s/iter. Eval: 0.1558 s/iter. Total: 0.4657 s/iter. ETA=0:03:41
[02/17 05:57:50] mask2former INFO: Inference done 629/1092. Dataloading: 0.0071 s/iter. Inference: 0.3025 s/iter. Eval: 0.1554 s/iter. Total: 0.4650 s/iter. ETA=0:03:35
[02/17 05:57:56] mask2former INFO: Inference done 641/1092. Dataloading: 0.0071 s/iter. Inference: 0.3023 s/iter. Eval: 0.1553 s/iter. Total: 0.4649 s/iter. ETA=0:03:29
[02/17 05:58:01] mask2former INFO: Inference done 653/1092. Dataloading: 0.0072 s/iter. Inference: 0.3021 s/iter. Eval: 0.1547 s/iter. Total: 0.4641 s/iter. ETA=0:03:23
[02/17 05:58:06] mask2former INFO: Inference done 664/1092. Dataloading: 0.0072 s/iter. Inference: 0.3020 s/iter. Eval: 0.1547 s/iter. Total: 0.4640 s/iter. ETA=0:03:18
[02/17 05:58:11] mask2former INFO: Inference done 677/1092. Dataloading: 0.0072 s/iter. Inference: 0.3016 s/iter. Eval: 0.1542 s/iter. Total: 0.4631 s/iter. ETA=0:03:12
[02/17 05:58:17] mask2former INFO: Inference done 689/1092. Dataloading: 0.0071 s/iter. Inference: 0.3016 s/iter. Eval: 0.1541 s/iter. Total: 0.4630 s/iter. ETA=0:03:06
[02/17 05:58:22] mask2former INFO: Inference done 701/1092. Dataloading: 0.0071 s/iter. Inference: 0.3011 s/iter. Eval: 0.1538 s/iter. Total: 0.4622 s/iter. ETA=0:03:00
[02/17 05:58:27] mask2former INFO: Inference done 713/1092. Dataloading: 0.0071 s/iter. Inference: 0.3009 s/iter. Eval: 0.1535 s/iter. Total: 0.4616 s/iter. ETA=0:02:54
[02/17 05:58:32] mask2former INFO: Inference done 725/1092. Dataloading: 0.0071 s/iter. Inference: 0.3004 s/iter. Eval: 0.1535 s/iter. Total: 0.4611 s/iter. ETA=0:02:49
[02/17 05:58:37] mask2former INFO: Inference done 737/1092. Dataloading: 0.0071 s/iter. Inference: 0.3006 s/iter. Eval: 0.1530 s/iter. Total: 0.4608 s/iter. ETA=0:02:43
[02/17 05:58:43] mask2former INFO: Inference done 750/1092. Dataloading: 0.0071 s/iter. Inference: 0.2998 s/iter. Eval: 0.1530 s/iter. Total: 0.4600 s/iter. ETA=0:02:37
[02/17 05:58:48] mask2former INFO: Inference done 762/1092. Dataloading: 0.0071 s/iter. Inference: 0.2997 s/iter. Eval: 0.1529 s/iter. Total: 0.4598 s/iter. ETA=0:02:31
[02/17 05:58:53] mask2former INFO: Inference done 774/1092. Dataloading: 0.0070 s/iter. Inference: 0.2995 s/iter. Eval: 0.1528 s/iter. Total: 0.4595 s/iter. ETA=0:02:26
[02/17 05:58:59] mask2former INFO: Inference done 786/1092. Dataloading: 0.0071 s/iter. Inference: 0.2993 s/iter. Eval: 0.1526 s/iter. Total: 0.4591 s/iter. ETA=0:02:20
[02/17 05:59:04] mask2former INFO: Inference done 798/1092. Dataloading: 0.0071 s/iter. Inference: 0.2990 s/iter. Eval: 0.1523 s/iter. Total: 0.4586 s/iter. ETA=0:02:14
[02/17 05:59:09] mask2former INFO: Inference done 811/1092. Dataloading: 0.0071 s/iter. Inference: 0.2985 s/iter. Eval: 0.1520 s/iter. Total: 0.4578 s/iter. ETA=0:02:08
[02/17 05:59:14] mask2former INFO: Inference done 824/1092. Dataloading: 0.0071 s/iter. Inference: 0.2980 s/iter. Eval: 0.1519 s/iter. Total: 0.4570 s/iter. ETA=0:02:02
[02/17 05:59:20] mask2former INFO: Inference done 837/1092. Dataloading: 0.0071 s/iter. Inference: 0.2974 s/iter. Eval: 0.1517 s/iter. Total: 0.4563 s/iter. ETA=0:01:56
[02/17 05:59:25] mask2former INFO: Inference done 848/1092. Dataloading: 0.0071 s/iter. Inference: 0.2973 s/iter. Eval: 0.1519 s/iter. Total: 0.4564 s/iter. ETA=0:01:51
[02/17 05:59:30] mask2former INFO: Inference done 860/1092. Dataloading: 0.0071 s/iter. Inference: 0.2973 s/iter. Eval: 0.1518 s/iter. Total: 0.4563 s/iter. ETA=0:01:45
[02/17 05:59:35] mask2former INFO: Inference done 872/1092. Dataloading: 0.0070 s/iter. Inference: 0.2971 s/iter. Eval: 0.1517 s/iter. Total: 0.4560 s/iter. ETA=0:01:40
[02/17 05:59:41] mask2former INFO: Inference done 883/1092. Dataloading: 0.0071 s/iter. Inference: 0.2971 s/iter. Eval: 0.1518 s/iter. Total: 0.4561 s/iter. ETA=0:01:35
[02/17 05:59:46] mask2former INFO: Inference done 895/1092. Dataloading: 0.0071 s/iter. Inference: 0.2967 s/iter. Eval: 0.1519 s/iter. Total: 0.4558 s/iter. ETA=0:01:29
[02/17 05:59:51] mask2former INFO: Inference done 906/1092. Dataloading: 0.0070 s/iter. Inference: 0.2968 s/iter. Eval: 0.1519 s/iter. Total: 0.4559 s/iter. ETA=0:01:24
[02/17 05:59:56] mask2former INFO: Inference done 918/1092. Dataloading: 0.0070 s/iter. Inference: 0.2965 s/iter. Eval: 0.1519 s/iter. Total: 0.4556 s/iter. ETA=0:01:19
[02/17 06:00:01] mask2former INFO: Inference done 931/1092. Dataloading: 0.0070 s/iter. Inference: 0.2960 s/iter. Eval: 0.1517 s/iter. Total: 0.4549 s/iter. ETA=0:01:13
[02/17 06:00:07] mask2former INFO: Inference done 943/1092. Dataloading: 0.0070 s/iter. Inference: 0.2958 s/iter. Eval: 0.1518 s/iter. Total: 0.4548 s/iter. ETA=0:01:07
[02/17 06:00:12] mask2former INFO: Inference done 955/1092. Dataloading: 0.0070 s/iter. Inference: 0.2956 s/iter. Eval: 0.1519 s/iter. Total: 0.4546 s/iter. ETA=0:01:02
[02/17 06:00:18] mask2former INFO: Inference done 968/1092. Dataloading: 0.0070 s/iter. Inference: 0.2955 s/iter. Eval: 0.1516 s/iter. Total: 0.4542 s/iter. ETA=0:00:56
[02/17 06:00:23] mask2former INFO: Inference done 980/1092. Dataloading: 0.0070 s/iter. Inference: 0.2955 s/iter. Eval: 0.1514 s/iter. Total: 0.4541 s/iter. ETA=0:00:50
[02/17 06:00:28] mask2former INFO: Inference done 992/1092. Dataloading: 0.0070 s/iter. Inference: 0.2952 s/iter. Eval: 0.1515 s/iter. Total: 0.4538 s/iter. ETA=0:00:45
[02/17 06:00:33] mask2former INFO: Inference done 1005/1092. Dataloading: 0.0070 s/iter. Inference: 0.2947 s/iter. Eval: 0.1512 s/iter. Total: 0.4530 s/iter. ETA=0:00:39
[02/17 06:00:38] mask2former INFO: Inference done 1019/1092. Dataloading: 0.0070 s/iter. Inference: 0.2939 s/iter. Eval: 0.1508 s/iter. Total: 0.4518 s/iter. ETA=0:00:32
[02/17 06:00:43] mask2former INFO: Inference done 1032/1092. Dataloading: 0.0070 s/iter. Inference: 0.2935 s/iter. Eval: 0.1505 s/iter. Total: 0.4510 s/iter. ETA=0:00:27
[02/17 06:00:49] mask2former INFO: Inference done 1046/1092. Dataloading: 0.0069 s/iter. Inference: 0.2929 s/iter. Eval: 0.1500 s/iter. Total: 0.4499 s/iter. ETA=0:00:20
[02/17 06:00:54] mask2former INFO: Inference done 1061/1092. Dataloading: 0.0069 s/iter. Inference: 0.2920 s/iter. Eval: 0.1494 s/iter. Total: 0.4484 s/iter. ETA=0:00:13
[02/17 06:00:59] mask2former INFO: Inference done 1076/1092. Dataloading: 0.0069 s/iter. Inference: 0.2913 s/iter. Eval: 0.1488 s/iter. Total: 0.4470 s/iter. ETA=0:00:07
[02/17 06:01:04] mask2former INFO: Inference done 1091/1092. Dataloading: 0.0068 s/iter. Inference: 0.2905 s/iter. Eval: 0.1482 s/iter. Total: 0.4457 s/iter. ETA=0:00:00
[02/17 07:44:47] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 07:44:48] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 07:44:48] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 07:44:58] mask2former INFO: Inference done 11/1092. Dataloading: 0.0039 s/iter. Inference: 0.1162 s/iter. Eval: 0.1032 s/iter. Total: 0.2234 s/iter. ETA=0:04:01
[02/17 07:45:03] mask2former INFO: Inference done 31/1092. Dataloading: 0.0046 s/iter. Inference: 0.1322 s/iter. Eval: 0.1119 s/iter. Total: 0.2487 s/iter. ETA=0:04:23
[02/17 07:45:08] mask2former INFO: Inference done 52/1092. Dataloading: 0.0050 s/iter. Inference: 0.1284 s/iter. Eval: 0.1111 s/iter. Total: 0.2446 s/iter. ETA=0:04:14
[02/17 07:45:14] mask2former INFO: Inference done 72/1092. Dataloading: 0.0051 s/iter. Inference: 0.1292 s/iter. Eval: 0.1129 s/iter. Total: 0.2473 s/iter. ETA=0:04:12
[02/17 07:45:19] mask2former INFO: Inference done 92/1092. Dataloading: 0.0049 s/iter. Inference: 0.1287 s/iter. Eval: 0.1150 s/iter. Total: 0.2487 s/iter. ETA=0:04:08
[02/17 07:45:24] mask2former INFO: Inference done 112/1092. Dataloading: 0.0049 s/iter. Inference: 0.1285 s/iter. Eval: 0.1160 s/iter. Total: 0.2495 s/iter. ETA=0:04:04
[02/17 07:45:29] mask2former INFO: Inference done 133/1092. Dataloading: 0.0049 s/iter. Inference: 0.1291 s/iter. Eval: 0.1149 s/iter. Total: 0.2490 s/iter. ETA=0:03:58
[02/17 07:45:34] mask2former INFO: Inference done 153/1092. Dataloading: 0.0049 s/iter. Inference: 0.1300 s/iter. Eval: 0.1145 s/iter. Total: 0.2495 s/iter. ETA=0:03:54
[02/17 07:45:39] mask2former INFO: Inference done 174/1092. Dataloading: 0.0049 s/iter. Inference: 0.1300 s/iter. Eval: 0.1138 s/iter. Total: 0.2487 s/iter. ETA=0:03:48
[02/17 07:45:44] mask2former INFO: Inference done 195/1092. Dataloading: 0.0049 s/iter. Inference: 0.1303 s/iter. Eval: 0.1129 s/iter. Total: 0.2482 s/iter. ETA=0:03:42
[02/17 07:45:49] mask2former INFO: Inference done 214/1092. Dataloading: 0.0049 s/iter. Inference: 0.1310 s/iter. Eval: 0.1139 s/iter. Total: 0.2499 s/iter. ETA=0:03:39
[02/17 07:45:54] mask2former INFO: Inference done 235/1092. Dataloading: 0.0049 s/iter. Inference: 0.1314 s/iter. Eval: 0.1136 s/iter. Total: 0.2500 s/iter. ETA=0:03:34
[02/17 07:45:59] mask2former INFO: Inference done 256/1092. Dataloading: 0.0050 s/iter. Inference: 0.1313 s/iter. Eval: 0.1128 s/iter. Total: 0.2491 s/iter. ETA=0:03:28
[02/17 07:46:05] mask2former INFO: Inference done 276/1092. Dataloading: 0.0051 s/iter. Inference: 0.1312 s/iter. Eval: 0.1132 s/iter. Total: 0.2495 s/iter. ETA=0:03:23
[02/17 07:46:10] mask2former INFO: Inference done 296/1092. Dataloading: 0.0051 s/iter. Inference: 0.1320 s/iter. Eval: 0.1129 s/iter. Total: 0.2501 s/iter. ETA=0:03:19
[02/17 07:46:15] mask2former INFO: Inference done 316/1092. Dataloading: 0.0051 s/iter. Inference: 0.1323 s/iter. Eval: 0.1132 s/iter. Total: 0.2506 s/iter. ETA=0:03:14
[02/17 07:46:20] mask2former INFO: Inference done 335/1092. Dataloading: 0.0051 s/iter. Inference: 0.1331 s/iter. Eval: 0.1134 s/iter. Total: 0.2516 s/iter. ETA=0:03:10
[02/17 07:46:25] mask2former INFO: Inference done 355/1092. Dataloading: 0.0051 s/iter. Inference: 0.1335 s/iter. Eval: 0.1133 s/iter. Total: 0.2519 s/iter. ETA=0:03:05
[02/17 07:46:30] mask2former INFO: Inference done 374/1092. Dataloading: 0.0051 s/iter. Inference: 0.1345 s/iter. Eval: 0.1132 s/iter. Total: 0.2528 s/iter. ETA=0:03:01
[02/17 07:46:35] mask2former INFO: Inference done 394/1092. Dataloading: 0.0051 s/iter. Inference: 0.1345 s/iter. Eval: 0.1131 s/iter. Total: 0.2528 s/iter. ETA=0:02:56
[02/17 07:46:40] mask2former INFO: Inference done 413/1092. Dataloading: 0.0051 s/iter. Inference: 0.1351 s/iter. Eval: 0.1134 s/iter. Total: 0.2537 s/iter. ETA=0:02:52
[02/17 07:46:46] mask2former INFO: Inference done 434/1092. Dataloading: 0.0051 s/iter. Inference: 0.1346 s/iter. Eval: 0.1132 s/iter. Total: 0.2531 s/iter. ETA=0:02:46
[02/17 07:46:51] mask2former INFO: Inference done 455/1092. Dataloading: 0.0051 s/iter. Inference: 0.1350 s/iter. Eval: 0.1127 s/iter. Total: 0.2529 s/iter. ETA=0:02:41
[02/17 07:46:56] mask2former INFO: Inference done 476/1092. Dataloading: 0.0051 s/iter. Inference: 0.1354 s/iter. Eval: 0.1118 s/iter. Total: 0.2525 s/iter. ETA=0:02:35
[02/17 07:47:01] mask2former INFO: Inference done 495/1092. Dataloading: 0.0051 s/iter. Inference: 0.1363 s/iter. Eval: 0.1120 s/iter. Total: 0.2534 s/iter. ETA=0:02:31
[02/17 07:47:06] mask2former INFO: Inference done 515/1092. Dataloading: 0.0051 s/iter. Inference: 0.1365 s/iter. Eval: 0.1120 s/iter. Total: 0.2537 s/iter. ETA=0:02:26
[02/17 07:47:12] mask2former INFO: Inference done 534/1092. Dataloading: 0.0051 s/iter. Inference: 0.1370 s/iter. Eval: 0.1122 s/iter. Total: 0.2544 s/iter. ETA=0:02:21
[02/17 07:47:17] mask2former INFO: Inference done 555/1092. Dataloading: 0.0051 s/iter. Inference: 0.1366 s/iter. Eval: 0.1123 s/iter. Total: 0.2540 s/iter. ETA=0:02:16
[02/17 07:47:22] mask2former INFO: Inference done 575/1092. Dataloading: 0.0050 s/iter. Inference: 0.1366 s/iter. Eval: 0.1123 s/iter. Total: 0.2541 s/iter. ETA=0:02:11
[02/17 07:47:27] mask2former INFO: Inference done 596/1092. Dataloading: 0.0050 s/iter. Inference: 0.1365 s/iter. Eval: 0.1122 s/iter. Total: 0.2538 s/iter. ETA=0:02:05
[02/17 07:47:32] mask2former INFO: Inference done 615/1092. Dataloading: 0.0050 s/iter. Inference: 0.1367 s/iter. Eval: 0.1127 s/iter. Total: 0.2545 s/iter. ETA=0:02:01
[02/17 07:47:37] mask2former INFO: Inference done 634/1092. Dataloading: 0.0050 s/iter. Inference: 0.1368 s/iter. Eval: 0.1128 s/iter. Total: 0.2548 s/iter. ETA=0:01:56
[02/17 07:47:42] mask2former INFO: Inference done 654/1092. Dataloading: 0.0050 s/iter. Inference: 0.1371 s/iter. Eval: 0.1126 s/iter. Total: 0.2548 s/iter. ETA=0:01:51
[02/17 07:47:48] mask2former INFO: Inference done 675/1092. Dataloading: 0.0051 s/iter. Inference: 0.1369 s/iter. Eval: 0.1126 s/iter. Total: 0.2546 s/iter. ETA=0:01:46
[02/17 07:47:53] mask2former INFO: Inference done 695/1092. Dataloading: 0.0051 s/iter. Inference: 0.1370 s/iter. Eval: 0.1127 s/iter. Total: 0.2548 s/iter. ETA=0:01:41
[02/17 07:47:58] mask2former INFO: Inference done 715/1092. Dataloading: 0.0051 s/iter. Inference: 0.1369 s/iter. Eval: 0.1127 s/iter. Total: 0.2548 s/iter. ETA=0:01:36
[02/17 07:48:03] mask2former INFO: Inference done 735/1092. Dataloading: 0.0051 s/iter. Inference: 0.1371 s/iter. Eval: 0.1126 s/iter. Total: 0.2549 s/iter. ETA=0:01:30
[02/17 07:48:08] mask2former INFO: Inference done 755/1092. Dataloading: 0.0051 s/iter. Inference: 0.1373 s/iter. Eval: 0.1125 s/iter. Total: 0.2550 s/iter. ETA=0:01:25
[02/17 07:48:13] mask2former INFO: Inference done 775/1092. Dataloading: 0.0051 s/iter. Inference: 0.1374 s/iter. Eval: 0.1124 s/iter. Total: 0.2550 s/iter. ETA=0:01:20
[02/17 07:48:19] mask2former INFO: Inference done 795/1092. Dataloading: 0.0051 s/iter. Inference: 0.1376 s/iter. Eval: 0.1125 s/iter. Total: 0.2553 s/iter. ETA=0:01:15
[02/17 07:48:24] mask2former INFO: Inference done 815/1092. Dataloading: 0.0051 s/iter. Inference: 0.1377 s/iter. Eval: 0.1126 s/iter. Total: 0.2555 s/iter. ETA=0:01:10
[02/17 07:48:29] mask2former INFO: Inference done 835/1092. Dataloading: 0.0051 s/iter. Inference: 0.1379 s/iter. Eval: 0.1125 s/iter. Total: 0.2556 s/iter. ETA=0:01:05
[02/17 07:48:34] mask2former INFO: Inference done 854/1092. Dataloading: 0.0051 s/iter. Inference: 0.1381 s/iter. Eval: 0.1126 s/iter. Total: 0.2559 s/iter. ETA=0:01:00
[02/17 07:48:39] mask2former INFO: Inference done 872/1092. Dataloading: 0.0051 s/iter. Inference: 0.1384 s/iter. Eval: 0.1128 s/iter. Total: 0.2564 s/iter. ETA=0:00:56
[02/17 07:48:44] mask2former INFO: Inference done 891/1092. Dataloading: 0.0051 s/iter. Inference: 0.1388 s/iter. Eval: 0.1128 s/iter. Total: 0.2567 s/iter. ETA=0:00:51
[02/17 07:48:50] mask2former INFO: Inference done 912/1092. Dataloading: 0.0051 s/iter. Inference: 0.1385 s/iter. Eval: 0.1127 s/iter. Total: 0.2564 s/iter. ETA=0:00:46
[02/17 07:48:55] mask2former INFO: Inference done 930/1092. Dataloading: 0.0051 s/iter. Inference: 0.1390 s/iter. Eval: 0.1127 s/iter. Total: 0.2569 s/iter. ETA=0:00:41
[02/17 07:49:00] mask2former INFO: Inference done 950/1092. Dataloading: 0.0051 s/iter. Inference: 0.1391 s/iter. Eval: 0.1126 s/iter. Total: 0.2568 s/iter. ETA=0:00:36
[02/17 07:49:05] mask2former INFO: Inference done 969/1092. Dataloading: 0.0051 s/iter. Inference: 0.1391 s/iter. Eval: 0.1128 s/iter. Total: 0.2570 s/iter. ETA=0:00:31
[02/17 07:49:10] mask2former INFO: Inference done 988/1092. Dataloading: 0.0051 s/iter. Inference: 0.1393 s/iter. Eval: 0.1128 s/iter. Total: 0.2572 s/iter. ETA=0:00:26
[02/17 07:49:15] mask2former INFO: Inference done 1008/1092. Dataloading: 0.0051 s/iter. Inference: 0.1393 s/iter. Eval: 0.1128 s/iter. Total: 0.2572 s/iter. ETA=0:00:21
[02/17 07:49:20] mask2former INFO: Inference done 1028/1092. Dataloading: 0.0051 s/iter. Inference: 0.1394 s/iter. Eval: 0.1127 s/iter. Total: 0.2572 s/iter. ETA=0:00:16
[02/17 07:49:25] mask2former INFO: Inference done 1048/1092. Dataloading: 0.0051 s/iter. Inference: 0.1394 s/iter. Eval: 0.1126 s/iter. Total: 0.2571 s/iter. ETA=0:00:11
[02/17 07:49:30] mask2former INFO: Inference done 1069/1092. Dataloading: 0.0050 s/iter. Inference: 0.1392 s/iter. Eval: 0.1125 s/iter. Total: 0.2568 s/iter. ETA=0:00:05
[02/17 07:49:35] mask2former INFO: Inference done 1091/1092. Dataloading: 0.0050 s/iter. Inference: 0.1388 s/iter. Eval: 0.1123 s/iter. Total: 0.2562 s/iter. ETA=0:00:00
[02/17 08:52:30] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 08:52:30] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 08:52:30] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 08:52:44] mask2former INFO: Inference done 11/1092. Dataloading: 0.0042 s/iter. Inference: 0.2497 s/iter. Eval: 0.1361 s/iter. Total: 0.3900 s/iter. ETA=0:07:01
[02/17 08:52:50] mask2former INFO: Inference done 24/1092. Dataloading: 0.0059 s/iter. Inference: 0.2634 s/iter. Eval: 0.1344 s/iter. Total: 0.4038 s/iter. ETA=0:07:11
[02/17 08:52:55] mask2former INFO: Inference done 37/1092. Dataloading: 0.0057 s/iter. Inference: 0.2621 s/iter. Eval: 0.1301 s/iter. Total: 0.3979 s/iter. ETA=0:06:59
[02/17 08:53:00] mask2former INFO: Inference done 50/1092. Dataloading: 0.0067 s/iter. Inference: 0.2599 s/iter. Eval: 0.1323 s/iter. Total: 0.3991 s/iter. ETA=0:06:55
[02/17 08:53:05] mask2former INFO: Inference done 62/1092. Dataloading: 0.0070 s/iter. Inference: 0.2653 s/iter. Eval: 0.1356 s/iter. Total: 0.4080 s/iter. ETA=0:07:00
[02/17 08:53:11] mask2former INFO: Inference done 75/1092. Dataloading: 0.0067 s/iter. Inference: 0.2637 s/iter. Eval: 0.1357 s/iter. Total: 0.4062 s/iter. ETA=0:06:53
[02/17 08:53:16] mask2former INFO: Inference done 88/1092. Dataloading: 0.0069 s/iter. Inference: 0.2651 s/iter. Eval: 0.1357 s/iter. Total: 0.4078 s/iter. ETA=0:06:49
[02/17 08:53:21] mask2former INFO: Inference done 100/1092. Dataloading: 0.0067 s/iter. Inference: 0.2672 s/iter. Eval: 0.1365 s/iter. Total: 0.4105 s/iter. ETA=0:06:47
[02/17 08:53:26] mask2former INFO: Inference done 113/1092. Dataloading: 0.0065 s/iter. Inference: 0.2642 s/iter. Eval: 0.1374 s/iter. Total: 0.4082 s/iter. ETA=0:06:39
[02/17 08:53:31] mask2former INFO: Inference done 126/1092. Dataloading: 0.0064 s/iter. Inference: 0.2643 s/iter. Eval: 0.1367 s/iter. Total: 0.4076 s/iter. ETA=0:06:33
[02/17 08:53:37] mask2former INFO: Inference done 139/1092. Dataloading: 0.0063 s/iter. Inference: 0.2642 s/iter. Eval: 0.1365 s/iter. Total: 0.4071 s/iter. ETA=0:06:27
[02/17 08:53:42] mask2former INFO: Inference done 152/1092. Dataloading: 0.0063 s/iter. Inference: 0.2652 s/iter. Eval: 0.1355 s/iter. Total: 0.4072 s/iter. ETA=0:06:22
[02/17 08:53:47] mask2former INFO: Inference done 166/1092. Dataloading: 0.0062 s/iter. Inference: 0.2647 s/iter. Eval: 0.1339 s/iter. Total: 0.4050 s/iter. ETA=0:06:15
[02/17 08:53:53] mask2former INFO: Inference done 181/1092. Dataloading: 0.0061 s/iter. Inference: 0.2621 s/iter. Eval: 0.1325 s/iter. Total: 0.4008 s/iter. ETA=0:06:05
[02/17 08:53:58] mask2former INFO: Inference done 195/1092. Dataloading: 0.0060 s/iter. Inference: 0.2610 s/iter. Eval: 0.1307 s/iter. Total: 0.3977 s/iter. ETA=0:05:56
[02/17 08:54:03] mask2former INFO: Inference done 207/1092. Dataloading: 0.0061 s/iter. Inference: 0.2626 s/iter. Eval: 0.1335 s/iter. Total: 0.4022 s/iter. ETA=0:05:55
[02/17 08:54:09] mask2former INFO: Inference done 214/1092. Dataloading: 0.0063 s/iter. Inference: 0.2695 s/iter. Eval: 0.1384 s/iter. Total: 0.4143 s/iter. ETA=0:06:03
[02/17 08:54:14] mask2former INFO: Inference done 221/1092. Dataloading: 0.0066 s/iter. Inference: 0.2765 s/iter. Eval: 0.1440 s/iter. Total: 0.4272 s/iter. ETA=0:06:12
[02/17 08:54:20] mask2former INFO: Inference done 233/1092. Dataloading: 0.0066 s/iter. Inference: 0.2765 s/iter. Eval: 0.1439 s/iter. Total: 0.4270 s/iter. ETA=0:06:06
[02/17 08:54:25] mask2former INFO: Inference done 240/1092. Dataloading: 0.0067 s/iter. Inference: 0.2801 s/iter. Eval: 0.1491 s/iter. Total: 0.4361 s/iter. ETA=0:06:11
[02/17 08:54:30] mask2former INFO: Inference done 252/1092. Dataloading: 0.0067 s/iter. Inference: 0.2794 s/iter. Eval: 0.1492 s/iter. Total: 0.4354 s/iter. ETA=0:06:05
[02/17 08:54:35] mask2former INFO: Inference done 264/1092. Dataloading: 0.0068 s/iter. Inference: 0.2791 s/iter. Eval: 0.1489 s/iter. Total: 0.4349 s/iter. ETA=0:06:00
[02/17 08:54:40] mask2former INFO: Inference done 276/1092. Dataloading: 0.0067 s/iter. Inference: 0.2791 s/iter. Eval: 0.1488 s/iter. Total: 0.4347 s/iter. ETA=0:05:54
[02/17 08:54:45] mask2former INFO: Inference done 289/1092. Dataloading: 0.0067 s/iter. Inference: 0.2786 s/iter. Eval: 0.1472 s/iter. Total: 0.4326 s/iter. ETA=0:05:47
[02/17 08:54:50] mask2former INFO: Inference done 301/1092. Dataloading: 0.0067 s/iter. Inference: 0.2787 s/iter. Eval: 0.1469 s/iter. Total: 0.4323 s/iter. ETA=0:05:41
[02/17 08:54:55] mask2former INFO: Inference done 313/1092. Dataloading: 0.0066 s/iter. Inference: 0.2788 s/iter. Eval: 0.1466 s/iter. Total: 0.4321 s/iter. ETA=0:05:36
[02/17 08:55:00] mask2former INFO: Inference done 326/1092. Dataloading: 0.0066 s/iter. Inference: 0.2778 s/iter. Eval: 0.1460 s/iter. Total: 0.4305 s/iter. ETA=0:05:29
[02/17 08:55:06] mask2former INFO: Inference done 339/1092. Dataloading: 0.0066 s/iter. Inference: 0.2771 s/iter. Eval: 0.1454 s/iter. Total: 0.4292 s/iter. ETA=0:05:23
[02/17 08:55:11] mask2former INFO: Inference done 351/1092. Dataloading: 0.0065 s/iter. Inference: 0.2770 s/iter. Eval: 0.1457 s/iter. Total: 0.4293 s/iter. ETA=0:05:18
[02/17 08:55:16] mask2former INFO: Inference done 364/1092. Dataloading: 0.0065 s/iter. Inference: 0.2768 s/iter. Eval: 0.1450 s/iter. Total: 0.4284 s/iter. ETA=0:05:11
[02/17 08:55:21] mask2former INFO: Inference done 376/1092. Dataloading: 0.0065 s/iter. Inference: 0.2774 s/iter. Eval: 0.1446 s/iter. Total: 0.4286 s/iter. ETA=0:05:06
[02/17 08:55:27] mask2former INFO: Inference done 389/1092. Dataloading: 0.0064 s/iter. Inference: 0.2771 s/iter. Eval: 0.1444 s/iter. Total: 0.4280 s/iter. ETA=0:05:00
[02/17 08:55:32] mask2former INFO: Inference done 401/1092. Dataloading: 0.0064 s/iter. Inference: 0.2772 s/iter. Eval: 0.1442 s/iter. Total: 0.4279 s/iter. ETA=0:04:55
[02/17 08:55:37] mask2former INFO: Inference done 415/1092. Dataloading: 0.0064 s/iter. Inference: 0.2762 s/iter. Eval: 0.1433 s/iter. Total: 0.4260 s/iter. ETA=0:04:48
[02/17 08:55:42] mask2former INFO: Inference done 428/1092. Dataloading: 0.0064 s/iter. Inference: 0.2761 s/iter. Eval: 0.1427 s/iter. Total: 0.4253 s/iter. ETA=0:04:42
[02/17 08:55:47] mask2former INFO: Inference done 441/1092. Dataloading: 0.0064 s/iter. Inference: 0.2756 s/iter. Eval: 0.1424 s/iter. Total: 0.4244 s/iter. ETA=0:04:36
[02/17 08:55:53] mask2former INFO: Inference done 454/1092. Dataloading: 0.0064 s/iter. Inference: 0.2758 s/iter. Eval: 0.1418 s/iter. Total: 0.4241 s/iter. ETA=0:04:30
[02/17 08:55:58] mask2former INFO: Inference done 468/1092. Dataloading: 0.0063 s/iter. Inference: 0.2753 s/iter. Eval: 0.1413 s/iter. Total: 0.4230 s/iter. ETA=0:04:23
[02/17 08:56:03] mask2former INFO: Inference done 481/1092. Dataloading: 0.0063 s/iter. Inference: 0.2759 s/iter. Eval: 0.1405 s/iter. Total: 0.4227 s/iter. ETA=0:04:18
[02/17 08:56:08] mask2former INFO: Inference done 493/1092. Dataloading: 0.0063 s/iter. Inference: 0.2758 s/iter. Eval: 0.1406 s/iter. Total: 0.4228 s/iter. ETA=0:04:13
[02/17 08:56:14] mask2former INFO: Inference done 506/1092. Dataloading: 0.0063 s/iter. Inference: 0.2753 s/iter. Eval: 0.1403 s/iter. Total: 0.4221 s/iter. ETA=0:04:07
[02/17 08:56:19] mask2former INFO: Inference done 519/1092. Dataloading: 0.0063 s/iter. Inference: 0.2752 s/iter. Eval: 0.1400 s/iter. Total: 0.4216 s/iter. ETA=0:04:01
[02/17 08:56:24] mask2former INFO: Inference done 531/1092. Dataloading: 0.0063 s/iter. Inference: 0.2755 s/iter. Eval: 0.1398 s/iter. Total: 0.4216 s/iter. ETA=0:03:56
[02/17 08:56:29] mask2former INFO: Inference done 544/1092. Dataloading: 0.0063 s/iter. Inference: 0.2749 s/iter. Eval: 0.1400 s/iter. Total: 0.4213 s/iter. ETA=0:03:50
[02/17 08:56:34] mask2former INFO: Inference done 557/1092. Dataloading: 0.0063 s/iter. Inference: 0.2744 s/iter. Eval: 0.1399 s/iter. Total: 0.4206 s/iter. ETA=0:03:45
[02/17 08:56:40] mask2former INFO: Inference done 571/1092. Dataloading: 0.0063 s/iter. Inference: 0.2738 s/iter. Eval: 0.1394 s/iter. Total: 0.4196 s/iter. ETA=0:03:38
[02/17 08:56:45] mask2former INFO: Inference done 585/1092. Dataloading: 0.0063 s/iter. Inference: 0.2731 s/iter. Eval: 0.1389 s/iter. Total: 0.4184 s/iter. ETA=0:03:32
[02/17 08:56:50] mask2former INFO: Inference done 597/1092. Dataloading: 0.0063 s/iter. Inference: 0.2735 s/iter. Eval: 0.1388 s/iter. Total: 0.4187 s/iter. ETA=0:03:27
[02/17 08:56:55] mask2former INFO: Inference done 610/1092. Dataloading: 0.0063 s/iter. Inference: 0.2730 s/iter. Eval: 0.1386 s/iter. Total: 0.4180 s/iter. ETA=0:03:21
[02/17 08:57:00] mask2former INFO: Inference done 623/1092. Dataloading: 0.0062 s/iter. Inference: 0.2728 s/iter. Eval: 0.1382 s/iter. Total: 0.4174 s/iter. ETA=0:03:15
[02/17 08:57:05] mask2former INFO: Inference done 636/1092. Dataloading: 0.0062 s/iter. Inference: 0.2726 s/iter. Eval: 0.1383 s/iter. Total: 0.4173 s/iter. ETA=0:03:10
[02/17 08:57:10] mask2former INFO: Inference done 648/1092. Dataloading: 0.0062 s/iter. Inference: 0.2727 s/iter. Eval: 0.1382 s/iter. Total: 0.4173 s/iter. ETA=0:03:05
[02/17 08:57:16] mask2former INFO: Inference done 661/1092. Dataloading: 0.0062 s/iter. Inference: 0.2728 s/iter. Eval: 0.1380 s/iter. Total: 0.4171 s/iter. ETA=0:02:59
[02/17 08:57:21] mask2former INFO: Inference done 672/1092. Dataloading: 0.0062 s/iter. Inference: 0.2732 s/iter. Eval: 0.1383 s/iter. Total: 0.4178 s/iter. ETA=0:02:55
[02/17 08:57:26] mask2former INFO: Inference done 685/1092. Dataloading: 0.0062 s/iter. Inference: 0.2729 s/iter. Eval: 0.1380 s/iter. Total: 0.4172 s/iter. ETA=0:02:49
[02/17 08:57:31] mask2former INFO: Inference done 699/1092. Dataloading: 0.0062 s/iter. Inference: 0.2725 s/iter. Eval: 0.1376 s/iter. Total: 0.4164 s/iter. ETA=0:02:43
[02/17 08:57:36] mask2former INFO: Inference done 712/1092. Dataloading: 0.0062 s/iter. Inference: 0.2724 s/iter. Eval: 0.1374 s/iter. Total: 0.4162 s/iter. ETA=0:02:38
[02/17 08:57:42] mask2former INFO: Inference done 725/1092. Dataloading: 0.0062 s/iter. Inference: 0.2721 s/iter. Eval: 0.1374 s/iter. Total: 0.4158 s/iter. ETA=0:02:32
[02/17 08:57:47] mask2former INFO: Inference done 739/1092. Dataloading: 0.0062 s/iter. Inference: 0.2719 s/iter. Eval: 0.1369 s/iter. Total: 0.4151 s/iter. ETA=0:02:26
[02/17 08:57:52] mask2former INFO: Inference done 753/1092. Dataloading: 0.0062 s/iter. Inference: 0.2717 s/iter. Eval: 0.1367 s/iter. Total: 0.4146 s/iter. ETA=0:02:20
[02/17 08:57:57] mask2former INFO: Inference done 765/1092. Dataloading: 0.0062 s/iter. Inference: 0.2717 s/iter. Eval: 0.1367 s/iter. Total: 0.4147 s/iter. ETA=0:02:15
[02/17 08:58:02] mask2former INFO: Inference done 777/1092. Dataloading: 0.0062 s/iter. Inference: 0.2718 s/iter. Eval: 0.1366 s/iter. Total: 0.4147 s/iter. ETA=0:02:10
[02/17 08:58:08] mask2former INFO: Inference done 790/1092. Dataloading: 0.0062 s/iter. Inference: 0.2718 s/iter. Eval: 0.1364 s/iter. Total: 0.4145 s/iter. ETA=0:02:05
[02/17 08:58:13] mask2former INFO: Inference done 802/1092. Dataloading: 0.0062 s/iter. Inference: 0.2718 s/iter. Eval: 0.1366 s/iter. Total: 0.4148 s/iter. ETA=0:02:00
[02/17 08:58:18] mask2former INFO: Inference done 814/1092. Dataloading: 0.0062 s/iter. Inference: 0.2719 s/iter. Eval: 0.1369 s/iter. Total: 0.4151 s/iter. ETA=0:01:55
[02/17 08:58:23] mask2former INFO: Inference done 828/1092. Dataloading: 0.0062 s/iter. Inference: 0.2716 s/iter. Eval: 0.1366 s/iter. Total: 0.4145 s/iter. ETA=0:01:49
[02/17 08:58:29] mask2former INFO: Inference done 842/1092. Dataloading: 0.0062 s/iter. Inference: 0.2711 s/iter. Eval: 0.1364 s/iter. Total: 0.4139 s/iter. ETA=0:01:43
[02/17 08:58:34] mask2former INFO: Inference done 855/1092. Dataloading: 0.0062 s/iter. Inference: 0.2710 s/iter. Eval: 0.1362 s/iter. Total: 0.4136 s/iter. ETA=0:01:38
[02/17 08:58:39] mask2former INFO: Inference done 868/1092. Dataloading: 0.0062 s/iter. Inference: 0.2708 s/iter. Eval: 0.1362 s/iter. Total: 0.4132 s/iter. ETA=0:01:32
[02/17 08:58:44] mask2former INFO: Inference done 882/1092. Dataloading: 0.0062 s/iter. Inference: 0.2704 s/iter. Eval: 0.1359 s/iter. Total: 0.4126 s/iter. ETA=0:01:26
[02/17 08:58:49] mask2former INFO: Inference done 895/1092. Dataloading: 0.0062 s/iter. Inference: 0.2701 s/iter. Eval: 0.1359 s/iter. Total: 0.4123 s/iter. ETA=0:01:21
[02/17 08:58:54] mask2former INFO: Inference done 908/1092. Dataloading: 0.0062 s/iter. Inference: 0.2701 s/iter. Eval: 0.1359 s/iter. Total: 0.4123 s/iter. ETA=0:01:15
[02/17 08:59:00] mask2former INFO: Inference done 921/1092. Dataloading: 0.0062 s/iter. Inference: 0.2697 s/iter. Eval: 0.1361 s/iter. Total: 0.4121 s/iter. ETA=0:01:10
[02/17 08:59:05] mask2former INFO: Inference done 934/1092. Dataloading: 0.0062 s/iter. Inference: 0.2696 s/iter. Eval: 0.1361 s/iter. Total: 0.4119 s/iter. ETA=0:01:05
[02/17 08:59:10] mask2former INFO: Inference done 947/1092. Dataloading: 0.0062 s/iter. Inference: 0.2694 s/iter. Eval: 0.1361 s/iter. Total: 0.4118 s/iter. ETA=0:00:59
[02/17 08:59:15] mask2former INFO: Inference done 960/1092. Dataloading: 0.0062 s/iter. Inference: 0.2692 s/iter. Eval: 0.1360 s/iter. Total: 0.4115 s/iter. ETA=0:00:54
[02/17 08:59:20] mask2former INFO: Inference done 973/1092. Dataloading: 0.0062 s/iter. Inference: 0.2691 s/iter. Eval: 0.1360 s/iter. Total: 0.4113 s/iter. ETA=0:00:48
[02/17 08:59:26] mask2former INFO: Inference done 986/1092. Dataloading: 0.0062 s/iter. Inference: 0.2689 s/iter. Eval: 0.1360 s/iter. Total: 0.4112 s/iter. ETA=0:00:43
[02/17 08:59:31] mask2former INFO: Inference done 1000/1092. Dataloading: 0.0062 s/iter. Inference: 0.2686 s/iter. Eval: 0.1358 s/iter. Total: 0.4107 s/iter. ETA=0:00:37
[02/17 08:59:36] mask2former INFO: Inference done 1014/1092. Dataloading: 0.0062 s/iter. Inference: 0.2684 s/iter. Eval: 0.1356 s/iter. Total: 0.4103 s/iter. ETA=0:00:32
[02/17 08:59:41] mask2former INFO: Inference done 1027/1092. Dataloading: 0.0062 s/iter. Inference: 0.2684 s/iter. Eval: 0.1355 s/iter. Total: 0.4102 s/iter. ETA=0:00:26
[02/17 08:59:46] mask2former INFO: Inference done 1040/1092. Dataloading: 0.0062 s/iter. Inference: 0.2683 s/iter. Eval: 0.1354 s/iter. Total: 0.4099 s/iter. ETA=0:00:21
[02/17 08:59:52] mask2former INFO: Inference done 1055/1092. Dataloading: 0.0062 s/iter. Inference: 0.2679 s/iter. Eval: 0.1350 s/iter. Total: 0.4091 s/iter. ETA=0:00:15
[02/17 08:59:57] mask2former INFO: Inference done 1069/1092. Dataloading: 0.0062 s/iter. Inference: 0.2676 s/iter. Eval: 0.1348 s/iter. Total: 0.4086 s/iter. ETA=0:00:09
[02/17 09:00:02] mask2former INFO: Inference done 1084/1092. Dataloading: 0.0062 s/iter. Inference: 0.2671 s/iter. Eval: 0.1344 s/iter. Total: 0.4078 s/iter. ETA=0:00:03
[02/17 10:49:53] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 10:49:53] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 10:49:53] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 10:50:08] mask2former INFO: Inference done 11/1092. Dataloading: 0.0038 s/iter. Inference: 0.2455 s/iter. Eval: 0.1203 s/iter. Total: 0.3696 s/iter. ETA=0:06:39
[02/17 10:50:13] mask2former INFO: Inference done 23/1092. Dataloading: 0.0051 s/iter. Inference: 0.2726 s/iter. Eval: 0.1373 s/iter. Total: 0.4151 s/iter. ETA=0:07:23
[02/17 10:50:18] mask2former INFO: Inference done 36/1092. Dataloading: 0.0056 s/iter. Inference: 0.2722 s/iter. Eval: 0.1342 s/iter. Total: 0.4122 s/iter. ETA=0:07:15
[02/17 10:50:24] mask2former INFO: Inference done 49/1092. Dataloading: 0.0057 s/iter. Inference: 0.2737 s/iter. Eval: 0.1333 s/iter. Total: 0.4128 s/iter. ETA=0:07:10
[02/17 10:50:29] mask2former INFO: Inference done 62/1092. Dataloading: 0.0056 s/iter. Inference: 0.2694 s/iter. Eval: 0.1327 s/iter. Total: 0.4078 s/iter. ETA=0:07:00
[02/17 10:50:34] mask2former INFO: Inference done 75/1092. Dataloading: 0.0055 s/iter. Inference: 0.2668 s/iter. Eval: 0.1333 s/iter. Total: 0.4057 s/iter. ETA=0:06:52
[02/17 10:50:39] mask2former INFO: Inference done 88/1092. Dataloading: 0.0054 s/iter. Inference: 0.2641 s/iter. Eval: 0.1330 s/iter. Total: 0.4026 s/iter. ETA=0:06:44
[02/17 10:50:44] mask2former INFO: Inference done 101/1092. Dataloading: 0.0054 s/iter. Inference: 0.2625 s/iter. Eval: 0.1340 s/iter. Total: 0.4020 s/iter. ETA=0:06:38
[02/17 10:50:49] mask2former INFO: Inference done 114/1092. Dataloading: 0.0056 s/iter. Inference: 0.2620 s/iter. Eval: 0.1342 s/iter. Total: 0.4018 s/iter. ETA=0:06:32
[02/17 10:50:55] mask2former INFO: Inference done 128/1092. Dataloading: 0.0057 s/iter. Inference: 0.2590 s/iter. Eval: 0.1352 s/iter. Total: 0.4000 s/iter. ETA=0:06:25
[02/17 10:51:00] mask2former INFO: Inference done 141/1092. Dataloading: 0.0057 s/iter. Inference: 0.2581 s/iter. Eval: 0.1350 s/iter. Total: 0.3989 s/iter. ETA=0:06:19
[02/17 10:51:05] mask2former INFO: Inference done 153/1092. Dataloading: 0.0057 s/iter. Inference: 0.2601 s/iter. Eval: 0.1347 s/iter. Total: 0.4006 s/iter. ETA=0:06:16
[02/17 10:51:10] mask2former INFO: Inference done 165/1092. Dataloading: 0.0058 s/iter. Inference: 0.2635 s/iter. Eval: 0.1340 s/iter. Total: 0.4034 s/iter. ETA=0:06:13
[02/17 10:51:15] mask2former INFO: Inference done 177/1092. Dataloading: 0.0058 s/iter. Inference: 0.2650 s/iter. Eval: 0.1344 s/iter. Total: 0.4053 s/iter. ETA=0:06:10
[02/17 10:51:20] mask2former INFO: Inference done 189/1092. Dataloading: 0.0059 s/iter. Inference: 0.2666 s/iter. Eval: 0.1350 s/iter. Total: 0.4075 s/iter. ETA=0:06:07
[02/17 10:51:26] mask2former INFO: Inference done 201/1092. Dataloading: 0.0059 s/iter. Inference: 0.2676 s/iter. Eval: 0.1353 s/iter. Total: 0.4089 s/iter. ETA=0:06:04
[02/17 10:51:31] mask2former INFO: Inference done 214/1092. Dataloading: 0.0059 s/iter. Inference: 0.2670 s/iter. Eval: 0.1354 s/iter. Total: 0.4084 s/iter. ETA=0:05:58
[02/17 10:51:36] mask2former INFO: Inference done 226/1092. Dataloading: 0.0059 s/iter. Inference: 0.2677 s/iter. Eval: 0.1356 s/iter. Total: 0.4093 s/iter. ETA=0:05:54
[02/17 10:51:41] mask2former INFO: Inference done 239/1092. Dataloading: 0.0059 s/iter. Inference: 0.2684 s/iter. Eval: 0.1355 s/iter. Total: 0.4098 s/iter. ETA=0:05:49
[02/17 10:51:46] mask2former INFO: Inference done 252/1092. Dataloading: 0.0059 s/iter. Inference: 0.2677 s/iter. Eval: 0.1350 s/iter. Total: 0.4086 s/iter. ETA=0:05:43
[02/17 10:51:52] mask2former INFO: Inference done 265/1092. Dataloading: 0.0058 s/iter. Inference: 0.2669 s/iter. Eval: 0.1350 s/iter. Total: 0.4078 s/iter. ETA=0:05:37
[02/17 10:51:57] mask2former INFO: Inference done 278/1092. Dataloading: 0.0058 s/iter. Inference: 0.2669 s/iter. Eval: 0.1345 s/iter. Total: 0.4073 s/iter. ETA=0:05:31
[02/17 10:52:02] mask2former INFO: Inference done 291/1092. Dataloading: 0.0058 s/iter. Inference: 0.2672 s/iter. Eval: 0.1342 s/iter. Total: 0.4073 s/iter. ETA=0:05:26
[02/17 10:52:07] mask2former INFO: Inference done 304/1092. Dataloading: 0.0058 s/iter. Inference: 0.2672 s/iter. Eval: 0.1346 s/iter. Total: 0.4077 s/iter. ETA=0:05:21
[02/17 10:52:13] mask2former INFO: Inference done 317/1092. Dataloading: 0.0058 s/iter. Inference: 0.2670 s/iter. Eval: 0.1343 s/iter. Total: 0.4073 s/iter. ETA=0:05:15
[02/17 10:52:18] mask2former INFO: Inference done 330/1092. Dataloading: 0.0058 s/iter. Inference: 0.2663 s/iter. Eval: 0.1346 s/iter. Total: 0.4069 s/iter. ETA=0:05:10
[02/17 10:52:23] mask2former INFO: Inference done 343/1092. Dataloading: 0.0058 s/iter. Inference: 0.2657 s/iter. Eval: 0.1345 s/iter. Total: 0.4062 s/iter. ETA=0:05:04
[02/17 10:52:28] mask2former INFO: Inference done 356/1092. Dataloading: 0.0058 s/iter. Inference: 0.2659 s/iter. Eval: 0.1346 s/iter. Total: 0.4064 s/iter. ETA=0:04:59
[02/17 10:52:33] mask2former INFO: Inference done 369/1092. Dataloading: 0.0058 s/iter. Inference: 0.2652 s/iter. Eval: 0.1348 s/iter. Total: 0.4059 s/iter. ETA=0:04:53
[02/17 10:52:39] mask2former INFO: Inference done 382/1092. Dataloading: 0.0058 s/iter. Inference: 0.2653 s/iter. Eval: 0.1347 s/iter. Total: 0.4059 s/iter. ETA=0:04:48
[02/17 10:52:44] mask2former INFO: Inference done 395/1092. Dataloading: 0.0058 s/iter. Inference: 0.2652 s/iter. Eval: 0.1344 s/iter. Total: 0.4055 s/iter. ETA=0:04:42
[02/17 10:52:49] mask2former INFO: Inference done 408/1092. Dataloading: 0.0058 s/iter. Inference: 0.2653 s/iter. Eval: 0.1347 s/iter. Total: 0.4059 s/iter. ETA=0:04:37
[02/17 10:52:54] mask2former INFO: Inference done 421/1092. Dataloading: 0.0058 s/iter. Inference: 0.2659 s/iter. Eval: 0.1344 s/iter. Total: 0.4063 s/iter. ETA=0:04:32
[02/17 10:53:00] mask2former INFO: Inference done 433/1092. Dataloading: 0.0059 s/iter. Inference: 0.2661 s/iter. Eval: 0.1348 s/iter. Total: 0.4069 s/iter. ETA=0:04:28
[02/17 10:53:05] mask2former INFO: Inference done 446/1092. Dataloading: 0.0059 s/iter. Inference: 0.2657 s/iter. Eval: 0.1346 s/iter. Total: 0.4063 s/iter. ETA=0:04:22
[02/17 10:53:10] mask2former INFO: Inference done 459/1092. Dataloading: 0.0059 s/iter. Inference: 0.2657 s/iter. Eval: 0.1346 s/iter. Total: 0.4062 s/iter. ETA=0:04:17
[02/17 10:53:15] mask2former INFO: Inference done 472/1092. Dataloading: 0.0059 s/iter. Inference: 0.2662 s/iter. Eval: 0.1338 s/iter. Total: 0.4060 s/iter. ETA=0:04:11
[02/17 10:53:20] mask2former INFO: Inference done 484/1092. Dataloading: 0.0059 s/iter. Inference: 0.2667 s/iter. Eval: 0.1338 s/iter. Total: 0.4064 s/iter. ETA=0:04:07
[02/17 10:53:26] mask2former INFO: Inference done 497/1092. Dataloading: 0.0059 s/iter. Inference: 0.2669 s/iter. Eval: 0.1341 s/iter. Total: 0.4070 s/iter. ETA=0:04:02
[02/17 10:53:31] mask2former INFO: Inference done 509/1092. Dataloading: 0.0059 s/iter. Inference: 0.2669 s/iter. Eval: 0.1343 s/iter. Total: 0.4072 s/iter. ETA=0:03:57
[02/17 10:53:36] mask2former INFO: Inference done 521/1092. Dataloading: 0.0059 s/iter. Inference: 0.2674 s/iter. Eval: 0.1346 s/iter. Total: 0.4081 s/iter. ETA=0:03:53
[02/17 10:53:41] mask2former INFO: Inference done 534/1092. Dataloading: 0.0059 s/iter. Inference: 0.2673 s/iter. Eval: 0.1348 s/iter. Total: 0.4080 s/iter. ETA=0:03:47
[02/17 10:53:46] mask2former INFO: Inference done 547/1092. Dataloading: 0.0059 s/iter. Inference: 0.2672 s/iter. Eval: 0.1346 s/iter. Total: 0.4077 s/iter. ETA=0:03:42
[02/17 10:53:52] mask2former INFO: Inference done 560/1092. Dataloading: 0.0059 s/iter. Inference: 0.2670 s/iter. Eval: 0.1346 s/iter. Total: 0.4076 s/iter. ETA=0:03:36
[02/17 10:53:57] mask2former INFO: Inference done 573/1092. Dataloading: 0.0059 s/iter. Inference: 0.2670 s/iter. Eval: 0.1344 s/iter. Total: 0.4074 s/iter. ETA=0:03:31
[02/17 10:54:02] mask2former INFO: Inference done 586/1092. Dataloading: 0.0059 s/iter. Inference: 0.2670 s/iter. Eval: 0.1343 s/iter. Total: 0.4072 s/iter. ETA=0:03:26
[02/17 10:54:07] mask2former INFO: Inference done 598/1092. Dataloading: 0.0059 s/iter. Inference: 0.2670 s/iter. Eval: 0.1347 s/iter. Total: 0.4077 s/iter. ETA=0:03:21
[02/17 10:54:13] mask2former INFO: Inference done 611/1092. Dataloading: 0.0059 s/iter. Inference: 0.2669 s/iter. Eval: 0.1347 s/iter. Total: 0.4076 s/iter. ETA=0:03:16
[02/17 10:54:18] mask2former INFO: Inference done 624/1092. Dataloading: 0.0059 s/iter. Inference: 0.2669 s/iter. Eval: 0.1349 s/iter. Total: 0.4078 s/iter. ETA=0:03:10
[02/17 10:54:23] mask2former INFO: Inference done 637/1092. Dataloading: 0.0059 s/iter. Inference: 0.2668 s/iter. Eval: 0.1348 s/iter. Total: 0.4077 s/iter. ETA=0:03:05
[02/17 10:54:28] mask2former INFO: Inference done 649/1092. Dataloading: 0.0059 s/iter. Inference: 0.2672 s/iter. Eval: 0.1350 s/iter. Total: 0.4082 s/iter. ETA=0:03:00
[02/17 10:54:34] mask2former INFO: Inference done 662/1092. Dataloading: 0.0059 s/iter. Inference: 0.2673 s/iter. Eval: 0.1351 s/iter. Total: 0.4084 s/iter. ETA=0:02:55
[02/17 10:54:39] mask2former INFO: Inference done 674/1092. Dataloading: 0.0059 s/iter. Inference: 0.2681 s/iter. Eval: 0.1352 s/iter. Total: 0.4093 s/iter. ETA=0:02:51
[02/17 10:54:44] mask2former INFO: Inference done 686/1092. Dataloading: 0.0059 s/iter. Inference: 0.2680 s/iter. Eval: 0.1354 s/iter. Total: 0.4094 s/iter. ETA=0:02:46
[02/17 10:54:50] mask2former INFO: Inference done 699/1092. Dataloading: 0.0059 s/iter. Inference: 0.2680 s/iter. Eval: 0.1354 s/iter. Total: 0.4095 s/iter. ETA=0:02:40
[02/17 10:54:55] mask2former INFO: Inference done 712/1092. Dataloading: 0.0059 s/iter. Inference: 0.2677 s/iter. Eval: 0.1354 s/iter. Total: 0.4091 s/iter. ETA=0:02:35
[02/17 10:55:00] mask2former INFO: Inference done 724/1092. Dataloading: 0.0059 s/iter. Inference: 0.2680 s/iter. Eval: 0.1353 s/iter. Total: 0.4093 s/iter. ETA=0:02:30
[02/17 10:55:05] mask2former INFO: Inference done 736/1092. Dataloading: 0.0059 s/iter. Inference: 0.2682 s/iter. Eval: 0.1353 s/iter. Total: 0.4095 s/iter. ETA=0:02:25
[02/17 10:55:10] mask2former INFO: Inference done 749/1092. Dataloading: 0.0059 s/iter. Inference: 0.2682 s/iter. Eval: 0.1353 s/iter. Total: 0.4096 s/iter. ETA=0:02:20
[02/17 10:55:15] mask2former INFO: Inference done 762/1092. Dataloading: 0.0059 s/iter. Inference: 0.2682 s/iter. Eval: 0.1352 s/iter. Total: 0.4094 s/iter. ETA=0:02:15
[02/17 10:55:21] mask2former INFO: Inference done 775/1092. Dataloading: 0.0059 s/iter. Inference: 0.2684 s/iter. Eval: 0.1351 s/iter. Total: 0.4095 s/iter. ETA=0:02:09
[02/17 10:55:26] mask2former INFO: Inference done 788/1092. Dataloading: 0.0059 s/iter. Inference: 0.2683 s/iter. Eval: 0.1352 s/iter. Total: 0.4095 s/iter. ETA=0:02:04
[02/17 10:55:32] mask2former INFO: Inference done 802/1092. Dataloading: 0.0059 s/iter. Inference: 0.2680 s/iter. Eval: 0.1350 s/iter. Total: 0.4091 s/iter. ETA=0:01:58
[02/17 10:55:37] mask2former INFO: Inference done 815/1092. Dataloading: 0.0060 s/iter. Inference: 0.2679 s/iter. Eval: 0.1351 s/iter. Total: 0.4090 s/iter. ETA=0:01:53
[02/17 10:55:42] mask2former INFO: Inference done 828/1092. Dataloading: 0.0060 s/iter. Inference: 0.2679 s/iter. Eval: 0.1350 s/iter. Total: 0.4090 s/iter. ETA=0:01:47
[02/17 10:55:47] mask2former INFO: Inference done 840/1092. Dataloading: 0.0060 s/iter. Inference: 0.2680 s/iter. Eval: 0.1350 s/iter. Total: 0.4091 s/iter. ETA=0:01:43
[02/17 10:55:52] mask2former INFO: Inference done 853/1092. Dataloading: 0.0060 s/iter. Inference: 0.2680 s/iter. Eval: 0.1350 s/iter. Total: 0.4091 s/iter. ETA=0:01:37
[02/17 10:55:58] mask2former INFO: Inference done 866/1092. Dataloading: 0.0060 s/iter. Inference: 0.2682 s/iter. Eval: 0.1349 s/iter. Total: 0.4091 s/iter. ETA=0:01:32
[02/17 10:56:03] mask2former INFO: Inference done 878/1092. Dataloading: 0.0060 s/iter. Inference: 0.2685 s/iter. Eval: 0.1351 s/iter. Total: 0.4097 s/iter. ETA=0:01:27
[02/17 10:56:08] mask2former INFO: Inference done 891/1092. Dataloading: 0.0060 s/iter. Inference: 0.2683 s/iter. Eval: 0.1353 s/iter. Total: 0.4096 s/iter. ETA=0:01:22
[02/17 10:56:14] mask2former INFO: Inference done 905/1092. Dataloading: 0.0060 s/iter. Inference: 0.2679 s/iter. Eval: 0.1354 s/iter. Total: 0.4094 s/iter. ETA=0:01:16
[02/17 10:56:19] mask2former INFO: Inference done 918/1092. Dataloading: 0.0060 s/iter. Inference: 0.2681 s/iter. Eval: 0.1353 s/iter. Total: 0.4095 s/iter. ETA=0:01:11
[02/17 10:56:24] mask2former INFO: Inference done 929/1092. Dataloading: 0.0060 s/iter. Inference: 0.2685 s/iter. Eval: 0.1355 s/iter. Total: 0.4100 s/iter. ETA=0:01:06
[02/17 10:56:29] mask2former INFO: Inference done 941/1092. Dataloading: 0.0060 s/iter. Inference: 0.2685 s/iter. Eval: 0.1356 s/iter. Total: 0.4101 s/iter. ETA=0:01:01
[02/17 10:56:35] mask2former INFO: Inference done 954/1092. Dataloading: 0.0060 s/iter. Inference: 0.2685 s/iter. Eval: 0.1355 s/iter. Total: 0.4101 s/iter. ETA=0:00:56
[02/17 10:56:40] mask2former INFO: Inference done 967/1092. Dataloading: 0.0060 s/iter. Inference: 0.2684 s/iter. Eval: 0.1355 s/iter. Total: 0.4100 s/iter. ETA=0:00:51
[02/17 10:56:45] mask2former INFO: Inference done 979/1092. Dataloading: 0.0060 s/iter. Inference: 0.2683 s/iter. Eval: 0.1358 s/iter. Total: 0.4101 s/iter. ETA=0:00:46
[02/17 10:56:50] mask2former INFO: Inference done 993/1092. Dataloading: 0.0060 s/iter. Inference: 0.2681 s/iter. Eval: 0.1357 s/iter. Total: 0.4099 s/iter. ETA=0:00:40
[02/17 10:56:56] mask2former INFO: Inference done 1006/1092. Dataloading: 0.0060 s/iter. Inference: 0.2679 s/iter. Eval: 0.1356 s/iter. Total: 0.4096 s/iter. ETA=0:00:35
[02/17 10:57:01] mask2former INFO: Inference done 1019/1092. Dataloading: 0.0060 s/iter. Inference: 0.2678 s/iter. Eval: 0.1355 s/iter. Total: 0.4093 s/iter. ETA=0:00:29
[02/17 10:57:06] mask2former INFO: Inference done 1032/1092. Dataloading: 0.0060 s/iter. Inference: 0.2676 s/iter. Eval: 0.1355 s/iter. Total: 0.4092 s/iter. ETA=0:00:24
[02/17 10:57:11] mask2former INFO: Inference done 1045/1092. Dataloading: 0.0060 s/iter. Inference: 0.2676 s/iter. Eval: 0.1353 s/iter. Total: 0.4090 s/iter. ETA=0:00:19
[02/17 10:57:16] mask2former INFO: Inference done 1058/1092. Dataloading: 0.0059 s/iter. Inference: 0.2676 s/iter. Eval: 0.1351 s/iter. Total: 0.4088 s/iter. ETA=0:00:13
[02/17 10:57:21] mask2former INFO: Inference done 1072/1092. Dataloading: 0.0059 s/iter. Inference: 0.2674 s/iter. Eval: 0.1349 s/iter. Total: 0.4084 s/iter. ETA=0:00:08
[02/17 10:57:26] mask2former INFO: Inference done 1087/1092. Dataloading: 0.0059 s/iter. Inference: 0.2669 s/iter. Eval: 0.1346 s/iter. Total: 0.4075 s/iter. ETA=0:00:02
[02/17 12:48:02] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 12:48:07] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 12:48:07] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 12:48:31] mask2former INFO: Inference done 11/1092. Dataloading: 0.0048 s/iter. Inference: 0.2851 s/iter. Eval: 0.1365 s/iter. Total: 0.4263 s/iter. ETA=0:07:40
[02/17 12:48:37] mask2former INFO: Inference done 25/1092. Dataloading: 0.0054 s/iter. Inference: 0.2630 s/iter. Eval: 0.1220 s/iter. Total: 0.3905 s/iter. ETA=0:06:56
[02/17 12:48:42] mask2former INFO: Inference done 40/1092. Dataloading: 0.0050 s/iter. Inference: 0.2537 s/iter. Eval: 0.1150 s/iter. Total: 0.3738 s/iter. ETA=0:06:33
[02/17 12:48:47] mask2former INFO: Inference done 53/1092. Dataloading: 0.0051 s/iter. Inference: 0.2508 s/iter. Eval: 0.1308 s/iter. Total: 0.3869 s/iter. ETA=0:06:41
[02/17 12:48:52] mask2former INFO: Inference done 59/1092. Dataloading: 0.0059 s/iter. Inference: 0.2769 s/iter. Eval: 0.1546 s/iter. Total: 0.4375 s/iter. ETA=0:07:31
[02/17 12:48:58] mask2former INFO: Inference done 66/1092. Dataloading: 0.0064 s/iter. Inference: 0.2972 s/iter. Eval: 0.1705 s/iter. Total: 0.4743 s/iter. ETA=0:08:06
[02/17 12:49:03] mask2former INFO: Inference done 77/1092. Dataloading: 0.0063 s/iter. Inference: 0.2988 s/iter. Eval: 0.1707 s/iter. Total: 0.4761 s/iter. ETA=0:08:03
[02/17 12:49:09] mask2former INFO: Inference done 87/1092. Dataloading: 0.0066 s/iter. Inference: 0.3016 s/iter. Eval: 0.1772 s/iter. Total: 0.4857 s/iter. ETA=0:08:08
[02/17 12:49:14] mask2former INFO: Inference done 97/1092. Dataloading: 0.0076 s/iter. Inference: 0.3022 s/iter. Eval: 0.1806 s/iter. Total: 0.4907 s/iter. ETA=0:08:08
[02/17 12:49:19] mask2former INFO: Inference done 110/1092. Dataloading: 0.0076 s/iter. Inference: 0.2979 s/iter. Eval: 0.1739 s/iter. Total: 0.4796 s/iter. ETA=0:07:50
[02/17 12:49:24] mask2former INFO: Inference done 123/1092. Dataloading: 0.0074 s/iter. Inference: 0.2933 s/iter. Eval: 0.1695 s/iter. Total: 0.4704 s/iter. ETA=0:07:35
[02/17 12:49:29] mask2former INFO: Inference done 135/1092. Dataloading: 0.0072 s/iter. Inference: 0.2913 s/iter. Eval: 0.1669 s/iter. Total: 0.4655 s/iter. ETA=0:07:25
[02/17 12:49:35] mask2former INFO: Inference done 149/1092. Dataloading: 0.0071 s/iter. Inference: 0.2869 s/iter. Eval: 0.1631 s/iter. Total: 0.4573 s/iter. ETA=0:07:11
[02/17 12:49:40] mask2former INFO: Inference done 161/1092. Dataloading: 0.0070 s/iter. Inference: 0.2870 s/iter. Eval: 0.1605 s/iter. Total: 0.4547 s/iter. ETA=0:07:03
[02/17 12:49:45] mask2former INFO: Inference done 173/1092. Dataloading: 0.0069 s/iter. Inference: 0.2876 s/iter. Eval: 0.1584 s/iter. Total: 0.4532 s/iter. ETA=0:06:56
[02/17 12:49:50] mask2former INFO: Inference done 186/1092. Dataloading: 0.0068 s/iter. Inference: 0.2857 s/iter. Eval: 0.1560 s/iter. Total: 0.4488 s/iter. ETA=0:06:46
[02/17 12:49:55] mask2former INFO: Inference done 198/1092. Dataloading: 0.0069 s/iter. Inference: 0.2857 s/iter. Eval: 0.1545 s/iter. Total: 0.4473 s/iter. ETA=0:06:39
[02/17 12:50:00] mask2former INFO: Inference done 210/1092. Dataloading: 0.0068 s/iter. Inference: 0.2852 s/iter. Eval: 0.1544 s/iter. Total: 0.4466 s/iter. ETA=0:06:33
[02/17 12:50:05] mask2former INFO: Inference done 223/1092. Dataloading: 0.0067 s/iter. Inference: 0.2836 s/iter. Eval: 0.1535 s/iter. Total: 0.4440 s/iter. ETA=0:06:25
[02/17 12:50:11] mask2former INFO: Inference done 236/1092. Dataloading: 0.0067 s/iter. Inference: 0.2828 s/iter. Eval: 0.1523 s/iter. Total: 0.4420 s/iter. ETA=0:06:18
[02/17 12:50:16] mask2former INFO: Inference done 249/1092. Dataloading: 0.0066 s/iter. Inference: 0.2816 s/iter. Eval: 0.1516 s/iter. Total: 0.4400 s/iter. ETA=0:06:10
[02/17 12:50:21] mask2former INFO: Inference done 261/1092. Dataloading: 0.0066 s/iter. Inference: 0.2821 s/iter. Eval: 0.1514 s/iter. Total: 0.4403 s/iter. ETA=0:06:05
[02/17 12:50:26] mask2former INFO: Inference done 273/1092. Dataloading: 0.0066 s/iter. Inference: 0.2813 s/iter. Eval: 0.1513 s/iter. Total: 0.4394 s/iter. ETA=0:05:59
[02/17 12:50:32] mask2former INFO: Inference done 286/1092. Dataloading: 0.0067 s/iter. Inference: 0.2806 s/iter. Eval: 0.1503 s/iter. Total: 0.4377 s/iter. ETA=0:05:52
[02/17 12:50:37] mask2former INFO: Inference done 298/1092. Dataloading: 0.0067 s/iter. Inference: 0.2804 s/iter. Eval: 0.1499 s/iter. Total: 0.4372 s/iter. ETA=0:05:47
[02/17 12:50:42] mask2former INFO: Inference done 312/1092. Dataloading: 0.0066 s/iter. Inference: 0.2793 s/iter. Eval: 0.1487 s/iter. Total: 0.4348 s/iter. ETA=0:05:39
[02/17 12:50:47] mask2former INFO: Inference done 326/1092. Dataloading: 0.0066 s/iter. Inference: 0.2774 s/iter. Eval: 0.1479 s/iter. Total: 0.4321 s/iter. ETA=0:05:30
[02/17 12:50:53] mask2former INFO: Inference done 339/1092. Dataloading: 0.0066 s/iter. Inference: 0.2769 s/iter. Eval: 0.1475 s/iter. Total: 0.4312 s/iter. ETA=0:05:24
[02/17 12:50:58] mask2former INFO: Inference done 352/1092. Dataloading: 0.0066 s/iter. Inference: 0.2763 s/iter. Eval: 0.1472 s/iter. Total: 0.4303 s/iter. ETA=0:05:18
[02/17 12:51:03] mask2former INFO: Inference done 365/1092. Dataloading: 0.0066 s/iter. Inference: 0.2759 s/iter. Eval: 0.1464 s/iter. Total: 0.4291 s/iter. ETA=0:05:11
[02/17 12:51:08] mask2former INFO: Inference done 378/1092. Dataloading: 0.0066 s/iter. Inference: 0.2755 s/iter. Eval: 0.1455 s/iter. Total: 0.4278 s/iter. ETA=0:05:05
[02/17 12:51:13] mask2former INFO: Inference done 390/1092. Dataloading: 0.0067 s/iter. Inference: 0.2756 s/iter. Eval: 0.1454 s/iter. Total: 0.4278 s/iter. ETA=0:05:00
[02/17 12:51:19] mask2former INFO: Inference done 403/1092. Dataloading: 0.0066 s/iter. Inference: 0.2753 s/iter. Eval: 0.1449 s/iter. Total: 0.4270 s/iter. ETA=0:04:54
[02/17 12:51:24] mask2former INFO: Inference done 416/1092. Dataloading: 0.0066 s/iter. Inference: 0.2753 s/iter. Eval: 0.1444 s/iter. Total: 0.4265 s/iter. ETA=0:04:48
[02/17 12:51:29] mask2former INFO: Inference done 429/1092. Dataloading: 0.0066 s/iter. Inference: 0.2750 s/iter. Eval: 0.1442 s/iter. Total: 0.4260 s/iter. ETA=0:04:42
[02/17 12:51:35] mask2former INFO: Inference done 442/1092. Dataloading: 0.0066 s/iter. Inference: 0.2751 s/iter. Eval: 0.1437 s/iter. Total: 0.4255 s/iter. ETA=0:04:36
[02/17 12:51:40] mask2former INFO: Inference done 455/1092. Dataloading: 0.0065 s/iter. Inference: 0.2747 s/iter. Eval: 0.1430 s/iter. Total: 0.4244 s/iter. ETA=0:04:30
[02/17 12:51:45] mask2former INFO: Inference done 468/1092. Dataloading: 0.0065 s/iter. Inference: 0.2746 s/iter. Eval: 0.1426 s/iter. Total: 0.4239 s/iter. ETA=0:04:24
[02/17 12:51:50] mask2former INFO: Inference done 481/1092. Dataloading: 0.0065 s/iter. Inference: 0.2746 s/iter. Eval: 0.1423 s/iter. Total: 0.4236 s/iter. ETA=0:04:18
[02/17 12:51:56] mask2former INFO: Inference done 494/1092. Dataloading: 0.0065 s/iter. Inference: 0.2744 s/iter. Eval: 0.1419 s/iter. Total: 0.4229 s/iter. ETA=0:04:12
[02/17 12:52:01] mask2former INFO: Inference done 506/1092. Dataloading: 0.0065 s/iter. Inference: 0.2747 s/iter. Eval: 0.1418 s/iter. Total: 0.4231 s/iter. ETA=0:04:07
[02/17 12:52:06] mask2former INFO: Inference done 518/1092. Dataloading: 0.0064 s/iter. Inference: 0.2748 s/iter. Eval: 0.1419 s/iter. Total: 0.4232 s/iter. ETA=0:04:02
[02/17 12:52:11] mask2former INFO: Inference done 530/1092. Dataloading: 0.0064 s/iter. Inference: 0.2747 s/iter. Eval: 0.1419 s/iter. Total: 0.4231 s/iter. ETA=0:03:57
[02/17 12:52:16] mask2former INFO: Inference done 543/1092. Dataloading: 0.0064 s/iter. Inference: 0.2746 s/iter. Eval: 0.1417 s/iter. Total: 0.4228 s/iter. ETA=0:03:52
[02/17 12:52:21] mask2former INFO: Inference done 556/1092. Dataloading: 0.0064 s/iter. Inference: 0.2747 s/iter. Eval: 0.1413 s/iter. Total: 0.4225 s/iter. ETA=0:03:46
[02/17 12:52:26] mask2former INFO: Inference done 568/1092. Dataloading: 0.0064 s/iter. Inference: 0.2741 s/iter. Eval: 0.1417 s/iter. Total: 0.4224 s/iter. ETA=0:03:41
[02/17 12:52:32] mask2former INFO: Inference done 580/1092. Dataloading: 0.0064 s/iter. Inference: 0.2743 s/iter. Eval: 0.1415 s/iter. Total: 0.4223 s/iter. ETA=0:03:36
[02/17 12:52:37] mask2former INFO: Inference done 593/1092. Dataloading: 0.0063 s/iter. Inference: 0.2741 s/iter. Eval: 0.1413 s/iter. Total: 0.4219 s/iter. ETA=0:03:30
[02/17 12:52:42] mask2former INFO: Inference done 606/1092. Dataloading: 0.0063 s/iter. Inference: 0.2739 s/iter. Eval: 0.1411 s/iter. Total: 0.4214 s/iter. ETA=0:03:24
[02/17 12:52:47] mask2former INFO: Inference done 619/1092. Dataloading: 0.0064 s/iter. Inference: 0.2736 s/iter. Eval: 0.1407 s/iter. Total: 0.4209 s/iter. ETA=0:03:19
[02/17 12:52:52] mask2former INFO: Inference done 632/1092. Dataloading: 0.0064 s/iter. Inference: 0.2735 s/iter. Eval: 0.1406 s/iter. Total: 0.4206 s/iter. ETA=0:03:13
[02/17 12:52:58] mask2former INFO: Inference done 645/1092. Dataloading: 0.0064 s/iter. Inference: 0.2735 s/iter. Eval: 0.1405 s/iter. Total: 0.4205 s/iter. ETA=0:03:07
[02/17 12:53:03] mask2former INFO: Inference done 658/1092. Dataloading: 0.0064 s/iter. Inference: 0.2734 s/iter. Eval: 0.1402 s/iter. Total: 0.4202 s/iter. ETA=0:03:02
[02/17 12:53:08] mask2former INFO: Inference done 670/1092. Dataloading: 0.0064 s/iter. Inference: 0.2734 s/iter. Eval: 0.1403 s/iter. Total: 0.4201 s/iter. ETA=0:02:57
[02/17 12:53:13] mask2former INFO: Inference done 683/1092. Dataloading: 0.0064 s/iter. Inference: 0.2729 s/iter. Eval: 0.1401 s/iter. Total: 0.4195 s/iter. ETA=0:02:51
[02/17 12:53:18] mask2former INFO: Inference done 695/1092. Dataloading: 0.0064 s/iter. Inference: 0.2730 s/iter. Eval: 0.1403 s/iter. Total: 0.4198 s/iter. ETA=0:02:46
[02/17 12:53:23] mask2former INFO: Inference done 707/1092. Dataloading: 0.0063 s/iter. Inference: 0.2732 s/iter. Eval: 0.1402 s/iter. Total: 0.4199 s/iter. ETA=0:02:41
[02/17 12:53:29] mask2former INFO: Inference done 720/1092. Dataloading: 0.0063 s/iter. Inference: 0.2731 s/iter. Eval: 0.1401 s/iter. Total: 0.4196 s/iter. ETA=0:02:36
[02/17 12:53:34] mask2former INFO: Inference done 732/1092. Dataloading: 0.0063 s/iter. Inference: 0.2732 s/iter. Eval: 0.1399 s/iter. Total: 0.4196 s/iter. ETA=0:02:31
[02/17 12:53:39] mask2former INFO: Inference done 746/1092. Dataloading: 0.0063 s/iter. Inference: 0.2728 s/iter. Eval: 0.1395 s/iter. Total: 0.4188 s/iter. ETA=0:02:24
[02/17 12:53:44] mask2former INFO: Inference done 759/1092. Dataloading: 0.0063 s/iter. Inference: 0.2723 s/iter. Eval: 0.1396 s/iter. Total: 0.4183 s/iter. ETA=0:02:19
[02/17 12:53:49] mask2former INFO: Inference done 771/1092. Dataloading: 0.0063 s/iter. Inference: 0.2723 s/iter. Eval: 0.1396 s/iter. Total: 0.4184 s/iter. ETA=0:02:14
[02/17 12:53:54] mask2former INFO: Inference done 783/1092. Dataloading: 0.0065 s/iter. Inference: 0.2723 s/iter. Eval: 0.1395 s/iter. Total: 0.4184 s/iter. ETA=0:02:09
[02/17 12:53:59] mask2former INFO: Inference done 795/1092. Dataloading: 0.0065 s/iter. Inference: 0.2722 s/iter. Eval: 0.1395 s/iter. Total: 0.4184 s/iter. ETA=0:02:04
[02/17 12:54:05] mask2former INFO: Inference done 808/1092. Dataloading: 0.0065 s/iter. Inference: 0.2721 s/iter. Eval: 0.1395 s/iter. Total: 0.4182 s/iter. ETA=0:01:58
[02/17 12:54:10] mask2former INFO: Inference done 821/1092. Dataloading: 0.0065 s/iter. Inference: 0.2722 s/iter. Eval: 0.1394 s/iter. Total: 0.4182 s/iter. ETA=0:01:53
[02/17 12:54:15] mask2former INFO: Inference done 834/1092. Dataloading: 0.0065 s/iter. Inference: 0.2718 s/iter. Eval: 0.1395 s/iter. Total: 0.4180 s/iter. ETA=0:01:47
[02/17 12:54:21] mask2former INFO: Inference done 847/1092. Dataloading: 0.0066 s/iter. Inference: 0.2718 s/iter. Eval: 0.1394 s/iter. Total: 0.4179 s/iter. ETA=0:01:42
[02/17 12:54:26] mask2former INFO: Inference done 860/1092. Dataloading: 0.0065 s/iter. Inference: 0.2714 s/iter. Eval: 0.1394 s/iter. Total: 0.4174 s/iter. ETA=0:01:36
[02/17 12:54:31] mask2former INFO: Inference done 873/1092. Dataloading: 0.0065 s/iter. Inference: 0.2710 s/iter. Eval: 0.1394 s/iter. Total: 0.4171 s/iter. ETA=0:01:31
[02/17 12:54:36] mask2former INFO: Inference done 885/1092. Dataloading: 0.0065 s/iter. Inference: 0.2713 s/iter. Eval: 0.1396 s/iter. Total: 0.4175 s/iter. ETA=0:01:26
[02/17 12:54:41] mask2former INFO: Inference done 898/1092. Dataloading: 0.0065 s/iter. Inference: 0.2712 s/iter. Eval: 0.1396 s/iter. Total: 0.4174 s/iter. ETA=0:01:20
[02/17 12:54:46] mask2former INFO: Inference done 910/1092. Dataloading: 0.0065 s/iter. Inference: 0.2711 s/iter. Eval: 0.1396 s/iter. Total: 0.4174 s/iter. ETA=0:01:15
[02/17 12:54:52] mask2former INFO: Inference done 923/1092. Dataloading: 0.0065 s/iter. Inference: 0.2712 s/iter. Eval: 0.1396 s/iter. Total: 0.4174 s/iter. ETA=0:01:10
[02/17 12:54:57] mask2former INFO: Inference done 936/1092. Dataloading: 0.0065 s/iter. Inference: 0.2711 s/iter. Eval: 0.1394 s/iter. Total: 0.4172 s/iter. ETA=0:01:05
[02/17 12:55:02] mask2former INFO: Inference done 948/1092. Dataloading: 0.0065 s/iter. Inference: 0.2711 s/iter. Eval: 0.1395 s/iter. Total: 0.4172 s/iter. ETA=0:01:00
[02/17 12:55:07] mask2former INFO: Inference done 960/1092. Dataloading: 0.0065 s/iter. Inference: 0.2713 s/iter. Eval: 0.1395 s/iter. Total: 0.4174 s/iter. ETA=0:00:55
[02/17 12:55:13] mask2former INFO: Inference done 972/1092. Dataloading: 0.0065 s/iter. Inference: 0.2713 s/iter. Eval: 0.1397 s/iter. Total: 0.4176 s/iter. ETA=0:00:50
[02/17 12:55:18] mask2former INFO: Inference done 985/1092. Dataloading: 0.0064 s/iter. Inference: 0.2712 s/iter. Eval: 0.1396 s/iter. Total: 0.4174 s/iter. ETA=0:00:44
[02/17 12:55:23] mask2former INFO: Inference done 998/1092. Dataloading: 0.0064 s/iter. Inference: 0.2711 s/iter. Eval: 0.1396 s/iter. Total: 0.4172 s/iter. ETA=0:00:39
[02/17 12:55:28] mask2former INFO: Inference done 1011/1092. Dataloading: 0.0064 s/iter. Inference: 0.2710 s/iter. Eval: 0.1394 s/iter. Total: 0.4169 s/iter. ETA=0:00:33
[02/17 12:55:33] mask2former INFO: Inference done 1025/1092. Dataloading: 0.0064 s/iter. Inference: 0.2706 s/iter. Eval: 0.1390 s/iter. Total: 0.4161 s/iter. ETA=0:00:27
[02/17 12:55:39] mask2former INFO: Inference done 1040/1092. Dataloading: 0.0064 s/iter. Inference: 0.2701 s/iter. Eval: 0.1387 s/iter. Total: 0.4153 s/iter. ETA=0:00:21
[02/17 12:55:44] mask2former INFO: Inference done 1054/1092. Dataloading: 0.0064 s/iter. Inference: 0.2700 s/iter. Eval: 0.1383 s/iter. Total: 0.4149 s/iter. ETA=0:00:15
[02/17 12:55:49] mask2former INFO: Inference done 1068/1092. Dataloading: 0.0063 s/iter. Inference: 0.2697 s/iter. Eval: 0.1380 s/iter. Total: 0.4142 s/iter. ETA=0:00:09
[02/17 12:55:54] mask2former INFO: Inference done 1082/1092. Dataloading: 0.0063 s/iter. Inference: 0.2694 s/iter. Eval: 0.1377 s/iter. Total: 0.4135 s/iter. ETA=0:00:04
[02/17 14:45:23] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 14:45:24] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 14:45:24] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 14:45:38] mask2former INFO: Inference done 11/1092. Dataloading: 0.0048 s/iter. Inference: 0.2688 s/iter. Eval: 0.1300 s/iter. Total: 0.4036 s/iter. ETA=0:07:16
[02/17 14:45:43] mask2former INFO: Inference done 23/1092. Dataloading: 0.0058 s/iter. Inference: 0.2720 s/iter. Eval: 0.1405 s/iter. Total: 0.4184 s/iter. ETA=0:07:27
[02/17 14:45:48] mask2former INFO: Inference done 36/1092. Dataloading: 0.0063 s/iter. Inference: 0.2686 s/iter. Eval: 0.1348 s/iter. Total: 0.4097 s/iter. ETA=0:07:12
[02/17 14:45:53] mask2former INFO: Inference done 49/1092. Dataloading: 0.0065 s/iter. Inference: 0.2638 s/iter. Eval: 0.1367 s/iter. Total: 0.4071 s/iter. ETA=0:07:04
[02/17 14:45:59] mask2former INFO: Inference done 62/1092. Dataloading: 0.0066 s/iter. Inference: 0.2660 s/iter. Eval: 0.1358 s/iter. Total: 0.4085 s/iter. ETA=0:07:00
[02/17 14:46:04] mask2former INFO: Inference done 75/1092. Dataloading: 0.0064 s/iter. Inference: 0.2648 s/iter. Eval: 0.1347 s/iter. Total: 0.4060 s/iter. ETA=0:06:52
[02/17 14:46:09] mask2former INFO: Inference done 87/1092. Dataloading: 0.0065 s/iter. Inference: 0.2663 s/iter. Eval: 0.1387 s/iter. Total: 0.4116 s/iter. ETA=0:06:53
[02/17 14:46:14] mask2former INFO: Inference done 100/1092. Dataloading: 0.0063 s/iter. Inference: 0.2642 s/iter. Eval: 0.1375 s/iter. Total: 0.4081 s/iter. ETA=0:06:44
[02/17 14:46:19] mask2former INFO: Inference done 112/1092. Dataloading: 0.0062 s/iter. Inference: 0.2652 s/iter. Eval: 0.1380 s/iter. Total: 0.4095 s/iter. ETA=0:06:41
[02/17 14:46:24] mask2former INFO: Inference done 125/1092. Dataloading: 0.0061 s/iter. Inference: 0.2641 s/iter. Eval: 0.1376 s/iter. Total: 0.4080 s/iter. ETA=0:06:34
[02/17 14:46:30] mask2former INFO: Inference done 138/1092. Dataloading: 0.0063 s/iter. Inference: 0.2650 s/iter. Eval: 0.1371 s/iter. Total: 0.4085 s/iter. ETA=0:06:29
[02/17 14:46:35] mask2former INFO: Inference done 151/1092. Dataloading: 0.0062 s/iter. Inference: 0.2652 s/iter. Eval: 0.1369 s/iter. Total: 0.4085 s/iter. ETA=0:06:24
[02/17 14:46:40] mask2former INFO: Inference done 164/1092. Dataloading: 0.0062 s/iter. Inference: 0.2659 s/iter. Eval: 0.1364 s/iter. Total: 0.4086 s/iter. ETA=0:06:19
[02/17 14:46:46] mask2former INFO: Inference done 177/1092. Dataloading: 0.0063 s/iter. Inference: 0.2654 s/iter. Eval: 0.1359 s/iter. Total: 0.4077 s/iter. ETA=0:06:13
[02/17 14:46:51] mask2former INFO: Inference done 190/1092. Dataloading: 0.0063 s/iter. Inference: 0.2647 s/iter. Eval: 0.1350 s/iter. Total: 0.4061 s/iter. ETA=0:06:06
[02/17 14:46:56] mask2former INFO: Inference done 203/1092. Dataloading: 0.0062 s/iter. Inference: 0.2645 s/iter. Eval: 0.1354 s/iter. Total: 0.4062 s/iter. ETA=0:06:01
[02/17 14:47:01] mask2former INFO: Inference done 217/1092. Dataloading: 0.0062 s/iter. Inference: 0.2643 s/iter. Eval: 0.1346 s/iter. Total: 0.4052 s/iter. ETA=0:05:54
[02/17 14:47:07] mask2former INFO: Inference done 230/1092. Dataloading: 0.0063 s/iter. Inference: 0.2643 s/iter. Eval: 0.1349 s/iter. Total: 0.4055 s/iter. ETA=0:05:49
[02/17 14:47:12] mask2former INFO: Inference done 244/1092. Dataloading: 0.0062 s/iter. Inference: 0.2636 s/iter. Eval: 0.1343 s/iter. Total: 0.4042 s/iter. ETA=0:05:42
[02/17 14:47:17] mask2former INFO: Inference done 257/1092. Dataloading: 0.0062 s/iter. Inference: 0.2634 s/iter. Eval: 0.1343 s/iter. Total: 0.4040 s/iter. ETA=0:05:37
[02/17 14:47:23] mask2former INFO: Inference done 270/1092. Dataloading: 0.0062 s/iter. Inference: 0.2636 s/iter. Eval: 0.1344 s/iter. Total: 0.4043 s/iter. ETA=0:05:32
[02/17 14:47:28] mask2former INFO: Inference done 283/1092. Dataloading: 0.0063 s/iter. Inference: 0.2637 s/iter. Eval: 0.1342 s/iter. Total: 0.4043 s/iter. ETA=0:05:27
[02/17 14:47:33] mask2former INFO: Inference done 296/1092. Dataloading: 0.0063 s/iter. Inference: 0.2632 s/iter. Eval: 0.1340 s/iter. Total: 0.4035 s/iter. ETA=0:05:21
[02/17 14:47:38] mask2former INFO: Inference done 309/1092. Dataloading: 0.0062 s/iter. Inference: 0.2632 s/iter. Eval: 0.1338 s/iter. Total: 0.4033 s/iter. ETA=0:05:15
[02/17 14:47:43] mask2former INFO: Inference done 323/1092. Dataloading: 0.0062 s/iter. Inference: 0.2622 s/iter. Eval: 0.1337 s/iter. Total: 0.4022 s/iter. ETA=0:05:09
[02/17 14:47:48] mask2former INFO: Inference done 336/1092. Dataloading: 0.0062 s/iter. Inference: 0.2620 s/iter. Eval: 0.1336 s/iter. Total: 0.4020 s/iter. ETA=0:05:03
[02/17 14:47:53] mask2former INFO: Inference done 348/1092. Dataloading: 0.0062 s/iter. Inference: 0.2622 s/iter. Eval: 0.1340 s/iter. Total: 0.4025 s/iter. ETA=0:04:59
[02/17 14:47:59] mask2former INFO: Inference done 361/1092. Dataloading: 0.0062 s/iter. Inference: 0.2623 s/iter. Eval: 0.1344 s/iter. Total: 0.4030 s/iter. ETA=0:04:54
[02/17 14:48:04] mask2former INFO: Inference done 374/1092. Dataloading: 0.0062 s/iter. Inference: 0.2628 s/iter. Eval: 0.1342 s/iter. Total: 0.4033 s/iter. ETA=0:04:49
[02/17 14:48:09] mask2former INFO: Inference done 387/1092. Dataloading: 0.0062 s/iter. Inference: 0.2628 s/iter. Eval: 0.1342 s/iter. Total: 0.4033 s/iter. ETA=0:04:44
[02/17 14:48:15] mask2former INFO: Inference done 400/1092. Dataloading: 0.0062 s/iter. Inference: 0.2629 s/iter. Eval: 0.1343 s/iter. Total: 0.4035 s/iter. ETA=0:04:39
[02/17 14:48:20] mask2former INFO: Inference done 413/1092. Dataloading: 0.0062 s/iter. Inference: 0.2625 s/iter. Eval: 0.1342 s/iter. Total: 0.4030 s/iter. ETA=0:04:33
[02/17 14:48:25] mask2former INFO: Inference done 426/1092. Dataloading: 0.0063 s/iter. Inference: 0.2627 s/iter. Eval: 0.1337 s/iter. Total: 0.4027 s/iter. ETA=0:04:28
[02/17 14:48:30] mask2former INFO: Inference done 439/1092. Dataloading: 0.0063 s/iter. Inference: 0.2627 s/iter. Eval: 0.1337 s/iter. Total: 0.4028 s/iter. ETA=0:04:23
[02/17 14:48:36] mask2former INFO: Inference done 454/1092. Dataloading: 0.0063 s/iter. Inference: 0.2621 s/iter. Eval: 0.1328 s/iter. Total: 0.4013 s/iter. ETA=0:04:16
[02/17 14:48:41] mask2former INFO: Inference done 468/1092. Dataloading: 0.0062 s/iter. Inference: 0.2619 s/iter. Eval: 0.1323 s/iter. Total: 0.4005 s/iter. ETA=0:04:09
[02/17 14:48:46] mask2former INFO: Inference done 482/1092. Dataloading: 0.0062 s/iter. Inference: 0.2621 s/iter. Eval: 0.1316 s/iter. Total: 0.4000 s/iter. ETA=0:04:04
[02/17 14:48:51] mask2former INFO: Inference done 494/1092. Dataloading: 0.0062 s/iter. Inference: 0.2626 s/iter. Eval: 0.1316 s/iter. Total: 0.4004 s/iter. ETA=0:03:59
[02/17 14:48:56] mask2former INFO: Inference done 506/1092. Dataloading: 0.0062 s/iter. Inference: 0.2630 s/iter. Eval: 0.1319 s/iter. Total: 0.4012 s/iter. ETA=0:03:55
[02/17 14:49:02] mask2former INFO: Inference done 520/1092. Dataloading: 0.0062 s/iter. Inference: 0.2626 s/iter. Eval: 0.1318 s/iter. Total: 0.4007 s/iter. ETA=0:03:49
[02/17 14:49:07] mask2former INFO: Inference done 532/1092. Dataloading: 0.0062 s/iter. Inference: 0.2630 s/iter. Eval: 0.1319 s/iter. Total: 0.4012 s/iter. ETA=0:03:44
[02/17 14:49:12] mask2former INFO: Inference done 545/1092. Dataloading: 0.0062 s/iter. Inference: 0.2627 s/iter. Eval: 0.1319 s/iter. Total: 0.4009 s/iter. ETA=0:03:39
[02/17 14:49:17] mask2former INFO: Inference done 558/1092. Dataloading: 0.0062 s/iter. Inference: 0.2628 s/iter. Eval: 0.1316 s/iter. Total: 0.4007 s/iter. ETA=0:03:33
[02/17 14:49:22] mask2former INFO: Inference done 570/1092. Dataloading: 0.0062 s/iter. Inference: 0.2633 s/iter. Eval: 0.1318 s/iter. Total: 0.4014 s/iter. ETA=0:03:29
[02/17 14:49:27] mask2former INFO: Inference done 583/1092. Dataloading: 0.0062 s/iter. Inference: 0.2635 s/iter. Eval: 0.1317 s/iter. Total: 0.4015 s/iter. ETA=0:03:24
[02/17 14:49:33] mask2former INFO: Inference done 596/1092. Dataloading: 0.0062 s/iter. Inference: 0.2635 s/iter. Eval: 0.1318 s/iter. Total: 0.4016 s/iter. ETA=0:03:19
[02/17 14:49:38] mask2former INFO: Inference done 608/1092. Dataloading: 0.0062 s/iter. Inference: 0.2639 s/iter. Eval: 0.1321 s/iter. Total: 0.4023 s/iter. ETA=0:03:14
[02/17 14:49:43] mask2former INFO: Inference done 621/1092. Dataloading: 0.0062 s/iter. Inference: 0.2640 s/iter. Eval: 0.1319 s/iter. Total: 0.4022 s/iter. ETA=0:03:09
[02/17 14:49:48] mask2former INFO: Inference done 633/1092. Dataloading: 0.0062 s/iter. Inference: 0.2645 s/iter. Eval: 0.1320 s/iter. Total: 0.4028 s/iter. ETA=0:03:04
[02/17 14:49:53] mask2former INFO: Inference done 645/1092. Dataloading: 0.0063 s/iter. Inference: 0.2646 s/iter. Eval: 0.1321 s/iter. Total: 0.4031 s/iter. ETA=0:03:00
[02/17 14:49:59] mask2former INFO: Inference done 658/1092. Dataloading: 0.0063 s/iter. Inference: 0.2646 s/iter. Eval: 0.1321 s/iter. Total: 0.4030 s/iter. ETA=0:02:54
[02/17 14:50:04] mask2former INFO: Inference done 671/1092. Dataloading: 0.0063 s/iter. Inference: 0.2648 s/iter. Eval: 0.1321 s/iter. Total: 0.4033 s/iter. ETA=0:02:49
[02/17 14:50:09] mask2former INFO: Inference done 683/1092. Dataloading: 0.0063 s/iter. Inference: 0.2652 s/iter. Eval: 0.1323 s/iter. Total: 0.4039 s/iter. ETA=0:02:45
[02/17 14:50:15] mask2former INFO: Inference done 697/1092. Dataloading: 0.0063 s/iter. Inference: 0.2650 s/iter. Eval: 0.1322 s/iter. Total: 0.4035 s/iter. ETA=0:02:39
[02/17 14:50:20] mask2former INFO: Inference done 710/1092. Dataloading: 0.0063 s/iter. Inference: 0.2650 s/iter. Eval: 0.1321 s/iter. Total: 0.4035 s/iter. ETA=0:02:34
[02/17 14:50:25] mask2former INFO: Inference done 723/1092. Dataloading: 0.0063 s/iter. Inference: 0.2651 s/iter. Eval: 0.1321 s/iter. Total: 0.4035 s/iter. ETA=0:02:28
[02/17 14:50:30] mask2former INFO: Inference done 736/1092. Dataloading: 0.0063 s/iter. Inference: 0.2652 s/iter. Eval: 0.1321 s/iter. Total: 0.4037 s/iter. ETA=0:02:23
[02/17 14:50:36] mask2former INFO: Inference done 748/1092. Dataloading: 0.0063 s/iter. Inference: 0.2653 s/iter. Eval: 0.1323 s/iter. Total: 0.4040 s/iter. ETA=0:02:18
[02/17 14:50:41] mask2former INFO: Inference done 761/1092. Dataloading: 0.0063 s/iter. Inference: 0.2652 s/iter. Eval: 0.1324 s/iter. Total: 0.4040 s/iter. ETA=0:02:13
[02/17 14:50:46] mask2former INFO: Inference done 774/1092. Dataloading: 0.0063 s/iter. Inference: 0.2651 s/iter. Eval: 0.1324 s/iter. Total: 0.4038 s/iter. ETA=0:02:08
[02/17 14:50:51] mask2former INFO: Inference done 787/1092. Dataloading: 0.0063 s/iter. Inference: 0.2650 s/iter. Eval: 0.1323 s/iter. Total: 0.4037 s/iter. ETA=0:02:03
[02/17 14:50:56] mask2former INFO: Inference done 800/1092. Dataloading: 0.0063 s/iter. Inference: 0.2651 s/iter. Eval: 0.1323 s/iter. Total: 0.4037 s/iter. ETA=0:01:57
[02/17 14:51:02] mask2former INFO: Inference done 813/1092. Dataloading: 0.0063 s/iter. Inference: 0.2652 s/iter. Eval: 0.1322 s/iter. Total: 0.4038 s/iter. ETA=0:01:52
[02/17 14:51:07] mask2former INFO: Inference done 826/1092. Dataloading: 0.0063 s/iter. Inference: 0.2649 s/iter. Eval: 0.1323 s/iter. Total: 0.4037 s/iter. ETA=0:01:47
[02/17 14:51:12] mask2former INFO: Inference done 839/1092. Dataloading: 0.0063 s/iter. Inference: 0.2647 s/iter. Eval: 0.1324 s/iter. Total: 0.4035 s/iter. ETA=0:01:42
[02/17 14:51:17] mask2former INFO: Inference done 851/1092. Dataloading: 0.0063 s/iter. Inference: 0.2651 s/iter. Eval: 0.1327 s/iter. Total: 0.4042 s/iter. ETA=0:01:37
[02/17 14:51:23] mask2former INFO: Inference done 864/1092. Dataloading: 0.0063 s/iter. Inference: 0.2651 s/iter. Eval: 0.1327 s/iter. Total: 0.4042 s/iter. ETA=0:01:32
[02/17 14:51:28] mask2former INFO: Inference done 877/1092. Dataloading: 0.0063 s/iter. Inference: 0.2654 s/iter. Eval: 0.1325 s/iter. Total: 0.4044 s/iter. ETA=0:01:26
[02/17 14:51:33] mask2former INFO: Inference done 890/1092. Dataloading: 0.0063 s/iter. Inference: 0.2655 s/iter. Eval: 0.1325 s/iter. Total: 0.4044 s/iter. ETA=0:01:21
[02/17 14:51:38] mask2former INFO: Inference done 903/1092. Dataloading: 0.0063 s/iter. Inference: 0.2655 s/iter. Eval: 0.1324 s/iter. Total: 0.4043 s/iter. ETA=0:01:16
[02/17 14:51:44] mask2former INFO: Inference done 916/1092. Dataloading: 0.0063 s/iter. Inference: 0.2656 s/iter. Eval: 0.1324 s/iter. Total: 0.4044 s/iter. ETA=0:01:11
[02/17 14:51:49] mask2former INFO: Inference done 929/1092. Dataloading: 0.0063 s/iter. Inference: 0.2655 s/iter. Eval: 0.1323 s/iter. Total: 0.4042 s/iter. ETA=0:01:05
[02/17 14:51:54] mask2former INFO: Inference done 942/1092. Dataloading: 0.0063 s/iter. Inference: 0.2656 s/iter. Eval: 0.1322 s/iter. Total: 0.4042 s/iter. ETA=0:01:00
[02/17 14:51:59] mask2former INFO: Inference done 955/1092. Dataloading: 0.0063 s/iter. Inference: 0.2654 s/iter. Eval: 0.1322 s/iter. Total: 0.4040 s/iter. ETA=0:00:55
[02/17 14:52:04] mask2former INFO: Inference done 968/1092. Dataloading: 0.0063 s/iter. Inference: 0.2654 s/iter. Eval: 0.1323 s/iter. Total: 0.4040 s/iter. ETA=0:00:50
[02/17 14:52:10] mask2former INFO: Inference done 980/1092. Dataloading: 0.0063 s/iter. Inference: 0.2657 s/iter. Eval: 0.1323 s/iter. Total: 0.4044 s/iter. ETA=0:00:45
[02/17 14:52:15] mask2former INFO: Inference done 993/1092. Dataloading: 0.0063 s/iter. Inference: 0.2657 s/iter. Eval: 0.1321 s/iter. Total: 0.4042 s/iter. ETA=0:00:40
[02/17 14:52:20] mask2former INFO: Inference done 1007/1092. Dataloading: 0.0063 s/iter. Inference: 0.2656 s/iter. Eval: 0.1319 s/iter. Total: 0.4039 s/iter. ETA=0:00:34
[02/17 14:52:25] mask2former INFO: Inference done 1020/1092. Dataloading: 0.0063 s/iter. Inference: 0.2658 s/iter. Eval: 0.1317 s/iter. Total: 0.4039 s/iter. ETA=0:00:29
[02/17 14:52:30] mask2former INFO: Inference done 1034/1092. Dataloading: 0.0062 s/iter. Inference: 0.2654 s/iter. Eval: 0.1315 s/iter. Total: 0.4033 s/iter. ETA=0:00:23
[02/17 14:52:36] mask2former INFO: Inference done 1048/1092. Dataloading: 0.0062 s/iter. Inference: 0.2652 s/iter. Eval: 0.1313 s/iter. Total: 0.4028 s/iter. ETA=0:00:17
[02/17 14:52:41] mask2former INFO: Inference done 1063/1092. Dataloading: 0.0062 s/iter. Inference: 0.2647 s/iter. Eval: 0.1310 s/iter. Total: 0.4020 s/iter. ETA=0:00:11
[02/17 14:52:46] mask2former INFO: Inference done 1079/1092. Dataloading: 0.0062 s/iter. Inference: 0.2641 s/iter. Eval: 0.1305 s/iter. Total: 0.4009 s/iter. ETA=0:00:05
[02/17 16:44:15] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 16:44:16] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 16:44:16] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 16:44:31] mask2former INFO: Inference done 11/1092. Dataloading: 0.0038 s/iter. Inference: 0.2728 s/iter. Eval: 0.1246 s/iter. Total: 0.4012 s/iter. ETA=0:07:13
[02/17 16:44:36] mask2former INFO: Inference done 25/1092. Dataloading: 0.0050 s/iter. Inference: 0.2561 s/iter. Eval: 0.1237 s/iter. Total: 0.3849 s/iter. ETA=0:06:50
[02/17 16:44:42] mask2former INFO: Inference done 38/1092. Dataloading: 0.0055 s/iter. Inference: 0.2643 s/iter. Eval: 0.1248 s/iter. Total: 0.3947 s/iter. ETA=0:06:56
[02/17 16:44:47] mask2former INFO: Inference done 51/1092. Dataloading: 0.0059 s/iter. Inference: 0.2663 s/iter. Eval: 0.1272 s/iter. Total: 0.3995 s/iter. ETA=0:06:55
[02/17 16:44:52] mask2former INFO: Inference done 64/1092. Dataloading: 0.0058 s/iter. Inference: 0.2642 s/iter. Eval: 0.1277 s/iter. Total: 0.3977 s/iter. ETA=0:06:48
[02/17 16:44:58] mask2former INFO: Inference done 77/1092. Dataloading: 0.0059 s/iter. Inference: 0.2655 s/iter. Eval: 0.1290 s/iter. Total: 0.4004 s/iter. ETA=0:06:46
[02/17 16:45:03] mask2former INFO: Inference done 89/1092. Dataloading: 0.0060 s/iter. Inference: 0.2667 s/iter. Eval: 0.1322 s/iter. Total: 0.4050 s/iter. ETA=0:06:46
[02/17 16:45:08] mask2former INFO: Inference done 101/1092. Dataloading: 0.0061 s/iter. Inference: 0.2690 s/iter. Eval: 0.1342 s/iter. Total: 0.4094 s/iter. ETA=0:06:45
[02/17 16:45:13] mask2former INFO: Inference done 114/1092. Dataloading: 0.0061 s/iter. Inference: 0.2680 s/iter. Eval: 0.1353 s/iter. Total: 0.4095 s/iter. ETA=0:06:40
[02/17 16:45:19] mask2former INFO: Inference done 127/1092. Dataloading: 0.0061 s/iter. Inference: 0.2681 s/iter. Eval: 0.1360 s/iter. Total: 0.4104 s/iter. ETA=0:06:35
[02/17 16:45:24] mask2former INFO: Inference done 140/1092. Dataloading: 0.0061 s/iter. Inference: 0.2677 s/iter. Eval: 0.1352 s/iter. Total: 0.4091 s/iter. ETA=0:06:29
[02/17 16:45:29] mask2former INFO: Inference done 152/1092. Dataloading: 0.0061 s/iter. Inference: 0.2677 s/iter. Eval: 0.1366 s/iter. Total: 0.4104 s/iter. ETA=0:06:25
[02/17 16:45:34] mask2former INFO: Inference done 165/1092. Dataloading: 0.0061 s/iter. Inference: 0.2690 s/iter. Eval: 0.1353 s/iter. Total: 0.4105 s/iter. ETA=0:06:20
[02/17 16:45:40] mask2former INFO: Inference done 178/1092. Dataloading: 0.0061 s/iter. Inference: 0.2690 s/iter. Eval: 0.1341 s/iter. Total: 0.4093 s/iter. ETA=0:06:14
[02/17 16:45:45] mask2former INFO: Inference done 191/1092. Dataloading: 0.0061 s/iter. Inference: 0.2696 s/iter. Eval: 0.1340 s/iter. Total: 0.4098 s/iter. ETA=0:06:09
[02/17 16:45:50] mask2former INFO: Inference done 204/1092. Dataloading: 0.0061 s/iter. Inference: 0.2688 s/iter. Eval: 0.1342 s/iter. Total: 0.4092 s/iter. ETA=0:06:03
[02/17 16:45:55] mask2former INFO: Inference done 216/1092. Dataloading: 0.0062 s/iter. Inference: 0.2692 s/iter. Eval: 0.1343 s/iter. Total: 0.4098 s/iter. ETA=0:05:58
[02/17 16:46:00] mask2former INFO: Inference done 229/1092. Dataloading: 0.0062 s/iter. Inference: 0.2687 s/iter. Eval: 0.1342 s/iter. Total: 0.4092 s/iter. ETA=0:05:53
[02/17 16:46:06] mask2former INFO: Inference done 242/1092. Dataloading: 0.0063 s/iter. Inference: 0.2680 s/iter. Eval: 0.1348 s/iter. Total: 0.4093 s/iter. ETA=0:05:47
[02/17 16:46:11] mask2former INFO: Inference done 256/1092. Dataloading: 0.0066 s/iter. Inference: 0.2670 s/iter. Eval: 0.1343 s/iter. Total: 0.4080 s/iter. ETA=0:05:41
[02/17 16:46:16] mask2former INFO: Inference done 268/1092. Dataloading: 0.0066 s/iter. Inference: 0.2671 s/iter. Eval: 0.1352 s/iter. Total: 0.4090 s/iter. ETA=0:05:37
[02/17 16:46:21] mask2former INFO: Inference done 280/1092. Dataloading: 0.0066 s/iter. Inference: 0.2676 s/iter. Eval: 0.1355 s/iter. Total: 0.4098 s/iter. ETA=0:05:32
[02/17 16:46:27] mask2former INFO: Inference done 293/1092. Dataloading: 0.0066 s/iter. Inference: 0.2676 s/iter. Eval: 0.1347 s/iter. Total: 0.4090 s/iter. ETA=0:05:26
[02/17 16:46:32] mask2former INFO: Inference done 305/1092. Dataloading: 0.0066 s/iter. Inference: 0.2684 s/iter. Eval: 0.1346 s/iter. Total: 0.4096 s/iter. ETA=0:05:22
[02/17 16:46:37] mask2former INFO: Inference done 318/1092. Dataloading: 0.0066 s/iter. Inference: 0.2685 s/iter. Eval: 0.1346 s/iter. Total: 0.4098 s/iter. ETA=0:05:17
[02/17 16:46:42] mask2former INFO: Inference done 330/1092. Dataloading: 0.0066 s/iter. Inference: 0.2688 s/iter. Eval: 0.1348 s/iter. Total: 0.4103 s/iter. ETA=0:05:12
[02/17 16:46:47] mask2former INFO: Inference done 342/1092. Dataloading: 0.0066 s/iter. Inference: 0.2703 s/iter. Eval: 0.1344 s/iter. Total: 0.4114 s/iter. ETA=0:05:08
[02/17 16:46:53] mask2former INFO: Inference done 355/1092. Dataloading: 0.0066 s/iter. Inference: 0.2703 s/iter. Eval: 0.1341 s/iter. Total: 0.4111 s/iter. ETA=0:05:02
[02/17 16:46:58] mask2former INFO: Inference done 368/1092. Dataloading: 0.0066 s/iter. Inference: 0.2699 s/iter. Eval: 0.1339 s/iter. Total: 0.4105 s/iter. ETA=0:04:57
[02/17 16:47:03] mask2former INFO: Inference done 381/1092. Dataloading: 0.0066 s/iter. Inference: 0.2696 s/iter. Eval: 0.1340 s/iter. Total: 0.4103 s/iter. ETA=0:04:51
[02/17 16:47:08] mask2former INFO: Inference done 394/1092. Dataloading: 0.0066 s/iter. Inference: 0.2699 s/iter. Eval: 0.1340 s/iter. Total: 0.4106 s/iter. ETA=0:04:46
[02/17 16:47:14] mask2former INFO: Inference done 406/1092. Dataloading: 0.0066 s/iter. Inference: 0.2699 s/iter. Eval: 0.1348 s/iter. Total: 0.4114 s/iter. ETA=0:04:42
[02/17 16:47:19] mask2former INFO: Inference done 420/1092. Dataloading: 0.0066 s/iter. Inference: 0.2691 s/iter. Eval: 0.1344 s/iter. Total: 0.4102 s/iter. ETA=0:04:35
[02/17 16:47:24] mask2former INFO: Inference done 433/1092. Dataloading: 0.0065 s/iter. Inference: 0.2692 s/iter. Eval: 0.1342 s/iter. Total: 0.4100 s/iter. ETA=0:04:30
[02/17 16:47:30] mask2former INFO: Inference done 446/1092. Dataloading: 0.0065 s/iter. Inference: 0.2694 s/iter. Eval: 0.1340 s/iter. Total: 0.4100 s/iter. ETA=0:04:24
[02/17 16:47:35] mask2former INFO: Inference done 459/1092. Dataloading: 0.0065 s/iter. Inference: 0.2695 s/iter. Eval: 0.1334 s/iter. Total: 0.4095 s/iter. ETA=0:04:19
[02/17 16:47:40] mask2former INFO: Inference done 471/1092. Dataloading: 0.0065 s/iter. Inference: 0.2697 s/iter. Eval: 0.1334 s/iter. Total: 0.4097 s/iter. ETA=0:04:14
[02/17 16:47:45] mask2former INFO: Inference done 484/1092. Dataloading: 0.0066 s/iter. Inference: 0.2697 s/iter. Eval: 0.1333 s/iter. Total: 0.4096 s/iter. ETA=0:04:09
[02/17 16:47:50] mask2former INFO: Inference done 497/1092. Dataloading: 0.0066 s/iter. Inference: 0.2697 s/iter. Eval: 0.1332 s/iter. Total: 0.4096 s/iter. ETA=0:04:03
[02/17 16:47:55] mask2former INFO: Inference done 509/1092. Dataloading: 0.0066 s/iter. Inference: 0.2703 s/iter. Eval: 0.1332 s/iter. Total: 0.4102 s/iter. ETA=0:03:59
[02/17 16:48:01] mask2former INFO: Inference done 521/1092. Dataloading: 0.0066 s/iter. Inference: 0.2704 s/iter. Eval: 0.1334 s/iter. Total: 0.4105 s/iter. ETA=0:03:54
[02/17 16:48:06] mask2former INFO: Inference done 534/1092. Dataloading: 0.0066 s/iter. Inference: 0.2702 s/iter. Eval: 0.1332 s/iter. Total: 0.4101 s/iter. ETA=0:03:48
[02/17 16:48:11] mask2former INFO: Inference done 546/1092. Dataloading: 0.0066 s/iter. Inference: 0.2705 s/iter. Eval: 0.1334 s/iter. Total: 0.4106 s/iter. ETA=0:03:44
[02/17 16:48:16] mask2former INFO: Inference done 558/1092. Dataloading: 0.0065 s/iter. Inference: 0.2706 s/iter. Eval: 0.1336 s/iter. Total: 0.4108 s/iter. ETA=0:03:39
[02/17 16:48:21] mask2former INFO: Inference done 570/1092. Dataloading: 0.0065 s/iter. Inference: 0.2709 s/iter. Eval: 0.1338 s/iter. Total: 0.4114 s/iter. ETA=0:03:34
[02/17 16:48:27] mask2former INFO: Inference done 583/1092. Dataloading: 0.0065 s/iter. Inference: 0.2712 s/iter. Eval: 0.1335 s/iter. Total: 0.4114 s/iter. ETA=0:03:29
[02/17 16:48:32] mask2former INFO: Inference done 595/1092. Dataloading: 0.0066 s/iter. Inference: 0.2712 s/iter. Eval: 0.1340 s/iter. Total: 0.4118 s/iter. ETA=0:03:24
[02/17 16:48:37] mask2former INFO: Inference done 607/1092. Dataloading: 0.0066 s/iter. Inference: 0.2716 s/iter. Eval: 0.1341 s/iter. Total: 0.4123 s/iter. ETA=0:03:19
[02/17 16:48:42] mask2former INFO: Inference done 620/1092. Dataloading: 0.0066 s/iter. Inference: 0.2716 s/iter. Eval: 0.1342 s/iter. Total: 0.4125 s/iter. ETA=0:03:14
[02/17 16:48:48] mask2former INFO: Inference done 633/1092. Dataloading: 0.0066 s/iter. Inference: 0.2714 s/iter. Eval: 0.1343 s/iter. Total: 0.4123 s/iter. ETA=0:03:09
[02/17 16:48:53] mask2former INFO: Inference done 645/1092. Dataloading: 0.0066 s/iter. Inference: 0.2716 s/iter. Eval: 0.1345 s/iter. Total: 0.4128 s/iter. ETA=0:03:04
[02/17 16:48:58] mask2former INFO: Inference done 658/1092. Dataloading: 0.0066 s/iter. Inference: 0.2716 s/iter. Eval: 0.1342 s/iter. Total: 0.4124 s/iter. ETA=0:02:58
[02/17 16:49:03] mask2former INFO: Inference done 671/1092. Dataloading: 0.0066 s/iter. Inference: 0.2714 s/iter. Eval: 0.1341 s/iter. Total: 0.4122 s/iter. ETA=0:02:53
[02/17 16:49:09] mask2former INFO: Inference done 684/1092. Dataloading: 0.0065 s/iter. Inference: 0.2716 s/iter. Eval: 0.1338 s/iter. Total: 0.4121 s/iter. ETA=0:02:48
[02/17 16:49:14] mask2former INFO: Inference done 697/1092. Dataloading: 0.0065 s/iter. Inference: 0.2716 s/iter. Eval: 0.1337 s/iter. Total: 0.4119 s/iter. ETA=0:02:42
[02/17 16:49:19] mask2former INFO: Inference done 709/1092. Dataloading: 0.0066 s/iter. Inference: 0.2715 s/iter. Eval: 0.1340 s/iter. Total: 0.4122 s/iter. ETA=0:02:37
[02/17 16:49:24] mask2former INFO: Inference done 721/1092. Dataloading: 0.0066 s/iter. Inference: 0.2716 s/iter. Eval: 0.1342 s/iter. Total: 0.4125 s/iter. ETA=0:02:33
[02/17 16:49:29] mask2former INFO: Inference done 734/1092. Dataloading: 0.0066 s/iter. Inference: 0.2718 s/iter. Eval: 0.1340 s/iter. Total: 0.4125 s/iter. ETA=0:02:27
[02/17 16:49:35] mask2former INFO: Inference done 747/1092. Dataloading: 0.0066 s/iter. Inference: 0.2717 s/iter. Eval: 0.1338 s/iter. Total: 0.4123 s/iter. ETA=0:02:22
[02/17 16:49:40] mask2former INFO: Inference done 759/1092. Dataloading: 0.0066 s/iter. Inference: 0.2719 s/iter. Eval: 0.1339 s/iter. Total: 0.4126 s/iter. ETA=0:02:17
[02/17 16:49:45] mask2former INFO: Inference done 772/1092. Dataloading: 0.0066 s/iter. Inference: 0.2720 s/iter. Eval: 0.1339 s/iter. Total: 0.4126 s/iter. ETA=0:02:12
[02/17 16:49:51] mask2former INFO: Inference done 785/1092. Dataloading: 0.0066 s/iter. Inference: 0.2720 s/iter. Eval: 0.1339 s/iter. Total: 0.4126 s/iter. ETA=0:02:06
[02/17 16:49:56] mask2former INFO: Inference done 798/1092. Dataloading: 0.0067 s/iter. Inference: 0.2719 s/iter. Eval: 0.1338 s/iter. Total: 0.4125 s/iter. ETA=0:02:01
[02/17 16:50:01] mask2former INFO: Inference done 811/1092. Dataloading: 0.0066 s/iter. Inference: 0.2718 s/iter. Eval: 0.1339 s/iter. Total: 0.4124 s/iter. ETA=0:01:55
[02/17 16:50:06] mask2former INFO: Inference done 823/1092. Dataloading: 0.0067 s/iter. Inference: 0.2720 s/iter. Eval: 0.1338 s/iter. Total: 0.4126 s/iter. ETA=0:01:50
[02/17 16:50:11] mask2former INFO: Inference done 836/1092. Dataloading: 0.0067 s/iter. Inference: 0.2719 s/iter. Eval: 0.1337 s/iter. Total: 0.4124 s/iter. ETA=0:01:45
[02/17 16:50:17] mask2former INFO: Inference done 849/1092. Dataloading: 0.0067 s/iter. Inference: 0.2716 s/iter. Eval: 0.1338 s/iter. Total: 0.4122 s/iter. ETA=0:01:40
[02/17 16:50:22] mask2former INFO: Inference done 862/1092. Dataloading: 0.0067 s/iter. Inference: 0.2714 s/iter. Eval: 0.1339 s/iter. Total: 0.4121 s/iter. ETA=0:01:34
[02/17 16:50:27] mask2former INFO: Inference done 874/1092. Dataloading: 0.0067 s/iter. Inference: 0.2714 s/iter. Eval: 0.1341 s/iter. Total: 0.4124 s/iter. ETA=0:01:29
[02/17 16:50:32] mask2former INFO: Inference done 886/1092. Dataloading: 0.0067 s/iter. Inference: 0.2717 s/iter. Eval: 0.1342 s/iter. Total: 0.4127 s/iter. ETA=0:01:25
[02/17 16:50:37] mask2former INFO: Inference done 899/1092. Dataloading: 0.0067 s/iter. Inference: 0.2713 s/iter. Eval: 0.1342 s/iter. Total: 0.4123 s/iter. ETA=0:01:19
[02/17 16:50:42] mask2former INFO: Inference done 912/1092. Dataloading: 0.0067 s/iter. Inference: 0.2710 s/iter. Eval: 0.1342 s/iter. Total: 0.4120 s/iter. ETA=0:01:14
[02/17 16:50:48] mask2former INFO: Inference done 924/1092. Dataloading: 0.0067 s/iter. Inference: 0.2711 s/iter. Eval: 0.1344 s/iter. Total: 0.4123 s/iter. ETA=0:01:09
[02/17 16:50:53] mask2former INFO: Inference done 936/1092. Dataloading: 0.0067 s/iter. Inference: 0.2712 s/iter. Eval: 0.1346 s/iter. Total: 0.4126 s/iter. ETA=0:01:04
[02/17 16:50:58] mask2former INFO: Inference done 948/1092. Dataloading: 0.0067 s/iter. Inference: 0.2713 s/iter. Eval: 0.1347 s/iter. Total: 0.4128 s/iter. ETA=0:00:59
[02/17 16:51:03] mask2former INFO: Inference done 960/1092. Dataloading: 0.0067 s/iter. Inference: 0.2716 s/iter. Eval: 0.1348 s/iter. Total: 0.4132 s/iter. ETA=0:00:54
[02/17 16:51:08] mask2former INFO: Inference done 971/1092. Dataloading: 0.0067 s/iter. Inference: 0.2718 s/iter. Eval: 0.1351 s/iter. Total: 0.4137 s/iter. ETA=0:00:50
[02/17 16:51:14] mask2former INFO: Inference done 984/1092. Dataloading: 0.0067 s/iter. Inference: 0.2717 s/iter. Eval: 0.1350 s/iter. Total: 0.4135 s/iter. ETA=0:00:44
[02/17 16:51:19] mask2former INFO: Inference done 997/1092. Dataloading: 0.0067 s/iter. Inference: 0.2718 s/iter. Eval: 0.1348 s/iter. Total: 0.4134 s/iter. ETA=0:00:39
[02/17 16:51:24] mask2former INFO: Inference done 1009/1092. Dataloading: 0.0067 s/iter. Inference: 0.2719 s/iter. Eval: 0.1348 s/iter. Total: 0.4135 s/iter. ETA=0:00:34
[02/17 16:51:29] mask2former INFO: Inference done 1023/1092. Dataloading: 0.0067 s/iter. Inference: 0.2717 s/iter. Eval: 0.1345 s/iter. Total: 0.4130 s/iter. ETA=0:00:28
[02/17 16:51:34] mask2former INFO: Inference done 1037/1092. Dataloading: 0.0067 s/iter. Inference: 0.2714 s/iter. Eval: 0.1344 s/iter. Total: 0.4125 s/iter. ETA=0:00:22
[02/17 16:51:39] mask2former INFO: Inference done 1051/1092. Dataloading: 0.0067 s/iter. Inference: 0.2710 s/iter. Eval: 0.1340 s/iter. Total: 0.4118 s/iter. ETA=0:00:16
[02/17 16:51:44] mask2former INFO: Inference done 1065/1092. Dataloading: 0.0066 s/iter. Inference: 0.2705 s/iter. Eval: 0.1338 s/iter. Total: 0.4111 s/iter. ETA=0:00:11
[02/17 16:51:50] mask2former INFO: Inference done 1079/1092. Dataloading: 0.0066 s/iter. Inference: 0.2702 s/iter. Eval: 0.1335 s/iter. Total: 0.4104 s/iter. ETA=0:00:05
[02/17 18:41:43] mask2former.data.dataset_mappers.mask_former_sceneflow_dataset_mapper INFO: [MaskFormerSceneFlowDatasetMapper] Augmentations used in inference: []
[02/17 18:41:43] detectron2.data.common INFO: Serializing 4370 elements to byte tensors and concatenating them all ...
[02/17 18:41:43] detectron2.data.common INFO: Serialized dataset takes 1.22 MiB
[02/17 18:41:57] mask2former INFO: Inference done 11/1092. Dataloading: 0.0040 s/iter. Inference: 0.2471 s/iter. Eval: 0.1504 s/iter. Total: 0.4015 s/iter. ETA=0:07:13
[02/17 18:42:03] mask2former INFO: Inference done 23/1092. Dataloading: 0.0051 s/iter. Inference: 0.2737 s/iter. Eval: 0.1443 s/iter. Total: 0.4233 s/iter. ETA=0:07:32
[02/17 18:42:08] mask2former INFO: Inference done 35/1092. Dataloading: 0.0053 s/iter. Inference: 0.2718 s/iter. Eval: 0.1452 s/iter. Total: 0.4225 s/iter. ETA=0:07:26
[02/17 18:42:13] mask2former INFO: Inference done 49/1092. Dataloading: 0.0057 s/iter. Inference: 0.2628 s/iter. Eval: 0.1414 s/iter. Total: 0.4101 s/iter. ETA=0:07:07
[02/17 18:42:18] mask2former INFO: Inference done 62/1092. Dataloading: 0.0057 s/iter. Inference: 0.2601 s/iter. Eval: 0.1414 s/iter. Total: 0.4074 s/iter. ETA=0:06:59
[02/17 18:42:23] mask2former INFO: Inference done 74/1092. Dataloading: 0.0056 s/iter. Inference: 0.2657 s/iter. Eval: 0.1410 s/iter. Total: 0.4124 s/iter. ETA=0:06:59
[02/17 18:42:29] mask2former INFO: Inference done 88/1092. Dataloading: 0.0055 s/iter. Inference: 0.2627 s/iter. Eval: 0.1372 s/iter. Total: 0.4055 s/iter. ETA=0:06:47
[02/17 18:42:34] mask2former INFO: Inference done 101/1092. Dataloading: 0.0055 s/iter. Inference: 0.2636 s/iter. Eval: 0.1374 s/iter. Total: 0.4067 s/iter. ETA=0:06:43
[02/17 18:42:39] mask2former INFO: Inference done 113/1092. Dataloading: 0.0056 s/iter. Inference: 0.2647 s/iter. Eval: 0.1380 s/iter. Total: 0.4084 s/iter. ETA=0:06:39
[02/17 18:42:44] mask2former INFO: Inference done 127/1092. Dataloading: 0.0057 s/iter. Inference: 0.2637 s/iter. Eval: 0.1355 s/iter. Total: 0.4051 s/iter. ETA=0:06:30
[02/17 18:42:49] mask2former INFO: Inference done 140/1092. Dataloading: 0.0058 s/iter. Inference: 0.2632 s/iter. Eval: 0.1346 s/iter. Total: 0.4038 s/iter. ETA=0:06:24
[02/17 18:42:55] mask2former INFO: Inference done 154/1092. Dataloading: 0.0058 s/iter. Inference: 0.2615 s/iter. Eval: 0.1340 s/iter. Total: 0.4013 s/iter. ETA=0:06:16
[02/17 18:43:00] mask2former INFO: Inference done 168/1092. Dataloading: 0.0058 s/iter. Inference: 0.2589 s/iter. Eval: 0.1329 s/iter. Total: 0.3977 s/iter. ETA=0:06:07
[02/17 18:43:05] mask2former INFO: Inference done 181/1092. Dataloading: 0.0058 s/iter. Inference: 0.2599 s/iter. Eval: 0.1326 s/iter. Total: 0.3984 s/iter. ETA=0:06:02
[02/17 18:43:10] mask2former INFO: Inference done 195/1092. Dataloading: 0.0058 s/iter. Inference: 0.2587 s/iter. Eval: 0.1323 s/iter. Total: 0.3968 s/iter. ETA=0:05:55
[02/17 18:43:16] mask2former INFO: Inference done 208/1092. Dataloading: 0.0057 s/iter. Inference: 0.2596 s/iter. Eval: 0.1317 s/iter. Total: 0.3971 s/iter. ETA=0:05:51
[02/17 18:43:21] mask2former INFO: Inference done 220/1092. Dataloading: 0.0057 s/iter. Inference: 0.2607 s/iter. Eval: 0.1319 s/iter. Total: 0.3984 s/iter. ETA=0:05:47
[02/17 18:43:26] mask2former INFO: Inference done 232/1092. Dataloading: 0.0059 s/iter. Inference: 0.2613 s/iter. Eval: 0.1320 s/iter. Total: 0.3994 s/iter. ETA=0:05:43
[02/17 18:43:31] mask2former INFO: Inference done 244/1092. Dataloading: 0.0059 s/iter. Inference: 0.2618 s/iter. Eval: 0.1324 s/iter. Total: 0.4003 s/iter. ETA=0:05:39
[02/17 18:43:36] mask2former INFO: Inference done 257/1092. Dataloading: 0.0060 s/iter. Inference: 0.2618 s/iter. Eval: 0.1324 s/iter. Total: 0.4004 s/iter. ETA=0:05:34
[02/17 18:43:41] mask2former INFO: Inference done 269/1092. Dataloading: 0.0060 s/iter. Inference: 0.2623 s/iter. Eval: 0.1329 s/iter. Total: 0.4014 s/iter. ETA=0:05:30
[02/17 18:43:46] mask2former INFO: Inference done 282/1092. Dataloading: 0.0060 s/iter. Inference: 0.2623 s/iter. Eval: 0.1328 s/iter. Total: 0.4012 s/iter. ETA=0:05:24
[02/17 18:43:51] mask2former INFO: Inference done 295/1092. Dataloading: 0.0061 s/iter. Inference: 0.2625 s/iter. Eval: 0.1327 s/iter. Total: 0.4015 s/iter. ETA=0:05:19
[02/17 18:43:57] mask2former INFO: Inference done 308/1092. Dataloading: 0.0061 s/iter. Inference: 0.2628 s/iter. Eval: 0.1322 s/iter. Total: 0.4012 s/iter. ETA=0:05:14
[02/17 18:44:02] mask2former INFO: Inference done 321/1092. Dataloading: 0.0060 s/iter. Inference: 0.2629 s/iter. Eval: 0.1318 s/iter. Total: 0.4008 s/iter. ETA=0:05:08
[02/17 18:44:07] mask2former INFO: Inference done 333/1092. Dataloading: 0.0061 s/iter. Inference: 0.2635 s/iter. Eval: 0.1326 s/iter. Total: 0.4023 s/iter. ETA=0:05:05
[02/17 18:44:12] mask2former INFO: Inference done 345/1092. Dataloading: 0.0061 s/iter. Inference: 0.2641 s/iter. Eval: 0.1330 s/iter. Total: 0.4033 s/iter. ETA=0:05:01
[02/17 18:44:17] mask2former INFO: Inference done 357/1092. Dataloading: 0.0061 s/iter. Inference: 0.2647 s/iter. Eval: 0.1329 s/iter. Total: 0.4038 s/iter. ETA=0:04:56
[02/17 18:44:22] mask2former INFO: Inference done 371/1092. Dataloading: 0.0061 s/iter. Inference: 0.2643 s/iter. Eval: 0.1325 s/iter. Total: 0.4030 s/iter. ETA=0:04:50
[02/17 18:44:28] mask2former INFO: Inference done 383/1092. Dataloading: 0.0061 s/iter. Inference: 0.2653 s/iter. Eval: 0.1320 s/iter. Total: 0.4035 s/iter. ETA=0:04:46
[02/17 18:44:33] mask2former INFO: Inference done 396/1092. Dataloading: 0.0061 s/iter. Inference: 0.2650 s/iter. Eval: 0.1322 s/iter. Total: 0.4034 s/iter. ETA=0:04:40
[02/17 18:44:38] mask2former INFO: Inference done 408/1092. Dataloading: 0.0060 s/iter. Inference: 0.2662 s/iter. Eval: 0.1323 s/iter. Total: 0.4046 s/iter. ETA=0:04:36
[02/17 18:44:43] mask2former INFO: Inference done 420/1092. Dataloading: 0.0061 s/iter. Inference: 0.2668 s/iter. Eval: 0.1324 s/iter. Total: 0.4055 s/iter. ETA=0:04:32
[02/17 18:44:48] mask2former INFO: Inference done 433/1092. Dataloading: 0.0061 s/iter. Inference: 0.2666 s/iter. Eval: 0.1321 s/iter. Total: 0.4049 s/iter. ETA=0:04:26
[02/17 18:44:53] mask2former INFO: Inference done 446/1092. Dataloading: 0.0060 s/iter. Inference: 0.2664 s/iter. Eval: 0.1318 s/iter. Total: 0.4044 s/iter. ETA=0:04:21
[02/17 18:44:59] mask2former INFO: Inference done 460/1092. Dataloading: 0.0060 s/iter. Inference: 0.2662 s/iter. Eval: 0.1311 s/iter. Total: 0.4034 s/iter. ETA=0:04:14
[02/17 18:45:04] mask2former INFO: Inference done 474/1092. Dataloading: 0.0060 s/iter. Inference: 0.2665 s/iter. Eval: 0.1302 s/iter. Total: 0.4028 s/iter. ETA=0:04:08
[02/17 18:45:09] mask2former INFO: Inference done 487/1092. Dataloading: 0.0060 s/iter. Inference: 0.2667 s/iter. Eval: 0.1301 s/iter. Total: 0.4030 s/iter. ETA=0:04:03
[02/17 18:45:14] mask2former INFO: Inference done 500/1092. Dataloading: 0.0060 s/iter. Inference: 0.2663 s/iter. Eval: 0.1303 s/iter. Total: 0.4027 s/iter. ETA=0:03:58
[02/17 18:45:19] mask2former INFO: Inference done 514/1092. Dataloading: 0.0060 s/iter. Inference: 0.2658 s/iter. Eval: 0.1298 s/iter. Total: 0.4018 s/iter. ETA=0:03:52
[02/17 18:45:25] mask2former INFO: Inference done 526/1092. Dataloading: 0.0060 s/iter. Inference: 0.2662 s/iter. Eval: 0.1299 s/iter. Total: 0.4023 s/iter. ETA=0:03:47
[02/17 18:45:30] mask2former INFO: Inference done 539/1092. Dataloading: 0.0060 s/iter. Inference: 0.2663 s/iter. Eval: 0.1300 s/iter. Total: 0.4024 s/iter. ETA=0:03:42
[02/17 18:45:35] mask2former INFO: Inference done 552/1092. Dataloading: 0.0060 s/iter. Inference: 0.2663 s/iter. Eval: 0.1300 s/iter. Total: 0.4024 s/iter. ETA=0:03:37
[02/17 18:45:40] mask2former INFO: Inference done 565/1092. Dataloading: 0.0060 s/iter. Inference: 0.2663 s/iter. Eval: 0.1300 s/iter. Total: 0.4024 s/iter. ETA=0:03:32
[02/17 18:45:46] mask2former INFO: Inference done 578/1092. Dataloading: 0.0062 s/iter. Inference: 0.2664 s/iter. Eval: 0.1298 s/iter. Total: 0.4024 s/iter. ETA=0:03:26
[02/17 18:45:51] mask2former INFO: Inference done 592/1092. Dataloading: 0.0062 s/iter. Inference: 0.2659 s/iter. Eval: 0.1293 s/iter. Total: 0.4015 s/iter. ETA=0:03:20
[02/17 18:45:56] mask2former INFO: Inference done 604/1092. Dataloading: 0.0062 s/iter. Inference: 0.2665 s/iter. Eval: 0.1295 s/iter. Total: 0.4022 s/iter. ETA=0:03:16
[02/17 18:46:01] mask2former INFO: Inference done 617/1092. Dataloading: 0.0062 s/iter. Inference: 0.2666 s/iter. Eval: 0.1294 s/iter. Total: 0.4023 s/iter. ETA=0:03:11
[02/17 18:46:07] mask2former INFO: Inference done 630/1092. Dataloading: 0.0062 s/iter. Inference: 0.2669 s/iter. Eval: 0.1293 s/iter. Total: 0.4024 s/iter. ETA=0:03:05
[02/17 18:46:12] mask2former INFO: Inference done 643/1092. Dataloading: 0.0062 s/iter. Inference: 0.2669 s/iter. Eval: 0.1289 s/iter. Total: 0.4021 s/iter. ETA=0:03:00
[02/17 18:46:17] mask2former INFO: Inference done 657/1092. Dataloading: 0.0062 s/iter. Inference: 0.2666 s/iter. Eval: 0.1287 s/iter. Total: 0.4016 s/iter. ETA=0:02:54
[02/17 18:46:22] mask2former INFO: Inference done 670/1092. Dataloading: 0.0063 s/iter. Inference: 0.2665 s/iter. Eval: 0.1288 s/iter. Total: 0.4017 s/iter. ETA=0:02:49
[02/17 18:46:28] mask2former INFO: Inference done 683/1092. Dataloading: 0.0063 s/iter. Inference: 0.2667 s/iter. Eval: 0.1288 s/iter. Total: 0.4020 s/iter. ETA=0:02:44
[02/17 18:46:33] mask2former INFO: Inference done 696/1092. Dataloading: 0.0063 s/iter. Inference: 0.2668 s/iter. Eval: 0.1288 s/iter. Total: 0.4020 s/iter. ETA=0:02:39
[02/17 18:46:38] mask2former INFO: Inference done 709/1092. Dataloading: 0.0063 s/iter. Inference: 0.2668 s/iter. Eval: 0.1290 s/iter. Total: 0.4022 s/iter. ETA=0:02:34
[02/17 18:46:43] mask2former INFO: Inference done 721/1092. Dataloading: 0.0063 s/iter. Inference: 0.2671 s/iter. Eval: 0.1291 s/iter. Total: 0.4025 s/iter. ETA=0:02:29
[02/17 18:46:48] mask2former INFO: Inference done 734/1092. Dataloading: 0.0063 s/iter. Inference: 0.2672 s/iter. Eval: 0.1287 s/iter. Total: 0.4024 s/iter. ETA=0:02:24
[02/17 18:46:54] mask2former INFO: Inference done 747/1092. Dataloading: 0.0063 s/iter. Inference: 0.2671 s/iter. Eval: 0.1290 s/iter. Total: 0.4024 s/iter. ETA=0:02:18
[02/17 18:46:59] mask2former INFO: Inference done 760/1092. Dataloading: 0.0063 s/iter. Inference: 0.2667 s/iter. Eval: 0.1293 s/iter. Total: 0.4025 s/iter. ETA=0:02:13
[02/17 18:47:04] mask2former INFO: Inference done 772/1092. Dataloading: 0.0063 s/iter. Inference: 0.2669 s/iter. Eval: 0.1294 s/iter. Total: 0.4027 s/iter. ETA=0:02:08
[02/17 18:47:09] mask2former INFO: Inference done 786/1092. Dataloading: 0.0063 s/iter. Inference: 0.2667 s/iter. Eval: 0.1294 s/iter. Total: 0.4025 s/iter. ETA=0:02:03
[02/17 18:47:14] mask2former INFO: Inference done 798/1092. Dataloading: 0.0063 s/iter. Inference: 0.2669 s/iter. Eval: 0.1296 s/iter. Total: 0.4029 s/iter. ETA=0:01:58
[02/17 18:47:20] mask2former INFO: Inference done 811/1092. Dataloading: 0.0063 s/iter. Inference: 0.2668 s/iter. Eval: 0.1297 s/iter. Total: 0.4029 s/iter. ETA=0:01:53
[02/17 18:47:25] mask2former INFO: Inference done 824/1092. Dataloading: 0.0063 s/iter. Inference: 0.2670 s/iter. Eval: 0.1297 s/iter. Total: 0.4031 s/iter. ETA=0:01:48
[02/17 18:47:30] mask2former INFO: Inference done 838/1092. Dataloading: 0.0063 s/iter. Inference: 0.2666 s/iter. Eval: 0.1296 s/iter. Total: 0.4026 s/iter. ETA=0:01:42
[02/17 18:47:36] mask2former INFO: Inference done 851/1092. Dataloading: 0.0063 s/iter. Inference: 0.2666 s/iter. Eval: 0.1296 s/iter. Total: 0.4026 s/iter. ETA=0:01:37
[02/17 18:47:41] mask2former INFO: Inference done 864/1092. Dataloading: 0.0063 s/iter. Inference: 0.2665 s/iter. Eval: 0.1296 s/iter. Total: 0.4024 s/iter. ETA=0:01:31
[02/17 18:47:46] mask2former INFO: Inference done 877/1092. Dataloading: 0.0063 s/iter. Inference: 0.2663 s/iter. Eval: 0.1297 s/iter. Total: 0.4023 s/iter. ETA=0:01:26
[02/17 18:47:51] mask2former INFO: Inference done 890/1092. Dataloading: 0.0062 s/iter. Inference: 0.2663 s/iter. Eval: 0.1298 s/iter. Total: 0.4024 s/iter. ETA=0:01:21
[02/17 18:47:56] mask2former INFO: Inference done 903/1092. Dataloading: 0.0063 s/iter. Inference: 0.2662 s/iter. Eval: 0.1299 s/iter. Total: 0.4025 s/iter. ETA=0:01:16
[02/17 18:48:02] mask2former INFO: Inference done 916/1092. Dataloading: 0.0062 s/iter. Inference: 0.2663 s/iter. Eval: 0.1299 s/iter. Total: 0.4026 s/iter. ETA=0:01:10
[02/17 18:48:07] mask2former INFO: Inference done 930/1092. Dataloading: 0.0062 s/iter. Inference: 0.2662 s/iter. Eval: 0.1297 s/iter. Total: 0.4022 s/iter. ETA=0:01:05
[02/17 18:48:12] mask2former INFO: Inference done 944/1092. Dataloading: 0.0062 s/iter. Inference: 0.2659 s/iter. Eval: 0.1295 s/iter. Total: 0.4017 s/iter. ETA=0:00:59
[02/17 18:48:17] mask2former INFO: Inference done 950/1092. Dataloading: 0.0063 s/iter. Inference: 0.2673 s/iter. Eval: 0.1308 s/iter. Total: 0.4045 s/iter. ETA=0:00:57
[02/17 18:48:23] mask2former INFO: Inference done 957/1092. Dataloading: 0.0063 s/iter. Inference: 0.2685 s/iter. Eval: 0.1322 s/iter. Total: 0.4072 s/iter. ETA=0:00:54
[02/17 18:48:28] mask2former INFO: Inference done 965/1092. Dataloading: 0.0064 s/iter. Inference: 0.2695 s/iter. Eval: 0.1331 s/iter. Total: 0.4091 s/iter. ETA=0:00:51
[02/17 18:48:33] mask2former INFO: Inference done 976/1092. Dataloading: 0.0064 s/iter. Inference: 0.2700 s/iter. Eval: 0.1338 s/iter. Total: 0.4102 s/iter. ETA=0:00:47
[02/17 18:48:38] mask2former INFO: Inference done 985/1092. Dataloading: 0.0064 s/iter. Inference: 0.2709 s/iter. Eval: 0.1343 s/iter. Total: 0.4117 s/iter. ETA=0:00:44
[02/17 18:48:44] mask2former INFO: Inference done 998/1092. Dataloading: 0.0064 s/iter. Inference: 0.2708 s/iter. Eval: 0.1341 s/iter. Total: 0.4114 s/iter. ETA=0:00:38
[02/17 18:48:49] mask2former INFO: Inference done 1012/1092. Dataloading: 0.0064 s/iter. Inference: 0.2705 s/iter. Eval: 0.1339 s/iter. Total: 0.4109 s/iter. ETA=0:00:32
[02/17 18:48:54] mask2former INFO: Inference done 1025/1092. Dataloading: 0.0064 s/iter. Inference: 0.2705 s/iter. Eval: 0.1339 s/iter. Total: 0.4109 s/iter. ETA=0:00:27
[02/17 18:48:59] mask2former INFO: Inference done 1037/1092. Dataloading: 0.0064 s/iter. Inference: 0.2707 s/iter. Eval: 0.1339 s/iter. Total: 0.4111 s/iter. ETA=0:00:22
[02/17 18:49:04] mask2former INFO: Inference done 1050/1092. Dataloading: 0.0064 s/iter. Inference: 0.2706 s/iter. Eval: 0.1338 s/iter. Total: 0.4110 s/iter. ETA=0:00:17
[02/17 18:49:10] mask2former INFO: Inference done 1064/1092. Dataloading: 0.0064 s/iter. Inference: 0.2704 s/iter. Eval: 0.1335 s/iter. Total: 0.4104 s/iter. ETA=0:00:11
[02/17 18:49:15] mask2former INFO: Inference done 1078/1092. Dataloading: 0.0063 s/iter. Inference: 0.2701 s/iter. Eval: 0.1333 s/iter. Total: 0.4099 s/iter. ETA=0:00:05
[02/17 18:49:20] mask2former INFO: Inference done 1092/1092. Dataloading: 0.0063 s/iter. Inference: 0.2698 s/iter. Eval: 0.1331 s/iter. Total: 0.4093 s/iter. ETA=0:00:00
[02/17 20:06:07] detectron2.engine.hooks INFO: Overall training speed: 24232 iterations in 16:52:36 (2.5073 s / it)
[02/17 20:06:07] detectron2.engine.hooks INFO: Total training time: 18:06:09 (1:13:32 on hooks)
